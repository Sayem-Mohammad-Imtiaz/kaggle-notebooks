{"cells":[{"metadata":{},"cell_type":"markdown","source":"**CORD-19 Extract Related Documents using Bert**\n* This script introduces useful functions to extract most related papers or paragraphs based on a query using the STOA NLP pretrained models, Bert.\n* This task is regarded as Next-Sequence predication. The query is the first sequence and the paper or paragraph is the second sequence. Bert can generate a sequence-relation score to evaluate the possibility that the second sequence is the next sequence of the first sequence.\n* Given one query, we test each paper or paragraph and rank the sequence-relation scores. The higher the score, the more related the paper with the query."},{"metadata":{},"cell_type":"markdown","source":"**Load Package**\n* For Bert, we use the huggingface implementation from \"https://github.com/huggingface/transformers\".\n* I use Pytorch as the deep learning framework and tensor process.\n* tqdm is a good process bar."},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport os\nfrom transformers import *\nimport torch\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Process Data**\n* In the dataset, there are four folders and each folder contains a list of json files. Each json file has the information of a document.\n* To simplify the processing later, we organize the list of json files of documents in a single folder to one json file. \n* The output json file is a list, each item is a dictionary for a document."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_docs_json (data_folder, save_path=None, save_file_name='full_docs.json'):\n    #################################################################\n    # Create json file for whole docs and save it to current path\n    #\n    # input: \n    # data_folder - the data folder path contains paper json files\n    # \n    # output: \n    # full_docs - a list of dictionary, each dct has the paper info\n    #################################################################\n\n    full_docs = []\n    for jsonfile in os.listdir(data_folder):\n        paper_entity = {}\n        with open(os.path.join(data_folder, jsonfile), 'r', encoding='utf8') as f:\n            paper = json.load(f)\n        paper_entity['paper_id'] = paper['paper_id']\n        paper_entity['title'] = paper['metadata']['title']\n        abstract = paper['abstract']\n        abstract = [item['text'].strip() for item in abstract]\n        abstract = ' '.join(abstract)\n        paper_entity['abstract'] = abstract\n        body_text = paper['body_text']\n        paper_entity['body_text'] = body_text\n        full_docs.append(paper_entity)\n    if save_path is not None:\n        save_file = os.path.join(save_path, save_file_name)\n    else:\n        save_file = save_file_name\n    with open(save_file, 'w', encoding='utf8') as wf:\n        json.dump(full_docs, wf)\n    return full_docs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Example**\n* Processing all documents in biorxiv_medrxiv foldere into one json file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example:\nfull_docs_biorxiv = create_docs_json(\"/kaggle/input/CORD-19-research-challenge/2020-03-13/biorxiv_medrxiv/biorxiv_medrxiv\", save_file_name='full_docs_full_docs_biorxiv.json')\nprint(\"There are \" + str(len(full_docs_biorxiv)) + \" documents organized into one json file.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract Related Documents Using Bert**\n* In this script, I did not use mini-batch, i.e. Each document is a input data, to process the data because Bert is a large model. People may need to use different batch size for different hardware setting. You can modify the functions with mini-batch to speed up the extraction."},{"metadata":{},"cell_type":"markdown","source":"**Functions**\n* text_similarity_score: Compare the query text and candidate candidate and compute the sequence-relation score. The model is Bert model, you can either use pre-trained Bert model or fine-tuning by yourself. The input for the Bert model is the concatenation of query and candidate. We add a special token \"[SEP]\" between query and candidate to facilite Bert understand the sequence better. As one of the most basic Bert model \"bert-base-uncased\" has a max-sequence length 512, we will sequence input to length 512. Therefore, you might need to find interesting queries for various questions and interesting text to using this. \n* extract_similar_papers: Given a single query and the list of documents returned from function \"create_docs_json\" (you can either use the return value or load from saved json), this function computes the score between query with each document in the list and rank them from high to low. This function returns the top N ranked documents (you can set N with the parameter num_rets). In this function, I use the title+abstract as the candidate for each paper/document. Query will be compared with the title+abstract. You can find your own way to represent the informaiton of a paper/document.\n* extract_similar_txtsegs: Similar as the previous function, but extract text paragraphs or segments instead of documents. Given a single query and the list of documents returned from function \"create_docs_json\" (you can either use the return value or load from saved json), this function computes the score between query with each paragraphs in all documents and rank them from high to low. This function returns the top N ranked paragraphs (you can set N with the parameter num_rets)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def text_similarity_score (query, candidate, model, tokenizer, max_seq_length=512):\n    #################################################################\n    # Compute similarity score between query text and candidate text\n    # input: \n    # query - the query text for comparison\n    # candidate - candidate text for comparison\n    # model - comparison model\n    # tokenizer - tokenizer for tokenzing the plain text\n    #\n    # output: \n    # txt_sim_score - similarity score\n    #################################################################\n\n    txt = query.strip() + \"[SEP]\" + candidate.strip()\n    input_ids = torch.tensor(tokenizer.encode(txt, add_special_tokens=True, max_length=max_seq_length)).unsqueeze(0)\n    outputs = model(input_ids)\n    txt_sim_score = outputs[0][0][0].item()\n    return txt_sim_score\n\n\ndef extract_similar_papers (query, data_dct, model, tokenizer, num_rets=30):\n    #################################################################\n    # Compute Extract most related papers for the query\n    # input: \n    # query - the query text for extraction\n    # data_dct - data structure contians all paper information\n    # model - similarity comparison model\n    # tokenizer - tokenizer for tokenzing the plain text\n    # num_rets(optional) - default=30. the number of papers to extract.\n    #\n    # output: \n    # a list of most related papers. [(score, [paper_id, txt]) ... ]\n    #################################################################\n\n    print(\"Extracting similar papers ....\")\n    print(\"Query is :\")\n    print(query)\n    paper_scores = []\n    for paper in tqdm(data_dct):\n        paper_id = paper['paper_id']\n        candidate = paper['title'].strip()+\". \"+paper['abstract'].strip()\n        score = text_similarity_score(query, candidate, model, tokenizer)\n        paper_item = (score, [paper_id, candidate])\n        paper_scores.append(paper_item)\n    paper_scores.sort(key=lambda x: x[0], reverse=True)\n    return paper_scores[:num_rets]\n\n\ndef extract_similar_txtsegs (query, data_dct, model, tokenizer, num_rets=60):\n    #################################################################\n    # Compute Extract most related text segments for the query\n    # input: \n    # query - the query text for extraction\n    # data_dct - data structure contians all paper information\n    # model - similarity comparison model\n    # tokenizer - tokenizer for tokenzing the plain text\n    # num_rets(optional) - default=30. the number of segments to extract.\n    #\n    # output: \n    # a list of most related segments. [(score, [paper_id, txt]) ... ]\n    #################################################################\n\n    print(\"Extracting similar text segments ....\")\n    print(\"Query is :\")\n    print(query)\n    seg_scores = []\n    for paper in tqdm(data_dct):\n        paper_id = paper['paper_id']\n        candidates = paper['body_text']\n        candidates = [item['text'].strip() for item in candidates]\n        for candidate in candidates:\n            score = text_similarity_score(query, candidate, model, tokenizer)\n            seg_item = (score, [paper_id, candidate])\n            seg_scores.append(seg_item)\n    seg_scores.sort(key=lambda x: x[0], reverse=True)\n    return seg_scores[:num_rets]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Example using these functions**\n* We use the json file for biorxiv track. \n* In this example, I only use the \"bert-base-uncased\". You can use other pre-trained model or fine-tuning by yourself to get better results."},{"metadata":{},"cell_type":"markdown","source":"**Define variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_backbone = '/kaggle/input/pytorchbertbaseuncased/bert-base-uncased/bert-base-uncased-vocab.txt'\nmodel_backbone = '/kaggle/input/pytorchbertbaseuncased/bert-base-uncased/'\nfull_docs_json = \"/kaggle/working/full_docs_full_docs_biorxiv.json\"\nquery = \"The range of incubation periods for the disease in humans is important to learn.\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Load Model ... \")\ntokenizer = BertTokenizer.from_pretrained(tokenizer_backbone)\nmodel = BertForNextSentencePrediction.from_pretrained(model_backbone)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Load Data ... \")\nif full_docs_json is not None and full_docs_json!=\"\":\n    with open(full_docs_json, \"r\", encoding=\"utf8\") as f:\n        full_docs = json.load(f)\nelse:\n    full_docs = create_docs_json(\"2020-03-13/biorxiv_medrxiv/biorxiv_medrxiv\", save_file_name='full_docs_full_docs_biorxiv.json')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract Documents**\n* The runing time is a little slow. You can try by yourself or use mini-batch to speed up."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Extracting documents .... \")\nif full_docs_json is not None and full_docs_json!=\"\":\n    extracted_papers = extract_similar_papers(query, full_docs, model, tokenizer)\n    print(extracted_papers[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract Paragraphs/Segments**\n* The runing time is a little slow. You can try by yourself or use mini-batch to speed up."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Extracting paragraphs .... \")\n# if full_docs_json is not None and full_docs_json!=\"\":\n    # extracted_segs = extract_similar_txtsegs(query, full_docs, model, tokenizer)\n    # print(extracted_segs[10])\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}