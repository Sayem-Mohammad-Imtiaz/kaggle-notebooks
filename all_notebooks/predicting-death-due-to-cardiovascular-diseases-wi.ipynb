{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting death due to Cardiovascular diseases with Scikit-learn\n\n\n## Main statistics, how to predict with scikit-learn, comparaison of algorithms and conclusion\n. \n\nThis notebook is covering 4 main studies: \n\n* Understand the main staistics about healtcare related to cardiovascular diseases  \n* Discover how to predict death of a patient using scikit-learn\n* Compare several ML algorithms using a loop \n* conclusion\n\nthe notebook include information and tips to shape data, optimise algorithms and plot intresting plots.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## LIBRAIRIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## import Data and reshape"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['anaemia'] = df['anaemia'].astype(float)\ndf['creatinine_phosphokinase'] = df['creatinine_phosphokinase'].astype(float)\ndf['diabetes'] = df['diabetes'].astype(float)\ndf['ejection_fraction'] = df['ejection_fraction'].astype(float)\ndf['high_blood_pressure'] = df['high_blood_pressure'].astype(float)\ndf['platelets'] = df['platelets'].astype(float)\ndf['serum_creatinine'] = df['serum_creatinine'].astype(float)\ndf['serum_sodium'] = df['serum_sodium'].astype(float)\ndf['sex'] = df['sex'].astype(float)\ndf['smoking'] = df['smoking'].astype(float)\ndf['time'] = df['time'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocess data\nbins =(-1,0.5,2)\ngroups_names=['survived','dead']\ndf['DEATH_EVENT'] = pd.cut(df['DEATH_EVENT'], bins=bins, labels = groups_names)\ndf['DEATH_EVENT'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_quality=LabelEncoder()\ndf['DEATH_EVENT'] = label_quality.fit_transform(df['DEATH_EVENT'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['DEATH_EVENT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.countplot(df['DEATH_EVENT'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we can separate the dataset as response variable and feature variable\nX = df.drop('DEATH_EVENT', axis = 1)\ny = df['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some statistics and studies before using ML tools"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorr = df.corr() #Correlation matrix for CB player\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nplt.matshow(corr, cmap='RdBu', fignum=fig.number)\nplt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical');\nplt.yticks(range(len(corr.columns)), corr.columns);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We compare platelets statistics between survivors and dead patient, we dont see any significant difference expect more extrems values within the dead group"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df[['platelets', 'DEATH_EVENT']].boxplot(by='DEATH_EVENT', figsize=(10,6))\nax.set_ylabel('platelets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same analysis is made with a factor that is showed by the correlation matrix to be important in the death probability. \nIt is confirmed by the next plot with a significant difference of death regarding the ejection fraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df[['ejection_fraction', 'DEATH_EVENT']].boxplot(by='DEATH_EVENT', figsize=(10,6))\nax.set_ylabel('ejection_fraction')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['age'].plot(kind='density', figsize=(14,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare inputs for ML tools"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_inputs = df[['age','anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction',\n'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium',\n'sex', 'smoking', 'time']].values\n# extracting quality labels\nall_labels = df['DEATH_EVENT'].values\n# a test to see what the inputs look like\nall_inputs[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train and split the data\nX_train, X_test, y_train, y_test = train_test_split(all_inputs, all_labels, test_size= 0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test of firsts values\nX_train[:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data ready to be compiled into ML tools - Test of few ML tools "},{"metadata":{},"cell_type":"markdown","source":"# Decision tree  Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#trying decision tree classfier \n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create the classifier\ndecision_tree_classifier = DecisionTreeClassifier()\n\n\n# Train the classifier on the training set\ndecision_tree_classifier.fit(X_train, y_train)\n\n\n# Validate the classifier on the testing set using classification accuracy\ndecision_tree_classifier.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators = 200)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rfc[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see  the RFC efficients \nprint(classification_report(y_test, pred_rfc))\nprint(confusion_matrix(y_test,pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Validate the classifier on the testing set using classification accuracy\nrfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # SVM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"Clf = svm.SVC()\nClf.fit(X_train,y_train)\npref_clf = Clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see  the SVM efficients \nprint(classification_report(y_test, pred_rfc))\nprint(confusion_matrix(y_test,pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Validate the classifier on the testing set using classification accuracy\nClf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing other  ML methods using a loop and comparing results! "},{"metadata":{"trusted":true},"cell_type":"code","source":"#selecting the models and the model names in an array\nmodels=[LogisticRegression(),\n        LinearSVC(),\n        SVC(kernel='rbf'),\n        KNeighborsClassifier(),\n        RandomForestClassifier(),\n        DecisionTreeClassifier(),\n        GradientBoostingClassifier(),\n        GaussianNB()]\nmodel_names=['Logistic Regression',\n             'Linear SVM',\n             'rbf SVM',\n             'K-Nearest Neighbors',\n             'Random Forest Classifier',\n             'Decision Tree',\n             'Gradient Boosting Classifier',\n             'Gaussian NB']\n\n\n# creating an accuracy array and a matrix to join the accuracy of the models\n# and the name of the models so we can read the results easier\nacc=[]\nm={}\n\n\n# next we're going to iterate through the models, and get the accuracy for each\nfor model in range(len(models)):\n     clf=models[model]\n     clf.fit(X_train,y_train)\n     pred=clf.predict(X_test)\n     acc.append(accuracy_score(pred,y_test))\n\n\nm={'Algorithm':model_names,'Accuracy':acc}\n\n\n# just putting the matrix into a data frame and listing out the results\nacc_frame=pd.DataFrame(m)\nacc_frame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How to optimize parameters for a specific algorithm. Random forest classifier case"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier = RandomForestClassifier()\n\n\n# setting up the parameters for our grid search\n# You can check out what each of these parameters mean on the Scikit webiste!\n# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\nparameter_grid = {'n_estimators': [10, 25, 50, 100, 200],\n'max_features': ['auto', 'sqrt', 'log2'],\n'criterion': ['gini', 'entropy'],\n'max_features': [1, 2, 3, 4]}\n\n\n# Stratified K-Folds cross-validator allows us mix up the given test/train data per run\n# with k-folds each test set should not overlap across all shuffles. This allows us to \n# ultimately have \"more\" test data for our model\ncross_validation = StratifiedKFold(n_splits=10)\n\n\n# running the grid search function with our random_forest_classifer, our parameter grid\n# defineda bove, and our cross validation method\ngrid_search = GridSearchCV(random_forest_classifier,\nparam_grid=parameter_grid,\ncv=cross_validation)\n\n\n# using the defined grid search above, we're going to test it out on our\n# data set\ngrid_search.fit(all_inputs, all_labels)\n\n\n# printing the best scores, parameters, and estimator for our Random Forest classifer\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\n\n\ngrid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n\n          max_depth=None, max_features=2, max_leaf_nodes=None,\n\n          min_impurity_decrease=0.0, min_impurity_split=None,\n\n          min_samples_leaf=1, min_samples_split=2,\n\n          min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n\n          oob_score=False, random_state=None, verbose=0,\n\n          warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test of the new optimised Random forest classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier = grid_search.best_estimator_\n\n\nrf_df = pd.DataFrame({'accuracy': cross_val_score(random_forest_classifier, all_inputs, all_labels, cv=10),\n                      'classifier': ['Random Forest'] * 10})\nrf_df.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='classifier', y='accuracy', data=rf_df)\nsns.stripplot(x='classifier', y='accuracy', data=rf_df, jitter=True, color='black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}