{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install swifter\n!pip install wordninja\n!pip install pandarallel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\n##_______________________________________________________________________________________________________________________________________\n\n\n## pandas, numpy\nimport pandas as pd\nimport numpy as np\nfrom numpy import random\nfrom tqdm import tqdm\ntqdm.pandas()\n# import swifter\n##_______________________________________________________________________________________________________________________________________\n\n\n## gensim\nimport gensim\nfrom gensim.models import KeyedVectors,Word2Vec\n##_______________________________________________________________________________________________________________________________________\n\n\n## sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import f1_score,precision_score,recall_score\n##_______________________________________________________________________________________________________________________________________\n\n\n## nltk, nlp, spacy\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.chunk import conlltags2tree\nfrom nltk.tree import Tree\nfrom nltk.tag import StanfordNERTagger\nfrom nltk.tokenize import word_tokenize\n# import wordninja\nimport re\nfrom textblob import TextBlob\nimport spacy\nfrom spacy import displacy\nimport en_core_web_sm\n##_______________________________________________________________________________________________________________________________________\n\n\n## plotting\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n##_______________________________________________________________________________________________________________________________________\n\n\n## imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n##_______________________________________________________________________________________________________________________________________\n\n\n## tensorflow, keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Convolution1D,GlobalMaxPool1D,GlobalMaxPooling1D,Attention,Dropout,Dense,Embedding,Bidirectional,GRU,GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.utils import to_categorical\n##_______________________________________________________________________________________________________________________________________\n\n\nimport pickle\nimport heapq\nfrom collections import OrderedDict\nfrom collections import Counter\nfrom multiprocessing import Pool\nfrom string import punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## basic cleaning\n\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n#     text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    return text\n    \n# df['consumer_complaint_narrative'] = df.consumer_complaint_narrative.astype(str)\n# df['clean_narrative'] = df['consumer_complaint_narrative'].swifter.apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## advanced cleaning\nstop_words = set(stopwords.words('english'))\nlem = WordNetLemmatizer()\n\ndef removal(text):\n    text = re.sub('XXXX',' UNKNOWN ',text)\n    text = re.sub('XX/XX/','',text)\n    text = re.sub('UNKNOWN   UNKNOWN','UNKNOWN',text)\n    text = re.sub('\\n',' ',text)\n    text = re.sub('  ',' ',text)\n    return text\n\ndef cleaning(text):\n    text = removal(text)\n    #text = text.lower()\n    words = word_tokenize(text)\n    words = [w for w in words if w not in stop_words]\n    #words = [w for w in words if len(w)>2]\n    words = [lem.lemmatize(w,'v') for w in words]\n    return ' '.join(words)\n\ndef total_clean(text):\n    \n    text = text.lower()\n    text = re.sub('[^A-Za-z0-9]',' ',text)\n    text = cleaning(text)\n    text = wordninja.split(text)\n    text = ' '.join(text)\n#     text = remove3ConsecutiveDuplicates(text)\n    #blob = TextBlob(text)\n    #text = blob.correct()\n    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n    #text = word_tokenize(str(text))\n    #text = ' '.join([w for w in text if len(w)>1])\n    #text = re.sub('sap','asap',str(text))\n    return text\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['consumer_complaint_narrative'] = df.consumer_complaint_narrative.astype(str)\n# df['clean_narrative'] = df['consumer_complaint_narrative'].swifter.apply(total_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling_dict = {'Closed with explanation': 8000, 'Closed with non-monetary relief': 8361, 'Closed with monetary relief': 4969,\n# 'Closed': 1722, 'Untimely response':520}\n# undersample = RandomUnderSampler(sampling_strategy=sampling_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_under, y_under = undersample.fit_resample(df[['consumer_complaint_narrative', 'company_public_response',\n#        'clean_narrative']], df.company_response_to_consumer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_under[y_under =='Untimely response'] = 'Closed with explanation'\n# y_under[y_under =='Closed'] = 'Closed with explanation'\n# y_under.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling_dict = {'Closed with explanation': 10242, 'Closed with non-monetary relief': 10000, 'Closed with monetary relief': 10000}\n# oversample = RandomOverSampler(sampling_strategy=sampling_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_over, y_over = oversample.fit_resample(X_under, y_under)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_over.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer = Tokenizer()\n# x = X_over.clean_narrative\n# y = pd.get_dummies(y_over).values\n\n# tokenizer.fit_on_texts(x)\n# seq = tokenizer.texts_to_sequences(x)\n# pad_seq = pad_sequences(seq,maxlen = 500,padding='post',truncating='pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab_size = len(tokenizer.word_index)+1\n# vocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word2vec = KeyedVectors.load_word2vec_format('/kaggle/input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin', \\\n#         binary=True)\n# print('Found %s word vectors of word2vec' % len(word2vec.vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_matrix = np.zeros((vocab_size, 300))\n# for word, i in tokenizer.word_index.items():\n#     if word in word2vec.vocab:\n#         embedding_matrix[i] = word2vec.word_vec(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = Sequential()\n# model.add(Embedding(vocab_size,300,input_length=500,weights = [embedding_matrix],trainable = False))\n# model.add(Bidirectional(GRU(32,return_sequences=True)))\n# #model.add(Convolution1D(32,2,activation='relu'))\n# #model.add(Convolution1D(64,3,activation = 'relu'))\n# model.add(GlobalMaxPooling1D())\n# model.add(Dense(32,activation = 'relu'))\n# #model.add(Dropout(0.2))\n# model.add(Dense(3,activation = 'softmax'))\n\n# model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy',f1_m,precision_m,recall_m])\n\n# filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n# callbacks_list = [checkpoint]\n\n# model.fit(pad_seq,y,batch_size = 128,epochs = 8,validation_split = 0.10,callbacks=callbacks_list)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# * **Tf-IDF top 500 word extraction**"},{"metadata":{"trusted":true},"cell_type":"code","source":" \n# from sklearn.feature_extraction.text import CountVectorizer\n# import re\n\n# stop_words = set(stopwords.words('english'))\n# #get the text column \n# docs=clean_df['Total Clean Text'].to_list()\n \n\n# cv=CountVectorizer(max_df=0.85,stop_words=stop_words)\n# word_count_vector=cv.fit_transform(docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# import re\n\n# stop_words = set(stopwords.words('english'))\n# #get the text column \n# docs=clean_df['Total Clean Text'].to_list()\n \n\n# cv=CountVectorizer(max_df=0.85,stop_words=stop_words)\n# word_count_vector=cv.fit_transform(docs)\n# from sklearn.feature_extraction.text import TfidfTransformer\n\n# feature_names=cv.get_feature_names()\n# tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n# tfidf_transformer.fit(word_count_vector) \n\n# def extract_topn_from_vector(doc,topn=500):\n    \n#     tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n#     coo_matrix = tf_idf_vector.tocoo()\n    \n#     top500=heapq.nlargest(topn, coo_matrix.data)\n    \n#     dictionary = OrderedDict(dict())\n    \n#     for idx,score in list(zip(coo_matrix.col,coo_matrix.data)):\n#         dictionary[feature_names[idx]]=score\n    \n#     words = [w for w in doc.split() if w not in stop_words]\n#     try:\n        \n#         results = OrderedDict({x:dictionary[x] for x in words if dictionary[x] in top500})\n#         return ' '.join(results.keys())\n#     except:\n#         print('not converted')\n#         return doc\n\n# # urgent_complaints['top_500'] = urgent_complaints['Total Clean'].swifter.apply(extract_topn_from_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# def extract_topn_from_vector(doc,topn=500):\n    \n#     tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n#     coo_matrix = tf_idf_vector.tocoo()\n    \n#     top500=heapq.nlargest(topn, coo_matrix.data)\n    \n#     dictionary = OrderedDict(dict())\n    \n#     for idx,score in list(zip(coo_matrix.col,coo_matrix.data)):\n#         dictionary[feature_names[idx]]=score\n    \n#     words = [w for w in doc.split() if w not in stop_words]\n#     try:\n        \n#         results = OrderedDict({x:dictionary[x] for x in words if dictionary[x] in top500})\n#         return ' '.join(results.keys())\n#     except:\n#         print('not converted')\n#         return doc\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_names=cv.get_feature_names()\n \n# # get the document that we want to extract keywords from\n# doc=clean_df['Total Clean Text'][1]\n\n# keywords=extract_topn_from_vector(doc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# urgent_complaints['top_500'] = urgent_complaints['Total Clean'].swifter.apply(extract_topn_from_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pickle\n# with open('tfidf_nostopw.pk', 'wb') as fin:\n#     pickle.dump(tfidf_transformer, fin)\n# fin.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# le = LabelEncoder()\n# tokenizer = Tokenizer()\n# x = urgent_complaints['top_500']\n# y = le.fit_transform(urgent_complaints['Target'])\n\n# tokenizer.fit_on_texts(x)\n# seq = tokenizer.texts_to_sequences(x)\n# pad_seq = pad_sequences(seq,maxlen = 500,padding='post',truncating='pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab_size = len(tokenizer.word_index)+1\n# vocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word2vec = KeyedVectors.load_word2vec_format('/kaggle/input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin', \\\n#         binary=True)\n# print('Found %s word vectors of word2vec' % len(word2vec.vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_matrix = np.zeros((vocab_size, 300))\n# for word, i in tokenizer.word_index.items():\n#     if word in word2vec.vocab:\n#         embedding_matrix[i] = word2vec.word_vec(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras import backend as K\n\n# def recall_m(y_true, y_pred):\n#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#     recall = true_positives / (possible_positives + K.epsilon())\n#     return recall\n\n# def precision_m(y_true, y_pred):\n#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n#     precision = true_positives / (predicted_positives + K.epsilon())\n#     return precision\n\n# def f1_m(y_true, y_pred):\n#     precision = precision_m(y_true, y_pred)\n#     recall = recall_m(y_true, y_pred)\n#     return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def LSTM_model():\n    \n#     model = Sequential()\n#     # input layer\n#     model.add(Embedding(vocab_size,300,input_length=500,weights = [embedding_matrix],trainable = False))\n \n#     # LSTM layer\n#     model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n    \n# #     # hidden layer with dropout\n# #     model.add(Dense(32,activation = 'relu'))\n# #     model.add(Dropout(0.2))\n\n#     # output layer\n#     model.add(Dense(1,activation = 'sigmoid'))\n\n#     model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy',f1_m,precision_m,recall_m])\n\n#     filepath=\"f1_score-{epoch:02d}-{val_f1_m:.2f}.h5\"\n\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def CNN_model():\n#     model = Sequential()\n    \n#     # input layer\n#     model.add(Embedding(vocab_size,300,input_length=500,weights = [embedding_matrix],trainable = False))\n    \n#     # CNN layer\n#     # model.add(Convolution1D(32,2,activation='relu'))\n#     model.add(Convolution1D(64,3,activation = 'relu'))\n    \n#     # pooling layer\n#     model.add(GlobalMaxPooling1D())\n    \n#     # hidden layer\n#     model.add(Dense(32,activation = 'relu'))\n#     model.add(Dropout(0.2))\n\n#     # output layer\n#     model.add(Dense(1,activation = 'sigmoid'))\n\n#     model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy',f1_m,precision_m,recall_m])\n\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Bi directional CRU for binary classification\n\n# def BiDirGRU_model():\n#     model = Sequential()\n    \n#     #input layer\n#     model.add(Embedding(vocab_size,300,input_length=500,weights = [embedding_matrix],trainable = False))\n    \n#     # Bi-Directional GRU layer\n#     model.add(Bidirectional(GRU(64,return_sequences=True)))\n\n#     # pooling layer\n#     model.add(GlobalMaxPooling1D())\n#     model.add(Dense(32,activation = 'relu'))\n    \n# #     # hidden layer\n# #     model.add(Dense(32,activation = 'relu'))\n# #     model.add(Dropout(0.2))\n\n#     # output layer\n#     model.add(Dense(1,activation = 'sigmoid'))\n\n#     model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy',f1_m,precision_m,recall_m])\n\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filepath=\"f1_score-{epoch:02d}-{val_f1_m:.2f}.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n# es = EarlyStopping(monitor='val_loss',patience = 5)\n# callbacks_list = [es,checkpoint]\n# estimator1 = KerasClassifier(build_fn = LSTM_model, epochs=20, batch_size=64 ,  callbacks=callbacks_list)\n# estimator2 = KerasClassifier(build_fn = CNN_model, epochs=20, batch_size=64 ,  callbacks=callbacks_list)\n# estimator3 = KerasClassifier(build_fn = BiDirGRU_model, epochs=20, batch_size=64 ,  callbacks=callbacks_list)\n\n# kfold = StratifiedKFold(n_splits=10, shuffle=True)\n# results = cross_val_score(pipeline, pad_seq, y, cv=kfold, fit_params = {'mlp__callbacks': callbacks_list})\n# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filepath=\"LSTM_f1_score-{epoch:02d}-{val_f1_m:.2f}.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n# es = EarlyStopping(monitor='val_loss',patience = 5)\n# callbacks_list = [es,checkpoint]\n\n# model1 = LSTM_model()\n# model1.fit(pad_seq,y,batch_size = 64,epochs = 30, validation_split = 0.20, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filepath=\"Bi-Dir-GRUf1_score-{epoch:02d}-{val_f1_m:.2f}.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n# es = EarlyStopping(monitor='val_loss',patience = 5)\n# callbacks_list = [es,checkpoint]\n\n# model2 = BiDirGRU_model()\n# model2.fit(pad_seq,y,batch_size = 64,epochs = 30, validation_split = 0.25, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filepath=\"CNN-f1_score-{epoch:02d}-{val_f1_m:.2f}.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n# es = EarlyStopping(monitor='val_loss',patience = 5)\n# callbacks_list = [es,checkpoint]\n\n# model3 = CNN_model()\n# model3.fit(pad_seq,y,batch_size = 64,epochs = 30, validation_split = 0.20, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# st = StanfordNERTagger('/kaggle/input/stanfordenglish3class/english.all.3class.distsim.crf.ser',\\\n#                         '/kaggle/input/stanfordnerjar/stanford-ner.jar',\\\n#                         encoding='utf-8')\n\n# def stanfordNE2BIO(tagged_sent):\n#     bio_tagged_sent = []\n#     prev_tag = \"O\"\n#     for token, tag in tagged_sent:\n#         if tag == \"O\": #O\n#             bio_tagged_sent.append((token, tag))\n#             prev_tag = tag\n#             continue\n#         if tag != \"O\" and prev_tag == \"O\": # Begin NE\n#             bio_tagged_sent.append((token, \"B-\"+tag))\n#             prev_tag = tag\n#         elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n#             bio_tagged_sent.append((token, \"I-\"+tag))\n#             prev_tag = tag\n#         elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n#             bio_tagged_sent.append((token, \"B-\"+tag))\n#             prev_tag = tag\n\n#     return bio_tagged_sent\n\n# def stanfordNE2tree(text):\n    \n#     tokenized_text = word_tokenize(text)\n#     ne_tagged_sent = st.tag(tokenized_text)\n    \n#     bio_tagged_sent = stanfordNE2BIO(ne_tagged_sent)\n#     sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n#     sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n\n#     sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n#     ne_tree = conlltags2tree(sent_conlltags)\n    \n#     ne_in_sent=[]\n#     for subtree in ne_tree:\n#         if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n#             ne_label = subtree.label()\n#             ne_string = \" \".join([token for token, pos in subtree.leaves()])\n#             ne_in_sent.append((ne_string, ne_label))\n#     return ne_in_sent\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean_df['entities'] = clean_df['Total Clean Text'].swifter.apply(entities)\n# stanfordNE2tree(clean_df['Total Clean Text'][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nlp = en_core_web_sm.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def spacy_ner(text):\n#     doc = nlp(text)\n#     organisation = [X.text for X in doc.ents if X.label_=='ORG']\n#     return ', '.join(organisation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean_df['entities'] = clean_df['Total Clean Text'].swifter.apply(spacy_ner)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_partitions = 4 #number of partitions to split dataframe\n# num_cores = 4 #number of cores on your machine\n\n# def parallelize_dataframe(df, func):\n#     df_split = np.array_split(df, num_partitions)\n#     pool = Pool(num_cores)\n#     df = pd.concat(pool.map(func, df_split))\n#     pool.close()\n#     pool.join()\n#     return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def multiply_columns(data):\n#     data['organisation'] = data['Total Clean Text'].swifter.apply(spacy_ner)\n#     return data\n# clean_df = parallelize_dataframe(clean_df, multiply_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandarallel\n# pandarallel.pandarallel.initialize(progress_bar= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean_df['Organisation'] = clean_df['Total Clean Text'].parallel_apply(spacy_ner)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# companies = pd.read_csv('/kaggle/working/All_organisations.csv', names= ['organisations'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# companies.organisations = companies.organisations.str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"companies = pd.Dataframe(companies.organisations.unique(),column=['organisations'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# org = pd.read_csv('/kaggle/input/company-corpus/All_organisations.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom word embedding using Wrod2vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_data = pd.read_csv('/kaggle/input/cfpbmarch2020/complaints-2020-05-04_21_58.csv')\n# embedding_data['Consumer complaint narrative'] = embedding_data['Consumer complaint narrative'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_data = embedding_data.drop_duplicates(subset='Consumer complaint narrative')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## basic cleaning\n\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\nlem = WordNetLemmatizer()\n\ndef corpus_related_cleaning(text):\n    text = re.sub('XXXX',' UNKNOWN ',text)\n    text = re.sub('XX/XX/','',text)\n    text = re.sub('UNKNOWN   UNKNOWN','UNKNOWN',text)\n    text = re.sub('\\n',' ',text)\n    text = re.sub('  ',' ',text)\n\n    return text\n\ndef clean_text(text):\n    \n    \n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n#     text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = corpus_related_cleaning(text)\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    words = word_tokenize(text)\n#     text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    words = [lem.lemmatize(w,'v') for w in words if w not in STOPWORDS]\n#     words = [lem.lemmatize(w,'v') for w in words]\n    return ' '.join(words)\n    \n# df['consumer_complaint_narrative'] = df.consumer_complaint_narrative.astype(str)\n\n\n# df['clean_narrative'] = df['consumer_complaint_narrative'].swifter.apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# ## multiprocessing method 1\n\n\n# import pandas as pd\n# import numpy as np\n# import seaborn as sns\n# from multiprocessing import Pool\n\n# num_partitions = 4 #number of partitions to split dataframe\n# num_cores = 4 #number of cores on your machine\n\n# def parallelize_dataframe(df, func):\n#     df_split = np.array_split(df, num_partitions)\n#     pool = Pool(num_cores)\n#     df = pd.concat(pool.map(func, df_split))\n#     pool.close()\n#     pool.join()\n#     return df\n\n# def multiply_columns(data):\n#     data['Clean_narrative'] = data['Consumer complaint narrative'].swifter.apply(clean_text)\n#     return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# embedding_data = parallelize_dataframe(embedding_data, multiply_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# import tensorflow as tf\n# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# import pandarallel\n# pandarallel.pandarallel.initialize(progress_bar= True)\n\n# test_data['Clean_narrative'] = test_data['Consumer complaint narrative'].parallel_apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_data.to_csv('cleaned_mar20.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# %%time\n\n# ## multiprocessing method 2\n# import multiprocessing as mp\n# from tqdm import tqdm\n\n# pool = mp.Pool(mp.cpu_count())\n\n# Clean_narratives = pool.map(clean_text, embedding_data['Consumer complaint narrative'])\n# pool.terminate()\n# pool.join()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# creating training corpus for custom embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_data = pd.read_csv('/kaggle/input/cleanedmar20/cleaned_mar20.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_data['Company response to consumer'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_data = embedding_data[(embedding_data['Company response to consumer'].str.contains('Closed with non-monetary relief')) | (embedding_data['Company response to consumer'].str.contains('Closed with monetary relief'))]\nembedding_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# train_sentences = list(embedding_data.Clean_narrative.swifter.apply(str.split).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# %%time\n\n# model = gensim.models.Word2Vec(sentences=train_sentences, size=300, workers=4)\n# model.save('custom_corpus_500k.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n# custom_embed = Word2Vec.load(\"/kaggle/working/custom_corpus_500k.model\")\ncustom_embed = Word2Vec.load('/kaggle/input/customembed/custom_corpus_500k.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_embed.most_similar('fraud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(custom_embed.wv.vocab.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_data['Company response to consumer'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_data = embedding_data.fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom yellowbrick.text import TSNEVisualizer\nfrom yellowbrick.datasets import load_hobbies\n\n# Load the data and create document vectors\ncorpus = load_hobbies()\ntfidf = TfidfVectorizer()\n\nX = tfidf.fit_transform(embedding_data['Clean_narrative'][:3000])\ny = embedding_data['Company response to consumer']\n\n# Create the visualizer and draw the vectors\ntsne = TSNEVisualizer()\ntsne.fit(X, y)\ntsne.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# import re\n\n# stop_words = set(stopwords.words('english'))\n# #get the text column \n# docs=embedding_data['Clean_narrative'].to_list()\n\n# cv=CountVectorizer(max_df=0.85,stop_words=stop_words)\n# word_count_vector=cv.fit_transform(docs)\n# from sklearn.feature_extraction.text import TfidfTransformer\n \n# tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n# tfidf_transformer.fit(word_count_vector)\n\n# feature_names=cv.get_feature_names()\n\n# def extract_topn_from_vector(doc,topn=500):\n    \n#     tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n#     coo_matrix = tf_idf_vector.tocoo()\n    \n#     top500=heapq.nlargest(topn, coo_matrix.data)\n    \n#     dictionary = OrderedDict(dict())\n    \n#     for idx,score in list(zip(coo_matrix.col,coo_matrix.data)):\n#         dictionary[feature_names[idx]]=score\n    \n#     words = [w for w in doc.split() if w not in stop_words]\n#     try:\n        \n#         results = OrderedDict({x:dictionary[x] for x in words if dictionary[x] in top500})\n#         return ' '.join(results.keys())\n#     except:\n#         print('not converted')\n#         return doc\n\n# embedding_data['top_500'] = embedding_data['Clean_narrative'].parallel_apply(extract_topn_from_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ntokenizer = Tokenizer()\nx = embedding_data['Clean_narrative']\ny = le.fit_transform(embedding_data['Company response to consumer'])\n# y=pd.get_dummies(embedding_data['Company response to consumer']).values\n\ntokenizer.fit_on_texts(x)\nseq = tokenizer.texts_to_sequences(x)\npad_seq = pad_sequences(seq,maxlen = 500,padding='post',truncating='pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling_dict = {0: 27480, 1: 27480}\n# undersample = RandomUnderSampler(sampling_strategy=sampling_dict)\n\n# X_under, y_under = undersample.fit_resample(pad_seq, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('tokenizer_bin.pkl','wb') as fin:\n    pickle.dumps(tokenizer)\nfin.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# ids = embedding_data[\"Consumer complaint narrative\"]\n# embedding_data[ids.isin(ids[ids.duplicated()])].sort_values('Consumer complaint narrative').to_csv('duplicates.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)+1\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n    if word in custom_embed.wv.vocab:\n        embedding_matrix[i] = custom_embed.wv.word_vec(word)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## model for imbalanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n#input layer\nmodel.add(Embedding(vocab_size,300,input_length=500,weights = [embedding_matrix],trainable = True))\n\n# Bi-Directional GRU layer\nmodel.add(Bidirectional(GRU(64,return_sequences=True)))\n\n# pooling layer\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(32,activation = 'relu'))\n\n#     # hidden layer\n#     model.add(Dense(32,activation = 'relu'))\n#     model.add(Dropout(0.2))\n\n# output layer\nmodel.add(Dense(1,activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy',f1_m,precision_m,recall_m])\n\nfilepath=\"Bi-Dir-GRUf1_score-{epoch:02d}-{val_f1_m:.2f}.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nes = EarlyStopping(monitor='val_loss',patience = 4)\ncallbacks_list = [es,checkpoint]\n# model2.fit(pad_seq,y,batch_size = 32,epochs = 15, validation_split = 0.25, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load model\n\nloaded_model = tf.keras.models.load_model('/kaggle/input/binmodelmonetarygru/Bi-Dir-GRUf1_score-03-0.90.h5',custom_objects={'f1_m':f1_m,'precision_m':precision_m,'recall_m':recall_m})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(pad_seq,y,test_size = 0.1,random_state= 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## fitting imbalanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train,y_train,batch_size = 32,epochs = 3, validation_split = 0.1, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## fitting balanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_under,y_under,test_size = 0.1,random_state= 42)\n\nmodel.fit(x_train,y_train,batch_size = 32,epochs = 5, validation_split = 0.1, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = loaded_model.predict(x_test)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score,precision_score,recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall = []\nfor thresh in np.arange(0,1,0.01):\n    thresh = np.round(thresh,2)\n    print('Recall Score at threshold {0} is {1}'.format(thresh,recall_score(y_test,(predictions>thresh).astype(int))))\n    recall.append(recall_score(y_test,(predictions>thresh).astype(int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = []\nfor thresh in np.arange(0,1,0.01):\n    thresh = np.round(thresh,2)\n    print('Precision Score at threshold {0} is {1}'.format(thresh,precision_score(y_test,(predictions>thresh).astype(int))))\n    precision.append(precision_score(y_test,(predictions>thresh).astype(int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = []\nfor thresh in np.arange(0,1,0.01):\n    thresh = np.round(thresh,2)\n    print('F1 Score at threshold {0} is {1}'.format(thresh,f1_score(y_test,(predictions>thresh).astype(int))))\n    f1.append(f1_score(y_test,(predictions>thresh).astype(int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_axis = range(0,100)\n# plt.plot(x_axis,recall,label = 'Recall')\n# plt.plot(x_axis,precision,label = 'Precision')\n# plt.plot(x_axis,f1,label = 'F1 Score')\n\n# plt.legend()\n\n# plt.show()\n\nx_axis = range(0,100)\nplt.figure(figsize=(20,12))\nplt.plot(x_axis,recall,label = 'Recall')\nplt.plot(x_axis,precision,label = 'Precision')\nplt.plot(x_axis,f1,label = 'F1 Score')\nplt.legend()\n\n\nplt.xticks(range(100))\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_prediction = []\nfor i in tqdm(range(len(x_test))):\n    if predictions[i]>0.5:\n        x_test_prediction.append(1)\n    else:\n        x_test_prediction.append(0)\n\n# prediction = np.array(april_prediction_2_classes)\n# encoded = np.array(df2['Encoded'].values)\n#encoded\nprint(classification_report(y_test,x_test_prediction))\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.heatmap(confusion_matrix(y_test,x_test_prediction),annot = True,fmt = 'g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_prediction = []\nfor i in tqdm(range(len(x_test))):\n    if predictions[i]>0.43:\n        x_test_prediction.append(1)\n    else:\n        x_test_prediction.append(0)\n\n# prediction = np.array(april_prediction_2_classes)\n# encoded = np.array(df2['Encoded'].values)\n#encoded\nprint(classification_report(y_test,x_test_prediction))\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.heatmap(confusion_matrix(y_test,x_test_prediction),annot = True,fmt = 'g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\nprecision, recall, thresholds = precision_recall_curve(y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc\nauc = auc(recall, precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the precision-recall curves\nno_skill = len(y_test[y_test==1]) / len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\nplt.plot(recall, precision, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score,precision_score,recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/testapril/complaints-2020-05-07_09_29.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\nloaded_model = tf.keras.models.load_model('/kaggle/input/binmodelmonetarygru/Bi-Dir-GRUf1_score-03-0.90.h5',custom_objects={'f1_m':f1_m,'precision_m':precision_m,'recall_m':recall_m})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandarallel\npandarallel.pandarallel.initialize(progress_bar= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['Clean_narrative'] = test_data['Consumer complaint narrative'].progress_apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df1 = test_data[test_data['Company response to consumer']=='Closed with explanation']\ntest_df1.reset_index(drop = True,inplace = True)\ntest_df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nb = tokenizer.texts_to_sequences(test_df1['Clean_narrative'])\nb_pad = pad_sequences(b,maxlen=500)\nresults = loaded_model.predict(b_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(results).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"april_prediction = []\nfor i in tqdm(range(len(test_df1))):\n    if results[i]>0.5:\n        april_prediction.append(1)\n    else:\n        april_prediction.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(april_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = test_data[test_data['Company response to consumer']!='Closed with explanation']\ndf2.reset_index(drop = True,inplace = True)\ndf2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['Clean_narrative'] = df2['Consumer complaint narrative'].progress_apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nb = tokenizer.texts_to_sequences(df2['Clean_narrative'])\nb_pad = pad_sequences(b,maxlen=500)\nresults = loaded_model.predict(b_pad)\n\napril_prediction_2_classes = []\n\nfor i in tqdm(range(len(df2))):\n  \n    if results[i]>0.29:\n        april_prediction_2_classes.append(1)\n    else:\n        april_prediction_2_classes.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['threshold 0.5'] = april_prediction_2_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['threshold 0.43'] = april_prediction_2_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['threshold 0.29'] = april_prediction_2_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.to_csv('cross_tab.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(april_prediction_2_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encoding(text):\n    if text == 'Closed with non-monetary relief':\n        return int(1)\n    elif text == \"Closed with monetary relief\":\n        return int(0)\n    \ndf2['Encoded'] = df2['Company response to consumer'].apply(encoding)\ndf2['Encoded'] = df2['Encoded'].astype(\"int64\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = np.array(april_prediction_2_classes)\nencoded = np.array(df2['Encoded'].values)\n#encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction.shape,encoded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(encoded,prediction))\nsns.heatmap(confusion_matrix(encoded,prediction),annot = True,fmt = 'g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(encoded,prediction))\nsns.heatmap(confusion_matrix(encoded,prediction),annot = True,fmt = 'g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(encoded,prediction))\nsns.heatmap(confusion_matrix(encoded,prediction),annot = True,fmt = 'g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall = []\nfor thresh in np.arange(0,1,0.01):\n    thresh = np.round(thresh,2)\n    print('Recall Score at threshold {0} is {1}'.format(thresh,recall_score(df2['Encoded'],(results>thresh).astype(int))))\n    recall.append(recall_score(df2['Encoded'],(results>thresh).astype(int)))\nprecision = []\nfor thresh in np.arange(0,1,0.01):\n    thresh = np.round(thresh,2)\n    print('Precision Score at threshold {0} is {1}'.format(thresh,precision_score(df2['Encoded'],(results>thresh).astype(int))))\n    precision.append(precision_score(df2['Encoded'],(results>thresh).astype(int)))\nf1 = []\nfor thresh in np.arange(0,1,0.01):\n    thresh = np.round(thresh,2)\n    print('F1 Score at threshold {0} is {1}'.format(thresh,f1_score(df2['Encoded'],(results>thresh).astype(int))))\n    f1.append(f1_score(df2['Encoded'],(results>thresh).astype(int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_axis = range(0,100)\nplt.figure(figsize=(20,12))\nplt.plot(x_axis,recall,label = 'Recall')\nplt.plot(x_axis,precision,label = 'Precision')\nplt.plot(x_axis,f1,label = 'F1 Score')\nplt.legend()\n\n\nplt.xticks(range(100))\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"4+3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"april_prediction_2_classes = []\nfor i in tqdm(range(len(df2))):\n  \n    if results[i]>0.38:\n        april_prediction_2_classes.append(1)\n    else:\n        april_prediction_2_classes.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = np.array(april_prediction_2_classes)\nencoded = np.array(df2['Encoded'].values)\n#encoded\nprint(classification_report(encoded,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(confusion_matrix(encoded,prediction),annot = True,fmt = 'g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## final model evaluation on test and out of time data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\nprediction = []\nfor i in tqdm(range(len(x_test))):\n    if predictions[i]>0.5:\n        prediction.append(1)\n    else:\n        prediction.append(0)\nencoded = y_test\n\nprint(classification_report(encoded,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(confusion_matrix(encoded,prediction),annot = True,fmt = 'g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"34/(34+484)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([df2['Clean_narrative'],df2['Encoded'],prediction], columns=['narrative','encoded','pred']).to_csv('swap_analysis.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['Encoded'].shape,results.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}