{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hitters - Non-Linear Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Aim\nThe aim in this notebook is to create non-linear models that predict salaries of baseball players based on their statistics and info, \nand to reduce RMSE (Root Mean Square Error) as much as possible.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Description\n**Context**\n\nThis dataset is part of the R-package ISLR and is used in the related book by G. James et al. (2013) \"An Introduction to Statistical Learning with applications in R\" to demonstrate how Ridge regression and the LASSO are performed using R.\n\n**Content**\n\nThis dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n**Format**\n\nA data frame with 322 observations of major league players on the following 20 variables.\n\n**AtBat**: Number of times at bat in 1986\n\n**Hits**: Number of hits in 1986\n\n**HmRun**: Number of home runs in 1986 \n\n**Runs**: Number of runs in 1986 \n\n**RBI**: Number of runs batted in in 1986 \n\n**Walks**: Number of walks in 1986 \n\n**Years**: Number of years in the major leagues \n\n**CAtBat**: Number of times at bat during his career \n\n**CHits**: Number of hits during his career \n\n**CHmRun**: Number of home runs during his career \n\n**CRuns**: Number of runs during his career \n\n**CRBI**: Number of runs batted in during his career \n\n**CWalks**: Number of walks during his career \n\n**League**: A factor with levels A and N indicating player’s league at the end of 1986 \n\n**Division**: A factor with levels E and W indicating player’s division at the end of 1986 \n\n**PutOuts**: Number of put outs in 1986 \n\n**Assists**: Number of assists in 1986 \n\n**Errors**: Number of errors in 1986 \n\n**Salary**: 1987 annual salary on opening day in thousands of dollars \n\n**NewLeague**: A factor with levels A and N indicating player’s league at the beginning of 1987","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler, Normalizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n!pip install xgboost\nimport xgboost\nfrom xgboost import XGBRegressor\n!pip install lightgbm\nfrom lightgbm import LGBMRegressor\n!pip install catboost\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hitters = pd.read_csv('../input/hitters-baseball-data/Hitters.csv')\ndf = hitters.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creation and assignment of new variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['HitRatio'] = df['Hits'] / df['AtBat']\ndf['RunRatio'] = df['HmRun'] / df['Runs']\ndf['CHitRatio'] = df['CHits'] / df['CAtBat']\ndf['CRunRatio'] = df['CHmRun'] / df['CRuns']\n\ndf['Avg_AtBat'] = df['CAtBat'] / df['Years']\ndf['Avg_Hits'] = df['CHits'] / df['Years']\ndf['Avg_HmRun'] = df['CHmRun'] / df['Years']\ndf['Avg_Runs'] = df['CRuns'] / df['Years']\ndf['Avg_RBI'] = df['CRBI'] / df['Years']\ndf['Avg_Walks'] = df['CWalks'] / df['Years']\ndf['Avg_PutOuts'] = df['PutOuts'] / df['Years']\ndf['Avg_Assists'] = df['Assists'] / df['Years']\ndf['Avg_Errors'] = df['Errors'] / df['Years']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Changing cathegorical variables into binary using Label Encoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndf['League'] = le.fit_transform(df['League'])\ndf['Division'] = le.fit_transform(df['Division'])\ndf['NewLeague'] = le.fit_transform(df['NewLeague'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping N/A values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping outliers using Local Outlier Factor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nclf.fit_predict(df)\ndf_scores = clf.negative_outlier_factor_\nnp.sort(df_scores)[0:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thrs = np.sort(df_scores)[3]\nthrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df_scores < thrs].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating dependent and independent variables for the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfx = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfx = dfx.drop(['AtBat','Hits','HmRun','Runs','RBI','Salary','League','Division','NewLeague'], axis = 1)\n# I dropped 'Salary' since it's dependent variable\n# I dropped 'League', 'Division' and 'NewLeague' in order to perform a better scaling\n# I dropped the others because some of the new assigned variables are better representatives","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardization usin Robust Scaler and assigning X and y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = dfx.columns\nscaler = RobustScaler()\nX = scaler.fit_transform(dfx)\nX = pd.DataFrame(X, columns = cols)\ny = df[['Salary']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Train and Test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state  = 46)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a function that performs required operations using model names","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_func(alg):\n    if alg == CatBoostRegressor:\n        model = alg(verbose = False).fit(X_train, y_train)\n    else:\n        model = alg().fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    model_name = alg.__name__\n    print(model_name, 'RMSE: ', RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a function that performs hiperparameter optimization using model names and parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_func(alg, **param):\n    \n    model = alg().fit(X_train, y_train)\n    params = {}\n    for key, value in param.items():\n        params[key] = value\n    \n    cv_model = GridSearchCV(model, params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train, y_train)\n    print(cv_model.best_params_)\n    tuned_model = alg(**cv_model.best_params_).fit(X_train, y_train)\n    y_pred = tuned_model.predict(X_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    model_name = alg.__name__\n    print(model_name, 'RMSE: ', RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RMSE values for all non-linear models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [KNeighborsRegressor, SVR, MLPRegressor, GradientBoostingRegressor, DecisionTreeRegressor, RandomForestRegressor, XGBRegressor, LGBMRegressor, CatBoostRegressor]\nfor model in models:\n    model_func(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RMSE values for all non-linear models after hiperparemeter optimization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = KNeighborsRegressor, n_neighbors = np.arange(2,30,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = SVR, C = [0.01, 0.02, 0.2, 0.1, 0.5, 0.8, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = MLPRegressor, alpha = [0.1, 0.02, 0.01, 0.001, 0.0001], hidden_layer_sizes = [(10,20), (5,5), (100,100)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = GradientBoostingRegressor, max_depth = [3,5,8], learning_rate = [0.001,0.01,0.1], n_estimators = [100,200,500,1000], subsample = [0.3,0.5,0.8,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = DecisionTreeRegressor, max_depth = [2,3,4,5,10,20], min_samples_split = [2,5,10,20,30,50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = RandomForestRegressor, max_depth = [5,10,None], max_features = [5,10,15,20], n_estimators = [500, 1000], min_samples_split = [2,5,20,30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = XGBRegressor, max_depth = [2,3,4,5,8], learning_rate = [0.1,0.5,0.01], n_estimators = [100,200,500,1000], colsample_bytree = [0.4,0.7,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_func(alg = LGBMRegressor, max_depth = [1,2,3,4,5,6,7,8,9,10], n_estimators = [20,40,100,200,500,1000], learning_rate = [0.1,0.01,0.5,1])","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"cv_func(alg = CatBoostRegressor, iterations = [200], learning_rate = [0.02, 0.03, 0.05], depth = [8, 10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# final model can differ after each run. Gradient Boosting Regressor was the best when I ran the code, so I created final model manually using its stats.\nfinal_model = GradientBoostingRegressor(learning_rate = 0.01, max_depth = 5, n_estimators = 500, subsample = 0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comments","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Hitters Dataset is read\n\n#### Data Preprocessing \n\n* New variables are created using some of the existing variables.\n* N/A values are dropped.\n* Cathegorical variables are turned into 1-0 labels using Label Encoder.\n* Local Outlier Factor is used to make outlier analysis and outliers are dropped.\n* Standardization is performed dropping the dependent variable(Salary) and some other variables. \n* Dependent and independent variables are assigned and split into train and test sets.\n\n#### Modelling\n\n* Two functions are defined. First one performs model creation, prediction and calculation of RMSE value using the model name. The second function\ndoes the same thing using Grid Search and different parameters. \n* Models are created using functions with following non-linear models : KNeighborsRegressor, SVR, MLPRegressor, GradientBoostingRegressor, \nDecisionTreeRegressor, RandomForestRegressor, XGBRegressor, LGBMRegressor, CatBoostRegressor\n* Final model is created with the non-linear model that results the least and its optimum hiperparameter.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}