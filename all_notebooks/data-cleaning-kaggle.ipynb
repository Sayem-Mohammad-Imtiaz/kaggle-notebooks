{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data Cleaning Exercise**","metadata":{}},{"cell_type":"code","source":"#All our necesary libraries and the data\nimport pandas as pd\nimport numpy as np\n\n#read in the data\nnfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")\n\n#set seed for reproducibility\nnp.random.seed(0) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check the head of the data\n# We can already see a few data points missing\nnfl_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the info on the data\nnfl_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#How much data is missing\nmissing_vals = nfl_data.isnull().sum()\n\n#Take a look at the first 15 columns\nmissing_vals[0:15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#What is the percentage missing cells\n\ntotal_cells = np.product(nfl_data.shape)\ntotal_missing = missing_vals.sum()\n\npercent_missing = (total_missing/total_cells)*100\nprint ('{}%'.format(percent_missing))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That implies about a quarter of the data is missing. That is a lot**","metadata":{}},{"cell_type":"code","source":"#Several ways to go about handling our missing values\n#One of the less recommended approaches but an easy quick approach nonetheless is removing all the unavailable data\n#Take note to remove data accross columns not the rows\ndropped_data = nfl_data.dropna(axis=1)\ndropped_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets confirm if really all na_values were dropped\ndropped_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the data, we can see although all na_values were dropped successfully, 61 columns were lost in the process\nprint('Columns of original data: %d \\n' % nfl_data.shape[1])\nprint('Columns with dropped data: %d \\n' % dropped_data.shape[1])\nprint('Amount of lost data: %d' % (nfl_data.shape[1] - dropped_data.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's try to break down our data into a smaller subset for better usability**","metadata":{}},{"cell_type":"code","source":"#Get a small subset of the NFL dataset\nsubset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\nsubset_nfl_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's attempt filling all the missing values with 0\nsubset_nfl_data.fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Another method will be to fill all na_values with values that comes directly after it in the same column\n#Fill any remaining na_value with 0\nsubset_nfl_data.fillna(method='bfill', axis=0).fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Let's attempt another dataset**","metadata":{}},{"cell_type":"code","source":"#read in the data and check the head\nfrisco_permits = pd.read_csv('../input/building-permit-applications-data/Building_Permits.csv')\nfrisco_permits.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frisco_permits.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set seed for reproducibility\nnp.random.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Amount of missing values\nmissing_values = frisco_permits.isnull().sum()\nmissing_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Percentage missing values\ntotal_cells = np.product(frisco_permits.shape)\ntotal_missing_values = missing_values.sum()\n\npercent_missing = (total_missing_values/total_cells)*100\nprint('The percentage of missing values is %.2f'%percent_missing+ '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**26.26% is more than a quarter of our data missing**","metadata":{}},{"cell_type":"code","source":"frisco_permits[['Street Number Suffix','Zipcode']].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets try a rows missing values removal\nfrisco_na_rows_dropped = frisco_permits.dropna(axis=0)\nfrisco_na_rows_dropped","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All the rows in our dataset have na_values hence the dropping of all the data**","metadata":{}},{"cell_type":"code","source":"#Lets try a columns missing values removal\nfrisco_na_cols_dropped = frisco_permits.dropna(axis=1)\nfrisco_na_cols_dropped","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frisco_na_cols_dropped.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('We lost {} columns'.format(frisco_permits.shape[1] - frisco_na_cols_dropped.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using bfill to fill in NaN values\nfrisco_permits_with_na_imputed = frisco_permits.fillna(method='bfill', axis=0).fillna(0)\nfrisco_permits_with_na_imputed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frisco_permits_with_na_imputed.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SCALING AND NORMALIZATION** ","metadata":{}},{"cell_type":"markdown","source":"**Scaling in Data analysis refers to changing numeric values to fit in a range of 0 - 1**","metadata":{}},{"cell_type":"code","source":"#Let's import our scaling tool and a few visualization libraries\nfrom mlxtend.preprocessing import minmax_scaling \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Set seed for reproducibility\nnp.random.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets generate data we can scale and normalize so we see it in practice\noriginal_data = np.random.exponential(size=1000)\noriginal_data[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Minmax Scaling the original data between 0 and 1\nscaled_data = minmax_scaling(original_data, columns=[0])\nscaled_data[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets do a visualization for comparison\nfig, ax = plt.subplots(1,2)\nsns.histplot(original_data, ax=ax[0], kde=True, alpha=0.5)\nax[0].set_title('Original Data')\nsns.histplot(scaled_data, ax=ax[1], kde=True, alpha=0.5)\nax[1].set_title('Scaled Data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalization is the transformation of a data range into a normal distribution**","metadata":{}},{"cell_type":"code","source":"#Lets import the Box-Cox Normalization tool\nfrom scipy import stats\n\n#Normalize the original data\nnormalized_data = stats.boxcox(original_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets do a comparison via visualization \nfig, ax = plt.subplots(1,2)\nsns.histplot(original_data, ax=ax[0], kde=True, alpha=0.5)\nax[0].set_title('Original Data')\nsns.histplot(normalized_data, ax=ax[1], kde=True, alpha=0.5)\nax[1].set_title('Normalized Data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let us attempt the same processes with another dataset**","metadata":{}},{"cell_type":"code","source":"#Firstly, we'll import all the tools and libraries we need\nimport numpy as np\nimport pandas as pd\n\nfrom mlxtend.preprocessing import minmax_scaling\nfrom scipy import stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Read in the data\nkickstarters_2017 = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv')\n\n#Set seed for reproducibility\nnp.random.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check out the information and head on our dataframe\nkickstarters_2017.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kickstarters_2017.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will be working with the goals of each campaign(usd_goal_real). We will scale and view plots to see differences between and after scaling**","metadata":{}},{"cell_type":"code","source":"#Get the original data\noriginal_data = pd.DataFrame(kickstarters_2017['usd_goal_real'])\n\n#Scale the original data\nscaled_data = minmax_scaling(original_data, columns=['usd_goal_real'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the original and scaled data for comparison\nfig, ax = plt.subplots(1,2, figsize=(15,6))\nsns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0], kde=True)\nax[0].set_title('Original Data')\nsns.distplot(scaled_data, ax=ax[1], kde=True)\nax[1].set_title('Scaled Data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets do a print of the original data vs scaled data\nprint('Original Data\\nPreview:\\n', original_data.head())\nprint('Minimum value: ', float(original_data.min()),'\\nMaximum value: ', float(original_data.max()))\nprint('='*30)\nprint('Scaled Data\\nPreview:\\n', scaled_data.head())\nprint('Minimum value:', float(scaled_data.min()),'\\nMaximum value:', float(scaled_data.max()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Now, let us scale and normalize the goal and usd_pledged columns resp.**","metadata":{}},{"cell_type":"code","source":"#Select the goal column and transform to a dataframe\noriginal_goal_data = pd.DataFrame(kickstarters_2017.goal)\n\n#Scale the data \nscaled_goal_data = minmax_scaling(original_goal_data, columns=['goal'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let us visualize\nfig, ax = plt.subplots(1,2,figsize=(10,3))\nsns.distplot(original_goal_data, ax=ax[0])\nax[0].set_title('Original Goal Data')\nsns.distplot(scaled_goal_data, ax=ax[1])\nax[1].set_title('Scaled Goal Data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now, let us normalize the usd_pledged column\npledges = (kickstarters_2017.usd_pledged_real)\n#normalized_pledges = stats.boxcox(pledges)\n\n#NOTE: Box-Cox only accepts +ve data\n#Lets get all the positive pledges, we'll do this by getting their indexes and that to get the values\nindex_pos_pledge = kickstarters_2017.usd_pledged_real > 0\npositive_pledges = kickstarters_2017.usd_pledged_real.loc[index_pos_pledge]\n\n# Normalizing the positive pledges with Box-Cox\nnormalized_pledges = pd.Series(stats.boxcox(positive_pledges)[0],\n                              name='usd_pledged_real',\n                              index=positive_pledges.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize the original and normalized data\nfig, ax = plt.subplots(1,2,figsize=(10,3))\nsns.distplot(positive_pledges, ax=ax[0])\nax[0].set_title('Original Data')\nsns.distplot(normalized_pledges, ax=ax[1])\nax[1].set_title('Normalized Data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Original data\\nPreview:\\n', positive_pledges.head())\nprint('Minimum value:', float(positive_pledges.min()),\n      '\\nMaximum value:', float(positive_pledges.max()))\nprint('='*30)\n\nprint('Normalized data\\nPreview:\\n', normalized_pledges.head())\nprint('Minimum value:', float(normalized_pledges.min()),\n      '\\nMaximum value:', float(normalized_pledges.max()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **PARSING DATES**","metadata":{}},{"cell_type":"code","source":"#We already have a couple of our needed libraries loaded, let's load one more for this exercise and read in the data\n\nimport datetime\n\nlandslides = pd.read_csv('../input/landslide-events/catalog.csv')\n\n#Set seed for reproducibility\nnp.random.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's check our data out\nlandslides.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"landslides.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(landslides['date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"landslides['date'].dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The date is an object as can be seen from the datatype. Lets parse it to a datetime object\nlandslides['new_date'] = pd.to_datetime(landslides['date'], format='%m/%d/%y')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check our new column out\nlandslides['new_date'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In the case where you get a situation where there are multiple formats of date entered into the column, the code can go like this**\n\n**landslides['new_date'] = pd.datetime(landslides['date'], infer_datetime_format=True)**","metadata":{}},{"cell_type":"code","source":"#From a datetime object, we can easily extract elements like day, month or the year\n#Let us get the day of the month from our new column\n#The same thing from the original date column would have given an error\n\nday_of_the_month = landslides['new_date'].dt.day\nday_of_the_month.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets remove the na's\nday_of_the_month.isnull().sum()\nday_of_the_month = day_of_the_month.dropna()\n\nsns.distplot(day_of_the_month, kde=False, bins=31)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets try our hands at another Date Parsing exercise**","metadata":{}},{"cell_type":"code","source":"#Lets read in our data \n\nearthquakes = pd.read_csv('../input/earthquake-database/database.csv')\nnp.random.seed(0)\nearthquakes.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the Date column to see if its the right datatype\nprint(earthquakes['Date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Change the Date column to a datetime object\n#earthquakes['new_date'] = pd.to_datetime(earthquakes['Date'], format='%m/%d/%y')\n#earthquakes['new_date'].head()\n\nearthquakes['new_date'] = pd.to_datetime(earthquakes['Date'], infer_datetime_format=True,\n                                        format='%m/%d/%y')\nearthquakes['new_date'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We can see from the results that the new columnm is still an object meaning there is some data not conforming to the datetime change\n#One way we can investigate is to check the length of data under Date to verify any discrepancies\n\ndate_len = earthquakes.Date.str.len()\ndate_len.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From the previous result, 23,409 data have 10 characters and 3 have 24. Lets located the three\nindices = np.where([date_len == 24])[1]\nprint('Indices with discrepancies:',indices)\nearthquakes.loc[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now that we have identified where the discrepancies lie, we can manually change the data \nearthquakes.loc[3378,'Date'] = '02/23/1975'\nearthquakes.loc[7512, 'Date'] = '04/28/1985'\nearthquakes.loc[20650, 'Date'] = '03/11/2011'\n\n#Let us recreate the new column and check for datatype\nearthquakes['new_date'] = pd.to_datetime(earthquakes['Date'], format='%m/%d/%Y')\nprint(earthquakes['new_date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Select day of the month and plot\nday_of_month = earthquakes['new_date'].dt.day\n\nsns.displot(day_of_month, bins=30, kde=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}