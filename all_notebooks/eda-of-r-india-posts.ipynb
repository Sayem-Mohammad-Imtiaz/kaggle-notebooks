{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstopWords = stopwords.words('english')\nRE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/reddit-india-flair-detector/rindia_ver2.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Title Length'] = data['Title'].astype(str).apply(len)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(subset=['Title','Date'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Flair Distribution"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(\n                x = data['Flair'].value_counts()[:25].index.tolist(),\n                y = data['Flair'].value_counts()[:25].values.tolist())])\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Title Length Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, x=\"Title Length\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Date'] = data['Date'].apply(lambda x: x[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date vs No of submissions"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, x=\"Date\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the Title"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = re.sub('#', '', text)  # remove hashtags\n    text = re.sub('@\\S+', '', text)  # remove mentions\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  # remove punctuations\n    text = re.sub('\\s+', ' ', text)  # remove extra whitespace\n    text = RE_EMOJI.sub('',text)\n    words = word_tokenize(text)\n    clean_text = []\n    for word in words:\n        if word not in stopWords:\n            clean_text.append(word)\n    cln_txt = ' '.join(clean_text)\n    return cln_txt.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Clean Title'] = data['Title'].apply(clean_text)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# N-gram distribution\n**Reference - https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a**\n## Unigram count"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(data['Clean Title'], 20)\ndf1 = pd.DataFrame(common_words, columns = ['TitleText' , 'count'])\nunigram = df1.groupby('TitleText').sum()['count'].sort_values(ascending=False)\nfig = go.Figure(data=[go.Bar(\n                y = unigram.tolist(),\n                x = unigram.index.tolist())])\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bigram count"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2),stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_bigram(data['Clean Title'], 20)\ndf2 = pd.DataFrame(common_words, columns = ['TitleText' , 'count'])\nbigram = df2.groupby('TitleText').sum()['count'].sort_values(ascending=False)\nfig2 = go.Figure(data=[go.Bar(\n                y = bigram.tolist(),\n                x = bigram.index.tolist())])\n\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trigram count"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3),stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_trigram(data['Clean Title'], 20)\ndf3 = pd.DataFrame(common_words, columns = ['TitleText' , 'count'])\ntrigram = df3.groupby('TitleText').sum()['count'].sort_values(ascending=False)\nfig3 = go.Figure(data=[go.Bar(\n                y = trigram.tolist(),\n                x = trigram.index.tolist())])\n\nfig3.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}