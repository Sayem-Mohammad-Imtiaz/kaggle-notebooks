{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm_notebook as tqdm\nimport os\n\nfrom plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"## $1.1$ Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data taken from Kaggle: https://www.kaggle.com/manasvee1/donorschooseorg-application-screening\n\nproject_data = pd.read_csv('../input/donorschooseorg-application-screening/train.csv')\nresource_data = pd.read_csv('../input/donorschooseorg-application-screening/resources.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how to replace elements in list python: https://stackoverflow.com/a/2582163/4084039\ncols = ['Date' if x=='project_submitted_datetime' else x for x in list(project_data.columns)]\n\n#sort dataframe based on time pandas python: https://stackoverflow.com/a/49702492/4084039\nproject_data['Date'] = pd.to_datetime(project_data['project_submitted_datetime'])\nproject_data.drop('project_submitted_datetime', axis=1, inplace=True)\nproject_data.sort_values(by=['Date'], inplace=True)\n\n# how to reorder columns pandas python: https://stackoverflow.com/a/13148611/4084039\nproject_data = project_data[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.2a$ preprocessing of `project_subject_categories`"},{"metadata":{"trusted":true},"cell_type":"code","source":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())\n    \nproject_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.2b$ preprocessing of `project_subject_subcategories`\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())\n\nproject_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.2c$ preprocessing of `project_grade_category`"},{"metadata":{"trusted":true},"cell_type":"code","source":"proj_grade_cat = []\n\nfor i in range(len(project_data)):\n    pgc = project_data[\"project_grade_category\"][i].replace(\" \", \"_\")\n    proj_grade_cat.append(pgc)\n    \nproject_data.drop(['project_grade_category'], axis=1, inplace=True)\nproject_data[\"project_grade_category\"] = proj_grade_cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.3$ Text preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getProcessedData(txt_type, working_data):\n    preprocessed_data = []\n    # tqdm is for printing the status bar\n    \n    for sentance in tqdm(working_data[txt_type].values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_data.append(sent.lower().strip())\n        \n    return preprocessed_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2><font color='red'> $1.4$ Preprocessing of `project_title` </font></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Covered above","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.5$ Preparing data for models\nwe are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data (optinal)\n       \n       - quantity : numerical (optinal)\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical"},{"metadata":{},"cell_type":"markdown","source":"### $1.5.1$ Vectorizing Categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getCountDict(cat_type):\n    count_dict = {}\n    info_list = project_data[cat_type]\n    project_data.loc[project_data[cat_type].isnull(), cat_type] = 'nan'\n    \n    for phrase in info_list:\n        for data in phrase.split():\n            if data not in count_dict: count_dict[data] = 0\n            else: count_dict[data] += 1\n            \n    return dict(sorted(count_dict.items(), key=lambda x: x[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef getFitCAT_Vectorizer(working_data, cat_type, hstack_features):\n    '''\n    Fit on only train data.\n    '''\n    working_data.loc[working_data[cat_type].isnull(), cat_type] = 'nan'\n    #print (working_data.keys())\n    \n    if 1:\n        sorted_cat_dict = getCountDict(cat_type)\n        print ('Keys...', sorted_cat_dict.keys())\n        hstack_features += sorted_cat_dict.keys()\n        vectorizer = CountVectorizer(vocabulary=sorted_cat_dict.keys(), lowercase=False, binary=True)\n    \n    vectorizer.fit(working_data[cat_type].values)\n    return vectorizer\n    \ndef getVectorizeCategData(working_data, cat_type, data_type):\n    working_data.loc[working_data[cat_type].isnull(), cat_type] = 'nan'\n    \n    categories_one_hot = vectorizer.transform(working_data[cat_type].values)\n    #print(vectorizer.get_feature_names())\n    print(\"Shape of matrix after one hot encodig \",categories_one_hot.shape)\n    \n    return categories_one_hot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $1.5.2$ Vectorizing Text data"},{"metadata":{},"cell_type":"markdown","source":"#### $1.5.2.1$ Bag of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFitBOW_Vectorizer(preprocessed_data):\n    vectorizer = CountVectorizer(min_df=10)\n    vectorizer.fit(preprocessed_data)\n    \n    return vectorizer\n\ndef getBOWVectorizeTxtData(preprocessed_data, vectorizer):\n    text_bow = vectorizer.transform(preprocessed_data)\n    print(\"Shape of matrix after one hot encodig \",text_bow.shape)\n    \n    return text_bow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.5.2.2 TFIDF vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef getFitTFIDF_Vectorizer(preprocessed_data):\n    vectorizer = TfidfVectorizer(min_df=10)\n    vectorizer.fit(preprocessed_data)\n    return vectorizer\n\ndef getTFIDFVectorizeTxtData(preprocessed_data, vectorizer):\n    text_tfidf = vectorizer.transform(preprocessed_data)\n    print(\"Shape of matrix after one hot encodig \",text_tfidf.shape)\n    return text_tfidf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.5.2.3 Using Pretrained Models: Avg W2V"},{"metadata":{"trusted":true},"cell_type":"code","source":"# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n# make sure you have the glove_vectors file\nwith open('../input/glove-vectors/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAVG_W2V(preprocessed_data):\n    avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n    \n    for sentence in tqdm(preprocessed_data): # for each review/sentence\n        vector = np.zeros(300) # as word vectors are of zero length\n        cnt_words =0; # num of words with a valid vector in the sentence/review\n        for word in sentence.split(): # for each word in a review/sentence\n            if word in glove_words:\n                vector += model[word]\n                cnt_words += 1\n        if cnt_words != 0:\n            vector /= cnt_words\n        avg_w2v_vectors.append(vector)\n\n    print(len(avg_w2v_vectors))\n    print(len(avg_w2v_vectors[0]))\n    \n    return avg_w2v_vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### $1.5.2.3$ Using Pretrained Models: TFIDF weighted W2V\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFitTFIDF_W2V(preprocessed_data):\n    tfidf_model = TfidfVectorizer()\n    tfidf_model.fit(preprocessed_data)\n    return tfidf_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getTFIDF_W2V(preprocessed_data, tfidf_model):\n    \n    # we are converting a dictionary with word as a key, and the idf as a value\n    dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n    tfidf_words = set(tfidf_model.get_feature_names())\n    \n    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n    for sentence in tqdm(preprocessed_data): # for each review/sentence\n        vector = np.zeros(300) # as word vectors are of zero length\n        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n        for word in sentence.split(): # for each word in a review/sentence\n            if (word in glove_words) and (word in tfidf_words):\n                vec = model[word] # getting the vector for each word\n                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n                tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n                tf_idf_weight += tf_idf\n        if tf_idf_weight != 0:\n            vector /= tf_idf_weight\n        tfidf_w2v_vectors.append(vector)\n\n    print(len(tfidf_w2v_vectors))\n    print(len(tfidf_w2v_vectors[0]))\n    \n    return tfidf_w2v_vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $1.5.3$ Vectorizing Numerical features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nproject_data = pd.merge(project_data, price_data, on='id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\ndef getFitNUM_Vectorizer(working_data, num_type):\n    '''\n    Fit on only train data.\n    '''\n    \n    num_scalar = Normalizer()\n    num_scalar.fit(working_data[num_type].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n    return num_scalar\n\ndef getNUM_Vectors(working_data, num_type, num_scalar):\n    # Now standardize the data with above maen and variance.\n    num_standardized = num_scalar.transform(working_data[num_type].values.reshape(-1, 1))\n    \n    return num_standardized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $1.5.4$ Merging all the above features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n\ndef getMergedFeatures(working_data, merge_on):\n    valid_cols = []\n    for key, value in working_data.items():\n        if key in merge_on:\n            valid_cols.append(value)\n   \n    return hstack(tuple(valid_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Assignment $8$: DT\n"},{"metadata":{},"cell_type":"markdown","source":"<h1>$2.$ Decision Tree </h1>"},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.1$ Splitting data into Train and cross validation(or test): Stratified Sampling</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# please write all the code with proper documentation, and proper titles for each subsection\n# go through documentations and blogs before you start coding\n# first figure out what to do, and then think about how to do.\n# reading and understanding error messages will be very much helpfull in debugging your code\n# when you plot any graph make sure you use \n    # a. Title, that describes your plot, this will be very helpful to the reader\n    # b. Legends if needed\n    # c. X-axis label\n    # d. Y-axis label\n    \nfrom sklearn.model_selection import train_test_split\n\n#Classes of X & project_data have almost same proportion.\nX = project_data[:50000]\ny = X['project_is_approved']\n\n#Breaking into only train and test as I am gonna use cross-validation.\nX_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.2$ Make Data Model Ready: encoding numerical, categorical features</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ordered dict will be used to ensure one to one correspondence between datapoints features and hstack_features.\n\nfrom collections import OrderedDict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict = {'X_tr':OrderedDict({}), 'X_test': OrderedDict({})}\ncols_dict = OrderedDict({'cat_cols': ['school_state','clean_categories', 'clean_subcategories', 'project_grade_category', 'teacher_prefix'],\n                 'num_cols': ['price', 'teacher_number_of_previously_posted_projects']\n            })\nhstack_features = []\n\nfor col_type, cols_name in cols_dict.items():\n    if col_type == 'cat_cols':\n        for cat_type in cols_name:\n            print (cat_type)\n            vectorizer = getFitCAT_Vectorizer(X_tr, cat_type, hstack_features)\n            for data_type, data_part in [('X_tr', X_tr), ('X_test', X_test)]:\n                hot_encode = getVectorizeCategData(data_part, cat_type, vectorizer)\n                data_dict[data_type][cat_type] = hot_encode\n    else:\n        for num_type in cols_name:\n            vectorizer = getFitNUM_Vectorizer(X_tr, num_type)\n            hstack_features.append(num_type)\n            for data_type, data_part in [('X_tr', X_tr), ('X_test', X_test)]:\n                num_vectors = getNUM_Vectors(data_part, num_type, vectorizer)\n                data_dict[data_type][num_type] = num_vectors\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.3$ Make Data Model Ready: encoding eassay, and project_title</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"essay_hot_info = {}\n\nfor col_type in ['essay','project_title']:\n    for data_type, data_part in [('X_tr', X_tr), ('X_test', X_test)]:\n        preprocessed_data = getProcessedData(col_type, data_part)\n        \n        if data_type == 'X_tr':\n            vectorizer_bog = getFitBOW_Vectorizer(preprocessed_data)\n        text_bow = getBOWVectorizeTxtData(preprocessed_data, vectorizer_bog)\n        data_dict[data_type]['%s_text_bow'%col_type] = text_bow\n        \n        if data_type == 'X_tr':\n            vectorizer_tfidf = getFitTFIDF_Vectorizer(preprocessed_data)\n        text_tfidf = getTFIDFVectorizeTxtData(preprocessed_data, vectorizer_tfidf)\n        data_dict[data_type]['%s_text_tfidf'%col_type] = text_tfidf\n        \n        text_w2v = getAVG_W2V(preprocessed_data)\n        data_dict[data_type]['%s_text_w2v'%col_type] = text_w2v\n        \n        if data_type == 'X_tr':\n            vectorizer_tfidfw2v = getFitTFIDF_W2V(preprocessed_data)\n        text_tfidfw2v = getTFIDF_W2V(preprocessed_data, vectorizer_tfidfw2v)\n        data_dict[data_type]['%s_text_tfidfw2v'%col_type] = text_tfidfw2v\n        \n        if col_type == \"essay\" and data_type == 'X_test':\n            essay_hot_info['bow'] = (vectorizer_bog, text_bow)\n            essay_hot_info['tfidf'] = (vectorizer_tfidf, text_tfidf)\n            \n    hstack_features += vectorizer_bog.get_feature_names()\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.4$ Applying Decision Tree on different kind of featurization as mentioned in the instructions</h2>\n\n<br>Apply Decision Tree on different kind of featurization as mentioned in the instructions\n<br> For Every model that you work on make sure you do the step $2$ and step $3$ of instrucations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree \nfrom sklearn.metrics import roc_auc_score, auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reference: https://matplotlib.org/3.1.0/gallery/pyplots/pyplot_scales.html#sphx-glr-gallery-pyplots-pyplot-scales-py\n\nfrom sklearn.model_selection import GridSearchCV\nimport itertools \n\ndef getAUCs(data_pnts_tr, y_tr):\n    auc_tr = []\n    auc_cv = []\n    depth =  [1, 5, 10, 50, 100, 500]\n    min_samples_split = [5, 10, 100, 500]\n    parameters = {  'max_depth': depth, \n                    'min_samples_split': min_samples_split }\n    \n    dtc = tree.DecisionTreeClassifier(class_weight='balanced')\n    clf = GridSearchCV(dtc, parameters, cv=3, scoring='roc_auc')\n    clf.fit(data_pnts_tr, y_tr)\n\n    auc_tr = clf.cv_results_['mean_train_score']\n    auc_tr_std = clf.cv_results_['std_train_score']\n    auc_cv = clf.cv_results_['mean_test_score'] \n    auc_cv_std= clf.cv_results_['std_test_score']\n    #list(itertools.product(depth, min_samples_split))\n    return depth, min_samples_split, auc_tr, auc_cv\n\ndef plotPerformance3d(hyper1, hyper2, auc_tr1, auc_cv1, plt_title):\n    rows = hyper1\n    cols = hyper2\n    cm_tr = np.array(auc_tr1).reshape((len(rows), len(cols)))\n    df_tr = pd.DataFrame(cm_tr, columns=cols, index=rows)\n    cm_cv = np.array(auc_cv1).reshape((len(rows), len(cols)))\n    df_cv = pd.DataFrame(cm_cv, columns=cols, index=rows)\n    \n    plt.figure(figsize=(15, 15))\n\n    ax_tr = plt.subplot(221)\n    sns.heatmap(df_tr, annot=True, ax=ax_tr, fmt='g')\n    ax_tr.set_xlabel('min_samples_split')\n    ax_tr.set_ylabel('max_depth')\n    plt.title(\"Training data's AUCs on various depth & minsplit using %s on text features\"%plt_title)\n    \n    ax_cv = plt.subplot(222)\n    sns.heatmap(df_cv, annot=True, ax=ax_cv, fmt='g')\n    ax_cv.set_xlabel('min_samples_split')\n    ax_cv.set_ylabel('max_depth')\n    plt.title(\"CV data's AUCs on various depth & minsplit using %s on text features\"%plt_title)\n    \n    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef trainOnOptimalHP(optimal_c, X_tr, y_tr):\n    dtc = tree.DecisionTreeClassifier(max_depth=optimal_c[0], min_samples_split=optimal_c[1], class_weight='balanced')\n    dtc.fit(X_tr, y_tr)\n    return dtc\n\ndef getROC_Data(data_pnts_test, y_test, data_pnts_tr, y_tr, dtc):\n    predicted_y_test = dtc.predict_proba(data_pnts_test)[:, 1]\n    predicted_y_tr = dtc.predict_proba(data_pnts_tr)[:, 1]\n    \n    fpr_test, tpr_test, thres_test = roc_curve(y_test, predicted_y_test)\n    fpr_tr, tpr_tr, thres_tr = roc_curve(y_tr, predicted_y_tr)\n    \n    return [fpr_test, tpr_test, thres_test], [fpr_tr, tpr_tr, thres_tr]\n    \ndef makeROC(test_data, train_data, plt_title, roc_data_test1, roc_data_train1):\n    fpr_tr, tpr_tr, _ = train_data\n    fpr_test, tpr_test, _ = test_data\n    \n    plt.plot(fpr_tr, tpr_tr, label='AUC_Train: %s'%auc(roc_data_test1[0],roc_data_test1[1]))\n    plt.plot(fpr_test, tpr_test, label='AUC_Test: : %s'%auc(roc_data_train1[0],roc_data_train1[1]))\n    plt.title(\"ROC Curve using %s on text features\"%plt_title)\n    \n    plt.xlabel('FPT')\n    plt.ylabel('TPR')\n    plt.legend()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copied & Edited from here - \n#https://www.kaggle.com/willkoehrsen/visualize-a-decision-tree-w-python-scikit-learn\nfrom sklearn.tree import export_graphviz\nfrom subprocess import call\n\ndef plotGraph(dtc, hstack_features, plt_title):\n    # Export as dot file\n    export_graphviz(dtc, out_file='tree_%s.dot'%plt_title, \n                    rounded = True, proportion = False, \n                    precision = 2, filled = True,\n                    feature_names = hstack_features,\n                    class_names = ['0','1'])\n    # Convert to png\n\n    call(['dot', '-Tpng', 'tree_%s.dot'%plt_title, '-o', 'tree_%s.png'%plt_title, '-Gdpi=600'])\n\n    # Display in python\n    plt.figure(figsize=(20, 20))\n    plt.imshow(plt.imread('tree_%s.png'%plt_title))\n    plt.title('Decision tree using %s on text features'%plt_title)\n    plt.axis('off');\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reference: https://stackoverflow.com/questions/19233771/sklearn-plot-confusion-matrix-with-labels/48018785\n#fmt='g' reason: https://stackoverflow.com/questions/29647749/seaborn-showing-scientific-notation-in-heatmap-for-3-digit-numbers\nfrom sklearn.metrics import confusion_matrix\n\ndef getConfusionMatrix(dtc, data_pnts_test, y_true, plt_title):\n    y_pred = dtc.predict(data_pnts_test)\n    cm = confusion_matrix(y_true, y_pred) # Predicted values are column wise!\n    \n    ax = plt.subplot()\n    sns.heatmap(cm, annot=True, ax=ax, fmt='g')\n    \n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('Actual Labels')\n    ax.set_title('Confusion Matrix using %s on text features'%plt_title)\n    ax.xaxis.set_ticklabels(['0','1'])\n    ax.yaxis.set_ticklabels(['0','1'])\n\n    return y_pred\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from here -\n# https://www.geeksforgeeks.org/generating-word-cloud-python/\n\nfrom wordcloud import WordCloud\n\ndef plotWordCloud(y_pred, y_test, essay_hot_info, plt_title, ktype=''):\n    one_hot_featr = essay_hot_info[ktype][0].get_feature_names()\n    one_hot_enc = essay_hot_info[ktype][1].toarray()\n    one_hot_enc_cols = one_hot_enc.shape[1]\n    \n    word_corpus = ''\n    i = 0\n    for each_x in tqdm(y_pred):\n        if each_x and not y_test[i]:\n            for j in range(one_hot_enc_cols):\n                if one_hot_enc[i][j] >= 0.5:\n                    word_corpus = \"%s %s\"%(word_corpus, one_hot_featr[j].strip())\n        i += 1\n    \n    wordcloud = WordCloud(width = 800, height = 800, \n                    background_color ='white', \n                    stopwords = stopwords,\n                    collocations = False,\n                    min_font_size = 10).generate(word_corpus) \n    \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.title('Word cloud of essay words which are present in False-Positive test data using %s on essay features '%plt_title)\n    #plt.tight_layout(pad = 0) \n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndef plotBoxPlot(y_pred, y_test, X_test, plt_title):\n    fp_price = []\n    \n    for i, y in enumerate(y_pred):\n        if y_pred and not y_test[i]:\n            fp_price.append(X_test['price'].iloc[i])\n    \n    plt.boxplot(fp_price)\n    plt.title('Box plot on price of false-positive test data points using %s on text features'%plt_title)\n    #plt.xticks('')\n    plt.ylabel('Price.')\n    plt.grid()\n    plt.show()\n    \ndef plotPDF(y_pred, y_test, X_test, plt_title):\n    x_axis1 = 'teacher_number_of_previously_posted_projects'\n    legend1 = 'False-Positive test data points'\n    fp_tnppp = []\n    \n    for i, y in enumerate(y_pred):\n        if y_pred and not y_test[i]:\n            fp_tnppp.append(X_test[x_axis1].iloc[i])\n    \n    plt.figure(figsize=(10,3))\n    sns.distplot(fp_tnppp, label=legend1, hist=False)\n    plt.title(\"PDF of No. of previously posted projects of False-Positive test data using %s on text features\"%plt_title)\n    plt.xlabel(x_axis1)\n    plt.legend()\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $2.4.1$ Applying Decision Tree on BOW,<font color='red'> SET $1$</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"set1_cols = ['school_state','clean_categories', 'clean_subcategories', 'project_grade_category', 'teacher_prefix',\n             'price', 'teacher_number_of_previously_posted_projects', \n             'essay_text_bow', 'project_title_text_bow']\nplt_title1 = 'BOW'\n\ndata_pnts_tr1 = getMergedFeatures(data_dict['X_tr'], set1_cols)\ndata_pnts_test1 = getMergedFeatures(data_dict['X_test'], set1_cols)\n\nhyper1a, hyper2b, auc_tr1, auc_cv1 = getAUCs(data_pnts_tr1, y_tr)\nplotPerformance3d(hyper1a, hyper2b, auc_tr1, auc_cv1, plt_title1)\noptimal_C1 = (10, 500)\ndtc1 = trainOnOptimalHP(optimal_C1, data_pnts_tr1, y_tr)\nroc_data_test1, roc_data_train1 = getROC_Data(data_pnts_test1, y_test, data_pnts_tr1, y_tr, dtc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"makeROC(roc_data_test1, roc_data_train1, plt_title1, roc_data_test1, roc_data_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = getConfusionMatrix(dtc1, data_pnts_tr1, y_tr, plt_title1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_pred = getConfusionMatrix(dtc1, data_pnts_test1, y_test, plt_title1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotWordCloud(y1_pred, np.array(y_test), essay_hot_info, plt_title1, ktype='bow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBoxPlot(list(y1_pred), np.array(y_test), X_test, plt_title1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPDF(list(y1_pred), np.array(y_test), X_test, plt_title1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### $2.4.1.1$ Graphviz visualization of Decision Tree on BOW,<font color='red'> SET $1$</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph(dtc1, hstack_features, plt_title1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $2.4.$2 Applying Decision Tree on TFIDF,<font color='red'> SET $2$</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"set2_cols = ['school_state','clean_categories', 'clean_subcategories', 'project_grade_category', 'teacher_prefix',\n             'price', 'teacher_number_of_previously_posted_projects', \n             'essay_text_tfidf', 'project_title_text_tfidf']\nplt_title2 = 'TFIDF'\n\ndata_pnts_tr2 = getMergedFeatures(data_dict['X_tr'], set2_cols)\ndata_pnts_test2 = getMergedFeatures(data_dict['X_test'], set2_cols)\nhyper2a, hyper2b, auc_tr2, auc_cv2 = getAUCs(data_pnts_tr2, y_tr)\nplotPerformance3d(hyper2a, hyper2b, auc_tr2, auc_cv2, plt_title2)\noptimal_C2 = (10,500)\ndtc2 = trainOnOptimalHP(optimal_C2, data_pnts_tr2, y_tr)\nroc_data_test2, roc_data_train2 = getROC_Data(data_pnts_test2, y_test, data_pnts_tr2, y_tr, dtc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"makeROC(roc_data_test2, roc_data_train2, plt_title2, roc_data_test2, roc_data_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = getConfusionMatrix(dtc2, data_pnts_tr2, y_tr, plt_title2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = getConfusionMatrix(dtc2, data_pnts_test2, y_test, plt_title2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotWordCloud(y_pred2, np.array(y_test), essay_hot_info, plt_title2, ktype='tfidf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBoxPlot(list(y_pred2), np.array(y_test), X_test, plt_title2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPDF(list(y_pred2), np.array(y_test), X_test, plt_title2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### $2.4.2.1$ Graphviz visualization of Decision Tree on TFIDF,<font color='red'> SET $2$</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph(dtc2, hstack_features, plt_title2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $2.4.3$ Applying Decision Tree on AVG W2V,<font color='red'> SET $3$</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"set3_cols = ['school_state','clean_categories', 'clean_subcategories', 'project_grade_category', 'teacher_prefix',\n             'price', 'teacher_number_of_previously_posted_projects', \n             'essay_text_avgW2V', 'project_title_text_avgW2V']\nplt_title3 = \"AVG-W2V\"\n\ndata_pnts_tr3 = getMergedFeatures(data_dict['X_tr'], set1_cols)\ndata_pnts_test3 = getMergedFeatures(data_dict['X_test'], set1_cols)\n\nhyper3a, hyper3b, auc_tr3, auc_cv3 = getAUCs(data_pnts_tr3, y_tr)\nplotPerformance3d(hyper3a, hyper3b, auc_tr3, auc_cv3, plt_title3)\noptimal_C3 = (10, 500)\ndtc3 = trainOnOptimalHP(optimal_C3, data_pnts_tr3, y_tr)\nroc_data_test3, roc_data_train3 = getROC_Data(data_pnts_test3, y_test, data_pnts_tr3, y_tr, dtc3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"makeROC(roc_data_test3, roc_data_train3, plt_title3, roc_data_test3, roc_data_train3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = getConfusionMatrix(dtc3, data_pnts_tr3, y_tr, plt_title3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y3_pred = getConfusionMatrix(dtc3, data_pnts_test3, y_test, plt_title3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBoxPlot(list(y3_pred), np.array(y_test), X_test, plt_title3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPDF(list(y3_pred), np.array(y_test), X_test, plt_title3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $2.4.4$ Applying Decision Tree on TFIDF W2V,<font color='red'> SET $4$</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"set4_cols = ['school_state','clean_categories', 'clean_subcategories', 'project_grade_category', 'teacher_prefix',\n             'price', 'teacher_number_of_previously_posted_projects', \n             'essay_text_tfidfW2V', 'project_title_text_tfidfW2V']\nplt_title4 = \"TFIDF-W2V\"\n\ndata_pnts_tr4 = getMergedFeatures(data_dict['X_tr'], set4_cols)\ndata_pnts_test4 = getMergedFeatures(data_dict['X_test'], set4_cols)\n\nhyper4a, hyper4b, auc_tr4, auc_cv4 = getAUCs(data_pnts_tr4, y_tr)\nplotPerformance3d(hyper4a, hyper4b, auc_tr4, auc_cv4, plt_title4)\noptimal_C4 = (10,500)\ndtc4 = trainOnOptimalHP(optimal_C4, data_pnts_tr4, y_tr)\nroc_data_test4, roc_data_train4 = getROC_Data(data_pnts_test4, y_test, data_pnts_tr4, y_tr, dtc4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"makeROC(roc_data_test4, roc_data_train4, plt_title4, roc_data_test4, roc_data_train4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = getConfusionMatrix(dtc4, data_pnts_tr4, y_tr, plt_title4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y4_pred = getConfusionMatrix(dtc4, data_pnts_test4, y_test, plt_title4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBoxPlot(list(y4_pred), np.array(y_test), X_test, plt_title4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPDF(list(y4_pred), np.array(y_test), X_test, plt_title4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.5$ [Task-2] Getting top $5k$ features using `feature_importances_`</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport math\ndef getAUC_LR(data_pnts_tr, y_tr):\n    auc_tr = []\n    auc_cv = []\n    Cs = [10**i for i in range(-5,5)]\n    parameters = {'C':Cs}\n    \n    LoR = LogisticRegression(class_weight=\"balanced\")\n    clf = GridSearchCV(LoR, parameters, cv=3, scoring='roc_auc')\n    clf.fit(data_pnts_tr, y_tr)\n\n    auc_tr = clf.cv_results_['mean_train_score']\n    auc_tr_std = clf.cv_results_['std_train_score']\n    auc_cv = clf.cv_results_['mean_test_score'] \n    auc_cv_std= clf.cv_results_['std_test_score']\n\n    return list(map(lambda x: math.log(x),Cs)), auc_tr, auc_cv\n\ndef trainOnOptimalC(optimal_c, X_tr, y_tr):\n    LoR = LogisticRegression(C=optimal_c, class_weight='balanced')\n    LoR.fit(X_tr, y_tr)\n    return LoR\n\ndef plotPerformance(Cs, auc_tr, auc_cv, plt_title):\n    plt.figure(figsize=(20,10))\n    plt.plot(Cs, auc_tr, label='AUC_Train')\n    plt.plot(Cs, auc_cv, label='AUC_Validation')\n    \n    plt.scatter(Cs, auc_tr, label='Coordinates')\n    plt.scatter(Cs, auc_cv, label='Coordinates')\n    \n    if 1:\n        plt.xlabel('Hyperparameter - Log(C)')\n        plt.ylabel('AUC')\n        plt.title(\"AUC on various Log(C)s using %s on text features\"%plt_title)\n    \n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = tree.DecisionTreeClassifier(class_weight='balanced')\ndtc.fit(data_pnts_tr2, y_tr)\n\nfeature_impce = dtc.feature_importances_\nrelevant_features = []\ncols = []\n\nfor i, val in enumerate(feature_impce):     #dtc2 is optimal decision tree model for set 2.\n    if val > 0:\n        relevant_features.append(feature_impce[i])\n        cols.append(i)\n        \nX_tr_new = data_pnts_tr2.todense()[:, cols]\nX_test_new = data_pnts_test2.todense()[:, cols]\nplt_title_new = '5000 features with TFIDF'\n\nC_new, auc_tr_new, auc_cv_new = getAUC_LR(X_tr_new, y_tr)\nplotPerformance(C_new, auc_tr_new, auc_cv_new, plt_title_new)\noptimal_C_new = 0.082\nLoR = trainOnOptimalC(optimal_C_new, X_tr_new, y_tr)\nroc_data_test_new, roc_data_train_new = getROC_Data(X_test_new, y_test, X_tr_new, y_tr, LoR)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"makeROC(roc_data_test_new, roc_data_train_new, plt_title_new, roc_data_test_new, roc_data_train_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = getConfusionMatrix(LoR, X_tr_new, y_tr, plt_title_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_new_pred = getConfusionMatrix(LoR, X_test_new, y_test, plt_title_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBoxPlot(list(y_new_pred), np.array(y_test), X_test, plt_title_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPDF(list(y_new_pred), np.array(y_test), X_test, plt_title_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>$3.$ Conclusions</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reference: Assignment-2\n\nfrom prettytable import PrettyTable\n\ntable = PrettyTable()\ntable.field_names = [\"Vectorizer\", \"Hyper Parameter\", \"AUC\"]\n\ntable.add_row([\"BOW\", optimal_C1, auc(roc_data_test1[0],roc_data_test1[1])])\ntable.add_row([\"TFIDF\", optimal_C2, auc(roc_data_test2[0],roc_data_test2[1])])\ntable.add_row([\"AVG_W2V\", optimal_C3, auc(roc_data_test3[0],roc_data_test3[1])])\ntable.add_row([\"TFIDF_W2V\", optimal_C4, auc(roc_data_test4[0],roc_data_test4[1])])\ntable.add_row([\"TFIDF 5k\", optimal_C_new, auc(roc_data_test_new[0],roc_data_test_new[1])])\n\nprint (table)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}