{"cells":[{"metadata":{},"cell_type":"markdown","source":"This project is inspired by the idea of Lucas: Clustering and Visualisation using Folium Maps (https://www.kaggle.com/lucaspcarlini/clustering-and-visualisation-using-folium-maps)"},{"metadata":{},"cell_type":"markdown","source":"library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport folium\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.decomposition import PCA\nfrom hmmlearn import hmm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** load dataset**"},{"metadata":{},"cell_type":"markdown","source":"this is a countrywide weather events dataset with more than 5 million events, which covers 49 states of the United States. Examples of weather events are rain, snow, storm, and freezing condition. The data is collected from January 2016 to December 2019, using historical weather reports that exist for airport-based weather stations across the country.\n\nWeather event is a spatiotemporal entity, where such an entity is associated with location and time. Following is the description of available weather event types in this dataset: Cold: The case of having extremely low temperature, with temperature below -23.7 degrees of Celsius.\n\nFog: The case where there is low visibility condition as a result of fog or haze.\n\nHail: The case of having solid precipitation including ice pellets and hail.\n\nRain: The case of having rain, ranging from light to heavy.\n\nSnow: The case of having snow, ranging from light to heavy.\n\nStorm: The extremely windy condition, where the wind speed is at least 60 km/h.\n\nOther Precipitation: Any other type of precipitation which cannot be assigned to previously described event types."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/us-weather-events/US_WeatherEvents_2016-2019.csv')\ndf.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data cleaning"},{"metadata":{},"cell_type":"markdown","source":"In this project, the duration of each weather event was the key feature used to cluster regions. It was calculated by using event end time minus start time, any single event that lasted more than 30 days was initially eliminated, then any events outside three standard deviation away from mean were also removed."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"datetimeFormat = '%Y-%m-%d %H:%M:%S'\ndf['End']=pd.to_datetime(df['EndTime(UTC)'], format=datetimeFormat)\ndf['Start']=pd.to_datetime(df['StartTime(UTC)'], format=datetimeFormat)\ndf['Duration']=df['End']-df['Start']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf['Duration'] = df['Duration']/(60*60) #in hours\ndf = df[(df['Duration']< 30*24) & (df['Duration'] != 0)] #remove obvious wrong data\ndf.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check events distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Overall Duration Summary\")\nprint(\"--Count\", df['Duration'].size)\nprint(\"--%miss.\", sum(df['Duration'].isnull()))\nprint(\"--card.\",df['Duration'].unique().size)\nprint(\"--min\",df['Duration'].min())\nprint(\"--lowerBoundary.\",df['Duration'].median()-1.5*((df['Duration'].quantile(0.75))-df['Duration'].quantile(0.25)))\nprint(\"--1stQrt\",df['Duration'].quantile(0.25))\nprint(\"--mean\",df['Duration'].mean())\nprint(\"--median\",df['Duration'].median())\nprint(\"--3rdQrt\",df['Duration'].quantile(0.75))\nprint(\"--upperBoundary.\",df['Duration'].median()+1.5*((df['Duration'].quantile(0.75))-df['Duration'].quantile(0.25)))\nprint(\"--95%Boundary.\",df['Duration'].mean()+1.96*df['Duration'].std())\nprint(\"--max\",df['Duration'].max())\nprint(\"--Std.Dev\",df['Duration'].std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(df['Duration']< 10)]\ndf['Duration'].size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**data normalization**\nFor better comparison of each weather events, all the duration time of them were normalized to the range 0 to 100, zero means that event never happened in the year and 100 means that event happened all the time during the year."},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.groupby(['AirportCode','City','State', \n                  'LocationLat', 'LocationLng','Type']).agg({'Duration':['sum']}).reset_index()\ndf2.columns=pd.MultiIndex.from_tuples(((\"AirportCode\", \" \"),(\"City\", \" \"),\n                                       (\"State\", \" \"), (\"LocationLat\", \" \"),\n                                       (\"LocationLng\", \" \"), (\"Type\", \" \"), (\"Duration\", \" \")))\ndf2.columns = df2.columns.get_level_values(0)\ndf2['Duration'] = df2['Duration']/(24*4*3.65) #yearly percentage  \ndf2 = df2.sort_values(by='Duration')\ndf2.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_flat = df2.pivot(index='AirportCode', columns='Type', values=['Duration']).reset_index().fillna(0)\ndf_flat.columns=pd.MultiIndex.from_tuples(((' ', 'AirportCode'),(' ', 'Cold'),(' ', 'Fog'),\n            (' ',  'Hail'),(' ', 'Precipitation'),(' ', 'Rain'),(' ', 'Snow'),(' ', 'Storm')))\ndf_flat.columns = df_flat.columns.get_level_values(1)\n#df_flat().tail(3)\nuniqueKey = df2[['AirportCode', 'City', \n                 'State', 'LocationLat', 'LocationLng']].sort_values(by='AirportCode').drop_duplicates()\nweather = pd.merge(df_flat, uniqueKey, how='inner', on='AirportCode')\nweather.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory data analysis**\n\n# **National wide weather events duration**\n\nThe national wide weather event duration information was shown in figure 1. Rain lasted around 4.5% period of a year, which is the most durable weather event. Fog and snow lasted around 1.5% time of a year. The duration of cold, precipitation, storm and hail were all below 0.5%."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_sum = px.histogram(df2, x='Type', y= 'Duration',  histfunc = 'avg',\n                      title = 'fig 1. National wide weather events duration')\nfig_sum.update_xaxes(categoryorder='mean descending')\nfig_sum.update_yaxes(title_text='mean of duration% per year')\nfig_sum.update_layout(height=750, width=1000)\nfig_sum.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **State wide weather events distribution**\n\nThe statewide weather event duration information was shown in figure 2.\n\nThe top three states with long rainy days are OR, DE, MA; The top three states with long foggy days are NH, WA, CA; The top three states with long snow days are MI, VT, MT; The top three states with long cold days are ND, MN, NJ; The top three states with long precipitation days are NY, FL, LA; The top three states with long storm days are NH, CO, WY; The top three states with long hail days are CT, NY, DE.\n\nThe top three states with short rainy days are NV, UT, AZ; The top three states with short foggy days are AZ, NV, NM; The top three states with short snow days are FL, LA, TX; The top three states with short cold days are MO, WA, CT; The top three states with short precipitation days are UT, NV, CO; The top three states with short storm days are AL, GA, KY; The top three states with short hail days are FL, AZ, GA.\n\nAll of these information matched our common sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_state=make_subplots(rows=7, cols=1, shared_yaxes=False, vertical_spacing=0.05)\n\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Rain'], name='Rain', histfunc ='avg'),1,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Fog'], name='Fog', histfunc ='avg'),2,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Snow'], name='Snow', histfunc ='avg'),3,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Cold'], name='Cold', histfunc ='avg'),4,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Precipitation'], name='Precipitation', histfunc ='avg'),5,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Storm'], name='Storm', histfunc ='avg'),6,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Hail'], name='Hail', histfunc ='avg'),7,1)\n\nfig_state['layout']['xaxis7'].update(title=\"State\")\nfig_state['layout']['yaxis4'].update(title=\"duration% per year\")\nfig_state.update_xaxes(categoryorder='mean descending')\nfig_state.update_layout( title_text=\"fig 2. State wide weather events distribution\")\nfig_state.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **City wide weather events distribution**\n\n# **cities involved**\n\nAll the cities involved in this dataset are marked as blue dot in figure 3. Large amount of cities distribued in west coast and east part of the United States, few of them located in the middle."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_city = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      scope=\"usa\",\n                      title ='fig 3. Cities involved in this dataset')\n#fig_city.update_layout(height=750, width=1000)\nfig_city.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Rain**\n\nThe city-wide rain distribution is shown in figure 4. United States cities with long rainy durations are distributed at the west coast and east areas (dark blue markers), cities in the middle part of the region has lower chance of raining (light blue area)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_rain = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Rain\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 4. City wide rainy days percentage each year from 2016 to 2019')\n#fig_rain.update_layout(height=750, width=1000)\nfig_rain.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Fog**\n\nThe city-wide fog distribution is shown in figure 5. Like the distribution of rain, cities with long fog days are distributed at the west coast and east areas (dark blue markers), cities in the middle part of the United States has lower chance of fog (light blue area)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_fog = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Fog\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      #range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 5. City wide foggy days percentage each year from 2016 to 2019')\n#fig_fog.update_layout(height=750, width=1000)\nfig_fog.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Snow**\n\nThe city-wide snow distribution is shown in figure 6. Cities in the north and middle part of the United States have higher chance of snow (dark blue markers), cities in the south east area have lower chance of snow (light blue area)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_snow = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Snow\",\n                      #size=\"Snow\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      #range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 6. City wide snow days percentage each year from 2016 to 2019')\n#fig_snow.update_layout(height=750, width=1000)\nfig_snow.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cold**\n\nCold is defined as the case of having extremely low temperature, with temperature below -23.7 degrees of Celsius. The city-wide cold distribution is shown in figure 7. Cities in the north part of the United States have higher chance of cold, it is consistent with our common sense. However, there are large numbers of cities with high chance of cold distributed in the south east area, which is counter intuitive, thus the weather event cold may not be a good feature to cluster regions, and it was removed for the followed analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_cold = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Cold\",\n                      #size=\"Snow\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      #range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 7. City wide cold days percentage each year from 2016 to 2019')\n#fig_cold.update_layout(height=750, width=1000)\nfig_cold.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **K means clustering**\n **algorithm**\n\nK-means algorithm is one of the most employed clustering algorithms. K-means aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean. The algorithm proceeds as follows:\n\nPick K random points as cluster center positions.\n\nAssign each point to the nearest center.\n\nRecompute each cluster mean as the mean of the vectors assigned to that cluster.\n\nIf centers moved, go to step 2.\n\n**Distance**\n\nThe algorithm requires a distance measure to be deﬁned in the data space, and the Euclidean distance is often used. For example, the Euclidean distance between u = (u 1 , u 2 ) and v = (v 1 , v 2 ) is calculated by the following expression:\n\nr(u, v) = √（(u1-v1)2+(u2-v2)2\n\n**Pick K random points as cluster center position**\n\nThe elbow method is used to determine the optimal number of clusters. It plots the value of the cost function produced by different values of k. When k increases, the average distortion will increase, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters [1].\n\nFrom figure 8, the elbow happened when k = 3 or 4. Thus I will pick 4 as the number of clusters in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_flat.drop(['AirportCode','Cold', 'Hail'], axis=1)\nX.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ndistortions = []\n\nK = range(1,20)\nfor k in K:\n    kmean = KMeans(n_clusters=k, random_state=0, n_init = 50, max_iter = 500)\n    kmean.fit(X)\n    distortions.append(kmean.inertia_)\n\nfig_kmean = px.scatter(x=K, y=distortions, trendline=distortions, title='fig 8. The Elbow Method')\nfig_kmean.update_xaxes(title_text='k')\nfig_kmean.update_yaxes(title_text='distortion')\n#fig_kmean.update_layout(height=650, width=1000)\nfig_kmean.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **K means clustering results**\n\n**city-wide cluster distribution**\n\nThe city-wide results of K-means clustering is shown in figure 9. It seems k=4 is a good number of clusters. And cluster 0 (blue), 2 (green) and 3 (purple) have clear boundary with each other. Cities in cluster 1 (red) widely spread in the United States."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=4, random_state=0).fit(X)\n\ndf_flat['Cluster'] = (kmeans.labels_).astype(str)\ndf_cluster = pd.merge(df_flat[['AirportCode','Cluster']], weather.drop(['Cold','Hail'], axis=1), \n                      how='inner', on='AirportCode')\ndf_cluster.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_cluster = px.scatter_geo(df_cluster, lat='LocationLat', lon='LocationLng',\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      scope=\"usa\",\n                      color_discrete_sequence =['#AB63FA', '#EF553B', '#00CC96','#636EFA'],\n                      color = 'Cluster',\n                      title ='fig 9. City wide weather cluster distribution')\n#fig_cluster.update_layout(height=750, width=1000)\nfig_cluster.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**state-wide cluster distribution**\n\nTo better visulize each cluster's distribution in each state, clusters are grouped as state and location was calculated as the average of each cities latidude and logitude in that cluster. The state wide cluster distribution is shown in figure 10. Almost all states have mixtured types of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster2 = df_cluster.groupby(['State','Cluster']).agg({'Cluster':['count']}).reset_index()\ndf_cluster2.columns=pd.MultiIndex.from_tuples(((\"State\", \" \"),(\"Cluster\", \" \"),(\"Count\", \" \")))\ndf_cluster2.columns = df_cluster2.columns.get_level_values(0)\n#df_cluster2.tail(3) #state with each cluster counts\n\ndf_loc = df_cluster[['State','Cluster','LocationLat', 'LocationLng']]\ndf_loc1 = df_loc.groupby(['State','Cluster']).agg({'LocationLat':'mean'}).reset_index()\ndf_loc2 = df_loc.groupby(['State','Cluster']).agg({'LocationLng':'mean'}).reset_index()\ndf_loc3 = pd.merge(df_loc1,df_loc2, how='inner', on=['State','Cluster'])\n#df_loc3.tail(3) #state with cluster and location\n\ndf_clusterS = pd.merge(df_loc3,df_cluster2, how='inner', on=['State','Cluster'])\ndf_clusterS.tail(3) #state with each cluster count location","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_clusterS = px.scatter_geo(df_clusterS, lat='LocationLat', lon='LocationLng', \n                     color='Cluster',\n                     size='Count',\n                     color_discrete_sequence=['#636EFA', '#AB63FA', '#EF553B','#00CC96'],\n                     hover_name='State',\n                     scope=\"usa\",\n                     title = 'fig 10. State wide weather cluster distribution')\n#fig_clusterS.update_layout(height=750, width=1000)\nfig_clusterS.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**property of each cluster**\n\nThe property of each cluster is summarized in figure 11. Compare to other clusters, cluster 3 (purple) has highest chance of rain, cluster 2 (green) has highest chance of snow, cluster 1 (red) has lowest chance of rain while highest chance of storm; cluster 0 (blue) is similar to cluster 3 while its’ chance of rain is slightly lower than cluster 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"prop = df_cluster[['Cluster', 'Fog',\n                   'Precipitation','Rain', 'Snow', 'Storm']].groupby(['Cluster']).mean().reset_index()\nprop2 = prop.transpose().reset_index()\nprop2 = prop2[(prop2['index'] !='Cluster')].sort_values(by=0)\nprop2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from plotly.subplots import make_subplots\nfig_prop=make_subplots(rows=1, cols=4, shared_yaxes=True,horizontal_spacing=0)\n\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[0], name='Cluster 0'), row=1, col=1)\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[1], name='Cluster 1'), row=1, col=2)\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[2], name='Cluster 2'), row=1, col=3)\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[3], name='Cluster 3'), row=1, col=4)\n\nfig_prop.update_yaxes(title_text=\"duration%/year\", row=1, col=1)\nfig_prop.update_layout(title_text=\"fig 11. Weather distribution in each cluster\")\n#fig_prop.update_layout(height=550, width=1000)\nfig_prop.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**representive cities in each cluster**\n\nFour cities were selected in each cluster, Kansas City, MO is the representive city of clsuter 0; Dever, CO is the representive city of cluster 1; Detroit, MI is the representive city of cluster 2; Seattle, MA is the representive city of cluster 3. All the cities weather information are shown in figure 10, their weather properties are similar to their corresponding cluster as shown in figure 12."},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = weather[(weather['City']=='Seattle')| (weather['City']=='Detroit')|(weather['City']== 'Kansas City')|(weather['City']== 'Denver')]\n#df3\ndf4 = df3[['Storm', 'Precipitation','Snow', 'Fog','Rain', 'City']].groupby(['City']).mean().reset_index()\ndf4 = df4.transpose().reset_index()\ndf4.columns = df4.iloc[0]\ndf4 = df4[(df4['City'] !='City')]\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_city=make_subplots(rows=1, cols=4, shared_yaxes=True,horizontal_spacing=0)\n\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Kansas City'], name='Cluster0'), row=1, col=1)\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Denver'], name='Cluster1'), row=1, col=2)\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Detroit'], name='Cluster2'), row=1, col=3)\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Seattle'], name='Cluster3'), row=1, col=4)\n\nfig_city['layout']['xaxis1'].update(title=\"Kansas City, MO\")\nfig_city['layout']['xaxis2'].update(title=\"Denver, CO\")\nfig_city['layout']['xaxis3'].update(title=\"Detroit, MI\")\nfig_city['layout']['xaxis4'].update(title=\"Seattle, WA\")\nfig_city.update_yaxes(title_text=\"duration%/year\", row=1, col=1)\nfig_city.update_layout(title_text=\"fig 12. Representative cities in each cluster\")\n#fig_city.update_layout(height=550, width=1000)\nfig_city.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Principle component analysis**\nA way for obtaining a better understanding of the data is to visualize it; however, it is not straightforward to visualize high-dimensional data. This was a problem, for example, we could only choose pairs of weather events and plot them.\n\nPrincipal Component Analysis (PCA) is a technique that is widely used for applications such as dimensionality reduction, visualization and lossy data compression. This project only focused on the visualization aspect. PCA can be deﬁned as the orthogonal linear transformation of the data to a lower dimensional linear space, known as the principal subspace, such that the greatest variance by any projection of the data comes to lie on the ﬁrst coordinate (called the ﬁrst principal component), the second greatest variance on the second coordinate, and so on. Intuitively, PCA ﬁnds a meaningful coordinate basis to express the dataset [2].\n\nIn order to visualize how are the clusters are related in the original high dimension space, PCA was applied to the original dataset. The result of PCA is shown in figure 11, the cumulative variance explained for the first three component is able to cover more than 95% of the original information, therefore, reducing the dimensionality of the dataset (figure 13 left). After mapping the cluster labels to the PCA dataset and visualize it in three dimensions (figure 13 right), it showed that four cluster is a reasonable to identify similar samples within the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA().fit(X)\npcaX = pca.transform(X)\nc0 = []\nc1 = []\nc2 = []\nc3 = []\n\nfor i in range(len(pcaX)):\n    if kmeans.labels_[i] == 0:\n        c0.append(pcaX[i])\n    if kmeans.labels_[i] == 1:\n        c1.append(pcaX[i])\n    if kmeans.labels_[i] == 2:\n        c2.append(pcaX[i])\n    if kmeans.labels_[i] == 3:\n        c3.append(pcaX[i])\nc0 = np.array(c0)\nc1 = np.array(c1)\nc2 = np.array(c2)\nc3 = np.array(c3)\n\nfig_pca = make_subplots(rows=1, cols=2, column_widths=[0.3, 0.7], specs=[[{'type':'domain'}, {'type': 'mesh3d'}]])\n\nfig_pca.add_trace(go.Pie(values = pca.explained_variance_ratio_), row=1, col=1)\n\nfig_pca.add_trace(go.Scatter3d(x=c0[:,0], y=c0[:,1], z=c0[:,2],\n                               mode='markers', name='Cluster0'),  row=1, col=2)\nfig_pca.add_trace(go.Scatter3d(x=c1[:,0], y=c1[:,1], z=c1[:,2], \n                               mode='markers', name='Cluster1'),  row=1, col=2)\nfig_pca.add_trace(go.Scatter3d(x=c2[:,0], y=c2[:,1], z=c2[:,2], \n                               mode='markers', name='Cluster2'),  row=1, col=2)\nfig_pca.add_trace(go.Scatter3d(x=c3[:,0], y=c3[:,1], z=c3[:,2], \n                               mode='markers', name='Cluster3'),  row=1, col=2)\n\nfig_pca.update_layout(height=750, width=1000, title_text=\n          \"fig 13. PCA: explained variance%(left) and First 3 Component with mapped cluster(right)\")\n\nfig_pca.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# forecast Atlanta's weather events"},{"metadata":{"trusted":true},"cell_type":"code","source":"atlanta = df[(df['City']== 'Atlanta')]\nprint(atlanta['LocationLat'].unique())\nprint(atlanta['LocationLng'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nm = folium.Map(location=[33.755238, -84.388115])\nfolium.Marker(\n    location=[33.6301, -84.4418],\n).add_to(m)\nfolium.Marker(\n    location=[33.8784, -84.298],\n).add_to(m)\nfolium.Marker(\n    location=[33.7776, -84.5246],\n).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Atlanta has three airports, in this project will focus on the largest airport : Hartsfield-Jackson Atlanta International Airport.\n\nThe last observation of the event was in Dec 2019, different seasons have different weather properties, thus only events from month Nov, Dec and Jan were considered."},{"metadata":{"trusted":true},"cell_type":"code","source":"atlanta = atlanta[(atlanta['LocationLng']== -84.4418)]\n\natlanta_m = atlanta.loc[(atlanta['Start'].dt.month==12) | (atlanta['Start'].dt.month==11)| (atlanta['Start'].dt.month==1)]\natlanta_m = atlanta_m.loc[(atlanta_m['Type']!='Hail')&(atlanta_m['Type']!='Precipitation')]\nprint(atlanta_m['Type'].unique())\nprint(atlanta_m['Severity'].unique())\nprint(atlanta_m['Type'].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"last five events selected as testing dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = atlanta_m[['Type', 'Severity']]\nX2_test = X2.tail(5)\nX2_train = X2.head(675)\nprint(X2_train.tail())\nprint(X2_test)\nprint(X2['Severity'].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"few events were in severe and heavy, since HMMlearn cannot handle observation states (rain, fog, etc.) bigger than hidden states (light, moderate, etc), thus combine severe and heavy together."},{"metadata":{"trusted":true},"cell_type":"code","source":"Type = {'Rain':0, 'Fog':1, 'Snow':2} \nX2.Type = [Type[item] for item in X2.Type] \n\nSeverity = {'Light':0, 'Moderate':1, 'Severe':2, 'Heavy':2}\nX2.Severity = [Severity[item] for item in X2.Severity] \nX2_test = X2.tail(5)\nX2_train = X2.head(675)\nprint(X2_train.tail())\nprint(X2_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forecast weather events using Markov Chain\n\ntransition matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function that implements the Markov model to forecast the state. [3]\ndef transition_matrix(transitions):\n    n = 1+ max(transitions) #number of states\n\n    M = [[0]*n for _ in range(n)]\n\n    for (i,j) in zip(transitions,transitions[1:]):\n        M[i][j] += 1\n\n    #now convert to probabilities:\n    for row in M:\n        s = sum(row)\n        if s > 0:\n            row[:] = [f/s for f in row]\n    return M\n\n#test:\nt = list(X2_train.Type)\nm = transition_matrix(t)\nfor row in m: print(' '.join('{0:.2f}'.format(x) for x in row))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"forecast weather using gained transition matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The statespace\nstates = [\"Rain\",\"Fog\",\"Snow\"]\n\n# Possible sequences of events\ntransitionName = [[\"RR\",\"RF\",\"RS\"],[\"FR\",\"FF\",\"FS\"],[\"SR\",\"SF\",\"SS\"]]\n\n# Probabilities matrix (transition matrix)\ntransitionMatrix = [[0.89,0.10,0.02],[0.59,0.39,0.02],[0.33,0.07,0.60]]\n\n# A function that implements the Markov model to forecast the state. [4]\ndef activity_forecast(days):\n    # Choose the starting state\n    activityToday = \"Fog\"\n    print(\"Start state: \" + activityToday)\n    # Shall store the sequence of states taken. So, this only has the starting state for now.\n    activityList = [activityToday]\n    i = 0\n    # To calculate the probability of the activityList\n    prob = 1\n    while i != days:\n        if activityToday == \"Rain\":\n            change = np.random.choice(transitionName[0],replace=True,p=transitionMatrix[0])\n            if change == \"RR\":\n                prob = prob * 0.89\n                activityList.append(\"Rain\")\n                pass\n            elif change == \"RF\":\n                prob = prob * 0.1\n                activityToday = \"Fog\"\n                activityList.append(\"Fog\")\n            else:\n                prob = prob * 0.02\n                activityToday = \"Snow\"\n                activityList.append(\"Snow\")\n        elif activityToday == \"Fog\":\n            change = np.random.choice(transitionName[1],replace=True,p=transitionMatrix[1])\n            if change == \"FR\":\n                prob = prob * 0.59\n                activityList.append(\"Rain\")\n                pass\n            elif change == \"FF\":\n                prob = prob * 0.39\n                activityToday = \"Fog\"\n                activityList.append(\"Fog\")\n            else:\n                prob = prob * 0.02\n                activityToday = \"Snow\"\n                activityList.append(\"Snow\")\n        elif activityToday == \"Snow\":\n            change = np.random.choice(transitionName[2],replace=True,p=transitionMatrix[2])\n            if change == \"SR\":\n                prob = prob * 0.33\n                activityList.append(\"Rain\")\n                pass\n            elif change == \"SF\":\n                prob = prob * 0.07\n                activityToday = \"Fog\"\n                activityList.append(\"Fog\")\n            else:\n                prob = prob * 0.6\n                activityToday = \"Snow\"\n                activityList.append(\"Snow\")\n        i += 1  \n    print(\"Possible states: \" + str(activityList))\n    print(\"Probability of the possible sequence of states: \" + str(prob))\n    \n# Function that forecasts the possible state for the next 4 events\nactivity_forecast(4)\nprint('Acutal sequence of states: Rain, Fog, Rain, Rain, Rain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4/5 accuracy with 0.08 probability"},{"metadata":{"trusted":true},"cell_type":"code","source":"activity_forecast(4)\nprint('Acutal sequence of states: Rain, Fog, Rain, Rain, Rain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1/5 accuracy with 0.02 probability"},{"metadata":{},"cell_type":"markdown","source":"# predict weather severity using HMM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from hmmlearn import hmm\nmodel = hmm.MultinomialHMM(n_components=3, n_iter=50, tol=0.001)\nmodel.startprob_=np.array([0.82, 0.132, 0.044])\nmodel.fit(np.array(X2_train))\n#model.fit(X2_train['Type'], X2_train['Severity'])\nprint('startprob', np.round(model.startprob_,3))\nprint('transmat', np.round(model.transmat_,3))\nprint('emissionprob', np.round(model.emissionprob_,3))\nprint('score', model.score(X2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Acutal sequence of states: 0, 2, 0, 0 ,0')\nseen = np.array(X2_test[['Type']])\nmodel.decode(seen, algorithm='viterbi')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4/5 accuracy with log(-4.6)=0.66 probability"},{"metadata":{},"cell_type":"markdown","source":"# Reference\n\n[1] https://www.oreilly.com/library/view/statistics-for machine/9781788295758/c71ea970-0f3c-4973-8d3a-b09a7a6553c1.xhtml\n\n[2] https://setosa.io/ev/principal-component-analysis/\n\n[3] https://stackoverflow.com/questions/46657221/generating-markov-transition-matrix-in-python\n\n[4] https://www.datacamp.com/community/tutorials/markov-chains-python-tutorial"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}