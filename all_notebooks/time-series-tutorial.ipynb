{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is first Kernel on Time Series analysis. In this kernel I have tried to cover topics which are related to univariate time series such as AR, MA,ARIMA as well as ETS models. Hope everyone will like this kernel. ","metadata":{}},{"cell_type":"markdown","source":"![Time Series](https://images.unsplash.com/photo-1560221328-12fe60f83ab8?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1053&q=80)                     ","metadata":{}},{"cell_type":"markdown","source":"\n<a class=\"anchor\" id=\"0.1\"></a>\n# **Table of Contents**\n\n- 1.[Introduction to Time Series](#1)\n- 2.[What is Time Series?](#2)\n- 3.[What is not a Time Series](#3)\n- 4.[Where we can find the Time Series Data?](#4)\n- 5.[Features of the Time Series Data](#5)\n- 6.[Time Series Assumptions](#6)\n- 7.[Time Series Types](#7)\n- 8.[Reading and Saving Time Series Objects in Python](#8)\n- 9.[Components of the Time Series](#9)\n- 10.[Decomposition of Time Series](#10)\n- 11.[Moving average forecast](#11)\n- 12.[Handling Missing Values](#12)\n- 13.[Time Series  Range, Accuracy and Various Requirements](#13)\n- 14.[ETS Models](#14)\n     - 14.1  [SES, Holt & Holt-Winter Model](#14.1)\n        - 14.1.1  [SES - ETS(A, N, N) - Simple smoothing with additive errors](#14.1.1)\n        - 14.1.2  [Holt - ETS(A, A, N) - Holt's linear method with additive errors](#14.1.2)\n        - 14.1.3  [Holt-Winters - ETS(A, A, A) - Holt Winter's linear method with additive errors](#14.1.3)\n        - 14.1.4  [Holt-Winters - ETS(A, A, M) - Holt Winter's linear method ](#14.1.4)\n     - 14.2 [Model finalization](#14.2)\n        - 14.2.1  [Regression on Time](#14.2.1)\n        - 14.2.2  [ Regression on Time With Seasonal Components](#14.2.2)\n        - 14.2.3  [Naive Approach](#14.2.3)\n        - 14.2.4  [Simple Average](#14.2.4)\n        - 14.2.5  [Moving Average(MA)](#14.2.5)\n        - 14.2.6  [Simple Exponential Smoothing](#14.2.6)\n        - 14.2.7  [Holt's Linear Trend Method (Double Exponential Smoothing)](#14.2.7)\n        - 14.2.8  [Holt-Winters Method - Additive seasonality](#14.2.8)\n        - 14.2.9  [Holt-Winters Method - Multiplicative Model](#14.2.9)\n- 15.[AUTO REGRESSIVE Models](#15)\n     - 15.1  [Random Walk](#15.1)\n     - 15.2  [ARIMA Model](#15.2)\n     - 15.3  [Auto ARIMA](#15.3)\n- 16.[References](#16)\n  \n","metadata":{}},{"cell_type":"markdown","source":"\n# **1.Introduction to Time Series** <a class=\"anchor\" id=\"1\"></a>\n\n[Table of Contents](#0.1)\n\nEvery company in this world faces certain challenges and risks such as high compeition , failure of technology, labour unrest, inflation, recession, and change in government laws. Thus we can say that every buisness operates under risk and uncertainity. That's why forecast is necessary to lessen the adverse effect of the risks and to tell us in advance of any incoming dangers. There are various methods of forecast- mostly commonly used are :-\n\n1. Regression\n2. Data Mining Mehthods\n3. Time Series \n\nWhy these different teechniques are required for forecasting? The reason is that we have to deal with different types of data which possess different features , so to handle this different techniques are used for forecasting. For example, In case of regression or CART we have one response and a number of predictors. \n\n> **Forecasting is a technique that uses historical data as inputs to make informed estimates that are predictive in determining the direction of future trends. Businesses utilize forecasting to determine how to allocate their budgets or plan for anticipated expenses for an upcoming period of time.**\n\nIn this kernel , we are going to see the details about the time series data and how to analyze and forecast them. ","metadata":{}},{"cell_type":"markdown","source":"# **2.What is Time Series?**  <a class=\"anchor\" id=\"2\"></a>\n\n[Table of Contents](#0.1)\n\n\nA time series is a series of measurements on the **same variable** collected over time. These measurements are made at regular time intervals.A time series is a series of data points indexed in time order. Most commonly, a time series is a sequence taken at successive **equally spaced points** in time. Thus it is a sequence of **discrete-time data**. \n\nIntervals of the Time Series Data \n\n1. Yearly :- GDP , Macro-economic series\n2. Quarterly :- Revenue of a company.\n3. Monthly:- Sales, Expenditure, salary\n4. Weekly:- Demand , Price of Petrol and diesal\n5. Daily:- Closing price of stock, sensex value, daily transaction of ATM machine\n6. Hourly:- AAQI\n\nTime series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period.\n\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"markdown","source":"# 3.What is not a Time Series? <a class=\"anchor\" id=\"3\"></a>\n\n[Table of Contents](#0.1)\n\nData collected on multiple items at the **same point of time** is not a time series! For example Sales of different products at the same time. Also that kind of data where the **time periods are not same**. For example in a single time series both yearly and quarterly data cannot be mixed. ","metadata":{}},{"cell_type":"markdown","source":"# 4.Where we can find the Time Series Data? <a class=\"anchor\" id=\"4\"></a>\n\n[Table of Contents](#0.1)\n\nThe most common examples where we can encounter the time series data are following :- \n\n* Evaluation of manpower requirements from historic data and take a decision on hiring\n* Understanding the Stock movements based on the past data and advising their clients how best to invest. \n* Based on the consumption of the products , the grocery company can decide on which location the new shop can be set up.\n* In airline domain, based on the demand of the airline tickets between towns, airlines can create their dynamic ticket pricing.\n* For the hotels , based on the past data of the booking pattern it can decide on whether any discount can be offered at certain time of the year.\n\n> In short, we can say that time series data is being collected and utilized in all data driven decisions mechanisms. \n","metadata":{}},{"cell_type":"markdown","source":"# 5.Features of the Time Series Data <a class=\"anchor\" id=\"5\"></a>\n\n[Table of Contents](#0.1)\n\nThe following features mentioned below makes the time series analysis challenging and none of the other machine learning techniques applicable because of the following reasons :- \n\n* Data are depedent on each other.\n* In the case of time series , **ordering of data matters a lot**. \n* Ordering is very significant because there is dependency and changing the order will change the data structure.\n\nPlease note that in case where data is cross sectional , order of the obeservation does not matter but if the data is time series order of all the observations are important. ","metadata":{}},{"cell_type":"markdown","source":"# 6.Time Series Assumptions <a class=\"anchor\" id=\"6\"></a>\n\n[Table of Contents](#0.1)\n\nSome of the most common assumptions made for time series are based on the common sense. But always Keep in mind one thing \n\n> Very long range forecasts does not work well !!\n\n* Forecast is done by keeping in mind that the market and the other conditions are not going to change in the future.\n* There will be not any change in the market.\n* But the change is gradual and not a drastic change.\n* Situations like recession in 2008 US market will send the forecasts into a tizzy. \n* Events like demonetization would throw the forecasts into disarray\n\nBased on the data available , we should not try to forecast for more than a few periods ahead.","metadata":{}},{"cell_type":"markdown","source":"# 7.Time Series Types  <a class=\"anchor\" id=\"7\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### a) Univariate Time Series\n\nA univariate Time series is a series of data  with a single time dependant variable like Demand for a product at time,t. \n\n> A time series that consists of single (scalar) observations recorded sequentially over equal time increments. Some examples are monthly CO2 concentrations and southern oscillations to predict el nino effects.\n\nFor example, have a look at the sample dataset below that consists of the minimum temperatures across the months of the year from the Southern Hemisphere from 1981 to 1990. Here, temperature is the dependent variable (dependent on Time).\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport  matplotlib.pyplot  as  plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Read the time series data\n\nunivariate_series   =  pd.read_csv('/kaggle/input/time-series-data/daily-min-temperatures.csv', header = 0, index_col = 0, parse_dates = True, squeeze = True)\n\n### Print first five records\nunivariate_series.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot the time series data to detect patterns","metadata":{}},{"cell_type":"code","source":"univariate_series.plot()\nplt.ylabel('Minimum Temp')\nplt.title('Minimum temperature in Southern Hemisphere \\n  from 1981 to 1990')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we want to predict the  temperature for the next few months, we will try to look at the past values and try to gauge and extract the pattern. \nHere we observe a pattern within each year indicating a seasonal effect. Such observations will help us in predicting future values.\n\n**Note: We have used only one variable here , Temp (the temperature of the past 19 years).**\n\nHence this is called as the Univariate Time Series Analysis/Forecasting. ","metadata":{}},{"cell_type":"markdown","source":"### b) Multivariate Time Series \n\nA multivariate time series data contains more than one time dependant variable. Each variable here depends on not only the past values but also has some dependency on other variables.This dependency is used for forecasting the future values. For reference , please have a look at the following website \n\n https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/\n ","metadata":{}},{"cell_type":"markdown","source":"**Air pollution forecasting**\n\nhttps://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n\nThis dataset tells us about the weather and the level of the pollution each hour for five years at the US embassy located in Beijing,China. The data includes the date-time, the pollution called PM2.5 concentaration, and the other weather information including dew point, temeperature, pressure, wind direction , wind speed and the cumulative no of hours of snow and rain. \n\nThe complete feature list of the raw data is as follows :\n\n| Sl No | Variable | Description |\n| --- | --------------- | ------------------------------ |\n| 1 | No | row number | \n| 2 | qyear | year of data in this row | \n| 3 | month | month of data in this row | \n| 4 | day | day of data in this row | \n| 5 | hour | hour of data in this row | \n| 6 | pm2.5 | PM2.5 concentration | \n| 7 | DEWP | Dew Point | \n| 8 | TEMP | Temperature | \n| 9 | PRES | Pressure | \n| 10 | cbwd | Combined wind direction | \n| 11 | Iws | Cumulated wind speed | \n| 12 | Is | Cumulated hours of snow | \n| 13 | Ir | Cumulated hours of rain | \n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"\n**Given the weather conditions and pollution for prior hours, we forecast the pollution at the next hour.**","metadata":{}},{"cell_type":"markdown","source":"* 1) Consolidate the date-time information into a single date-time so that we can use it as an index in Pandas.\n* 2) Treat NA values. \n\nA quick check reveals NA values for pm2.5 for the first 24 hours. We will, therefore, need to remove the first row of data. There are also a few scattered “NA” values later in the dataset; we can mark them with 0 values for now.","metadata":{}},{"cell_type":"markdown","source":"* 1) Load the raw dataset and parses the date-time information as the Pandas DataFrame index. \n* 2) Drop the “No” column \n* 3) Name each column. \n* 4) Replace NA values with “0” \n* 5) Remove first 24 hours.","metadata":{}},{"cell_type":"code","source":"# process the date time information \n\nfrom datetime import datetime\ndef parse(x):\n    return datetime.strptime(x,'%Y %m %d %H')\n\n# Load dataset\npollution_df = pd.read_csv(\"/kaggle/input/time-series-data/pollution.csv\",parse_dates = [['year', 'month', 'day', 'hour']],index_col=0, date_parser=parse)\npollution_df.drop('No', axis=1, inplace=True)\n# manually specify column names\n\npollution_df.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\npollution_df.index.name = 'date'\n\n# mark all NA values with 0\npollution_df['pollution'].fillna(0, inplace=True)\n\n# drop the first 24 hours\npollution_df = pollution_df[24:]\n\n# summarize first 5 rows\nprint(pollution_df.head(5))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values = pollution_df.values\n\n# specify columns to plot\n\ngroups = [0, 1, 2, 3, 5, 6, 7]\ni = 1\n\n# plot each column\nplt.figure()\n\nfor group in groups:\n    plt.subplot(len(groups), 1, i)\n    plt.plot(values[:, group])\n    plt.title(pollution_df.columns[group], y=0.5, loc='right')\n    i += 1\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **8.Reading and Saving Time Series Objects in Python** <a class=\"anchor\" id=\"8\"></a>\n\n[Table of Contents](#0.1)\n\n### Example 1 \n\n#### Use US Airpassengers data set","metadata":{}},{"cell_type":"code","source":"airPax_df = pd.read_csv('/kaggle/input/time-series-data/AirPassengers.csv')\n#Parse strings to datetime type\nairPax_df['Month'] = pd.to_datetime(airPax_df['Month'],infer_datetime_format=True) #convert from string to datetime\nairPax_df_indexed = airPax_df.set_index(['Month'])\nairPax_df_indexed.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(airPax_df_indexed) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Save the TS object\nairPax_df_indexed.to_csv('ts1.csv', index = True, sep = ',')\n\n### Check the object retrieved\nseries1 = pd.read_csv('ts1.csv', header = 0)\n\n\n### Check\nprint(type(series1))\nprint(series1.head(2).T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example 2\n\nReading the  GDP of India series data  and save this TS object using python.\nData is yearly from 1960-1-1 to 2017-12-31.\n\nhttps://pythontips.com/2013/08/02/what-is-pickle-in-python/\n\nAny object in python can be pickled so that it can be saved on disk. What pickle does is that it “serialises” the object first before writing it to file. \n\nPickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script.\n\nNow we store the TS object and retrieve the same from the pickle object.","metadata":{}},{"cell_type":"code","source":"india_gdp_df = pd.read_csv(\"/kaggle/input/time-series-data/GDPIndia.csv\")\ndate_rng = pd.date_range(start='1/1/1960', end='31/12/2017', freq='A')\nindia_gdp_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Year'])\nindia_gdp_df.head(5).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(india_gdp_df.TimeIndex, india_gdp_df.GDPpercapita)\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Load as a pickle object\n\nimport pickle\n\nwith open('GDPIndia.obj', 'wb') as fp:\n        pickle.dump(india_gdp_df, fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Retrieve the pickle object\n\nwith open('GDPIndia.obj', 'rb') as fp:\n     india_gdp1_df = pickle.load(fp)\n        \nindia_gdp1_df.head(5).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9.Components of the Time Series <a class=\"anchor\" id=\"9\"></a>\n\n[Table of Contents](#0.1)\n\nThe components of the time series are following :- \n\n* **Trend** :- A gradual shift or movement to relatively higher or lower values over a long period of time.\n       \n       1. When the time series analysis shows a general trend , that is upward . It is called uptrend.\n       2. When the time series analysis shows a general trend , that is downward. It is called downtrend.\n       3. When there is no trend, we call it horizontal or stationary trend.\n      \n* **Seasonality** :- It means upward or downward swings. Repeating periods within a fixed period of time. It is usually observed within a period of time.For   example , if you live in a country with cold winters and hot summers, your air conditioning costs goes high in summer and low in winters. \n* **Cyclic Patterns** :- It refers to repeating up and down movements. It usually go over more than a year of time.It is much harder to predict.\n* **Irregular** :- It refers to erratic, unsystematic, 'residual' flutuations. It is for short duration and non repeating. It happens due to random        variations or unforeseen events. It generally contains the white noise which we will see in the coming sections. \n\n","metadata":{}},{"cell_type":"markdown","source":"### Trend & Seasonality\n\nConsider the example of **shampoo sales dataset**. \n\nThis Dataset describes the monthly number of sales of shampoo over a 3 year period. The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright, and Hyndman (1998).\n\n**Data source**:https://github.com/jbrownlee/Datasets\n\nBelow is a sample of the first 5 rows of data, including the header row.\n\n| Month | Sales |\n| ---- | -------- |\n| 1-01 | 266.0 | \n| 1-02 | 145.9 | \n| 1-03 | 183.1 | \n| 1-04 | 119.3 | \n| 1-05 | 180.3 | \n\n\n","metadata":{}},{"cell_type":"code","source":"def parser(x):\n    return datetime.strptime('190'+x, '%Y-%m')\n \nseries = pd.read_csv('/kaggle/input/time-series-data/shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n\nseries.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The above plot shows an increasing trend.\n","metadata":{}},{"cell_type":"markdown","source":"**Minimum Daily Temperatures Dataset**\n\nhttps://machinelearningmastery.com/time-series-seasonality-with-python/\n\nThis dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.\n\nData source: Data Market https://datamarket.com/data/set/22r0/sales-of-shampoo-over-a-three-year-period\n\nThe units are in degrees Celsius and there are 3,650 observations. The source of the data is credited as the Australian Bureau of Meteorology.\n\nA sample of the first 5 rows of data, including the header row is shown below:\n\n| Date | Temperature |\n| ------- | -------- | \n| 1981-01-01 | 20.7 | \n| 1981-01-02 | 17.9 | \n| 1981-01-03 | 18.8 | \n| 1981-01-04 | 14.6 | \n| 1981-01-05 | 15.8 | \n","metadata":{"trusted":true}},{"cell_type":"code","source":"### Read the time series data\n\nseries   =  pd.read_csv('/kaggle/input/time-series-data/daily-min-temperatures.csv', header = 0, index_col = 0, parse_dates = True, squeeze = True)\n\nseries.plot()\nplt.ylabel('Minimum Temp')\nplt.title('Minimum temperature in Southern Hemisphere \\n From 1981 to 1990')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The above plot shows a strong seasonality component.","metadata":{"trusted":true}},{"cell_type":"markdown","source":"We can draw a boxplot to check the variation across months in a year (1990).\nIt appears that we have a seasonal component each year showing swing from summer to winter.","metadata":{}},{"cell_type":"code","source":"months         = pd.DataFrame()\none_year       = series['1990'] \ngroups         = one_year.groupby(pd.Grouper(freq='M')) \nmonths         = pd.concat([pd.DataFrame(x[1].values) for x in groups], axis=1) \nmonths         = pd.DataFrame(months) \nmonths.columns = range(1,13) \nmonths.boxplot() \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot shows the signiﬁcant change in distribution of minimum temperatures across the months of the year from the Southern Hemisphere summer in January to the Southern Hemisphere winter in the middle of the year, and back to summer again.","metadata":{}},{"cell_type":"markdown","source":"We group the Minimum Daily Temperatures dataset by years. A box and whisker plot is then created for each year and lined up side-by-side for direct comparison.","metadata":{}},{"cell_type":"code","source":"groups = series.groupby(pd.Grouper(freq='A')) \nyears  = pd.DataFrame() \nfor name, group in groups: \n    years[name.year] = group.values \nyears.boxplot() \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't observe much year-by-year variation ","metadata":{}},{"cell_type":"markdown","source":"**Tractor Sales Series Data**\n\nThis data shows the details of the no of the tractors sold on the monthly basis for the year 2020. \n\n","metadata":{}},{"cell_type":"code","source":"tractor_df = pd.read_csv(\"/kaggle/input/time-series-data/TractorSales.csv\")\ntractor_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = pd.date_range(start='2003-01-01', freq='MS', periods=len(tractor_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import calendar\ntractor_df['Month'] = dates.month\ntractor_df['Month'] = tractor_df['Month'].apply(lambda x: calendar.month_abbr[x])\ntractor_df['Year'] = dates.year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tractor.drop(['Month-Year'], axis=1, inplace=True)\ntractor_df.rename(columns={'Number of Tractor Sold':'Tractor-Sales'}, inplace=True)\ntractor_df = tractor_df[['Month', 'Year', 'Tractor-Sales']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tractor_df.set_index(dates, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tractor_df = tractor_df[['Tractor-Sales']]\ntractor_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tractor_df.plot()\nplt.ylabel('Tractor Sales')\nplt.title(\"Tractor Sales from 2003 to 2014\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows a strong seasonality and trend component.\n\nWe can draw a boxplot to check the variation across months in a year (2011). ","metadata":{}},{"cell_type":"code","source":"months         = pd.DataFrame()\none_year       = tractor_df['2011'] \ngroups         = one_year.groupby(pd.Grouper(freq='M')) \nmonths         = pd.concat([pd.DataFrame(x[1].values) for x in groups], axis=1) \nmonths         = pd.DataFrame(months) \nmonths.columns = range(1,13) \nmonths.boxplot() \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that we have a seasonal component each year showing swing from May to Aug.","metadata":{}},{"cell_type":"code","source":"tractor_df['2003']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also use the Champagne Series data, food sales data and plot the time series.Check whether the data shows trend or seasonality or both.","metadata":{}},{"cell_type":"markdown","source":"# **10.Decomposition of Time Series** <a class=\"anchor\" id=\"10\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### Additive Model","metadata":{}},{"cell_type":"markdown","source":"### Additive Decomposition\n\n* An additive model suggests that the components are added together.\n* An additive model is linear where changes over time are consistently made by the same amount.\n* A linear seasonality has the same frequency (width of the cycles) and amplitude (height of the cycles).","metadata":{}},{"cell_type":"markdown","source":"The statsmodels library provides an implementation of the naive, or classical, decomposition method in a function called seasonal_decompose(). You need to specify whether the model is additive or multiplicative.\n\nThe seasonal_decompose() function returns a result object which contains arrays to access four pieces of data from the decomposition.\n\nhttps://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/","metadata":{}},{"cell_type":"markdown","source":"**Additive model decomposition on Retail Turnover data**\n\n","metadata":{}},{"cell_type":"code","source":"turnover_df= pd.read_csv('/kaggle/input/time-series-data/RetailTurnover.csv')\ndate_rng = pd.date_range(start='1/7/1982', end='31/3/1992', freq='Q')\nturnover_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Quarter'])\nturnover_df.head()\nplt.plot(turnover_df.TimeIndex, turnover_df.Turnover)\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nimport  statsmodels.api as sm\ndecompTurnover_df = sm.tsa.seasonal_decompose(turnover_df.Turnover, model=\"additive\", freq=4)\ndecompTurnover_df.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running the above code performs the decomposition, and plots the 4 resulting series. We observe that the trend and seasonality are clearly separated.","metadata":{}},{"cell_type":"code","source":"trend = decompTurnover_df.trend\nseasonal = decompTurnover_df.seasonal\nresidual = decompTurnover_df.resid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trend.head(12))\nprint(seasonal.head(12))\nprint(residual.head(12))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Champagne Series data, perform additive model decomposition.\n\nHint:\ndate_rng = pd.date_range(start='1/1/1964', end='30/9/1972', freq='M')\ndate_rng\nChamp['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])","metadata":{}},{"cell_type":"markdown","source":"### Multiplicative model\n\n### Multiplicative Decomposition\n\n* An additive model suggests that the components are multipled together.\n* An additive model is non-linear such as quadratic or exponential. \n* Changes increase or decrease over time.\n* A non-linear seasonality has an increasing or decreasing frequency (width of the cycles) and / or amplitude (height of the cycles) over time.","metadata":{}},{"cell_type":"markdown","source":"**Perform multiplicative model decomposition on International Air Passengers Data.**","metadata":{}},{"cell_type":"code","source":"\nairPax_df = pd.read_csv('/kaggle/input/time-series-data/AirPax.csv')\nprint(airPax_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_rng = pd.date_range(start='1/1/1949', end='31/12/1960', freq='M')\nprint(date_rng)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"airPax_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])\nprint(airPax_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decompAirPax = sm.tsa.seasonal_decompose(airPax_df.Passenger, model=\"multiplicative\", freq=12)\ndecompAirPax.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running the above code performs the decomposition, and plots the 4 resulting series.\nWe observe that the trend and seasonality are clearly separated.","metadata":{}},{"cell_type":"code","source":"seasonal = decompAirPax.seasonal\nseasonal.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Perform multiplicative model decomposition on Tractor Sales Series**\n\n## Visualization of Seasonality - Month plots\nLet us use retail turnover data and observe seasonality using visualization techniques.","metadata":{}},{"cell_type":"code","source":"quarterly_turnover = pd.pivot_table(turnover_df, values = \"Turnover\", columns = \"Quarter\", index = \"Year\")\nquarterly_turnover","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quarterly_turnover.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quarterly_turnover.boxplot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n\nWe see very clearly that the turnover is high in Quarter 1 and very low in quarter 2.","metadata":{}},{"cell_type":"markdown","source":"**Let us use Petrol data and observe seasonality using visualization techniques**","metadata":{}},{"cell_type":"code","source":"petrol_df = pd.read_csv('/kaggle/input/time-series-data/Petrol.csv')\npetrol_df.head()\ndate_rng = pd.date_range(start='1/1/2001', end='30/9/2013', freq='Q')\n\n#date_rng\npetrol_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Quarter'])\nprint(petrol_df.head())\n\nplt.plot(petrol_df.TimeIndex, petrol_df.Consumption)\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seasonal Indices\n\n* Seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly. \n* Seasonality may be caused by various factors, such as weather, vacation, and holidays and consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series.","metadata":{}},{"cell_type":"markdown","source":"# **11.Moving average forecast** <a class=\"anchor\" id=\"11\"></a>\n\n[Table of Contents](#0.1)\n\nMoving Average Smoothing is a naive and effective technique in time series forecasting.\n\nSmoothing is a technique applied to time series to remove the fine-grained variation between time steps. \n\nCalculating a moving average involves creating a new series where the values are comprised of the average of raw observations in the original time series.\n\nA moving average requires that you specify a window size called the window width. This defines the number of raw observations used to calculate the moving average value. \n\n### Two main types of moving averages:\n#### 1) Centered moving average  - calculated as the average of raw observations at, before and after time, t.\n#### 2) Trailing moving average - uses historical observations and is used on time series forecasting.","metadata":{}},{"cell_type":"markdown","source":"The rolling() function on the Series Pandas object will automatically group observations into a window.\n\nYou can specify the window size, and by default, a trailing window is created. Once the window is created, we can use the mean value, which forms our transformed dataset.\n","metadata":{}},{"cell_type":"markdown","source":"###  Average and moving average for Air Temp data","metadata":{}},{"cell_type":"code","source":"airTemp_df =  pd.read_csv('/kaggle/input/time-series-data/AirTemp.csv')\ndate_rng =  pd.date_range(start='1/1/1920', end='31/12/1939', freq='M')\nairTemp_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])\nairTemp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot the average temp","metadata":{}},{"cell_type":"code","source":"plt.plot(airTemp_df.TimeIndex, airTemp_df.AvgTemp)\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot the average forecast","metadata":{}},{"cell_type":"code","source":"temp_avg = airTemp_df.copy()\ntemp_avg['avg_forecast'] = airTemp_df['AvgTemp'].mean()\n\nplt.figure(figsize=(12,8))\nplt.plot(airTemp_df['AvgTemp'], label='Data')\nplt.plot(temp_avg['avg_forecast'], label='Average Forecast')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot the moving average forecast and average temperature","metadata":{}},{"cell_type":"code","source":"mvg_avg = airTemp_df.copy()\nmvg_avg['moving_avg_forecast'] = airTemp_df['AvgTemp'].rolling(12).mean()\nplt.plot(airTemp_df['AvgTemp'], label='Average Temperature')\nplt.plot(mvg_avg['moving_avg_forecast'], label='Moving Average Forecast')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Moving average of window size 5 for US GDP","metadata":{}},{"cell_type":"code","source":"USGDP_df    = pd.read_csv('/kaggle/input/time-series-data/GDPIndia.csv', header=0)\nprint(USGDP_df.head())\ndate_rng = pd.date_range(start='1/1/1929', end='31/12/1991', freq='A')\nprint(date_rng)\n\nUSGDP_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Year'])\nplt.plot(USGDP_df.TimeIndex, USGDP_df.GDPpercapita)\n\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mvg_avg_USGDP = USGDP_df.copy()\nmvg_avg_USGDP['moving_avg_forecast'] = USGDP_df['GDPpercapita'].rolling(5).mean()\nplt.plot(USGDP_df['GDPpercapita'], label='US GDP')\nplt.plot(mvg_avg_USGDP['moving_avg_forecast'], label='US GDP MA(5)')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Moving average line is close to the original data line.","metadata":{}},{"cell_type":"markdown","source":"### Moving average of window size 3 for India GDP","metadata":{}},{"cell_type":"code","source":"IndiaGDP_df = pd.read_csv('/kaggle/input/time-series-data/GDPIndia.csv', header=0)\n\ndate_rng = pd.date_range(start='1/1/1960', end='31/12/2017', freq='A')\nIndiaGDP_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Year'])\n\nprint(IndiaGDP_df.head())\n\nplt.plot(IndiaGDP_df.TimeIndex, IndiaGDP_df.GDPpercapita)\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mvg_avg_IndiaGDP = IndiaGDP_df.copy()\nmvg_avg_IndiaGDP['moving_avg_forecast'] = IndiaGDP_df['GDPpercapita'].rolling(3).mean()\n\nplt.plot(IndiaGDP_df['GDPpercapita'], label='India GDP per Capita')\nplt.plot(mvg_avg_IndiaGDP['moving_avg_forecast'], label='India GDP/Capita MA(3)')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Moving average line is close to the original data line.","metadata":{}},{"cell_type":"markdown","source":"# 12.Handling Missing Values <a class=\"anchor\" id=\"12\"></a>\n\n[Table of Contents](#0.1)\n\n### Missing Data\n\n#### 1. \tNo missing data is allowed in time series as data is ordered.\n#### 2. \tIt is simply not possible to shift the series to fill in the gaps.\n\n\n### Reasons for missing data\n\n#### 1) \tData is not collected or recorded\n#### 2) \tData never existed\n#### 3) \tData corruption\n","metadata":{}},{"cell_type":"markdown","source":"#### Mark missing values\n\n* NaN is the default missing value marker for reasons of computational speed and convenience. \n* We can easily detect this value with data of different types: floating point, integer, Boolean and general object. \n* However, the Python None will arise and we wish to also consider that missing.\n* To make detecting missing values easier across different array dtypes, pandas provides functions, isna() and notna(), which are also methods on Series and DataFrame objects.\n* For datetime64[ns] types, NaT represents missing values. Pandas objects provide histocompatibility between NaT and NaN.","metadata":{}},{"cell_type":"markdown","source":"### Inserting missing values\n\nYou can assign missing values by simply assigning to containers. The missing value will be chosen based on the dtype.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy  as np\ns = pd.Series([1,2,3,4,5,6])\ns.loc[4] = np.NaN\nprint(s)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculations with missing values\n\n**Descriptive statistics and computational statistical methods are written to take into account for missing data.  **\n\n**Examples:**\n\n* When summing data, NA(missing) values will be treated as zeros.\n* If the data are all NA, the result will be 0.\n* Cumulative methods like cumsum() and cumprod() ignore NA values by default but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna = False","metadata":{}},{"cell_type":"markdown","source":"A time series is a series of data points indexed in time order.  Most of the times, users want to replace the missing values in time series data by neighbouring non-missing values.  There may be a need for imputation or interpolation of missing values lying in between known values. \n\nMethods of imputation for replacing missing values (meaningful values)\n\n| Method | When suitable |\n| ---------------------------- | ------------------------------------ |\n| Take average of the nearest neighbours | Data has no seasonality |\n| Take average of the seasons from two or all available years | Data has seasonality |\n| Interpolate function of pandas |  |\n| Linear interpolation | Relationship in the interval of two samples is a first order polynomial |\n| Polynomial such as Quadratic or Cubic interpolation | Second or third order polynomial describes the interval between two samples |\n| Spline | Handles non-uniform spacing of samples |","metadata":{}},{"cell_type":"markdown","source":"#### Example\n\nThis example is taken from the following link\n\nhttps://www.dezyre.com/recipes/deal-with-missing-values-in-timeseries-in-python\n\n\n\n","metadata":{}},{"cell_type":"code","source":"def handle_missing_values():\n    print()\n    print(format('How to deal with missing values in a Timeseries in Python',\n                 '*^82'))\n    \n    # Create date\n    time_index = pd.date_range('28/03/2017', periods=5, freq='M')\n\n    # Create data frame, set index\n    df = pd.DataFrame(index=time_index);\n    print(df)\n\n    # Create feature with a gap of missing values\n    df['Sales'] = [1.0,2.0,np.nan,np.nan,5.0];\n    print(); print(df)\n\n    # Interpolate missing values\n    df1= df.interpolate();\n    print(); print(df1)\n\n    # Forward-fill Missing Values\n    df2 = df.ffill();\n    print(); print(df2)\n\n    # Backfill Missing Values\n    df3 = df.bfill();\n    print(); print(df3)\n\n    # Interpolate Missing Values But Only Up One Value\n    df4 = df.interpolate(limit=1, limit_direction='forward');\n    print(); print(df4)\n\n    # Interpolate Missing Values But Only Up Two Values\n    df5 = df.interpolate(limit=2, limit_direction='forward');\n    print(); print(df5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"handle_missing_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Example 2 :- Checking on the water consumption data**\n\n1. For reference , you can have a look at this link below , the example taken here by me is from this article:- \n\nhttps://medium.com/@drnesr/filling-gaps-of-a-time-series-using-python-d4bfddd8c460","metadata":{}},{"cell_type":"code","source":"waterConsumption_df=pd.read_csv(\"/kaggle/input/time-series-data/WaterConsumption.csv\")\nwaterConsumption_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the column to DateTime format\nwaterConsumption_df.Date = pd.to_datetime(waterConsumption_df.Date, format='%d-%m-%Y')\nwaterConsumption_df = waterConsumption_df.set_index('Date')\nwaterConsumption_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For charting purposes, we will add a column that contains the missing values only.\n\nwaterConsumption_df = waterConsumption_df.assign(missing= np.nan)\nwaterConsumption_df.missing[waterConsumption_df.target.isna()] = waterConsumption_df.reference\nwaterConsumption_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waterConsumption_df.plot(style=['k--', 'bo-', 'r*'],figsize=(20, 10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling using mean or median\n# Creating a column in the dataframe\n# instead of : df['NewCol']=0, we use\n# df = df.assign(NewCol=default_value)\n# to avoid pandas warning.\nwaterConsumption_df = waterConsumption_df.assign(FillMean=waterConsumption_df.target.fillna(waterConsumption_df.target.mean()))\nwaterConsumption_df = waterConsumption_df.assign(FillMedian=waterConsumption_df.target.fillna(waterConsumption_df.target.median()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing using the rolling average\nwaterConsumption_df = waterConsumption_df.assign(RollingMean=waterConsumption_df.target.fillna(waterConsumption_df.target.rolling(24,min_periods=1,).mean()))\n# imputing using the rolling median\nwaterConsumption_df = waterConsumption_df.assign(RollingMedian=waterConsumption_df.target.fillna(waterConsumption_df.target.rolling(24,min_periods=1,).median()))# imputing using the median","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imputing using interpolation with different methods\n\nwaterConsumption_df = waterConsumption_df.assign(InterpolateLinear=waterConsumption_df.target.interpolate(method='linear'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateTime=waterConsumption_df.target.interpolate(method='time'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateQuadratic=waterConsumption_df.target.interpolate(method='quadratic'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateCubic=waterConsumption_df.target.interpolate(method='cubic'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSLinear=waterConsumption_df.target.interpolate(method='slinear'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateAkima=waterConsumption_df.target.interpolate(method='akima'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolatePoly5=waterConsumption_df.target.interpolate(method='polynomial', order=5)) \nwaterConsumption_df = waterConsumption_df.assign(InterpolatePoly7=waterConsumption_df.target.interpolate(method='polynomial', order=7))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSpline3=waterConsumption_df.target.interpolate(method='spline', order=3))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSpline4=waterConsumption_df.target.interpolate(method='spline', order=4))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSpline5=waterConsumption_df.target.interpolate(method='spline', order=5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scoring the results and see which is better\n\n# Import a scoring metric to compare methods\nfrom sklearn.metrics import r2_score\n\nresults = [(method, r2_score(waterConsumption_df.reference, waterConsumption_df[method])) for method in list(waterConsumption_df)[3:]]\nresults_df = pd.DataFrame(np.array(results), columns=['Method', 'R_squared'])\nresults_df.sort_values(by='R_squared', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data after imputation\n\nfinal_df= waterConsumption_df[['reference', 'target', 'missing', 'InterpolateTime' ]]\nfinal_df.plot(style=['b-.', 'ko', 'r.', 'rx-'], figsize=(20,10));\nplt.ylabel('Temperature');\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n          fancybox=True, shadow=True, ncol=5, prop={'size': 14} );","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Limitation** \n\n* For the time interpolation to succeed, the dataframe must have the index in Date format with intervals of 1 day or more, (daily, monthly, …) however, it will not work for time-based data, like hourly data and so.\n* if it is important to use a different index for the dataframe, the use the reset_index().set_index('Date'), do the interpolation, and then apply the reset_index().set_index('DesiredIndex').\n* If the data contains another dividing column, like the type of merchandise, and we are imputing sales, then the imputation should be for each merchandise separately.\n\n","metadata":{}},{"cell_type":"markdown","source":"# 13.Time Series Range, Accuracy and Various Requirements <a class=\"anchor\" id=\"13\"></a>\n\n[Table of Contents](#0.1)\n","metadata":{}},{"cell_type":"markdown","source":"Time Series forecast models can both make predictions and provide a confidence interval for those predictions.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Forecast Range","metadata":{}},{"cell_type":"markdown","source":"**Confidence intervals provide an upper and lower expectation for the real observation. **\n\nThese are useful for assessing the range of real possible outcomes for a prediction and for better understanding the skill of the model.\n\nFor example, the ARIMA implementation in the statsmodel python library can be used to fit an ARIMA model. It returns an ARIMAResults object. \n\nThe object provides the forecast() function returns three values:\n* 1) Forecast: The forecasted value in the \n* 2) Standard Error of the model: \n* 3) Confidence Interval: The 95% confidence interval for the forecast","metadata":{}},{"cell_type":"markdown","source":"### Forecast Accuracy\n\nThe error in the forecast is the difference between the actual value and the forecast.\n\nTwo popular accuracy measures are RMSE and MAPE.","metadata":{}},{"cell_type":"markdown","source":"### Forecast  Requirements\n\nA time series model must contain a key time column that contains unique values, input columns, and at least one predictable column.\n\nTime series data often requires cleaning, scaling, and even transformation\n\n**Frequency:** Data may be provided at a frequency that is too high to model or is unvenly spread through time requiring  resampling for use in models.\n\n**Outliers:** Data may contain corrupt or extreme outlier values that need to be identified and handled.\n\n**Frequency:**\n\n* Frequencies may be too granular or not granular enough to get insights.\n* The pandas library in Pyhton provides the capability to increase or decrease the sampling frequency of the time series data.\n\n**Resampling:**\n\n* Resampling may be required if the data is not available at the same frequency that you want to make predictions.\n* Resampling may be required to provide additional structure or insight into the learning problem for supervised learning models.","metadata":{}},{"cell_type":"markdown","source":"**Up-sampling**\n* Increase the frequencies of the sample, example: months to days\n* Care may be needed in deciding how the fine-grained observations are calculated using interpolation.\n\n* The function, resample() available in the pandas library works on the Series and DataFrame objects.\n* This can be used to group records when down-sampling and make space for new observations when up-sampling.","metadata":{}},{"cell_type":"markdown","source":"### Example \n\n**Up-sampling frequency**\n\n* The observations in the Shampoo Sales are monthly. We need to up-sample the frequency from monthly to daily and use an interpolation scheme to fill in the new daily frequency.\n\n* We can use this function to transform our monthly dataset into a daily dataset by calling resampling and specifying preferred frequency of calendar day frequency or D.","metadata":{}},{"cell_type":"code","source":"\ndef parser(x):\n       return datetime.strptime('190'+x, '%Y-%m')\n\nshampoo_df = pd.read_csv('/kaggle/input/time-series-data/shampoo.csv', header = 0, index_col = 0, parse_dates = True, \n                               squeeze = True, date_parser = parser)\n\nupsampled_ts = shampoo_df.resample('D').mean()\nprint(upsampled_ts .head(36))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nWe observe that the resample() function has created the rows by putting NaN values as new values for dates other than day 01. \n\nNext we can interpolate the missing  values at this new frequency. The function, interpolate() of pandas library is used to interpolate the missing values. \nWe use a linear interpolation which draws a straight line between available data, on the first day of the month and fills in values at the chosen frequency from this line. ","metadata":{}},{"cell_type":"code","source":"interpolated = upsampled_ts.interpolate(method = 'linear')\ninterpolated.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Another common interpolation**\n\n* Another common interpolation method is to use a polynomial or a spline to connect the values.\nThis creates more curves and look more natural on many datasets.\n* Using a spline interpolation requires you specify the order (count of terms in the polynomial); we use 2.","metadata":{}},{"cell_type":"code","source":"interpolated1 = upsampled_ts.interpolate(method = 'spline', order = 2)\ninterpolated1.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(interpolated1.head(12))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Down-sampling Frequency**\n\n* The sales data is monthly, but we prefer the data to be quarterly. The year can be divided into 4 business quarters, 3 months a piece. \n* The resample() function will group all observations by the new frequency.\n* We need to decide how to create a new quarterly value from each group of 3 records. We shall use the mean() function to calculate the average monthly sales numbers for the quarter","metadata":{}},{"cell_type":"code","source":"resample = shampoo_df.resample('Q')\nquarterly_mean_sales = resample.mean()\nprint(quarterly_mean_sales.head())\nquarterly_mean_sales.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example \nWe can turn monthly data into yearly data. Down-sample the data using the alias, A for year-end frequency and this time use sum to calculate the total sales each year.","metadata":{}},{"cell_type":"code","source":"resample = shampoo_df.resample('A')\nyearly_mean_sales = resample.sum()\nprint(yearly_mean_sales.head() )\nyearly_mean_sales.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Outliers**\nData may contain corrupt or extreme outlier values that need to be identified and handled.\n\n####  Detection of outliers in time series is difficult.\n* If a trend is present in the data, then usual method of detecting outliers by boxplot may not work.\n* If seasonality is present in the data, one particular season's data may be too small or too large compared to others.\n\n#### Decomposition helps in identifying unsual observations\n\n* If trend and seasonality are not adequate to explain the observation\n\n#### Outliers cannot be eliminated - they need to be imputed as closely as possible by using the knowledge gained from decomposition.","metadata":{}},{"cell_type":"markdown","source":"### Types of Trends\n\n* Deterministic Trends: They consistently increase or decrease and are easier to identify.\n* Stochastic Trends: They increase and decrease inconsistently \n\n#### Detrend a time series is by differencing","metadata":{}},{"cell_type":"code","source":"def parser(x): \n    return datetime.strptime('190'+x, '%Y-%m')\n\nshampoo_df=pd.read_csv(\"/kaggle/input/time-series-data/shampoo.csv\",header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\n\nX = shampoo_df.values \ndiff = list() \nfor i in range(1, len(X)): \n     value = X[i] - X[i - 1] \n     diff.append(value) \nplt.plot(diff) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\n\nWe don't see any particular trend in the data.","metadata":{}},{"cell_type":"markdown","source":"An identified trend can be modeled. Once modeled, it can be removed from the time series dataset. \n\n#### Detrend by model fitting","metadata":{}},{"cell_type":"markdown","source":"We will use Shampoo dataset.\n\n* A linear model can be fit on the time index to predict the observation. \n* Get a trend line from the predictions from this model.\n* Subtract these predictions from the original time series to provide a detrended version of the dataset.\n\nWe will use a scikit-learn LinearRegression model to train the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \n\n# fit linear model \nX = [i for i in range(0, len(shampoo_df))] \nX = np.reshape(X, (len(X), 1))\ny = shampoo_df.values \nmodel = LinearRegression() \nmodel.fit(X, y) \n\n# calculate trend \ntrend = model.predict(X) \n\n# plot trend \nplt.plot(y) \nplt.plot(trend) \nplt.show() \n\n# detrend \ndetrended = [y[i]-trend[i] for i in range(0, len(shampoo_df))] \n\n# plot detrended \nplt.plot(detrended) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\n\nWe have plotted the trend line in orange colour over the original dataset in blue colour.","metadata":{}},{"cell_type":"markdown","source":"## Seasonal variation may be present in Time series data.\n\n* Seasonal variation, or seasonality, are cycles that repeat regularly over time.\n\n* By plotting and reviewing the data, you can determine if there is any seasonality in the data.\n* We can try with different scales and by adding a trend line.\n* Once the seasonality is identified, it can be modeled. When you remove the model of seasonality from the time series, it is called deseasonalizing or seasonal adjustment.\n\n** Seasonal adjustment with differencing**\n\nWe can test the seasonality differencing method on the daily minimum temperature data.","metadata":{}},{"cell_type":"code","source":"# deseasonalize monthly data by differencing \n\nmin_temperature = pd.read_csv('/kaggle/input/time-series-data/daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\nresample       = min_temperature.resample('M') \nmonthly_mean   = resample.mean() \n\nX = min_temperature.values \ndiff = list() \nmonths_in_year = 12 \n\nfor i in range(months_in_year, len(monthly_mean)): \n    value = monthly_mean[i] - monthly_mean[i - months_in_year] \n    diff.append(value) \n\nplt.plot(diff) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy measures","metadata":{}},{"cell_type":"markdown","source":"We would have used several models such as moving average, exponential smoothing, etc. before selecting the best model. \n\nThe model selection may depend on the chosen forecasting accuracy measure such as:","metadata":{}},{"cell_type":"markdown","source":"* Mean Absolute Error,  MAE = (1/n) (|Y1 - F1| + |Y2- F2| + ... + |Yn - Fn|)\n* Mean Absolute Percentage Error,  MAPE = (1/n) ((|Y1 - F1|/Y1) + (|Y2 - F2|/Y2) + ... + (|Yn- Fn|/Yn) * 100)\n* Mean Squared Error, MSE =  (1/n) ((Y1 - F1)^2 + (Y2- F2)^2 + ... + (Yn - Fn)^2)\n* Root Mean Square Error, RMSE = square root of MSE\n\nwhere n is the number of observations\nYn is the actual value of Y at time n\nFn is the corresponding forecasted value.\nRMSE and MAPE are two most popular accuracy measures of forecasting.","metadata":{}},{"cell_type":"markdown","source":"### Example \n\nLet us take the Daily Female Births Dataset as an example. \n\nThis dataset describes the number of daily female births in California in 1959.\n\nFit a moving average of window width 3 and evalue the model measures such as RMSE and MAPE.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import  mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define functions to calculate MAE  and MAPE ","metadata":{}},{"cell_type":"code","source":"def MAE(y,yhat):\n    diff = np.abs(np.array(y)-np.array(yhat))\n    try:\n        mae =  round(np.mean(np.fabs(diff)),3)\n    except:\n        print(\"Error while calculating\")\n        mae = np.nan\n    return mae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MAPE(y, yhat): \n    y, yhat = np.array(y), np.array(yhat)\n    try:\n        mape =  round(np.mean(np.abs((y - yhat) / y)) * 100,2)\n    except:\n        print(\"Observed values are empty\")\n        mape = np.nan\n    return mape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"female_birth_series =  pd.read_csv('/kaggle/input/time-series-data/daily-total-female-births.csv', header=0, index_col=0, parse_dates=True, squeeze=True) \n\n# tail rolling average transform\nrolling =  female_birth_series.rolling(window = 3) # arbitrarily chosen\n\nrolling_mean = rolling.mean()\nfemale_birth_series.plot()\n\nrolling_mean.plot(color = 'red')\nplt.show()\n\n# Zoomed plot original and transformed dataset\nfemale_birth_series[:100].plot()\nrolling_mean[:100].plot(color = 'red')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_df = pd.DataFrame( {'Observed':female_birth_series.values, 'Predicted':rolling_mean})\ny_df .dropna(axis = 0, inplace = True)\nprint(y_df.tail())\n\nrmse = np.sqrt(mean_squared_error(y_df.Observed, y_df.Predicted))\nprint(\"\\n\\n Accuracy measures \")\nprint('RMSE: %.3f' % rmse)\nn = y_df.shape[0]\n\nmae = MAE(y_df.Observed, y_df.Predicted)\nprint('MAE: %d' % np.float(mae))\n\nmape = MAPE(y_df.Observed, y_df.Predicted)\nprint('MAPE: %.3f' % np.float(mape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can try it with the window width 2 and 4 and check whether the accuracy has increased or not.You can  Use Champagne Sales data and observe seasonality using visualization techniques.\n\nAlso apply a moving average model for Champagne Sales data after decomposing with various window widths and compare the performance measures such as RMSE and MAPE.","metadata":{}},{"cell_type":"markdown","source":"# **14.ETS Models** <a class=\"anchor\" id=\"14\"></a>\n\n[Table of Contents](#0.1)\n\n\nThe ETS model is a time series univariate forecasting method; its use focuses on trend and seasonal components.It is a time series forecasting method for univariate data.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### **14.1SES, Holt & Holt-Winter Model** <a class=\"anchor\" id=\"14.1\"></a>\n\n[Table of Contents](#0.1)\n\n### Exponential Smoothing methods\n#####  Exponential smoothing methods consist of flattening time series data. \n##### Exponential smoothing averages or exponentially weighted moving averages consist of forecast based on previous periods data with exponentially declining influence on the older observations.\n##### Exponential smoothing methods consist of special case exponential moving with notation ETS (Error, Trend, Seasonality) where each can be none(N), additive (N), additive damped (Ad), Multiplicative (M) or multiplicative damped (Md).\n##### One or more parameters control how fast the weights decay.\n##### These parameters have values between 0 and 1\n\n","metadata":{}},{"cell_type":"markdown","source":"### 14.1.1SES -  ETS(A, N, N) - Simple smoothing with additive errors <a class=\"anchor\" id=\"14.1.1\"></a>\n\n###### The simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). \n###### This method is suitable for forecasting data with no clear trend or seasonal pattern.\n\nIn Single ES, the forecast at time (t + 1) is given by Winters,1960\n\n* $F_{t+1} = \\alpha Y_t + (1-\\alpha) F_t$\n\nParameter $\\alpha$ is called the smoothing constant and its value lies between 0 and 1.\nSince the model uses only one smoothing constant, it is called Single Exponential Smoothing.","metadata":{}},{"cell_type":"markdown","source":"We have the petrol data from Jan 2001 to Sep 2013.\n* Split the data into train and test in the ratio 70:30\n* Use Single Exponential Smoothing method to forecast sales using the test data.\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","metadata":{"trusted":true}},{"cell_type":"code","source":"import statsmodels.tsa.holtwinters     as      ets\nimport statsmodels.tools.eval_measures as      fa\nfrom   sklearn.metrics                 import  mean_squared_error\nfrom   statsmodels.tsa.holtwinters     import  SimpleExpSmoothing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MAPE(y, yhat): \n    y, yhat = np.array(y), np.array(yhat)\n    try:\n        mape =  round(np.sum(np.abs(yhat - y)) / np.sum(y) * 100,2)\n    except:\n        print(\"Observed values are empty\")\n        mape = np.nan\n    return mape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"petrol_df =  pd.read_csv('/kaggle/input/time-series-data/Petrol.csv')\ndate_rng  =  pd.date_range(start='1/1/2001', end='30/9/2013', freq='Q')\nprint(date_rng)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"petrol_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Quarter'])\nprint(petrol_df.head(3).T)\n\nplt.plot(petrol_df.TimeIndex, petrol_df.Consumption)\nplt.title('Original data before split')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating train and test set \n\ntrain = petrol_df[0:int(len(petrol_df)*0.7)] \ntest= petrol_df[int(len(petrol_df)*0.7):]\n\nprint(\"\\n Training data start at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.min()],['Year','Quarter'])\nprint(\"\\n Training data ends at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.max()],['Year','Quarter'])\n\nprint(\"\\n Test data start at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.min()],['Year','Quarter'])\n\nprint(\"\\n Test data ends at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.max()],['Year','Quarter'])\n\nplt.plot(train.TimeIndex, train.Consumption, label = 'Train')\nplt.plot(test.TimeIndex, test.Consumption,  label = 'Test')\nplt.legend(loc = 'best')\nplt.title('Original data after split')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*SimpleExpSmoothing* class must be instantiated and passed the training data. \n\nThe fit() function is then called providing the fit configuration, the alpha value, smoothing_level. \nIf this is omitted or set to None, the model will automatically optimize the value.\n\nWe will try with various values of $\\alpha$ such as 0.1, 0.5 and 0.99 and then let the model optimize $\\alpha$.\n\nA value close to 1 indicates fast learning (that is, only the most recent values influence the forecasts), whereas a value close to 0 indicates slow learning (past observations have a large influence on forecasts).\n\n— Page 89, Practical Time Series Forecasting with R, 2016.\n\nhttps://www.amazon.com/Practical-Time-Forecasting-Hands-Analytics/dp/0997847913/ref=as_li_ss_tl?ie=UTF8&qid=1527636709&sr=8-5&keywords=time+series+forecasting&linkCode=sl1&tag=inspiredalgor-20&linkId=dcb38fa9efe5af617b48b2922c4c149f","metadata":{}},{"cell_type":"code","source":"# create class\nmodel = SimpleExpSmoothing(np.asarray(train['Consumption']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit model\n\nalpha_list = [0.1, 0.5, 0.99]\n\npred_SES = test.copy() # Have a copy of the test dataset\n\nfor alpha_value in alpha_list:\n\n    alpha_str =  \"SES\" + str(alpha_value)\n    mode_fit_i  =  model.fit(smoothing_level = alpha_value, optimized=False)\n    pred_SES[alpha_str]  =  mode_fit_i.forecast(len(test['Consumption']))\n    rmse =  np.sqrt(mean_squared_error(test['Consumption'], pred_SES[alpha_str]))\n    mape =  MAPE(test['Consumption'],pred_SES[alpha_str])\n###\n    print(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse, mape))\n    plt.figure(figsize=(16,8))\n    plt.plot(train.TimeIndex, train['Consumption'], label ='Train')\n    plt.plot(test.TimeIndex, test['Consumption'], label  ='Test')\n    plt.plot(test.TimeIndex, pred_SES[alpha_str], label  = alpha_str)\n    plt.title('Simple Exponential Smoothing with alpha ' + str(alpha_value))\n    plt.legend(loc='best') \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us get the optimum value for $\\alpha$ by omitting the value and leave it for the model to decide.","metadata":{}},{"cell_type":"code","source":"pred_opt   =  SimpleExpSmoothing(train['Consumption']).fit(optimized = True)\nprint('')\nprint('== Simple Exponential Smoothing ')\nprint('')\n\nprint('')\nprint('Smoothing Level', np.round(pred_opt.params['smoothing_level'], 4))\nprint('Initial Level',   np.round(pred_opt.params['initial_level'], 4))\nprint('')\n\ny_pred_opt           = pred_opt.forecast(steps = 16)\ndf_pred_opt          = pd.DataFrame({'Y_hat':y_pred_opt,'Y':test['Consumption'].values})\n\nrmse_opt             =  np.sqrt(mean_squared_error(test['Consumption'], y_pred_opt))\nmape_opt             =  MAPE(test['Consumption'], y_pred_opt)\n\nalpha_value          = np.round(pred_opt.params['smoothing_level'], 4)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))\n\nplt.figure(figsize=(16,8))\nplt.plot(train.TimeIndex, train['Consumption'], label = 'Train')\nplt.plot(test.TimeIndex, test['Consumption'],  label = 'Test')\nplt.plot(test.TimeIndex, y_pred_opt,           label = 'SES_OPT')\nplt.title('Simple Exponential Smoothing with alpha ' + str(alpha_value))\nplt.legend(loc='best') \nplt.show()\n\nprint(df_pred_opt.head().T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nWe observe that for the optimum $\\alpha$ value, both RMSE and MAPE are smallest when compared to other $\\alpha$ values of 0.1,0.5 and 0.99.","metadata":{}},{"cell_type":"markdown","source":"You can also try in this manner \n\n* Change the Split ratio as 75:25.\n* Use Single Exponential Smoothing method to forecast sales for the test data using the optimum smoothing level.\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","metadata":{}},{"cell_type":"markdown","source":"## 14.1.2Holt - ETS(A, A, N) - Holt's linear method with additive errors <a class=\"anchor\" id=\"14.1.2\"></a>\n\n[Table of Contents](#0.1)\n\n## Double Exponential Smoothing\n\n* One of the drawbacks of the single exponential smoothing is that the model does not do well in the presence of the trend.\n* This model is an extension of SES and also known as Double Exponential model\n* Applicable when data has Trend but no seasonality\n* Two separate components are considered: Level and Trend\n* Level is the local mean\n* One smoothing parameter α corresponds to the level series\n* A second smoothing parameter β corresponds to the trend series.","metadata":{}},{"cell_type":"markdown","source":"**Double Exponential Smoothing uses two equations to forecast future values of the time series, one for forecating the short term avarage value or level and the other for capturing the trend.**\n\n* Intercept or Level equation, $L_t$ is given by:\n$L_t = {\\alpha}{Y_t}  + (1 - \\alpha)F_t$ \n\n* Trend equation is given by \n$T_t = {\\beta}{(L_t - L_{t-1})}  + (1 - \\beta)T_{t-1}$ \n\nHere, $\\alpha$ and $\\beta$ are the smoothing constants for level and trend, respectively, \n* 0 <$\\alpha$ < 1 and 0 < $\\beta$ < 1.\n\nThe forecast at time t + 1 is given by\n* $F_{t+1} = L_t + T_t$\n* $F_{t+n} = L_t + nT_t$","metadata":{}},{"cell_type":"markdown","source":"Consider the following example \n\n* Use the data in example 2 and use Double Exponential Smoothing method to forecast sales for the test data\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","metadata":{}},{"cell_type":"code","source":"from   statsmodels.tsa.holtwinters import  Holt\nmodel = Holt(np.asarray(train['Consumption']))\n\nmodel_fit = model.fit()\n\nprint('')\nprint('==Holt model Exponential Smoothing Parameters ==')\nprint('')\nalpha_value = np.round(model_fit.params['smoothing_level'], 4)\nprint('Smoothing Level', alpha_value )\nprint('Smoothing Slope', np.round(model_fit.params['smoothing_slope'], 4))\nprint('Initial Level',   np.round(model_fit.params['initial_level'], 4))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Pred_Holt = test.copy()\n\nPred_Holt['Opt'] = model_fit.forecast(len(test['Consumption']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(train['Consumption'], label='Train')\nplt.plot(test['Consumption'], label='Test')\nplt.plot(Pred_Holt['Opt'], label='HoltOpt')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred_opt =  pd.DataFrame({'Y_hat':Pred_Holt['Opt'] ,'Y':test['Consumption'].values})\nrmse_opt =  np.sqrt(mean_squared_error(df_pred_opt.Y, df_pred_opt.Y_hat))\nmape_opt =  MAPE(df_pred_opt.Y, df_pred_opt.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Print the model parameters","metadata":{}},{"cell_type":"code","source":"print(model_fit.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nFor the same pertol data, we have tried both SES and HOLT model.\n\n| Model | RMSE | MAPE |\n| ----- | ------ | ----- |\n| SES |  0.9386  | 83.39 |\n| Holt | 0.8283 | 63.01 |  \n\nNote the decrease in both RMSE and MAPE for the HOLT model.","metadata":{}},{"cell_type":"markdown","source":"You can also use the above example with the following \n\n* Make the changes to model fit parameters as follows:\n\n* smoothing_level = 0.3\n* smoothing_slope = 0.1\n* optimized       = False\n\n* Build the model using the training data (Same split as in example 3)\n* Evaluate the model performance by calculating the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.\n* Is there any improvement or degradation in predicting the values?","metadata":{}},{"cell_type":"markdown","source":"### 14.1.3Holt-Winters - ETS(A, A, A) - Holt Winter's linear method with additive errors     <a class=\"anchor\" id=\"14.1.3\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### Multi-Steps Forecast","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\npred1 = ExponentialSmoothing(np.asarray(train['Consumption']), trend='additive', damped=False, seasonal='additive',\n                                  seasonal_periods = 12).fit() #[:'2017-01-01']\nprint('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred1.params['smoothing_level'], 4)\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', np.round(pred1.params['smoothing_slope'], 4))\nprint('Smoothing Seasonal: ', np.round(pred1.params['smoothing_seasonal'], 4))\nprint('Initial Level: ', np.round(pred1.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(pred1.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(pred1.params['initial_seasons'], 4))\nprint('')\n\n### Forecast for next 16 months\n\ny_pred1 =  pred1.forecast(steps = 16)\ndf_pred1 = pd.DataFrame({'Y_hat':y_pred1,'Y':test['Consumption']})\nprint(df_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Plot\n\nfig2, ax = plt.subplots()\nax.plot(df_pred1.Y, label='Original')\nax.plot(df_pred1.Y_hat, label='Predicted')\n\nplt.legend(loc='upper left')\nplt.title('Holt-Winters Additive ETS(A,A,A) Method 1')\nplt.ylabel('Qty')\nplt.xlabel('Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse    =  np.sqrt(mean_squared_error(df_pred1.Y, df_pred1.Y_hat))\nmape    =  MAPE(df_pred1.Y, df_pred1.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse, mape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pred1.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that MAPE for Holt-Winters Additive ETS(A,A,A) Method is 128.21 and it is very much higher than (63.01) ETS(A, A, N) - Holt's Model.\n\nWe will try with  Air Passengers data.","metadata":{}},{"cell_type":"code","source":"AirPax =  pd.read_csv('/kaggle/input/time-series-data/AirPax.csv')\ndate_rng = pd.date_range(start='1/1/1949', end='31/12/1960', freq='M')\nprint(date_rng)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AirPax['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])\nprint(AirPax.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating train and test set \n\ntrain = AirPax[0:int(len(AirPax)*0.7)] \ntest= AirPax[int(len(AirPax)*0.7):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Training data start at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.min()],['Year','Month'])\nprint(\"\\n Training data ends at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.max()],['Year','Month'])\n\nprint(\"\\n Test data start at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.min()],['Year','Month'])\n\nprint(\"\\n Test data ends at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.max()],['Year','Month'])\n\nplt.plot(train.TimeIndex, train.Passenger, label = 'Train')\nplt.plot(test.TimeIndex, test.Passenger,  label = 'Test')\nplt.legend(loc = 'best')\nplt.title('Original data after split')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = ExponentialSmoothing(np.asarray(train['Passenger']),\n                                  seasonal_periods=12 ,seasonal='add').fit(optimized=True)\n\nprint(pred.params)\n\nprint('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', np.round(pred.params['smoothing_slope'], 4))\nprint('Smoothing Seasonal: ', np.round(pred.params['smoothing_seasonal'], 4))\nprint('Initial Level: ', np.round(pred.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(pred.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(pred.params['initial_seasons'], 4))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_HoltW = test.copy()\npred_HoltW['HoltW'] = model_fit.forecast(len(test['Passenger']))\nplt.figure(figsize=(16,8))\nplt.plot(train['Passenger'], label='Train')\nplt.plot(test['Passenger'], label='Test')\nplt.plot(pred_HoltW['HoltW'], label='HoltWinters')\nplt.title('Holt-Winters Additive ETS(A,A,A) Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(np.round(pred.params['smoothing_slope'], 4)) +\n          '  Gamma: ' + str(np.round(pred.params['smoothing_seasonal'], 4)))\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred_opt =  pd.DataFrame({'Y_hat':pred_HoltW['HoltW'] ,'Y':test['Passenger'].values})\n\nrmse_opt    =  np.sqrt(mean_squared_error(df_pred_opt.Y, df_pred_opt.Y_hat))\nmape_opt    =  MAPE(df_pred_opt.Y, df_pred_opt.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14.1.4 Holt-Winters - ETS(A, A, M) - Holt Winter's linear method <a class=\"anchor\" id=\"14.1.4\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### Multi-Steps Forecast","metadata":{}},{"cell_type":"markdown","source":"### Example \n\nUse Air Passengers data and fit ETS(A, A, M)  model to predict the last 12 months.\n* Build the Holt Winter's lETS(A, A, M) model to forecast sales for the test data\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","metadata":{}},{"cell_type":"code","source":"pred = ExponentialSmoothing(np.asarray(train['Passenger']),\n                                  seasonal_periods=12 ,seasonal='multiplicative').fit(optimized=True)\n\nprint(pred.params)\n\nprint('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', np.round(pred.params['smoothing_slope'], 4))\nprint('Smoothing Seasonal: ', np.round(pred.params['smoothing_seasonal'], 4))\nprint('Initial Level: ', np.round(pred.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(pred.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(pred.params['initial_seasons'], 4))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_HoltW = test.copy()\n\npred_HoltW['HoltWM'] = pred.forecast(len(test['Passenger']))\nplt.figure(figsize=(16,8))\nplt.plot(train['Passenger'], label='Train')\nplt.plot(test['Passenger'], label='Test')\nplt.plot(pred_HoltW['HoltWM'], label='HoltWinters')\nplt.title('Holt-Winters Multiplicative ETS(A,A,M) Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(np.round(pred.params['smoothing_slope'], 4)) +\n          '  Gamma: ' + str(np.round(pred.params['smoothing_seasonal'], 4)))\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Report model accuracy","metadata":{}},{"cell_type":"code","source":"df_pred_opt =  pd.DataFrame({'Y_hat':pred_HoltW['HoltWM'] ,'Y':test['Passenger'].values})\n\nrmse_opt    =  np.sqrt(mean_squared_error(df_pred_opt.Y, df_pred_opt.Y_hat))\nmape_opt    =  MAPE(df_pred_opt.Y, df_pred_opt.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\n\n**MAPE of ETS(A,A,M) model is 16.92 and it is lesser than 17.47, MAPE of  ETS(A,A,A) model, **\n\nHence, prediction of Holt-Winter - Additive Trend and Multiplicative Seasonality model is better than Holt-Winter - Additive model","metadata":{}},{"cell_type":"markdown","source":"## 14.2Model finalization <a class=\"anchor\" id=\"14.2\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"We are going to use the shampoo dataset. This dataset describes the monthly number of sales of shampoo over a 3 year period.\nThe units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright and Hyndman (1998).\n\nThe entire dataset is taken from Data Market.\n\nhttps://datamarket.com/data/set/22r0/sales-of-shampoo-over-a-three-year-period\n\n### Example \n\n* Split the data into train and test. \n* Build different time series models on train data set and test it on test data set\n* Compare models performance.","metadata":{}},{"cell_type":"code","source":"#Importing data\ndef parser(x):\n    return pd.datetime.strptime('190'+x, '%Y-%m')\n \nshampoo_df = pd.read_csv('/kaggle/input/time-series-data/shampoo.csv', header=0, parse_dates=True, squeeze=True, date_parser=parser)\n\nprint(shampoo_df.head())\nshampoo_df.plot()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating train and test set\n\ntrain    =   shampoo_df[0:int(len(shampoo_df)*0.7)] \ntest     =   shampoo_df[int(len(shampoo_df)*0.7):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Plot data\n\ntrain['Sales'].plot(figsize=(15,8), title= 'Monthly Sales', fontsize=14)\ntest['Sales'].plot(figsize=(15,8), title= 'Monthly Sales', fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shampoo_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14.2.1Method  1: Regression on Time <a class=\"anchor\" id=\"14.2.1\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"shampoo_df1         =   shampoo_df.copy() # Make a copy\n\ntime        = [i+1 for i in range(len(shampoo_df))]\nshampoo_df1['time'] = time\nmonthDf     = shampoo_df1[['Month']]\n\nshampoo_df1.drop('Month', axis=1, inplace=True)\nshampoo_df1.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating train and test set \ntrain=shampoo_df1[0:int(len(shampoo_df1)*0.7)] \ntest=shampoo_df1[int(len(shampoo_df1)*0.7):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train.drop('Sales', axis=1)\nx_test  = test.drop('Sales', axis=1)\ny_train = train[['Sales']]\ny_test  = test[['Sales']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions         = model.predict(x_test)\ny_test['RegOnTime'] = predictions\n\nplt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_test['RegOnTime'], label='Regression On Time')\nplt.legend(loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import sqrt\nrmse = sqrt(mean_squared_error(test.Sales, y_test.RegOnTime))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_test.RegOnTime)\nprint(\"For RegressionOnTime,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultsDf = pd.DataFrame({'Method':['RegressionOnTime'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14.2.2Method 2: Regression on Time With Seasonal Components <a class=\"anchor\" id=\"14.2.2\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"time = [i+1 for i in range(len(shampoo_df))]\nshampoo_df1 = shampoo_df.copy()\nshampoo_df1['time'] = time\nprint(shampoo_df1.head())\nprint(shampoo_df1.shape[0])\nmonthSeasonality = ['m1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shampoo_df1['monthSeasonality'] = monthSeasonality * 3\nshampoo_df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthDf = shampoo_df1[['Month']]\nshampoo_df1.drop('Month', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shampoo_df1Complete = pd.get_dummies(shampoo_df1, drop_first=True)\nshampoo_df1Complete.head(2).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating train and test set \ntrain=shampoo_df1Complete[0:int(len(shampoo_df1Complete)*0.7)] \ntest=shampoo_df1Complete[int(len(shampoo_df1Complete)*0.7):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train  = train.drop('Sales', axis=1)\nx_test   = test.drop('Sales', axis=1)\ny_train  = train[['Sales']]\ny_test   = test[['Sales']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression()\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\ny_test['RegOnTimeSeasonal'] = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_test['RegOnTimeSeasonal'], label='Regression On Time With Seasonal Components')\nplt.legend(loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse = sqrt(mean_squared_error(test.Sales, y_test.RegOnTimeSeasonal))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_test.RegOnTimeSeasonal)\nprint(\"For RegOnTimeSeasonal,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['RegressionOnTimeSeasonal'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14.2.3Method 3: Naive Approach: $\\hat{y}_{t+1} = y_t$ <a class=\"anchor\" id=\"14.2.3\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"dd= np.asarray(train.Sales)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat = test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat['naive'] = dd[len(dd)-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(train.index, train['Sales'], label='Train')\nplt.plot(test.index,test['Sales'], label='Test')\nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast')\nplt.legend(loc='best')\nplt.title(\"Naive Forecast\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse = sqrt(mean_squared_error(test.Sales, y_hat.naive))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_hat.naive)\nprint(\"For Naive model,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Naive model'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can infer from the RMSE and MAPE values and the graphs above, that Naive method and Regression on Time With Seasonal Components model are not suited for datasets with high variability. \n\nNaive method is best suited for stable datasets. We can still improve our score by adopting different techniques. \n\nNow we will look at another technique and try to improve our score.","metadata":{}},{"cell_type":"markdown","source":"### 14.2.4Method 4: Simple Average <a class=\"anchor\" id=\"14.2.4\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"y_hat_avg = test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_avg['avg_forecast'] = train['Sales'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Average Forecast')\nplt.legend(loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse = sqrt(mean_squared_error(test.Sales, y_hat_avg.avg_forecast))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_hat_avg.avg_forecast)\nprint(\"For Simple Average model,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Simple Average'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\n\nWe can see that this model didn’t improve our score. Hence we can infer from the score that this method works best when the average at each time period remains constant. Though the score of Naive method is better than Average method, but this does not mean that the Naive method is better than Average method on all datasets. We should move step by step to each model and confirm whether it improves our model or not.","metadata":{}},{"cell_type":"markdown","source":"### 14.2.5Method 5: Moving Average(MA) <a class=\"anchor\" id=\"14.2.5\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"shampoo_df1 = shampoo_df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shampoo_df1['moving_avg_forecast_4']  = shampoo_df['Sales'].rolling(4).mean()\nshampoo_df1['moving_avg_forecast_6']  = shampoo_df['Sales'].rolling(6).mean()\nshampoo_df1['moving_avg_forecast_8']  = shampoo_df['Sales'].rolling(8).mean()\nshampoo_df1['moving_avg_forecast_12'] = shampoo_df['Sales'].rolling(12).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['moving_avg_forecast_4','moving_avg_forecast_6','moving_avg_forecast_8','moving_avg_forecast_12']\n\n#Creating train and test set \ntrain=shampoo_df1[0:int(len(shampoo_df1)*0.7)] \ntest=shampoo_df1[int(len(shampoo_df1)*0.7):]\n\ny_hat_avg = test.copy()\n\nfor col_name in cols:\n    \n    plt.figure(figsize=(16,8))\n    plt.plot(train['Sales'], label='Train')\n    plt.plot(test['Sales'], label='Test')\n    plt.plot(y_hat_avg[col_name], label = col_name)\n    plt.legend(loc = 'best')\n\n    rmse = sqrt(mean_squared_error(test.Sales, y_hat_avg[col_name]))\n    rmse = round(rmse, 3)\n    mape = MAPE(test.Sales, y_hat_avg[col_name])\n    print(\"For Simple Average model, %s  RMSE is %3.3f MAPE is %3.2f\" %(col_name, rmse, mape))\n\n    tempResultsDf = pd.DataFrame({'Method':[col_name], 'rmse': [rmse], 'mape' : [mape]})\n    resultsDf = pd.concat([resultsDf, tempResultsDf])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(resultsDf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far, the moving average of window width 4 gives the lowest MAPE and RMSE.\nLet us try other models as well.","metadata":{}},{"cell_type":"markdown","source":"### 14.2.6Method 6: Simple Exponential Smoothing <a class=\"anchor\" id=\"14.2.6\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create class\nmodel = SimpleExpSmoothing(train['Sales'])\nmodel_fit = model.fit(optimized = True)\nprint('')\nprint('== Simple Exponential Smoothing ')\nprint('')\n\nprint('')\nprint('Smoothing Level', np.round(model_fit.params['smoothing_level'], 4))\nprint('Initial Level',   np.round(model_fit.params['initial_level'], 4))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_avg['SES']= model_fit.forecast(len(test['Sales']))\n\nalpha_value = np.round(model_fit.params['smoothing_level'], 4)\n\n\nplt.figure(figsize=(16,8))\nplt.plot(train.index, train['Sales'], label = 'Train')\nplt.plot(test.index, test['Sales'],   label = 'Test')\nplt.plot(test.index, y_hat_avg.SES,   label = 'SES_OPT')\nplt.title('Simple Exponential Smoothing with alpha ' + str(alpha_value))\nplt.legend(loc='best') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_opt=  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg.SES))\nmape_opt=  MAPE(test['Sales'], y_hat_avg.SES)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method': 'SES', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(resultsDf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14.2.7Method 7: Holt's Linear Trend Method (Double Exponential Smoothing) <a class=\"anchor\" id=\"14.2.7\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\ny_hat_avg = test.copy()\nmodel_fit = Holt(np.asarray(train['Sales'])).fit()\ny_hat_avg['Holt_linear'] = model_fit.forecast(len(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('')\nprint('==Holt model Exponential Smoothing Parameters ==')\nprint('')\nalpha_value = np.round(model_fit.params['smoothing_level'], 4)\nbeta_value  = np.round(model_fit.params['smoothing_slope'], 4)\n\nprint('Smoothing Level', alpha_value )\nprint('Smoothing Slope', beta_value)\nprint('Initial Level',   np.round(model_fit.params['initial_level'], 4))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_opt             =  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['Holt_linear']))\nmape_opt             =  MAPE(test['Sales'], y_hat_avg['Holt_linear'])\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method': 'Holt_linear', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nprint(resultsDf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nThe model is slightly better than SES model but worse than all moving average models.","metadata":{}},{"cell_type":"markdown","source":"### 14.2.8Method 8: Holt-Winters Method - Additive seasonality <a class=\"anchor\" id=\"14.2.8\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"y_hat_avg = test.copy()\nmodel_fit = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods = 12 ,trend='add', seasonal='add').fit()\ny_hat_avg['Holt_Winter'] = model_fit.forecast(len(test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nbeta_value  = np.round(model_fit.params['smoothing_slope'], 4)\ngamma_value = np.round(model_fit.params['smoothing_seasonal'], 4) \n\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', beta_value)\nprint('Smoothing Seasonal: ', gamma_value)\nprint('Initial Level: ', np.round(model_fit.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(model_fit.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(model_fit.params['initial_seasons'], 4))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.title('Holt-Winters Multiplicative ETS(A,A,M) Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(beta_value) +\n          '  Gamma: ' + str(gamma_value))\nplt.legend(loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_opt=  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['Holt_Winter']))\nmape_opt=  MAPE(test['Sales'], y_hat_avg['Holt_Winter'])\n\nprint(\"For alpha = %1.2f, beta = %1.2f, gamma = %1.2f, RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, beta_value, gamma_value, rmse_opt, mape_opt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method': 'Holt_Winter', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nprint(resultsDf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Beta is the smoothing factor to trends and it is zero and this gives more weight to the old  trend. \n* Gamma is the smoothing factor to seasonal index and is it zero and this gives, more weight  to old seasonal periods. ","metadata":{}},{"cell_type":"markdown","source":"### 14.2.9Method 9: Holt-Winters Method - Multiplicative Model <a class=\"anchor\" id=\"14.2.9\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"y_hat_avg = test.copy()\nmodel_fit = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods = 12 ,trend='add', seasonal='Multiplicative').fit()\ny_hat_avg['Holt_Winter_M'] = model_fit.forecast(len(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('')\nprint('== Holt-Winters Additive ETS(A,A,M) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nbeta_value  = np.round(model_fit.params['smoothing_slope'], 4)\ngamma_value = np.round(model_fit.params['smoothing_seasonal'], 4) \n\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', beta_value)\nprint('Smoothing Seasonal: ', gamma_value)\nprint('Initial Level: ', np.round(model_fit.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(model_fit.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(model_fit.params['initial_seasons'], 4))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter_M'], label='Holt_Winter_M')\nplt.title('Holt-Winters Multiplicative  Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(beta_value) +\n          '  Gamma: ' + str(gamma_value))\nplt.legend(loc='best')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_opt=  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['Holt_Winter_M']))\nmape_opt=  MAPE(test['Sales'], y_hat_avg['Holt_Winter_M'])\n\nprint(\"For alpha = %1.2f, beta = %1.2f, gamma = %1.2f, RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, beta_value, gamma_value, rmse_opt, mape_opt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method': 'Holt_Winter M', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nprint(resultsDf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\n**As of now, we observe that Moving average of window width of 4 seems to be a good fit for the data.**","metadata":{}},{"cell_type":"markdown","source":"# **15.AUTO REGRESSIVE Models** <a class=\"anchor\" id=\"15\"></a>\n\n[Table of Contents](#0.1)\n\nAuto-regressive (AR) and moving average (MA) models are popular models that are frequently used for forecasting.\n\nAR and MA models are combined to create models such as auto-regressive moving average (ARMA) and auto-regressive integrated moving average (ARIMA) models. \n\nThe initial ARMA and ARIMA models were developed by Box and Jenkins in 1970.\n\nARMA models are basically regression models; auto-regression means regression of a variable on itself measured at different time periods. \n\nThe main assumption of AR model is that the time series data is stationary.\n\nA stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time.\n\nhttp://people.duke.edu/~rnau/411diff.htm\n\nWhen the time series data is not stationary, then we convert the non-stationary data before applying AR models. \n\n### Lags\n\nTaking the difference between consecutive observations is called a lag-1 difference.\n\nFor time series with a seasonal component, the lag may be expected to be the period (width) of the seasonality.\n\n**White noise of the residuals:**\n\nWhite noise is a process of residuals $\\epsilon_t$ that are uncorrelated and follow normal distribution with mean 0 and constant standard deviation. In AR models, one of the main assumptions is the errors follow a white noise.\n\n### Auto-Regressive  (AR) Models\n\nAuto-Regression is a regression of a variable on itself measured at different time points. \nAuto-Regressive model with lag 1, AR(1) is given by \n* $Y_{t+1} = \\beta Y_t + \\epsilon_{t+1}$  and this same as\n* $Y_{t+1} - \\mu = \\beta (Y_t - \\mu) + \\epsilon_{t+1}$  and this same as\n* where $\\epsilon_{t+1}$ is a sequence of uncorrelated residuals that follow normal distribution with zero mean and constant deviation. \n * $Y_{t+1} - \\mu$ is interpreted as a deviation from mean value $mu$ and known as mean centered series.\n\n","metadata":{}},{"cell_type":"markdown","source":"The Augmented Dickey Fuller Test (ADF) is unit root test for stationarity. \nThe null hypothesis is that time series is non-stationary.\nAlternative hypothesis is that time series is stationary.","metadata":{}},{"cell_type":"markdown","source":"### AR Model indentification\n\n\n### Auto-Correlation Function (ACF) or correlogram and Partial Auto-Correlation Function (PACF)","metadata":{}},{"cell_type":"markdown","source":"#### Autocorrelation Function (ACF)\n\n**A plot of auto-correlation of different lags is called ACF.**\n\nThe plot summarizes the correlation of an observation with lag values. The x-axis shows the lag and the y-axis shows the correlation coeﬃcient between -1 and 1 for negative and positive correlation.\n\n#### Partial Autocorrelation Function (PACF)\n\n**A plot of partial auto-correlation for different values of lags is called PACF.**\n\nThe plot summarizes the correlations for an observation with lag values that is not accounted for by prior lagged observations.\n\nBoth plots are drawn as bar charts showing the 95% and 99% conﬁdence intervals as horizontal lines. Bars that cross these conﬁdence intervals are therefore more signiﬁcant and worth noting. Some useful patterns you may observe on these plots are:\n\nThe number of lags is p when:\n* The partial auto-correlation, |$\\rho_{pk}$| > 1.96 / $\\sqrt{n}$ for first p values and cuts off to zero. \n* The auto-correlation function, $\\rho_k$ decreases exponentially.","metadata":{}},{"cell_type":"markdown","source":"*  The model is AR if the ACF trails oﬀ after a lag and has a hard cut-oﬀ in the PACF after a lag.    This lag is taken as the value for p.\n\n*  The model is MA if the PACF trails oﬀ after a lag and has a hard cut-oﬀ in the ACF after the   lag. This lag value is taken as the value for q.\n\n*  The model is a mix of AR and MA if both the ACF and PACF trail oﬀ.\n\n* For an **ARIMA (p,d,q)** process, it becomes non-stationary to stationary after differencing it for **d** times.","metadata":{}},{"cell_type":"markdown","source":"### Plot ACF and PACF","metadata":{}},{"cell_type":"code","source":"from   statsmodels.tsa.stattools  import  adfuller\ndata= pd.read_csv('/kaggle/input/time-series-data/TractorSales.csv', header=0, parse_dates=[0], squeeze=True)\n\ndates= pd.date_range(start='2003-01-01', freq='MS', periods=len(data))\n\ndata['Month']= dates.month\ndata['Month']= data['Month'].apply(lambda x: calendar.month_abbr[x])\ndata['Year']= dates.year\n\ndata.drop(['Month-Year'], axis=1, inplace=True)\ndata.rename(columns={'Number of Tractor Sold':'Tractor-Sales'}, inplace=True)\n\ndata= data[['Month', 'Year', 'Tractor-Sales']]\ndata.set_index(dates, inplace=True)\n\nsales_ts = data['Tractor-Sales']\n\nresult = adfuller(sales_ts) \n\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This series is not stationary.\nTry differencing - lag 1","metadata":{}},{"cell_type":"code","source":"sales_ts_diff   = sales_ts - sales_ts.shift(periods=1)\nsales_ts_diff.dropna(inplace=True)\n\nresult = adfuller(sales_ts_diff) \n\npval              = result[1]\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) \n\nif pval < 0.05:\n    print('Data is stationary')\nelse:\n    print('Data after differencing is not stationary; so try log diff')\n    sales_ts_log      = np.log10(sales_ts)\n    sales_ts_log.dropna(inplace=True)\n    sales_ts_log_diff = sales_ts_log.diff(periods=1)\n    sales_ts_log_diff.dropna(inplace=True)\n    result            = adfuller(sales_ts_log_diff) \n\n    pval              = result[1]\n    print('ADF Statistic: %f' % result[0]) \n    print('p-value: %f' % result[1]) \n    if pval < 0.05:\n        print('Data after log differencing is stationary')\n    else:\n        print('Data after log differencing is not stationary; try second order differencing')\n        sales_ts_log_diff2 = sales_ts_log.diff(periods = 2)\n        sales_ts_log_diff2.dropna(inplace=True)\n        result         =   adfuller(sales_ts_log_diff2) \n        pval              = result[1]\n        print('ADF Statistic: %f' % result[0]) \n        print('p-value: %f' % result[1]) \n        if pval < 0.05:\n            print('Data after log differencing 2nd order is stationary')\n        else:\n            print('Data after log differencing 2nd order is not stationary')\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ACF and PACF plots:\nfrom   statsmodels.tsa.stattools import acf, pacf\nimport matplotlib.pyplot as plt\n\n\nlag_acf    =   acf(sales_ts_log_diff2,   nlags=20)\nlag_pacf   =   pacf(sales_ts_log_diff2, nlags=20, method='ols')\n\n#Plot ACF: \n\nplt.figure(figsize = (15,5))\nplt.subplot(121) \nplt.stem(lag_acf)\nplt.axhline(y = 0, linestyle='--',color='black')\nplt.axhline(y = -1.96/np.sqrt(len(sales_ts_log_diff2)),linestyle='--',color='gray')\nplt.axhline(y = 1.96/np.sqrt(len(sales_ts_log_diff2)),linestyle='--',color='gray')\nplt.xticks(range(0,22,1))\nplt.xlabel('Lag')\nplt.ylabel('ACF')\nplt.title('Autocorrelation Function')\n#Plot PACF:\n\nplt.subplot(122)\nplt.stem(lag_pacf)\nplt.axhline(y = 0, linestyle = '--', color = 'black')\nplt.axhline(y =-1.96/np.sqrt(len(sales_ts_log_diff2)), linestyle = '--', color = 'gray')\nplt.axhline(y = 1.96/np.sqrt(len(sales_ts_log_diff2)),linestyle = '--', color = 'gray')\nplt.xlabel('Lag')\nplt.xticks(range(0,22,1))\nplt.ylabel('PACF')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()\n\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look at the ACF and PACF plots of the log differenced series.\n* Observe our first significant value at lag 5 for ACF \n* Our significant value is at the lag 2 for the PACF\n* These values suggest us to use p = 2 and q = 5.\n\n**Here d = 2 since we could get stationary only when we did differencing twice on the log values.**","metadata":{}},{"cell_type":"markdown","source":"## **15.1Random Walk**  <a class=\"anchor\" id=\"15.1\"></a>\n\n[Table of Contents](#0.1)\n\nA **random walk** is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers. An elementary example of a random walk is the random walk on the integer number line, \n${\\displaystyle \\mathbb {Z} } $\n, which starts at 0 and at each step moves +1 or −1 with equal probability.\nhttps://en.wikipedia.org/wiki/Random_walk","metadata":{}},{"cell_type":"markdown","source":"Random walk process is defined as:\ny(t) = $\\beta_0 + \\beta_1 X_{t-1}$ + $\\epsilon_t$\n\nwhere \n* y(t) is the next value in the series.\n* $\\beta_0$ is a coefficient that if set to a value other than zero adds a constant to the drift to the random walk.\n* $\\beta_1$ is a coefficient to weight the previous time step and is set to 1.\n* $X_{t-1}$ is the observtion at the previous time step.\n* $\\epsilon_t$ is the white noise or random fluctuation at that time.","metadata":{}},{"cell_type":"markdown","source":"##  Random Walk and Autocorrelation\n\n##### We calculate the correlation between each observation and the observations at previous time steps. \n\n##### Autocorrelation plot or a correlogram plot is a plot of these correlations. \n\n##### We expect a strong auto-correlation with the previous observation and a linear fall oﬀ from there with previous lag values.","metadata":{}},{"cell_type":"markdown","source":"### Random Walk and Stationarity\n\n###### When the values of Time series data are not a function of time, we have a stationary time series.\n\n######  We know that the observations in a random walk are dependent on time. The current observation is a random step from the previous observation.\n\n######  Not all non-stationary time series are random walks and  a non-stationary time series does not have a consistent mean and/or variance over time. \n\n######  We conﬁrm non-stationary property of random walk using a statistical signiﬁcance test, speciﬁcally the Augmented Dickey-Fuller test","metadata":{}},{"cell_type":"markdown","source":"### Predicting a Random Walk\n\n###### A random walk is cannot reasonably be predicted. \n\n######  We expect that the best prediction would be to use the observation at the previous time step as what will happen in the next time step.  We know that in the random walk, the next time step will be a function of the prior time step. This is  called the naive forecast, or a persistence model.","metadata":{}},{"cell_type":"markdown","source":"### Example\n\nIn our examples, we use the same seed for the random number generator to ensure that we get the same random walk.","metadata":{}},{"cell_type":"code","source":"from random import  seed, random\nseed(1) \nrandom_walk = list() \nrandom_walk.append(-1 if random() < 0.5 else 1)\nfor i in range(1, 1000): \n    movement = -1 if random() < 0.5 else 1 \n    value    = random_walk[i-1] + movement \n    random_walk.append(value) \n    \npd.plotting.autocorrelation_plot(random_walk) \nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Check stationary property\n\nresult = adfuller(random_walk) \n\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) \nprint('Critical Values:') \nfor key, value in result[4].items(): \n    print('\\t%s: %.3f' % (key, value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare dataset for predicting a random walk\n\ntraining_size     = int(len(random_walk) * 0.70) \ntraining, test    = random_walk[0 : training_size], random_walk[training_size:]  \n\npredictions       = list() \nhist              = training[-1] \n\nfor i in range(len(test)): \n    yhat = hist \n    predictions.append(yhat) \n    hist = test[i] \n    \nrmse = np.sqrt(mean_squared_error(test, predictions))  \nprint('\\n\\nPredicting a Random Walk \\n RMSE: %.3f' % rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\n* We see a trend in the first 500 lag observations.\n* We observe that the random walk is non-stationary since the p value > 0.05.\n* We know the variation from one time step to the next is either -1 or 1 and we get RMSE as 1","metadata":{}},{"cell_type":"markdown","source":"## Random walk with drift\n\n[Table of Contents](#0.1)\n\n### Example 2\n\n**Simulate stock returns using a random walk with drift**","metadata":{}},{"cell_type":"markdown","source":"Simulate Stock prices as follows:\n\n* Generate a random walk with mean 0.0001 and standard deviation of 0.1. Add 1 for total return.\n* Convert the total returns to get a starting value of 100.\n\nPlot the simulated random walk with drift.","metadata":{}},{"cell_type":"markdown","source":"Simulate Stock prices as follows:\n\n* Generate a random walk with mean 0.0001 and standard deviation of 0.1. Add 1 for total return.\n* Convert the total returns to get a starting value of 100.\n\nPlot the simulated random walk with drift.","metadata":{}},{"cell_type":"code","source":"# Generate\nseed(1234)\nrw_steps  = np.random.normal(loc = 0.001, scale = 0.01, size = 1000) + 1\n\n### Initialize first element to 1\nrw_steps[0] = 1\n\n### Simulate the stock price\n\nPrice = rw_steps * np.cumprod(rw_steps)\nPrice = Price * 100\n\n### Plot the simulated stock prices\nplt.plot(rw_steps)\nplt.title(\"Simulated random walk with drift\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Check stationary property\n\nresult = adfuller(Price) \n\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) \nprint('Critical Values:') \n\nfor key, value in result[4].items(): \n    print('\\t%s: %.3f' % (key, value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Prediction\n\ntraining_size     = int(len(Price) * 0.70) \ntraining, test    = random_walk[0 : training_size], random_walk[training_size:]  \n\npredictions       = list() \nhist              = training[-1] \n\nfor i in range(len(test)): \n    yhat = hist \n    predictions.append(yhat) \n    hist = test[i] \n    \nrmse = np.sqrt(mean_squared_error(test, predictions))  \nprint('\\n\\nPredicting a Random Walk with drift \\nRMSE: %.3f' % rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\n* We see a trend in the lag observations.\n* We observe that the random walk is non-stationary since the p value > 0.05.\n* We know the variation from one time step to the next is either -1 or 1 and we get RMSE as 1","metadata":{}},{"cell_type":"markdown","source":"## 15.2Arima Model         <a class=\"anchor\" id=\"15.2\"></a>\n\n[Table of Contents](#0.1)\n\nOne of the most common methods used in time series forecasting is known as the ARIMA model, which stands for Auto Regressive Integrated Moving Average. ARIMA is a model that can be fitted to time series data to predict future points in the series.\n\nWe can split the Arima term into three terms, AR, I, MA:\n\nAR(p) stands for the autoregressive model, the p parameter is an integer that confirms how many lagged series are going to be used to forecast periods ahead.\n\nI(d) is the differencing part, the d parameter tells how many differencing orders are going to be used to make the series stationary.\n\nMA(q) stands for moving average model, the q is the number of lagged forecast error terms in the prediction equation. SARIMA is seasonal ARIMA and it is used with time series with seasonality.","metadata":{}},{"cell_type":"markdown","source":"Let's see the example of data from quandel. The data descibes the bank of England's official statistics on spot exchange rates for the EURO into US dollars.   ","metadata":{}},{"cell_type":"code","source":"from pandas import DataFrame\nfrom io import StringIO\nimport time, json\nfrom datetime import date\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15, 6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the data\nbank_df = pd.read_csv('/kaggle/input/time-series-data/BOE-XUDLERD.csv')\nbank_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting to time series data \nbank_df['Date'] = pd.to_datetime(bank_df['Date'])\nindexed_df = bank_df.set_index('Date')\nts = indexed_df['Value']\nts.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize the raw data\n\nplt.plot(ts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Resample the data as it contains too much variations \n\nts_week = ts.resample('W').mean()\nplt.plot(ts_week)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for stationarity using dickey fuller test","metadata":{}},{"cell_type":"code","source":"def test_stationarity(timeseries):\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=52,center=False).mean() \n    rolstd = timeseries.rolling(window=52,center=False).std()\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\ntest_stationarity(ts_week)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the test statistics is more than 5 % critical value and the p-value is larger than 0.05 , the moving average is not constant over time and the null hypothesis of the Dickey-Fuller test cannot be rejected. This shows the weekly time series is not stationary. \n\nAs such , we need to transform this series into a stationary time series. ","metadata":{}},{"cell_type":"markdown","source":"### Differencing \n\nWe can apply the concept of differencing to stationarize the data. Before differencing it is better to do the log trasnsformation of the data","metadata":{}},{"cell_type":"code","source":"ts_week_log = np.log(ts_week)\nts_week_log_diff = ts_week_log - ts_week_log.shift()\nplt.plot(ts_week_log_diff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Again confirming with the dickey-fuller test \n\nts_week_log_diff.dropna(inplace=True)\ntest_stationarity(ts_week_log_diff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference** \n\nThe test statistic is less than 1% of the critical value, shows that the time series is stationary with 99% confidence. Now we can apply the statistical models like ARIMA to forecast the future values \n\n","metadata":{}},{"cell_type":"code","source":"#ACF and PACF\nlag_acf = acf(ts_week_log_diff, nlags=10)\nlag_pacf = pacf(ts_week_log_diff, nlags=10, method='ols')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=7.96/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=7.96/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference drawn from the ACF and PACF values** \n\nUsing the plot we can determine the values for p and q respectively :\n\n1. p: the lag value where the PACF cuts off (drop to 0) for the first time. So here p =2.\n2. q: the lag value where the ACF chart crosses the upper confidence interval for the first time. if you look closely q=1. ","metadata":{}},{"cell_type":"code","source":"# Optimal values fot ARIMA(p,d,q) model are (2,1,1). Hence plot the ARIMA model using the value (2,1,1)\nmodel = ARIMA(ts_week_log, order=(2, 1, 1))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(ts_week_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_week_log_diff)**2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the results of the ARIMA model and plot the residuals\n\nprint(results_ARIMA.summary())\n# plot residual errors\nresiduals = DataFrame(results_ARIMA.resid)\nresiduals.plot(kind='kde')\nprint(residuals.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predictions \n\npredictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint (predictions_ARIMA_diff.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is returning the results we want to see, we can scale the model predictions back to the original scale. Hence the remove the first order differencing and take exponent to restore the predictions back to their original scale.","metadata":{}},{"cell_type":"code","source":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_log = pd.Series(ts_week_log.iloc[0], index=ts_week_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(ts_week)\nplt.plot(predictions_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts_week)**2)/len(ts_week)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing datsets\nsize = int(len(ts_week_log) - 15)\ntrain, test = ts_week_log[0:size], ts_week_log[size:len(ts_week_log)]\nhistory = [x for x in train]\npredictions = list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training the model and forecasting \n\nsize = int(len(ts_week_log) - 15)\ntrain, test = ts_week_log[0:size], ts_week_log[size:len(ts_week_log)]\nhistory = [x for x in train]\npredictions = list()\nprint('Printing Predicted vs Expected Values...')\nprint('\\n')\nfor t in range(len(test)):\n    model = ARIMA(history, order=(2,1,1))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(float(yhat))\n    obs = test[t]\n    history.append(obs)\nprint('predicted=%f, expected=%f' % (np.exp(yhat), np.exp(obs)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validating the model \n\nerror = mean_squared_error(test, predictions)\nprint('\\n')\nprint('Printing Mean Squared Error of Predictions...')\nprint('Test MSE: %.6f' % error)\npredictions_series = pd.Series(predictions, index = test.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting forecasted vs Observed values \n\nfig, ax = plt.subplots()\nax.set(title='Spot Exchange Rate, Euro into USD', xlabel='Date', ylabel='Euro into USD')\nax.plot(ts_week[-60:], 'o', label='observed')\nax.plot(np.exp(predictions_series), 'g', label='rolling one-step out-of-sample forecast')\nlegend = ax.legend(loc='upper left')\nlegend.get_frame().set_facecolor('w')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **15.3 Auto Arima**        <a class=\"anchor\" id=\"15.3\"></a>\n\n[Table of Contents](#0.1)\n\n It uses the AIC (Akaike Information Criterion) & BIC(Bayesian Information Criterion) values generated by trying different combinations of p,q & d values to fit the model.\n \n* In an ARIMA model there are 3 parameters, namely p, q and d that help model major aspects of a time series: seasonality, trend and noise.\n\n* If our model has a seasonal component, we use Seasonal ARIMA with parameters, P, Q and D related to seasonal components of the model.\n\n\n### auto.arima\n\nThe module auto.arima fits the best ARIMA model to univariate time series according to either AIC, AICc or BIC value. This function conducts a search over possible model within the order constraints provided.\n\n### AIC \n\nThe Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. \n\n### AICc is AIC with a correction for small sample sizes. \n\n### BIC \n\nBayesian information criterion (BIC) or Schwarz information criterion (also SIC, SBC, SBIC) is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC). \n\nhttps://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_BIC\nhttps://en.wikipedia.org/wiki/Bayesian_information_criterion\n","metadata":{}},{"cell_type":"markdown","source":"We use tractor sales data to replicate auto.arima in python.","metadata":{}},{"cell_type":"code","source":"import sys\nimport warnings\nimport itertools\nwarnings.filterwarnings(\"ignore\")\n\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nimport statsmodels.formula.api as smf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tractor_sales_Series = pd.read_csv(\"/kaggle/input/time-series-data/TractorSales.csv\")\ntractor_sales_Series.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = pd.date_range(start='2003-01-01', freq='MS', periods=len(tractor_sales_Series))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import calendar\ndata['Month'] = dates.month\ndata['Month'] = data['Month'].apply(lambda x: calendar.month_abbr[x])\ndata['Year'] = dates.year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.drop(['Month-Year'], axis=1, inplace=True)\ndata.rename(columns={'Number of Tractor Sold':'Tractor-Sales'}, inplace=True)\ndata = data[['Month', 'Year', 'Tractor-Sales']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.set_index(dates, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract out the time-series\nsales_ts = data['Tractor-Sales']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.plot(sales_ts)\nplt.xlabel('Years')\nplt.ylabel('Tractor Sales')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nWe observe both trend and multiplicative seasonaliy from the plot shown above.\n\nWe try moving averages of various window widths such as 4, 6,8 and 12.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, sharey=False, sharex=False)\nfig.set_figwidth(14)\nfig.set_figheight(8)\naxes[0][0].plot(sales_ts.index, sales_ts, label='Original')\naxes[0][0].plot(sales_ts.index, sales_ts.rolling(window=4).mean(), label='4-Months Rolling Mean')\naxes[0][0].set_xlabel(\"Years\")\naxes[0][0].set_ylabel(\"Number of Tractor's Sold\")\naxes[0][0].set_title(\"4-Months Moving Average\")\naxes[0][0].legend(loc='best')\naxes[0][1].plot(sales_ts.index, sales_ts, label='Original')\naxes[0][1].plot(sales_ts.index, sales_ts.rolling(window=6).mean(), label='6-Months Rolling Mean')\naxes[0][1].set_xlabel(\"Years\")\naxes[0][1].set_ylabel(\"Number of Tractor's Sold\")\naxes[0][1].set_title(\"6-Months Moving Average\")\naxes[0][1].legend(loc='best')\naxes[1][0].plot(sales_ts.index, sales_ts, label='Original')\naxes[1][0].plot(sales_ts.index, sales_ts.rolling(window=8).mean(), label='8-Months Rolling Mean')\naxes[1][0].set_xlabel(\"Years\")\naxes[1][0].set_ylabel(\"Number of Tractor's Sold\")\naxes[1][0].set_title(\"8-Months Moving Average\")\naxes[1][0].legend(loc='best')\naxes[1][1].plot(sales_ts.index, sales_ts, label='Original')\naxes[1][1].plot(sales_ts.index, sales_ts.rolling(window=12).mean(), label='12-Months Rolling Mean')\naxes[1][1].set_xlabel(\"Years\")\naxes[1][1].set_ylabel(\"Number of Tractor's Sold\")\naxes[1][1].set_title(\"12-Months Moving Average\")\naxes[1][1].legend(loc='best')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Determing rolling statistics\n\nrolmean = sales_ts.rolling(window = 4).mean()\nrolstd = sales_ts.rolling(window = 4).std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot rolling statistics:\norig = plt.plot(sales_ts, label='Original')\nmean = plt.plot(rolmean, label='Rolling Mean')\nstd = plt.plot(rolstd, label = 'Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dickey-Fuller Test - Let's run the Dicky Fuller Test on the timeseries and verify the null hypothesis that the TS is non-stationary.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\ndftest = adfuller(sales_ts)\ndftest\nprint('DF test statistic is %3.3f' %dftest[0])\nprint('DF test p-value is %1.4f' %dftest[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Though the variation in standard deviation is small, rolling mean is clearly increasing with time and this is not a stationary series. Also, the test statistic is way more than the critical values.","metadata":{}},{"cell_type":"markdown","source":"As we observed while plotting the moving average over months that there is a monhly pattern, now, let’s decipher the seasonal component.","metadata":{}},{"cell_type":"markdown","source":"#### Seasonality – Time Series Decomposition","metadata":{}},{"cell_type":"markdown","source":"Observe how number of tractors sold vary on a month on month basis. We will plot a stacked annual plot to observe seasonality in our data.","metadata":{}},{"cell_type":"code","source":"monthly_sales_data = pd.pivot_table(data, values = \"Tractor-Sales\", columns = \"Year\", index = \"Month\")\nmonthly_sales_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly_sales_data = monthly_sales_data.reindex(index = ['Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nmonthly_sales_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly_sales_data.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yearly_sales_data = pd.pivot_table(data, values = \"Tractor-Sales\", columns = \"Month\", index = \"Year\")\nyearly_sales_data = yearly_sales_data[['Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']]\nyearly_sales_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yearly_sales_data.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yearly_sales_data.boxplot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inferences\n\nThe tractor sales have been increasing without fail every year.\nJuly and August are the peak months for tractor sales and the variance and the mean value in July and August are also much higher than any of the other months.\nWe can see a seasonal cycle of 12 months where the mean value of each month starts with a increasing trend in the beginning of the year and drops down towards the end of the year. We can see a seasonal effect with a cycle of 12 months.","metadata":{}},{"cell_type":"markdown","source":"### Time Series Decomposition","metadata":{}},{"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(sales_ts, model='multiplicative')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = decomposition.plot()\nfig.set_figwidth(8)\nfig.set_figheight(6)\nfig.suptitle('Decomposition of multiplicative time series')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some of our key observations from this analysis:\n\n1) Trend: 12-months moving average looks quite similar to a straight line hence we could have easily used linear regression to estimate the trend in this data.\n\n2) Seasonality: Seasonal plot displays a fairly consistent month-on-month pattern. The monthly seasonal components are average values for a month after removal of trend. Trend is removed from the time series using the following formula:\n\nSeasonality_t × Remainder_t = Y_t/Trend_t\n \n3) Irregular Remainder (random): is the residual left in the series after removal of trend and seasonal components. Remainder is calculated using the following formula:\n\nRemainder_t = Y_t / (Trend_t × Seasonality_t)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.plot(sales_ts.diff(periods=1))\nplt.xlabel('Years')\nplt.ylabel('Tractor Sales')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe seasonality even after differencing.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.plot(np.log10(sales_ts))\nplt.xlabel('Years')\nplt.ylabel('Log (Tractor Sales)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe trend and seasonality even after taking log of the observations.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(np.log10(sales_ts).diff(periods=1))\nplt.xlabel('Years')\nplt.ylabel('Differenced Log (Tractor Sales)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_ts_log = np.log10(sales_ts)\nsales_ts_log.dropna(inplace=True)\n\nsales_ts_log_diff = sales_ts_log.diff(periods=1) # same as ts_log_diff = ts_log - ts_log.shift(periods=1)\nsales_ts_log_diff.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\nfig.set_figwidth(12)\nfig.set_figheight(4)\nsmt.graphics.plot_acf(sales_ts_log, lags=30, ax=axes[0])\nsmt.graphics.plot_pacf(sales_ts_log, lags=30, ax=axes[1])\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nonstationary series have an ACF that remains significant for half a dozen or more lags, rather than quickly declining to zero. You must difference such a series until it is stationary before you can identify the process\n\nThe above ACF is “decaying”, or decreasing, very slowly, and remains well above the significance range (blue band) for at least a dozen lags. This is indicative of a non-stationary series.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\nfig.set_figwidth(12)\nfig.set_figheight(4)\nplt.xticks(range(0,30,1), rotation = 90)\nsmt.graphics.plot_acf(sales_ts_log_diff, lags=30, ax=axes[0])\nsmt.graphics.plot_pacf(sales_ts_log_diff, lags=30, ax=axes[1])\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nThe above ACF has “decayed” fast and remains within the significance range (blue band) except for a few (5) lags. This is indicative of a stationary series.","metadata":{}},{"cell_type":"code","source":"# Define the p, d and q parameters to take any value between 0 and 2\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, d and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seasonal_pdq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separate data into train and test\ndata['date'] = data.index\ntrain = data[data.index < '2013-01-01']\ntest = data[data.index >= '2013-01-01']\ntrain_sales_ts_log = np.log10(train['Tractor-Sales'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_aic = np.inf\nbest_pdq = None\nbest_seasonal_pdq = None\ntemp_model = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in pdq:\n    for param_seasonal in seasonal_pdq:\n        \n        try:\n            temp_model = sm.tsa.statespace.SARIMAX(train_sales_ts_log,\n                                             order = param,\n                                             seasonal_order = param_seasonal,\n                                             enforce_stationarity=True)\n            results = temp_model.fit()\n\n            \n            if results.aic < best_aic:\n                best_aic = results.aic\n                best_pdq = param\n                best_seasonal_pdq = param_seasonal\n        except:\n            #print(\"Unexpected error:\", sys.exc_info()[0])\n            continue\nprint(\"Best SARIMAX{}x{}12 model - AIC:{}\".format(best_pdq, best_seasonal_pdq, best_aic))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\n* The best fit model is selected based on Akaike Information Criterion (AIC) , and Bayesian Information Criterion (BIC) values. The idea is to choose a model with minimum AIC and BIC values.\n\nFor ARIMA(p, d, q) × (P, D, Q)S,\nwe got SARIMAX(0, 1, 1)x(1, 0, 1, 12)12 model with the least AIC:-600.0908420381976\n\nHere, \n* p = non-seasonal AR order = 0,\n* d = non-seasonal differencing = 1,\n* q = non-seasonal MA order = 1,\n* P = seasonal AR order = 1,\n* D = seasonal differencing = 0,\n* Q = seasonal MA order = 1,\n* S = time span of repeating seasonal pattern = 12","metadata":{}},{"cell_type":"markdown","source":"### Predict sales on in-sample date using the best fit ARIMA model","metadata":{}},{"cell_type":"code","source":"best_model = sm.tsa.statespace.SARIMAX(train_sales_ts_log,\n                                      order=(0, 1, 1),\n                                      seasonal_order=(1, 0, 1, 12),\n                                      enforce_stationarity=True)\nbest_results = best_model.fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_results.summary().tables[0])\nprint(best_results.summary().tables[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dynamic = best_results.get_prediction(start=pd.to_datetime('2012-01-01'), dynamic=True, full_results=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dynamic_ci = pred_dynamic.conf_int()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred99 = best_results.get_forecast(steps=24, alpha=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the predicted and true values of our time series\nsales_ts_forecasted = pred_dynamic.predicted_mean\ntestCopy = test.copy()\ntestCopy['sales_ts_forecasted'] = np.power(10, pred99.predicted_mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testCopy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the root mean square error\nmse = ((testCopy['Tractor-Sales'] - testCopy['sales_ts_forecasted']) ** 2).mean()\nrmse = np.sqrt(mse)\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axis = train['Tractor-Sales'].plot(label='Train Sales', figsize=(10, 6))\ntestCopy['Tractor-Sales'].plot(ax=axis, label='Test Sales', alpha=0.7)\ntestCopy['sales_ts_forecasted'].plot(ax=axis, label='Forecasted Sales', alpha=0.7)\naxis.set_xlabel('Years')\naxis.set_ylabel('Tractor Sales')\nplt.legend(loc='best')\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forecast sales using the best fit ARIMA model","metadata":{}},{"cell_type":"code","source":"# Get forecast 36 steps (3 years) ahead in future\nn_steps = 36\npred_uc_99 = best_results.get_forecast(steps=36, alpha=0.01) # alpha=0.01 signifies 99% confidence interval\npred_uc_95 = best_results.get_forecast(steps=36, alpha=0.05) # alpha=0.05 95% CI\n\n# Get confidence intervals 95% & 99% of the forecasts\npred_ci_99 = pred_uc_99.conf_int()\npred_ci_95 = pred_uc_95.conf_int()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_steps = 36\nidx = pd.date_range(data.index[-1], periods=n_steps, freq='MS')\nfc_95 = pd.DataFrame(np.column_stack([np.power(10, pred_uc_95.predicted_mean), np.power(10, pred_ci_95)]), \n                     index=idx, columns=['forecast', 'lower_ci_95', 'upper_ci_95'])\nfc_99 = pd.DataFrame(np.column_stack([np.power(10, pred_ci_99)]), \n                     index=idx, columns=['lower_ci_99', 'upper_ci_99'])\nfc_all = fc_95.combine_first(fc_99)\nfc_all = fc_all[['forecast', 'lower_ci_95', 'upper_ci_95', 'lower_ci_99', 'upper_ci_99']] # just reordering columns\nfc_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the forecast along with the confidence band\n\naxis = sales_ts.plot(label='Observed', figsize=(8, 4))\nfc_all['forecast'].plot(ax=axis, label='Forecast', alpha=0.7)\naxis.fill_between(fc_all.index, fc_all['lower_ci_95'], fc_all['upper_ci_95'], color='k', alpha=.15)\naxis.set_xlabel('Years')\naxis.set_ylabel('Tractor Sales')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot ACF and PACF for residuals of ARIMA model to ensure no more information is left for extraction","metadata":{}},{"cell_type":"code","source":"best_results.plot_diagnostics(lags=30, figsize=(16,12))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nWe need to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If it is not that it signifies that the model can be further improved and we repeat the process with the residuals.\n\nIn this case, our model diagnostics suggests that the model residuals are normally distributed based on the following:\n\n1. The KDE plot of the residuals on the top right is almost similar with the normal distribution.\n2. The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution with N(0, 1). Again, this is a strong indication that the residuals are normally distributed.\n3. The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself.\n\nThose observations coupled with the fact that there are no spikes outside the insignificant zone for both ACF and PACF plots lead us to conclude that that residuals are random with no information or juice in them and our model produces a satisfactory fit that could help us understand our time series data and forecast future values. It sems that our ARIMA model is working fine.","metadata":{}},{"cell_type":"markdown","source":"# 16. References      <a class=\"anchor\" id=\"16\"></a>\n\n[Table of Contents](#0.1)\n\n1. Time Series lecture by Great Learning by great lakes(https://www.youtube.com/watch?v=FPM6it4v8MY)(Myself Ex-PG Student)\n2. Machine Learning Mastery by Jason Brownlee (https://machinelearningmastery.com/)\n3. Introduction to Time Series Analysis by (Douglas C Montgomery, Cheryl L Jennings,Murat Kulachi)\n  \n  ","metadata":{}}]}