{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/mlpoke/MLPOke.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0. Introduction \nIn this notebook, I will introduce some basic encoding schemes in a very understandable way. The target audiences are those who just get to know machine learning and want quick access to these techniques. For convenience purposes, I will also provide the sklearn version and the library corresponding to each method.\n\nBy the end of this post, I hope that you would have a better idea of how to apply different encoding schemes.\n\nFor best result, you should look at the data frame -> description -> code"},{"metadata":{},"cell_type":"markdown","source":"There 3 main routes to encode the categorical.\n\n**Classic Encoders:** Ordinal, OneHot, Binary, Frequency, Hashing\n\n**Contrast Encoders:** Helmert, Backward Difference\n\n**Bayesian Encoders:** Target, Leave One Out, Weight Of Evidence, James-Stein, M-estimator\n\nAnd there are many more! However, once you know how these most common encoding schemes work, you will find it fairly easy to google the other one.\n\nLet's create a random pokemon data set!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom category_encoders import *\n\n\ndata={'Type':['Fire','Water','Bug', 'Fire', 'Fire','Bug','Water','Bug','Ice'],'Height':['Short','Normal','Very short','Tall','Normal','Short','Tall','Very short','Tall'],'Stats_total':[495,525,195,580, 525,500,670,405,580],'Legendary':[0,0,0,1,0,0,1,0,1]}\ndf_main=pd.DataFrame(data)\ndf_main","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to examine the columns feature of this data by applying different encoders."},{"metadata":{},"cell_type":"markdown","source":"# I. Classic Encoders\n\nAs the name suggests, classical encoders are well known and widely used. Their concept are also pretty straight-forward. "},{"metadata":{},"cell_type":"markdown","source":"# 1) Ordinal Encoding\n\n\"Ordinal\" means ordered, so this only works on the ordinal feature. \nMost of the time, unique values in the ordinal column are of type string and written in a human language. Thus, we need to manually assign a numerical ranking according to their order."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_main.copy()\nheight_dict ={'Very short':1, 'Short':2, 'Normal':3, 'Tall':4}\ndf['Ordinal_Height']=df.Height.map(height_dict)\ndf[['Height','Ordinal_Height']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) One-hot encoding\nOne-hot encoding can be explained by a 2 steps process:\n\n* Split all the categories in one column to different columns\n\n* Put the check mark '1' for the appropriate location.\n\nThe `get_dummies` function in pandas can do the job"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_main.copy()\ndf_Height=pd.get_dummies(df[['Height']],prefix='T')\n\npd.concat([df[['Height']],df_Height],axis=1).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sklearn can do the similar things:  \n(*I still prefer using get_dummies since it gives us a nicer label.*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\ndf=df_main.copy()\n\nohe=OneHotEncoder()\nohe=ohe.fit_transform(df[['Height']]).toarray()\nnewdata=pd.DataFrame(ohe)\n\ndfh=pd.concat([df[['Height']],newdata],axis=1)\ndfh.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Binary Encoding\n\nThis encoding is different from what you think it is\n\nThere are 3 steps:\n* Going down the column, every time it sees a new category, it gives a number, starting from 1 (and the next one is 2)\n* Convert these number into binary\n* Place each digit in this binary in a separate column.\n\nImagine that you have 200 different categories. One hot encoding will create 200 different columns. It the meantime, binary encoding only need 8 columns. (Since 11001000 is 200 in base 2).\n\nIn the code below, I will add the encounter step column so you can see how it works.\n\n**Note**: People sometimes refer hot-encoding as binary encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import BinaryEncoder\ndf=df_main.copy()\n\nbe=BinaryEncoder(cols=['Type'])\nnewdata=be.fit_transform(df['Type'])\n\nEncounterStep= pd.DataFrame([1,2,3,1,1,3,2,3,4],columns=[\"EncounterStep\"]) #Test it your self if this correct\ndfh=pd.concat([df[['Type']],EncounterStep,newdata],axis=1)\ndfh","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) Frequency Encoding\n\nGive each category the probability (occurence/total event)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_main.copy()\n\ndfTemp=df.groupby(\"Type\").size()/len(df) #Group it by type, find the size of each type, and divide by total event\ndf['Type_freq']=df['Type'].map(dfTemp) #dfTemp is a dataframe type\n\npd.concat([df[['Type']],df['Type_freq']],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Hashing Encoding\n\nHashing converts categorical variables to a higher dimensional space of integers. I won't comment on the methodology much here since [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html \"reference\") explain it very well.\n\nThe `n_feature` is the number of columns you want to add. These new columns distinguish the corresponding category. However, you can adjust `n_feature` to any number. This is like binary encoding on steroids! \n\n**Advantage**\n* Deal with large scale categorical features\n* High speed and reduced memory usage\n\n**Disadvantage**\n* No inverse-transformation method"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction import FeatureHasher\ndf=df_main.copy()\n\nfg = FeatureHasher(n_features=2, input_type='string')\nhashed_features = fg.fit_transform(df['Type'])\nhashed_features = hashed_features.toarray()\n\ndf=pd.concat([df[['Type']], pd.DataFrame(hashed_features)], axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Contrast encoders\nContrast coding allows for recentering of categorical variables such that the intercept of a model is not the mean of one level of a category, but instead, the mean of all data points in the data set.\n\nMany people argue that these encodings are not very effective so I won't talk alot about it.\n\n# 1) Helmert (reverse) Encoding\nHelmert coding compares each level of a categorical variable to the mean of the subsequent levels\nMore about this [here](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#HELMERT)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import HelmertEncoder\ndf=df_main.sample(5)\n\nhe=HelmertEncoder(cols=['Height'])\nnewcolumn=he.fit_transform(df['Height'])\n\ndf=pd.concat([df[['Height']],newcolumn],axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Backward Difference Encoding\n\nIn backward difference coding, the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level. [Read more](http://www.statsmodels.org/dev/contrasts.html)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import BackwardDifferenceEncoder\ndf=df_main.sample(5)\n\nbwde = BackwardDifferenceEncoder()\nnewcolumns=bwde.fit_transform(df['Type'])\n\npd.concat([df[['Type']],newcolumns],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Bayesian Target Encoders\n\nThe general idea of this method is to take the target into account. \n\n**Advantage:** \n\n* Require minimal effort, only create one column for any number of categories in that feature\n\n* Most favorite encoding scheme in Kaggle competition\n\n**Disadvantage:**\n\n* Only work for supervised learning (thus, inherently leaky). This means that when dealing with unsupervised data, it gets worse!\n\n* Need regularization for the previous reason"},{"metadata":{},"cell_type":"markdown","source":"# 1) Target Encoding\nThe basic idea is \n$$TE_i=\\frac{\\text{total true}(y_i)}{\\text{total}(y_i)}\\cdot \\lambda$$\n\nwhere $y_i$ is a category and $\\lambda$ is a smoothing function (For more, search additive or Laplace smoothing)\n\nLet's compare the table without the smoothing function..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_main.copy()\n\nmean_encode=df.groupby(\"Type\")['Legendary'].mean()\ndf['Type_legendary']=df['Type'].map(mean_encode)\n\ndf[['Type','Legendary','Type_legendary']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...and sk-learn TargetEncoder with smoothing.\n\n**Note:** By default, the \"smoothing\" coefficient is 1. The bigger the value, the stronger our regularization. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import TargetEncoder\ndf=df_main.copy()\n\nTE = TargetEncoder(cols=['Type'])\ndf['Type_legendary']=TE.fit_transform(df['Type'],df['Legendary'])\n\ndf[['Type','Legendary','Type_legendary']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Leave One Out Encoding\nThis is very similar to target encoding but excludes the current row’s target when calculating the mean target for a level to reduce the effect of outliers.\n\nAdditionally, you can add some (Gaussian) noise to the data to prevent overfitting. Change the sigma function from 0 to any value between 0 and 1 do the trick.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import LeaveOneOutEncoder\ndf=df_main.copy()\n\nLOOE = LeaveOneOutEncoder(cols=['Type'], sigma=0.2)\ndf['Type_legendary']=LOOE.fit_transform(df['Type'], df['Legendary'])\n\ndf[['Type','Legendary','Type_legendary']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Weight of Evidence Encoding\n\nis a measure of how much the evidence supports or undermines a hypothesis. The Weight of Evidence in sklearn is the adjacent version of it which is just adding some value on the top and the bottom:\n\n$$WoE=\\bigg[ \\ln\\bigg( \\frac{\\text{Distribution of goods}+adj}{\\text{Distribution of Bads}+adj}\\bigg) \\bigg]$$\n\nwhere $adj$ is the adjacent factor is a function that avoids division by 0.\n\nAdvantage:\n\n* Work well with logistic regression since WoE transformation has the same logistic scale.\n* Can use WoE to compare across feature since their values are standardized.\n           \nDisadvantages: \n\n* May lose information due to some category may have the same WoE\n* Does not take into account features correlation\n* Overfit \n\nNote: We can adjust the adj factor by changing regularization. (By default it is 1). When setting it equal to 0. You come back to the original WOE and may encounter division by 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import WOEEncoder\n\nWOEE = WOEEncoder(cols=['Type'],regularization=0.5)\ndf['Type_legendary']=WOEE.fit_transform(df['Type'], df['Legendary'])\n\ndf[['Type','Legendary','Type_legendary']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4)James-Stein Encoding\nThis is target encoding but is more roburst. It is defined by the formula:\n   $$JS_i = (1-B)\\cdot \\text{mean}(y_i) + B\\cdot\\text{mean}(y)$$\nwhere $\\text{mean}(y)$ is the global mean of the target, $\\text{mean}(y_i)$ is the mean of the category, and $B$ is the weight. \n\nThe weight B depends on the $\\sigma (y)$ and $\\sigma (y_i)$, which is the variance of the target. However, we do not know what the variance is so we have to estimate it. More about this method [here](https://kiwidamien.github.io/james-stein-encoder.html). \n\n**Note:** The limitation of James-Stein is it work only best for the feature that has a normal distribution. \n\nIn the sklearn version, the default sigma is $0.05$."},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import JamesSteinEncoder\n\nJSE= JamesSteinEncoder(sigma=0.1)\nnewcolumns=JSE.fit_transform(df['Type'], df['Legendary'])\n\ndf['JSE_col']=newcolumns\ndf[['Type','Legendary','JSE_col']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) M-estimator Encoding\n\nM-Estimate Encoder is a simplified version of Target Encoder. The M stands for maximum likelihood-type. It has only one hyper-parameter — $m$, which represents the power of regularization. The higher the value of m results into stronger shrinking. Recommended values for $m$ is in the range of $1$ to $100$. Read more [here](https://en.wikipedia.org/wiki/M-estimator)\n\n**Note:** By default, $m=1$."},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import MEstimateEncoder\ndf=df_main.copy()\n\nMEE=MEstimateEncoder(m=2)\nnewcolumns = MEE.fit_transform(df['Type'], df['Legendary'])\n\ndf['MEE_col']=newcolumns\ndf[['Type','Legendary','MEE_col']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# V. Conclusion\nThere are no single formula for encoding a feature. However, if you understand the 12 encoding techniques I introduced above, it would be able to move fast. Moreover, it always worth try all the techniques that are applicable to the feature and decide which one works best. Try to input different regularization coefficient values and see if they increase your score. The cheat-sheet below will help you make some initial decisions. \n\nHave fun playing with encoders!"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/somepictures/en_dis.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}