{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree._tree import TREE_LEAF\n\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef prune_index(inner_tree, index, threshold):\n    if inner_tree.value[index].max() > threshold:\n        # turn node into a leaf by \"unlinking\" its children\n        inner_tree.children_left[index] = TREE_LEAF\n        inner_tree.children_right[index] = TREE_LEAF\n    # if there are shildren, visit them as well\n    if inner_tree.children_left[index] != TREE_LEAF:\n        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n\n\ndef get_results(data, target):\n    cutoff = int(len(data)*0.5)\n    training_data = data[:cutoff]\n    training_target = target[:cutoff]\n    test_data = data[cutoff:]\n    test_target = target[cutoff:]\n    positive_test_data = [x for x, y in zip(test_data, test_target) if y > 0.9]\n    classifiers = [(\"KNN(3)\", KNeighborsClassifier(n_neighbors=3)),\n                   (\"KNN(5)\", KNeighborsClassifier(n_neighbors=5)),\n                   (\"SVM\", SVC(gamma='scale')),\n                   (\"SVM (rbf)\", SVC(kernel='rbf', gamma='scale')),\n                   (\"NN\", MLPClassifier(solver='lbfgs', hidden_layer_sizes=(len(data[0]),), random_state=1))]\n    for name, classifier in classifiers:\n        classifier.fit(training_data, training_target)\n        training_accuracy = len(list(filter(\n            None, training_target == classifier.predict(training_data))))\n        test_accuracy = len(list(filter(\n            None, test_target == classifier.predict(test_data))))\n        training_accuracy /= len(training_data)\n        test_accuracy /= len(test_data)\n        positive_test_count = len(list(filter(lambda t: t > 0.9, classifier.predict(positive_test_data))))\n        print(\"{} (train): {}%\".format(name, training_accuracy * 100.0))\n        print(\"{} (test): {}%\".format(name, test_accuracy * 100.0))\n        print(\"{} (positive test): {} of {} ({}%)\".format(name, positive_test_count, len(positive_test_data), positive_test_count / len(positive_test_data) * 100.))\n        plot_learning_curve(name, classifier, data, target)\n\n    # Do algos with pruning after\n    classifier = DecisionTreeClassifier()\n    classifier.fit(training_data, training_target)\n    prune_index(classifier.tree_, 0, 50)\n    training_accuracy = len(list(filter(\n       None, training_target == classifier.predict(training_data))))\n    test_accuracy = len(list(filter(\n        None, test_target == classifier.predict(test_data))))\n    training_accuracy /= len(training_data)\n    test_accuracy /= len(test_data)\n    positive_test_count = len(list(filter(lambda t: t > 0.9, classifier.predict(positive_test_data))))\n    print(\"Decision Tree (train): {}%\".format(training_accuracy * 100.0))\n    print(\"Decision Tree (test): {}%\".format(test_accuracy * 100.0))\n    print(\"Decision Tree (positive test): {} of {} ({}%)\".format(positive_test_count, len(positive_test_data), positive_test_count / len(positive_test_data) * 100.))\n    plot_learning_curve(\"Decision Tree\", classifier, data, target)\n\n    classifier = AdaBoostClassifier()\n    classifier.fit(training_data, training_target)\n    for estimator in classifier.estimators_:\n        prune_index(estimator.tree_, 0, 50)\n    training_accuracy = len(list(filter(\n       None, training_target == classifier.predict(training_data))))\n    test_accuracy = len(list(filter(\n        None, test_target == classifier.predict(test_data))))\n    training_accuracy /= len(training_data)\n    test_accuracy /= len(test_data)\n    positive_test_count = len(list(filter(lambda t: t > 0.9, classifier.predict(positive_test_data))))\n    print(\"Boosted Decision Tree (train): {}%\".format(training_accuracy * 100.0))\n    print(\"Boosted Decision Tree (test): {}%\".format(test_accuracy * 100.0))\n    print(\"Boosted Decision Tree (positive test): {} of {} ({}%)\".format(positive_test_count, len(positive_test_data), positive_test_count / len(positive_test_data) * 100.))\n    plot_learning_curve(\"Boosted Decision Tree\", classifier, data, target)\n\ndef load_creditcard():\n    inputs = []\n    outputs = []\n    df = pd.read_csv('../input/creditcardfraud/creditcard.csv')\n    inputs = df[df.keys()[1:-1]]\n    outputs = df[df.keys()[-1]]\n    length = int(len(inputs.values) * 0.2)\n    return inputs.values[:length], outputs.values[:length]\n\ndef load_weather():\n    inputs = []\n    outputs = []\n    df = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\n    for bool_field in ('RainTomorrow', 'RainToday',):\n        df[bool_field] = df[bool_field].where(df[bool_field].values != 'Yes', 1)\n        df[bool_field] = df[bool_field].where(df[bool_field].values == 1, 0)\n    df = df.fillna(0)\n    inputs = df[[\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"Humidity9am\", \"Humidity3pm\",\n                 \"Pressure9am\", \"Pressure3pm\", \"Temp9am\", \"Temp3pm\", \"RainToday\"]]\n    outputs = df[\"RainTomorrow\"]\n    length = int(len(inputs.values) * 0.2)\n    return inputs.values[:length], outputs.values[:length]\n\ndef plot_learning_curve(title, estimator, data, target):\n    # From https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, data, target, cv=3)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"get_results(*load_creditcard())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"631a53df35ba1997b19230556776eabeff6d7131"},"cell_type":"code","source":"plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"715d9ed1ff4dc4188cc123555905bd5c10d14994"},"cell_type":"code","source":"get_results(*load_weather())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e35920575e6dd88949508bb50b1f1cf75a080828"},"cell_type":"code","source":"plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}