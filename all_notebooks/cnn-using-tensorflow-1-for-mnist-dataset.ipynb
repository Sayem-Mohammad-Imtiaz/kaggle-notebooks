{"cells":[{"metadata":{},"cell_type":"markdown","source":" <strong>The MNIST database of handwritten digits</strong> has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n\n<br>\n<strong>The MNIST database (Modified National Institute of Standards and Technology database)</strong> is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments.Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\n\n<br>\n\n<strong>MNIST database</strong> contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support-vector machine to get an error rate of 0.8%. An extended dataset similar to MNIST called EMNIST has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits and characters.\n\n<br>\n\n\n<img src = \"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\">\n                          \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pip install tensorflow-gpu==1.15  # GPU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf \n\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the important modules, libraries, and frameworks\nimport numpy as np \n\nimport matplotlib.pyplot as  plt\n\nimport os \n\nimport cv2\n\nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the training and test sets\ntrain_df = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")\ntest_df = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print the shpae of training and test data sets\nprint(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explore the dataset\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the training dataset into  features --> tr_x and labels --> tr_y\ntr_x = train_df[train_df.columns[train_df.columns != 'label']]\ntr_y = train_df[train_df.columns[train_df.columns == 'label']]\nprint(\"The shape of training features:{0}\\nThe shape of labels:{1}\".format(tr_x.shape,tr_y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert the training features and labels into numpy array to feed the CNN During training\ntr_x = tr_x.values\n\ntr_y = tr_y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the testing dataset into  features --> test_x and labels --> test_y\ntest_x = test_df[test_df.columns[test_df.columns != 'label']]\ntest_y = test_df[test_df.columns[test_df.columns == 'label']]\nprint(\"The shape of testing features:{0}\\nThe shape of labels:{1}\".format(test_x.shape,test_y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert the training features and labels into numpy array to feed the CNN during testing\ntest_x = test_x.values\n\ntest_y = test_y.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the data for CNN Model\n\n<br>\n<strong>For MNIST,</strong> we will be using two convolutional layers, each followed by a relu and a maxpool layers,\nand two fully connected layers. Strides for all convolutional layers are [1, 1, 1, 1].\n\n<br>\n\n<strong>The Architecture of the model looks like that:</strong>\n\n<img src = \"https://i.imgur.com/lAh42E5.jpg\">\n     \n\n\n## 1- The First step  to bulid the CNN is to Set up CNN weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the input and output placeholders\nx = tf.placeholder(tf.float32, [None, 28*28])\ny = tf.placeholder(tf.float32, [None, 10])\n\n#Apply 32 convolutions of window-size 5*5\nw1 = tf.Variable(tf.random_normal([5,5,1,32]))\nb1 = tf.Variable(tf.random_normal([32]))\n\n#Then Apply 32 more  convolutions of window-size 5*5\nw2 = tf.Variable(tf.random_normal([5,5,32,64]))\nb2 = tf.Variable(tf.random_normal([64]))\n\n#Then we introduced a fully-connected l ayer\nw3 = tf.Variable(tf.random_normal([7*7*64,1024]))\nb3 = tf.Variable(tf.random_normal([1024]))\n\n#Finaly, we define the variables for a fully-connected linear layer\nw_out = tf.Variable(tf.random_normal([1024,10]))\nb_out = tf.Variable(tf.random_normal([10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets Create dic to hold our  parameters to can get it after updated\nparameters = {\"W1\": w1, \"b1\": b1, \"W2\": w2, \"b2\": b2, \"W3\": w3, \"b3\": b3, \"WO\": w_out, \"bO\": b_out}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2- Define some helper functions\n<strong>we define a helper functions</strong> to perform a convolution, add a bias term, and\nthen an activation function. <strong>Together,</strong> these three steps form a convolution layer of the\nnetwork.\n\n<br>\n\n<strong>The helper functions are:</strong>\n<ul>\n    <li>Conv_layer</li> to create convolutional layer\n    <li>maxpool_layer</li>to create max-pool layer\n    </ul>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a convolutional layer\ndef conv_layer(x, w, b):\n    conv = tf.nn.conv2d(x, w, strides = [1,1,1,1], padding = 'SAME')\n    conv_with_b = tf.nn.bias_add(conv, b)\n    conv_out = tf.nn.relu(conv_with_b)\n    return conv_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a max-pool layer\ndef maxpool_layer(conv, k = 2):\n    return tf.nn.max_pool(conv, ksize = [1,k,k,1], strides = [1,k,k,1], padding = 'SAME')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using these helper functions tp create the Architecture of the CNN model\n<strong>we will create function called model(x) that build the CNN model </strong>\n<ul>\n    <li>The Function takes the input features, x</li>\n    <li>The Function returns the output classes\n    </ul>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(x):\n    #Reshape the features in the form(m,height,wideth,channels), and m represents the number of training examples\n    x_reshaped = tf.reshape(x, shape = [-1, 28, 28, 1])\n    \n    #Construct the first layer of convolution and max-pooling\n    conv_out1 = conv_layer(x_reshaped, w1, b1)\n    maxpool_out1 = maxpool_layer(conv_out1)\n    \n    #Construct the second layer of convolution and max-pooling\n    conv_out2 = conv_layer(maxpool_out1, w2, b2)\n    maxpool_out2 = maxpool_layer(conv_out2)\n    \n    #Finally, Construct the final fully connected layer\n    ##1.First flatten the output from the second layer\n    maxpool_reshaped = tf.reshape(maxpool_out2, [-1, w3.get_shape().as_list()[0]])\n    \n    ##2.Create the linear part of the fully connected layer\n    linear_part = tf.add(tf.matmul(maxpool_reshaped, w3), b3)\n    \n    ##3.Create the non-linear part of the fully connected layer \n    ##in other word,applay the activation function on the linear part\n    nonlinear_part = tf.nn.relu(linear_part)\n    \n    #Get the the output ten classses\n    output = tf.add(tf.matmul(nonlinear_part, w_out), b_out)\n    \n    return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3- Define some operations to measure the cost and accuracy\n\n<strong>some things you should know</strong>\n<ul>\n    <li><strong>Cross Entropy:</strong></li> Our loss function used for classifications problems\n    <li><strong>ADAM Optimizer:</strong></li> Our optimizer used instead of Gradient Descent\n    </ul>\n    \n    <br>\n## Note:\n\nWe cast  <strong>correct_pred</strong> to float to get decimal value not integer because we will divide integer by integer\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Construct the model\nmodel_op = model(x)\n\n#Define the classification loss function\ncost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = model_op, labels = y))\n\n#Define the training optimizer to minimize the loss function\ntrain_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n\n#Define the Calculation to get the accuracy of the model\n##1.first define the correct predictions between the model and the ground truth\ncorrect_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(y, 1))\n\n##2.Then Define the accuracy fomula which is the number of examples correctly classified over the total number of examples\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4- Process the data before feeding the CNN for training \n\n### we will process the data using:\n<ul>\n    <li><strong>One-hot encoding</strong></li> In digital circuits, one-hot refers to a group of bits among which the legal combinations of\nvalues are only those with a single high (1) bit and all the others low (0).\nIn this case, one-hot encoding means that if the output of the image is the digit 7, then the\noutput will be encoded as a vector of 10 elements with all elements being 0, except for the\nelement at index 7 which is 1.\n    <li><strong>Normalizing the data</strong></li> we normalize the data to get fixed range, between 0 and 1, and thus faster training\n    <ul>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding the output using one_hot() function\ndef one_hot(labels):\n    labels_ = np.zeros((60000, 10))\n    labels_[np.arange(60000), labels] = 1\n    labels_ = np.array(labels_)\n    return labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing the features in the training and testing data\ntr_x = tr_x /255\ntest_x = test_x /255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5- Training the classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_train(parameters):\n    sess = tf.Session()\n    \n    #important to initialize the variables in order to use it \n    sess.run(tf.global_variables_initializer())\n    \n    onehot_labels = one_hot(tr_y)\n    batch_size = 256\n    #Loop through 1000 epochs\n    for i in range(0, 1000):    \n        #Train the network in batchse\n        for j in range(0, 60000, batch_size):\n            batch_features = tr_x[j:j+batch_size, :]\n            batch_onehot_labels = onehot_labels[j:j+batch_size, :]\n            sess.run(train_op, feed_dict = {x: batch_features, y: batch_onehot_labels})\n            cost_ = sess.run(cost, feed_dict = {x: batch_features, y: batch_onehot_labels})\n            accuracy_ = sess.run(accuracy, feed_dict = {x: batch_features, y: batch_onehot_labels})\n            \n            if j % 2048 == 0:\n                print(\"At j:{0}, the accuracy:{1}\".format(j, accuracy_))\n        print(\"Reached epoch\",i ,\"cost J = \", cost_)\n        \n    # lets save the parameters in a variable\n    parameters = sess.run(parameters)\n    print(\"\\n\\nParameters have been trained!\") \n    return parameters,sess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters,sess = model_train(parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}