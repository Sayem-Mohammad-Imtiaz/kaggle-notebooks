{"cells":[{"metadata":{},"cell_type":"markdown","source":"1-Import and read the video, extract frames from it, and save them as images\n\n2-Label a few images for training the model (Don’t worry, I have done it for you)\n\n3-Build our model on training data\n\n4-Make predictions for the remaining images\n\n5-Calculate the screen time of both TOM and JERRY"},{"metadata":{},"cell_type":"markdown","source":"### Let us start with importing all the necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2     # for capturing videos\nimport math   # for mathematical operations\nimport matplotlib.pyplot as plt    # for plotting the images\n%matplotlib inline\nimport pandas as pd\nfrom keras.preprocessing import image   # for preprocessing the images\nimport numpy as np    # for mathematical operations\nfrom keras.utils import np_utils\nfrom skimage.transform import resize   # for resizing images","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step – 1: Read the video, extract frames from it and save them as images"},{"metadata":{},"cell_type":"markdown","source":"Now we will load the video and convert it into frames. We will first capture the video from the given directory using the VideoCapture() function, and then we’ll extract frames from the video and save them as an image using the imwrite() function. Let’s code it"},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nvideoFile = \"../input/video-classification-tutorial/Tom and jerry.mp4\"\ncap = cv2.VideoCapture(videoFile)   # capturing the video from the given path\nframeRate = cap.get(5) #frame rate\nx=1\nwhile(cap.isOpened()):\n    frameId = cap.get(1) #current frame number\n    ret, frame = cap.read()\n    if (ret != True):\n        break\n    if (frameId % math.floor(frameRate) == 0):\n        filename =\"frame%d.jpg\" % count;count+=1\n        cv2.imwrite(filename, frame)\ncap.release()\nprint (\"Done!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once this process is complete, ‘Done!’ will be printed on the screen as confirmation that the frames have been created.\n\nLet us try to visualize an image (frame). We will first read the image using the imread() function of matplotlib, and then plot it using the imshow() function."},{"metadata":{"trusted":true},"cell_type":"code","source":"img = plt.imread('./frame0.jpg')   # reading image using its name\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting excited, yet?\n\nThis is the first frame from the video. We have extracted one frame for each second, from the entire duration of the video. Since the duration of the video is 4:58 minutes (298 seconds), we now have 298 images in total.\n\nOur task is to identify which image has TOM, and which image has JERRY. If our extracted images would have been similar to the ones present in the popular Imagenet dataset, this challenge could have been a breeze. How? We could simply have used models pre-trained on that Imagenet data and achieved a high accuracy score! But then where’s the fun in that?\n\nWe have cartoon images so it’ll be very difficult (if not impossible) for any pre-trained model to identify TOM and JERRY in a given video."},{"metadata":{},"cell_type":"markdown","source":"### Step – 2: Label a few images for training the model"},{"metadata":{},"cell_type":"markdown","source":"So how do we go about handling this? A possible solution is to manually give labels to a few of the images and train the model on them. Once the model has learned the patterns, we can use it to make predictions on a previously unseen set of images.\n\nKeep in mind that there could be frames when neither TOM nor JERRY are present. So, we will treat it as a multi-class classification problem. The classes which I have defined are:\n\n\n0 – neither JERRY nor TOM\n\n1 – for JERRY\n\n2 – for TOM\n\nDon’t worry, Go ahead and download the mapping.csv file which contains each image name and their corresponding class (0 or 1 or 2)."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/video-classification-tutorial/mapping.csv')     # reading the csv file\ndata.head()      # printing first five rows of the file","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mapping file contains two columns:\n\nImage_ID: Contains the name of each image\n\nClass: Contains corresponding class for each image\n\nOur next step is to read the images which we will do based on their names, aka, the Image_ID column."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [ ]     # creating an empty array\nfor img_name in data.Image_ID:\n    img = plt.imread('' + img_name)\n    X.append(img)  # storing each image in array X\nX = np.array(X)    # converting list to array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tada! We now have the images with us. Remember, we need two things to train our model:\n\n\nTraining images, and\n\nTheir corresponding class\n\nSince there are three classes, we will one hot encode them using the to_categorical() function of keras.utils."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.Class\ndummy_y = np_utils.to_categorical(y)    # one hot encoding Classes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be using a **VGG16** pretrained model which takes an input image of shape (224 X 224 X 3). Since our images are in a different size, we need to reshape all of them. We will use the resize() function of skimage.transform to do this."},{"metadata":{"trusted":true},"cell_type":"code","source":"image = []\nfor i in range(0,X.shape[0]):\n    a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3\n    image.append(a)\nX = np.array(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the images have been reshaped to 224 X 224 X 3. But before passing any input to the model, we must preprocess it as per the model’s requirement. Otherwise, the model will not perform well enough. Use the preprocess_input() function of **keras.applications.vgg16** to perform this step."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import preprocess_input\nX = preprocess_input(X)      # preprocessing the input data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need a validation set to check the performance of the model on unseen images. We will make use of the train_test_split() function of the sklearn.model_selection module to randomly divide images into training and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, dummy_y, test_size=0.3, random_state=42)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Building the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import Dense, InputLayer, Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now load the VGG16 pretrained model and store it as base_model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will make predictions using this model for X_train and X_valid, get the features, and then use those features to retrain the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = base_model.predict(X_train)\nX_valid = base_model.predict(X_valid)\nX_train.shape, X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The shape of X_train and X_valid is (208, 7, 7, 512), (90, 7, 7, 512) respectively. In order to pass it to our neural network, we have to reshape it to 1-D."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(208, 7*7*512)      # converting to 1-D\nX_valid = X_valid.reshape(90, 7*7*512)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now preprocess the images and make them zero-centered which helps the model to converge faster."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = X_train/X_train.max()      # centering(normalized) the data\nX_valid = X_valid/X_train.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will build our model. This step can be divided into 3 sub-steps:\n\n1-Building the model\n\n2-Compiling the model\n\n3-Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# i. Building the model\nmodel = Sequential()\nmodel.add(InputLayer((7*7*512,)))    # input layer\nmodel.add(Dense(units=1024, activation='sigmoid')) # hidden layer\nmodel.add(Dense(3, activation='softmax'))    # output layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a hidden layer with 1,024 neurons and an output layer with 3 neurons (since we have 3 classes to predict). Now we will compile our model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the final step, we will fit the model and simultaneously also check its performance on the unseen images, i.e., validation images:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train, y_train, epochs=100, validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see it is performing really well on the training as well as the validation images. We got an accuracy of around 92% on unseen images. And this is how we train a model on video data to get predictions for each frame."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}