{"cells":[{"metadata":{},"cell_type":"markdown","source":"In the latest version of the dataset, I have included two separate files:\n\n1. train.csv containing 45000 rows for training\n2. valid.csv containing 15000 rows for validation\n\nI did this for a more clear comparison between notebooks and their models.\n\nThis notebook is an example of conforming to this change. It is completely based on the following notebook:\n\nhttps://www.kaggle.com/heyytanay/stack-overflow-qa-classification-87-acc"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_metric(clf, testX, testY, name):\n    \"\"\"\n    Small function to plot ROC-AUC values and confusion matrix\n    \"\"\"\n    styles = ['bmh', 'classic', 'fivethirtyeight', 'ggplot']\n\n    plt.style.use(random.choice(styles))\n    plot_confusion_matrix(clf, testX, testY)\n    plt.title(f\"Confusion Matrix [{name}]\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing and Some EDA"},{"metadata":{},"cell_type":"markdown","source":"Read the data and don't use the low quality edit data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = pd.read_csv(\"../input/60k-stack-overflow-questions-with-quality-rate/train.csv\")\ndata2 = pd.read_csv(\"../input/60k-stack-overflow-questions-with-quality-rate/valid.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the open questions are grouped under a single class (1), while the closed one is grouped under (0)"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = data.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata['Y'] = data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\n\ndata2 = data2.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata2['Y'] = data2['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"labels = ['Open Questions', 'Low Quality Question - Close', 'Low Quality Question - Edit']\nvalues = [len(data[data['Y'] == 2]), len(data[data['Y'] == 0]), len(data[data['Y'] == 1])]\nplt.style.use('classic')\nplt.figure(figsize=(16, 9))\nplt.pie(x=values, labels=labels, autopct=\"%1.1f%%\")\nplt.title(\"Target Value Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's join the title and the body of the text data so that we can use both of them in our classification"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data['text'] = data['Title'] + ' ' + data['Body']\ndata = data.drop(['Title', 'Body'], axis=1)\n\ndata2['text'] = data2['Title'] + ' ' + data2['Body']\ndata2 = data2.drop(['Title', 'Body'], axis=1)\n\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean the data\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\ndata['text'] = data['text'].apply(clean_text)\ndata2['text'] = data2['text'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the Data\nLet's now split the dataset into training and validation sets"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Training Sets\ntrain = data\ntrainX = train['text']\ntrainY = train['Y'].values\n\n# Validation Sets\nvalid = data2\nvalidX = valid['text']\nvalidY = valid['Y'].values\n\nassert trainX.shape == trainY.shape\nassert validX.shape == validY.shape\n\nprint(f\"Training Data Shape: {trainX.shape}\\nValidation Data Shape: {validX.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing the Data\nLet's vectorize the data so it's in the numerical format"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Load the vectorizer, fit on training set, transform on validation set\nvectorizer = TfidfVectorizer()\ntrainX = vectorizer.fit_transform(trainX)\nvalidX = vectorizer.transform(validX)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\nLet's start with different non-deep learning approaches for this task."},{"metadata":{},"cell_type":"markdown","source":"## 1. Logistic Regression\nLet's first start with our good old, Logistic Regression!"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nlr_classifier = LogisticRegression(C=1.)\nlr_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Logsitic Regression Classifier is: {(lr_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(lr_classifier, validX, validY, \"Logistic Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Multinomial Naive Bayes\nLet's now switch to the naive the bayes, the NAIVE BAYES!"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nnb_classifier = MultinomialNB()\nnb_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Naive Bayes Classifier is: {(nb_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Naive Bayes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Random Forest Classifier\nLet's now enter the forest with the Random Forest Classifier and see where it takes us!"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Random Forest Classifier is: {(rf_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Decision Tree Classifier\nLet's now take some decisions using the Decision Tree Classifer"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Decision Tree Clf. is: {(dt_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. KNN Classifier\nWe now are going to use KNN Classifier for this task."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nkn_classifier = KNeighborsClassifier()\nkn_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of KNN Clf. is: {(kn_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. XGBoost\nFinally, let's use the XGBoost Classifier and then we'll compare all the different classifiers so far"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Define and fit the classifier on the data\nxg_classifier = XGBClassifier()\nxg_classifier.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of XGBoost Clf. is: {(xg_classifier.score(validX, validY))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Also plot the metric\nplot_metric(xg_classifier, validX, validY, \"XGBoost Classifier\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}