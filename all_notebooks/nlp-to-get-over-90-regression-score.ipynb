{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Understanding\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/us-airbnb-open-data/AB_US_2020.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will exclude the rows having Null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.dropna()\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_columns1= [df1[col].nunique() for col in df1]\ncount=pd.DataFrame({\"Unique\" : unique_columns1})\ncount['Column']=list(df1.columns)\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.name = df1.name.astype(str)\ndf1.name=df1.name.str.lower()\ndf1.name=df1.name.replace(['-','@','$','&','!'],'',regex=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To replace special characters from name column"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\ncomment_words = '' \nstopwords = set(STOPWORDS) \nfor val in df1.name: \n    val = str(val) \n    tokens = val.split() \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower()       \n    comment_words += \" \".join(tokens)+\" \"  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)                        \nplt.figure(figsize = (8, 12), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)   \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading stop words to exclude them from the text in columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"unigrams\"] = df1[\"name\"].apply(nltk.word_tokenize)\ndf1[\"unigrams\"] =df1[\"unigrams\"].apply(lambda x: [item for item in x if item not in stop_words])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrslt=pd.Series(np.concatenate([x for x in df1.unigrams])).value_counts()\nrslt = pd.DataFrame({'ngrams': list(rslt.keys()),\n                   'count': list(rslt[:])})\nrslt=rslt[rslt.ngrams != '.' ]\nrslt=rslt.head(20)\nrslt1=rslt['ngrams'][1:20]\nrslt1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"20 most frequently observed words in Name column"},{"metadata":{},"cell_type":"markdown","source":"# Data Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef inner(text):    \n    for word in text.split(' '):\n        for c in rslt1:\n            if word==c:\n               return c\n    return 'Null'       \nc1=[]\nfor text in df1['name']:\n    c1.append(inner(text))\ndf1['Name1']=c1\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df1[['Name1','neighbourhood_group','room_type',\n        'city','price']]\n\nplt.hist(df2['price'], bins =20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"considering those cases with price less than $ 1000"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df2[(df2['price']<1000)  ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnames = list(df2.columns)\nfig, axs = plt.subplots(4, 1,figsize=(15,15))\nplt.subplots_adjust(left = 0.25, right = 0.9, bottom = 0.1, top = 1.5,\n                        wspace = 0.2, hspace = 1.2)\n\nfor column_name in names[:-1]: \n    ax = axs[names.index(column_name)]\n    cd= df2[column_name].value_counts()[:5]\n    cd1= pd.DataFrame({'label': list(cd.keys()),\n                   'count': list(cd[:])})\n    s=cd1['label']\n    \n    for s1 in s:\n        # Subset to the airline\n        subset = df2[df2[column_name] == s1]\n        \n        # Draw the density plot\n        sns.distplot(subset['price'], hist = False, kde = True,\n                     kde_kws = {'shade': True,'linewidth': 1},ax=ax,\n                     label = s1).set(xlim=(0))\n        ax.set_xlabel( ' Target Price')\n        ax.set_title(column_name + ' density')\n        ax.grid('on')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Density distribution for top 5 frequencies of Name1, neighbourhood group,room type and city against Price."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_group = df2.groupby('Name1').mean()  \ndf_group.sort_values(by=['price'], ascending=False) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparison of distribution of price for top two mean price vs bottom two mean price based on text (name)"},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['Name1','price']\n\n\nfor column_name in names[:-1]: \n    cd=['ocean','beach','Null','cozy','room']\n    cd1= pd.DataFrame(cd,columns=['label'])\n    s=cd1['label']\n    \n    for s1 in s:\n        # Subset to the airline\n        subset = df2[df2[column_name] == s1]\n        \n        # Draw the density plot\n        ax=sns.distplot(subset['price'], hist = False, kde=True,\n                     kde_kws = {'shade': True,'linewidth': 2},\n                     label = s1).set(xlim=(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=[]\nfor c in df1.last_review:\n    x1.append([c[:2],c[3:5],c[6:8]])\ndt = pd.DataFrame(x1, columns=['day', 'month', 'year'],index=df1.index)\ndt[['day', 'month', 'year']] = dt[['day', 'month', 'year']].astype(int)\ndt.year=dt.year+2000\n\nimport datetime\n\ndt['rev']= pd.to_datetime(dt.year*10000+dt.month*100+dt.day,format='%Y%m%d')\ndt['rev']= dt['rev'].dt.date\ndt['now']=datetime.date.today()\ndt['dif']=dt.now-dt.rev\ndf1['days']=dt['dif']\ndf1['days']=(df1['days']/np.timedelta64(1, 'D')).astype(np.int64)\n\ndf1=df1[(df1['price']<1000)]\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of days since last review. For cases where "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.drop([  'id','host_id','latitude','longitude','last_review'], axis = 1)\ncorr = df1.corr()\nprint(corr)\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = pd.get_dummies(df1[['neighbourhood_group','room_type',\n        'city','Name1']])\n\ndf11 = pd.concat([df1, dummy], axis = 1)\nx = df11.drop(['Name1','neighbourhood_group','room_type',\n        'city','price','name','host_name','neighbourhood','unigrams'], axis = 1)\nx= (x-np.min(x)) / (np.max(x)-np.min(x))\ny = df11['price']\ny= (y-np.min(y)) / (np.max(y)-np.min(y))\nx.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"                   \nfrom sklearn.model_selection import train_test_split\nx_train , x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2 , random_state = 21)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression().fit(x_train, y_train)\nreg.score(x_train, y_train)\ny1=reg.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y1)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, y1)\nprint(\"R2:\", r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y1)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nz = np.polyfit(y_test, y1, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='red')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\nmodel=regressor.fit(x, y)  \ny1 = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse = mean_squared_error(y_test, y1)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, y1)\nprint(\"R2:\", r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y1)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nz = np.polyfit(y_test, y1, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='red')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_nodes = []\nmax_depths = []\nfor ind_tree in model.estimators_:\n    n_nodes.append(ind_tree.tree_.node_count)\n    max_depths.append(ind_tree.tree_.max_depth)\n    \nprint(f'Average number of nodes {int(np.mean(n_nodes))}')\nprint(f'Average maximum depth {int(np.mean(max_depths))}')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}