{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/hotel-booking-demand/hotel_bookings.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making a copy of our data\ndata = df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting an overview of the number of unique values present in each of the columns\ndata.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using the above results, we understand that:-\n##### 1) The dataset captures data of two hotels. \n##### 2) The column arrival_date_year has 3 unique values i.e. it captures data of 3 years of bookings made by their customers.\n##### 3) The dataset deals with international travel as well, as the \"country\" column has 177 unique values.\n#### More inferences can be made out of this but we'll have a detailed look as we move forward with the EDA."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Names of the two hotels\ndata.hotel.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking missing values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"null_check = data.isnull().sum()\ncols_with_missing_values = null_check[null_check != 0]\ncols_with_missing_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As there are only 4 null values for 'children' column and 488 null values for 'country' column (out of 1,19,390 entries), we can simply drop those rows without losing much information.\n\n#### For 'agent' and 'company' columns, we can do a little more digging and decide."},{"metadata":{"trusted":true},"cell_type":"code","source":"# deleting rows with null values for 'children' and 'country'\ndata.dropna(subset = ['children', 'country'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''to preserve important information for 'agent' and 'company' columns, \n  we'll replace the null values with 0 since both columns contain numerical data.'''\ndata.agent.fillna(0, inplace = True)\ndata.company.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#verifying whether we have handled the null values or not\nnull_check = data.isnull().sum()\ncols_with_missing_values = null_check[null_check != 0]\ncols_with_missing_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using the above result, we have verified that the dataset doesn't have any missing values now."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of Countries with Maximum Number of Bookings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the frequency of booking of each country into a Data Frame\ncountry_dist = data.groupby('country').count()['hotel']\ncountry_dist = pd.DataFrame(country_dist)\n\n#The Data Frame has columns \"Country\" and \"No. of Bookings\"\ncountry_dist['Country'] = country_dist.index\ncountry_dist = country_dist.rename(columns = {'hotel': 'No. of Bookings'})\n\n#Sorting the DataFrame in descending order and getting only those countries which have bookings more than 1000.\ncountry_dist = country_dist.sort_values(by = 'No. of Bookings', ascending = False)\npopular_country_dist = country_dist[country_dist['No. of Bookings'] > 1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_country_dist.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that PRT has unusually high number of bookings i.e. approx 36k more number of bookings than the second highest value for GBR.\n\n### Due to this reason, identifying PRT as an outlier, we exclude it from our barplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = popular_country_dist['Country'][1:], y = popular_country_dist['No. of Bookings'][1:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## No. of Bookings Monthly (Cancelled vs Successful) "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the monthly frequency of cancelled and successful bookings \nmonthly_dist = data[data.is_canceled == 0].groupby('arrival_date_month').count()['hotel']\nmonthly_cancelled_dist = data[data.is_canceled == 1].groupby('arrival_date_month').count()['hotel']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Storing the data into two dataframes and concatenating both of them to get a single dataframe with columns \"No. of Bookings\"\n\"Month\" and \"is_canceled\"'''\nmonthly_dist = pd.DataFrame(monthly_dist)\nmonthly_cancelled_dist = pd.DataFrame(monthly_cancelled_dist)\n\nmonthly_dist = monthly_dist.rename(columns = {\"hotel\" : \"No. of Bookings\"})\nmonthly_cancelled_dist = monthly_cancelled_dist.rename(columns = {\"hotel\" : \"No. of Bookings\"})\n\nmonthly_dist['is_canceled'] = 'No'\nmonthly_cancelled_dist['is_canceled'] = 'Yes'\n\nmonthly_dist[\"Month\"] = monthly_dist.index\nmonthly_cancelled_dist[\"Month\"] = monthly_cancelled_dist.index\n\nmonthly_freq = pd.concat([monthly_dist, monthly_cancelled_dist])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months_in_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.factorplot(\"Month\", \"No. of Bookings\", col=\"is_canceled\", data=monthly_freq, kind=\"bar\", order = months_in_order)\nprint(type(ax))\n#ax.set(title = 'Monthly Distribution of Bookings acc. to the status of booking.')\nax.set_xticklabels(rotation=45)\nax1,ax2 = ax.axes[0]\nax1.axhline(7000, ls = '--', linewidth = 2)\nax2.axhline(5000, ls = '--', linewidth = 2)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lead Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_time = data['lead_time']\nlead_time = pd.DataFrame(sorted(lead_time, reverse = True), columns = ['Lead'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(lead_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We have the following observations:\n####     1) Most of the bookings in the dataset have a lead time between 0 to 100 days.\n####     2) We can see a huge peak where the lead time is less than 10-20 days. We can infer that a large part of people arrived at the stay in less than a month from their booking time.\n####     2) As the lead time increases, the number of bookings reduce, and there are very few bookings which have a lead time of more than a year ( > 365 )."},{"metadata":{},"cell_type":"markdown","source":"### Therefore, plotting the Lead Time distribution when:\n#### 1) Distribution when Lead Time < 100 Days.\n#### 2) Distribution when Lead Time > 100 Days and < 365 Days.\n#### 3) Distribution when Lead Time is more than a year."},{"metadata":{"trusted":true},"cell_type":"code","source":"a4_dims = (21, 6)\nfig, ax = plt.subplots(1,3,figsize=a4_dims)\nsns.distplot(lead_time[lead_time['Lead'] < 100], ax = ax[0])\nsns.distplot(lead_time[(lead_time['Lead'] > 100) & (lead_time['Lead'] < 365)], ax = ax[1])\nsns.distplot(lead_time[lead_time['Lead'] > 365], ax = ax[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}