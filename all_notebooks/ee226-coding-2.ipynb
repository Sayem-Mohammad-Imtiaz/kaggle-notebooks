{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport time\n\nwindow_size = 2000\nmax_same_bucket = 2\ntime_loc = 10000\n\nbuckets = []\n\ndef merge():\n    for i in range(len(buckets)-1,max_same_bucket-1,-1):\n        if buckets[i]['bit_sum']==buckets[i-max_same_bucket]['bit_sum']:\n            buckets[i-max_same_bucket]['timestamp']=buckets[i-max_same_bucket+1]['timestamp']\n            buckets[i-max_same_bucket]['bit_sum']+=buckets[i-max_same_bucket+1]['bit_sum']\n            del buckets[i-max_same_bucket+1]\n\ndef delete_expire(timestamp):\n    if len(buckets)>0 and timestamp-window_size==buckets[0]['timestamp']:\n        del buckets[0]\n\ndef DGIM():\n    bit_sum = 0\n    start_time = time.time()\n    with open(\"../input/coding2/stream_data.txt\") as f:\n        for i in range(time_loc):\n            tmp = f.read(2)\n            if tmp:\n                delete_expire(i+1)\n                if int(tmp.strip('\\t'))==1:\n                    new_bucket = {\"timestamp\":i+1,\"bit_sum\":1}\n                    buckets.append(new_bucket)\n                    merge()\n    for i in range(len(buckets)):\n        bit_sum+=buckets[i]['bit_sum']\n    bit_sum-=buckets[0]['bit_sum']/2\n    end_time = time.time()\n    return bit_sum, end_time-start_time\n\nbit_sum, cost_time = DGIM()\nprint(\"number of 1-bits: {}\".format(int(bit_sum)))\nprint(\"cost time: {}\".format(cost_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport time\n\nwindow_size = 2000\nmax_same_bucket = 2\ntime_loc = 10000\n\ndef count_precise():\n    bit_sum = 0\n    start_time = time.time()\n    with open(\"../input/coding2/stream_data.txt\") as f:\n        beg_loc = 0 if time_loc<=window_size else 2*(time_loc-window_size)\n        f.seek(beg_loc)\n        for i in range(time_loc if time_loc<=window_size else window_size):\n            tmp = f.read(2)\n            if tmp and int(tmp.strip('\\t'))==1:\n                bit_sum+=1\n    end_time = time.time()\n    return bit_sum, end_time-start_time\n\nbit_sum, cost_time = count_precise()\nprint(\"actual number of 1-bits: {}\".format(bit_sum))\nprint(\"actual cost time: {}\".format(cost_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DGIM will cost little more time because of the calculation process. However, it will save a lot of space and we could query the result online by using DGIM. The difference between the ground truth and the DGIM result is also small.","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport pandas as pd\nimport random\nimport numpy as np\n\nminhash_num = 100\nbands_num = 20\n\ndata=pd.read_csv(\"../input/coding2/docs_for_lsh.csv\",index_col='doc_id').to_numpy().transpose()\n\n\ndef generate_sig():\n    length = data.shape[0]\n    num_files = data.shape[1]\n    sig=np.zeros((minhash_num,num_files),dtype=np.int)\n    #print(sig.shape)\n    for i in range(minhash_num):\n        sig[i,:]=np.random.permutation(data).argmax(axis=0)\n    return sig\n    \ndef find_most_similar():\n    num_files = data.shape[0]\n    sim_x,sim_y=0,0\n    max_jac_sim = 0\n    for i in range(num_files):\n        for j in range(i,num_files):\n            cur_sim=0\n            if i!=j:\n                for k in range(minhash_num):\n                    if signature[k][i]==signature[k][j]:\n                        cur_sim+=1\n            if cur_sim>max_jac_sim:\n                max_jac_sim=cur_sim\n                sim_x=i\n                sim_y=j\n    print(sim_x)\n    print(sim_y)\n                    \n#print(data.shape[0])\nsignature=generate_sig()\n#for i in range(minhash_num):\n    #generate_one_sig()\n    #print(\"finish\",i)\n#print(signature)\n#find_most_similar()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport hashlib\n\n\ndef LSH(m,b,r):\n    hash_bucket={}\n    beg, end=0,r\n    cnt=0\n    while end<=m.shape[0]:\n        cnt+=1\n        for col in range(m.shape[1]):\n            hashfc= hashlib.md5()\n            s=str(m[beg: beg + r, col]) #+ str(cnt)\n            hashfc.update(s.encode())\n            tag=hashfc.hexdigest()\n            \n            if (tag,cnt) not in hash_bucket:\n                hash_bucket[(tag,cnt)]=[col]\n            elif col not in hash_bucket[(tag,cnt)]:\n                hash_bucket[(tag,cnt)].append(col)\n        beg+=r\n        end+=r\n    return hash_bucket\n        \n\nrow=minhash_num//bands_num\n#print(row,bands_num)\nsignature=np.array(signature)\nhash_bucket=LSH(signature,bands_num,row)\n#print(hash_bucket)\nres={}\nquery=0\nfor (key,i) in hash_bucket:\n    if query in hash_bucket[(key,i)]:\n        for j in hash_bucket[(key,i)]:\n            if j not in res:\n                res[j]=1\n            else:\n                res[j]+=1\nres_order=sorted(res.items(),key=lambda x:x[1],reverse=True)\nx=0\noutputs=[]\nfor i in range(len(res_order)):\n    if x>=30:\n        break\n    if res_order[i][0]!=query:\n        x+=1\n        outputs.append(res_order[i][0])\n    \n#print(len(res))\nprint(\"Predicted 30 most similar files:\")\nprint(outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score\nground_truth=[]\nfile0=data[:,0]\nfor i in range(30):\n    filex=data[:,outputs[i]]\n    ground_truth.append(jaccard_score(file0,filex))\nprint(\"Corresponding jaccard score:\")\nprint(ground_truth)","metadata":{},"execution_count":null,"outputs":[]}]}