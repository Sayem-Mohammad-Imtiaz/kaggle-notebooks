{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Word embeddings & analysis of Poetry\n* Digital humanities\n* Gutenberg corpus includes preamble/non poetry. Could be filtered.\n* Primarily English. \n* Data may further be cleaned by using lemmatization etc'. +- pretrained w2v models, multilingual. ([Example of loading a pretrained W2V model and finetuning it](https://www.kaggle.com/rtatman/fine-tuning-word2vec)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from gensim.utils import simple_preprocess\nfrom gensim.sklearn_api.phrases import PhrasesTransformer # phrases/ coallocations - https://radimrehurek.com/gensim/sklearn_api/phrases.html\nfrom gensim.sklearn_api import phrases\nfrom gensim.models.phrases import Phrases #, ENGLISH_CONNECTOR_WORDS\nfrom gensim.models import Word2Vec\n\n# import re\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interesting_words_list = [\"salt\",\"pepper\",\"spice\",\"spices\",\"herbs\",\"herbal\",\"sweet\",\"spicy\",\"salty\",\"moist\",\n                          \"paprika\",\"saffron\",\"mace\",\"lavender\",\"honey\",\"honeysuckle\",\"chile\",\"marjoram\",\"sugar\",\"tea\",\"mint\",\n                          \"taste\",\"smell\",\"aroma\",\n                          \"cinnamon\",\"cardamom\",\"peppercorn\",\"turmeric\",\"anise\",\"zaatar\",\n                          \"fork\",\"knife\",\"dish\",\"food\",\"plate\",\n                         \"basil\",\"cilantro\",\"chili\",\"cumin\",\"onion\",\"garlic\",\"dill\",\"horseradish\",\"radish\",\"mustard\",\"peppermint\",\"pepper\",\"sage\",\"vanilla\",\"wasabi\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/gutenberg-poetry-dataset/Gutenberg-Poetry.csv\",\n#                  nrows=1234,\n                 usecols=[\"s\"]).drop_duplicates().rename(columns={\"s\":\"text\"})\ndf = df.loc[df[\"text\"].str.split().str.len()>1]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## https://stackoverflow.com/questions/51049568/attributeerror-on-spacy-token-pos\n## we could also lemmatize\n# from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n\ndf[\"text\"] = df[\"text\"].apply(lambda x: simple_preprocess(x, deacc=True, max_len=50,min_len=2))\n# df.drop_duplicates(\"text\",inplace=True)\nprint(df.shape)\n# ## drop duplicates - on list\n# df = df[~df[\"text\"].apply(pd.Series).duplicated()]\n# print(df.shape)\n\n## phrases coallocation\nm = PhrasesTransformer(min_count=6,max_vocab_size=30000000)\n# df[\"text\"] = m.fit_transform(df[\"text\"].values)\nm.fit(df[\"text\"])\ndf[\"text\"]  = m.transform(df[\"text\"])\n\n\nsentences = df[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"].str.len().describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#an example sentence in the data\nprint(sentences.iloc[7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word2Vec\n#training the gensim on the data\n#Using the Cbow architecture for the word2Vec\n\nmodel_cbow = Word2Vec(sentences, min_count = 2, size = 200, workers = 3, window = 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Any example word vector\nprint('chief\\n:',model_cbow['chief']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarity of the words\nprint(model_cbow.similarity('chief', 'indian'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most similar words to a word\n* CBOW model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the 10 most similar words to indian:\\n')\nmodel_cbow.most_similar('indian')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model_cbow.most_similar(w, topn=13)])\n    except: pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining a tsne function to visualize\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\ndef plot_tsne(model, num):\n    labels = []\n    tokens = []\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    tsne = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 1500, random_state = 23) # orig 2500 n_iter\n    data = tsne.fit_transform(tokens[:num])\n    x = []\n    y = []\n    for each in data:\n        x.append(each[0])\n        y.append(each[1])\n    plt.figure(figsize = (13, 13))\n    for i in range(num):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i],\n                     xy = (x[i], y[i]),\n                     xytext = (5,2),\n                     textcoords = 'offset points',\n                     ha = 'right',\n                     va = 'bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualising the cbow architecture(only the first 120)\nplot_tsne(model_cbow, 120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's see how the skipgram model works on the data\nmodel_skipgram = Word2Vec(sentences, min_count = 2, size = 200, workers = 3, window = 6, sg = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computing the similarities of the words\nprint(model_skipgram.similarity('indian', 'chief'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### skipgram - most similar words"},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model_skipgram.most_similar(w, topn=13)])\n    except: pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the 10 most similar words to indian:\\n')\nmodel_skipgram.most_similar('indian')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualising the skipgram archtecture(only the first 100)\nplot_tsne(model_skipgram,100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### glove based model/embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the glove package for embeddings\n!pip install glove_python","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from glove import Corpus, Glove\ncorpus = Corpus()\ncorpus.fit(sentences, window = 5)\nglove = Glove(no_components = 150, learning_rate = 0.05)\nglove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = False)\nglove.add_dictionary(corpus.dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computing the similarities of the words\nprint(glove.most_similar('indian', number = 9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in glove.most_similar(w, number=13)])\n    except: pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now visualising first 80 words using tsne\ndef plot_tsne_glove(model, num):\n    labels = []\n    tokens = []\n    for word in model.wv.vocab:\n        tokens.append(glove.word_vectors[glove.dictionary[word]])\n        labels.append(word)\n    tsne = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 1500, random_state = 23) # was n_iter 2500 originally\n    data = tsne.fit_transform(tokens[:num])\n    x = []\n    y = []\n    for each in data:\n        x.append(each[0])\n        y.append(each[1])\n    plt.figure(figsize = (12, 12))\n    for i in range(num):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i],\n                     xy = (x[i], y[i]),\n                     xytext = (5,2),\n                     textcoords = 'offset points',\n                     ha = 'right',\n                     va = 'bottom')\n    plt.title('Word vectorization using Glove')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne_glove(model_skipgram, 120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Finetuning a pretrained model\n* Example : https://www.kaggle.com/rtatman/fine-tuning-word2vec\n* We will use conceptnet numberbatch embeddings - can be downloaded manually, via [Gensim's downloader api](https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html), or imported from one of the kaggle datasets (as done here)\n    * https://www.kaggle.com/joeskimo/conceptnet\n    * https://www.kaggle.com/blackitten13/gensim-embeddings-dataset\n    * Example numberbatch loading + \"cleaning\" code snippets: https://gist.github.com/ixaxaar/9fc209e7ba1c88b87f287028396609f1"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.models import Word2Vec \nfrom gensim.models import KeyedVectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load pretrained conceptnet numberatch + Clean it's format (\"remove \"/c/en\" and similar prefixes of language?)\n## takes ~ 2 minutes to load\nmodel = KeyedVectors.load_word2vec_format(\"../input/conceptnet/numberbatch-en-19.08.txt\",binary=False, unicode_errors='ignore',limit=800000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar('indian')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Pretrained model, without finetuning on poetry:\")\nfor w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model.most_similar(w, topn=10)])\n    except: pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2 = Word2Vec(size=300, min_count=1)\nmodel_2.build_vocab(sentences)\ntotal_examples = model_2.corpus_count\nmodel_2.build_vocab([list(model.vocab.keys())], update=True)\n\nmodel_2.intersect_word2vec_format(\"../input/conceptnet/numberbatch-en-19.08.txt\",binary=False, unicode_errors='ignore')\nmodel_2.train(sentences, total_examples=total_examples, epochs=model_2.iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Finetuned model on poetry:\")\nfor w in interesting_words_list:\n    try: print(w,\"\\n\",[i[0] for i in model_2.most_similar(w, topn=13)])\n    except: pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.train(sentences, total_examples=total_examples, epochs=model_2.iter)\n### AttributeError: 'Word2VecKeyedVectors' object has no attribute 'train' \n# print(\"Pretrained model, without finetuning on poetry:\")\n# for w in interesting_words_list:\n#     try: print(w,\"\\n\",[i[0] for i in model.most_similar(w, topn=12)])\n#     except: pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}