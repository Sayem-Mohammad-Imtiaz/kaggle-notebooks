{"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"9ea99ad7-2317-450b-a8fe-8f18bb7737ff","_uuid":"baf68c0b5d7bf3d9d65885dfdf41b4d31f72e6e1"},"source":"Welcome to the third data of the Regression 5-Day Challenge! Over the last two days of the challenge, we've:\n\n* [Learned about different types of regression (Poisson, linear and logistic) and when to use them](https://www.kaggle.com/rtatman/regression-challenge-day-1)\n* [Learned how to fit & evaluate a model with diagnostic plots](https://www.kaggle.com/rtatman/regression-challenge-day-2)\n\nToday, we're going to learn about how to interpret a model and how to tell whether our input variable actually does have a strong relationship to our output variable. To figure this out, we'll need to learn about a new concept: coefficients.\n\n## What's a coefficient?\n\nA coefficient expresses that strength of the relationship between the input value and the output value. You can read a coefficient as \"for every increase of the input value by one unit, the output value will change by [whatever number the coefficient is] units\". In linear regression, you can think of the coefficient as the slope of the line. Today we're going to fit a model and learn how to see and interpret its coefficients.\n___\n\n<center>\n[**You can check out a video that goes with this notebook by clicking here.**](https://www.youtube.com/embed/4OnNnu6GqCs)","cell_type":"markdown"},{"metadata":{"_cell_guid":"89e4edf9-bc14-4f85-9635-495e18301a3d","_uuid":"0afe412728ed22e2e953afecc2e038839e674f90"},"source":"## Example: Can we predict how likely a hard drive is to fail?\n___\n\nLet's see if we can predict the probability that a hard drive will fail based on the Read Error Rate (which is in the column smart_1_normalized in this dataset). Intuitively, I'd expect that a hard drive is more likely to fail if the Read Error Rate is higher, i.e. there are more read errors. \n\nFirst we need to set up our environment, though.","cell_type":"markdown"},{"metadata":{"_cell_guid":"673245d4-89c5-4ff0-b55a-592a8fb02841","_uuid":"37dd079652c5784e2ed69715b8e795cd8277a2bd","_kg_hide-output":true},"source":"```\n# read in libraries\nlibrary(tidyverse)\nlibrary(boot)\n\n# data. I'll use the hard_drives dataset & the cameras dataset is for you \n# to use as part of your exerise\nhard_drives <- read_csv(\"../input/hard-drive-test-data/harddrive.csv\", n_max = 100000)\ncameras <- read_csv(\"../input/1000-cameras-dataset/camera_dataset.csv\")\n```","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"dd42d46a-351f-4b8b-b217-4701a68b8984","_uuid":"f2894ecce52a62817269a36d02f502e5940cf304","collapsed":true},"source":"import numpy as np\nimport pandas as pd \nfrom pandas import read_csv\n\n# Plotting libraries\nimport seaborn as sns\nfrom ggplot import *","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"2bcca6b5-eff2-4e2f-8b80-4b00bf93ea19","_uuid":"414f4280732a5503bc6c4ebc772d0966914c4344","collapsed":true},"source":"hard_drive = read_csv(\"../input/hard-drive-test-data/harddrive.csv\")\ncameras = read_csv(\"../input/1000-cameras-dataset/camera_dataset.csv\")","cell_type":"code"},{"metadata":{"_cell_guid":"86b83dec-72bf-452d-85cd-ec88c66c02db","_uuid":"69a2f2e36ac01d6247fc4582897a0c021729c094"},"source":"Because \"failure\" is a categorical variable with two values, I'm going to use the \"binomial\" family to fit a logistic regression model. In this dataset 1 indicates that a hard drive did fail and 0 indicates that it didn't.","cell_type":"markdown"},{"metadata":{"_cell_guid":"3d4945cf-0652-404e-b9c3-a10a6c72abf3","_uuid":"94a4127956125274e8e0e23465907a648075062f","collapsed":true,"scrolled":true},"source":"```\n# predict probability of failure give the read error rate\nmodel <- glm(failure ~ smart_1_normalized, data = hard_drives, family = \"binomial\")\n```","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"ecab614c-84b3-4f7d-bb9d-f94c5151f842","_uuid":"cc0ad07c29b27c8be198ce080128c5e964b6497e","collapsed":true},"source":"X = hard_drive['smart_1_normalized'][:, np.newaxis]\nY = hard_drive['failure']\n\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression()\nresult = reg.fit (X,Y)\nprint(result.intercept_, result.coef_)","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4458f4e0-b30b-43c2-adc6-2db6983d3db9","_uuid":"6a8c5b9b3f29109ca0b77e5b368ae65b809558bf","collapsed":true},"source":"import statsmodels.api as sm\n# Note the swap of X and y\nmodel = sm.OLS(Y, X)\nresults = model.fit()\n# Statsmodels gives R-like statistical output\nprint(results.summary())\n","cell_type":"code"},{"metadata":{"_cell_guid":"b6e1afe4-6339-4c77-a1e9-2f2785937df8","_uuid":"b39b87f65f9a40431980665945cce298986b7a26"},"source":"Great, so now we have a model! But how can we tell what's going on? We can do this using the \"summary\" function.\n\n> **Note:** I'd recommend doing diagnostic plots at this point, but this is a pretty big model and they took fooooorrrrever to plot, so I'm skipping them here. Don't be lazy like I'm being! :P","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"455bea88-a519-4a8d-a025-78c26daeb6fe","_uuid":"eb6ecb4ff6258694b9973e85762cad8271ba656e","collapsed":true},"source":"# summary of the model\nsummary(model)","cell_type":"code"},{"metadata":{"_cell_guid":"ba4ec79a-8d22-4c22-abe1-755c104cfc8a","_uuid":"0282c4f8f71b3f4c7fb7ba555e1554eb52cc4532"},"source":"Wow, that's a lot of output! Let's take it a bit at a time and go through it together. \n\n**Call**: This is just the call that you made to the function. It will be the exact same code you typed into R. This can be helpful for seeing if you made any typos.\n\n**(Deviance) Residuals:** You can pretty much ignore these for logistic regression. For Poisson or logistic regression, you want these to be more-or-less normally distributed (which is the same thing the top two diagnostic plots are checking). You can check this by seeing if the absolute value of 1Q and 3Q are close(ish) to each other, and if the median is close to 0. The mean is not shown because it's always 0. If any of these are super off then you probably have some weird skew in your data. (This will also show up in your diagnostic plots!)\n\n**Coefficients:** This is the meat of the output.\n\n* **Intercept**: For Poisson and linear regression, this is the predicted output when all our inputs are 0. For logistic regression, this value will be further away from 0 the bigger the difference between the number of observation in each class.. The the standard error represents how uncertain we are about this (lower is better). In this case, because our intercept is far from 0 and our standard error is much smaller than the intercept, we can be pretty sure that one of our classes (failed or didn't fail) has a lot of more observations in it. (In this case it's \"didn't fail\", thankfully!)\n* **Various inputs** (each input will be on a different line): This estimate represents how much we think the output will change each time we increase this input by 1. The bigger the estimate, the bigger the effect of this input variable on the output. The standard error is how certain about it we are. Usually, we can be pretty sure an input is informative is the standard error is 1/10 of the estimate.  So in this case we're pretty sure the intercept is important.\n* **Signif. Codes**: This is a key to the significance of each :input and the intercept. These are only correct if you only ever fit one model to your data. (In other words, they’re great for experimental data if you from the start which variables you’re interested in and not as informative for data analysis or variable selection.)\n\n> **Wait, why can't we use statistical significance?** You can, I just wouldn't generally recommend it. In data science you'll often be fitting multiple models using the same dataset to try and pick the best model. If you ever run more than one test for statistical significance on the same dataset, you need to adust your p-value to make up for it. You can think about it this way: if you decide that you'll accept results that are below p = 0.05, you're basically saying that you're ok with being wrong one in twenty times. If you then do five tests, however, and for each one there's a 1/20 chance that you'll be wrong, you now have a 1/4 chance of having been wrong on at least one of those tests... but you don't know which one. You can correct for it ([by multiplying the p-value you'll accept as significant by the number of tests you'll preform](http://mathworld.wolfram.com/BonferroniCorrection.html)) but in practice I find it's generally easier to avoid using p-values altogether. \n\n**(Dispersion parameter for binomial family taken to be 1):** You'll only see this for Poisson and binomial (logistic) regression. It's just letting you know that there has been an additional scaling parameter added to help fit the model. You can ignore it. :)\n\n**Null deviance:** The null deviance tells us how well we can predict our output *only* using the intercept. Smaller is better.\n\n**Residual deviance:** The residual deviance tells us how well we can predict our output using the intercept and our inputs. Smaller is better. The bigger the difference between the null deviance and residual deviance is, the more helpful our input variables were for predicting the output variable.\n\n**AIC:** The AIC is the \"Akaike information criterion\" and it's an estimate of how well your model is describing the patterns in your data. It's mainly used for comparing models trained on the same dataset. If you need to pick between models, the model with the *lower* AIC is doing a better job describing the variance in the data.\n\n**Number of Fisher Scoring iterations:** This is just a measure of how long it took to fit you model. You can safely ignore it. \n\nOK, that was a lot! Let's get the summary of our model again and quickly go over what this tells us.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"56385b7b-7fe4-44f7-b67e-272040b672aa","_uuid":"9295597ce0e118d1f83c15774465688a99605d0d","collapsed":true},"source":"# summary of the model\nsummary(model)","cell_type":"code"},{"metadata":{"_cell_guid":"e7bd9068-9b59-4c7e-b5fc-9d9e68c07ba0","_uuid":"0ecc236e1dbc8a6f7c7a64c1f64365ee7fc97d38"},"source":"From looking at the Deviance Residuals, we can tell that there's a really big skew in our residuals. Since the mean of the residuals is always 0 (due to the way that the model is fit) the median is lower than the mean. For that matter, the 3rd quartile is lower than the mean! However, since this is logistic regression and we don't need our residuals to be normally distributed, this doesn't really matter.\n\nFrom looking at the coefficients, we can tell that our classes are pretty unbalanced (because our intercept is far from 0 and our standard error is much smaller than it). We can also tell that our input, smart_1_normalized, doesn't have a very strong relationship to our output because the standard error (0.009) is more than 1/10 of our estimate (0.02). Our suspicions are confirmed when we look at the difference between the null & residual deviance: adding the smart_1_normalized term barely improved our deviance at all! \n\nSo, based on this model, we can say that it seems like smart_1_normalized, on its own, isn't a really good predictor of whether a hard drive is going to fail or not. We can double check this intution by plotting our model.","cell_type":"markdown"},{"metadata":{"_cell_guid":"248fdaf9-68f1-4f33-ace7-9a739098cb33","_uuid":"e0786eb15b42f3471936eedda10d03078dcdceea","collapsed":true},"source":"```\nggplot(hard_drives, aes(x = smart_1_normalized, y = failure)) + # draw a \n    geom_point() + # add points\n    geom_smooth(method = \"glm\", # plot a regression...\n    method.args = list(family = \"binomial\")) # ...from the binomial family\n```","cell_type":"markdown"},{"metadata":{"_cell_guid":"f5a81c6a-c109-4cfd-b32a-2a3909101fd9","_uuid":"d3677179af31be745258a18beafa773600b4fca9"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"61423db1-9f3d-44aa-97e7-6e34329581aa","_uuid":"567d6661b8bc5b90be4730ef045953566c442916"},"source":"This plot does confirm what we've learned from our model: our classes are very imbalanced and the input variable is not that helpful. It looks like this model is pretty much always just predicting that a hard drive *won't* fail, since most hard drives don't.","cell_type":"markdown"},{"metadata":{"_cell_guid":"1da05bb5-a89d-42c9-8f86-f3a9f8ecf3ff","_uuid":"aa5fa3edfaca1ac1591a7fb9abbaa7547f115833"},"source":"## Your turn!\n___\n\nNow it's your turn to come up with a model and interpret it!\n\n1. Pick a question to answer to using the Cameras dataset. Pick a variable to predict and one variable to use to predict it.\n2. Fit a GLM model of the appropriate family. (Check out [Monday's challenge](https://www.kaggle.com/rtatman/regression-challenge-day-1) if you need a refresher).\n3. *Optional but recommended:* Plot diagnostic plots for your model. Does it seem like your model is a good fit for your data? If you're fitting a linear or Poisson model, are the residuals normally distributed (no patterns in the first plot and the points in the second plot are all in a line)? Are there any influential outliers?\n4. Check out your model using the summary() function. Does your input variable have a strong relationship to the output variable you're predicting?\n5. Write a couple sentences describing what you've learned from your model. (It could just be that it's not a very good model!)\n5. Plot your two variables & use \"geom_smooth\" and the appropriate family to fit and plot a model. Does this confirm what you learned from examining your model?\n6. *Optional:* If you want to share your analysis with friends or to ask for help, you’ll need to make it public so that other people can see it.\n    * Publish your kernel by hitting the big blue “publish” button. (This may take a second.)\n    * Change the visibility to “public” by clicking on the blue “Make Public” text (right above the “Fork Notebook” button).\n    * Tag your notebook with 5daychallenge","cell_type":"markdown"},{"metadata":{"_cell_guid":"088c5e52-513d-4d08-89a8-0d2bdff5162a","_uuid":"d3d4fc22ffb0916380126ca0691639270e91c68a"},"source":"** Stealing method from Day1 **","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9cca4d4d-2c04-4089-9b32-b61bd14c8c29","_uuid":"07eeb52df68f7ac14740d7774ca945086feb1a58","collapsed":true},"source":"# your work goes here! :)\nimport pandas as pd\nimport numpy as np\n\nfrom ggplot.geoms.geom import geom\nfrom ggplot.stats import smoothers\nfrom ggplot.utils import is_date\n\nclass stat_smooth(geom):\n    \"\"\"\n    Smoothed line charts for inspecting trends in your data. There are 3 types of\n    smoothing algorithms you can use:\n        LOESS ('loess', 'lowess'): Non-parmetric, local regression technique for\n            calculating a smoothed curve.\n        linear model ('lm'): Fits a linear model to your (x, y) coordinates\n        moving average ('ma'): Calculates average of last N points in (x, y) coordinates\n    In addition to plotting the smoothed line, stat_smooth will also display the\n    standard error bands of the smoothed data (controlled by se=True/False).\n    Parameters\n    ----------\n    x:\n        x values for (x, y) coordinates\n    y:\n        y values for (x, y) coordinates. these will ultimately be smoothed\n    color:\n        color of the outer line\n    alpha:\n        transparency of color\n    size:\n        thickness of line\n    linetype:\n        type of the line ('solid', 'dashed', 'dashdot', 'dotted')\n    se:\n        boolean value for whether or not to display standard error bands; defaults to True\n    method:\n        type of smoothing to ues ('loess', 'ma', 'lm')\n    window:\n        number of periods to include in moving average calculation\n    Examples\n    --------\n    \"\"\"\n\n    DEFAULT_AES = {'color': 'black'}\n    DEFAULT_PARAMS = {'geom': 'smooth', 'position': 'identity', 'method': 'auto',\n            'se': True, 'n': 80, 'fullrange': False, 'level': 0.95,\n            'span': 2/3., 'window': None}\n    REQUIRED_AES = {'x', 'y'}\n    _aes_renames = {'size': 'linewidth', 'linetype': 'linestyle'}\n\n    def plot(self, ax, data, _aes):\n        (data, _aes) = self._update_data(data, _aes)\n        variables = _aes.data\n        data = data[list(variables.values())]\n        data = data.dropna()\n        x = data[variables['x']]\n        y = data[variables['y']]\n\n        params = {'alpha': 0.2}\n\n        se = self.params.get('se', True)\n        method = self.params.get('method', 'lm')\n        level = self.params.get('level', 0.95)\n        window = self.params.get('window', None)\n        span = self.params.get('span', 2/3.)\n\n        if method == \"lm\":\n            x, y, y1, y2 = smoothers.lm(x, y, 1-level)\n        elif method == \"ma\":\n            x, y, y1, y2 = smoothers.mavg(x, y, window=window)\n        else:\n            x, y, y1, y2 = smoothers.lowess(x, y, span=span)\n\n        smoothed_data = pd.DataFrame(dict(x=x, y=y, y1=y1, y2=y2))\n        try:  # change in Pandas-0.19\n            smoothed_data = smoothed_data.sort_values(by='x')\n        except:  # before Pandas-0.19\n            smoothed_data = smoothed_data.sort('x')\n\n        params = self._get_plot_args(data, _aes)\n        if 'alpha' not in params:\n            params['alpha'] = 0.2\n\n        order = np.argsort(x)\n        if self.params.get('se', True)==True:\n            if is_date(smoothed_data.x.iloc[0]):\n                dtype = smoothed_data.x.iloc[0].__class__\n                x = np.array([i.toordinal() for i in smoothed_data.x])\n                ax.fill_between(x, smoothed_data.y1, smoothed_data.y2, **params)\n                new_ticks = [dtype(i) for i in ax.get_xticks()]\n                ax.set_xticklabels(new_ticks)\n            else:\n                ax.fill_between(smoothed_data.x, smoothed_data.y1, smoothed_data.y2, **params)\n        if self.params.get('fit', True)==True:\n            del params['alpha']\n            ax.plot(smoothed_data.x, smoothed_data.y, **params)\n","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"38b940e4-89a8-403c-952d-953a4c6ee4ec","_uuid":"f829907f740e780124d450a419a1ea3390b3fa65","collapsed":true},"source":"cameras.describe()","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"debba4c1-f610-4f8d-90ec-961860734141","_uuid":"2f085dc75f20e648b002d0e9b0e6aaf109327a91","collapsed":true},"source":"cameras.dtypes","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"274823d8-5a70-4654-8792-13fe32c867e0","_uuid":"d6006911d8e549ae5ed61585076c6f4a4149739c","collapsed":true},"source":"cameras.head()","cell_type":"code"},{"metadata":{"_cell_guid":"fcb73875-2562-439f-a8f7-7d56d7be1bdb","_uuid":"3b4c3f33b707e89d9c02b8541ee4403d49c39934"},"source":"** filling NA with 0 **","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4eca16d8-f68f-4722-a59c-46994e3ba9b6","_uuid":"ea97dc2c730a795aecac5fc2ebb1f818fb1b695c","collapsed":true},"source":"cameras = cameras.fillna(0)","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b4171e01-6f66-451e-a413-5e7cbb0c8e54","_uuid":"9bf5395434b21f2739d05d8e83bb4281313b09e9","collapsed":true},"source":"X = cameras['Dimensions'][:, np.newaxis]\nY = cameras['Price']\n\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression()\nresult = reg.fit (X,Y)\nprint(result.intercept_, result.coef_)","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4a44be52-eea8-4e3f-a615-d93cde557e2c","_uuid":"db11be1353bfd0cf75f2c282266d6fb8cb9a661f","collapsed":true},"source":"ggplot(cameras, aes(x='Dimensions', y='Price')) + geom_point() + \\\nstat_smooth(method=\"lm\", color='blue')","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9028aadb-71ec-4dc7-93eb-92f7c0a71042","_uuid":"cd771913b857ae9b0ac5e198341b280577e5a48c","collapsed":true},"source":"X = cameras['Low resolution'][:, np.newaxis]\nY = cameras['Price']\n\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression()\nresult = reg.fit (X,Y)\nprint(result.intercept_, result.coef_)\n\nggplot(cameras, aes(x='Low resolution', y='Price')) + geom_point() + \\\nstat_smooth(method=\"lm\", color='blue')","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"e7e467fa-cd1b-474f-a7ee-739697a2a9f2","_uuid":"b5b34fa6e8c1fd3006dc4d91ddec26197baa8bad","collapsed":true},"source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn import linear_model","cell_type":"code"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"09dee8e9-4abd-421e-8063-598ede23e4a6","_uuid":"4cd16ab7a6d160b9257425d62126219e1d0a91a3","collapsed":true},"source":"# Quick plot of the data using seaborn\nsns.pairplot(cameras, hue=\"Price\")\n# sns.plt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"40502f05-f3f3-4a22-b65b-59e0f50b9055","_uuid":"5ba713b5dd6927e72fa2ee5a09ee6c928530542e"},"source":"Want more? Ready for a different dataset? [This notebook](https://www.kaggle.com/rtatman/datasets-for-regression-analysis/) has additional dataset suggestions for you to practice regression with. ","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"60a6c833-c6bf-4aa0-841b-51d4d07cd0c4","_uuid":"5dc4a8c7c773989909ca7d6f73c602815001bb7c","collapsed":true},"source":"sns.lmplot(x=\"Effective pixels\", y=\"Max resolution\", data=cameras)\n# sns.plt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"7139f78c-6fd4-411b-99ea-5044f69c4917","_uuid":"2f0997463e5d966a60fe85e669a4fd9db708d06b"},"source":"some useful reference http://marcharper.codes/2016-06-14/Linear+Regression+with+Statsmodels+and+Scikit-Learn.html\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"ddcbede0-eaad-4de5-85a4-8509ae38b306","_uuid":"63fa0545c1395e77056ef05bd1acece7ded4842f","collapsed":true},"source":"X = cameras[\"Effective pixels\"][:, np.newaxis]\nY = cameras[\"Max resolution\"]\n\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression()\nresult = reg.fit (X,Y)\nprint(result.intercept_, result.coef_)\nggplot(cameras, aes(x=\"Effective pixels\", y=\"Max resolution\")) + geom_point() + \\\nstat_smooth(method=\"lm\", color='blue')","cell_type":"code"},{"metadata":{"_cell_guid":"85b159cf-160d-4b9e-838d-27a75f9e9d15","_uuid":"f7f40248cdcb3274534ae07f5a0c176dd8ce8eb5"},"source":"Effective pixels of a camera does have strong relationship with Max Solution of a camera","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"e896511b-c1e2-4b05-b5b7-dee24bfff2f9","_uuid":"7b4e83f07300c8613e1051ccb17cb706a0946486","collapsed":true},"source":"","cell_type":"code"}],"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4}