{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Failure Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Background on the Dataset\n\nThe data was collected from the Institute of Cardiology and Allied hospital Faisalabad-Pakistan during April-December 2015.  (Ahmad T, Munir A, Bhatti SH, Aftab M, Raza MA (2017) Survival analysis of heart failure patients: A case study. PLoS ONE 12(7): e0181001. https://doi.org/10.1371/journal.pone.0181001). \n\nChicco & Jurman, 2020, found that a machine learning model using only the ejection fraction and serum creatinine from this dataset was enough to predict death during the follow up period.  (Chicco, D., Jurman, G. Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Med Inform Decis Mak 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5).\n\nThe dataset consists of 299 patients who were 40 years old or more, and who had left ventricular systolic dysfunction falling into NYHA classes 3 and 4.  The features are not causes of heart failure, but symptoms of it.\n\nSerum creatinine's normal level is 1.5, and any measurement greater than 1.5 is an indicator of renal dysfunction (Ahmad et. al., 2017).\n\nEjection fraction is the percentage of blood pumped out of the heart during a single contraction.  Normal values range from 50-75%, and a value < 40% inidicates heart failure (Chicco & Jurman, 2020).  \n\nEach patient has 1 follow up time, which can vary from 4 days to 285 days after their appointment.  \n\nA description of each feature can be found here: https://www.kaggle.com/andrewmvd/heart-failure-clinical-data/discussion/193109."},{"metadata":{},"cell_type":"markdown","source":"## Identifying the Problem to be Solved\n\nAlthough it seems reasonable to assume patients in this dataset died from heart failure, there is no way to be certain.  There is also no way of knowing if patients died shortly after their follow up.  The follow up time varies between patients.  A patient who was ok for their follow up 30 days later could have died on day 31, but they would be labeled as death_event = 0 in this dataset.  For this reason, and given the small size of the dataset, I expect there will be a large number of outliers.\n\nSince the follow up times are all different, I think it is misguided to say that death can be predicted from this dataset, despite Chicco & Jurman, 2020 having success in this effort.  If 2 patients have nearly identical feature vectors, but patient A was followed up on day 10 and was ok, but patient B was followed up on day 190 and had died, then the model would be confused because the labels would be different.  In one case, the model is essentially learning to predict whether a patient died within 10 days, and in the other, it is learning to predict whether a patient died within 190.  The only way it would make sense to predict death would be if all of the follow up times were exactly the same.  \n\nSo instead of using this data to predict death, it seems to make more sense to cluster it into 2 groups, and identify the feature differences between these groups.  This may help determine how different symptoms measure the severity of heart failure.  The death_event variable can be used as a guide to separate the clusters, but it will not be treated as ground truth.\n\n**TLDR: I will cluster the data by severe & less severe heart failure, then examine how the attributes vary between these groups.**"},{"metadata":{},"cell_type":"markdown","source":"## Data Ingestion"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\nd.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.rename(columns={\"DEATH_EVENT\": \"death_event\"}, inplace=True)\nd.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nThere are no null values in the data.  Let's see what the univarite distributions look like.  \n\nThings to look for/verify:\n1. The patients are all supposed to be >= 40 years old.  Are they?\n2. Are there any univariate outliers or unusual categories?\n3. Are there any rare levels of categorical features?\n4. Do any variables have skewed distributions that may need to be corrected?\n5. Is there a class imbalance in the target?  This is less important for my goal, but it would be important for anyone trying to fit a classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in d.columns.tolist():\n    sns.distplot(d[col], kde=False)\n    plt.title(f\"Distribution of {col}\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age is distributed as expected.\n\nThere are some skews but serum_creatine and creatinine_phosphokinase are the worst offenders.  I will plan to log transform these.\n\nThe only categorical features are the binary ones, and there are no rare categories.\n\nejection_fraction has some unsual values.\n\nThe target variable (death_event) is imbalanced toward 0.\n\nThe time variable (follow up days) is interesting.  Is there any evidence that patients who were followed up with sooner had worse cases of heart failure?  Might they have died sooner, on average?  I'll explore these questions in a moment.  First, let's see what is going on with ejection_fraction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# what happens to people whose hearts can only pump out 25% of their blood?\nd[d['ejection_fraction'] <= 25].groupby(\"death_event\")[\"death_event\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dead = d[d['ejection_fraction'] <= 25].loc[d.death_event == 1]\nnot_dead = d[d['ejection_fraction'] <= 25].loc[d.death_event == 0]\nsns.distplot(dead.time, label=\"dead\", color=\"Crimson\")\nsns.distplot(not_dead.time, label=\"not dead\", color=\"Green\")\nplt.legend()\nplt.title(\"Follow Up Times for Patients with Low Ejection Rates\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So low ejection rates seem to correspond to more severe cases of heart failure.  \n\nLet's take a look at all of the features, split by death_event."},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply log transformations\noriginal_vars = d[['serum_creatinine', 'creatinine_phosphokinase']].copy()\nd['log_serum_creatinine'] = np.log(d['serum_creatinine']+1e-9)\nd['log_creatinine_phosphokinase'] = np.log(d['creatinine_phosphokinase']+1e-9)\n\n# standardize the non-binary features\nfeatures_to_scale = d.columns[d.dtypes == 'float'].tolist() + ['ejection_fraction', 'serum_sodium', 'time']\nscaler = StandardScaler()\nd[features_to_scale] = scaler.fit_transform(d[features_to_scale])\n\n# remove the original values\nd.drop(original_vars.columns.tolist(), axis=1, inplace=True)\n\n# plot distributions by death_event\nfor col in d.columns:\n    fig = px.histogram(\n        d, x=col, color=\"death_event\", \n        marginal=\"violin\", hover_data=d.columns,\n        title = f\"Distribution of {col} vs death_event\", \n        labels=col,\n        template=\"plotly_dark\",\n        color_discrete_map={0: \"Green\", 1: \"Crimson\"}\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These plots are looking for 2 things:\n1. Do the distributions of any of these variables vary by the target?\n2. Do any of the outliers vary by the target?\n\nejection_fraction is lower for those who died.  Outliers in this feature follow that pattern too: high end outliers lived (higher ejection fraction) and low end outliers died (lower ejection fraction).\n\nlog_serum_creatinine is slightly lower, on aveage, for those who lived, while those who died have a broader range that extends to larger values.\n\nejection_fraction and serum_creatinine were found to be the most useful features by Chicco & Jurman, 2020.  So these patterns align with what they found."},{"metadata":{},"cell_type":"markdown","source":"## Clustering\n\nI want to divide the patients into 2 groups, where one consists of more severe heart failure, and the other is less severe.  The death_event variable can serve as a guide for this clustering, because patients who died likely had more severe heart failure.  By using it as a guide to divide the data, I can create 2 exemplars: 1 sample that is characteristic of patients who lived and another sample that is characteristic of patients who died.  These exemplars can serve as cluster centroids, and I can then find the pairwise Euclidean distances between each centroid and each observation.  This is a lot like k-means, but rather than assigning clusters, I will keep the distances as new features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the dataset by death_event\ndead = d.loc[d.death_event == 1].copy().reset_index(drop=True)\nnot_dead = d.loc[d.death_event == 0].copy().reset_index(drop=True)\n\n# create exemplars by averaging the features in each partition\n# the result will be a single synthetic observation that is characteristic of dead/not dead\n# the mean will ensure that the effect of outliers, which would otherwise trip up a classifier, are smoothed out\n# note that the data has already been standardized, so univariate outliers will have less of an impact on the mean\n\ndead[[c for c in dead.columns if c not in [\"death_event\"]]] = pd.DataFrame(dead[[c for c in dead.columns if c not in [\"death_event\"]]].mean(axis=0).values.reshape(1, -1))\ndead = dead.loc[0,:]\n\nnot_dead[[c for c in not_dead.columns if c not in [\"death_event\"]]] = pd.DataFrame(not_dead[[c for c in not_dead.columns if c not in [\"death_event\"]]].mean(axis=0).values.reshape(1, -1))\nnot_dead = not_dead.loc[0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dead","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_dead","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the pairwise distance to each exemplar\n# and add these distances as new features\nd['dist_from_dead'] = cdist(d.drop(\"death_event\", axis=1), dead.drop(\"death_event\", axis=0).values.reshape(1, -1), 'euclid')\nd['dist_from_not_dead'] = cdist(d.drop([\"death_event\", \"dist_from_dead\"], axis=1), not_dead.drop(\"death_event\", axis=0).values.reshape(1, -1), 'euclid')\nd.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A simple classifier could be built around these 2 features.  If dist_from_dead < dist_from_not_dead, then the classifier could predict 1 for death_event.  Otherwise it could predict 0.  If this simple rule were used, how would it do?"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.where(d.dist_from_dead < d.dist_from_not_dead, 1, 0)\nprint(\n    \"Accuracy if this were a classifier:\", \n    round((sum(preds==d.death_event)/len(d)) * 100, 2),\n    \"%\"\n)\n# confusion matrix\npd.crosstab(d.death_event, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This simple rule makes a decent classifier.  But again, since the meaning of \"predicting death\" for this dataset is ambiguous due to the different follow up times, my goal is to explore the feature differences between the groups. \n\nLet's first explore the outliers: the patients who were more similar to the dead exemplar but who lived, and the patients who were more similar to the not_dead exemplar but who still died.  According to the continengcy table, there are 33 and 21 of those, respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect patients who were closer to death but did not die\nd[(d['dist_from_dead'] < d['dist_from_not_dead']) & (d['death_event'] == 0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect patients who were closer to not_dead but who still died\nd[(d['dist_from_not_dead'] < d['dist_from_dead']) & (d['death_event'] == 1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting a Classifier, Just for Kicks and Giggles\n\nSince I used death_event to guide the partitioning used to create the exemplars, let's just see if a classifier could find better boundaries.  We can look at the accuracy and confusion matrix to see how a classifier might compare.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"dsub = d[[\"death_event\", \"time\", \"ejection_fraction\", \"log_serum_creatinine\"]].copy()\n\nx_train, x_test, y_train, y_test = train_test_split(\n    dsub.drop(\"death_event\", axis=1),\n    dsub[\"death_event\"],\n    test_size=0.5,\n    random_state=14,\n    shuffle=True,\n    stratify=dsub[\"death_event\"],\n)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nknn = KNeighborsClassifier(\n    n_neighbors=5, \n    algorithm='auto', \n    p=2, \n    metric='minkowski'\n)\nknn.fit(x_train, y_train)\nknn_preds = knn.predict(x_test)\n\nsvc = SVC(\n    C=1.0,\n    kernel='rbf',\n    gamma='scale',\n    random_state=14\n)\nsvc.fit(x_train, y_train)\nsvc_preds = svc.predict(x_test)\n\nprint(\n    \"Accuracy of KNN, k=5:\", \n    round((sum(knn_preds==y_test)/len(y_test)) * 100, 2),\n    \"%\"\n)\n# confusion matrix\nprint(pd.crosstab(y_test, svc_preds), \"\\n\")\nprint(\n    \"Accuracy of SVC:\", \n    round((sum(svc_preds==y_test)/len(y_test)) * 100, 2),\n    \"%\"\n)\n# confusion matrix\nprint(pd.crosstab(y_test, svc_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the classifiers did not do much better than the simple rule.  In fact, tweaking the hyperparameters, feature selection, and train/test split produces values that are both higher and lower than 81% accuracy, showing that all 3 methods are comparable.\n\nLet's return to the primary objective though: examining the feature differences between the groups."},{"metadata":{},"cell_type":"markdown","source":"## Factor Analysis\n\nNow I want to see how each feature varies by cluster.  Some feature values should stand out as being more characteristic of severe heart failure."},{"metadata":{"trusted":true},"cell_type":"code","source":"d['cluster'] = preds\ndsub = d.drop([\"death_event\", \"dist_from_dead\", \"dist_from_not_dead\"], axis=1).copy()\n\nfig = px.parallel_coordinates(\n    dsub.drop([\"anaemia\", \"diabetes\", \"sex\", \"log_creatinine_phosphokinase\"], axis=1), \n    color=\"cluster\", \n    title=\"Parallel Coordinates by Cluster\",\n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=0.5\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The few features that stand out are time, age (age is not really a symptom of heart failure), serum creatinine, ejection fraction.  \n\nLet's do some PCA and look at the factor loadings.  The features that are not symptoms of heart failure will be removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"dsub = d.drop([\"death_event\", \"dist_from_dead\", \"dist_from_not_dead\", \"age\", \"sex\", \"smoking\", \"cluster\"], axis=1).copy()\n\npca = PCA(n_components=None, whiten=False)\npca_dim = pca.fit_transform(dsub.values)\nloadings = pca.components_ * np.sqrt(pca.explained_variance_)\nfor factor in range(loadings.shape[0]):\n    ldf = pd.DataFrame({\n        \"feature\": dsub.columns.to_list(),\n        \"loading\": loadings[factor]\n    })\n    print(f\"Principal Component {factor}\")\n    print(ldf)\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the first 2 principal components with death event colored yellow\nplt.figure(figsize=(10, 5))\nplt.xlabel(\"Latent Variable 1 (explains most variance)\")\nplt.ylabel(\"Latent Variable 2 (explains 2nd most variance)\")\nplt.title(\"PCA 2-Dimension Plot with Death Event Colored\")\nplt.scatter(pca_dim[:, 0], pca_dim[:, 1], c=preds)\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first principal component, which explains the most variance, has the largest loadings from ejection_fraction and serum_sodium.  They are both negative, meaning they impact the component in the same direction.  log_serum_creatinine has a large positive loading, meaning it impacts the component in the opposite direction of ejection_fraction and serium_sodium.\n\nThe second principal component has large loadings from log_creatinine_phosphokinase, time, and ejection_fraction.  \n\nThe third principal component appears to chiefly capture plaetelets, as its loading far outweighs the rest.\n\nSo given the loadings from the first 3 principal components, it seems the most important factors are ejection_fraction, serum_creatinine, serum_sodium, creatinine_phosphokinase, and time.  All that is left to determine is to figure out how these impact heart failure.\n\nLooking back at Ahmad et. al., 2017, a serum sodium < 135 can result in hypnonatremia when the body swells with water.  High creatinine levels are a signal for renal (kidney) failure.  **So a combination of a low ejection fraction, low sodium, high creatinine, and high creatinine phosphokinase seems problematic, as this combination indicates the most severe heart failure.  It also makes sense that a patient with severe heart failure would have a sooner follow up appointment.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}