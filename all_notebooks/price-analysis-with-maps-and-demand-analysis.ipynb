{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Welcome to my Kernel. In this Kernel I will explore Airbnb Data in Berlin. Airbnb is taking over typical hospitality services so its good to know what factors influence price; what kind of reviews people are giving and what is general trend(demand) of visitors each year.\n\n**Table of Contents**\n> 1. Import Libraries\n> 1. Obtain Data\n> > 1. Picking columns to work with\n> > 1. Data Cleaning\n> > 1. Deal with NULL & N/A values\n> 1. Data Visualization & Price correlation analysis\n> 1. Data Visualization using maps\n> 1. Visitor Trend Analysis\n> 1. Sentiment Analysis using Word clouds"},{"metadata":{},"cell_type":"markdown","source":"Input data files are available in the \"../input/\" directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will be needed to work with maps\n!pip install folium","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is required for wordcloud\n!pip install wordcloud","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib as mpl\nimport scipy\nfrom scipy.stats import norm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport folium\nimport datetime\nimport warnings\nfrom math import sin, cos, sqrt, atan2, radians\n\n%matplotlib inline\npd.set_option('display.max_columns', 500)\nmpl.style.use(['seaborn-darkgrid'])\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the raw datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listing_summ = pd.read_csv(\"/kaggle/input/berlin-airbnb-data/listings_summary.csv\")\nprint(\"listing_summ : \" + str(df_listing_summ.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rsumm = pd.read_csv(\"/kaggle/input/berlin-airbnb-data/reviews_summary.csv\")\nprint(\"reviews_summary : \" + str(df_rsumm.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ignore unwanted columns like URLs\ncolumns_to_keep = ['id','host_has_profile_pic','host_since',\n                   'latitude', 'longitude','property_type', 'room_type', 'accommodates', 'bathrooms',  \n                   'bedrooms', 'bed_type', 'amenities', 'price', 'cleaning_fee',\n                   'security_deposit', 'minimum_nights',  \n                   'instant_bookable', 'cancellation_policy','availability_365']\ndf_listing_summ = df_listing_summ[columns_to_keep].set_index('id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"listing_summ : \" + str(df_listing_summ.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Check null values in each column\n* Check $ sign; map t/f to 1/0\n* Clean N/A values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listing_summ.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert f,t to 0 or 1\ndf_listing_summ['instant_bookable'] = df_listing_summ['instant_bookable'].map({'f':0,'t':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill f for N/A in host_has_profile_pic column for further correct mapping\nset(df_listing_summ['host_has_profile_pic'])\ndf_listing_summ['host_has_profile_pic'].fillna('f',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert f,t to 0 or 1\ndf_listing_summ['host_has_profile_pic'] = df_listing_summ['host_has_profile_pic'].map({'f':0,'t':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove $ from price, fee columns and convert to float\ndf_listing_summ['price'] = df_listing_summ['price'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_listing_summ['cleaning_fee'] = df_listing_summ['cleaning_fee'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_listing_summ['security_deposit'] = df_listing_summ['security_deposit'].str.replace('$', '').str.replace(',', '').astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cleaning_fee cleanup of N/a replace with median value\ndf_listing_summ['cleaning_fee'].fillna(df_listing_summ['cleaning_fee'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#security_deposit cleanup of N/a replace with median value\ndf_listing_summ['security_deposit'].fillna(df_listing_summ['security_deposit'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cleanup bathroom , bedroom columns\ndf_listing_summ['bathrooms'].fillna(1,inplace=True)\ndf_listing_summ['bedrooms'].fillna(1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now I start building a correlation [](http://)of various factors that will influence price but before that lets remove any outliers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check distribution of price column\ndf_listing_summ['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we see above 75% of prices are near 70 Euros , but min is 0 & max is 9000. Lets drop anything above 200 and even price of 0 or 1 Euro doesnt make sense**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listing_summ.drop(df_listing_summ[ (df_listing_summ.price > 200) | (df_listing_summ.price == 0) | (df_listing_summ.price == 1) ].index, axis=0, inplace=True)\ndf_listing_summ['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot of price column\nred_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\ndf_listing_summ['price'].plot(kind='box', xlim=(0, 175), vert=False, flierprops=red_square, figsize=(10,2));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets break up amenties that will help in drawing a correlation to price better as amenties might impact price**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listing_summ['No_of_amentities'] = df_listing_summ['amenities'].apply(lambda x:len(x.split(',')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listing_summ['Laptop_friendly_workspace'] = df_listing_summ['amenities'].str.contains('Laptop friendly workspace')\ndf_listing_summ['TV'] = df_listing_summ['amenities'].str.contains('TV')\ndf_listing_summ['Family_kid_friendly'] = df_listing_summ['amenities'].str.contains('Family/kid friendly')\ndf_listing_summ['Host_greets_you'] = df_listing_summ['amenities'].str.contains('Host greets you')\ndf_listing_summ['Smoking_allowed'] = df_listing_summ['amenities'].str.contains('Smoking allowed')\ndf_listing_summ['Hot_water'] = df_listing_summ['amenities'].str.contains('Hot water')\ndf_listing_summ['Fridge'] = df_listing_summ['amenities'].str.contains('Refrigerator')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping amenities as we have inferred above as different categories\ndropped = ['amenities']\ndf_listing_summ.drop(dropped,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert false,true to 0 or 1\ndf_listing_summ['Laptop_friendly_workspace'] = df_listing_summ['Laptop_friendly_workspace'].astype(int)\ndf_listing_summ['TV'] = df_listing_summ['TV'].astype(int)\ndf_listing_summ['Family_kid_friendly'] = df_listing_summ['Family_kid_friendly'].astype(int)\ndf_listing_summ['Host_greets_you'] = df_listing_summ['Host_greets_you'].astype(int)\ndf_listing_summ['Smoking_allowed'] = df_listing_summ['Smoking_allowed'].astype(int)\ndf_listing_summ['Hot_water'] = df_listing_summ['Hot_water'].astype(int)\ndf_listing_summ['Fridge'] = df_listing_summ['Fridge'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets also calculate distances from city center,airport and railway station that will again help in drawing a correlation to price**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate distance from central berlin\ndef haversine_distance_central(row):\n    berlin_lat,berlin_long = radians(52.5200), radians(13.4050)\n    R = 6373.0\n    long = radians(row['longitude'])\n    lat = radians(row['latitude'])\n    \n    dlon = long - berlin_long\n    dlat = lat - berlin_lat\n    a = sin(dlat / 2)**2 + cos(lat) * cos(berlin_lat) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    return R * c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate distance from airport\ndef haversine_distance_airport(row):\n    berlin_lat,berlin_long = radians(52.3733), radians(13.5064)\n    R = 6373.0\n    long = radians(row['longitude'])\n    lat = radians(row['latitude'])\n    \n    dlon = long - berlin_long\n    dlat = lat - berlin_lat\n    a = sin(dlat / 2)**2 + cos(lat) * cos(berlin_lat) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    return R * c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate distance from berlin railway station\ndef haversine_distance_rail(row):\n    berlin_lat,berlin_long = radians(52.5073), radians(13.3324)\n    R = 6373.0\n    long = radians(row['longitude'])\n    lat = radians(row['latitude'])\n    \n    dlon = long - berlin_long\n    dlat = lat - berlin_lat\n    a = sin(dlat / 2)**2 + cos(lat) * cos(berlin_lat) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    return R * c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listing_summ['distance_central'] = df_listing_summ.apply(haversine_distance_central,axis=1)\ndf_listing_summ['distance_airport'] = df_listing_summ.apply(haversine_distance_airport,axis=1)\ndf_listing_summ['distance_railways'] = df_listing_summ.apply(haversine_distance_rail,axis=1)\ndf_listing_summ['distance_avg'] = ( df_listing_summ['distance_central'] + df_listing_summ['distance_airport'] + df_listing_summ['distance_railways'] )/3.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Now we  are ready to see price is dependent on how many factors for top 1000 properties; so first I will sort by price descending and then generate a correlation matrix **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_listing_summ.sort_values(by='price',ascending=False,axis=0,inplace=True) #sorting frame by price desc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list_summ_top10000 = df_listing_summ.head(10000)\ndf_list_summ_top1000 = df_listing_summ.head(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\ncorr = df_listing_summ.corr()\n\n# generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# set up the matplotlib figure\nfig, ax = plt.subplots(figsize=(20, 15))\n\n# generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\":.5},cbar=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above matrix clearly tells us that price seems to depend largely on following factors - \n* No. of ameneties\n* Is it family or kids friendly\n* Cleaning fee\n* Capacity i.e how many guests it can accomodate\n\nAbove matrix also tells us that price is not much dependent on distance"},{"metadata":{},"cell_type":"markdown","source":"Lets gets p-values & pearson's coefficients for price w.r.t. some columns to support our above premise.\np-values indicate the degree of correlation between 2 variables.\n* p-value is < 0.001: we say there is strong evidence that the correlation is significant.\n* the p-value is < 0.05: there is moderate evidence that the correlation is significant.\n* the p-value is < 0.1: there is weak evidence that the correlation is significant.\n* the p-value is > 0.1: there is no evidence that the correlation is significant."},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(df_listing_summ['accommodates'], df_listing_summ['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(df_listing_summ['security_deposit'], df_listing_summ['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(df_listing_summ['No_of_amentities'], df_listing_summ['price'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now We are going to plot all top 1000 properties on a map to see where they are concentrated (central berlin, railway station or airport)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting a base map\nlat = 52.509\nlong = 13.381\nbase = folium.Map(location=[lat,long], zoom_start=12) #base map setting\nbase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbourhoods = folium.map.FeatureGroup()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lat_long_list = [[52.520,13.405],[52.373,13.506],[52.507,13.332]] #locatioms of central berlin , railway stn, airport","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(lat_long_list)):\n    neighbourhoods.add_child(\n        folium.CircleMarker(\n        lat_long_list[i],\n        radius = 16,\n        color='yellow',\n        fill=True,\n        fill_color='red',\n        fill_opacity=0.6\n        )\n    )\nbase.add_child(neighbourhoods)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbourhoods = folium.map.FeatureGroup()\nfor inc_lat,inc_long in zip(df_list_summ_top1000.longitude,df_list_summ_top1000.latitude):\n    neighbourhoods.add_child(\n    folium.CircleMarker(\n    [inc_long,inc_lat],\n    radius = 5,\n    color='yellow',\n    fill=True,\n    fill_color='blue',\n    fill_opacity=0.6\n    )\n)\nbase.add_child(neighbourhoods)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Above map plot indicates top 1000 properties are around central berlin & railway station and very few near airport\n* This is also evident from below distribution plots where properties are mostly around central berlin & railway station"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,6))\nax0 = fig.add_subplot(2, 2, 1)\nax1 = fig.add_subplot(2, 2, 2)\nax2 = fig.add_subplot(2, 2, 3)\n\nsns.distplot(df_list_summ_top1000[\"distance_central\"], bins=10, kde=False,ax=ax0)\nax0.set_title('Distances central berlin to apartments')\nax0.set_xlabel('distance_central')\nax0.set_ylabel('#properties')\n\nsns.distplot(df_list_summ_top1000[\"distance_railways\"], bins=10, kde=False,ax=ax1)\nax1.set_title('Distances railway station to apartments')\nax1.set_xlabel('distance_railways')\nax1.set_ylabel('#properties')\n\nsns.distplot(df_list_summ_top1000[\"distance_airport\"], bins=10, kde=False,ax=ax2)\nax2.set_title('Distances airport to apartments')\nax2.set_xlabel('distance_airport')\nax2.set_ylabel('#properties')\n\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.5, wspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ** So far we have worked on listing data-set. Now lets work on reviews summary and see what in-sights we can gather from it **"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I am going to generate month, day, year columns and then generate few line charts to see which month/year had most no. of visitors **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mapper(month):\n    date = datetime.datetime(2000, month, 1)  # You need a dateobject with the proper month\n    return date.strftime('%b')  # %b returns the months abbreviation, other options [here][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rsumm['date'] = pd.to_datetime(df_rsumm['date'])\ndf_rsumm['year'] = df_rsumm['date'].dt.year\ndf_rsumm['month'] = df_rsumm['date'].dt.month\ndf_rsumm['day'] = df_rsumm['date'].dt.day\n\ndf_rsumm['year'] = df_rsumm['year'].astype(int)\ndf_rsumm['month'] = df_rsumm['month'].astype(int)\ndf_rsumm['day'] = df_rsumm['day'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rsumm.sort_values(['year', 'month'], ascending=[True, True],axis=0,inplace=True) #sorting frame by year,month asc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rsumm['month'] = df_rsumm['month'].apply(mapper) ##convert month to month name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** I am creating a shallow copy of my frame (means changes in main frame dont affect this copy) as I will need to work on original frame with comments data for some sentiment analysis later **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rsumm_orig = df_rsumm.copy(deep=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped = ['reviewer_name','comments']\ndf_rsumm.drop(dropped,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rsumm['year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at above value counts ; lets do visitor trend (demand) analysis for 2015,2016,2017,2018 so lets filter the main frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2015 = df_rsumm[df_rsumm['year'] == 2015]\ndf_2016 = df_rsumm[df_rsumm['year'] == 2016]\ndf_2017 = df_rsumm[df_rsumm['year'] == 2017]\ndf_2018 = df_rsumm[df_rsumm['year'] == 2018]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped = ['year','day','id','date','listing_id']\ndf_2015.drop(dropped,axis=1,inplace=True)\ndf_2016.drop(dropped,axis=1,inplace=True)\ndf_2017.drop(dropped,axis=1,inplace=True)\ndf_2018.drop(dropped,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2015[\"count\"] = df_2015.groupby(\"month\")[\"reviewer_id\"].transform('count')\ndf_2016[\"count\"] = df_2016.groupby(\"month\")[\"reviewer_id\"].transform('count')\ndf_2017[\"count\"] = df_2017.groupby(\"month\")[\"reviewer_id\"].transform('count')\ndf_2018[\"count\"] = df_2018.groupby(\"month\")[\"reviewer_id\"].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped = ['reviewer_id']\ndf_2015.drop(dropped,axis=1,inplace=True)\ndf_2016.drop(dropped,axis=1,inplace=True)\ndf_2017.drop(dropped,axis=1,inplace=True)\ndf_2018.drop(dropped,axis=1,inplace=True)\ndf_2015 = df_2015.drop_duplicates()\ndf_2016 = df_2016.drop_duplicates()\ndf_2017 = df_2017.drop_duplicates()\ndf_2018 = df_2018.drop_duplicates()\ndf_2015=df_2015.reset_index(drop=True)\ndf_2016=df_2016.reset_index(drop=True)\ndf_2017=df_2017.reset_index(drop=True)\ndf_2018=df_2018.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 6))\nax0 = fig.add_subplot(2, 2, 1)\nax1 = fig.add_subplot(2, 2, 2)\nax2 = fig.add_subplot(2, 2, 3)\nax3 = fig.add_subplot(2, 2, 4)\n\ndf_2018.plot(kind='line', color='blue', x='month',y='count',marker='o',ax=ax0) # add to subplot 1\nax0.set_title('Visitors trend in 2018')\nax0.set_xlabel('Month')\nax0.set_ylabel('Number of visitors')\n\ndf_2017.plot(kind='line', color='red', x='month',y='count',marker='o',ax=ax1) # add to subplot 2\nax1.set_title('Visitors trend in 2017')\nax1.set_xlabel('Month')\nax1.set_ylabel('Number of visitors')\n\ndf_2016.plot(kind='line', color='cyan', x='month',y='count',marker='o',ax=ax2) # add to subplot 3\nax2.set_title('Visitors trend in 2016')\nax2.set_xlabel('Month')\nax2.set_ylabel('Number of visitors')\n\ndf_2015.plot(kind='line', color='green', x='month',y='count',marker='o',ax=ax3) # add to subplot 4\nax3.set_title('Visitors trend in 2015')\nax3.set_xlabel('Month')\nax3.set_ylabel('Number of visitors')\n\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Above plots indicate visitors have always peaked (most demand) during Sep each year**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped = ['listing_id','id','reviewer_id','reviewer_name','day','date']\ndf_rsumm_orig.drop(dropped,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2015_comments = df_rsumm_orig[df_rsumm_orig['year'] == 2015]\ndf_2016_comments = df_rsumm_orig[df_rsumm_orig['year'] == 2016]\ndf_2017_comments = df_rsumm_orig[df_rsumm_orig['year'] == 2017]\ndf_2018_comments = df_rsumm_orig[df_rsumm_orig['year'] == 2018]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2015_sep_comments = df_rsumm_orig[(df_rsumm_orig['year'] == 2015) & (df_rsumm_orig['month'].str.contains(\"Sep\"))]\ndf_2016_sep_comments = df_rsumm_orig[(df_rsumm_orig['year'] == 2016) & (df_rsumm_orig['month'].str.contains(\"Sep\"))]\ndf_2017_sep_comments = df_rsumm_orig[(df_rsumm_orig['year'] == 2017) & (df_rsumm_orig['month'].str.contains(\"Sep\"))]\ndf_2018_sep_comments = df_rsumm_orig[(df_rsumm_orig['year'] == 2018) & (df_rsumm_orig['month'].str.contains(\"Sep\"))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped = ['month']\ndf_2015_comments.drop(dropped,axis=1,inplace=True)\ndf_2016_comments.drop(dropped,axis=1,inplace=True)\ndf_2017_comments.drop(dropped,axis=1,inplace=True)\ndf_2018_comments.drop(dropped,axis=1,inplace=True)\ndf_2015_comments=df_2015_comments.reset_index(drop=True)\ndf_2016_comments=df_2016_comments.reset_index(drop=True)\ndf_2017_comments=df_2017_comments.reset_index(drop=True)\ndf_2018_comments=df_2018_comments.reset_index(drop=True)\n\ndf_2015_sep_comments.drop(dropped,axis=1,inplace=True)\ndf_2016_sep_comments.drop(dropped,axis=1,inplace=True)\ndf_2017_sep_comments.drop(dropped,axis=1,inplace=True)\ndf_2018_sep_comments.drop(dropped,axis=1,inplace=True)\ndf_2015_sep_comments=df_2015_sep_comments.reset_index(drop=True)\ndf_2016_sep_comments=df_2016_sep_comments.reset_index(drop=True)\ndf_2017_sep_comments=df_2017_sep_comments.reset_index(drop=True)\ndf_2018_sep_comments=df_2018_sep_comments.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list_year_wise = [df_2015_comments,df_2016_comments,df_2017_comments,df_2018_comments] #list of all frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list_sep = [df_2015_sep_comments,df_2016_sep_comments,df_2017_sep_comments,df_2018_sep_comments] #list of all frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev_comments_wc = WordCloud(\n    background_color='white',\n    max_words=100000, #if we dont give this it does for entire rows for that frame\n    stopwords = stopwords\n)\n#instantinate word cloud objects\ndef show_wclouds(text):\n    rev_comments_wc.generate(text)\n    return(rev_comments_wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15, 6))\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4)\nfor i in range(0,len(df_list_year_wise)):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.imshow(show_wclouds(str(df_list_year_wise[i]['comments'])),interpolation='bilinear')\n    ax.axis('off')\n    title=\"Review Comments trend in \"+str(df_list_year_wise[i]['year'].head(1).values[0])\n    ax.set_title(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Above indicates in 2016,2017 there were lot of visitors gave good review comments on properties but in 2015,2018 there were lot of cancellations as well **"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15, 6))\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4)\nfor i in range(0,len(df_list_sep)):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.imshow(show_wclouds(str(df_list_sep[i]['comments'])),interpolation='bilinear')\n    ax.axis('off')\n    title=\"Review Comments trend in SEP of \"+str(df_list_sep[i]['year'].head(1).values[0])\n    ax.set_title(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Above indicates in SEP of each year (where we had most no. of  visitors), most of them gave good reviews**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}