{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sn # data visualization\nfrom matplotlib import pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T16:32:21.243398Z","iopub.execute_input":"2021-05-22T16:32:21.244095Z","iopub.status.idle":"2021-05-22T16:32:22.59733Z","shell.execute_reply.started":"2021-05-22T16:32:21.243991Z","shell.execute_reply":"2021-05-22T16:32:22.596449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Motivation\n#### Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\n#### Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\n#### Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\n#### People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.","metadata":{}},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"df_raw = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:22.598739Z","iopub.execute_input":"2021-05-22T16:32:22.599218Z","iopub.status.idle":"2021-05-22T16:32:22.615243Z","shell.execute_reply.started":"2021-05-22T16:32:22.599182Z","shell.execute_reply":"2021-05-22T16:32:22.614141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"df_raw.columns","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:22.617103Z","iopub.execute_input":"2021-05-22T16:32:22.617414Z","iopub.status.idle":"2021-05-22T16:32:22.627713Z","shell.execute_reply.started":"2021-05-22T16:32:22.617385Z","shell.execute_reply":"2021-05-22T16:32:22.626702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:22.629244Z","iopub.execute_input":"2021-05-22T16:32:22.629823Z","iopub.status.idle":"2021-05-22T16:32:22.662063Z","shell.execute_reply.started":"2021-05-22T16:32:22.629791Z","shell.execute_reply":"2021-05-22T16:32:22.661302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw.info() # This dataset is very nice and doesnt need any manipulation liked vectorization, but it does require rescaling because some of the values are way too big","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:22.663183Z","iopub.execute_input":"2021-05-22T16:32:22.663581Z","iopub.status.idle":"2021-05-22T16:32:22.684745Z","shell.execute_reply.started":"2021-05-22T16:32:22.663552Z","shell.execute_reply":"2021-05-22T16:32:22.683659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw.hist(figsize=(15,15))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:22.686179Z","iopub.execute_input":"2021-05-22T16:32:22.68649Z","iopub.status.idle":"2021-05-22T16:32:24.710633Z","shell.execute_reply.started":"2021-05-22T16:32:22.686461Z","shell.execute_reply":"2021-05-22T16:32:24.709571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of positive samples:',sum(df_raw['DEATH_EVENT'] == 1))\nprint('Number of negative samples:',sum(df_raw['DEATH_EVENT'] == 0))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:24.711933Z","iopub.execute_input":"2021-05-22T16:32:24.712425Z","iopub.status.idle":"2021-05-22T16:32:24.734791Z","shell.execute_reply.started":"2021-05-22T16:32:24.712391Z","shell.execute_reply":"2021-05-22T16:32:24.733409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can choose to either remove or replace the NaN values and in this case there are so few that I can just remove them\ndf_raw.dropna(inplace = True)\n# Divide the raw data into two dataframes based on the categorical heart failure variable\ndf_raw_neg,df_raw_pos = df_raw.groupby(['DEATH_EVENT'])\n# We are taking an even sample of data from people who have had a heart failure and those who have not in order to reduce the bias of the model we will construct\ndf_filtered = pd.concat([df_raw_neg[1].sample(96),df_raw_pos[1].sample(96)]) \n# We want as much data as we can get while maintaining an similar number of positive and negative samples","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:24.738284Z","iopub.execute_input":"2021-05-22T16:32:24.73876Z","iopub.status.idle":"2021-05-22T16:32:24.758639Z","shell.execute_reply.started":"2021-05-22T16:32:24.738722Z","shell.execute_reply":"2021-05-22T16:32:24.757263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_filtered.hist(figsize=(15,15)) # We can visualize the new sample balanced around the categorical label we want to fit on","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:24.760905Z","iopub.execute_input":"2021-05-22T16:32:24.761266Z","iopub.status.idle":"2021-05-22T16:32:26.8512Z","shell.execute_reply.started":"2021-05-22T16:32:24.761228Z","shell.execute_reply":"2021-05-22T16:32:26.850123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn.boxplot(data=df_filtered,x='DEATH_EVENT',y='age') # Subtle spatial differences with age increasing the likelihood of heart disease","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:26.852822Z","iopub.execute_input":"2021-05-22T16:32:26.853237Z","iopub.status.idle":"2021-05-22T16:32:26.99315Z","shell.execute_reply.started":"2021-05-22T16:32:26.853199Z","shell.execute_reply":"2021-05-22T16:32:26.99243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn.boxplot(data=df_filtered,x='DEATH_EVENT',y='serum_sodium') # More visualizations to justify a descision tree or a support vector machine","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:26.994488Z","iopub.execute_input":"2021-05-22T16:32:26.995083Z","iopub.status.idle":"2021-05-22T16:32:27.146441Z","shell.execute_reply.started":"2021-05-22T16:32:26.995037Z","shell.execute_reply":"2021-05-22T16:32:27.145066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn.boxplot(data=df_filtered,x='DEATH_EVENT',y='platelets') # Platelets dont seem to really help distinguish the categorical label","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:27.148382Z","iopub.execute_input":"2021-05-22T16:32:27.148853Z","iopub.status.idle":"2021-05-22T16:32:27.307797Z","shell.execute_reply.started":"2021-05-22T16:32:27.148802Z","shell.execute_reply":"2021-05-22T16:32:27.306696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_filtered.corr() # We can find better features by seeing what correlates well with the categorical label","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:27.30926Z","iopub.execute_input":"2021-05-22T16:32:27.30958Z","iopub.status.idle":"2021-05-22T16:32:27.337726Z","shell.execute_reply.started":"2021-05-22T16:32:27.309549Z","shell.execute_reply":"2021-05-22T16:32:27.336322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After visualizing the data and looking at each features correlation with the label we can choose features which are likely to help our model\nFEATURES = ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium']\n# FEATURES = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n#        'ejection_fraction', 'high_blood_pressure', 'platelets',\n#        'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\n# I tested adding a few features to improve model performance and it seems that my Random Forest Model did better with more labels, so I used them all\n# I think the model was able to pick up on the subtle differences that we saw in the Boxplots better than I could.\n# The difference in model performance is about -10% when you use the first feature set detailed above with a limited set.\n# I think with more time I could pick better features that are more accessable to people who cannot collect all of the data perhaps in a developing country with limited infrastructure\nLABELS = ['DEATH_EVENT']","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:27.339578Z","iopub.execute_input":"2021-05-22T16:32:27.33991Z","iopub.status.idle":"2021-05-22T16:32:27.348211Z","shell.execute_reply.started":"2021-05-22T16:32:27.339878Z","shell.execute_reply":"2021-05-22T16:32:27.347117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_filtered[FEATURES].values\ny = df_filtered[LABELS].values.reshape(1,-1)[0]\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.7,shuffle=True)\n# We want to split our data in order to cross-validate our models later and ensure that they are properly generalizing","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:27.349737Z","iopub.execute_input":"2021-05-22T16:32:27.350169Z","iopub.status.idle":"2021-05-22T16:32:27.367941Z","shell.execute_reply.started":"2021-05-22T16:32:27.35013Z","shell.execute_reply":"2021-05-22T16:32:27.366673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Means","metadata":{}},{"cell_type":"code","source":"scalar = StandardScaler() # not really necessary for k-means, but it will be useful when we boost later with Ada\nkmeans = KMeans(n_clusters=2)\n# The point of trying k-means is to understand whether the data is spatially separable and whether the separation is relevant to the label class\npipe = Pipeline([('scalar',scalar),('kmeans',kmeans)]).fit(X_train) # The pipe is a sklean object that allows us to create more complex models that feed into each other seamlessly\n# While we dont need a scalar for kmeans it will be usefull later when we use Adaboost","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:02.19191Z","iopub.execute_input":"2021-05-22T16:33:02.192261Z","iopub.status.idle":"2021-05-22T16:33:02.227085Z","shell.execute_reply.started":"2021-05-22T16:33:02.192231Z","shell.execute_reply":"2021-05-22T16:33:02.226071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_filtered['kMeansPrediction'] = pipe.predict(X)\nsn.scatterplot(data=df_filtered,x='age', y='serum_sodium',hue='kMeansPrediction')\nprint(classification_report(y_test,(pipe.predict(X_test))))\n# After playing around with different features that were highly correlated to the label I came across a good set of features that gave the model a pretty good f1-score\n# Enough to justify moving forward with these variables","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:02.415775Z","iopub.execute_input":"2021-05-22T16:33:02.416132Z","iopub.status.idle":"2021-05-22T16:33:02.656111Z","shell.execute_reply.started":"2021-05-22T16:33:02.416094Z","shell.execute_reply":"2021-05-22T16:33:02.655032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Boosting Random Forest with AdaBoost\n","metadata":{}},{"cell_type":"code","source":"params_clf = [{'criterion' : [\"gini\", \"entropy\"],\n                'max_depth': [4,6,8,10,12],\n                'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n                'n_estimators': [100,200,400,600,800],\n          }]\nclf = RandomForestClassifier()\n# We want to find the best hyper parameters and for that we will use the GridSearchCV function provided by sklearn\ngrid_search_clf = GridSearchCV(clf,params_clf,cv=3,n_jobs=25,scoring='f1',verbose=True)\ngrid_search_clf.fit(X,y)\nprint(grid_search_clf.best_estimator_)\n# We want to perform a grid search to fine tune the hyperparameters","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-05-22T16:32:27.656486Z","iopub.execute_input":"2021-05-22T16:32:27.656959Z","iopub.status.idle":"2021-05-22T16:32:28.193382Z","shell.execute_reply.started":"2021-05-22T16:32:27.656924Z","shell.execute_reply":"2021-05-22T16:32:28.188386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"clf = grid_search_clf.best_estimator_.fit(X_train,y_train)\ndf_filtered['RandomForestPrediction'] = clf.predict(X)\nsn.scatterplot(data=df_filtered,x='age', y='serum_sodium',hue='RandomForestPrediction')\nprint(classification_report(y_test,clf.predict(X_test))) # sklearns classification report will give us a bunch of metrics to evaluate our model","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.1945Z","iopub.status.idle":"2021-05-22T16:32:28.194991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AdaBoost","metadata":{}},{"cell_type":"code","source":"params_ada = [{'n_estimators': [50,150,250,350]}]\nada = AdaBoostClassifier(clf)\n# We want to find the best hyper parameters so we will use the sklearn function again\ngrid_search_ada = GridSearchCV(ada,params_ada,cv=3,n_jobs=25,scoring='f1',verbose=True)\ngrid_search_ada.fit(X,y)\nprint(grid_search_ada.best_params_) ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.196757Z","iopub.status.idle":"2021-05-22T16:32:28.197416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada = grid_search_ada.best_estimator_\nboost_pipe = Pipeline([('scalar',scalar),('adaboost',ada)]).fit(X_train,y_train) # Using those parameters we will train the model using our train sample\ndf_filtered['AdaBoostPrediction'] = boost_pipe.predict(X) # Then we will predict for the whole dataset for visualization later\nsn.scatterplot(data=df_filtered,x='age', y='serum_sodium',hue='AdaBoostPrediction') # Graph our prediction\nprint(confusion_matrix(y_test,boost_pipe.predict(X_test))) # print the confusion matrix and classification report\nprint(classification_report(y_test,boost_pipe.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.198885Z","iopub.status.idle":"2021-05-22T16:32:28.199537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC Metrics","metadata":{}},{"cell_type":"code","source":"# fpr, tpr, thresholds = roc_curve(df_filtered['DEATH_EVENT'],df_filtered['AdaBoostPrediction'])\nfpr, tpr, thresholds = roc_curve(y_test,boost_pipe.predict(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.20074Z","iopub.status.idle":"2021-05-22T16:32:28.201414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn.lineplot(fpr,tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(['ROC'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.2032Z","iopub.status.idle":"2021-05-22T16:32:28.203901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Metrics on entire dataset: \")\nprint(confusion_matrix(df_raw[LABELS[0]],boost_pipe.predict(df_raw[FEATURES])))\nprint(classification_report(boost_pipe.predict(df_raw[FEATURES]),df_raw[LABELS[0]])) \n# Score on the entire dataset including the other ~180 samples\n# slightly higher than it should be because its already trained on the ~125 training samples and will have a high accuracy on them, but it hasnt seen ~180 samples + test size ~60\n# so the overall score is still really good with that in consideration","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.205077Z","iopub.status.idle":"2021-05-22T16:32:28.20576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Error Visualization","metadata":{}},{"cell_type":"code","source":"# We want to see where our model makes mistakes and visualizing that can help us make a better model\nsn.scatterplot(x=df_filtered['age'], y=df_filtered['serum_sodium'],hue=df_filtered['AdaBoostPrediction']==df_filtered['DEATH_EVENT'])\nplt.legend(['Correct','Not Correct'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.207015Z","iopub.status.idle":"2021-05-22T16:32:28.207678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn.scatterplot(x=df_filtered['ejection_fraction'], y=df_filtered['creatinine_phosphokinase'],hue=df_filtered['AdaBoostPrediction']==df_filtered['DEATH_EVENT'])\nplt.legend(['Correct','Not Correct']) # We just want to choose a few graphs to visualize where the error is made","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.209045Z","iopub.status.idle":"2021-05-22T16:32:28.209707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Probability of Development / Risk Assessment","metadata":{}},{"cell_type":"code","source":"df_filtered['Risk Assessment'] = boost_pipe.predict_proba(df_filtered[FEATURES])[:,1]\n# With the probability we can assign a risk to a patient based on their data and help the patient understand the severity of their case","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.211064Z","iopub.status.idle":"2021-05-22T16:32:28.211745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_filtered[FEATURES+LABELS+['AdaBoostPrediction','Risk Assessment']].head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.213266Z","iopub.status.idle":"2021-05-22T16:32:28.213922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"#### I think the use of Random Forest and AdaBoost was fairly straight-forward considering the distribution of the label classes were very well spatially separated and the correlation of the features was fairly good for many of the features, however; I think that I would have wanted to spend more time removing unnecessary features to improve the accessability of the model for, as discussed earlier, places with limited infrastructure and healthcare where getting those measurements would prove difficult.\n#### We can see that a majority of the incorrect guesses are spatially relevant, so I think if I had more time I would have liked to play more with the features and try to engineer features that would help spatially separate the data for the model to leverage\n#### I think in the future when I have taken more statistics I would like to offer a more robust analysis of the features and perhaps a hypothesis test for each feature to ensure that it is relevant to the model. That would be in order to understand exactly which features I dont need to reduce the requirements to make the model more accessable.","metadata":{}},{"cell_type":"code","source":"ada.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.215257Z","iopub.status.idle":"2021-05-22T16:32:28.215901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in range(len(ada.feature_importances_)):\n    if ada.feature_importances_[x] > 0.04:\n        print(FEATURES[x])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:28.217212Z","iopub.status.idle":"2021-05-22T16:32:28.217847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}