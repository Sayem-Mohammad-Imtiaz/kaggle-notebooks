{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/productdemandforecasting/Historical Product Demand.csv\",parse_dates=['Date'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Shape & data Types\n\nprint(df.shape)\nprint(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to check columns if they have any missing values, it will return number of Nan in the given columns\n\ndf.isnull().sum()\n\n# Mssing values are in dates\n# calculating % of data missing\nprint(\"% of Data missing =\", df.isnull().sum().sum()/len(df)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data missing is approxiately 1% of actual data, so we can remove it using Dropna\n\ndf.dropna(axis=0, inplace=True) #remove all rows with Nan\n\n#setting date as index columns\ndf.reset_index(drop = True)\ndf.isnull().sum()\n\n# Now there is no missing data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values('Date')[10:20]\n# Some value in Order_Demand column has (), therefore need tro remove before converting them into integer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Order_Demand'] = df['Order_Demand'].str.replace('(',\"\")\ndf['Order_Demand'] = df['Order_Demand'].str.replace(')',\"\")\ndf.sort_values('Date')[10:20]\n# Now () are removed from the column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the data type into integer\ndf['Order_Demand'] = df['Order_Demand'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(16,5)})\nsns.distplot(df['Order_Demand'], bins = 100);\n# it can seen that most of our demand lies between 0 to 500000, which is highly skewed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('Warehouse')['Order_Demand'].sum().sort_values(ascending=False)\n# It can be seen Warehouse J has maximum demand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(df.groupby('Product_Category')['Order_Demand'].sum().sort_values(ascending=False))\ndf1[\"% Contribution\"] = df1['Order_Demand']/df1['Order_Demand'].sum()*100\ndf1\n# It can be seen starting top 4 products category contribute more than 90% of the demand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.pivot_table(df,index=[\"Date\"],values=[\"Order_Demand\"],columns=[\"Product_Category\"],aggfunc=np.sum)\ndf2.columns = df2.columns.droplevel(0)\ndf2[\"Category_019\"].dropna()\n# Creating Pivot table with date as index, Product category as columns & and values as sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df2.resample('M').sum() # Resampling the data on monthly basis \ny.index.freq = \"M\" # Setting datetime frequency to Month\ny.head(20)\n# In Year 2011 so much data is missing, so we will exlude it ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_019 = pd.DataFrame(y[\"Category_019\"].iloc[12:-1]) # Including data from 2012 to 2016 end except last value\ndf_019.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nspan = 4\nalpha = 2/(span+1)\ndf_019['EWMA4'] = df_019[\"Category_019\"].ewm(alpha=alpha,adjust=False).mean() # Simple Weighted Moving Average\n# Simple Exponentional Smoothing\ndf_019['SES4']=SimpleExpSmoothing(df_019[\"Category_019\"]).fit(smoothing_level=alpha,optimized=False).fittedvalues.shift(-1)\n\n#Double Exponentional  Smothening\ndf_019['DESadd4'] = ExponentialSmoothing(df_019[\"Category_019\"], trend='add').fit().fittedvalues.shift(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_019[[\"Category_019\",'SES4','DESadd4']].plot(figsize = (20,6)) # Plot for Weighted Moving average & Double Exponentional, \n#It can be seen data has some seasonailty, therefore will use ARIMA, ARMA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will Ignore harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n! pip install pmdarima \nfrom statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf # for determining (p,q) orders\nfrom pmdarima import auto_arima # for determining ARIMA orders\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose      # for ETS Plots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to Check series is stationary or not\nfrom statsmodels.tsa.stattools import adfuller\n\ndef adf_test(series,title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print(f'Augmented Dickey-Fuller Test: {title}')\n    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out[f'critical value ({key})']=val\n        \n    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adf_test(df_019[\"Category_019\"])\n# Series is non stationary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit = auto_arima(df_019[\"Category_019\"], start_p=1, start_q=1,\n                          max_p=5, max_q=5, m=12,\n                          start_P=0, seasonal=True,\n                          d=None, D=1, trace=True,\n                          error_action='ignore',   # we don't want to know if an order does not work\n                          suppress_warnings=True,  # we don't want convergence warnings\n                          stepwise=True)           # set to stepwise\n\nfit.summary()\n# After running Auto Arima, Best order is SARIMAX(3, 1, 3)x(0, 1, [1, 2], 12) for which AIC is minimum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_019[\"Category_019\"])\n# Train & Test Data\ntrain = df_019[\"Category_019\"].iloc[:48]\ntest = df_019[\"Category_019\"].iloc[48:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SARIMAX(train,order=(3,1,3),seasonal_order=(0,1,1,12))\nresults = model.fit()\nresults.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain predicted values\nstart=len(train)\nend=len(train)+len(test)-1\npredictions = results.predict(start=start, end=end, dynamic=False, typ='levels').rename('SARIMA (3,1,3),(0,1,1,12) Predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = test.plot(legend=True,figsize=(12,6))\npredictions.plot(legend=True)\nax.autoscale(axis='x',tight=True)\n#plotting Test data & predicted demand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nerror = np.sqrt(mean_squared_error(test, predictions))\nprint(f'SARIMA(0,1,3)(1,0,1,12) RMSE Error: {error:11.10}')\nprint('Std of Test data:                  ', df_019[\"Category_019\"].std())\n# Comparison of RMSE & Std of data, as Std if very high compared to RMSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrain the model on the full data, and forecasting for next 4 months\nmodel = SARIMAX(df_019[\"Category_019\"],order=(3,1,3),seasonal_order=(0,1,1,12))\nresults = model.fit()\nfcast = results.predict(len(df_019[\"Category_019\"]),len(df_019[\"Category_019\"])+4,typ='levels').rename('SARIMA(3,1,3)(0,1,1,12) Forecast')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df_019[\"Category_019\"].plot(legend=True,figsize=(12,6))\nfcast.plot(legend=True)\nax.autoscale(axis='x',tight=True)\n#plotting actual data & 4 month forecasted demand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same steps need to be done for each category, considering the missing data, stationarity. Finding the best order to be fit into the model and using the train test split to validate the model, Finally forecasting for the required Months.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}