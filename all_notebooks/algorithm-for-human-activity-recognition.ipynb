{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"version":"3.6.3","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"}},"cells":[{"outputs":[],"metadata":{"_uuid":"7f87e23450193b407c8c65e3ddec4ac27b373fc2","_cell_guid":"4b6f0fa0-6a0c-453f-8ca9-6f0bcf225089"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"8f4e1f23fa90b9c6158096a07e47a4cf91e4bc2f","collapsed":true,"_cell_guid":"1a2e04ae-9cf5-403c-892d-90d75bc44bcf"},"source":"Train_Data=pd.read_csv(\"../input/train.csv\")\nTest_Data=pd.read_csv(\"../input/test.csv\")","execution_count":null,"cell_type":"code"},{"source":"<h1>Exploratory Analysis </h2>","metadata":{"_uuid":"ca89079f65e0c325a7b423b8f366b5278f3f8955","_cell_guid":"aeb4bef3-a67c-464b-aba3-dd62a9e008b4"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"b09a6dbe0c3723c53dc535ea39d2b7e4d0ab6e37","_cell_guid":"481cc2c3-7ed5-497f-8d63-d0a21645bae8"},"source":"len(Train_Data)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"a7a0054f22e29bafbf958ba518929884e9c8abbd","_cell_guid":"b81e0a79-85ad-4132-bb3e-c9fa0263a4c3"},"source":"len(Test_Data)","execution_count":null,"cell_type":"code"},{"source":"Let us train and test our model using the Training Data and perform final validation on the given test data. ","metadata":{"_uuid":"23f6bb6aec5e498fba22ee94bcdc8f88b826bed3","_cell_guid":"9c7bf088-ea35-4865-8e41-f44949112901"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"e1c1b0d13c98b657c530d3c17bfa7002aa2b96b1","_cell_guid":"17504d73-bc1c-47fd-9821-8b5c01408486"},"source":"Train_Data.sample(frac=0.1).head(n=5)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"049ecc1902e3d768d5911e0e501dc3dd36acc0f1","_cell_guid":"092e3a15-dde7-4d14-8161-51a90a4b6a20"},"source":"Train_Data.describe()","execution_count":null,"cell_type":"code"},{"source":"The below lists the variables in the dataset. ","metadata":{"_uuid":"fab47a0557fac463ff72114ba37bf69fc9266a25","_cell_guid":"46d8926c-0683-4708-a087-69f649adaefe"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"db35c84dd15d1212c0f49d42568b6d5501725f06","_cell_guid":"d406cc50-0df5-4368-a634-e84fa43a368e"},"source":"list(Train_Data.columns.values)","execution_count":null,"cell_type":"code"},{"source":"<h1>Pre-Processing </h1>","metadata":{"_uuid":"f846cdc99efde962ba641a4ed1b3ca51a83c7a83","_cell_guid":"f989c7f7-6bd6-4f10-9dae-645faf92b543"},"cell_type":"markdown"},{"source":"The activities are in the form of text which cannot be used directly as input features for machine learning algorithms So ,  convert them to categorical numerical data.","metadata":{"_uuid":"6cec1cf81ef4a877476c5cf03bc0038b3dac0afe","_cell_guid":"178fce0d-8e8f-43ad-9d62-5aac31a62ab4"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"d8d2410b7f3c5ceb897c915e131aa442864742b6","collapsed":true,"_cell_guid":"c184d70b-52e0-4893-86cd-aec918c70680"},"source":"#Split the Data for machine learning algorithms\n\ndef Featurize_Outcome(Data):\n    \n        '''The activity variable is in the form of strings , which cannot be directly given as input to the algorithm , \n        for the purpose we will convert them to numerical features'''\n        \n        Labels=[]\n        Key={}\n\n        Prev=None\n        Index=0\n\n        for i in Data:\n        \n                    if(i in Key):\n\n                         Labels.append(Key[i])\n        \n                    else:\n        \n                         Key[i]=Index\n                         Labels.append(Key[i])\n                         Index+=1\n                    \n        return(Labels)\n    \nTrain_Data_X=Train_Data\nTrain_Data_Y=Train_Data['Activity']\nTrain_Data_Y=Featurize_Outcome(Train_Data_Y)\nTrain_Data_X.drop('Activity',axis=1,inplace=True)\n\nTest_Data_X=Test_Data\nTest_Data_Y=Test_Data['Activity']\nTest_Data_Y=Featurize_Outcome(Test_Data_Y)\nTest_Data_X.drop('Activity',axis=1,inplace=True)\n","execution_count":null,"cell_type":"code"},{"source":"<h2>Learning Algorithms (Original Dataset)</h2>","metadata":{"_uuid":"3f75663ba68c9ea60ecfd490ebad54f20b8a1a9d","_cell_guid":"aab43b8d-de87-4336-904b-0db80d7f130c"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"4200f6246a6047fa80134f39637ba559f16129c9","collapsed":true,"_cell_guid":"60eb6a8f-9fa3-4140-8691-e2523b5886f8"},"source":"#Importing required libraries for developing machine learning models\n\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import tree\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"886ea880acc443f66ab91af97485577a3976f0d4","_cell_guid":"82bb31e6-835d-4bad-a393-5ba8d006f8d5"},"source":"from sklearn.ensemble import RandomForestClassifier\nRf=RandomForestClassifier()\nprint(\"Accuracy: \",cross_val_score(Rf,Train_Data_X,Train_Data_Y, cv=5).mean())","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"3ee300fc1f1eb9eaa50c2811f017d65c76cae98f","_cell_guid":"e9ef005a-be15-45f0-87ce-1fb7c7c41220"},"source":"gNB=GaussianNB()\nscores=cross_val_score(gNB,Train_Data_X,Train_Data_Y, cv=5)\nprint(\"Accuracy: \",scores.mean())","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"5c75be242361246506b0df9f60c48b34dd9b0108","_cell_guid":"54768b52-05d6-434a-9145-74a18723d3c0"},"source":"cl=tree.DecisionTreeClassifier()\nprint(\"Accuracy: \",cross_val_score(cl,Train_Data_X,Train_Data_Y,cv=5).mean())","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"1fe660f0b83ccabb4f11aeddd1b84750eb582902","_cell_guid":"e9057922-c4e1-4c87-97b0-ea1511ccb63f"},"source":"clf=svm.SVC(kernel='rbf')\nprint(\"Accuracy: \",cross_val_score(clf,Train_Data_X,Train_Data_Y, cv=5).mean())","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"680421283fb6313d487c65e192b0e8e10d1e7fab","_cell_guid":"187a760c-791e-4b04-b5bc-b21b5ea028f4"},"source":"clf=svm.SVC(kernel='linear')\nprint(\"Accuracy: \",cross_val_score(clf,Train_Data_X,Train_Data_Y, cv=5).mean())","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"fb8e5b88b5829512d94428fdb94d5e367d7b22b6","_cell_guid":"cd3bbaef-f18c-4425-b161-013c545fedcf"},"source":"scores=[]\nfor i in range(1,31):\n    neigh=KNeighborsClassifier(n_neighbors=i)\n    scores.append(cross_val_score(neigh,Train_Data_X,Train_Data_Y,cv=5).mean())\n    \nmax_a=0\nk_max=0\n\nfor i in range(0,30):\n    \n    if(scores[i]>=max_a):\n        \n        max_a=scores[i]\n        \n        if(i>k_max):\n                \n            k_max=i\n        \nprint(\"K is maximum in Knn for \",k_max,\" with a accuracy of \",max_a) ","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"be85132fe1a3ed465757fa45f4b73dee37309d1e","_cell_guid":"a1b0d667-8b68-4369-9ade-250eab689f61"},"source":"AB=AdaBoostClassifier()\nprint(\"Accuracy: \",cross_val_score(AB,Train_Data_X,Train_Data_Y, cv=5).mean())","execution_count":null,"cell_type":"code"},{"source":"<h2>Learning Algorithms (Dataset with Feature Selection Applied)</h2>","metadata":{"_uuid":"a0ab8d0682769ac7ee9eaf5eb1bdbbac016db745","_cell_guid":"6171a44c-dc94-4346-b832-d271179b8def"},"cell_type":"markdown"},{"source":"There are 563 columns of data which could be time consuming to train and could possibly lead to overfitting. It could possibly help to use Dimensionality reduction and feature selection algorithms.  Lets apply various feature elimination methods to Support Vector Machine with Linear Kernel.","metadata":{"_uuid":"85c2186f93c46da660801bdeb36c1a21dda2dcfe","_cell_guid":"e80ea394-6dde-413a-9c51-62f869af4855"},"cell_type":"markdown"},{"source":"<h2>Feature Elimination Based on Variance</h2>","metadata":{"_uuid":"3bb33c4dac5c39b1daa8e9b0e1cf44d23edf2bd4","_cell_guid":"7dae94d6-2067-4c36-bfa7-bbb22e5d69dc"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"7db20ada832def7b84defe95fe305746ea4504d6","collapsed":true,"_cell_guid":"cb54efac-d6b8-4179-aa13-1ce43b38f121"},"source":"#Import the requird library required for feature elimination based on variance. \nfrom sklearn.feature_selection import VarianceThreshold","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"824d0835efaf594dcc0f8b64d6e51995eb7f198f","collapsed":true,"_cell_guid":"d6d86caf-4915-45e1-ae7d-e0e1c80baad7"},"source":"sel=VarianceThreshold(.9 * (1 - .9))\nFS_Train_Data_X=sel.fit_transform(Train_Data_X)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"8dfcf968ca77ea826086f3aa0039a855f83587d5","_cell_guid":"1008608e-8b69-4f90-83b2-4cf3e23ed818"},"source":"FS_Train_Data_X.shape","execution_count":null,"cell_type":"code"},{"source":"The number of relevant features are reduced from 562 to 212 upon eliminating features based on variance. ","metadata":{"_uuid":"704473fbaa6e60fe5699ff77fb3f15b02826b95e","_cell_guid":"b9075e9c-2152-4809-abfb-06219b6aec40"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"5e512cd387bc71ab0d167b731d00faa3f40fbf6b","_cell_guid":"11be6f92-69f3-4931-8745-5a2f1b68fd04"},"source":"clf=svm.SVC(kernel='linear')\nprint(\"Accuracy: \",cross_val_score(clf,FS_Train_Data_X,Train_Data_Y, cv=5).mean())","execution_count":null,"cell_type":"code"},{"source":"From the above it could be noted that the there is a fall in accuracy upon feature elimination based on low variance. Lets try recursive feature elimination which involves a exhaustive search of best features using recursion.","metadata":{"_uuid":"28bd1adce9f895dc7c030b8b521850b224f10b43","_cell_guid":"86b16ec9-13f9-4091-b95c-5ff9afb4523b"},"cell_type":"markdown"},{"source":"<h2>Recursive Feature Elimination</h2>","metadata":{"_uuid":"453a549961b84ffe3720d7c89de78366d1358107","_cell_guid":"ec1f782c-c0df-4a9e-81bc-41ea6b4c6281"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"7e65982157b95f97e631da0073c88be9d6d753e8","collapsed":true,"_cell_guid":"4fb8683a-3242-4c95-9eaa-1ac5cc7307a5"},"source":"from sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"98f09570ea185445c90b39669da68893d6a529ff","_cell_guid":"2982972d-f146-49aa-9a6b-df473402ff9c"},"source":"clf=svm.SVC(kernel='linear')\nrfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(2),\n              scoring='accuracy')\nrfecv.fit(Train_Data_X,Train_Data_Y)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"a8dc1ae639bfdba9133bd53e94a8f3bbd5bb2fc9","_cell_guid":"2b9b1e7a-a754-45b3-91de-222c7c3b96ad"},"source":"print(\"Optimal number of features : %d\" % rfecv.n_features_)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"a484f205d3f9f384095a4fe5756c76b75c30263b","_cell_guid":"3ddc11dc-ec84-4cbb-b706-24bfae4b0142"},"source":"print(max(rfecv.grid_scores_))","execution_count":null,"cell_type":"code"},{"source":"Although eliminating features led to just a 1% improvement in accuracy , it could lead to a much simpler model making it less prone to overfitting. Let us evaluate the model on our final validation test set. ","metadata":{"_uuid":"73579b7ee465528f5c60571ade1e76b9bb644395","_cell_guid":"e95489a9-e465-430e-8b00-c2391d0b7195"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"71798672528230efb8e898a389b151519a655b46","collapsed":true,"_cell_guid":"ec782032-a620-4c05-9d0c-66eb14713f26"},"source":"print(\"Accuracy: \",cross_val_score(rfecv,Test_Data_X,Test_Data_Y,cv=5).mean())","execution_count":null,"cell_type":"code"},{"source":"From the above it could be noted that the algorithm peforms well even on the test set. ","metadata":{"_uuid":"cd4ea665e1a5b25bf313275fb783dacc66d08a03","_cell_guid":"be7a46d5-ef69-4466-ac9f-004e14db401c"},"cell_type":"markdown"}]}