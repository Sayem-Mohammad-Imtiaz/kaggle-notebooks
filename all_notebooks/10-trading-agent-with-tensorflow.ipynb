{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## 使用 Tensorflow 訓練交易員\n\n本篇的原理是透過既有市場資料訓練出 Agent 執行對應的策略並做到最佳優化，訓練完成的模型將可用於交易評估，必須注意的是市場中充滿黑天鵝，不同市場間無法通用，比如：當你使用 SPY 訓練的 Agent 無法應用在 FB、AMZN，反之亦此，大家可以下載已經訓練好的 SPY Agent(DQN_ep10.h5) 去執行\n\n> 目前本算法只支持 [買入、賣出、持平] 信號，可以 [做多long、做空short] 以及整合 backtrade 的算法還需要一段時間\n>\n> 本次算法參考了 [TA 指標](https://github.com/bukosabino/ta)、[Agent](https://github.com/Albert-Z-Guo/Deep-Reinforcement-Stock-Trading/blob/master/train.py)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from shutil import copyfile\n\ncopyfile(src = \"../input/quantitative-trading/keras-rl/requirements.txt\", dst = \"../working/requirements.txt\")\ncopyfile(src = \"../input/quantitative-trading/keras-rl/saved_models/DQN_ep5.h5\", dst = \"../working/DQN_ep5.h5\")\ncopyfile(src = \"../input/quantitative-trading/keras-rl/saved_models/DQN_ep10.h5\", dst = \"../working/DQN_ep10.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 安裝相依套件"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -r requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import time\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom empyrical import sharpe_ratio\nfrom scipy.signal import argrelextrema\nfrom statsmodels.nonparametric.kernel_regression import KernelReg\nfrom collections import deque,defaultdict\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 所有會用到的工具，之後解釋"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Portfolio:\n    def __init__(self, balance=50000):\n        self.initial_portfolio_value = balance\n        self.balance = balance\n        self.inventory = []\n        self.return_rates = []\n        self.portfolio_values = [balance]\n        self.buy_dates = []\n        self.sell_dates = []\n\n    def reset_portfolio(self):\n        self.balance = self.initial_portfolio_value\n        self.inventory = []\n        self.return_rates = []\n        self.portfolio_values = [self.initial_portfolio_value]\n\n        \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x))\n\n\ndef stock_close_prices(key):\n    prices = []\n    lines = open(\"../input/quantitative-trading/\" + key + \".csv\", \"r\").read().splitlines()\n    for line in lines[1:]:\n        prices.append(float(line.split(\",\")[4]))\n    return prices\n\n\ndef generate_price_state(stock_prices, end_index, window_size):\n    start_index = end_index - window_size\n    if start_index >= 0:\n        period = stock_prices[start_index:end_index+1]\n    else:\n        # if end_index cannot suffice window_size, pad with prices on start_index\n        period = -start_index * [stock_prices[0]] + stock_prices[0:end_index+1]\n    return sigmoid(np.diff(period))\n\n\ndef generate_portfolio_state(stock_price, balance, num_holding):\n    return [np.log(stock_price), np.log(balance), np.log(num_holding + 1e-6)]\n\n\ndef generate_combined_state(end_index, window_size, stock_prices, balance, num_holding):\n    prince_state = generate_price_state(stock_prices, end_index, window_size)\n    portfolio_state = generate_portfolio_state(stock_prices[end_index], balance, num_holding)\n    return np.array([np.concatenate((prince_state, portfolio_state), axis=None)])\n\n\ndef treasury_bond_daily_return_rate():\n    r_year = 2.75 / 100  # approximate annual U.S. Treasury bond return rate\n    return (1 + r_year)**(1 / 365) - 1\n\n\ndef maximum_drawdown(portfolio_values):\n    end_index = np.argmax(np.maximum.accumulate(portfolio_values) - portfolio_values)\n    if end_index == 0:\n        return 0\n    beginning_iudex = np.argmax(portfolio_values[:end_index])\n    return (portfolio_values[end_index] - portfolio_values[beginning_iudex]) / portfolio_values[beginning_iudex]\n\n\ndef evaluate_portfolio_performance(agent):\n    portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n    print(\"--------------------------------\")\n    print('投資組合總價值：         ${:.2f}'.format(agent.portfolio_values[-1]))\n    print('投資組合現金餘額：       ${:.2f}'.format(agent.balance))\n    print('投資組合持股數量：        {}'.format(len(agent.inventory)))\n    print('總收益:                ${:.2f}'.format(portfolio_return))\n    print('平均/每日投报率:         {:.3f}%'.format(np.mean(agent.return_rates) * 100))\n    print('調整後價格夏普比率：      {:.3f}'.format(sharpe_ratio(np.array(agent.return_rates)), risk_free=treasury_bond_daily_return_rate()))\n    print('最大回撤：              {:.3f}%'.format(maximum_drawdown(agent.portfolio_values) * 100))\n    print(\"--------------------------------\")\n    return portfolio_return\n\n\ndef plot_portfolio_transaction_history(stock_name, agent):\n\tportfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n\tdf = pd.read_csv('../output/{}.csv'.format(stock_name))\n\tbuy_prices = [df.iloc[t, 4] for t in agent.buy_dates]\n\tsell_prices = [df.iloc[t, 4] for t in agent.sell_dates]\n\tplt.figure(figsize=(15, 5), dpi=100)\n\tplt.title('{} 總回報率 {}： ${:.2f}'.format(agent.model_type, stock_name, portfolio_return))\n\tplt.plot(df['Date'], df['Close'], color='black', label=stock_name)\n\tplt.scatter(agent.buy_dates, buy_prices, c='green', alpha=0.5, label='buy')\n\tplt.scatter(agent.sell_dates, sell_prices,c='red', alpha=0.5, label='sell')\n\tplt.xticks(np.linspace(0, len(df), 10))\n\tplt.ylabel('Price')\n\tplt.legend()\n\tplt.grid()\n\tplt.show()\n\n\ndef buy_and_hold_benchmark(stock_name, agent):\n    df = pd.read_csv('./data/{}.csv'.format(stock_name))\n    dates = df['Date']\n    num_holding = agent.initial_portfolio_value // df.iloc[0, 4]\n    balance_left = agent.initial_portfolio_value % df.iloc[0, 4]\n    buy_and_hold_portfolio_values = df['Close']*num_holding + balance_left\n    buy_and_hold_return = buy_and_hold_portfolio_values.iloc[-1] - agent.initial_portfolio_value\n    return dates, buy_and_hold_portfolio_values, buy_and_hold_return\n\n\ndef plot_portfolio_performance_comparison(stock_name, agent):\n\tdates, buy_and_hold_portfolio_values, buy_and_hold_return = buy_and_hold_benchmark(stock_name, agent)\n\tagent_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n\tplt.figure(figsize=(15, 5), dpi=100)\n\tplt.title('{} vs. HODL'.format(agent.model_type))\n\tplt.plot(dates, agent.portfolio_values, color='green', label='{} 總收益： ${:.2f}'.format(agent.model_type, agent_return))\n\tplt.plot(dates, buy_and_hold_portfolio_values, color='blue', label='{} HODL 總收益： ${:.2f}'.format(stock_name, buy_and_hold_return))\n\t# compare with S&P 500 performance in 2018\n\tif '^GSPC' not in stock_name:\n\t\tdates, GSPC_buy_and_hold_portfolio_values, GSPC_buy_and_hold_return = buy_and_hold_benchmark('^GSPC_2018', agent)\n\t\tplt.plot(dates, GSPC_buy_and_hold_portfolio_values, color='red', label='S&P 500 2018 HODL 總收益： ${:.2f}'.format(GSPC_buy_and_hold_return))\n\tplt.xticks(np.linspace(0, len(dates), 10))\n\tplt.ylabel('資產價值 ($)')\n\tplt.legend()\n\tplt.grid()\n\tplt.show()\n\n\ndef plot_all(stock_name, agent):\n    fig, ax = plt.subplots(2, 1, figsize=(16,8), dpi=100)\n\n    portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n    df = pd.read_csv('../output/{}.csv'.format(stock_name))\n    buy_prices = [df.iloc[t, 4] for t in agent.buy_dates]\n    sell_prices = [df.iloc[t, 4] for t in agent.sell_dates]\n    ax[0].set_title('{} 總收益 {}: ${:.2f}'.format(agent.model_type, stock_name, portfolio_return))\n    ax[0].plot(df['Date'], df['Close'], color='black', label=stock_name)\n    ax[0].scatter(agent.buy_dates, buy_prices, c='green', alpha=0.5, label='buy')\n    ax[0].scatter(agent.sell_dates, sell_prices,c='red', alpha=0.5, label='sell')\n    ax[0].set_ylabel('Price')\n    ax[0].set_xticks(np.linspace(0, len(df), 10))\n    ax[0].legend()\n    ax[0].grid()\n\n    dates, buy_and_hold_portfolio_values, buy_and_hold_return = buy_and_hold_benchmark(stock_name, agent)\n    agent_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n    ax[1].set_title('{} vs. HODL'.format(agent.model_type))\n    ax[1].plot(dates, agent.portfolio_values, color='green', label='{} 總收益： ${:.2f}'.format(agent.model_type, agent_return))\n    ax[1].plot(dates, buy_and_hold_portfolio_values, color='blue', label='{} HODL 總收益： ${:.2f}'.format(stock_name, buy_and_hold_return))\n    # compare with S&P 500 performance in 2018 if stock is not S&P 500\n    if '^GSPC' not in stock_name:\n    \tdates, GSPC_buy_and_hold_portfolio_values, GSPC_buy_and_hold_return = buy_and_hold_benchmark('^GSPC_2018', agent)\n    \tax[1].plot(dates, GSPC_buy_and_hold_portfolio_values, color='red', label='S&P 500 2018 HODL 總收益： ${:.2f}'.format(GSPC_buy_and_hold_return))\n    ax[1].set_ylabel('資產價值 ($)')\n    ax[1].set_xticks(np.linspace(0, len(df), 10))\n    ax[1].legend()\n    ax[1].grid()\n\n    plt.subplots_adjust(hspace=0.5)\n    plt.savefig('../output/{}_trading_history.png'.format(stock_name))\n    plt.show()\n\n\ndef plot_portfolio_returns_across_epochs(model_name, returns_across_epochs):\n    len_epochs = len(returns_across_epochs)\n    plt.figure(figsize=(15, 5), dpi=100)\n    plt.title('投資組合收益')\n    plt.plot(returns_across_epochs, color='black')\n    plt.xlabel('Epoch')\n    plt.ylabel('Return Value')\n    plt.grid()\n    plt.savefig('../output/{}_returns_ep{}.png'.format(model_name, len_epochs))\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Agent(Portfolio):\n    def __init__(self, state_dim, balance, is_eval=False, model_name=\"\"):\n        super().__init__(balance=balance)\n        self.model_type = 'DQN'\n        self.state_dim = state_dim\n        self.action_dim = 3  # hold, buy, sell\n        self.memory = deque(maxlen=100)\n        self.buffer_size = 60\n\n        self.gamma = 0.95\n        self.epsilon = 1.0  # initial exploration rate\n        self.epsilon_min = 0.01  # minimum exploration rate\n        self.epsilon_decay = 0.995 # decrease exploration rate as the agent becomes good at trading\n        self.is_eval = is_eval\n        self.model = load_model('../{}.h5'.format(model_name)) if is_eval else self.model()\n\n        self.tensorboard = TensorBoard(log_dir='./logs/DQN_tensorboard', update_freq=90)\n        self.tensorboard.set_model(self.model)\n\n    def model(self):\n        model = Sequential()\n        model.add(Dense(units=64, input_dim=self.state_dim, activation='relu'))\n        model.add(Dense(units=32, activation='relu'))\n        model.add(Dense(units=8, activation='relu'))\n        model.add(Dense(self.action_dim, activation='softmax'))\n        model.compile(loss='mse', optimizer=Adam(lr=0.01))\n        return model\n\n    def reset(self):\n        self.reset_portfolio()\n        self.epsilon = 1.0 # reset exploration rate\n\n    def remember(self, state, actions, reward, next_state, done):\n        self.memory.append((state, actions, reward, next_state, done))\n\n    def act(self, state):\n        if not self.is_eval and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        options = self.model.predict(state)\n        return np.argmax(options[0])\n\n    def experience_replay(self):\n        # retrieve recent buffer_size long memory\n        mini_batch = [self.memory[i] for i in range(len(self.memory) - self.buffer_size + 1, len(self.memory))]\n\n        for state, actions, reward, next_state, done in mini_batch:\n            if not done:\n                Q_target_value = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n            else:\n                Q_target_value = reward\n            next_actions = self.model.predict(state)\n            next_actions[0][np.argmax(actions)] = Q_target_value\n            history = self.model.fit(state, next_actions, epochs=1, verbose=0)\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n        return history.history['loss'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = \"DQN\"\n# 資料過大會失準，盡量以年度、季度來拆分資料集\nstock_name = \"SPY_2018\"\nwindow_size = 10\nnum_epoch = 5\ninitial_balance = 50000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stock_prices = stock_close_prices(stock_name)\ntrading_period = len(stock_prices) - 1\nreturns_across_epochs = []\nnum_experience_replay = 0\naction_dict = {0: 'Hold', 1: 'Buy', 2: 'Sell'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agent = Agent(state_dim=window_size + 3, balance=initial_balance)\n\ndef hold(actions):\n    # encourage selling for profit and liquidity\n    next_probable_action = np.argsort(actions)[1]\n    if next_probable_action == 2 and len(agent.inventory) > 0:\n        max_profit = stock_prices[t] - min(agent.inventory)\n        if max_profit > 0:\n            sell(t)\n            actions[next_probable_action] = 1 # reset this action's value to the highest\n            return 'HODL', actions\n\ndef buy(t):\n    if agent.balance > stock_prices[t]:\n        agent.balance -= stock_prices[t]\n        agent.inventory.append(stock_prices[t])\n        return '購買: ${:.2f}'.format(stock_prices[t])\n\ndef sell(t):\n    if len(agent.inventory) > 0:\n        agent.balance += stock_prices[t]\n        bought_price = agent.inventory.pop(0)\n        profit = stock_prices[t] - bought_price\n        global reward\n        reward = profit\n        return '賣出: ${:.2f} | 獲利： ${:.2f}'.format(stock_prices[t], profit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'交易市場：     {stock_name}')\nprint(f'資料長度：     {trading_period} 天')\nprint(f'均線維度：     {window_size} 天')\nprint(f'訓練次數：     {num_epoch}')\nprint(f'使用模型：     {model_name}')\nprint('初始持有資金：  ${:,}'.format(initial_balance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nfor e in range(1, num_epoch + 1):\n    print(f'\\n訓練迭代： {e}/{num_epoch}')\n\n    agent.reset() # reset to initial balance and hyperparameters\n    state = generate_combined_state(0, window_size, stock_prices, agent.balance, len(agent.inventory))\n\n    for t in range(1, trading_period + 1):\n        if t % 100 == 0:\n            print(f'\\n-------------------期數： {t}/{trading_period}-------------------')\n\n        reward = 0\n        next_state = generate_combined_state(t, window_size, stock_prices, agent.balance, len(agent.inventory))\n        previous_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n\n        actions = agent.model.predict(state)[0]\n        action = agent.act(state)\n        \n        # execute position\n        print('批次： {}\\tHODL 訊號： {:.4} \\t買入 訊號： {:.4} \\t賣出 訊號： {:.4}'.format(t, actions[0], actions[1], actions[2]))\n        if action != np.argmax(actions): print(f\"\\t\\t'{action_dict[action]}' is an exploration.\")\n        if action == 0: # hold\n            execution_result = hold(actions)\n        if action == 1: # buy\n            execution_result = buy(t)      \n        if action == 2: # sell\n            execution_result = sell(t)        \n        \n        # check execution result\n        if execution_result is None:\n            reward -= treasury_bond_daily_return_rate() * agent.balance  # missing opportunity\n        else:\n            if isinstance(execution_result, tuple): # if execution_result is 'Hold'\n                actions = execution_result[1]\n                execution_result = execution_result[0]   \n            print(execution_result)                \n\n        # calculate reward\n        current_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n        unrealized_profit = current_portfolio_value - agent.initial_portfolio_value\n        reward += unrealized_profit\n\n        agent.portfolio_values.append(current_portfolio_value)\n        agent.return_rates.append((current_portfolio_value - previous_portfolio_value) / previous_portfolio_value)\n\n        done = True if t == trading_period else False\n        agent.remember(state, actions, reward, next_state, done)\n\n        # update state\n        state = next_state\n\n        # experience replay\n        if len(agent.memory) > agent.buffer_size:\n            num_experience_replay += 1\n            loss = agent.experience_replay()\n            print('迭代： {}\\t損失： {:.2f}\\t執行動作： {}\\t獎勵： {:.2f}\\t現金餘額： {:.2f}\\t持有股數： {}'.format(e, loss, action_dict[action], reward, agent.balance, len(agent.inventory)))\n            agent.tensorboard.on_batch_end(num_experience_replay, {'loss': loss, 'portfolio value': current_portfolio_value})\n\n        if done:\n            portfolio_return = evaluate_portfolio_performance(agent)\n            returns_across_epochs.append(portfolio_return)\n\n    # save models periodically\n    if e % 5 == 0:\n        agent.model.save('../output/saved_models/DQN_ep' + str(e) + '.h5')\n        print('model saved')\n\nprint('總訓練時間： {0:.2f} 分鐘'.format((time.time() - start_time)/60))\nplot_portfolio_returns_across_epochs(model_name, returns_across_epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}