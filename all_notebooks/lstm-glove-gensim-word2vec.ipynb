{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n%config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook | hit tab\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n# df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df[:1].review.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_df = pd.read_csv('../input/movie-rating-cleaned/cleaned.csv', usecols=['review','sentiment'])\ncleaned_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(cleaned_df.sentiment)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets do some text cleaning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-reinstall\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install BeautifulSoup4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import preprocess_kgptalkie as ps\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# from tqdm._tqdm_notebook import tqdm_notebook\n# tqdm_notebook.pandas()\n\n# def text_preprocessing(df,col_name):\n#     column = col_name\n#     df[column] = df[column].progress_apply(lambda x:str(x).lower())\n#     df[column] = df[column].progress_apply(lambda x: ps.cont_exp(x)) #you're -> you are; i'm -> i am\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_emails(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_html_tags(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_urls(x))\n\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_special_chars(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_accented_chars(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.make_base(x)) #ran -> run,\n#     return(df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaned_df = text_preprocessing(df,'review')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaned_df.to_csv('/kaggle/working/cleaned.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaned_df = pd.read_csv('/kaggle/working/cleaned.csv', usecols=['review', 'sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaned_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## REmoving our custom stopwords \n# stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n#              \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n#              \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n#              \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n#              \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n#              \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n#              \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n#              \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n#              \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n#              \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n#              \"your\", \"yours\", \"yourself\", \"yourselves\" ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaned_df['review'] = cleaned_df.review.progress_apply(lambda x: ' '.join(x for x in x.split() if x not in stopwords ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(cleaned_df.review, cleaned_df.sentiment, test_size = 0.2, stratify = cleaned_df.sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing necesary libraries \nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we are gonna take num_words =10000 means 10000 max_features so our vocabulary size will be 10000\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenisation\nnum_words = 10000\ntokens = Tokenizer(num_words, split=' ')\ntotal_reviews = pd.concat([X_train,X_test], axis = 0)\ntokens.fit_on_texts(total_reviews.values) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokens.word_index) + 1 \nvocab_size # this shows vocabulary size ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this process will convert words into indexes or we can say words into sequences( it will be a 1d so using paddng we will convert into same length and convert into 2d matrix for passing in embedding layer)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tokens = tokens.texts_to_sequences(X_train)\nnp.array(X_train_tokens).shape\nlen(X_train_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_tokens = tokens.texts_to_sequences(X_test)\nnp.array(X_test_tokens).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### now lets do some padding to make our texts of equal length\nmax_length = max([len(s.split()) for s in cleaned_df.review])\nmax_length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train_pad = pad_sequences(X_train_tokens, maxlen = max_length,padding = 'post' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pad","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"post padding method means all zeros will come at the ending ! we must follow the same padding method for whole process "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_pad = pad_sequences(X_test_tokens, maxlen = max_length, padding  = 'post')\nX_test_pad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using GPU supported lstm and gru layers \nimport tensorflow\n# tf.compat.v1.keras.layers.CuDNNLSTM\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM,CuDNNGRU\nfrom tensorflow.keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here vocab_size means the number of words are we gonna train on , its same as the number of features , len(tokens.word_index) it shows all words even we know that it will use only n_words frequent words"},{"metadata":{},"cell_type":"markdown","source":"here word embedding layer outputs a 3D tenor for the LSTM and GRU or simply saying RNN "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nEMBEDDING_DIM = 100 # this means the embedding layer will create  a vector in 100 dimension\nmodel = Sequential()\nmodel.add(Embedding(num_words,EMBEDDING_DIM, input_length= X_train_pad.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(100)))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics = 'accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EarlyStopping and ModelCheckpoint\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\nmc = ModelCheckpoint('/kaggle/working/model.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size*EMBEDDING_DIM# number of parameters in our model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_embedding = model.fit(X_train_pad,y_train, epochs = 15, batch_size = 120, validation_data=(X_test_pad, y_test),verbose = 1, callbacks= [es, mc]  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10,5))\nax.plot(history_embedding.history['accuracy'])\nax.plot(history_embedding.history['val_accuracy'])\nax.plot(history_embedding.history['val_loss'])\nax.legend(['accuracy','val_accuracy','val_loss'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this model is not that good slightly overfitting "},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow import keras\n# model = keras.models.load_model('/kaggle/working/model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction = model.predict(X_test_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred =np.where(y_prediction>=0.5,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing on our real data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sample_1 = 'this movie is fantastic! : really like it because it is so good'\ntest_sample_2 = 'good movie'\ntest_sample_3 = 'i like acting in the movie'\ntest_sample_4 = 'if you like action , then this movie is waiting for you'\ntest_sample_5 = 'worst movie'\ntest_sample_6 = 'hmm its just okay movie'\ntest_sample_7 = 'its worst but nice'\ntest_samples= [test_sample_1, test_sample_7, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6]\ntest_samples_tokens = tokens.texts_to_sequences(test_samples)\ntest_samples_pad = pad_sequences(test_samples_tokens, maxlen = max_length, padding = 'post')\ntest_samples_pad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = np.where(model.predict(test_samples_pad)>=0.5,1,0)\nprint(model.predict(test_samples_pad))\n\npd.DataFrame({'text':test_samples,'prediction':prediction.flatten()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 1.Using Gensim word2vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading gensim model\nimport gensim.downloader as api\nglove_gensim  = api.load('glove-wiki-gigaword-100') #300 dimension","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_gensim.wv.most_similar(\"king\",topn=5)\n# from here we can calculate the most similar words according to the given words\n# this is already pretrained model as glove vector ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(glove_gensim.wv.vocab) # vocab size in glove_gensim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we need to create a weight matrix or we can easily generate keras embedding layer directly by using gensim method"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Vector dimension',glove_gensim['cricket'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_size = 100\ngensim_weight_matrix = np.zeros((num_words,vector_size))\ngensim_weight_matrix.shape\n# here num_words means we can say that max_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor word, index in tokens.word_index.items():\n    if index < num_words: # since index starts with zero \n        if word in glove_gensim.wv.vocab:\n            gensim_weight_matrix[index] = glove_gensim[word]\n        else:\n            gensim_weight_matrix[index] = np.zeros(100)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### buidling model"},{"metadata":{},"cell_type":"markdown","source":"here our embedding layers is taking our text data and converting them into a vector by using the glove_emb_matrix \nand here we are passing our text data into embedding \n\n* we can also add embedding weights by using layers index\n\nmodel.layers[0].set_weights([embedding_matrix])\n\n\nmodel.layers[0].trainable = False"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nEMBEDDING_DIM = 100 # this means the embedding layer will create  a vector in 100 dimension\nmodel_glove_gensim = Sequential()\nmodel_glove_gensim.add(Embedding(input_dim = num_words,# the whole vocabulary size \n                          output_dim = EMBEDDING_DIM, # vector space dimension\n                          input_length= X_train_pad.shape[1], # max_len of text sequence\n                          weights = [gensim_weight_matrix],trainable = False))\nmodel_glove_gensim.add(Dropout(0.2))\nmodel_glove_gensim.add(Bidirectional(CuDNNLSTM(100)))\nmodel_glove_gensim.add(Dense(1, activation = 'sigmoid'))\nmodel_glove_gensim.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics = 'accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_glove_gensim = model_glove_gensim.fit(X_train_pad,y_train, epochs = 15, batch_size = 120, validation_data=(X_test_pad, y_test),verbose = 1, callbacks= [es]  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualisation of history\nfig, ax = plt.subplots(figsize = (10,5))\nax.plot(history_glove_gensim.history['accuracy'])\nax.plot(history_glove_gensim.history['val_accuracy'])\nax.plot(history_glove_gensim.history['val_loss'])\nax.legend(['accuracy','val_accuracy','val_loss'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we can use gensim method for creating wordembedding automatically \n(glove_gensim.wv_get_keras_embedding(train_embeddings = False)\n\nbut the convergence epochs are much higher so it is not preffered for making model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nEMBEDDING_DIM = 100 # this means the embedding layer will create  a vector in 100 dimension\nmodel_glove_gensim = Sequential()\nmodel_glove_gensim.add(glove_gensim.wv.get_keras_embedding(train_embeddings=False)) #pretrained\nmodel_glove_gensim.add(Dropout(0.2))\nmodel_glove_gensim.add(Bidirectional(CuDNNLSTM(100)))\nmodel_glove_gensim.add(Dense(1, activation = 'sigmoid'))\nmodel_glove_gensim.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics = 'accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_glove_gensim = model_glove_gensim.fit(X_train_pad,y_train, epochs = 15, batch_size = 120, validation_data=(X_test_pad, y_test),verbose = 1, callbacks= [es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model_glove_gensim.predict(X_test_pad)\ny_pred = np.where(pred>0.5,1,0)\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Using Glove dataset for word2vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_vectors = {}\nfile = open('../input/glove6b/glove.6B.100d.txt', encoding = 'utf-8')\nfor line in file:\n    values = line.split()\n    word = values[0]\n    vectors = np.asarray(values[1:])\n    glove_vectors[word] = vectors\nfile.close()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(glove_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_size = glove_vectors['house'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_emb_matrix = np.zeros((num_words,vector_size))\nglove_emb_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor word, index in tokens.word_index.items():\n    if index < num_words: # since index starts with zero \n        embedding_vector = glove_vectors.get(word)\n        if embedding_vector is not None:\n            glove_emb_matrix[index] = embedding_vector\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_emb_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here our embedding layers is taking our text data and converting them into a vector by using the glove_emb_matrix \nand here we are passing our text data into embedding "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nEMBEDDING_DIM = 100 # this means the embedding layer will create  a vector in 100 dimension\nmodel_glove = Sequential()\nmodel_glove.add(Embedding(input_dim = num_words,# the whole vocabulary size \n                          output_dim = glove_emb_matrix.shape[1], # vector space dimension\n                          input_length= X_train_pad.shape[1], # max_len of text sequence\n    ))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(Bidirectional(CuDNNLSTM(100)))\nmodel_glove.add(Dense(1, activation = 'sigmoid'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove.layers[0].set_weights([glove_emb_matrix])\nmodel_glove.layers[0].trainable = False\nmodel_glove.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics = 'accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* we can also add embedding weights by using layers index\n\nmodel.layers[0].set_weights([embedding_matrix])\n\n\nmodel.layers[0].trainable = False"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_glove = model_glove.fit(X_train_pad,y_train, epochs = 10, batch_size = 120, validation_data=(X_test_pad, y_test),verbose = 1, callbacks= [es, mc]  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using pretrained glove vectors we will have no problem of overfitting and we will get a very generalised model and good model"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10,5))\nax.plot(history_glove.history['accuracy'])\nax.plot(history_glove.history['val_accuracy'])\nax.plot(history_glove.history['val_loss'])\nax.legend(['accuracy','val_accuracy','val_loss'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from graph we are sure that the glove vector embedding is outperforming the embedding layer training performance because the glove vectors are more robust and we can use on the big data "},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model_glove.predict(X_test_pad)\ny_pred = np.where(pred>0.5,1,0)\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_glove = np.where(model.predict(test_samples_pad)>=0.5,1,0)\nprint(model.predict(test_samples_pad))\n\npd.DataFrame({'text':test_samples,'prediction':prediction_glove.flatten()})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# saving the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove.save_weights('/kaggle/working/model_glove_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove.save('/kaggle/working/model_glove.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gensim word embedding is best for creating models and we can use lots of numerical operation like calculating average doc2vec and many more, using gensim.get_keras_word_embedding is not so good because the convergence time in this method is very big so its not preferred \n and we also can use glove dataset for word2vec but we cant use numerical operation on that **"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}