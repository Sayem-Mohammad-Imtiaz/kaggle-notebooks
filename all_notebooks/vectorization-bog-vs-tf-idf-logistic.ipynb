{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=+4 color=\"Black\"><center><b>Bag of Words & TF-IDF</b></center></font>\n<font size=-1 color=\"Black\"><center><b>*Series: All about NLP by Data Tattle </b></right></font>"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"Red\"><b>Please Upvote if you like the work</b></font>\n\n### It gives motivation to a working professional (like me) to contribute more."},{"metadata":{},"cell_type":"markdown","source":"### About this notebook\n\n#### This notebook is a part of Series \"[All about NLP](https://www.kaggle.com/datatattle/all-about-nlp)\" and will cover vectorization using Bag of Words & TF-IDF\n\n\n\n![](https://miro.medium.com/max/2428/0*Qq8FcR-mgnvjWZLQ.gif)"},{"metadata":{},"cell_type":"markdown","source":"Contents:\n\n* [1. Bag of Words](#1)\n* [2. TF-IDF](#2)\n* [3. BOG vs. TF-IDF](#3)    "},{"metadata":{},"cell_type":"markdown","source":"### Bag of Words\n\nWe can not feed texts (words) directly into the NLP or ML models as all the algorithms work on numbers. Hence BOG is used to preprocess the texts. Here TOTAL occurence of EACH word is counted and kept as a BAG OF WORDS. \n"},{"metadata":{},"cell_type":"markdown","source":"### Types of BOW\n#### a. Count Occurrence\n#### b. Normalized Count Occurrence\n#### c. TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Bag of Words</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"#### a. Count Occurrence\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re \nimport nltk \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding='latin1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train['text'] = train.OriginalTweet\ntrain[\"text\"] = train[\"text\"].astype(str)\n\ntest['text'] = test.OriginalTweet\ntest[\"text\"] = test[\"text\"].astype(str)\n\n# Data has 5 classes, let's convert them to 3\n\ndef classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"2\"\n    elif x == \"Extremely Negative\":\n        return \"0\"\n    elif x == \"Negative\":\n        return \"0\"\n    elif x ==  \"Positive\":\n        return \"2\"\n    else:\n        return \"1\"\n    \n\ntrain['label']=train['Sentiment'].apply(lambda x:classes_def(x))\ntest['label']=test['Sentiment'].apply(lambda x:classes_def(x))\n\n\ntrain.label.value_counts(normalize= True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x_train = train.text\ny_train = train.label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using NLTK\ntext = \"what is going to happen next in data science \\\nis a mystery what has happened is history it is an \\\ninterdisciplinary field that uses scientific method \\\nprocesses algorithms and systems to extract knowledge \\\nand insights from many structural and unstructured data \\\ndata science is related to data mining machine learning and big data\"\ntxt = nltk.sent_tokenize(text)\n\nword2count = {} \nfor data in txt: \n    words = nltk.word_tokenize(data) \n    for word in words: \n        if word not in word2count.keys(): \n            word2count[word] = 1\n        else: \n            word2count[word] += 1\n\nprint(word2count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This was simple text, however while modeling words may reach zillions. And we need to set a cut off as we don't use all the words.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import heapq \nfreq_words = heapq.nlargest(200, word2count, key=word2count.get)\nfreq_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [] \nfor data in txt: \n    vector = [] \n    for word in freq_words: \n        if word in nltk.word_tokenize(data): \n            vector.append(1) \n        else: \n            vector.append(0) \n    X.append(vector) \nX = np.asarray(X)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using SkLearn\ntext = \"Natural Language Processing (NLP) is a sub-field of artificial intelligence \\\nthat deals understanding and processing human language. In light of new advancements \\\nin machine learning, many organizations have begun applying natural language processing \\\nfor translation, chatbots and candidate filtering\"\n\ncount_vec = CountVectorizer()\ncount_occurs = count_vec.fit_transform([text])\ncount_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\ncount_occur_df.columns = ['Word', 'Count']\ncount_occur_df.sort_values('Count', ascending=False, inplace=True)\ncount_occur_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b. Normalized Count Occurrence"},{"metadata":{},"cell_type":"markdown","source":" If you think that extremely high frequency may dominate the result and causing model bias. Normalization can be apply to pipeline easily."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Natural Language Processing (NLP) is a sub-field of artificial intelligence \\\nthat deals understanding and processing human language. In light of new advancements \\\nin machine learning, many organizations have begun applying natural language processing \\\nfor translation, chatbots and candidate filtering\"\n\nnorm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')\nnorm_count_occurs = norm_count_vec.fit_transform([text])\nnorm_count_occur_df = pd.DataFrame((count, word) for word, count in zip(\n    norm_count_occurs.toarray().tolist()[0], norm_count_vec.get_feature_names()))\nnorm_count_occur_df.columns = ['Word', 'Count']\nnorm_count_occur_df.sort_values('Count', ascending=False, inplace=True)\nnorm_count_occur_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. TF-IDF</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Term Frequency - inverse document frequency is defined as a numeric statistic that is intended to reflect how important a word is to a document in a collection/ corpus\n\n### TF\n\nIt is a measure of how frequently a term (t) appears in a document:\n\n    tf = n / number of terms in a document\n \nIn above example \ntf (data)   = 5/41\ntf (science)= 2/41\n\n\n### IDF\n\nIDF is a measure of how important a term is\n\n    idf =  log (number of documents / number of documents with term 't')\n\nSince we took one text above, hence number of documents will be 1. But in practical word there are millions of documents. So let's assume we had 5 documents in total but data existed in one.\n\nSo, IDF for our text is:\nidf(data) = log(5/5)\n\n\n    tf-idf = tf * idf\n    \n#### Words with a higher score are more important, and those with a lower score are less important\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Natural Language Processing (NLP) is a sub-field of artificial intelligence \\\nthat deals understanding and processing human language. In light of new advancements \\\nin machine learning, many organizations have begun applying natural language processing \\\nfor translation, chatbots and candidate filtering\"\n\ntfidf_vec = TfidfVectorizer()\ntfidf_count_occurs = tfidf_vec.fit_transform([text])\ntfidf_count_occur_df = pd.DataFrame((count, word) for word, count in zip(\n    tfidf_count_occurs.toarray().tolist()[0], tfidf_vec.get_feature_names()))\ntfidf_count_occur_df.columns = ['Word', 'Count']\ntfidf_count_occur_df.sort_values('Count', ascending=False, inplace=True)\ntfidf_count_occur_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"stop_words = ['a', 'an', 'the']\n\n# Basic cleansing\ndef cleansing(text):\n    # Tokenize\n    tokens = text.split(' ')\n    # Lower case\n    tokens = [w.lower() for w in tokens]\n    # Remove stop words\n    tokens = [w for w in tokens if w not in stop_words]\n    return ' '.join(tokens)\n\n# All-in-one preproce\ndef preprocess_x(x):\n    processed_x = [cleansing(text) for text in x]\n    \n    return processed_x\n\ndef build_model(mode):\n    # Intent to use default paramaters for show case\n    vect = None\n    if mode == 'count':\n        vect = CountVectorizer()\n    elif mode == 'tf':\n        vect = TfidfVectorizer(use_idf=False, norm='l2')\n    elif mode == 'tfidf':\n        vect = TfidfVectorizer()\n    else:\n        raise ValueError('Mode should be either count or tfidf')\n    \n    return Pipeline([\n        ('vect', vect),\n        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))\n    ])\n\ndef pipeline(x, y, mode):\n    processed_x = preprocess_x(x)\n    \n    model_pipeline = build_model(mode)\n    cv = KFold(n_splits=5, shuffle=True)\n    \n    scores = cross_val_score(model_pipeline, processed_x, y, cv=cv, scoring='accuracy')\n    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n    \n    return model_pipeline","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"x = preprocess_x(x_train)\ny = y_train\n    \nmodel_pipeline = build_model(mode='count')\nmodel_pipeline.fit(x, y)\n\nprint('Number of Vocabulary: %d'% (len(model_pipeline.named_steps['vect'].get_feature_names())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. BOG vs. TF-IDF</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Using Count Vectorizer------')\nmodel_pipeline = pipeline(x_train, y_train, mode='count')\n\nprint('Using TF Vectorizer------')\nmodel_pipeline = pipeline(x_train, y_train, mode='tf')\n\nprint('Using TF-IDF Vectorizer------')\nmodel_pipeline = pipeline(x_train, y_train, mode='tfidf')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classifier used is Logistic Regression. \n#### Count BoW performs better than Tf-Idf in our case"},{"metadata":{},"cell_type":"markdown","source":"When to use BOW over Embeddings?\n\n1. Building a baseline model. \n2. If your dataset is small and context is domain specific, BoW may work better than Word Embedding. Context is very domain specific which means that you cannot find corresponding Vector from pre-trained word embedding models (GloVe, fastText etc)"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"Green\"><b>Related Work:</b></font>\n\nNext is Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)\n\n### See [here](https://www.kaggle.com/datatattle/all-about-nlp) for related work"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"Green\"><b>Please Upvote if you liked the work</b></font>"},{"metadata":{},"cell_type":"markdown","source":"\n![#Precious](https://i.imgur.com/5YSC6pg.gif)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}