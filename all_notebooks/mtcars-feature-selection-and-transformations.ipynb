{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso, ElasticNet\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mtcars=pd.read_csv('../input/mtcars/mtcars.csv')\nmtcars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mtcars1=mtcars.iloc[:,1:]\nmtcars1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mtcars1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mtcars1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformation"},{"metadata":{},"cell_type":"markdown","source":"### SQRT"},{"metadata":{"trusted":true},"cell_type":"code","source":"mtcar4=mtcars1.transform(lambda x:x**0.5)\nX_wo1=mtcar4.drop(['mpg'],axis=1)\nY_wo1=mtcar4['mpg'].values\nX_const_wo1=sm.add_constant(X_wo1)\nmodel_wo1=sm.OLS(Y_wo1,X_const_wo1).fit()\nmodel_wo1.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=mtcars1.drop(['mpg'],axis=1)\nY=mtcars1.mpg\nvif_sqrt=[variance_inflation_factor(X_const_wo1.values,i) for i in range(X_const_wo1.shape[1])]\npd.DataFrame({'vif':vif_sqrt[1:]},index=X.columns).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_wo1=X_wo1[['drat','vs']]\nx_train1,x_test1,y_train1,y_test1=train_test_split(X_wo1,Y_wo1,test_size=0.3,random_state=1)\nlin_reg_log=LinearRegression()\nlin_reg_log.fit(x_train1,y_train1)\nprint('R^2 for train:',lin_reg_log.score(x_train1,y_train1))\nprint('R^2 for test:',lin_reg_log.score(x_test1,y_test1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOG"},{"metadata":{"trusted":true},"cell_type":"code","source":"mtcar5=mtcars1[['disp','hp','drat','wt','qsec','mpg']]\nmt_log=mtcar5.transform(lambda x:np.log(x))\nmt_log[['cyl','vs','am','gear','carb']]=mtcars1[['cyl','vs','am','gear','carb']]\nmt_log.head()\nX_wo1=mt_log.drop(['mpg'],axis=1)\nY_wo1=mt_log['mpg'].values\n\nX_const_wo1=sm.add_constant(X_wo1)\nmodel_wo1=sm.OLS(Y_wo1,X_const_wo1).fit()\nmodel_wo1.summary()\n\nvif_sqrt=[variance_inflation_factor(X_const_wo1.values,i) for i in range(X_const_wo1.shape[1])]\nvif_pd=pd.DataFrame({'vif':vif_sqrt[1:]},index=X.columns).T\nprint(vif_pd)\n\nX_wo1=X_wo1[['hp','gear','am','vs']]\nx_train1,x_test1,y_train1,y_test1=train_test_split(X_wo1,Y_wo1,test_size=0.3,random_state=1)\nlin_reg_log=LinearRegression()\nlin_reg_log.fit(x_train1,y_train1)\nprint()\n\nprint('R^2 for train:',lin_reg_log.score(x_train1,y_train1))\nprint('R^2 for test:',lin_reg_log.score(x_test1,y_test1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inverse"},{"metadata":{"trusted":true},"cell_type":"code","source":"mtcar5=mtcars1[['disp','hp','drat','wt','qsec','mpg']]\nmt_log=mtcar5.transform(lambda x:1/x)\nmt_log[['cyl','vs','am','gear','carb']]=mtcars1[['cyl','vs','am','gear','carb']]\nmt_log.head()\nX_wo1=mt_log.drop(['mpg'],axis=1)\nY_wo1=mt_log['mpg'].values\nX_const_wo1=sm.add_constant(X_wo1)\nmodel_wo1=sm.OLS(Y_wo1,X_const_wo1).fit()\nmodel_wo1.summary()\nvif_sqrt=[variance_inflation_factor(X_const_wo1.values,i) for i in range(X_const_wo1.shape[1])]\nvif_pd=pd.DataFrame({'vif':vif_sqrt[1:]},index=X_wo1.columns).T\nprint(vif_pd)\n\nX_wo1=X_wo1[['drat','carb','vs']]\nx_train1,x_test1,y_train1,y_test1=train_test_split(X_wo1,Y_wo1,test_size=0.3,random_state=1)\nlin_reg_log=LinearRegression()\nlin_reg_log.fit(x_train1,y_train1)\nprint()\n\nprint('R^2 for train:',lin_reg_log.score(x_train1,y_train1))\nprint('R^2 for test:',lin_reg_log.score(x_test1,y_test1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* By Tansformation we get that log transformation gave us best R^2 for train: 0.7969614608648559\n* SQRT Transformation get R^2 for train: 0.5788558777828829\n* Inverse Transformation get R^2 for train: 0.6410026585549855\n* Thus Log gave us best result"},{"metadata":{},"cell_type":"markdown","source":"### ----------------------------------------------------------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"### Pearson Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor=mtcars1.corr()\ncor1=cor['mpg']\nfeatu=cor1[abs(cor1)>0.5][1:]# [1:] remove mpg from list\n\nmult_cor=mtcars1[['cyl', 'disp', 'hp', 'drat', 'wt', 'vs', 'am', 'carb','mpg']].corr()\ncor_max=max(abs(featu.values))\nfinal=featu[abs(featu.values)==cor_max]\nfinal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you see all are correlated to each other. So we need to select which one has highest correlation with target variable\n* Using Pearson Correlation we get 'wt' as most relevant feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=mtcars1[['wt']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wrapper Methods"},{"metadata":{},"cell_type":"markdown","source":"### Backward elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=mtcars1.drop(['mpg'],axis=1)\nY=mtcars1.mpg\nmodel=sm.OLS(Y,X).fit()\nmodel.pvalues","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin=LinearRegression()\ncols=list(X.columns)\nselect_feat=[]\n\nwhile(len(cols)>0):\n    p=[]\n    X1=X[cols]\n    model1=sm.OLS(Y,X1).fit()\n    p=pd.Series(model1.pvalues,index=X1.columns)\n    pmax=max(p)\n    if(pmax>0.05):\n        feature_with_p_max = p.idxmax()\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselect_feat=cols\nselect_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Using backward elimination we get 'wt', 'qsec', 'am' as most relevant features "},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=mtcars1[['wt', 'qsec', 'am']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recursive Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin=LinearRegression()\nX.columns\nhighsc=0\nnof=0\nsupport_score=[]\nnoflist=np.arange(1,11)\nfor n in noflist:\n    x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)\n    rfe=RFE(lin,n)\n    X_train_rfe=rfe.fit_transform(x_train,y_train)\n    X_test_rfe=rfe.transform(x_test)\n    lin.fit(X_train_rfe,y_train)\n    score=lin.score(X_test_rfe,y_test)\n    if(score>highsc):\n        highsc=score\n        nof=n\n        support_score=rfe.support_\n        \ntemp=pd.Series(support_score,index=X.columns)\nprint('No of optimum features:',n)\nprint('SCore for optimum features:',highsc)\nprint('Features Selected:\\n')\ntemp[temp==True].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Using Recursive Feature Selection we get 'drat', 'wt', 'gear', 'carb' as most relevant features "},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=mtcars1[['drat', 'wt', 'gear', 'carb']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection on basis of vif"},{"metadata":{"trusted":true},"cell_type":"code","source":"vif=[variance_inflation_factor(X.values, j) for j in range(X.shape[1])]\nvif_pd=pd.Series(vif,index=X.columns)\nvif_pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_vif(x):\n    output=pd.DataFrame()\n    vif=[variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n    cols = x.shape[1]\n    thresh=5.0\n    for i in range(cols):\n        print('Iteration:',i)\n        a=np.argmax(vif)\n        print('Max vif found at:',a)\n        if(vif[a]>thresh):\n            if i==0:\n                output=x.drop(x.columns[a],axis=1)\n                vif=[variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n            else:\n                output=output.drop(output.columns[a],axis=1)\n                vif=[variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        else:\n            break\n    return output.columns\ncalculate_vif(X).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Using Feature Selection on basis of vif we get 'disp', 'vs', 'am' as most relevant features "},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=mtcars1[['disp', 'vs', 'am']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LASSO"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=mtcars1.drop(['mpg'],axis=1)\nY=mtcars1.mpg\n\nreg = LassoCV()\nreg.fit(X, Y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,Y))\ncoef = pd.Series(reg.coef_, index = X.columns)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff=coef.sort_values()\ncoeff.plot(kind='bar')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=mtcars1[['disp', 'hp']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ElasticNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg1=ElasticNet()\nreg1.fit(X, Y)\nprint(\"Best alpha using built-in ElasticNet: %f\" % reg1.alpha)\nprint(\"Best score using built-in ElasticNet: %f\" %reg1.score(X,Y))\ncoef_elastic = pd.Series(reg1.coef_, index = X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff=coef_elastic.sort_values()\ncoeff.plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=mtcars1[['wt','carb','cyl','disp', 'hp','qsec']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"* Using Pearson Correlation method we get R^2 as 0.66. However it also has overfitting problem\n* Using Feature Selection on basis of vif we get R^2 as 0.60\n* Using Recursive Feature Selection we get R^2 as 0.70. We get overfitting using this methodd\n* Using Backward Elimination we get R^2 as 0.75\n* Using LASSO we get get R^2 as 0.60\n* Using ElasticNet we get get R^2 as 0.72"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}