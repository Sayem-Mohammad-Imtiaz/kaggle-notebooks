{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading the library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nnltk.download('punkt')","metadata":{"id":"KiI5Fv9tLAMS","outputId":"8b933c4b-93b0-469c-a1bc-c62ed49ac80d","execution":{"iopub.status.busy":"2021-08-30T11:20:11.361774Z","iopub.execute_input":"2021-08-30T11:20:11.362434Z","iopub.status.idle":"2021-08-30T11:20:11.371876Z","shell.execute_reply.started":"2021-08-30T11:20:11.362375Z","shell.execute_reply":"2021-08-30T11:20:11.371072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# laoding the datset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/olympic-news-dataset/olympic_news.csv',encoding= 'unicode_escape')","metadata":{"id":"WBiuanjkt3Hw","execution":{"iopub.status.busy":"2021-08-30T11:20:11.373486Z","iopub.execute_input":"2021-08-30T11:20:11.374016Z","iopub.status.idle":"2021-08-30T11:20:11.389279Z","shell.execute_reply.started":"2021-08-30T11:20:11.373959Z","shell.execute_reply":"2021-08-30T11:20:11.388099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"id":"9zbPQIVHLXCT","outputId":"a5f01284-b93b-45be-b046-1f2506dc0131","execution":{"iopub.status.busy":"2021-08-30T11:20:11.391292Z","iopub.execute_input":"2021-08-30T11:20:11.391594Z","iopub.status.idle":"2021-08-30T11:20:11.405765Z","shell.execute_reply.started":"2021-08-30T11:20:11.391564Z","shell.execute_reply":"2021-08-30T11:20:11.404774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"hLk8KkoVLXN7","outputId":"ebee10a4-36a7-495c-9c7e-3bd844f426a2","execution":{"iopub.status.busy":"2021-08-30T11:20:11.407524Z","iopub.execute_input":"2021-08-30T11:20:11.407815Z","iopub.status.idle":"2021-08-30T11:20:11.422521Z","shell.execute_reply.started":"2021-08-30T11:20:11.407786Z","shell.execute_reply":"2021-08-30T11:20:11.421315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I will drop the article_title column.\n# Reason: Well I am trying to keep things simple and easy.","metadata":{"id":"glHl6xENLXaF","execution":{"iopub.status.busy":"2021-08-30T11:20:11.423906Z","iopub.execute_input":"2021-08-30T11:20:11.424244Z","iopub.status.idle":"2021-08-30T11:20:11.427877Z","shell.execute_reply.started":"2021-08-30T11:20:11.424202Z","shell.execute_reply":"2021-08-30T11:20:11.426959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['article_title'], axis = 1, inplace=True)","metadata":{"id":"JYYjJqgFLXok","execution":{"iopub.status.busy":"2021-08-30T11:20:11.429104Z","iopub.execute_input":"2021-08-30T11:20:11.429617Z","iopub.status.idle":"2021-08-30T11:20:11.439568Z","shell.execute_reply.started":"2021-08-30T11:20:11.429566Z","shell.execute_reply":"2021-08-30T11:20:11.438684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"KBSrNHIcLYDk","outputId":"2cc91cbc-c3c6-429a-d763-604d60f54d71","execution":{"iopub.status.busy":"2021-08-30T11:20:11.441684Z","iopub.execute_input":"2021-08-30T11:20:11.442139Z","iopub.status.idle":"2021-08-30T11:20:11.457771Z","shell.execute_reply.started":"2021-08-30T11:20:11.442103Z","shell.execute_reply":"2021-08-30T11:20:11.456671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now I am looking at first 3 article_text using the while loop. \n#It will help me in getting proper understanding of the article text.\ni = 0\nwhile (i < 3):   \n    i = i + 1\n    print(df['article_text'][i], sep=' ')","metadata":{"id":"aaPS5UV8L57t","outputId":"e9dd81ee-7c3e-4766-db87-34a7e7d38ab9","execution":{"iopub.status.busy":"2021-08-30T11:26:51.741392Z","iopub.execute_input":"2021-08-30T11:26:51.741896Z","iopub.status.idle":"2021-08-30T11:26:51.751028Z","shell.execute_reply.started":"2021-08-30T11:26:51.741854Z","shell.execute_reply":"2021-08-30T11:26:51.749592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{"id":"LUw1umqMMnT8"}},{"cell_type":"markdown","source":"# **1. TOKENIZATION (Spliting the whole paragraph into sentence)**","metadata":{"id":"EACgsOt1Mr-U"}},{"cell_type":"markdown","source":"what is tokenization\n\nTokenization is a way of separating a piece of text into smaller units called tokens. \nHere, tokens can be either words, characters, or subwords. \nHence, tokenization can be broadly classified into 3 types\n1.word, 2.character, and 3.subword (n-gram characters) tokenization.","metadata":{"id":"GGCVF15ZNW7J"}},{"cell_type":"markdown","source":"In this case we are splitting the paragraph into sentences.","metadata":{"id":"bjMSTGx7Nf4j"}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nsentences = [sent_tokenize(s) for s in df['article_text']]\n\nsentences = [y for x in sentences for y in x] # flatten list\n\n# Above I have used list comprehension technique instead of conventional for loop method.\n#checking the first 3 sentences.\nsentences[:3]","metadata":{"id":"LBR-IzTfL52W","execution":{"iopub.status.busy":"2021-08-30T11:28:23.289757Z","iopub.execute_input":"2021-08-30T11:28:23.290246Z","iopub.status.idle":"2021-08-30T11:28:23.32557Z","shell.execute_reply.started":"2021-08-30T11:28:23.290211Z","shell.execute_reply":"2021-08-30T11:28:23.324448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. WORD EMBEDDING (Then spliting the sentecnec into words.)**","metadata":{"id":"Rm_zCdzrNsWL"}},{"cell_type":"markdown","source":"In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text.\nWord embeddings are a type of word representation that allows words with similar meaning to have a similar representation.","metadata":{"id":"0axu0xbiNwPT"}},{"cell_type":"markdown","source":"I am going to use Glove for word embedding.\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. \nTraining is performed on aggregated global word-word co-occurrence statistics from a corpus, \nand the resulting representations showcase interesting linear substructures of the word vector space\n\nRead more here https://nlp.stanford.edu/projects/glove/","metadata":{"id":"wlidMHulN2wE"}},{"cell_type":"code","source":"#downloading the \"glove.6B.100d.txt\"\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip","metadata":{"id":"1AODq8CgNFse","execution":{"iopub.status.busy":"2021-08-30T11:20:11.535475Z","iopub.execute_input":"2021-08-30T11:20:11.536208Z","iopub.status.idle":"2021-08-30T11:23:17.339876Z","shell.execute_reply.started":"2021-08-30T11:20:11.536158Z","shell.execute_reply":"2021-08-30T11:23:17.338872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","metadata":{"id":"Yj54S_14L5yE","execution":{"iopub.status.busy":"2021-08-30T11:23:17.343892Z","iopub.execute_input":"2021-08-30T11:23:17.344336Z","iopub.status.idle":"2021-08-30T11:23:36.440373Z","shell.execute_reply.started":"2021-08-30T11:23:17.344297Z","shell.execute_reply":"2021-08-30T11:23:36.439355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(word_embeddings)","metadata":{"id":"NiVLX_HVL5rm","outputId":"5fa45673-85f0-4905-806e-7cd40bbbfd6e","execution":{"iopub.status.busy":"2021-08-30T11:23:36.442897Z","iopub.execute_input":"2021-08-30T11:23:36.443383Z","iopub.status.idle":"2021-08-30T11:23:36.452234Z","shell.execute_reply.started":"2021-08-30T11:23:36.443326Z","shell.execute_reply":"2021-08-30T11:23:36.451363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Remove punctuations, special characters and numbers.**","metadata":{"id":"zXeJpbbdObxp"}},{"cell_type":"markdown","source":"Doing this will help in processing the text faster.","metadata":{"id":"-9gpDMHHO4WS"}},{"cell_type":"code","source":"clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \",  regex=True)","metadata":{"id":"KSsB-vCSL5jN","execution":{"iopub.status.busy":"2021-08-30T11:23:36.453547Z","iopub.execute_input":"2021-08-30T11:23:36.454002Z","iopub.status.idle":"2021-08-30T11:23:36.471381Z","shell.execute_reply.started":"2021-08-30T11:23:36.453952Z","shell.execute_reply":"2021-08-30T11:23:36.470558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(clean_sentences[0])\nprint(clean_sentences[1])\nprint(clean_sentences[2])","metadata":{"id":"Of9dYMwWL5SO","outputId":"7f37db6b-d60c-44d1-e717-8f1c82c78581","execution":{"iopub.status.busy":"2021-08-30T11:23:36.472453Z","iopub.execute_input":"2021-08-30T11:23:36.47289Z","iopub.status.idle":"2021-08-30T11:23:36.486775Z","shell.execute_reply.started":"2021-08-30T11:23:36.472859Z","shell.execute_reply":"2021-08-30T11:23:36.485835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**converting to lower case**\n\n**Reason:**\n\nI think for your particular use-case, it would be better to convert it to lowercase because ultimately, you will need to predict the words given a certain context. You probably won't be needing to predict sentence beginnings in your use-case. Also, if a noun is predicted you can capitalize it later. However consider the other way round. (Assuming your corpus is in English) Your model might treat a word which is in the beginning of a sentence with a capital letter different from the same word which appears later in the sentence but without any capital latter. This might lead to decline in the accuracy. Whereas I think, lowering the words would be a better trade off.","metadata":{"id":"dPj1pujdWM-r"}},{"cell_type":"code","source":"clean_sentences = [s.lower() for s in clean_sentences]","metadata":{"id":"Kyy-fZjaL5Cs","execution":{"iopub.status.busy":"2021-08-30T11:23:36.488132Z","iopub.execute_input":"2021-08-30T11:23:36.488624Z","iopub.status.idle":"2021-08-30T11:23:36.499191Z","shell.execute_reply.started":"2021-08-30T11:23:36.488578Z","shell.execute_reply":"2021-08-30T11:23:36.498206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Removing stops words**","metadata":{"id":"O35lWNXiWm08"}},{"cell_type":"markdown","source":"**What are the stop words?**\n\nThese are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\n\n**Why we remove the stop words?**\n\nStop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information. In order words, we can say that the removal of such words does not show any negative consequences on the model we train for our task.\nRemoval of stop words definitely reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training.","metadata":{"id":"lHxz876nWspb"}},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"id":"CvmbNG8FWmKa","outputId":"3bc350bb-14bd-4c05-bd88-9d9d8554658f","execution":{"iopub.status.busy":"2021-08-30T11:23:36.500535Z","iopub.execute_input":"2021-08-30T11:23:36.501054Z","iopub.status.idle":"2021-08-30T11:23:36.520494Z","shell.execute_reply.started":"2021-08-30T11:23:36.501016Z","shell.execute_reply":"2021-08-30T11:23:36.51918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:23:36.521878Z","iopub.execute_input":"2021-08-30T11:23:36.522229Z","iopub.status.idle":"2021-08-30T11:23:36.52809Z","shell.execute_reply.started":"2021-08-30T11:23:36.522195Z","shell.execute_reply":"2021-08-30T11:23:36.526856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","metadata":{"id":"XoN4WxLaWmhE","execution":{"iopub.status.busy":"2021-08-30T11:23:36.529843Z","iopub.execute_input":"2021-08-30T11:23:36.530209Z","iopub.status.idle":"2021-08-30T11:23:36.546739Z","shell.execute_reply.started":"2021-08-30T11:23:36.530176Z","shell.execute_reply":"2021-08-30T11:23:36.545612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","metadata":{"id":"3EipmM6QXV_m","execution":{"iopub.status.busy":"2021-08-30T11:23:36.548233Z","iopub.execute_input":"2021-08-30T11:23:36.548753Z","iopub.status.idle":"2021-08-30T11:23:36.576012Z","shell.execute_reply.started":"2021-08-30T11:23:36.548717Z","shell.execute_reply":"2021-08-30T11:23:36.575054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_sentences[0:5]","metadata":{"id":"Lm0PhSZYXV6S","outputId":"fce6ab58-d090-4576-b7e5-ad2b48c9d21b","execution":{"iopub.status.busy":"2021-08-30T11:23:36.577192Z","iopub.execute_input":"2021-08-30T11:23:36.577604Z","iopub.status.idle":"2021-08-30T11:23:36.584968Z","shell.execute_reply.started":"2021-08-30T11:23:36.577572Z","shell.execute_reply":"2021-08-30T11:23:36.583967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Vector representation of sentences**","metadata":{"id":"T5z2sa4z0VFi"}},{"cell_type":"code","source":"word_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","metadata":{"id":"DPfME2X0XVx6","execution":{"iopub.status.busy":"2021-08-30T11:23:36.586311Z","iopub.execute_input":"2021-08-30T11:23:36.586771Z","iopub.status.idle":"2021-08-30T11:23:55.316922Z","shell.execute_reply.started":"2021-08-30T11:23:36.586739Z","shell.execute_reply":"2021-08-30T11:23:55.315681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vector representation is prerequiste for applying similarity matrix.","metadata":{"execution":{"iopub.status.busy":"2021-08-30T11:23:55.320198Z","iopub.execute_input":"2021-08-30T11:23:55.32052Z","iopub.status.idle":"2021-08-30T11:23:55.325277Z","shell.execute_reply.started":"2021-08-30T11:23:55.320489Z","shell.execute_reply":"2021-08-30T11:23:55.324238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_vectors = []\nfor i in clean_sentences:\n  if len(i) != 0:\n    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n  else:\n    v = np.zeros((100,))\n  sentence_vectors.append(v)","metadata":{"id":"q5-UpSrI0gIO","execution":{"iopub.status.busy":"2021-08-30T11:23:55.326868Z","iopub.execute_input":"2021-08-30T11:23:55.327191Z","iopub.status.idle":"2021-08-30T11:23:55.356685Z","shell.execute_reply.started":"2021-08-30T11:23:55.327159Z","shell.execute_reply":"2021-08-30T11:23:55.35552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6. Similarity matrix**","metadata":{"id":"VGzjU6TO0-FA"}},{"cell_type":"markdown","source":"I will use cosine similarity for finding the similarity between the sentecnes. Sentences which has highest similairyt will be of more importance and we will rank them according to that and later on we will form the summarization using that. \n\n[Read more on cosine similarity.](https://www.machinelearningplus.com/nlp/cosine-similarity/#:~:text=Cosine%20similarity%20is%20a%20metric,in%20a%20multi%2Ddimensional%20space.&text=The%20smaller%20the%20angle%2C%20higher%20the%20cosine%20similarity.)","metadata":{"id":"QB2je1bb1EUD"}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity","metadata":{"id":"g7mIOowt2N_N","execution":{"iopub.status.busy":"2021-08-30T11:23:55.358214Z","iopub.execute_input":"2021-08-30T11:23:55.358566Z","iopub.status.idle":"2021-08-30T11:23:55.363312Z","shell.execute_reply.started":"2021-08-30T11:23:55.358532Z","shell.execute_reply":"2021-08-30T11:23:55.362453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_matrix = np.zeros([len(sentences), len(sentences)])\n# The above code will help me in forming the matrix of the size of sentences. ","metadata":{"id":"0gwzo6Mt2aRV","execution":{"iopub.status.busy":"2021-08-30T11:23:55.364346Z","iopub.execute_input":"2021-08-30T11:23:55.364639Z","iopub.status.idle":"2021-08-30T11:23:55.376804Z","shell.execute_reply.started":"2021-08-30T11:23:55.36461Z","shell.execute_reply":"2021-08-30T11:23:55.375496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(sentences)):\n  for j in range(len(sentences)):\n    if i != j:\n      similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]","metadata":{"id":"OXzRyPCK2ORG","execution":{"iopub.status.busy":"2021-08-30T11:23:55.378671Z","iopub.execute_input":"2021-08-30T11:23:55.379135Z","iopub.status.idle":"2021-08-30T11:24:32.920031Z","shell.execute_reply.started":"2021-08-30T11:23:55.379088Z","shell.execute_reply":"2021-08-30T11:24:32.918862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(similarity_matrix.shape)","metadata":{"id":"YfhXqE192OuW","outputId":"6558392a-af12-44e0-ad4c-554b08ae5e71","execution":{"iopub.status.busy":"2021-08-30T11:24:32.921429Z","iopub.execute_input":"2021-08-30T11:24:32.921774Z","iopub.status.idle":"2021-08-30T11:24:32.928275Z","shell.execute_reply.started":"2021-08-30T11:24:32.921737Z","shell.execute_reply":"2021-08-30T11:24:32.926919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **7. Converting similarity matrix sim_mat into a graph**","metadata":{"id":"tvBlDN6J4bHU"}},{"cell_type":"markdown","source":"The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings","metadata":{"id":"LNcETNUA71bL"}},{"cell_type":"code","source":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(nx_graph)","metadata":{"id":"syODLc0T2Orm","execution":{"iopub.status.busy":"2021-08-30T11:24:32.929736Z","iopub.execute_input":"2021-08-30T11:24:32.9302Z","iopub.status.idle":"2021-08-30T11:24:37.00136Z","shell.execute_reply.started":"2021-08-30T11:24:32.930169Z","shell.execute_reply":"2021-08-30T11:24:36.999853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **8. Summarization**","metadata":{"id":"gU1YotaN75iC"}},{"cell_type":"markdown","source":"Sorting the sentences on the basis of highest score","metadata":{"id":"0A4tkjrA8pkW"}},{"cell_type":"code","source":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","metadata":{"id":"NzTxFLOp2OLN","execution":{"iopub.status.busy":"2021-08-30T11:24:37.002951Z","iopub.execute_input":"2021-08-30T11:24:37.003334Z","iopub.status.idle":"2021-08-30T11:24:37.009734Z","shell.execute_reply.started":"2021-08-30T11:24:37.003297Z","shell.execute_reply":"2021-08-30T11:24:37.00866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract top 10 sentences as the summary\nfor i in range(10):\n  print(ranked_sentences[i][1])","metadata":{"id":"drS_4O5g2OB4","outputId":"d7c7fb6b-72ac-43c8-b248-f02df57d3656","execution":{"iopub.status.busy":"2021-08-30T11:24:37.011352Z","iopub.execute_input":"2021-08-30T11:24:37.011725Z","iopub.status.idle":"2021-08-30T11:24:37.026374Z","shell.execute_reply.started":"2021-08-30T11:24:37.011679Z","shell.execute_reply":"2021-08-30T11:24:37.024958Z"},"trusted":true},"execution_count":null,"outputs":[]}]}