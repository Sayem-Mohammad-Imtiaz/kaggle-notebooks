{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Starbucks sutomer loyalty prediciton","metadata":{}},{"cell_type":"markdown","source":"**Introduction**\n\nThe survey analysis results have a significant impact onto the business profitability in todays markets. Once running a coffee shop tt is vital to understand ASAP which service client prefer at most and which needed to be improved.\n\nIn this kernel I would like to estimate the factors that really influence the customers decision (loyalty) to continue visiting the StarBucks.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve, roc_auc_score, roc_curve \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/starbucks-customer-retention-malaysia-survey/Starbucks satisfactory survey encode cleaned.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Firstly, lets examine the feature that we will predict:\n\n0 - customer is not loyal\n\n1 - customer is loyal","metadata":{}},{"cell_type":"code","source":"df['loyal'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the feature loyal is imbalanced, thus we need to account for that once we will do the analysis","metadata":{}},{"cell_type":"code","source":"#The nest step is to assess the correlation betwenen the independent features and target variable\n\nplt.figure(figsize=(12,12))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we observe that some columns have values that do not disply any correlation,so they have a zero impact onto he target feature, thus we can drop them","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dropping constant features:**\n\nIn this step we will be removing the features which have constant features which are actually not important for solving the problem statement Variance Threshold Feature selector that removes all low-variance features. This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.","metadata":{}},{"cell_type":"code","source":"def VarianceThreshold(df):\n    ### It will zero variance features\n    from sklearn.feature_selection import VarianceThreshold\n    var_thres=VarianceThreshold(threshold=0)\n    var_thres.fit(df)\n    constant_columns = [column for column in df.columns\n                    if column not in df.columns[var_thres.get_support()]]\n    df.drop(constant_columns, axis = 1, inplace = True)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VarianceThreshold(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null values in our df","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nc= df.corr()\nsns.heatmap(c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once I have plotted the heatmap we can see that some features have similar correlation coeffcients (orange square at right down corner) as such, we need to drop them or impute by the feature engineering approachea to reduce the amount of predictors otherwise this may lead to model overfitting ","metadata":{}},{"cell_type":"markdown","source":"Firstly I plan to take care of the target variable (loyalty), we need to balance its values as we have the imbalanced data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(columns = ['loyal'])\ny = df['loyal']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#using SMOTE to balance the dataset\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nos = SMOTE(random_state=0)\nX_train_os,y_train_os=os.fit_resample(X_train,y_train)\nX_test_os,y_test_os=os.fit_resample(X_test,y_test)\nprint(\"The number of y_train classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of y_train classes after fit {}\".format(Counter(y_train_os)))\nprint(\"The number of y_test classes before fit {}\".format(Counter(y_test)))\nprint(\"The number of y_test classes after fit {}\".format(Counter(y_test_os)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two popular feature selection techniques that can be used for categorical input data and a categorical (class) target variable.\n\nThey are:\n\n1. Chi-Squared Statistic. \n2. Mutual Information Statistic.\n","metadata":{}},{"cell_type":"markdown","source":"Chi-Squared Statistic.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\n\nfs = SelectKBest(score_func=chi2, k='all')\nfs.fit(X_train, y_train)\nX_train_fs = fs.transform(X_train_os)\nX_test_fs = fs.transform(X_test_os)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n\tprint('Feature %d: %f' % (i, fs.scores_[i]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = pd.Series(fs.scores_)\n\nscores.index = X_train_os.columns\nscores.sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's plot the ordered mutual_info values per feature\nscores.sort_values(ascending=False).plot.bar(figsize=(20, 8))\nplt.xticks(fontsize= 22)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mutual Information Statistic.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X_train_os, y_train_os)\nmutual_info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train_os.columns\nmutual_info.sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's plot the ordered mutual_info values per feature\nmutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))\nplt.xticks(fontsize= 22)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_os.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, it is not clear which features I should use for preictions because the two aformentioned a;gorithms did not give us the same result, thus we need to apply something else to actually reduce the number of features in our dataframe. Before doing that I will show you the data overfitting after applying the all feature for the prediction of the target variable.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestClassifier\n\nestimators = {\n    'KNeighborsClassifier' :[KNeighborsClassifier()],\n    'Random Forest' :[RandomForestClassifier()]\n}\n\n\ndef mfit(estimators, X_train_om, y_train_os):\n    for m in estimators:\n        estimators[m][0].fit(X_train_os, y_train_os)\n        print(m+' fitted')\n\nmfit(estimators, X_train_os, y_train_os)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mpredict(estimators, X_test_os, y_test_os):\n    outcome = dict()\n    r_a_score = dict()\n    for m in estimators:\n        y_pred = estimators[m][0].predict(X_test_os)\n        #r_a_score[m] = roc_auc_score(y_test, y_pred)\n        outcome[m] = [y_pred, confusion_matrix(y_pred,y_test_os), classification_report(y_pred,y_test_os)]\n    return outcome, r_a_score\n\noutcome, r_a_score = mpredict(estimators, X_test_os, y_test_os)\nfor m in outcome:\n    print('------------------------'+m+'------------------------')\n    print(outcome[m][1])\n    print(outcome[m][2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Amazing results!! We got 92% accuracy with the high precision and recall values. The problem is that this model is very vulnerable to the variations if the features values, meaning that on the unknowm dataset we will get the lower accuracy , recall and precision. \n\nHowever, we do not know which feature to choose, as Feature selection did not give us the reliable data. Yes, we can choose one feature that is the same in both algorithms (Price rate) and make classification based only on this feature. We, however, want to examine the other service needs that nust be improved, thus ideally, we would like to use 30-40% of the features. If the number of variables in the data is very high, the regression models in this situation tend to perform badly. Besides, identifying important variables becomes challenging. In this scenario, we try to reduce the number of variables. \n\nLets say I do not want to decreease the numner of features as I am not sure whether the Mutual info or Chi2 give the me the reliable data, thus I will attemp to group the entire columns and to perform the feature reduction by the Factor analysis","metadata":{}},{"cell_type":"markdown","source":"\n\n# **Factor analysis**\n\nFactor analysis is widely utilized in market research, advertising, psychology, finance, and operation research. Market researchers use factor analysis to identify price-sensitive customers, identify brand features that influence consumer choice, and helps in understanding channel selection criteria for the distribution channel.\n\n**Assumptions:**\n\nThere are no outliers in data.\nSample size should be greater than the factor.\nThere should not be perfect multicollinearity.\nThere should not be homoscedasticity between the variables.\n\n**Factor Analysis implementation:**\n\nFactor Extraction: In this step, the number of factors and approach for extraction selected using variance partitioning methods such as principal components analysis and common factor analysis.\n\nFactor Rotation: In this step, rotation tries to convert factors into uncorrelated factors — the main goal of this step to improve the overall interpretability. There are lots of rotation methods that are available such as: Varimax rotation method, Quartimax rotation method, and Promax rotation method.\n\nFactor Analysis Vs. Principle Component Analysis\n1. PCA components explain the maximum amount of variance while factor analysis explains the covariance in data. \n2. PCA components are fully orthogonal to each other whereas factor analysis does not require factors to be orthogonal. \n3. PCA component is a linear combination of the observed variable while in FA, the observed variables are linear combinations of the unobserved variable or factor. \n4. PCA components are uninterpretable. In FA, underlying factors are labelable and interpretable. PCA is a kind of dimensionality reduction method whereas factor analysis is the latent variable method. \n5. PCA is a type of factor analysis. PCA is observational whereas FA is a modeling technique.\n\nThe info is taken from this (#reference)[https://www.datacamp.com/community/tutorials/introduction-factor-analysis]\n\n\nBefore applying the Factor Analysis we need to make sure that our dataframe is suitable for that:\nThe are two tests to be performed:\n\n1. Kaiser-Meyer-Olkin (KMO) test is used to check sampling adequacy for the overall data set. The statistic measures the proportion of variance among variables that could be common variance. This table shows two tests that indicate the suitability of your data for structure detection. The Kaiser-Meyer-Olkin Measure of Sampling Adequacy is a statistic that indicates the proportion of variance in your variables that might be caused by underlying factors. High values (close to 1.0) generally indicate that a factor analysis may be useful with your data. If the value is less than 0.50, the results of the factor analysis probably won't be very useful.(https://www.ibm.com/docs/en/spss-statistics/23.0.0?topic=detection-kmo-bartletts-test)\n\n2. Bartlett's test of sphericity tests the hypothesis that your correlation matrix is an identity matrix, which would indicate that your variables are unrelated and therefore unsuitable for structure detection. Small values (less than 0.05) of the significance level indicate that a factor analysis may be useful with your data.(https://www.ibm.com/docs/en/spss-statistics/23.0.0?topic=detection-kmo-bartletts-test)","metadata":{}},{"cell_type":"code","source":"pip install factor_analyzer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity,calculate_kmo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chi2,p = calculate_bartlett_sphericity(df)\nprint(\"Bartlett Sphericity Test\")\nprint(\"Chi squared value : \",chi2)\nprint(\"p value : \",p)\nif p < 0.05:\n    print('The FA might be usefull to reduce the number of features')\nelse:\n    print('The FA might be NOT usefull to reduce the number of features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity,calculate_kmo\nkmo_model = calculate_kmo(df)\nprint(kmo_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got this value: 0.6960887256821134. The values more than 0.5 can be questified by performing Factor analysis","metadata":{}},{"cell_type":"code","source":"df.columns\nx =df.drop('loyal', axis = 1)\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Subset of the data, the 14 columns containing the survey answers\nfrom factor_analyzer import FactorAnalyzer\nfa = FactorAnalyzer()\nfa.fit(x, 10)\n#Get Eigen values and plot them\nplt.figure(figsize=(15,10))\nev, v = fa.get_eigenvalues()\nev\nplt.scatter(range(1,x.shape[1]+1),ev, s = 800)\n\n\nplt.title('Scree Plot', fontsize = 20)\nplt.xlabel('Factors', fontsize = 20)\nplt.ylabel('Eigen Value', fontsize = 20)\nplt.xticks(np.arange(0, 20, 1.0))\n\nplt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the closest number is 7, so we will reduce the number of columns (features) to 7 from 19\n\nWhat are the factor loadings?\n\nThe factor loading is a matrix which shows the relationship of each variable to the underlying factor. It shows the correlation coefficient for observed variable and factor. It shows the variance explained by the observed variables.","metadata":{}},{"cell_type":"code","source":"x.shape\nx.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fa = FactorAnalyzer(7, rotation='varimax')\nfa.fit(x)\nprint(pd.DataFrame(fa.loadings_,index=x.columns))\n#loads = fa.loadings_\n#print(loads)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Basically, we can reduce the dimensions to the 7 groups meaning we need to assign new 7 columns in a new dataframe","metadata":{}},{"cell_type":"code","source":"print(pd.DataFrame(fa.get_communalities(),index=x.columns,columns=['Communalities']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum = 0\nsum_list = [i for i in fa.get_communalities()]\n\nfa.get_communalities().sum()/len(sum_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to check the average communality of teh factors. As MacCallum (2000,2001) suggested the average communality should be no lesser than 0.5 for 120 samples. In our case we have the lesser number, the difference however is not so critical (0.5 vs 0.47), thus I will use it is out study","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pingouin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pingouin as pg\n#Create the factors\nfactor1 = df[['productRate', 'priceRate', 'promoRate']] #service\nfactor2 = df[['gender', 'age', 'status']] #customer general info\nfactor3 = df[['location', 'visitNo']] #location\nfactor4 = df[['income', 'membershipCard']] #money\nfactor5 = df[['ambianceRate', 'serviceRate', 'wifiRate']] #service1\nfactor6 = df[['timeSpend', 'method']] #Inside\n\n\n\n\n#Get cronbach alpha\nfactor1_alpha = pg.cronbach_alpha(factor1)\nfactor2_alpha = pg.cronbach_alpha(factor2)\nfactor3_alpha = pg.cronbach_alpha(factor3)\nfactor4_alpha = pg.cronbach_alpha(factor4)\nfactor5_alpha = pg.cronbach_alpha(factor5)\nfactor6_alpha = pg.cronbach_alpha(factor6)\n\nprint(factor1_alpha, factor2_alpha, factor3_alpha, factor4_alpha, factor5_alpha, factor6_alpha)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_variables = fa.fit_transform(x)\nnew_variables","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Service'] = new_variables[:, 0]\ndf['Customer_info'] = new_variables[:, 1]\ndf['Location'] = new_variables[:, 2]\ndf['Money'] = new_variables[:, 3]\ndf['Service_1'] = new_variables[:, 4]\ndf['Inside'] = new_variables[:, 5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_factorized = df[['Service', 'Customer_info', 'Location', 'Money','Service_1','Inside', 'loyal']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_factorized.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_factorized.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df_factorized.drop(columns = ['loyal'])\ny = df_factorized['loyal']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#using SMOTE to balance the dataset\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nos = SMOTE(random_state=0)\nX_train_os,y_train_os=os.fit_resample(X_train,y_train)\nX_test_os,y_test_os=os.fit_resample(X_test,y_test)\nprint(\"The number of y_train classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of y_train classes after fit {}\".format(Counter(y_train_os)))\nprint(\"The number of y_test classes before fit {}\".format(Counter(y_test)))\nprint(\"The number of y_test classes after fit {}\".format(Counter(y_test_os)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import  XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier \n\n\nestimators = {\n    'KNeighborsClassifier' :[KNeighborsClassifier()],\n    'Random Forest' :[RandomForestClassifier()],\n    'XGBoost' :[XGBClassifier()],\n    'Logistic Regression': [LogisticRegression()],\n    'GaussianNB' :[GaussianNB()],\n    'Gradient Boost' :[GradientBoostingClassifier()],\n    'Decision Tree' :[DecisionTreeClassifier()],\n}\n\n\ndef mfit(estimators, X_train_om, y_train_os):\n    for m in estimators:\n        estimators[m][0].fit(X_train_os, y_train_os)\n        print(m+' fitted')\n\nmfit(estimators, X_train_os, y_train_os)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mpredict(estimators, X_test_os, y_test_os):\n    outcome = dict()\n    r_a_score = dict()\n    for m in estimators:\n        y_pred = estimators[m][0].predict(X_test_os)\n        #r_a_score[m] = roc_auc_score(y_test, y_pred)\n        outcome[m] = [y_pred, confusion_matrix(y_pred,y_test_os), classification_report(y_pred,y_test_os)]\n    return outcome, r_a_score\n\noutcome, r_a_score = mpredict(estimators, X_test_os, y_test_os)\nfor m in outcome:\n    print('------------------------'+m+'------------------------')\n    print(outcome[m][1])\n    print(outcome[m][2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}