{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> CORD-19 Solution Toolbox</h1>\n\n\nWe give here a minimal toolset to explore the dataset and start performing an EDA.  \n\n<a id=\"0\"></a>\nWe will provide the tools to:\n\n* <a href='#1'>Browse through the files in the collections;</a>  \n* <a href='#2'>Read content from JSON files;</a>  \n* <a href='#3'>Bulk process JSON files to extract content in a DataFrame;</a>  \n* <a href='#4'>Extract abstract content;</a>  \n* <a href='#5'>Visualize text content;</a>  \n* <a href='#6'>Visualize most frequent items in categorical features</a>  \n\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style='background:#13E3E1; border:0; color:black'><center>Load packages</center></h1>\n\nWe just load the minimum packages for now.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-26T10:08:39.139406Z","iopub.execute_input":"2021-06-26T10:08:39.140037Z","iopub.status.idle":"2021-06-26T10:08:39.772776Z","shell.execute_reply.started":"2021-06-26T10:08:39.139985Z","shell.execute_reply":"2021-06-26T10:08:39.771734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style='background:#13E3E1; border:0; color:black'><center>Explore the data</center></h1>","metadata":{}},{"cell_type":"code","source":"count = 0\nfile_exts = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        count += 1\n        file_ext = filename.split(\".\")[-1]\n        file_exts.append(file_ext)\n\nfile_ext_set = set(file_exts)\n\nprint(f\"Files: {count}\")\nprint(f\"Files extensions: {file_ext_set}\\n\\n=====================\\nFiles extension count:\\n=====================\")\nfile_ext_list = list(file_ext_set)\nfor fe in file_ext_list:\n    fe_count = file_exts.count(fe)\n    print(f\"{fe}: {fe_count}\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also look to the structure of directories, to see how the data is structured high-level:","metadata":{}},{"cell_type":"code","source":"count = 0\nfor root, folders, filenames in os.walk('/kaggle/input'):\n    print(root, folders)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Majority of files are in json format. The files are grouped in 3 main folders and multiple subfolders.\n","metadata":{}},{"cell_type":"code","source":"print(f\"pdf json: {len(os.listdir('/kaggle/input/CORD-19-research-challenge/document_parses/pdf_json'))}\")\nprint(f\"pmc json: {len(os.listdir('/kaggle/input/CORD-19-research-challenge/document_parses/pmc_json'))}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><small>Go to top</small></a>","metadata":{}},{"cell_type":"markdown","source":"\nWe provide some tools to explore the jsons.\n\n<a id=\"2\"></a><h1 style='background:#13E3E1; border:0; color:black'><center>Read a JSON File</center></h1>\n\n","metadata":{}},{"cell_type":"code","source":"json_folder_path = \"/kaggle/input/CORD-19-research-challenge/document_parses/pdf_json\"\njson_file_name = os.listdir(json_folder_path)[0]\nprint(json_file_name)\njson_path = os.path.join(json_folder_path, json_file_name)\n\nwith open(json_path) as json_file:\n    json_data = json.load(json_file)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use more easy, we can normalize the json. Here is the code.","metadata":{}},{"cell_type":"code","source":"json_data_df = pd.io.json.json_normalize(json_data)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The json was transformed in a row in a dataframe, with the column names resulted by aggregating the succesive levels of the json structure.   Let's check the result.","metadata":{}},{"cell_type":"code","source":"json_data_df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><small>Go to top</small></a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a><h1 style='background:#13E3E1; border:0; color:black'><center>Convert the folder in a dataframe</center></h1>\n\nLet's process now the folder. We will create a dataset with the data from the folder. We just take a subset of data (1000 samples).  For your work, just comment the line of code where the subset is declared and uncomment the line of code above, to process entire dataset.","metadata":{}},{"cell_type":"code","source":"print(f\"Files in folder: {len(os.listdir(json_folder_path))}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# to process all files, uncomment the next line and comment the line below\n# list_of_files = list(os.listdir(json_folder_path))\nlist_of_files = list(os.listdir(json_folder_path))[0:1000]\npmc_custom_license_df = pd.DataFrame()\n\nfor file in tqdm(list_of_files):\n    json_path = os.path.join(json_folder_path, file)\n    with open(json_path) as json_file:\n        json_data = json.load(json_file)\n    json_data_df = pd.io.json.json_normalize(json_data)\n    pmc_custom_license_df = pmc_custom_license_df.append(json_data_df)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pmc_custom_license_df.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><small>Go to top</small></a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a><h1 style='background:#13E3E1; border:0; color:black'><center>Extract abstract text</center></h1>\n\nLet's extract now abstract text from abstract column.  \n\nSimilar approach can be used to extract other parts from a dictionary-type field.\n","metadata":{}},{"cell_type":"code","source":"pmc_custom_license_df['abstract_text'] = pmc_custom_license_df['abstract'].apply(lambda x: x[0]['text'] if x else \"\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 500)\npmc_custom_license_df[['abstract', 'abstract_text']].head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><small>Go to top</small></a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a><h1 style='background:#13E3E1; border:0; color:black'><center>Visualize text content</center></h1>\n\nLet's present here few useful techniques for data visualization:\n\n* Worldclouds for text fields;\n\n* Countplot for category-type features.\n","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n    \ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n    ","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(pmc_custom_license_df['abstract_text'], title = 'Comm use subset - papers abstract - frequent words  (500 samples)')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(pmc_custom_license_df['bib_entries.BIBREF0.title'], title = 'Comm use subset - papers title - frequent words (500 samples)')\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><small>Go to top</small></a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a><h1 style='background:#13E3E1; border:0; color:black'><center>Visualize most frequent items in categorical features</center></h1>\n","metadata":{}},{"cell_type":"code","source":"pmc_custom_license_df.loc[((pmc_custom_license_df['bib_entries.BIBREF0.venue']==\"\") | ((pmc_custom_license_df['bib_entries.BIBREF0.venue'].isna()))), 'bib_entries.BIBREF0.venue'] = \"Not identified\"\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ndef plot_count(feature, title, df, size=1, show_percents=False):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[0:20], palette='Set3')\n    g.set_title(\"Number of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=10)\n    if(show_percents):\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()/2.,\n                    height + 3,\n                    '{:1.2f}%'.format(100*height/total),\n                    ha=\"center\") \n    ax.set_xticklabels(ax.get_xticklabels());\n    plt.show()    \n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_count('bib_entries.BIBREF0.venue', 'Comm use subset - Top 20 Journals (500 samples)', pmc_custom_license_df, 3.5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We eliminate \"Not identified\"","metadata":{}},{"cell_type":"code","source":"plot_count('bib_entries.BIBREF0.venue', 'Comm use subset - Top 20 Journals (500 samples)', \n           pmc_custom_license_df.loc[pmc_custom_license_df['bib_entries.BIBREF0.venue']!='Not identified'], 3.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><small>Go to top</small></a>","metadata":{}}]}