{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/column_2C_weka.csv')\nprint(plt.style.available) # available plot styles\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"833e5467abf281c0a63b29b8e8f268bc5ad01179"},"cell_type":"code","source":"# to see features and target variable\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a248a681de0914f1647cc69d349754a2f1b7ceab"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a92c4d52449a04b46865e09201cf3a31c1be9c3"},"cell_type":"markdown","source":"pd.plotting.scatter_matrix:\n\n* green: normal and red: abnormal\n* c: color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type"},{"metadata":{"trusted":true,"_uuid":"54c64c5b32eee4a223b1bd09e4582add6464dac9"},"cell_type":"code","source":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25b90576e4449f171eb6fa5db56ddfbab4c66d78"},"cell_type":"code","source":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d14a09e02fddbe0b5447fcca80da211da19c1e2"},"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier (n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67cf7981b170f13851a7ef5e446634597afd1ed1"},"cell_type":"markdown","source":"* train: use train set by fitting\n* test: make prediction on test set.\n* With train and test sets, fitted data and tested data are completely different\n* train_test_split(x,y,test_size = 0.3,random_state = 1)\n* x: features\n* y: target variables (normal,abnormal)\n* test_size: percentage of test size. Example test_size = 0.3, test size = 30% and train size = 70%\n* random_state: sets a seed. If this seed is same number, train_test_split() produce exact same split at each time\n* fit(x_train,y_train): fit on train sets\n* score(x_test,y_test)): predict and give accuracy on test sets"},{"metadata":{"trusted":true,"_uuid":"0eea54d171d52e9e3b64ab60ed45c3b4b7f8629c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n# print ('Prediction: {}'.format(prediction))\nprint('KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) #accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87f063d58eeb290d1ffad3fe8664f9846e382849"},"cell_type":"markdown","source":"Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n\nModel complexity:\n\n* K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace.\n* Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit.\n* At below, I range K value from 1 to 25(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 18, model is lead to underfit. Again accuracy is not enough. However look at when K is 18(best performance), accuracy has highest value almost 88%.\n\n"},{"metadata":{"trusted":true,"_uuid":"285cb97c16b55e5b07ffc3a5e91188e18267e7a6"},"cell_type":"code","source":"neig = np.arange (1,25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i,k in enumerate(neig):\n    #k from 1 to 25(exclude)\n    knn= KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    #test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n    \n#Plot\nplt.figure(figsize=[15,10])\nplt.plot(neig, test_accuracy,label = 'Test Accuracy')\nplt.plot(neig, train_accuracy, label = 'Train Accuracy')\nplt.legend()\nplt.title('-value VS accuracy-')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('GraphPlot.png')\nplt.show()\nprint(\"Best accuracy is {} K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd049f9abfc58c6d02dabe2c1a4f7ee97d5f4f8b"},"cell_type":"markdown","source":"Up to this point what i learn:\n* KNN\n    * How to split data\n    * How to fit, predict data\n    * How to measure medel performance (accuracy)\n    * How to choose hyperparameter (K)"},{"metadata":{"trusted":true,"_uuid":"a2f056d15b5771777d343bf135146184c168f505"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}