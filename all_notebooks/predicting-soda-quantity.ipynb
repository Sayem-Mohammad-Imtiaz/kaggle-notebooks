{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Problem Statement\n\n1) We have a dataset which included data about the demand of soda, using which we can predict the quantity. The data is in the form of a timeseries. \n\n2) We have created a Nested Cross Validation class, which also needs to be implemented while creating the model.  \n","metadata":{}},{"cell_type":"markdown","source":"# Importing all the libraries\n\nWe import pandas for reading and handling the data. Numpy provides us with very efficient ways to perform mathematical operations on arrays as well as easy array creation. Sklearn provides us with the tools that we need to build a model in the later stages of the pipeline. Seaborn and matplotlab are two important and highly used libraries for displaying and plotting graphs\n","metadata":{}},{"cell_type":"code","source":"#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Here is the code for our nested class","metadata":{}},{"cell_type":"code","source":"import logging\nfrom types import GeneratorType\nfrom sklearn.utils import indexable\nfrom sklearn.utils.validation import _num_samples\n\nLOGGER = logging.getLogger(__name__)\n\nclass NestedCV():\n    \n    \n    def __init__(self, k, delay: int = 0):\n         \n         \n\n        if k and k < 3:\n            raise ValueError(f'Cannot have n_splits less than 3 (k={k})')\n        self.k = k\n        \n        #super().__init__(k, shuffle=False, random_state=None)\n\n        \n        if delay < 0:\n            raise ValueError(f'Cannot have negative values of delay (delay={delay})')\n        self.delay = delay\n\n    \n\n    def split(self, X, date_column = None ,y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters:\n            X : array-like, shape (n_samples, n_features)\n                Training data, where n_samples is the number of samples  and n_features is the number of features.\n\n            y : array-like, shape (n_samples,)\n                Always ignored, exists for compatibility.\n\n            groups : array-like, with shape (n_samples,), optional\n                Always ignored, exists for compatibility.\n\n        Yields:\n            train : ndarray\n                The training set indices for that split.\n\n            test : ndarray\n                The testing set indices for that split.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)  # pylint: disable=unbalanced-tuple-unpacking\n        n_samples = _num_samples(X)\n        n_splits = self.k\n        n_folds = n_splits + 1\n        delay = self.delay\n\n        if n_folds > n_samples:\n            raise ValueError(f'Cannot have number of folds={n_folds} greater than the number of samples: {n_samples}.')\n\n        indices = np.arange(n_samples)\n        split_size = n_samples // n_folds\n\n        train_size = split_size * self.k\n        test_size = n_samples // n_folds\n        full_test = test_size + delay\n\n        if full_test + n_splits > n_samples:\n            raise ValueError(f'test_size\\\\({test_size}\\\\) + delay\\\\({delay}\\\\) = {test_size + delay} + '\n                             f'n_splits={n_splits} \\n'\n                             f' greater than the number of samples: {n_samples}. Cannot create fold logic.')\n\n        # Generate logic for splits.\n        # Overwrite fold test_starts ranges if force_step_size is specified.\n        \n        \n        step_size = split_size\n        range_start = (split_size - full_test) + split_size + (n_samples % n_folds)\n        test_starts = range(range_start, n_samples, step_size)\n\n        # Generate data splits.\n        for test_start in test_starts:\n            id_start =  0\n            # Ensure we always return a test set of the same size\n            if indices[test_start:test_start + full_test].size < full_test:\n                continue\n            yield (indices[id_start:test_start],\n                   indices[test_start + delay:test_start + full_test])\n\n#this is our main method.\n\nif __name__ == '__main__':\n    #creating fake values of x and x to test our algorithm\n    xx = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    yy = np.array([1, 2, 3, 4, 5, 6])\n    tscv = NestedCV(k=3)  # This is where we create an object of our class.\n    print(tscv)  \n    for train_index, test_index in tscv.split(xx):         # Calling our split function that yields a generator.\n        print('TRAIN:', train_index, 'TEST:', test_index)\n        X_train, X_test = xx[train_index], xx[test_index]\n        y_train, y_test = yy[train_index], yy[test_index]\n    print(\"---------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Reading The Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/predict-demand/train.csv\", index_col = 1)\nnot_index = pd.read_csv(\"../input/predict-demand/train.csv\") # This is the same as train but with a regular index, intsead of DateTime\ntest = pd.read_csv(\"../input/predict-demand/test.csv\",index_col = 1)\n\n\n# Converting the index to datetime format. \n\ntrain.index = pd.to_datetime(train.index)\ntest.index = pd.to_datetime(test.index)\n#not_index[\"date\"] = pd.to_datetime(not_index[\"date\"]) \n\n\nprint(train.head())\n\ndata1  = train.copy(deep = True) # We create a new value of train to mess around with, while keeping train intact.\n\ndata_cleaner = [data1,test,not_index] # Create a list of all the dataframes, which will help us while cleaning the Data. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T # describe gives us statistical insights about our data, while .T is used to transpose.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this we can infer that,  there are 6480 values in our train set and 1058 values in our test set.","metadata":{}},{"cell_type":"markdown","source":"# 2) Cleaning The Dataset","metadata":{}},{"cell_type":"code","source":"for dataset in data_cleaner:\n    \n    dataset.drop(columns = 'id', inplace = True) #We drop id because it doesn't help us in prediction\n    \n    # checking for null values\n    \n    dataset.info() \n    print(dataset.isnull().sum()) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Null Values","metadata":{}},{"cell_type":"markdown","source":"We can see that there are many missing values which would either need to be filled or removed. We can also infer that most of the columns are floats, so they dont need to be encoded anyhow. However columns like 'city,shop,brand,capacity,container' are of the object type, so we need to handle that as well.\n\nWe could impute our values based on the other values of the same column or we could just drop all the rows with Na values. Here we'll be dropping the rows because we wont lose alot of data.\n\nAlso we can see that the number of rows have increased from 6480 to 7560 after converting setting the index as datetime. Given that all those rows have null values we can just drop it.\n\nWe also drop the ID column because it doesn't help us in our prediction. We can also drop lat and long because we have city code and shop code, which gives us the location","metadata":{}},{"cell_type":"code","source":"# loopoing over our dataset list to drop null values.\nfor dataset in data_cleaner:\n    dataset.drop(dataset.index[6480:],inplace = True)  #drop all the values after 6480\n    dataset.dropna(axis = 0, how = 'any',inplace = True) # drop all the rows with null values\n    dataset.drop(columns = ['lat','long'], inplace = True) #We drop id because it doesn't help us in prediction\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Categorical variables","metadata":{}},{"cell_type":"markdown","source":"Here, we are using LabelEncoder to convert our categorical variables to numerical values. ","metadata":{}},{"cell_type":"code","source":"label = LabelEncoder() # creating label encoder instance\nfor dataset in data_cleaner:    \n    dataset['city_Code'] = label.fit_transform(dataset['city'])\n    dataset['shop_Code'] = label.fit_transform(dataset['shop'])\n    dataset['brand_Code'] = label.fit_transform(dataset['brand'])\n    dataset['container_Code'] = label.fit_transform(dataset['container'])\n    dataset['capacity_Code'] = label.fit_transform(dataset['capacity'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## After Pre-Processing Our Data","metadata":{}},{"cell_type":"code","source":"for dataset in data_cleaner:\n    \n    dataset.info()\n    print(dataset.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now as we can see, all the null values have been taken care of and all the categorical values have been converted to numerical values which our model will be able to understand, in all our dataframes.","metadata":{}},{"cell_type":"code","source":"data1.columns # Prints all the columns of data1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) EDA","metadata":{}},{"cell_type":"code","source":"count_plot_column_name = [ 'city_Code', 'shop_Code', 'brand_Code', 'container_Code', 'capacity_Code','quantity'] #this is a list of all the column names with numerical values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we are plotting a graph to count the number of observation in each type. Since count plot is mainly used for cateorical data, we will only use those. ","metadata":{}},{"cell_type":"code","source":"for i in count_plot_column_name:\n    \n    \n    plt.figure()\n    print(data1[i].value_counts())\n\n    sns.set_style('whitegrid')\n    sns.countplot(x=data1[i],data=data1, palette='YlGnBu_r')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation heatmap helps us in identifying how every variable is co-related with each other. It is really helpful in figuring which features play a role in deciding our outcome. The below function helps us eaily create a correlation matrix","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Our Model","metadata":{}},{"cell_type":"code","source":"column_names = [ 'pop','price', 'city_Code', 'shop_Code', 'brand_Code','container_Code', 'capacity_Code','quantity']\n\nx = data1[column_names]\ny = data1['quantity']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order for our data to make sense and not throw off the predictions with need to normalize/scale our data. SkLearn provides a great tool MinMaxScaler for this very purpose. We first fit the MinMaxScaler object to our train values. We then store out target variable in Y and delete it from the training features. giving us two scaled x and y variables. \n\nWe also apply PCA(Principal Component Analysis) in order to reduce the dimensionaltiy of our data. The steps followed are the same as MinMaxScaler.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\nnorm = MinMaxScaler().fit(x)\nx_norm = norm.transform(x)\nx_norm = pd.DataFrame(x_norm)\n\npca = PCA()\n\npca.fit(x)\n\npca_train = pca.transform(x_norm)\npca_train = pd.DataFrame(pca_train)\npca_test = pca_train[7]\n\n\ny_norm = x_norm[7]\nprint(y_norm)\n\nx_norm.drop(7, axis = 1,inplace=True)\npca_train.drop(7,axis = 1 , inplace = True)\nprint(x_norm)\n\ncorrelation_heatmap(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can conclude from our heat map, Quantity has the highest correlation with the price variable. Let's see how those two are related in a scatter plot.","metadata":{}},{"cell_type":"code","source":"sns.set_palette('RdPu')\nplt.figure()\nsns.set_context(\"poster\", font_scale=0.7)\nsns.scatterplot(data = data1, y='price', x='quantity', hue='capacity')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data","metadata":{}},{"cell_type":"markdown","source":"## Here we are using our NestedCV class to split the data","metadata":{}},{"cell_type":"code","source":"\nfrom pandas import read_csv\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom matplotlib import pyplot\n\nX = x.values\nsplits = NestedCV(k=3) # Here is where we create an istance of the class.\nindex = 1\nfor train_index, test_index in splits.split(X): #callng our split function which returns train and validation sets.\n    trainn = X[train_index]\n    testt = X[test_index]\n    print('Observations: %d' % (len(trainn) + len(testt)))\n    print('Training Observations: %d' % (len(trainn)))\n    print('Testing Observations: %d' % (len(testt)))\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## As we can see our data has been split itto 3 equal folds","metadata":{}},{"cell_type":"markdown","source":"# Checking stationarity","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(8,1,figsize=(20,15))\nfor i,column in enumerate([col for col in x.columns if col != 'hi']):\n    x[column].plot(ax=ax[i])\n    ax[i].set_title(column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The function 'coint_johansen' is a function we use to check whether our multivariate data is stationary or not. If we have a single variable then we can check the stationarity using the adfuller test.**","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.vector_ar.vecm import coint_johansen\n\ncoint_johansen(x_norm,-1,1).eig\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Since all our values are above 1 can say that our data is not stationary.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Here we are going to use our Nested KFold with Machine Learning Models With a Pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model           import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(pca_train,pca_test,test_size=0.3)\n\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model,x ,y, cv=tscv, scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# As we can see ScaleGBM has the highest accuracy score. The Score of LR ad LASSO are too high, which probably means that we've overfit our model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}