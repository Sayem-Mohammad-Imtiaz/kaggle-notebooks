{"cells":[{"metadata":{"_uuid":"98ebf3f1-bf92-49cf-9bc8-db3e4acdcdf1","_cell_guid":"719583da-a318-4f9b-b9f0-f455264d8784","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the stop_words from corpus to get more clean data and store in new column as review_new in same datafram\nfrom nltk.corpus import stopwords\nstop_words =stopwords.words('english')\ndef remove_stopwords(text):#remove_stopwords function gives stopwordfree sentences\n    words=[word for word in text.split() if word not in stop_words]\n    return words\ndf[\"review_new\"]=df['review'].apply(lambda x: ' '.join(remove_stopwords(x)))    \ndf.head()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# two classes are positive and negative, we have to classify our sentenses which give sentiment of the sentences.\nlables = []\nfor i in df['sentiment']:\n    if i == \"positive\":\n        lables.append(1)\n    else:\n        lables.append(0)\nY = lables                # dependent variable Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 10000 # We will only consider the 10K most used words in this dataset\ntokenizer = Tokenizer(num_words=max_words)     \ntokenizer.fit_on_texts(df['review_new']) \nsequences = tokenizer.texts_to_sequences(df['review_new']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have to pad sentenses for same length which is 100 here\nfrom keras.preprocessing.sequence import pad_sequences\nmaxlen = 100\npad = pad_sequences(sequences, maxlen=100)\npad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pad  #independent variable X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from  keras.layers import Embedding ,Flatten, Dense\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_dim = 50\n# model = Sequential()\n# model.add(Embedding(max_words, embedding_dim, input_length=100))\n\n# model.add(Flatten())\n\n# model.add(Dense(32, activation='relu'))\n\n# model.add(Dense(1, activation='sigmoid'))\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.compile(optimizer='adam',\n#               loss='binary_crossentropy',metrics=['acc'])\n              \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.4,random_state=50)\n# print(x_test.shape)\n# # print(y_test[0])\n# history = model.fit(x_train, y_train,\n#                     epochs=10,\n#                     batch_size=32,\n#                     validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pretrained GLoVe embeddings which I already have\n# if your training on locl machine then you must download from GLoVe paper by stanford\n# it is 100d (dimentional) embedding \n# to use embeddings we need to seperate words and their embeddings and store as key value pairs in an empty dictionary\n# then after we need a mean and standard deviation of values\nembeddings_index = {}\nfor line in f:                             \n    values = line.split()\n    word = values[0]\n    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n    embeddings_index[word] = embedding ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an array of all embeddings values\n#  find mean and standard deviation \n\n\nall_emb=np.stack(embeddings_index.values())\nemb_mean = all_emb.mean()\nemb_std = all_emb.std()\nprint(emb_mean)\nprint(emb_std)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(dict(list(embeddings_index.items())[0: 10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embeddings_index.get('word')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  form another matrix with same mean and standard deviation of dimentions equal to (number_of_words,embedding_dim)\n\n\nembedding_dim = 100\nnb_words = min(max_words, len(word_index))\nprint(nb_words)\nemb_matrix = np.random.normal(emb_mean,emb_std,(nb_words,embedding_dim))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_matrix.mean(),emb_matrix.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word,i in word_index.items():\n    if i >= max_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    # If there is an embedding vector, put it in the embedding matrix\n    if embedding_vector is not None: \n        emb_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(emb_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [emb_matrix], trainable = False))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',metrics=['acc'])\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.4,random_state=50)\nprint(x_test.shape)\n# print(y_test[0])\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_text = 'I love dogs. Dogs are the best. They are lovely, cuddly animals that only want the best for humans.'\n\nseq = tokenizer.texts_to_sequences([my_text])\nprint('raw seq:',seq)\nseq = pad_sequences(seq, maxlen=maxlen)\nprint('padded seq:',seq)\nprediction = model.predict(seq)\nprint('positivity:',prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_text = 'The bleak economic outlook will force many small businesses into bankruptcy.'\n\nseq = tokenizer.texts_to_sequences([my_text])\nprint('raw seq:',seq)\nseq = pad_sequences(seq, maxlen=maxlen)\nprint('padded seq:',seq)\nprediction = model.predict(seq)\nprint('positivity:',prediction)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}