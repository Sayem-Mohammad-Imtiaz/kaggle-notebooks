{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport seaborn as sns\nimport keras\nimport matplotlib.pyplot as plt\nimport cv2\nimport gc\nimport tensorflow as tf\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , Activation, MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau\nfrom glob import glob\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getPaths(birdsUsed):\n    \"\"\"\n    Returns list of paths to each directory containing our bird images\n    Parameters:\n        birdsUsed: Number of different bird species we want our model to identify\n    \"\"\"\n    trPaths = glob(\"/kaggle/input/100-bird-species/test/*/\")[:birdsUsed]\n    tePaths = glob(\"/kaggle/input/100-bird-species/train/*/\")[:birdsUsed]\n    crPaths = glob(\"/kaggle/input/100-bird-species/valid/*/\")[:birdsUsed]\n    lists = [trPaths, tePaths, crPaths]\n    return [val for tup in zip(*lists) for val in tup]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The owner of the dataset changed the format in the final stages of the project, so that's why the function below is a bit of a mess!","metadata":{}},{"cell_type":"code","source":"def getSets(paths, downwardScaleFactor, birdsUsed, trackProgress = True):\n    \"\"\"\n    Fetches and separates images into training, testing, and cross-validation sets. Each is returned as a numpy\n    array first containing numpy image representation of image, then a classification numpy array.\n    Parameters:\n        paths: list of paths to each directory containing bird images\n        downwardScaleFactor: By what factor we decrease image quality to reduce inputs. Base images are 224x224\n        birdsUsed: Number of different bird species we want our model to identify\n    \"\"\"\n    \n    train = []\n    test  = []\n    cross = []\n    species = np.asarray([0 for count in range(len(paths) // 3)], dtype=np.uint8)\n    img_size = 224 // downwardScaleFactor\n    \n    if trackProgress:\n        print(\"****** Fetching Images ******\")\n    \n    prev = 0\n    for bird in range(0, len(paths), 3):\n        \n        if trackProgress:\n            x = round(bird / birdsUsed, 1)\n            if x != prev:\n                prev = x\n                print(x)\n            \n        #Setting species identifier\n        species[((bird+1) // 3)-1] = 0\n        species[(bird+1) // 3] = 1\n        \n        for directory in range(3):\n        \n            #Finding name for each image of the given bird\n            wc = glob(paths[bird+directory] + \"/*\")\n            images = [wc[x].split('/')[-1] for x in range(len(wc))]\n\n            birdImages = []\n            for im in images:\n                imageData = getImage(paths[bird+directory] + im, np.copy(species), img_size)\n                birdImages.append(imageData)\n        \n            #Separating images -> 50% in training set, 20% in testing set, and 30% in cross-validation set\n            train += birdImages[0 : (len(images) // 2)]\n            test  += birdImages[len(images) // 2 : (len(images) // 2 + len(images) // 5)]\n            cross += birdImages[(len(images) // 2 + len(images) // 5) : ]\n        \n    return train, test, cross\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getImage(path, species, img_size):\n    \"\"\"\n    Reads the image located at the path and returns it as a numpy array representation.\n    Parameters:\n        path: path of the image\n        species: classification array that describes which species the bird belongs to\n        img_size: Number of pixels, width and height, to represent the image as.\n    \"\"\"\n    \n    #Reading image to numpy array\n    img = cv2.imread(path)\n    img = cv2.resize(img, (img_size, img_size))\n    img = np.asarray(img)\n  \n    return np.array([img, species], dtype = object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setsAsNumpyArrays(train, test, cross):\n    \"\"\"\n    Converts the training, testing, and cross-validation sets to numpy arrays\n    so they work with our keras model.\n    Parameters:\n        train: training set\n        test: testing set\n        corss: cross-validation set\n    \"\"\"\n\n    train = np.asarray(train, dtype = object)\n    test  = np.asarray(test,  dtype = object)\n    cross = np.asarray(cross, dtype = object)\n    \n    return train, test, cross","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def separateXandY(train, test, cross):\n    \"\"\"\n    Separates the image representations and classification arrays for use in the model.\n    Parameters:\n        train: training set\n        test: testing set\n        corss: cross-validation set\n    \"\"\"\n    \n    x_train = []\n    y_train = []\n\n    x_cross = []\n    y_cross = []\n\n    x_test = []\n    y_test = []\n\n    for feature, label in train:\n        x_train.append(feature)\n        y_train.append(label)\n\n    for feature, label in test:\n        x_test.append(feature)\n        y_test.append(label)\n\n    for feature, label in cross:\n        x_cross.append(feature)\n        y_cross.append(label)\n        \n    return x_train, y_train, x_cross, y_cross, x_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalizePixels(x_train, x_cross, x_test):\n    \"\"\"\n    Normalizes each RGB value to improve performance.\n    Parameters:\n        x_train: training images\n        x_cross: cross-validation images\n        x_test:  testing images\n    \"\"\"\n    x_train = np.array(x_train) / 255\n    x_cross = np.array(x_cross) / 255\n    x_test = np.array(x_test) / 255\n    return x_train, x_cross, x_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readyClassifArrs(train, test, cross):\n    \"\"\"\n    Converts classification arrays to numpy arrays so the model can use them.\n    Parameters:\n        train: training classif. array\n        test:  testing classif. array\n        cross: cross-validation classif. array\n    \"\"\"\n    train = np.asarray(train)\n    test  = np.asarray(test)\n    cross = np.asarray(cross)\n    \n    return train, test, cross","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setDatagen(x_train, rr, zr, wsr, hsr):\n    \"\"\"\n    Creates image data generator which attempts to prevent overfitting training set\n    Parameters:\n        x_train: training images\n        rr: rotation range 0-180\n        zr: zoom range 0-1\n        wsr: width shift range 0-1\n        hsr: height shift range 0-1\n    \"\"\"\n    datagen = ImageDataGenerator(\n            featurewise_center=False,  # set input mean to 0 over the dataset\n            samplewise_center=False,  # set each sample mean to 0\n            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n            samplewise_std_normalization=False,  # divide each input by its std\n            zca_whitening=False,  # apply ZCA whitening\n            rotation_range = 15,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range = 0.2, # Randomly zoom image \n            width_shift_range=0.10,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=0.10,  # randomly shift images vertically (fraction of total height)\n            horizontal_flip = True,  # randomly flip images\n            vertical_flip=False)  # randomly flip images\n\n    datagen.fit(x_train)\n    return datagen","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generateModel(downwardScaleFactor, birdsUsed, filters, filterScaling, kernel, kernelScaling, dense, layers):\n    \"\"\"\n    Generates a keras sequential model based on the parameters given\n    Parameters:\n        downwardScaleFactor: By what factor we decrease image quality to reduce inputs. Base images are 224x224\n        birdsUsed: Number of different bird species we want our model to identify\n        filters: initial number of filters used in base layer\n        filterScaling: factor by which filters increases for each following convolution layer\n        kernel: initial kernel count\n        kernelScaling: How much kernels decrease with each following convolution layer\n        dense: Size of final dense layer leading to classification\n        layers: number of layers in the model\n    \"\"\"\n    \n    model = Sequential()\n    img_size = 224 // downwardScaleFactor\n    \n    model.add(Conv2D(filters = filters, kernel_size = (kernel,kernel),padding = 'Same',activation ='relu', input_shape = (img_size,img_size,3)))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    for count in range(layers-1):\n        filters *= filterScaling\n        kernel -= kernelScaling\n        model.add(Conv2D(filters = filters, kernel_size = (kernel,kernel),padding = 'Same',activation ='relu'))\n        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n        \n    model.add(Flatten())\n    model.add(Dense(dense))\n    model.add(Activation('relu'))\n    model.add(Dense(birdsUsed, activation = \"softmax\"))\n\n    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n        \n    return model\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runModel(model, xt, yt, xc, yc, bs, ep, p, datagen):\n    \"\"\"\n    Trains the model using the testing data and cross-validation set.\n    Parmaters:\n        model: base model to use\n        xt: training images\n        yt: training classification array\n        xc: cross-validation images\n        yc: cross-validation classification array\n        bs: batch-size\n        ep: epochs\n        p: patience\n        datagen: previously constructed image data generator\n    \"\"\"\n    \n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = p,factor=0.3, min_lr=0.000001)\n    history = model.fit(datagen.flow(xt, yt, batch_size = bs),\n                        epochs = ep,\n                        validation_data = datagen.flow(xc, yc),\n                        callbacks = [learning_rate_reduction],\n                        verbose = 1)\n    return history, model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testModel(model, x_test, y_test):\n    \"\"\"\n    Tests models' accuracy on the testing images\n    Parameters:\n        model: trained model\n        x_test: testing images\n        y_test: testing classification array\n    \"\"\"\n    \n    modelEval = model.evaluate(x_test, y_test, verbose=1)\n    return modelEval[0], modelEval[1] * 100\n\n#print(\"Loss of the model is - \" , model.evaluate(x_test, y_test, verbose=0))\n#print(\"Accuracy of the model is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runModelTests():\n    \"\"\"\n    Main function to run trials\n    \"\"\"\n    \n    attempts = []\n    performance = {\"birds\": 0, \"scale\": 0, \"layers\": 0, \"filter-scale\":0, \"kernels\": 0,\n                   \"test-accuracy\": 0, \"train-accuracy\": 0, \"val-accuracy\": 0}\n    \n    scales = [6,4,3,2]\n    filenum = 0\n    \n    for birds in range(15, 31, 5):\n        #Number of distinct lables we're using\n        #birdsUsed = 20\n        \n        print(\"\\n\\nNew Bird Count!\\n\\n\")\n\n        for scale in scales:\n            \n            filenum += 1\n            \n            if filenum <= 10: #Picking up where last process concluded\n                print(\"File:\", filenum, \" Passed\")\n                continue\n            \n            #By what factor we decrease image quality to reduce inputs. Base images are 224x224\n            #downwardScaleFactor = 10\n            \n            print(\"\\nNew Scale\\n\")\n\n            paths = getPaths(birds)\n\n            #Readying the data for use\n            train, test, cross = getSets(paths, scale, birds, trackProgress = False)\n            \n            #print(\"****** Readying Images ******\")\n            train, test, cross = setsAsNumpyArrays(train, test, cross)\n            xtr, ytr, xcr, ycr, xte, yte = separateXandY(train, test, cross)\n            xtr, xcr, xte = normalizePixels(xtr, xcr, xte)\n            ytr, ycr, yte = readyClassifArrs(ytr, ycr, yte)\n                \n            #Setting up model\n            #print(\"****** Establishing Model ******\")\n            datagen = setDatagen(xtr, 15, 0.2, 0.1, 0.1) # rotation range, zoom range, width shift, height shift\n            \n            for layers in range(2, 5):\n                for fs in range(2, 3):\n                    for kernel in range(2, 7, 2):\n                        \n                        try:\n                        \n                            model = generateModel(scale, birds, 32, fs, kernel, 1, 256, layers)\n\n                            #Running model\n                            print(\"****** Running Model ******\")\n                            history, model = runModel(model, xtr, ytr, xcr, ycr, 32, 35, 3, datagen) # Batch size, Epochs, Patience\n                            loss, acc = testModel(model, xte, yte)\n\n                            perf = performance.copy()\n                            perf['birds'] = birds\n                            perf['scale'] = scale\n                            perf['kernels'] = kernel\n                            perf['layers'] = layers\n                            perf['filter-scale'] = fs\n                            perf['test-accuracy'] = acc\n                            perf['train-accuracy'] = history.history['accuracy'][-1]\n                            perf['val-accuracy'] = history.history['val_accuracy'][-1]\n\n                            attempts.append(perf)\n                            \n                            print(\"*** Model Completed ***\")\n                            \n                        except:\n                            pass\n                        \n            df = pd.DataFrame(attempts)\n            name = \"birdscale{}.csv\".format(filenum)\n            df.to_csv(name,index=False)\n            attempts = []\n                        \n    return attempts\n    \n#attempts = runModelTests()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post-test result examination","metadata":{}},{"cell_type":"code","source":"def examineResults():\n    \n    pathString = \"/kaggle/input/model-results/birdscale{}.csv\"\n    li = []\n\n    for count in range(1,17):\n        filename = pathString.format(count)\n        df = pd.read_csv(filename, index_col=None, header=0)\n        li.append(df)\n\n    df = pd.concat(li, axis=0, ignore_index=True)\n    df[\"train-accuracy\"] = df[\"train-accuracy\"] * 100\n    df[\"val-accuracy\"] = df[\"val-accuracy\"] * 100\n    \n    df[\"avg\"] = (df[\"test-accuracy\"] + df[\"train-accuracy\"] + df[\"val-accuracy\"]) / 3\n    \n    return df\n\ndf = examineResults()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def separateBirdCounts(df):\n    \"\"\"\n    Separates the dataframe by the amount of birds present in the model, returning four data frames\n    Parameters:\n        df: dataframe containing all performance data\n    \"\"\"\n    birds15 = df[df[\"birds\"] == 15]\n    birds20 = df[df[\"birds\"] == 20]\n    birds25 = df[df[\"birds\"] == 25]\n    birds30 = df[df[\"birds\"] == 30]\n    \n    return birds15, birds20, birds25, birds30\n\nbirds15, birds20, birds25, birds30 = separateBirdCounts(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def groupsByPerf(group, data):\n    \"\"\"\n    Groups the performance data by a specified column and measures it against performance\n    for every subset amount of birds.\n    Parameters:\n        group: Column to group by\n        data: list of performance dataframes separated by bird count\n    \"\"\"\n    groupings = []\n    \n    for item in data:\n        perf = item.groupby(group)['avg'].mean().to_frame()\n        perf[group] = perf.index\n        perf[group] = perf[group].astype(str)\n        groupings.append(perf)\n        \n    return groupings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotSubs(dfs, x, title, xlab):\n    \"\"\"\n    Plots four subplots for each subset of birds.\n    Parameters:\n        dfs: list of the four dataframes\n        x: name of the column we're measuring against performance\n        title: title for entire plot\n        xlab: x label for the subplots\n    \"\"\"\n    \n    [df1, df2, df3, df4] = dfs\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize = (7,7))\n    ax = ((ax1, ax2), (ax3, ax4))\n    fig.suptitle(title)\n    ax1.bar(df1[x], df1[\"avg\"])\n    ax1.set_title('15 Birds')\n    ax2.bar(df2[x], df2[\"avg\"])\n    ax2.set_title('20 Birds')\n    ax3.bar(df3[x], df3[\"avg\"])\n    ax3.set_title('25 Birds')\n    ax4.bar(df4[x], df4[\"avg\"])\n    ax4.set_title('30 Birds')\n    plt.setp(ax, ylim=(70, 85))\n    fig.supxlabel(xlab)\n    fig.supylabel(\"Average Model Performance\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting relationship between downward scale factor and average performance","metadata":{}},{"cell_type":"code","source":"scalePerfs = groupsByPerf(\"scale\", [birds15, birds20, birds25, birds30])\nplotSubs(scalePerfs, \"scale\",\n         \"Model Performance by Downward Scale Factor\", \"Downward Scale Factor\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting relationship between initial # of layers and average performance","metadata":{}},{"cell_type":"code","source":"layerPerfs = groupsByPerf(\"layers\", [birds15, birds20, birds25, birds30])\nplotSubs(layerPerfs, \"layers\",\n         \"Model Performance by # of Layers\", \"# of Layers\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting relationship between initial # of kernels and average performance","metadata":{}},{"cell_type":"code","source":"kernelPerfs = groupsByPerf(\"kernels\", [birds15, birds20, birds25, birds30])\nplotSubs(kernelPerfs, \"kernels\",\n         \"Model Performance by Intiial # of Kernels\", \"Initial # of Kernels\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Taking the maximum performing parameters to test other independent variables in model construction","metadata":{}},{"cell_type":"code","source":"\"\"\"\ndef furtherTests(birds):\n    \n    paths = getPaths(birds)\n    filenum = 1\n    \n    #Readying the data for use\n    train, test, cross = getSets(paths, 3, birds, trackProgress = False)\n    #print(\"****** Readying Images ******\")\n    train, test, cross = setsAsNumpyArrays(train, test, cross)\n    xtr, ytr, xcr, ycr, xte, yte = separateXandY(train, test, cross)\n    xtr, xcr, xte = normalizePixels(xtr, xcr, xte)\n    ytr, ycr, yte = readyClassifArrs(ytr, ycr, yte)\n    #Setting up model\n    #print(\"****** Establishing Model ******\")\n    datagen = setDatagen(xtr, 15, 0.2, 0.1, 0.1) # rotation range, zoom range, width shift, height shift\n    performance = {\"birds\": 30, \"scale\": 3, \"layers\": 4, \"filter-scale\":2, \"kernels\": 0,\n                   \"test-accuracy\": 0, \"train-accuracy\": 0, \"val-accuracy\": 0, \"filters\": 0,\n                    \"kernels\": 0, \"kernel-scaling\": 0, \"dense\": 0}\n    attempts = []\n    for filters in [32, 45]:\n        for kernels in [6,8]:\n            for ks in [1,2]:\n                for dense in [256, 512, 1024]:\n                        model = generateModel(3, birds, filters, 2, kernels, ks, dense, 4)\n                        print(\"****** Running Model ******\")\n                        history, model = runModel(model, xtr, ytr, xcr, ycr, 32, 35, 3, datagen) # Batch size, Epochs, Patience\n                        loss, acc = testModel(model, xte, yte)\n                        \n                        perf = performance.copy()\n                        perf['kernels'] = kernels\n                        perf['filters'] = filters\n                        perf['kernel-scaling'] = ks\n                        perf['dense'] = dense\n                        perf['filter-scale'] = fs\n                        perf['test-accuracy'] = acc\n                        perf['train-accuracy'] = history.history['accuracy'][-1]\n                        perf['val-accuracy'] = history.history['val_accuracy'][-1]\n                        \n                        attempts.append(perf)\n                    \n            df = pd.DataFrame(attempts)\n            name = \"SecondTest30Birds{}.csv\".format(filenum)\n            filenum += 1\n            df.to_csv(name,index=False)\n            attempts = []\n#furtherTests(30)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Graphing performance over epochs","metadata":{}},{"cell_type":"code","source":"\"\"\"\nepochs = [i for i in range(35)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ncross_acc = history.history['val_accuracy']\ncross_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , cross_acc , 'ro-' , label = 'Cross Validation Accuracy')\nax[0].set_title('Training & Validation Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , cross_loss , 'r-o' , label = 'Cross Validation Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Training & Validation Loss\")\nplt.show()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}