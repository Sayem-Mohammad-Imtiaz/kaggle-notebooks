{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install feature-engine","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e5c8453-c83c-4626-98c6-7fc7eeb4088b","_cell_guid":"3416bfc5-328c-4703-ab03-89d55f4e0d86","trusted":true},"cell_type":"code","source":"\n#Import the libraries\nimport  numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom feature_engine.categorical_encoders import  OrdinalCategoricalEncoder\nfrom feature_engine.missing_data_imputers import RandomSampleImputer\nfrom feature_engine.variable_transformers import PowerTransformer\nfrom scipy.stats import probplot\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import  Pipeline\nfrom sklearn.metrics import classification_report\n\n#Read the data\ndata = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\n#Getting the shape of the data\nprint(data.shape)\n\n#Check the data head\nprint(data.head())\n\n#Checking the percentage of missing values\nprint(data.isnull().mean()*100)\n#Make the Churn variable as binary variable\ndata['Churn'] = np.where(data['Churn'] == 'No', 0, 1)\n\n#Method to Segregate the categorical and numerical columns\ndef dtype_seg(data):\n    cat_col = []\n    num_col = []\n    for col in data.columns:\n        if(data[col].dtype == 'object'):\n            cat_col.append(col)\n        else:\n            num_col.append(col)\n    return  cat_col, num_col\n\ncategorical, numerical = dtype_seg(data)\nprint(f'the categorical columns : {categorical}')\nprint(f'the numerical columns : {numerical}')\n\n#Here we have to cast the data type of TotalCharges as float64\ndata['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n\n#Methods to join multiple words of the observation\ndef join_words(data, column):\n    for i in range(len(data[column])):\n        x = data[column][i].split()\n        x = '_'.join(x)\n        data[column][i] = x\n    return data[column]\n\n#Apply join words on MultipleLines column\ndata['MultipleLines'] = join_words(data, 'MultipleLines')\n\n#Apply join words on InternetService column\ndata['InternetService'] = join_words(data, 'InternetService')\n\n#Apply join words on OnlineSecurity column\ndata['OnlineSecurity'] = join_words(data, 'OnlineSecurity')\n\n#Apply join words on OnlineBackup column\ndata['OnlineBackup'] = join_words(data, 'OnlineBackup')\n\n#Apply join words on DeviceProtection column\ndata['DeviceProtection'] = join_words(data, 'DeviceProtection')\n\n#Apply join words on TechSupport column\ndata['TechSupport'] = join_words(data, 'TechSupport')\n\n#Apply join words on StreamingTV column\ndata['StreamingTV'] = join_words(data, 'StreamingTV')\n\n#Apply join words on StreamingMovies column\ndata['StreamingMovies'] = join_words(data, 'StreamingMovies')\n\n#Apply join words on Contract column\ndata['Contract'] = join_words(data, 'Contract')\n\n#Apply join words on PaymentMethod column\ndata['PaymentMethod'] = join_words(data, 'PaymentMethod')\n\n\n#Segregate the data to dependent and Independent\nX = data[['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n          'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV', 'StreamingMovies',\n          'Contract', 'PaperlessBilling', 'PaymentMethod', 'TotalCharges','SeniorCitizen', 'tenure', 'MonthlyCharges']]\ny = data['Churn']\n\n#Split the data to training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n\n#Getting the shape\nprint(X_train.shape, X_test.shape)\n\n\n#Method to visualise the distribution of the data\ndef diagonstic_plot(data,column):\n    plt.figure(figsize=(10,8))\n    plt.subplot(1,2,1)\n    sns.distplot(data[column].dropna())\n\n    plt.subplot(1,2,2)\n    probplot(data[column], dist='norm', plot=plt)\n    plt.show()\n\n#Check the TotalCharges data description\nprint(X_train['TotalCharges'].describe())\n\n#Impute the missing values for the Totalcharges\nimputer = RandomSampleImputer(random_state=['TotalCharges'], seed='observation', seeding_method='add')\nimputer.fit(X_train)\n\n#Transform the data\nX_train = imputer.transform(X_train)\nX_test = imputer.transform(X_test)\n\n#Transform the numerical variable for more Gaussian Approximation\nvt = PowerTransformer(exp=0.3)\nvt.fit(X_train)\n\n#Transform the variable\nX_train = vt.transform(X_train)\nX_test = vt.transform(X_test)\n\n#Method to get the number of unique values in each categorical feature\ndef get_nunique(data,categorical_col_list):\n    my_dict = {}\n    for column in categorical_col_list:\n        if(column!='customerID'):\n            my_dict[column] = data[column].nunique()\n    return my_dict\n#Print the number of unique categories associated with each categorical features\nprint(get_nunique(X_train,categorical))\n\n#Encode the categorcal variable\nencoder = OrdinalCategoricalEncoder(encoding_method='ordered')\nencoder.fit(X_train, y_train)\n\n#Transform the variable\nX_train = encoder.transform(X_train)\nX_test = encoder.transform(X_test)\n\n\n#Choosing the appropriate Hyperparamters for the model\n#Stage 1 choose optimal Penalty parameter\npenalty_dict = {}\nfor penalty in ['l1','l2']:\n    pipeline_p = Pipeline([\n        ('scaler', StandardScaler()),\n        ('log', LogisticRegression(penalty=penalty))\n    ])\n    score = cross_val_score(estimator=pipeline_p,X=X_train,y=y_train,scoring='f1',cv=10)\n    penalty_dict[penalty] = score.mean()\nprint(f'Penalty : {penalty_dict}')\n\n#Stage 2 choose dual  parameter\ndual_dict = {}\nfor dual in [False, True]:\n    pipeline_d = Pipeline([\n        ('scaler', StandardScaler()),\n        ('log', LogisticRegression(penalty='l2', dual=dual))\n    ])\n    score = cross_val_score(estimator=pipeline_d,X=X_train,y=y_train,scoring='f1',cv=10)\n    dual_dict[dual] = score.mean()\nprint(f'Dual : {dual_dict}')\n\n#Stage 3 choose optimal C value\nC_dict = {}\nfor C in [0.5,1.0,2.0,3.0,4.0]:\n    pipeline_c = Pipeline([\n        ('scaler', StandardScaler()),\n        ('log', LogisticRegression(penalty='l2', dual=False, C=C))\n    ])\n    score = cross_val_score(estimator=pipeline_c,X=X_train,y=y_train,scoring='f1',cv=10)\n    C_dict[C] = score.mean()\nprint(f'C : {C_dict}')\n\n\n#Stage 4 choose  fit_intercept value\nintercept_dict = {}\nfor intercept in [False,True]:\n    pipeline_i = Pipeline([\n        ('scaler', StandardScaler()),\n        ('log', LogisticRegression(penalty='l2', dual=False, C=1.0, fit_intercept=intercept))\n    ])\n    score = cross_val_score(estimator=pipeline_i,X=X_train,y=y_train,scoring='f1',cv=10)\n    intercept_dict[intercept] = score.mean()\nprint(f'Intercept : {intercept_dict}')\n\n#Stage 5 choose  solver value\nsolver_dict = {}\nfor solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n    pipeline_s = Pipeline([\n        ('scaler', StandardScaler()),\n        ('log', LogisticRegression(penalty='l2', dual=False, C=1.0, fit_intercept=False, solver=solver))\n    ])\n    score = cross_val_score(estimator=pipeline_s,X=X_train,y=y_train,scoring='f1',cv=10)\n    solver_dict[solver] = score.mean()\nprint(f'Solver : {solver_dict}')\n\n#Stage 6 choose  multi_class value\nmulticlass_dict = {}\nfor multiclass in ['auto', 'ovr', 'multinomial']:\n    pipeline_m = Pipeline([\n        ('scaler', StandardScaler()),\n        ('log', LogisticRegression(penalty='l2', dual=False, C=1.0, fit_intercept=False, solver='saga', multi_class=multiclass))\n    ])\n    score = cross_val_score(estimator=pipeline_m,X=X_train,y=y_train,scoring='f1',cv=10)\n    multiclass_dict[multiclass] = score.mean()\nprint(f'Multi-Class : {multiclass_dict}')\n\n\n#Scale the data\nscaler = StandardScaler()\nscaler.fit(X_train)\n\n#Transform the data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n#Applying the model\nclassifier = LogisticRegression(penalty='l2', dual=False, C=1.0, fit_intercept=False, solver='saga', multi_class='ovr')\nclassifier.fit(X_train,y_train)\n\n#Predict the test set result\ny_pred = classifier.predict(X_test)\n\n#Print classification Report for training set\nprint('Training Classification Report')\nprint(classification_report(y_train,classifier.predict(X_train)))\n\n#Print classification Report for test set\nprint('Training Classification Report')\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}