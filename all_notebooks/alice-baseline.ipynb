{"cells":[{"metadata":{"trusted":false,"_uuid":"73bdd66b3c81b225fe8f7aa712b1c77cbdf0100c"},"cell_type":"code","source":"import pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a4eaebcf476645fb86ffbc81803e5091eb4c1342"},"cell_type":"code","source":"# Read the training and test data sets, change paths if needed\ntrain_df = pd.read_csv('../input/train_sessions.csv',\n                       index_col='session_id')\ntest_df = pd.read_csv('../input/test_sessions.csv',\n                      index_col='session_id')\n\n# Convert time1, ..., time10 columns to datetime type\ntimes = ['time%s' % i for i in range(1, 11)]\ntrain_df[times] = train_df[times].apply(pd.to_datetime)\ntest_df[times] = test_df[times].apply(pd.to_datetime)\n\n# Sort the data by time\ntrain_df = train_df.sort_values(by='time1')\n\n# Look at the first rows of the training set\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6941a809bf788b6aa658477e352a28d1b1584911"},"cell_type":"code","source":"# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\nsites = ['site%s' % i for i in range(1, 11)]\ntrain_df[sites] = train_df[sites].fillna(0).astype(np.uint16)\ntest_df[sites] = test_df[sites].fillna(0).astype(np.uint16)\n\n# Load websites dictionary\nwith open(r\"../../mlcourse.ai/data/site_dic.pkl\", \"rb\") as input_file:\n    site_dict = pickle.load(input_file)\n\n# Create dataframe for the dictionary\nsites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])\nprint(u'Websites total:', sites_dict.shape[0])\nsites_dict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6cfad2799596cf561a25ab153ac6049fc45d5bb5"},"cell_type":"code","source":"time_df = pd.DataFrame(index=train_df.index)\ntime_df['target'] = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5afc7869452f0899db95ae647b0bb18ce3dde011"},"cell_type":"code","source":"# Create a separate dataframe where we will work with timestamps\ntime_df = pd.DataFrame(index=train_df.index)\ntime_df['target'] = train_df['target']\n\n# Find sessions' starting and ending\ntime_df['min'] = train_df[times].min(axis=1)\ntime_df['max'] = train_df[times].max(axis=1)\n\n# Calculate sessions' duration in seconds\ntime_df['seconds'] = (time_df['max'] - time_df['min']) / np.timedelta64(1, 's')\n\ntime_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5eda29f71821fbbcd5e6136dbded28bc7d49c2d2"},"cell_type":"code","source":"# Our target variable\ny_train = train_df['target']\n\n# United dataframe of the initial data \nfull_df = pd.concat([train_df.drop('target', axis=1), test_df])\n\n# Index to split the training and test data sets\nidx_split = train_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6a6911b2d7ade3e1a717ed86dc2d85401e9d3d75"},"cell_type":"code","source":"# Dataframe with indices of visited websites in session\nfull_sites = full_df[sites]\nfull_sites.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e32d7ba5e3d9140a85a9b899ffe9968a73fe250d"},"cell_type":"code","source":"# sequence of indices\nsites_flatten = full_sites.values.flatten()\n\n# and the matrix we are looking for \n# (make sure you understand which of the `csr_matrix` constructors is used here)\n# a further toy example will help you with it\nfull_sites_sparse = csr_matrix(([1] * sites_flatten.shape[0],\n                                sites_flatten,\n                                range(0, sites_flatten.shape[0]  + 10, 10)))[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"40487050b78dcc9ab780b513f615df8dc9acca95"},"cell_type":"code","source":"def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):\n    # Split the data into the training and validation sets\n    idx = int(round(X.shape[0] * ratio))\n    # Classifier training\n    lr = LogisticRegression(C=C, random_state=seed, solver='liblinear').fit(X[:idx, :], y[:idx])\n    # Prediction for validation set\n    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n    # Calculate the quality\n    score = roc_auc_score(y[idx:], y_pred)\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a0c9c86b8bddc224f00036710c9a304de456d6f1"},"cell_type":"code","source":"%%time\n# Select the training set from the united dataframe (where we have the answers)\nX_train = full_sites_sparse[:idx_split, :]\n\n# Calculate metric on the validation set\n#print(get_auc_lr_valid(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"13b12c207c632ee4a400318ae0d35268d380a36c"},"cell_type":"code","source":"# Function for writing predictions to a file\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fbf5741afa9909079529a733159fc43622d1e16e"},"cell_type":"code","source":"# Train the model on the whole training data set\n# Use random_state=17 for repeatability\n# Parameter C=1 by default, but here we set it explicitly\nlr = LogisticRegression(C=1.0, random_state=17, solver='liblinear').fit(X_train, y_train)\n\n# Make a prediction for test data set\nX_test = full_sites_sparse[idx_split:,:]\ny_test = lr.predict_proba(X_test)[:, 1]\n\n# Write it to the file which could be submitted\nwrite_to_submission_file(y_test, 'baseline_1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"29f12a2f4c616aa1b1fe5fb015c0732ada73bb40"},"cell_type":"code","source":"# Dataframe for new features\nfull_new_feat = pd.DataFrame(index=full_df.index)\n\n# Add start_month feature\nfull_new_feat['start_month'] = full_df['time1'].apply(lambda ts: \n                                                      100 * ts.year + ts.month).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2c398a8f180fb9a990cc28ecde5853d9849462fe"},"cell_type":"code","source":"# Add the new feature to the sparse matrix\ntmp = full_new_feat[['start_month']].values\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], tmp[:idx_split,:]]))\n\n# Compute the metric on the validation set\n#print(get_auc_lr_valid(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"431fd66dc8c98c8fe9153e7f80e5f7e0620cf004"},"cell_type":"code","source":"# Add the new standardized feature to the sparse matrix\ntmp = StandardScaler().fit_transform(full_new_feat[['start_month']])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], tmp[:idx_split,:]]))\n\n# Compute metric on the validation set\n#print(get_auc_lr_valid(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0eca7714d4f4ae6d1eb1a397d67f833834ddfb50"},"cell_type":"code","source":"full_new_feat['start_hour'] = full_df[times].min(axis=1).apply(lambda ts: ts.hour).astype('int64')\nfull_new_feat['morning'] = full_new_feat['start_hour'].apply(lambda x: 1 if x <= 11 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a53e8ab980c58002642bff83033c015411748841"},"cell_type":"code","source":"tmp = full_new_feat[['start_hour','morning']].values\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], tmp[:idx_split,:]]))\n\n# Compute the metric on the validation set\n#print(get_auc_lr_valid(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3ff24211bdaa345774cc9642032c399a0d724fce"},"cell_type":"code","source":"tmp_scaled = StandardScaler().fit_transform(full_new_feat[['start_month', \n                                                           'start_hour', \n                                                           'morning']])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n                             tmp_scaled[:idx_split,:]]))\n\n# Capture the quality with default parameters\n#score_C_1 = get_auc_lr_valid(X_train, y_train)\n#print(score_C_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"12673d5a02105eadd95d5d3677a55090f760e6b6"},"cell_type":"code","source":"from tqdm import tqdm\n\n# List of possible C-values\nCs = np.logspace(-3, 1, 10)\nscores = []\nfor C in tqdm(Cs):\n    scores.append(get_auc_lr_valid(X_train, y_train, C=C))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"eb094b79fd3cbdf50f2972f53ffbe96c5ffb76b0"},"cell_type":"code","source":"plt.plot(Cs, scores, 'ro-')\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('AUC-ROC')\nplt.title('Regularization Parameter Tuning')\n# horizontal line -- model quality with default C value\nplt.axhline(y=score_C_1, linewidth=.5, color='b', linestyle='dashed') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8be81e3d3e4fcc909c3603e3ae74b8a8a00b037a"},"cell_type":"code","source":"# Prepare the training and test data\ntmp_scaled = StandardScaler().fit_transform(full_new_feat[['start_month', 'start_hour', \n                                                           'morning']])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n                             tmp_scaled[:idx_split,:]]))\nX_test = csr_matrix(hstack([full_sites_sparse[idx_split:,:], \n                            tmp_scaled[idx_split:,:]]))\n\n# Train the model on the whole training data set using optimal regularization parameter\nlr = LogisticRegression(C=C, random_state=17, solver='liblinear').fit(X_train, y_train)\n\n# Make a prediction for the test set\ny_test = lr.predict_proba(X_test)[:, 1]\n\n# Write it to the submission file\nwrite_to_submission_file(y_test, 'baseline_2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2f95f347a46bd9f7e0b65fc4ee83088a853a4abd"},"cell_type":"code","source":"train_df_my = train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f64e6ff241615410700aa283f4cdb6947f75bb91"},"cell_type":"code","source":"train_df_my.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8f0dd4f7f7b1f2fd285ac1b9aabcc5b9e5e7c4a8"},"cell_type":"code","source":"train_df_my['day_of_week'] = train_df_my['time1'].apply(lambda ts: ts.dayofweek).astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0dfaeb0c767ed23804b412362ede5e78eb3ee949"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 1].groupby('day_of_week').size().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f0b1b546254c7f7c5e623710ef336e5f4aa13cd6"},"cell_type":"code","source":"train_df_my['start_month'] = train_df_my['time1'].apply(lambda ts: \n                                                      100 * ts.year + ts.month).astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"402e63d80e6f43b1475d79a429f0f4d30b3100cc"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 1].groupby('start_month').size().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e43fd123fcc3bbf75e58ff2a7cfa2ebecd01dddc"},"cell_type":"code","source":"train_df_my['start_hour'] = train_df_my['time1'].apply(lambda ts: ts.hour).astype('int8')\ntrain_df_my['morning'] = train_df_my['start_hour'].apply(lambda x: 1 if x <= 11 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bdae95d568e9d6df7bc486a1119fe7dd6cde3f81"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 1].groupby('start_hour').size().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e9a4b526654341dbcc87b4589a6952c48732530d"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 0].groupby('start_hour').size().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"906e48e4d846a2a661aa4a7704e7a99412c48411"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 1].groupby('morning').size().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"869239c94ab4087b90410cfc2036075f88970f26"},"cell_type":"code","source":"train_df_my['day_of_month'] = train_df_my['time1'].apply(lambda ts: ts.day).astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"94cb04beeb0db628e27ff82d5a809665865f015b"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 1].groupby('day_of_month').size().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"a9c5b477e97e52fa0f4ab7ff1962b79227dc5484"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 0].groupby('day_of_month').size().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ddecbb3c55b8133cbe9659f6452ea20521795f3"},"cell_type":"code","source":"top_sites_for_notA = pd.Series(train_df[train_df['target'] == 0][sites].values.flatten()\n                     ).value_counts().sort_values(ascending=False).head(80)\nprint(top_sites_for_A)\nsites_dict.loc[top_sites_for_A.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e71c23159467e6b08bfd69bfbc933807a6a61b50"},"cell_type":"code","source":"train_df_my['top_sites'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"86a3558fac87a674439d8270beaec3736c5e74c5"},"cell_type":"code","source":"for site in sites:\n    train_df_my['top_sites'] += train_df_my[site].isin(top_sites_for_notA).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"308e26fc6fc8db1d4f7cb39e48493bea211ead89"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 1].groupby('top_sites').size()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"06cc4bd8c777e92d73563a27b675692629ab1b54"},"cell_type":"code","source":"train_df_my[train_df_my['target'] == 0].groupby('top_sites').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1be8e1327d37311182c1fcc215bf742fc235edfc"},"cell_type":"code","source":"train_df_my['diff'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"493a175d3c7958ad5fd94074e69d2ff76b3068d5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d1e14193a5380d13862c9bbd500f2f218e135939"},"cell_type":"code","source":"full_new_feat['day_of_week'] = full_df['time1'].apply(lambda ts:ts.dayofweek).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a291d74a08770c5b3313edf463122e65445369e"},"cell_type":"code","source":"full_new_feat = pd.concat([full_new_feat, pd.get_dummies(full_new_feat['day_of_week'], prefix = 'day_of_week')], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c18e4e3b69321045781a09003902ea8c8998f2bd"},"cell_type":"code","source":"full_new_feat['seconds'] = (full_df[times].max(axis = 1) - full_df[times].min(axis = 1)) / np.timedelta64(1, 's')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5145739055ba6af1fe321763b808bdb6e792f599"},"cell_type":"code","source":"full_new_feat['start_hour'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea5f4111190c47ff875aa5ff9e83c03a2ef94a43"},"cell_type":"code","source":"full_new_feat = pd.concat([full_new_feat, pd.get_dummies(full_new_feat['start_hour'], prefix = 'start_hour')], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8d29b0350b4e36634d337c4b5129a384a8b867c1"},"cell_type":"code","source":"full_new_feat['top_sites'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"de8655069d13f0baccab3ce4342240b8cca0256d"},"cell_type":"code","source":"for site in sites:\n    full_new_feat['top_sites'] += full_df[site].isin(top_sites_for_notA).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"924965d30c8c167b81e2a82b7f8b2440a5d47172"},"cell_type":"code","source":"full_new_feat['top_sites'] = np.sqrt(full_new_feat['top_sites'] * 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"961141cfad3759d2d1f151746ef367417aea5ea6"},"cell_type":"code","source":"full_new_feat['top'] = full_new_feat['top_sites'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"eb38c28a9cb42e6ad5cdfb577a977e1b43b54ab3"},"cell_type":"code","source":"full_new_feat['day_of_month'] = full_df['time1'].apply(lambda ts:ts.day).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"97654214da0267c0f43a28a60c498aed62a7d405"},"cell_type":"code","source":"full_to_text = full_df[sites].apply(\n    lambda x: \" \".join([str(a) for a in x.values if a != 0]), axis=1)\\\n               .values.reshape(len(full_df[sites]), 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"891085eebd5fa33570554b8ab3a5f9ca5c16dc8c"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9e5701ad7d7e354c05d546599a460999cb58fedc"},"cell_type":"code","source":"pipeline = Pipeline([\n    (\"vectorize\", CountVectorizer()),\n    (\"tfidf\", TfidfTransformer())\n])\npipeline.fit(full_to_text.ravel())\n\nX_full_sparse = pipeline.transform(full_to_text.ravel())\n\nX_full_sparse.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c4f2f7300a84cd21ea31956a442862fc74c928bd"},"cell_type":"code","source":"data = pd.DataFrame(index=full_df.index)\ndata = full_df[sites]\ndata['list_of_words'] = full_df['site1'].apply(str)\ndata['list_of_words'] += ','\nfor i in range(2, 10):\n    data['list_of_words'] += full_df['site%s'% i].apply(str)\n    data['list_of_words'] += ','\nfull_df['list_of_words'] = data['list_of_words'].apply(lambda x: x.split(','))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"f00c153d03ab8951aacbbda5c047cddac2fe2fc7"},"cell_type":"code","source":"from gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c89bc0b78630a7dcb669405e12dbe0fbb520a271"},"cell_type":"code","source":"text_model = word2vec.Word2Vec(data['list_of_words'], size=500, window=5, workers=-1)\nw2v = dict(zip(text_model.wv.index2word, text_model.wv.syn0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"30ba1477972e4f63d1f2897b5a34471e4fea56d6"},"cell_type":"code","source":"class sense_vectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.dim = len(next(iter(w2v.values())))\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[word] for word in words if word in self.word2vec] \n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n    \n    def fit(self, X):\n        return self ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8cb83cedc9b1e0f4ea127ab2404e36a8164df6e5"},"cell_type":"code","source":"full_new_feat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b259976fe50ec1dce25d7434f6222c4bb2fba2f7"},"cell_type":"code","source":"feats = ['start_month', 'start_hour', 'seconds', 'day_of_week', 'top']\nfeats += ['day_of_week_' + str(i) for i in range(7)]\nfeats += ['start_hour_' + str(i) for i in range(7, 24)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6ee9e9e04ca9c02dcff33552abdfa66dcb61cf15"},"cell_type":"code","source":"tmp_scaled = StandardScaler().fit_transform(full_new_feat[feats])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n                             tmp_scaled[:idx_split,:]]))\n\n# Capture the quality with default parameters\n#score_C_1 = get_auc_lr_valid(X_train, y_train)\n#print(score_C_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4bad803dcab6921a1196ff3943bf5007dcd86765"},"cell_type":"code","source":"from tqdm import tqdm\n\n# List of possible C-values\nCs = np.logspace(-3, 1, 10)\nscores = []\nfor C in tqdm(Cs):\n    scores.append(get_auc_lr_valid(X_train, y_train, C=C))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"3d895820dd428d76da5c0db98a4b7ef7d51554bf"},"cell_type":"code","source":"plt.plot(Cs, scores, 'ro-')\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('AUC-ROC')\nplt.title('Regularization Parameter Tuning')\n# horizontal line -- model quality with default C value\nplt.axhline(y=score_C_1, linewidth=.5, color='b', linestyle='dashed') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1fbfeb3833c96222e45e1b7fe1b7e8a4ed995443"},"cell_type":"code","source":"max(zip(scores, Cs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5da7535ee7fba438cea4e6db23c375879aa956f2"},"cell_type":"code","source":"C_optim = max(zip(scores, Cs))[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"787de0c5f87c3846c7d20328138a26384f813385"},"cell_type":"code","source":"# Prepare the training and test data\ntmp_scaled = StandardScaler().fit_transform(full_new_feat[feats])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n                             tmp_scaled[:idx_split,:]]))\nX_test = csr_matrix(hstack([full_sites_sparse[idx_split:,:], \n                            tmp_scaled[idx_split:,:]]))\n\n# Train the model on the whole training data set using optimal regularization parameter\nlr = LogisticRegression(C=C_optim, random_state=17, solver='liblinear').fit(X_train, y_train)\n\n# Make a prediction for the test set\ny_test = lr.predict_proba(X_test)[:, 1]\n\n# Write it to the submission file\nwrite_to_submission_file(y_test, 'm_h_s_dw_sh_t1.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}