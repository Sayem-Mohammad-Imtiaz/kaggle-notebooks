{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assessing the likelihood of paying back the loan for a peer-to-peer Lending company\n\n## The Data\nWe will be using a subset of the LendingClub DataSet obtained from Kaggle: https://www.kaggle.com/hadiyad/lendingclub-data-sets\n\nLendingClub is a US peer-to-peer lending company.\n\n### Goal\n\nGiven historical data on loans given out with information on whether or not the borrower defaulted (charge-off), can we build a model that can predict whether or not a borrower will pay back their loan? This way in the future when the company gets a new potential customer,can assess whether or not they are likely to pay back the  loan.\n\nThe \"loan_status\" column contains the desired label.","metadata":{}},{"cell_type":"markdown","source":"\n\n#### Importing necesarry libraries and Loading the info file which has the description for each column in the original dataset and ","metadata":{}},{"cell_type":"code","source":"#Importing the basic libraries needed for EDA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#To store the plot images in the notebook document using inline commands.\n%matplotlib inline\ndata_info = pd.read_csv('../input/lendingclub-data-sets/lending_club_info.csv',index_col='LoanStatNew')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Overview**\n#### There are many LendingClub data sets on Kaggle. Lets have a look at the information on this particular data set:","metadata":{}},{"cell_type":"code","source":"# aligning the data towards left to get full view of the description column \ndata_info.style.set_properties(**{'text-align': 'left'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_info.loc['revol_util']['Description'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feat_info(col_name):\n    print(data_info.loc[col_name]['Description'])\nfeat_info('mort_acc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Loading the dataset**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/lendingclub-data-sets/lending_club_loan_two.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nWe have to fetch an overall understanding on all the parameters to find which variables are important by viewing summary statistics and visualizing the data\n\n----","metadata":{}},{"cell_type":"markdown","source":"**Let's create a simple countplot in an attempt to predict loan_status.**","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = \"loan_status\", data = df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's create a histogram plot on loan_amnt column.**","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (20,10))\ng = sns.histplot(ax = ax,x = 'loan_amnt', data = df,bins = 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's explore correlation between the continuous feature variables. This can be done by calling the method corr()**","metadata":{}},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TASK: Let's visualize this using a heatmap.**\n\n* [Heatmap info](https://seaborn.pydata.org/generated/seaborn.heatmap.html#seaborn.heatmap)\n* [Help with resizing](https://stackoverflow.com/questions/56942670/matplotlib-seaborn-first-and-last-row-cut-in-half-of-heatmap-plot)","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (12,10))\nsns.heatmap(ax = ax ,data = df.corr(), annot=True,cmap = 'coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see almost a perfect correlation with the \"installment\" feature. Let's explore this feature further. Print out their descriptions and perform a scatterplot between them. Does this relationship make sense to you? Do you think there is duplicate information here?**","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x = 'installment', y = 'loan_amnt', data = df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Displaying the boxplot showing the relationship between the loan_status and the Loan Amount.**","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x = 'loan_status', y = 'loan_amnt', data = df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The summary statistics for the loan amount, grouped by the loan_status.**","metadata":{}},{"cell_type":"code","source":"df.groupby('loan_status').describe().loan_amnt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's explore the Grade and SubGrade columns that LendingClub attributes to the loans. What are the unique possible grades and subgrades?**","metadata":{}},{"cell_type":"code","source":"print(np.sort(df.grade.unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_grade_keys = np.sort(df.sub_grade.unique())\nprint(sub_grade_keys)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's Create a countplot per grade. Set the hue to the loan_status label.**","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = 'grade', data = df, hue = 'loan_status')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Display a count plot per subgrade. Let's explore both all loans made per subgrade as well being separated based on the loan_status. Let's have a look at a similar plot, but with hue being set to \"loan_status\"**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15,7))\nsorted_df = df.sort_values(by = ['sub_grade'])\nsns.countplot(x = 'sub_grade',data = sorted_df, ax = ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15,7))\nsns.countplot(x = 'sub_grade',data = sorted_df, ax = ax, hue = 'loan_status')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It looks like F and G subgrades don't get paid back that often. Isloating those and recreating the countplot just for those subgrades.**","metadata":{}},{"cell_type":"markdown","source":"**Grade values before F and G isolotion**","metadata":{}},{"cell_type":"code","source":"print(df['grade'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Grade values after F and G isolotion**","metadata":{}},{"cell_type":"code","source":"sorted_df['grade'] = sorted_df['grade'].apply(lambda x : \"F and G\" if x in ['F','G'] else x)\ndf['grade'] = df['grade'].apply(lambda x : \"F and G\" if x in ['F','G'] else x)\nprint(sorted_df['grade'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Countplot of Grade after F and G isolotion**\n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15,7))\nsns.countplot(x = 'grade',data = sorted_df, ax = ax, hue = 'loan_status')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15,7))\nsns.countplot(data = sorted_df.loc[sorted_df.grade == 'F and G'][['sub_grade','loan_status']],x = 'sub_grade', hue = 'loan_status', ax = ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As loan status is a binary classifier which just says literally conveys whether the loan is fully paid or not,we can create a new column called 'loan_repaid' which will contain a 1 if the loan status was \"Fully Paid\" and a 0 if it was \"Charged Off\".**","metadata":{}},{"cell_type":"code","source":"df['loan_repaid'] = df['loan_status'].apply(lambda x : 1 if x == \"Fully Paid\" else 0)\ndf[['loan_repaid','loan_status']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets create a bar plot showing the correlation of the numeric features to the new loan_repaid column.**","metadata":{}},{"cell_type":"code","source":"df.corrwith(df.loan_repaid).drop('loan_repaid').sort_values().plot(kind = 'bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---\n# Data PreProcessing\n\n**Goals: Missing data handling. Removing unnecessary or repetitive features. Convert categorical string features to dummy variables.**\n\nLets have a look at the dataframe using the method head()\n","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Data\n\n**Let's explore this missing data columns. We use a variety of factors to decide whether or not they would be useful, to see if we should keep, discard, or fill in the missing data.**","metadata":{}},{"cell_type":"markdown","source":"**Total length of the dataframe?**","metadata":{}},{"cell_type":"code","source":"len(df.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TASK: Total count of missing values per column.**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets display in terms of percentage of the total DataFrame**","metadata":{}},{"cell_type":"code","source":"(df.isnull().sum() * 100 / len(df)).sort_values(ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's examine emp_title and emp_length columns to see whether it will be okay to drop them.**","metadata":{}},{"cell_type":"code","source":"feat_info('emp_title')\nfeat_info('emp_length')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looks like there are many employement job titles are present. Finding out the unique employment job titles will help us estimate the importance of that column?**","metadata":{}},{"cell_type":"code","source":"df.emp_title.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.emp_title.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Realistically there are too many unique job titles to try to convert this to a dummy variable feature. Let's remove that emp_title column.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns = ['emp_title'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Lets create a count plot of the emp_length feature column.<br>\n* Sorting the order of the values will be challenge here<br>\n* Hence we use **CategoricalDtype** method from pandas to set the datatype for emp_length column with the specified order","metadata":{}},{"cell_type":"code","source":"cat_emp_length = pd.CategoricalDtype(\n    ['< 1 year','1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years'], \n    ordered=True,\n)\ndf['emp_length'] = df['emp_length'].astype(cat_emp_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (16,5))\nsns.countplot(data = df.sort_values(by = 'emp_length'), x = 'emp_length')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's plot out the countplot with a hue separating Fully Paid vs Charged Off**","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (16,5))\nsns.countplot(data = df.sort_values(by = 'emp_length'), x = 'emp_length', hue = 'loan_status')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This still doesn't really inform us if there is a strong relationship between employment length and being charged off, what we want is the percentage of charge offs per category. Essentially informing us what percent of people per employment category didn't pay back their loan.**","metadata":{}},{"cell_type":"code","source":"grp_ln_stat = pd.pivot_table(df, index = ['emp_length'],columns = ['loan_status'],values=[\"loan_amnt\"],aggfunc=len).loan_amnt\ngrp_ln_stat['percent'] = grp_ln_stat['Charged Off']/grp_ln_stat['Fully Paid']*100\ngrp_ln_stat.percent.plot(kind = 'bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets drop the column emp_length, As the Charge off rates are extremely similar across all employment lengths.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns = ['emp_length'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Revisiting the DataFrame to see what feature columns still have missing data.**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Review the title column vs the purpose column. Is this repeated information?**","metadata":{}},{"cell_type":"code","source":"df[['purpose','title']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['title'].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The title column is simply a string subcategory/description of the purpose column. Let's go ahead and drop the title column.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns = ['title'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's find out what the mort_acc feature represents**","metadata":{}},{"cell_type":"code","source":"feat_info('mort_acc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Displaying the unique values with its counts of the mort_acc column.**","metadata":{}},{"cell_type":"code","source":"df.mort_acc.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are many ways we could deal with this missing data. We could attempt to build a simple model to fill it in, such as a linear model, we could just fill it in based on the mean of the other columns, or you could even bin the columns into categories and then set NaN as its own category. As there is no 100% correct approach, Let's review the other column to see which most highly correlates to mort_acc**","metadata":{}},{"cell_type":"code","source":"df.corr().mort_acc.drop('mort_acc').sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looks like the total_acc feature correlates with the mort_acc, this makes sense! Let's try this fillna() approach. We will group the dataframe by the total_acc and calculate the mean value for the mort_acc per total_acc entry. To get the result below:**","metadata":{}},{"cell_type":"code","source":"total_acc_grp = df.groupby('total_acc').mean().mort_acc\ntotal_acc_grp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's fill in the missing mort_acc values based on their total_acc value. If the mort_acc is missing, then we will fill in that missing value with the mean value corresponding to its total_acc value from the Series we created above. We are going to use .apply() method on axis=1. Check out the link below for more info.**\n\n[Reference](https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe) ","metadata":{}},{"cell_type":"code","source":"df.loc[10].isnull().mort_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['mort_acc'] = df.apply(lambda x : total_acc_grp[x.total_acc] if x.isnull().mort_acc else x.mort_acc, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total null values in mort_acc\ndf['mort_acc'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Percentage of missing values in each columns\ndf.isnull().sum().sort_values(ascending = False)/len(df)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**revol_util and the pub_rec_bankruptcies have missing data points, but they account for less than 0.5% of the total data. Go ahead and remove the rows that are missing those values in those columns with dropna().**","metadata":{}},{"cell_type":"code","source":"print(\"length of dataframe before and after removing the missing values\")\nprint(len(df))\ndf.dropna(inplace = True)\nprint(len(df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Variables and Dummy Variables\n\n**We're done working with the missing data! Now we just need to deal with the string values due to the categorical columns.**\n\n**Let's list down all the columns that are currently non-numeric.","metadata":{}},{"cell_type":"code","source":"df.select_dtypes(include = ['object']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.term.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n**Let's now go through all the string features to see what we should do with them.**\n\n---\n\n\n### Term feature\n\n**Lets Convert the term feature into either a 36 or 60 integer numeric data type**","metadata":{}},{"cell_type":"code","source":"df['term_36_or_60'] = df.term.apply(lambda x: 0 if int(x[:3]) == 36 else 1)\ndf['term_36_or_60'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### grade feature\n\n**TASK: We already know grade is part of sub_grade, so just drop the grade feature.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns = ['grade','term'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Converting the subgrade into dummy variables. Then concatenate these new columns to the original dataframe.**","metadata":{}},{"cell_type":"code","source":"new_df = df.copy()\nnew_df = pd.get_dummies(data = new_df,columns = ['sub_grade'], prefix = 'grade',drop_first = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.select_dtypes(include = ['object']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### verification_status, application_type,initial_list_status,purpose \n**TASK: Convert these columns: ['verification_status', 'application_type','initial_list_status','purpose'] into dummy variables and concatenate them with the original dataframe. Remember to set drop_first=True and to drop the original columns.**","metadata":{}},{"cell_type":"code","source":"new_df = pd.get_dummies(data = new_df, columns =  ['verification_status', 'application_type','initial_list_status','purpose'] , prefix =  ['ver_status', 'app_type','init_list_status','purpose'],drop_first = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.select_dtypes(include = ['object']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### home_ownership\n**LEt's review the value_counts for the home_ownership column.**","metadata":{}},{"cell_type":"code","source":"new_df.home_ownership.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NONE and ANY classes can be merged with OTHER, so that we end up with just 4 categories, MORTGAGE, RENT, OWN, OTHER.**","metadata":{}},{"cell_type":"code","source":"new_df['home_ownership'] = new_df.home_ownership.replace(['NONE','ANY'], 'OTHER')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TASK: Now make this zip_code column into dummy variables using pandas. Concatenate the result and drop the original zip_code column along with dropping the address column.**","metadata":{}},{"cell_type":"code","source":"new_df['zip_code'] = new_df.address.apply(lambda x: x[-5:])\nnew_df.drop(columns = ['address'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = pd.get_dummies(data = new_df, columns = ['zip_code','home_ownership'], prefix = ['zip', 'home_own'], drop_first = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.select_dtypes(include = ['object']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### issue_d \n\n**This would be data leakage, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_date, Let's drop this feature.**","metadata":{}},{"cell_type":"code","source":"new_df.drop(columns = ['issue_d'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### earliest_cr_line\n**This appears to be a historical time stamp feature. Let's extract the year from this feature using a .apply function, then convert it to a numeric feature.**","metadata":{}},{"cell_type":"code","source":"new_df['earliest_cr_year'] = new_df.earliest_cr_line.apply(lambda x: int(x[-4:]))\nnew_df.drop(columns = ['earliest_cr_line'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.select_dtypes(include = ['object']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"markdown","source":"**Import train_test_split from sklearn.**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's drop the load_status column we created earlier, since its a duplicate of the loan_repaid column. We'll use the loan_repaid column since its already in 0s and 1s.**","metadata":{}},{"cell_type":"code","source":"new_df.drop(columns = ['loan_status'], inplace = True)\nlen(new_df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TASK: Set X and y variables to the .values of the features and label.**","metadata":{}},{"cell_type":"code","source":"X = new_df.drop(columns = ['loan_repaid']).values\ny = new_df.loan_repaid.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----\n----\n\n## Grabbing a Sample for Training Time\n\n### Using .sample() to grab a sample of the 490k+ entries to save time on training. Highly recommended for lower RAM computers or if you are not using GPU.\n\n----\n----","metadata":{}},{"cell_type":"code","source":"s_df = new_df.sample(frac=0.1,random_state=101)\nprint(len(s_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TASK: Perform a train/test split with test_size=0.2 and a random_state of 101.**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalizing the Data\n\n**A MinMaxScaler can be used to normalize the feature data X_train and X_test.**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train_new = scaler.fit_transform(X_train)\nX_test_new = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model Creation\n\n**Importing the necessary Keras functions.**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build a sequential model to will be trained on the data. You have unlimited options here, but here is what the solution uses: a model that goes 78 --> 39 --> 19--> 1 output neuron.**","metadata":{}},{"cell_type":"code","source":"# CODE HERE\nmodel = Sequential()\n\nmodel.add(Dense(78, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(39, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(19, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TASK: Fit the model to the training data for at least 25 epochs. Also add in the validation data for later plotting. Optional: add in a batch_size of 256.**","metadata":{}},{"cell_type":"code","source":"model.fit(x = X_train_new, y = y_train, epochs = 25, batch_size = 256, validation_data = (X_test_new, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** Save your model.**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('lend_club_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating Model Performance.\n\n**Plot out the validation loss versus the training loss.**","metadata":{}},{"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create predictions from the X_test set and display a classification report and confusion matrix for the X_test set.**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict_classes(X_test_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Quick check\n\n**Given the customer below, would you offer this person a loan?**","metadata":{}},{"cell_type":"code","source":"import random\nrandom.seed(101)\nrandom_ind = random.randint(0,len(df))\n\nnew_customer = new_df.drop('loan_repaid',axis=1).iloc[random_ind]\nnew_customer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_customer = scaler.transform(new_customer.values.reshape(1,78))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict_classes(new_customer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now check, did this person actually end up paying back their loan?**","metadata":{}},{"cell_type":"code","source":"new_df.iloc[random_ind].loan_repaid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We got it right!!**","metadata":{}}]}