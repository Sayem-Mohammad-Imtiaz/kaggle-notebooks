{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Import packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.iloc[:,:-1]\ny=df.DEATH_EVENT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_corr=X.corr()\nmask = np.zeros_like(X_corr)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(X_corr, annot=True, fmt='.2f', mask=mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"Corr = df.corr()\nCorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(Corr,vmin=0, vmax=1, center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,0:13]  \ny = df.iloc[:,-1]\n#apply SelectKBest class to extract best features\nparameters = SelectKBest(score_func=chi2, k=13)\nfit = parameters.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(13,'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(13).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['anaemia'].replace(1,'Anemic',inplace = True)\ndf['anaemia'].replace(0,'Non-Anemic',inplace = True)\ndf['diabetes'].replace(1,'Diabetic',inplace = True)\ndf['diabetes'].replace(0,'Non-Diabetic',inplace = True)\ndf['smoking'].replace(1,'Smoker',inplace = True)\ndf['smoking'].replace(0,'Non-Smoker',inplace = True)\ndf['high_blood_pressure'].replace(1,'Hypertension',inplace = True)\ndf['high_blood_pressure'].replace(0,'Other',inplace = True)\ndf['sex'].replace(1,'Male',inplace = True)\ndf['sex'].replace(0,'Female',inplace = True)\ndf['DEATH_EVENT'].replace(1,'Heart Attack',inplace = True)\ndf['DEATH_EVENT'].replace(0,'Alive',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['ejection_fraction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['serum_sodium'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.hist(df['serum_creatinine'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pip install plotly","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.pie(df, names='DEATH_EVENT', title=\"Number of Deaths\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ds = df['sex']\nds = ds.to_frame()\nds['DEATH_EVENT'] = df['DEATH_EVENT']\ndx = ds.value_counts().reset_index()\ndx.columns = ['Sex','DEATH_EVENT', 'Count']\n\nfig = px.bar(dx,x=\"Sex\",y=\"Count\",color=\"DEATH_EVENT\",title=\"Sex and Heart Attack\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(df,x='age',nbins=50,color='DEATH_EVENT',barmode = 'relative',title=('Age & Heart Attack Distribution'))\nfig.update_layout(title_x = 0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(df, names='diabetes', title=\"Diabetic Distribution\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trial and Error revealed that not considering Age column improves accuracy with correlation\n\nx = df[['ejection_fraction', 'serum_creatinine', 'serum_sodium', 'time']]\ny = df['DEATH_EVENT']\n\n#Spliting data into training and testing data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOGISTIC REGRESSION"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"lr=LogisticRegression()\nlr.fit(x_train,y_train)\np1=lr.predict(x_test)\ns1=accuracy_score(y_test,p1)\nprint(\"Logistic Regression Success Rate :\", \"{:.2f}%\".format(100*s1))\nplot_confusion_matrix(lr, x_test, y_test)\nprint(classification_report(y_test,p1))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding best parameters\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve, GridSearchCV, validation_curve\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn import linear_model, decomposition, datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nstd_slc = StandardScaler()\npca = decomposition.PCA()\nlogistic_Reg = linear_model.LogisticRegression()\npipe = Pipeline(steps=[('std_slc', std_slc),\n                           ('pca', pca),\n                           ('logistic_Reg', logistic_Reg)])\nn_components = list(range(1,x.shape[1]+1,1))\nC = np.logspace(-4, 4, 50)\npenalty = ['l1', 'l2']\nparameters = dict(pca__n_components=n_components,\n                      logistic_Reg__C=C,\n                      logistic_Reg__penalty=penalty)\nclf_GS = GridSearchCV(pipe, parameters)\nclf_GS.fit(x_train, y_train)\nprint('Best Penalty:', clf_GS.best_estimator_.get_params()['logistic_Reg__penalty'])\nprint('Best C:', clf_GS.best_estimator_.get_params()['logistic_Reg__C'])\nprint('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\nprint(); print(clf_GS.best_estimator_.get_params()['logistic_Reg'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After Best Parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(C=0.5689866029018293, penalty = 'l2')\nlr.fit(x_train,y_train)\npred_best=lr.predict(x_test)\ns1=accuracy_score(y_test,pred_best)\nprint(\"Logistic Regression Success Rate :\", \"{:.2f}%\".format(100*s1))\nplot_confusion_matrix(lr, x_test, y_test)\nprint(classification_report(y_test,pred_best))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve, GridSearchCV, validation_curve\nparam_range = [0.5689866029018293]\n\nplt.figure(figsize=(15, 10))\n\n# Apply logistic regression model to training data\nlr = LogisticRegression(C=0.5689866029018293)\n\n#Plot validation curve\ntrain_scores, test_scores = validation_curve(estimator=lr ,X=x\n                                                            ,y=y\n                                                            ,param_name='C'\n                                                            ,param_range=param_range\n                                                            )\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nplt.subplot(2,2,1)\nplt.plot(param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.plot(param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfclassifier = RandomForestClassifier()\n                                   \nrfcmodel = rfclassifier.fit(x_train, y_train)\ny_pred = rfcmodel.predict(x_test)\nfinal_acc =accuracy_score(y_test,y_pred)\nprint(\"Random Forest Classifier Success Rate :\", \"{:.6f}%\".format(100*final_acc))\nplot_confusion_matrix(rfcmodel, x_test, y_test)\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_estimators = [100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] \n\nhyperparameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\nrfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid_search = GridSearchCV(rfc, hyperparameters, cv = 3, verbose = 1,n_jobs = -1)\nbest_params = grid_search.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfclassifier = RandomForestClassifier(random_state = 1, max_depth = 5,     n_estimators = 10)\n                                   \nrfcmodel = rfclassifier.fit(x_train, y_train)\ny_pred = rfcmodel.predict(x_test)\nfinal_acc =accuracy_score(y_test,y_pred)\nprint(\"Random Forest Classifier Success Rate :\", \"{:.6f}%\".format(100*final_acc))\nplot_confusion_matrix(rfcmodel, x_test, y_test)\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SUPPORT VECTOR MACHINE WITH HYPER PARAMETERS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nf_svm=SVC(kernel = 'linear')\nf_svm.fit(x_train,y_train)\npredsvc=f_svm.predict(x_test)\nf_acc=accuracy_score(y_test,predsvc)\nprint(\"Support Vector Machine Success Rate :\", \"{:.6f}%\".format(100*f_acc))\nplot_confusion_matrix(f_svm, x_test, y_test)\nprint(classification_report(y_test, predsvc))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \n  \n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear']}  \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# fitting the model for grid search \ngrid.fit(x_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(grid.best_params_)\nprint(grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nf_svm=SVC(C=10, gamma=0.0001, kernel = 'rbf')\nf_svm.fit(x_train,y_train)\npredsvc=f_svm.predict(x_test)\nf_acc=accuracy_score(y_test,predsvc)\nprint(\"Support Vector Machine Success Rate :\", \"{:.6f}%\".format(100*f_acc))\nplot_confusion_matrix(f_svm, x_test, y_test)\nprint(classification_report(y_test, predsvc))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNEIGHBORS"},{"metadata":{},"cell_type":"markdown","source":"### WITHOUT TUNING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier()\nknn.fit(x_train,y_train)\npred_knn=knn.predict(x_test)\nknn_accuracy=accuracy_score(y_test,pred_knn)\nprint(\"The Model Accuracy is:\",knn_accuracy*100,\"%\")    \n\nplot_confusion_matrix(knn, x_test, y_test)\nprint(classification_report(y_test,pred_knn))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WITH TUNING"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n#List Hyperparameters that we want to tune.\nleaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\np=[1,2,3]\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n#Create new KNN object\nknn_2 = KNeighborsClassifier()\n#Use GridSearch\nclf = GridSearchCV(knn_2, hyperparameters, cv=10)\n#Fit the model\nbest_model = clf.fit(x_train,y_train)\n#Print The value of best Hyperparameters\nprint('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\nprint('Best p:', best_model.best_estimator_.get_params()['p'])\nprint('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=5, leaf_size=1,p=1)\nknn.fit(x_train,y_train)\npred_knn=knn.predict(x_test)\nknn_accuracy=accuracy_score(y_test,pred_knn)\nprint(\"The Model Accuracy is:\",knn_accuracy*100,\"%\")    \n\nplot_confusion_matrix(knn, x_test, y_test)\nprint(classification_report(y_test,pred_knn))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DECISIONTREE CLASSIFIER"},{"metadata":{},"cell_type":"markdown","source":"### WITHOUT TUNING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\nDTAcc= accuracy_score(y_test,y_pred)*100\nprint(\"The Accuracy of model is\",DTAcc)\nplot_confusion_matrix(classifier, x_test, y_test)\nprint(classification_report(y_test,y_pred))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WITH TUNING"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import decomposition, datasets\nfrom sklearn import tree\nfrom sklearn.pipeline import Pipeline\nstd_slc = StandardScaler()\npca = decomposition.PCA()\ndec_tree = tree.DecisionTreeClassifier()\npipe = Pipeline(steps=[('std_slc', std_slc),\n                           ('pca', pca),\n                           ('dec_tree', dec_tree)])\nn_components = list(range(1,X.shape[1]+1,1))\ncriterion = ['gini', 'entropy']\nmax_depth = [2,4,6,8,10,12]\nparameters = dict(pca__n_components=n_components,\n                      dec_tree__criterion=criterion,\n                      dec_tree__max_depth=max_depth)\nclf_GS = GridSearchCV(pipe, parameters)\nclf_GS.fit(x, y)\nprint('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\nprint('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\nprint('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\nprint(); print(clf_GS.best_estimator_.get_params()['dec_tree'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion='entropy', max_depth=2)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\nDTAcc= accuracy_score(y_test,y_pred)*100\nprint(\"The Accuracy of model is\",DTAcc)\nplot_confusion_matrix(classifier, x_test, y_test)\nprint(classification_report(y_test,y_pred))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FINAL ACCURACIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression Success Rate :\", \"{:.2f}%\".format(100*s1))\nprint(\"Random Forest Classifier Success Rate :\", \"{:.2f}%\".format(100*final_acc))\nprint(\"Support Vector Machine Success Rate :\", \"{:.2f}%\".format(100*f_acc))\nprint(\"Kneighbors Success Rate:\",\"{:.2f}%\".format(knn_accuracy*100))    \nprint(\"Decision Tree Sucess Rate\",\"{:.2f}%\".format(DTAcc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"  ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}