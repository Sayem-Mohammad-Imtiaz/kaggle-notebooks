{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import io, datasets, transforms\nfrom tqdm.notebook import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-01T11:47:56.283075Z","iopub.execute_input":"2021-07-01T11:47:56.283401Z","iopub.status.idle":"2021-07-01T11:47:57.820726Z","shell.execute_reply.started":"2021-07-01T11:47:56.283331Z","shell.execute_reply":"2021-07-01T11:47:57.819834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# constants\nimg_width = 384\nimg_height = 128\nbatch_size = 50\nop_str = '0123456789+-*/'","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:48:01.046326Z","iopub.execute_input":"2021-07-01T11:48:01.046655Z","iopub.status.idle":"2021-07-01T11:48:01.050756Z","shell.execute_reply.started":"2021-07-01T11:48:01.046626Z","shell.execute_reply":"2021-07-01T11:48:01.049639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outfile = open('annotations_digits.csv', 'w', newline='')\n\noutput = csv.writer(outfile)\noutput.writerow(['Image','Label','Left','Operator','Right','Value'])\n\nctr = 0\nnum = 0\nwhile ctr < 50000:\n    i = (num//10)%10\n    j = num%10\n    for k in ['+', '-', '*', '/']:\n        for l in ['prefix', 'postfix', 'infix']:\n            if ctr >= 50000:\n                break\n            if k == '/' and (j == 0 or (i/j)%1 != 0):\n                continue\n            ctr += 1\n            if l == 'prefix':\n                output.writerow([f'{ctr}.jpg', l, k, i, j, int(eval(f'{i}{k}{j}'))])\n            elif l == 'infix':\n                output.writerow([f'{ctr}.jpg', l, i, k, j, int(eval(f'{i}{k}{j}'))])\n            else:\n                output.writerow([f'{ctr}.jpg', l, i, j, k, int(eval(f'{i}{k}{j}'))])                \n        if ctr >= 50000:\n            break\n    num += 1\n\nprint(ctr)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:48:44.165643Z","iopub.execute_input":"2021-07-01T11:48:44.165964Z","iopub.status.idle":"2021-07-01T11:48:44.760077Z","shell.execute_reply.started":"2021-07-01T11:48:44.165935Z","shell.execute_reply":"2021-07-01T11:48:44.759231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#enable cuda\nuse_cuda = torch.cuda.is_available()\nif use_cuda:\n    device = torch.device('cuda')\n    loader_kwargs = {'num_workers': 1, 'pin_memory': True}\nelse:\n    device = torch.device('cpu')\n    loader_kwargs = {}\n    \nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:48:46.668486Z","iopub.execute_input":"2021-07-01T11:48:46.668805Z","iopub.status.idle":"2021-07-01T11:48:46.674136Z","shell.execute_reply.started":"2021-07-01T11:48:46.668774Z","shell.execute_reply":"2021-07-01T11:48:46.673321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ExpressionDataset(Dataset):\n    \"\"\"Binomial expression dataset\"\"\"\n    \n    def __init__(self, csv_file, root_dir):\n        self.labels = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transforms.ToTensor()\n        \n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        img_name = os.path.join(self.root_dir, self.labels.iloc[idx,0])\n        img = Image.open(img_name)\n        leftimg = img.crop((0,0,img_height,img_height))\n        middleimg = img.crop((img_height,0,img_height*2,img_height))\n        rightimg = img.crop((img_height*2,0,img_height*3,img_height))\n        exprtype, left, op, right, ans = self.labels.iloc[idx,1:]\n        exprtype_int = 0\n        if exprtype == \"infix\":\n            exprtype_int = 1\n        elif exprtype == \"postfix\":\n            exprtype_int = 2\n        return [self.transform(img).float(), self.transform(leftimg).float(), self.transform(middleimg).float(), self.transform(rightimg).float(), exprtype_int, op_str.index(left), op_str.index(op), op_str.index(right), ans]\n\ndataset = ExpressionDataset(csv_file='/kaggle/working/annotations_digits.csv', root_dir='../input/soml-hackathon/SoML/SoML-50-old/data')\nplt.imshow(dataset[10][3].squeeze())\nprint(len(dataset))\n\ntrain_set, test_set, validation_set = torch.utils.data.random_split(dataset, [40000,5000,5000])\n\ntrain_loader = DataLoader(train_set, batch_size=50, shuffle=True)\ntest_loader = DataLoader(test_set, batch_size=50, shuffle=True)\nvalidation_loader = DataLoader(validation_set, batch_size=50, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:48:48.628751Z","iopub.execute_input":"2021-07-01T11:48:48.629126Z","iopub.status.idle":"2021-07-01T11:48:48.825876Z","shell.execute_reply.started":"2021-07-01T11:48:48.629092Z","shell.execute_reply":"2021-07-01T11:48:48.825092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DigitNetwork(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size = 10)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(2420, 100)\n        self.fc2 = nn.Linear(100, 14)       \n#        self.input_layer = torch.nn.Linear(784, 112)\n#        self.hl2 = torch.nn.Linear(112, 14)\n#        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):        \n        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 3))\n        x = torch.flatten(x,1)\n#        print(x.shape)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training = self.training)\n        x = self.fc2(x)\n        return x\n#        x = self.input_layer(x)\n#        x = self.relu(x)\n#        x = self.hl2(x)\n    \n    def digit_acc(self, loader):\n        total = 0\n        correct = 0\n        \n        with torch.no_grad(): \n            for fullimg, leftimg, midimg, rightimg, types, left, mid, right, ans in loader:\n                batch_size = fullimg.shape[0]\n                sections = [[leftimg,left], [midimg,mid], [rightimg,right]]\n                for images, labels in sections:\n                    images, labels = images.to(device), labels.to(device)\n                    batch_size = images.shape[0]\n                    #images = images.reshape(batch_size, 28*28)\n                    output = digit_net(images)\n\n                    prediction = torch.argmax(output, dim=1)\n                    correct += torch.sum(prediction == labels)\n                    total += batch_size\n        \n        return correct/total\n    \n    def acc(self, loader):\n        total = 0\n        correct = 0\n        op_str = \"0123456789+-*/\"\n        \n        with torch.no_grad(): \n            for fullimg, leftimg, midimg, rightimg, types, left, mid, right, ans in loader:\n                ans = ans.to(device)\n                sections = [leftimg, midimg, rightimg]\n                batch_size = fullimg.shape[0]\n                predictions = []\n                answers = []\n                for images in sections:\n                    images = images.to(device)\n                    output = self(images)\n\n                    predicted = torch.argmax(output, dim=1)\n                    predictions.append(predicted)\n                for i in range(batch_size):\n                    l,m,r = predictions[0][i], predictions[1][i], predictions[2][i]\n                    if l >= 10: # prefix\n                        answers.append(eval(f'{m}{op_str[l]}{r}'))\n                    elif m >= 10: # infix\n                        answers.append(eval(f'{l}{op_str[m]}{r}'))\n                    elif r >= 10: # postfix\n                        answers.append(eval(f'{l}{op_str[r]}{m}'))\n                    else:\n                        answers.append(0)\n                \n                correct += torch.sum(torch.tensor(answers).to(device) == ans)\n                total += batch_size\n        \n        return correct/total","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:49:01.298045Z","iopub.execute_input":"2021-07-01T11:49:01.298413Z","iopub.status.idle":"2021-07-01T11:49:01.317072Z","shell.execute_reply.started":"2021-07-01T11:49:01.298372Z","shell.execute_reply":"2021-07-01T11:49:01.316063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"digit_net = DigitNetwork().to(device)\nlossfn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(digit_net.parameters(), lr=0.001)\n\nprint(f\"Accuracy: {digit_net.digit_acc(validation_loader)}\")\nfor epoch in range(8):\n    avg_loss = 0\n    num_iters = 0\n    digit_net.train()\n    for fullimgs, leftimgs, midimgs, rightimgs, types, left, mid, right, answers in tqdm(train_loader):\n        sections = [[leftimgs,left], [midimgs,mid], [rightimgs,right]]\n        for images, labels in sections:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            batch_size = images.shape[0]\n            #images = images.reshape(batch_size, 28*28)\n            output = digit_net(images)\n            loss = lossfn(output, labels)\n\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item()\n            num_iters += 1\n    digit_net.eval()    \n    print(f\"Loss: {avg_loss/num_iters}\")\n    print(f\"Accuracy: {digit_net.digit_acc(validation_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:49:13.733615Z","iopub.execute_input":"2021-07-01T11:49:13.733932Z","iopub.status.idle":"2021-07-01T12:08:57.112978Z","shell.execute_reply.started":"2021-07-01T11:49:13.733901Z","shell.execute_reply":"2021-07-01T12:08:57.111502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"digit_net.eval()\nprint(f\"Accuracy on expressions: {digit_net.acc(test_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T12:09:53.559312Z","iopub.execute_input":"2021-07-01T12:09:53.559684Z","iopub.status.idle":"2021-07-01T12:10:43.15053Z","shell.execute_reply.started":"2021-07-01T12:09:53.559648Z","shell.execute_reply":"2021-07-01T12:10:43.149696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(digit_net.state_dict(), '/kaggle/working/value_net_dict')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T12:13:07.197465Z","iopub.execute_input":"2021-07-01T12:13:07.197782Z","iopub.status.idle":"2021-07-01T12:13:07.217892Z","shell.execute_reply.started":"2021-07-01T12:13:07.197754Z","shell.execute_reply":"2021-07-01T12:13:07.217016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}