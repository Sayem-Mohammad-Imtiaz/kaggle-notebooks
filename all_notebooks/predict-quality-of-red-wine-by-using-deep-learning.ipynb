{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b4b2e5ddb04f7f66dfd86c901728d84055a4851"},"cell_type":"markdown","source":"Use StandardScaler to normalize the data, and SGDClassifier to classify them at first."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDClassifier\n\nwine = pd.read_csv('../input/winequality-red.csv')\nx = wine.drop('quality', axis = 1)\ny = wine['quality']\n\n# Split data to 'train' and 'test'\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 23)\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)\n\nsgdc = SGDClassifier(max_iter = 800)\nsgdc.fit(x_train, y_train)\ny_predict = sgdc.predict(x_test)\nscore = accuracy_score(y_test, y_predict)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2996529b611758a09d2174fbbafb6261ebbf1c61"},"cell_type":"markdown","source":"The accuracy for test dataset is only 0.578125, which is too low. So we could try KNeighborsClassifier then."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"7c15dda4b54b0abac9ef0f05e8ab99213b4d28f0"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 200)\nknn.fit(x_train, y_train)\ny_predict = knn.predict(x_test)\nscore = accuracy_score(y_test, y_predict)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7a601f71b639f6b9682ab5778b98d1388d8c62e"},"cell_type":"markdown","source":"KNeighborsClassifier's 0.61875 is better than SGDClassifier.\nHow about Deep Neural Network? Could it reach much more higher accuracy? Let's try."},{"metadata":{"trusted":true,"_uuid":"864fdd9ad6e5cb74a0892be6a46ff195cb4965fc"},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.contrib.layers import fully_connected\nfrom sklearn.preprocessing import LabelBinarizer\n\nlearning_rate = 0.001\n\nnr_features = x_train.shape[1]\nnr_samples = x_train.shape[0]\nquality_rank = len(y_train.unique())\n\nlb = LabelBinarizer()\nlabel_train = lb.fit_transform(y_train)\nlabel_test = lb.fit_transform(y_test)\n\n# Build graph of neural network\nwith tf.device('/cpu:0'):\n    x_holder = tf.placeholder(tf.float32, (None, nr_features))\n    y_holder = tf.placeholder(tf.int32, (None, quality_rank))\n    keep_prob = tf.placeholder(tf.float32)\n\n    input = tf.nn.dropout(x_holder, keep_prob = keep_prob)\n    \n    layer1 = fully_connected(input, 1024)\n    layer2 = fully_connected(layer1, 512)\n    layer3 = fully_connected(layer2, 256)\n    layer4 = fully_connected(layer3, 128)\n    logits = fully_connected(layer4, quality_rank)\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = y_holder)\n    loss = tf.reduce_sum(loss)\n    opt = tf.train.AdamOptimizer(learning_rate)\n    train_op = opt.minimize(loss)\n\n    # evaluation\n    correct_predict = tf.equal(tf.argmax(logits, 1), tf.argmax(y_holder, 1))\n    correct_predict = tf.cast(correct_predict, tf.float32)\n    accuracy = tf.reduce_mean(correct_predict)\n \n# Run graph (training)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(3000):\n        _, loss_ = sess.run([train_op, loss], feed_dict = {x_holder: x_train, y_holder: label_train, keep_prob: 0.9})\n        if i % 100 == 0:\n            acc = sess.run(accuracy, feed_dict = {x_holder: x_test, y_holder: label_test, keep_prob: 1.0})\n            print('loss: %g, accuracy: %g' % (loss_, acc))            ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"488dc1cf10b8cc3a50d56ac4618789e63febe990"},"cell_type":"markdown","source":"Seems 70% is the highest accuracy the Four-Layers-Neural-Network could reach."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}