{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:09.753264Z","iopub.execute_input":"2021-08-09T09:30:09.753643Z","iopub.status.idle":"2021-08-09T09:30:14.695231Z","shell.execute_reply.started":"2021-08-09T09:30:09.753609Z","shell.execute_reply":"2021-08-09T09:30:14.694388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check our GPU Availability\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:14.696909Z","iopub.execute_input":"2021-08-09T09:30:14.697274Z","iopub.status.idle":"2021-08-09T09:30:15.543258Z","shell.execute_reply.started":"2021-08-09T09:30:14.697236Z","shell.execute_reply":"2021-08-09T09:30:15.542284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"markdown","source":"We will load the individual dataset, create a target attribute which will indicate '1' if the news is fake. Combine both the dataframes and create the combine dataframe for modelling","metadata":{}},{"cell_type":"code","source":"# load the fake and real news datasets\nfake_news = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\nfake_news.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:15.547062Z","iopub.execute_input":"2021-08-09T09:30:15.547332Z","iopub.status.idle":"2021-08-09T09:30:16.952709Z","shell.execute_reply.started":"2021-08-09T09:30:15.547303Z","shell.execute_reply":"2021-08-09T09:30:16.951779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_news = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\ntrue_news.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:16.954407Z","iopub.execute_input":"2021-08-09T09:30:16.954758Z","iopub.status.idle":"2021-08-09T09:30:18.004343Z","shell.execute_reply.started":"2021-08-09T09:30:16.954723Z","shell.execute_reply":"2021-08-09T09:30:18.003447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a column with fake=1 in fake_news dataset\nfake_news['fake']=1\nfake_news.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.005704Z","iopub.execute_input":"2021-08-09T09:30:18.006089Z","iopub.status.idle":"2021-08-09T09:30:18.021766Z","shell.execute_reply.started":"2021-08-09T09:30:18.006058Z","shell.execute_reply":"2021-08-09T09:30:18.020782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a column with fake=0 in true_news dataset\ntrue_news['fake'] = 0\ntrue_news.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.02308Z","iopub.execute_input":"2021-08-09T09:30:18.023654Z","iopub.status.idle":"2021-08-09T09:30:18.039673Z","shell.execute_reply.started":"2021-08-09T09:30:18.023613Z","shell.execute_reply":"2021-08-09T09:30:18.038773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concat two fake and true news\nnews = pd.concat([fake_news, true_news])\nnews.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.040946Z","iopub.execute_input":"2021-08-09T09:30:18.041276Z","iopub.status.idle":"2021-08-09T09:30:18.063165Z","shell.execute_reply.started":"2021-08-09T09:30:18.041242Z","shell.execute_reply":"2021-08-09T09:30:18.062372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for any null values\nnews.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.064551Z","iopub.execute_input":"2021-08-09T09:30:18.06494Z","iopub.status.idle":"2021-08-09T09:30:18.09216Z","shell.execute_reply.started":"2021-08-09T09:30:18.064897Z","shell.execute_reply":"2021-08-09T09:30:18.090919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the info\nnews.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.096002Z","iopub.execute_input":"2021-08-09T09:30:18.096294Z","iopub.status.idle":"2021-08-09T09:30:18.134749Z","shell.execute_reply.started":"2021-08-09T09:30:18.096266Z","shell.execute_reply":"2021-08-09T09:30:18.133796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis and Data Visualizations","metadata":{}},{"cell_type":"code","source":"# Explore the target variable\nsns.countplot(x='fake', data=news)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.136811Z","iopub.execute_input":"2021-08-09T09:30:18.137169Z","iopub.status.idle":"2021-08-09T09:30:18.279539Z","shell.execute_reply.started":"2021-08-09T09:30:18.137115Z","shell.execute_reply":"2021-08-09T09:30:18.278513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore 2 text for the fake dataset\nnews[news['fake']==1]['text'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.280988Z","iopub.execute_input":"2021-08-09T09:30:18.281328Z","iopub.status.idle":"2021-08-09T09:30:18.306009Z","shell.execute_reply.started":"2021-08-09T09:30:18.28129Z","shell.execute_reply":"2021-08-09T09:30:18.30515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore 2 text for true news\nnews[news['fake']==0]['text'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.308392Z","iopub.execute_input":"2021-08-09T09:30:18.309007Z","iopub.status.idle":"2021-08-09T09:30:18.320713Z","shell.execute_reply.started":"2021-08-09T09:30:18.308963Z","shell.execute_reply":"2021-08-09T09:30:18.319649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore the subject column\nplt.figure(figsize=(10,5))\nsns.countplot(x='subject', data=news, hue='fake')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.322349Z","iopub.execute_input":"2021-08-09T09:30:18.32276Z","iopub.status.idle":"2021-08-09T09:30:18.621117Z","shell.execute_reply.started":"2021-08-09T09:30:18.322719Z","shell.execute_reply":"2021-08-09T09:30:18.62016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\nWe will create a new columns calld Month and Year from Date and Analyse whether fake or true news has some correlation with Month or Year in the timeline","metadata":{}},{"cell_type":"code","source":"news['date'] = pd.to_datetime(news['date'], errors='coerce')\nnews['Year'] = news['date'].dt.year\nnews['Month'] = news['date'].dt.month\n\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.622549Z","iopub.execute_input":"2021-08-09T09:30:18.622897Z","iopub.status.idle":"2021-08-09T09:30:18.899143Z","shell.execute_reply.started":"2021-08-09T09:30:18.622858Z","shell.execute_reply":"2021-08-09T09:30:18.898052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the impact of yead on tha target\nsns.countplot(x='Year', data=news, hue='fake')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:18.900705Z","iopub.execute_input":"2021-08-09T09:30:18.901059Z","iopub.status.idle":"2021-08-09T09:30:19.113985Z","shell.execute_reply.started":"2021-08-09T09:30:18.901023Z","shell.execute_reply":"2021-08-09T09:30:19.113166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the impact of Month on the target variable\nsns.countplot(x='Month', data=news, hue='fake')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.115285Z","iopub.execute_input":"2021-08-09T09:30:19.115688Z","iopub.status.idle":"2021-08-09T09:30:19.590593Z","shell.execute_reply.started":"2021-08-09T09:30:19.115651Z","shell.execute_reply":"2021-08-09T09:30:19.589585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will combine the title and text column**","metadata":{}},{"cell_type":"code","source":"news['text'] = news['title'] + news['text']\nnews.drop(labels=['title'], axis=1,inplace=True)\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.592066Z","iopub.execute_input":"2021-08-09T09:30:19.592425Z","iopub.status.idle":"2021-08-09T09:30:19.753784Z","shell.execute_reply.started":"2021-08-09T09:30:19.592378Z","shell.execute_reply":"2021-08-09T09:30:19.752777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the final data\n\nWe will remove the subject attribute - Since it perfectly distributes the target variable We will remove the Year attribute - This also has a clear division for the target variable We will remove the Month Attribute - This also has a very clear approach of demarcating the target variable\n\nFor now we will just go ahead with the \"text attribute\"","metadata":{}},{"cell_type":"code","source":"news.drop(labels=['subject','date', 'Year','Month'], axis=1, inplace=True)\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.755254Z","iopub.execute_input":"2021-08-09T09:30:19.755616Z","iopub.status.idle":"2021-08-09T09:30:19.769483Z","shell.execute_reply.started":"2021-08-09T09:30:19.755577Z","shell.execute_reply":"2021-08-09T09:30:19.76842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the dataset into training and testing","metadata":{}},{"cell_type":"code","source":"# We will shuffle the dataframe and extract the feature and label\nnews = news.sample(frac=1)\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.77082Z","iopub.execute_input":"2021-08-09T09:30:19.771291Z","iopub.status.idle":"2021-08-09T09:30:19.788132Z","shell.execute_reply.started":"2021-08-09T09:30:19.771245Z","shell.execute_reply":"2021-08-09T09:30:19.787379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into training and testing\nfrom sklearn.model_selection import train_test_split\n\ntrain_sentences, val_sentences, train_labels, val_labels=train_test_split(news['text'].to_numpy(),\n                                                                            news['fake'].to_numpy(),\n                                                                            test_size=0.2,\n                                                                            random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.789369Z","iopub.execute_input":"2021-08-09T09:30:19.789769Z","iopub.status.idle":"2021-08-09T09:30:19.944404Z","shell.execute_reply.started":"2021-08-09T09:30:19.789729Z","shell.execute_reply":"2021-08-09T09:30:19.943485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_sentences),len(val_sentences),len(train_labels),len(val_labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.94585Z","iopub.execute_input":"2021-08-09T09:30:19.946427Z","iopub.status.idle":"2021-08-09T09:30:19.954276Z","shell.execute_reply.started":"2021-08-09T09:30:19.946382Z","shell.execute_reply":"2021-08-09T09:30:19.952935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the first 10 samples\ntrain_sentences[:2], train_labels[:10]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.956076Z","iopub.execute_input":"2021-08-09T09:30:19.956714Z","iopub.status.idle":"2021-08-09T09:30:19.965698Z","shell.execute_reply.started":"2021-08-09T09:30:19.956653Z","shell.execute_reply":"2021-08-09T09:30:19.964543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting text into numbers\nWhen dealing with a text problem, one of the first things you'll have to do before you cna build a model is to covert your text to numbers.\n\nThere are a few ways to do this, namely:\n\n* Tokenization -direct mapping of token (a token could be a word or a character ) to a number.\n* Embedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned).\n","metadata":{}},{"cell_type":"markdown","source":"## Text Vectorization (tokenization)","metadata":{}},{"cell_type":"code","source":"# find the average number of tokens (words) in the training tweets\nround(sum([len(i.split()) for i in train_sentences])/len(train_sentences))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:19.967358Z","iopub.execute_input":"2021-08-09T09:30:19.96779Z","iopub.status.idle":"2021-08-09T09:30:20.886998Z","shell.execute_reply.started":"2021-08-09T09:30:19.967756Z","shell.execute_reply":"2021-08-09T09:30:20.885893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup text vectorization variables\nmax_vocab_length = 10000\nmax_length = 418\n\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                   output_mode='int',\n                                   output_sequence_length=max_length)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:20.888605Z","iopub.execute_input":"2021-08-09T09:30:20.889012Z","iopub.status.idle":"2021-08-09T09:30:22.575863Z","shell.execute_reply.started":"2021-08-09T09:30:20.888969Z","shell.execute_reply":"2021-08-09T09:30:22.574929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the text vectorizer to the training text\ntext_vectorizer.adapt(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:22.577249Z","iopub.execute_input":"2021-08-09T09:30:22.577603Z","iopub.status.idle":"2021-08-09T09:30:32.565027Z","shell.execute_reply.started":"2021-08-09T09:30:22.577567Z","shell.execute_reply":"2021-08-09T09:30:32.564058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a sample sentences and tekenize it\nsample_sentence = \"Please Do Not Forget To Upvoted\"\ntext_vectorizer([sample_sentence])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:32.570125Z","iopub.execute_input":"2021-08-09T09:30:32.570469Z","iopub.status.idle":"2021-08-09T09:30:32.608482Z","shell.execute_reply.started":"2021-08-09T09:30:32.570425Z","shell.execute_reply":"2021-08-09T09:30:32.607705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose a random sentence from the training dataset and tokeize it\nimport random\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text;\\n{random_sentence}\\\n\\n\\n Vectorized Version:\")\ntext_vectorizer([random_sentence])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:48.912056Z","iopub.execute_input":"2021-08-09T09:30:48.912376Z","iopub.status.idle":"2021-08-09T09:30:48.929575Z","shell.execute_reply.started":"2021-08-09T09:30:48.912347Z","shell.execute_reply":"2021-08-09T09:30:48.928578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = text_vectorizer.get_vocabulary()\nlen(words)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:32.952801Z","iopub.status.idle":"2021-08-09T09:30:32.953377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating an Embedding using an Embedding Layer\nTo make our embedding we're going to use TensorFlow's embedding layer\n\nThe parameters we care most about for our embedding layer:\n\n* input_dim = the size of our vocabulary\n* output_dim = the size of output embedding vector, for example, a value of 100 would mean each token gets represented by a vector 100 long\n* input_length = length of the sequences being passed to be embedding layer","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\nembedding = layers.Embedding(input_dim=max_vocab_length,\n                            output_dim=128,\n                            embeddings_initializer='uniform',\n                            input_length=max_length)\nembedding","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:59.602639Z","iopub.execute_input":"2021-08-09T09:30:59.602972Z","iopub.status.idle":"2021-08-09T09:30:59.611653Z","shell.execute_reply.started":"2021-08-09T09:30:59.602944Z","shell.execute_reply":"2021-08-09T09:30:59.610807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a random sentence from the training set\nrandom_sentenc = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nEmbedd version: \")\nembedding(text_vectorizer([random_sentence]))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:30:59.613304Z","iopub.execute_input":"2021-08-09T09:30:59.613864Z","iopub.status.idle":"2021-08-09T09:30:59.659196Z","shell.execute_reply.started":"2021-08-09T09:30:59.613826Z","shell.execute_reply":"2021-08-09T09:30:59.658444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"markdown","source":"We will be using LSTM(long-short term memory) neural network.","metadata":{}},{"cell_type":"code","source":"# Create an LSTM model\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(1,),dtype='string')\n\n# Pass inputs to text_vectorizer(convert text into numbers)\nx = text_vectorizer(inputs) \n\n# Convert text_vectorizer layer into embedding layer\nx = embedding(x)\n\n# Model \nx = layers.LSTM(64)(x)\n\n\n# output\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\n# Pass inputs and outputs to our model\nmodel = tf.keras.Model(inputs, outputs, name='model_LSTM')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:36:09.122218Z","iopub.execute_input":"2021-08-09T09:36:09.122575Z","iopub.status.idle":"2021-08-09T09:36:09.362022Z","shell.execute_reply.started":"2021-08-09T09:36:09.122541Z","shell.execute_reply":"2021-08-09T09:36:09.361176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:36:13.012282Z","iopub.execute_input":"2021-08-09T09:36:13.01267Z","iopub.status.idle":"2021-08-09T09:36:13.022332Z","shell.execute_reply.started":"2021-08-09T09:36:13.012638Z","shell.execute_reply":"2021-08-09T09:36:13.021097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(loss='binary_crossentropy',\n             optimizer=tf.keras.optimizers.Adam(),\n             metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:36:18.5629Z","iopub.execute_input":"2021-08-09T09:36:18.563366Z","iopub.status.idle":"2021-08-09T09:36:18.586185Z","shell.execute_reply.started":"2021-08-09T09:36:18.563326Z","shell.execute_reply":"2021-08-09T09:36:18.585247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model\nmodel_history = model.fit(train_sentences,\n                         train_labels,\n                          epochs=5,\n                         validation_data=(val_sentences, val_labels))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:36:22.537247Z","iopub.execute_input":"2021-08-09T09:36:22.537644Z","iopub.status.idle":"2021-08-09T09:40:06.538882Z","shell.execute_reply.started":"2021-08-09T09:36:22.537609Z","shell.execute_reply":"2021-08-09T09:40:06.537961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We got 99% accuracy on valid data**","metadata":{}},{"cell_type":"code","source":"# Make predictions \nmodel_prediction = model.predict(val_sentences)\nmodel_prediction[:10]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:34:48.777141Z","iopub.execute_input":"2021-08-09T09:34:48.777509Z","iopub.status.idle":"2021-08-09T09:34:52.129524Z","shell.execute_reply.started":"2021-08-09T09:34:48.777472Z","shell.execute_reply":"2021-08-09T09:34:52.128694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert model prediction to our val_labels\nmodel_preds = tf.squeeze(tf.round(model_prediction))\nmodel_preds[:10]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:34:52.130886Z","iopub.execute_input":"2021-08-09T09:34:52.13125Z","iopub.status.idle":"2021-08-09T09:34:52.140559Z","shell.execute_reply.started":"2021-08-09T09:34:52.131212Z","shell.execute_reply":"2021-08-09T09:34:52.139531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluatinon metrics\nfrom sklearn.metrics import accuracy_score, recall_score,precision_score, f1_score\n\nprint(f\"Accuracy Score: {accuracy_score(val_labels,model_preds)}\")\nprint(f\"Recall Score : {recall_score(val_labels, model_preds)}\")\nprint(f\"Precsion Score : {precision_score(val_labels, model_preds)}\")\nprint(f\"f1 Score : {f1_score(val_labels, model_preds)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:34:52.143075Z","iopub.execute_input":"2021-08-09T09:34:52.14352Z","iopub.status.idle":"2021-08-09T09:34:52.166817Z","shell.execute_reply.started":"2021-08-09T09:34:52.143482Z","shell.execute_reply":"2021-08-09T09:34:52.165724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**References for Feature Engineering and EDA**: https://www.kaggle.com/suvofalcon/fake-real-news-tensorflow-hub-99-accuracy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}