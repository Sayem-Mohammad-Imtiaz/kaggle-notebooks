{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Human Activity Recognition (HAR)\n\nIn this project we will be using the sensor data recorded using a smartphone. The [HAR dataset](https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones) provides various measurements of the Activities of Daily Living (ADL) of 30 subjects. It consists of various sensor measurements of people, while they are performing activities like standing, walking, sitting, lying down etc. Such a dataset is a mine of information, providing insights into the movement related aspects of individuals. For example, one person's speed of walking is different from another. It stands to reason that a person's physical condition can be correlated to his/her walking speed. This is just one example, we will look into more interesting questions.\n\nWe will address the following questions.\n\n1. Can we accurately predict the activity of a person using this dataset? If so, then which is the best model?\n2. Which attributes are the most vital ones for predicting the activity of a person?\n\nFirst, let's import the required library."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see the names of the files we are going to be working with."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/human-activity-recognition-with-smartphones/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/human-activity-recognition-with-smartphones/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the dataset\n\nLet's see the columns in the training set and understand what they mean.\n\nThe description of the dataset has the following:\nFor each record in the dataset the following is provided:\n* Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n* Triaxial Angular velocity from the gyroscope.\n* Its activity label.\n* An identifier of the subject who carried out the experiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for imbalance in the labeled instances\n\nLet's see how many instances of each label there is in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.Activity.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15, 5))\nsns.countplot(x = 'Activity', \n              data = train_data, \n              #palette = \"Blues_r\",\n              palette = 'winter',\n              order = train_data['Activity'].value_counts().index\n             )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The dataset does not have much of an imbalance in the classes."},{"metadata":{},"cell_type":"markdown","source":"## Distribution of attributes\n\nIt is important that we understand the distribution of the values in all the columns of our data, but as we have noticed, there are too many attributes (561 of them!). Surely, we cannot examine all of them. So, what shall we do? \n\nWell we could just get a glimpse of the distribution by looking at the first 5 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plots distribution of 6 columns\n\ndef plot_distribution(data, col):\n    fig, axes = plt.subplots(ncols = 3, nrows = 2, figsize = (15, 8))\n    for i, ax in zip(range(6), axes.flat):\n        sns.distplot(data[cols[i]], ax = ax)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select some body acceleration attributes\ncols = train_data.columns[:6]\nplot_distribution(train_data, cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select some gravitational acceleration attributes\ncols = train_data.columns[40:47]\nplot_distribution(train_data, cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the attributes are not nearly normally distributed. We observe skew in several gravitational acceleration measurements."},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the dataset\n\nIn order to get good results, the instances belonging to different labels must be seperable. We will use t-distibuted Stocastic Neighbor Embedding (t-SNE) to visualize the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will make a copy of the train data before applying t-SNE. We will also extract the labels from the data and store it seperately."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_data = train_data.copy()\ntsne_data.drop(['Activity', 'subject'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the counts of each activity."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train_data['Activity']\nlabel_counts = labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardization is done on the data prior to applying t-SNE. This is done in order to make the data look almost normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data(data):\n    scl = StandardScaler()\n    return scl, scl.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_model, scaled_data = scale_data(tsne_data)\nscaled_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will apply t-SNE on the scaled train data."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(random_state = 0)\ntsne_transformed = tsne.fit_transform(scaled_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = plt.figure(figsize = (25, 10))\ncolors = ['darkblue', 'mediumturquoise', 'darkgray', 'darkorchid', 'darkred', 'darkgreen']\nfor i, activity in enumerate(label_counts.index):\n    mask = (labels == activity).values\n    plt.scatter(x = tsne_transformed[mask][:,0],\n                y = tsne_transformed[mask][:,1],\n                color = colors[i],\n                alpha = 0.4,\n                label = activity)\nplt.title('Visualisation using t-SNE')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Can we accurately predict the activity of a person using this dataset? If so, then which is the best model?\n### Creating prediction model\n\nWe will try the simple decision tree classifier first. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_data.drop(['Activity', 'subject'], axis = 1)\ny_train = train_data['Activity']\n\nX_test = test_data.drop(['Activity', 'subject'], axis = 1)\ny_test = test_data['Activity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclf_dt = DecisionTreeClassifier(max_depth = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_dt = clf_dt.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_dt = clf_dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_train_test_accuracy(y_train, y_train_pred, y_test, y_test_pred, title):\n    acc_train = accuracy_score(y_train, y_train_pred)\n    acc_test = accuracy_score(y_test, y_test_pred)\n    \n    print('Train accuracy = ', acc_train)\n    print('Test accuracy = ', acc_test)\n\n    ax = plt.figure()\n    plt.bar(x = 'train accuracy', height = acc_train, color='darkblue')\n    plt.bar(x = 'test accuracy', height = acc_test, color='lightblue')\n    plt.xticks(['train accuracy', 'test accuracy'])\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_train_test_accuracy(y_train, y_train_pred_dt, y_test, y_test_pred_dt, 'Decision Tree Classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By examining the above figure it can be noticed that there is some amout of overfitting going on. Bagging is well-known method for reducing variance of a model."},{"metadata":{},"cell_type":"markdown","source":"### Bagging\n\nRandom Forest Classifier is an ensemble learning technique used to reduce the variance of a base learning algorithm (in our case the Decision Tree classifier)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf = RandomForestClassifier(n_estimators = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_rf = clf_rf.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_rf = clf_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_train_test_accuracy(y_train, y_train_pred_rf, y_test, y_test_pred_rf, 'Random Forest Classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy on test set has been improved!"},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting\nNext we will try gradient boosting based tree classifier. The base learner will be a weak classifier such as a Decision Tree with maximum depth 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(max_depth = 3, n_estimators = 500, random_state = 0)\nlgbm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_lgbm = lgbm.predict(X_train)\ny_test_pred_lgbm = lgbm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_train_test_accuracy(y_train, y_train_pred_lgbm, y_test, y_test_pred_lgbm, 'LGBM Classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! the accuracy has improved significantly. Thus the best model on this dataset is LGBM classifier."},{"metadata":{},"cell_type":"markdown","source":"## 2. Which attributes are the most vital ones for predicting the activity of a person?\n\nThis kind of question is best answered by looking at the importance scores provided by the XGBMBoost classifier. In fact, all gradient boosting methods return such info."},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import plot_importance\nplot_importance(lgbm, max_num_features = 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n\nIntially I did a t-SNE plot to see the sperability of the classes, and it was found that there were distinct demarcations between the classes. In other words, the data of different classes appear to be be quite easy to seperate. Then, a decision tree classifier was used to predict the actvity of persons in the test set, and its accuracy was only 85%. Next, a random forest classifier was used for prediction, and its accuracy was 93%, which is much better than that of a simple decision tree. Finally, a gradient boosting tree model was found to produce results with accuracy up to 95%. The most important features are Acceleration measurements along the three axes and also certain gyroscope measurements.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}