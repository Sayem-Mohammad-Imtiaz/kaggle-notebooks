{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk import ngrams, word_tokenize\nimport seaborn as sns\n%matplotlib inline\nsns.set(rc={'figure.figsize': (11, 8)})\nsns.set_palette('pastel')\nnltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wallstreet_raw = pd.read_csv('/kaggle/input/reddit-rwallstreetbets/r_wallstreetbets_posts.csv')\nwallstreet_raw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset will be limited to the columns (id, title, author, created_datetime) as the rest are of little interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"wallstreet_filtered = wallstreet_raw[['id', 'title', 'author', 'created_datetime']].copy()\nwallstreet_filtered.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The first question : How has the activity been on this subreddit?\n\nIn order to answer this, we will look at the number of post it has received over the years."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(data=wallstreet_filtered.created_datetime.dt.year.value_counts().reset_index(), x = 'index', y = 'created_datetime')\nax.set(xlabel = 'Year', ylabel = 'Number of posts');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The data roots back to 2012, which could be the year of inception of this subreddit.\n\n- The first major bit of activity is in 2018, which I believe is around the time that Bitcoin was rallying.\n\n- The amount of activity in 2021 has surpassed that of 2020 even though it is just the beginning of February.\n\n- Hence, the activity certainly has been at it's peak during the last couple of months."},{"metadata":{},"cell_type":"markdown","source":"A follow up on this is to look at the activity from first time post by authors.\n\nWe will look at the number of authors who posted for the first time over the years.\n\nIf the activity is from new followers of this subreddit, we should expect a large number of first time posts being made in the recent years. "},{"metadata":{"trusted":true},"cell_type":"code","source":"each_users_datetime_of_first_post = wallstreet_filtered.groupby(['author']).created_datetime.min().reset_index()\neach_users_datetime_of_first_post.columns = ['author', 'datetime_of_first_post']\neach_users_datetime_of_first_post['year'] = each_users_datetime_of_first_post.datetime_of_first_post.dt.year\neach_users_datetime_of_first_post.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(data=each_users_datetime_of_first_post.year.value_counts().reset_index(), x = 'index', y = 'year')\nax.set(xlabel = 'Year', ylabel = 'Number of first time authors');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The above plot shows that 2021 is the year which saw the most new authors. That means the activity is from new followers rather than the existing followers.\n\n- This reddit saw an amount of activity in 2021 that is more than the combined activity of the previous years.\n\n- Considering we are only one month into 2021, this is staggering.\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Having noticed that the majority of the traffic took place in 2020 and 2021, let's dig into these to see at what point this subreddit found momentum."},{"metadata":{"trusted":true},"cell_type":"code","source":"wallstreet_filtered_2020_2021 = wallstreet_filtered[wallstreet_filtered.created_datetime.dt.year.isin([2020, 2021])].copy()\nwallstreet_filtered_2020_2021['month'] = wallstreet_filtered_2020_2021.created_datetime.dt.month\nwallstreet_filtered_2020_2021['year'] = wallstreet_filtered_2020_2021.created_datetime.dt.year\nwallstreet_filtered_2020_2021.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(data= wallstreet_filtered_2020_2021.groupby(['year', 'month']).id.count().reset_index(), x= 'month', y= 'id', hue= 'year')\nax.set(xlabel = 'month', ylabel = 'count of comments')\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear as day that the traffic to this reddit only began at the start of 2021. Things were pretty normal before this."},{"metadata":{},"cell_type":"markdown","source":"Hence, for the rest of the analysis, we will focus on the activity in 2021 alone."},{"metadata":{"trusted":true},"cell_type":"code","source":"wallstreet_filtered_2021 = wallstreet_filtered[wallstreet_filtered.created_datetime.dt.year == 2021].copy()\nwallstreet_filtered_2021['day'] = wallstreet_filtered_2021.created_datetime.dt.day\nwallstreet_filtered_2021.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(data= wallstreet_filtered_2021.groupby(['day']).id.count().reset_index(), x='day', y= 'id')\nax.set(xlabel = 'Day of January', ylabel = 'Number of comments');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Looks like it was the 27th of January that kicked off activity on this Reddit."},{"metadata":{},"cell_type":"markdown","source":"## The next question of interest: Who were the most active authors?\n\nThese authors could be the most influential of the lot."},{"metadata":{"trusted":true},"cell_type":"code","source":"wallstreet_filtered_2021.author.value_counts().reset_index().rename(columns={'index': 'Author', 'author': 'Number of posts'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n- Interesting. A large amount of comments were deleted by the moderator. Wonder why.\n\n- This hinders my quest to find the most active authors.\n\n- However, I will be ignoring the deleted comments and continue looking.\n"},{"metadata":{},"cell_type":"markdown","source":"Let's visulize the activity pattern of the top 5 authors on this reddit."},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_active_authors = wallstreet_filtered_2021[ wallstreet_filtered_2021.author.isin(wallstreet_filtered_2021.author.value_counts()[1:6].index)]\ntop_5_active_authors.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_active_authors_posts_per_day_in_January = top_5_active_authors.groupby(['day', 'author']).id.count().reset_index()\ntop_5_active_authors_posts_per_day_in_January.columns = ['day', 'author', 'count']\ntop_5_active_authors_posts_per_day_in_January","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data= top_5_active_authors_posts_per_day_in_January, x='day', y= 'count', hue='author')\nplt.legend(loc='upper left');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above bar plot, the most consistent author is CappedCrib, who has been regular at posting.\n\n- There isn't much here to help us answer if there were any influential authors/followers.\n\n- Let's look at some of their posts."},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_active_authors[top_5_active_authors.author == 'dhiral1994'].sort_values(['day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_active_authors[(top_5_active_authors.author == 'CappedCrib') & (top_5_active_authors.day >= 26)].sort_values(['day'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The final question : What were the most common phrases used by the authors.\n\nI am sure it would be \"TO THE MOON\" but would be interesting to see what the others are."},{"metadata":{"trusted":true},"cell_type":"code","source":"wallstreet_2021_comments_cleaned = wallstreet_filtered_2021.title.apply(lambda c : ''.join([x for x in str(c).lower() if x.isalnum() or x.isspace()]))\nwallstreet_2021_comments_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ngrams(text, n):\n    '''\n    INPUT\n    text - string, any text to create ngrams for\n    n    - int, the n in the ngram i.e n = 2 will create set of all possible phrases of 2 words\n\n    OUTPUT\n    list of strings, each string being a phrase of n words\n    '''\n    output_grams = []\n    grams = ngrams(word_tokenize(text), n)\n    for gram in grams:\n        try:\n            output_grams.append(' '.join(gram))\n        except:\n            continue\n    return output_grams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_phrases(phrase_series, gram_n = 3, top_n = 10):\n    '''\n    INPUT\n    phrase_series - pandas series, a series of text for which a count of all phrases of n words is require\n    gram_n        - int, default 3, the n in the ngram i.e gram_n = 2 will create set of all possible phrases of 2 words\n    top_n         - int, default 10, the number of top results to return\n\n    OUTPUT\n    a pandas dataframe, with two columns (phrase, count) where phrase is the phrase of gram_n words and top_n number of rows\n    '''\n    phrase_list = phrase_series.apply(get_ngrams, args=(gram_n,))\n    phrase_dict = {}\n    for ele in phrase_list:\n        for phrase in ele:\n            phrase_dict.update({phrase: phrase_dict.get(phrase, 0) + 1})\n    return pd.DataFrame(sorted(phrase_dict.items(), key=lambda item: item[1], reverse=True)[:top_n], columns= ['phrase', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# five_word_phrases = get_top_n_phrases(wallstreet_2021_comments_cleaned, 5)\n# four_word_phrases = get_top_n_phrases(wallstreet_2021_comments_cleaned, 4)\n# three_word_phrases = get_top_n_phrases(wallstreet_2021_comments_cleaned, 3)\ntwo_word_phrases = get_top_n_phrases(wallstreet_2021_comments_cleaned, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data = two_word_phrases, x = 'count', y = 'phrase');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data = three_word_phrases, x = 'count', y = 'phrase');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data = four_word_phrases, x = 'count', y = 'phrase');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}