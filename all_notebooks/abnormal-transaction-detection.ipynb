{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Load packages**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd # (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport numpy as np # linear algebra\nimport warnings\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nimport lightgbm as lgb\nimport gc, datetime, random\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\n#from ml import simple\n\nwarnings.simplefilter(\"ignore\")\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction = pd.read_csv('../input/anomaly-detection/creditcard.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('transaction shape is {}'.format(transaction.shape))\ntransaction = transaction.sort_values(\"Time\")\ntransaction.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize Transaction Distribution by `Amount` during `Time`**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot all transaction\nsns.lmplot( x=\"Time\", y=\"Amount\", data=transaction, fit_reg=False, hue='Class', height=8, aspect=17/8.27)\nplt.title(\"Transaction Amount during Time\")\n\n# plot only fraudulent transaction\ntransaction[(transaction['Class'] == 1)].plot(x='Time', y='Amount', style='.', figsize=(15, 3), label='Fraudulent Transaction', color = 'orange')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fraudulent transaction occur mainly on small  `Amount` which do not exceed 2000.\nIt seems no specific Time that we show a cluster of fraudes. Maybe near to Time : 40000."},{"metadata":{},"cell_type":"markdown","source":"**Recognize categorical and numerical Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Add Index to each transaction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction = transaction.reset_index()\ntransaction.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** UMBALANCED Proportion of classes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(transaction)\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nplot_tr = sns.countplot(x='Class', data=transaction)\nplot_tr.set_title(\"Fraud Transactions Distribution \\n 0: No Fraud | 1: Fraud\", fontsize=18)\nplot_tr.set_xlabel(\"Is fraud?\", fontsize=16)\nplot_tr.set_ylabel('Count', fontsize=16)\nfor p in plot_tr.patches:\n    height = p.get_height()\n    plot_tr.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=15) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary classes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction.groupby('Class').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, we have a small ratio of fraud case compared to normal transaction. Most supervised machine learning classification algorithms are sensitive to unbalance in the predictor classes, and special techniques would have to be used to account for this unbalance (Under-sampling the normal transaction, Over-sampling the frand transaction). but now we will continu we our raw data."},{"metadata":{},"cell_type":"markdown","source":"**Check for missing Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_null = transaction.isnull().sum()/len(transaction) * 100\ndata_null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good! seems to be clean."},{"metadata":{},"cell_type":"markdown","source":"**Explore the must important Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.warnings.filterwarnings('ignore')\nsns.set(rc={'figure.figsize':(15,50)})\nfor num, alpha in enumerate(list(transaction.drop(columns =['index', 'Class'], axis = 1).columns)):\n    plt.subplot(10,3,num+1)\n    yes = transaction[(transaction['Class'] == 1)][alpha]\n    no = transaction[(transaction['Class'] == 0) ][alpha]\n    plt.hist(yes[yes>0], alpha=0.75, label='Fraud', color='r')\n    plt.hist(no[no>0], alpha=0.25, label='Not Fraud', color='g')\n    plt.ylim(0, 1000)\n    plt.legend(loc='upper right')\n    plt.ylabel(\"Amount is limit to 1000 (250000)\")\n    plt.title('Histogram of values  in column ' + str(alpha) )\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can delete some non importante column to improve model. Time, V3, V6, V7, V9, V10, V12, V14, V16 seem to be not important."},{"metadata":{},"cell_type":"markdown","source":"**Split Training and Testing Data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#we are going to divide 70/30 the data into training and testing set proportion\ntrain,test=train_test_split(transaction,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize distribution of trainning and testing datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot( x=\"Time\", y=\"Amount\", data=train, fit_reg=False, hue='Class', height=8, aspect=17/8.27)\nplt.title(\"Training distribution\")\ntrain.groupby('Class').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot( x=\"Time\", y=\"Amount\", data=test, fit_reg=False, hue='Class', height=8, aspect=17/8.27)\nplt.title(\"Testing distribution\")\ntest.groupby('Class').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract xTrain\nxTrain = train.drop(columns= ['index', 'Class'])\n# Extract xTest\nxTest = test.drop(columns= ['index', 'Class'])\n# Extract yTrain\nyTrain = train[('Class')]\n# Extract y Test\nyTest = test[('Class')]\n\nxTrain.sort_values('Time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('xTrain:', xTrain.shape)\nprint('yTrain:', yTrain.shape)\nprint('xTest:', xTest.shape)\nprint('yTest:', yTest.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LGBM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def makePredictions(train, test, target, lgb_params, NFOLDS=6):\n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    X,y = train, target   \n    P = test\n\n    predictions = np.zeros(len(test))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p/NFOLDS\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    test['prediction'] = predictions\n    \n    return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_predictions(train, test, features_columns, target, lgb_params, NFOLDS=2):\n    \n    folds = GroupKFold(n_splits=NFOLDS)\n\n    X,y = train[features_columns], train[target]    \n    P,P_y = test[features_columns], test[target]  \n    split_groups =  train['Time']\n\n    test = test[['index',target]]    \n    predictions = np.zeros(len(test))\n    oof = np.zeros(len(train))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p/NFOLDS\n        \n        oof_preds = estimator.predict(vl_x)\n        oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n\n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    test['prediction'] = predictions\n    print('OOF AUC:', metrics.roc_auc_score(y, oof))\n    if LOCAL_TEST:\n        print('Holdout AUC:', metrics.roc_auc_score(test[TARGET], test['prediction']))\n    \n    return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params3 = {\n                'objective':'binary',\n                'boosting_type':'gbdt',\n                'metric':'auc',\n                'n_jobs':-1,\n                'learning_rate':0.007,\n                'num_leaves': 2**8,\n                'max_depth':-1,\n                'tree_learner':'serial',\n                'colsample_bytree': 0.85,\n                'subsample_freq':1,\n                'subsample':0.85,\n                'n_estimators':1800,\n                'max_bin':255,\n                'verbose':-1,\n                'seed': 42,\n                #'early_stopping_rounds':100,\n                'reg_alpha':0.3,\n                'reg_lamdba':0.243\n            } \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = {'objective':'binary',\n               'boosting_type':'gbdt',\n               'metric':'auc',\n               'reg_lambda': 0.4,\n               'reg_alpha': 0.30000000000000004,\n               'num_leaves': 500,\n               'min_data_in_leaf': 120,\n               'learning_rate': 0.05,\n               'feature_fraction': 0.4,\n               'bagging_fraction': 0.2,\n               'class_weight':None,\n               'colsample_bytree':1.0,\n               'importance_type':'split',\n               'max_depth':-1,\n               'min_child_samples':20,\n               'min_child_weight':0.001,\n               'min_split_gain':0.0,\n               'n_estimators':1800, \n               'n_jobs':-1,\n               'num_leaves':31,\n               'learning_rate': 0.05,\n               'pre_dispatch':'2*n_jobs',\n               'random_state':None, \n               'refit':True,\n               'return_train_score':False, \n               'scoring':'roc_auc',\n               'tree_learner':'serial',\n               'seed': 42,\n               'verbose': -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = list(train.drop(columns= ['index', 'Class']).columns)\nvariables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(test.columns)\n#test = test.drop(columns= ['index', 'Class'])\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = 'Class'\nLOCAL_TEST = True\npredictions_1 = make_predictions(train, test, variables ,TARGET, best_params, NFOLDS=6)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_2 = makePredictions(xTrain, xTest, yTrain, best_params, NFOLDS=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Mat1 = pd.DataFrame({\"Class\":yTest, \"Prediction\": predictions_1['prediction']})\nMat2 = pd.DataFrame({\"Class\":yTest, \"Prediction\": predictions_2['prediction']})\nMat1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Mat2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Marix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_conf_Mat(Matrix):\n# Creates a confusion matrix\n    cm = confusion_matrix(Matrix['Class'].astype(np.int64), Matrix['Prediction'].astype(np.int64)) \n\n# Transform to df for easier plotting\n    cm_df = pd.DataFrame(cm,\n                     index = ['Class','Prediction'], \n                     columns = ['Class','Prediction'])\n\n    plt.figure(figsize=(5.5,4))\n    sns.heatmap(cm, annot=True)\n    plt.title('CM \\nAccuracy:{0:.3f}'.format(accuracy_score(Matrix['Class'].astype(np.int64), Matrix['Prediction'].astype(np.int64))))\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\nplot_conf_Mat(Mat1)\nplot_conf_Mat(Mat2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**WHEN WE REMOVE NO IMPORTANTE VARIABLES**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_drp = train.drop(columns=[ 'V3', 'V6', 'V7', 'V9', 'V10', 'V12', 'V14', 'V16'], axis = 1)\ntest_drp = train.drop(columns=['V3', 'V6', 'V7', 'V9', 'V10', 'V12', 'V14', 'V16'], axis = 1)\n\n\n#indices = (0, 2, 5, 6, 8, 9, 11, 13, 15)\n#for i in (indices):\n#    variables.pop(i)\nvariables_drp = variables\n#variables_drp.remove('Time')  \nvariables_drp.remove('V3')  \nvariables_drp.remove('V6')\nvariables_drp.remove('V7') \nvariables_drp.remove('V9')  \nvariables_drp.remove('V10')\nvariables_drp.remove('V12')\nvariables_drp.remove('V14')\nvariables_drp.remove('V16')  \n\n\nTARGET = 'Class'\nLOCAL_TEST = True\npredictions_1 = make_predictions(train_drp, test_drp, variables_drp ,TARGET, best_params, NFOLDS=6)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}