{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary Table:</h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"profile\">2. EDA<span class=\"badge badge-primary badge-pill\">2</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"profile\">3. Univariante Analysis<span class=\"badge badge-primary badge-pill\">3</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"profile\">4. Outlier Detection<span class=\"badge badge-primary badge-pill\">4</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"profile\">5. Handling Missing Data<span class=\"badge badge-primary badge-pill\">5</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"profile\">6. Modelling<span class=\"badge badge-primary badge-pill\">6</span></a>\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b>1 - Introduction</b></font><br><a id=\"1\"></a>\n\n* This kernel will present a simple EDA over data, and in the end usign some models to predict de employeer attrintion rate","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n!pip3 install catboost\n!pip3 install xgboost \n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hackerearth-employee-attrition/Train.csv')\ndf_test = pd.read_csv('/kaggle/input/hackerearth-employee-attrition/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b>2 - EDA</b></font><br><a id=\"2\"></a>\n\n* To start understanding our data will start plot the target variable distribution","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot(df['Attrition_rate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('skew',df['Attrition_rate'].skew())\nprint('kurtosis',df['Attrition_rate'].kurtosis())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b>3 - Univariante Analysis</b></font><br><a id=\"3\"></a>\n\n* To better undestanding about data we will explore some of features presents in the data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.pie(values=df['Gender'].value_counts(), names=df['Gender'].value_counts().index, title='Gernes')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The most part of the people in data set are women","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" * Exploring some of categorical features:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=5, specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}]])\n\ndf_aux = df[['Relationship_Status', 'Hometown', 'Unit', 'Decision_skill_possess', 'Compensation_and_Benefits']]\nk = 1\nfor column in df_aux.columns: \n    fig.add_bar(y=list(df_aux[column].value_counts()), \n                            x=df_aux[column].value_counts().index, name=column, row=1, col=k)\n    k+=1\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.figure_factory as ff\nfig = make_subplots(rows=1, cols=5)\ndf_num = df[['Time_since_promotion', 'growth_rate', 'Travel_Rate', 'Post_Level', 'Education_Level']]\n\nfig1 = ff.create_distplot([df_num['Time_since_promotion']], ['Time_since_promotion'])\nfig2 = ff.create_distplot([df_num['growth_rate']], ['growth_rate'])\nfig3 =  ff.create_distplot([df_num['Travel_Rate']], ['Travel_Rate'])\nfig4 =  ff.create_distplot([df_num['Post_Level']], ['Post_Level'])\nfig5 =  ff.create_distplot([df_num['Education_Level']], ['Education_Level'])\n\nfig.add_trace(go.Histogram(fig1['data'][0], marker_color='blue'), row=1, col=1)\nfig.add_trace(go.Histogram(fig2['data'][0],marker_color='red'), row=1, col=2)\nfig.add_trace(go.Histogram(fig3['data'][0], marker_color='green'), row=1, col=3)\nfig.add_trace(go.Histogram(fig4['data'][0],marker_color='yellow'), row=1, col=4)\nfig.add_trace(go.Histogram(fig5['data'][0],marker_color='purple'), row=1, col=5)\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* post level looklike normal distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b>4 - Outlier Detection</b></font><br><a id=\"4\"></a>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 20.7,5.27\ndf_aux = df[['Relationship_Status', 'Hometown', 'Unit', 'Decision_skill_possess', 'Compensation_and_Benefits', 'Attrition_rate']]\nf, axes = plt.subplots(1, 5)\nk = 0\nfor column in df_aux.columns[:-1]:\n    g = sns.boxplot(x=column, y='Attrition_rate',\n                    data=df_aux, ax=axes[k])\n    g.set_xticklabels(labels=g.get_xticklabels(),rotation=90)\n    k +=1 \ng","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So outlier maybe is a feature of data, upper limit in all the above charts is a value next to 0.5 at the moment nothing will be done about handling this data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b>5 - Handling missing data</b></font><br><a id=\"5\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(x=df.isna().sum().index, y=df.isna().sum())\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Var2 and Var4 are the features with the most missing values, and in the dataset description does not have detailed information on what these variables can be\n\n* Age has a considerable value of missing values. Time of service too\n\n* to better undestanding the relationship over each feature we will plot the correlation matrix \n\n* First to impute the data we will disregard the samples that contains missing data in time service feature","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax = sns.heatmap(df.corr(), annot=True, fmt=\".4f\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As matrix showed, there are high correlation among time_of_service and age, so maybe apply some interpolation can be help to impute the missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom scipy.interpolate import interp1d\ndf_age = df[~df['Time_of_service'].isna()]\ndf_age_ = df_age[~df_age['Age'].isna()]\ndf_age_ = df_age_.sort_values('Time_of_service',  ascending=False)\ninterpolate_poly = interp1d(kind='linear', x=list(df_age_['Time_of_service']), y=list(df_age_['Age']))\nages =[]\nfor age, time_service in zip(df_age['Age'], df_age['Time_of_service']):\n    if math.isnan(float(age)):\n        age_interpolated = interpolate_poly(time_service)\n        ages.append(age_interpolated)\n    else:\n        ages.append(int(age))\ndf_age['new_age'] = ages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_age = df_age.sort_values('Time_of_service', ascending=False)\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(df_age['Time_of_service']), \n                         y=list(df_age['Age']), mode='markers', name='Original Age'))\n\ndf_age2 = df_age[df_age['Age'].isna()]\nfig.add_trace(go.Scatter(x=list(df_age2['Time_of_service']), \n                         y=list(df_age2['new_age']), mode='markers', marker_color='red', name='Interpolated Age'))\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In chart above is possible to see handling over missing ages, linear interpolation apparently keep the structure of data not adding no value that is clearly a bias\n\n* So know whats means the features var2 and var4 i chose not to insert in the models for now\n\n* Pay Scale and Work Life balance will replace by mode","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pay = []\nwork = []\npp = df_age['Pay_Scale'].mode()\nww = df_age['Work_Life_balance'].mode()\nfor p, w in zip(df_age['Pay_Scale'], df_age['Work_Life_balance']):\n    if math.isnan(float(p)):\n        pay.append(pp)\n    else:\n        pay.append(p)\n    if math.isnan(float(w)):\n        work.append(ww)\n    else:\n        work.append(w)\n\ndf_age['Pay_Scale'] = pay\ndf_age['Work_Life_balance'] = work","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b>6 - Modelling</b></font><br><a id=\"6\"></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df_age.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rel_status = pd.get_dummies(df_age['Relationship_Status'])\nhometown = pd.get_dummies(df_age['Hometown'])\nunit = pd.get_dummies(df_age['Unit'])\ndecision = pd.get_dummies(df_age['Decision_skill_possess'])\ncompenssion = pd.get_dummies(df_age['Compensation_and_Benefits'])\nto_work = df_age[['Education_Level', 'Time_of_service', 'Time_since_promotion', 'growth_rate', 'Travel_Rate', 'Post_Level', 'Pay_Scale', 'Work_Life_balance', 'VAR1', 'VAR3', 'VAR5', 'VAR6','VAR7', 'Attrition_rate', 'new_age']]\ndf_to_modelling = pd.concat([to_work, compenssion, decision, unit, hometown, rel_status], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_to_modelling['Attrition_rate']\ndf_to_modelling = df_to_modelling.drop(['Attrition_rate'], axis=1)\ndf_to_modelling['Pay_Scale'] = df_to_modelling['Pay_Scale'].astype(float)\ndf_to_modelling['new_age'] = df_to_modelling['new_age'].astype(int)\ndf_to_modelling['Work_Life_balance'] = df_to_modelling['Work_Life_balance'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_predict(pred, true):\n    indexs = []\n    for i in range(len(pred)):\n        indexs.append(i)\n        \n\n    fig = go.Figure()\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=pred,\n        name=\"Predict\"\n    ))\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=true,\n        name=\"Test\"\n    ))\n\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df_to_modelling, y, random_state=42\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"param_random_tree = {\"max_depth\": [None],\n              \"max_features\": [10,15, 20, 30, 43],\n              \"min_samples_split\": [2, 3, 10,15],\n              \"min_samples_leaf\": [1, 3, 10,15],\n              \"n_estimators\" :[50,100,200,300,500]}\n\nrandom = RandomForestRegressor(random_state=42)\nclf = GridSearchCV(random, param_random_tree, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf.fit(X_train, y_train)\nprint(clf.best_estimator_)\nprint(clf.best_score_)\n# (max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {}\nrandom = RandomForestRegressor(max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)\nmodel = random.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['RF'] = score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Xgboost","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"import xgboost\nxgboost_params = {'max_features': [10,15, 20, 30],\n                  'n_estimators' :[25,50,100],\n                   'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n                  'gamma':[0.5, 0.1, 1, 10],\n                  'max_depth':[5, 10, 15]}\n\nxgb = xgboost.XGBRegressor(random_state=42)\nclf_xgb = GridSearchCV(xgb, xgboost_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_xgb.fit(df_to_modelling, y)\nprint(clf_xgb.best_estimator_)\nprint(clf_xgb.best_score_)\n\"\"\"\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=1, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=5, max_features=10,\n             min_child_weight=1, missing=nan, monotone_constraints='()',\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\"\"\"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgb = xgboost.XGBRegressor(gamma=1, random_state=42, max_depth=5, max_features=10,learning_rate=0.1, n_estimators=100)\nmodel = xgb.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['XGB'] = score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[10,20, 50, 100],\n                 'max_depth':[4, 6, 10, 15, 20, 50]}\ngbm = lgb.LGBMRegressor(random_state = 42)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_gbm.fit(df_to_modelling, y)\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)\n# (learning_rate=0.001, max_depth=6, n_estimators=50, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.LGBMRegressor(random_state = 42, learning_rate=0.001, max_depth=6, n_estimators=50)\nmodel = gbm.fit(df_to_modelling, y)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['LGBM'] = score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nadam_boosting_params = {'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1,1],\n                        'n_estimators':[10,20, 50, 100]}\nada = AdaBoostRegressor(random_state=42)\nclf_ada = GridSearchCV(ada, adam_boosting_params, cv=5,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_ada.fit(df_to_modelling, y)\nprint(clf_ada.best_estimator_)\nprint(clf_ada.best_score_)\n# (learning_rate=0.0001, n_estimators=100, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostRegressor(random_state=42, learning_rate=0.0001, n_estimators=100)\nmodel = ada.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['ADA'] = score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LinearSVR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVR\n\nsvr_params = {'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]}\nsvr = LinearSVR(random_state=42)\nclf_svr = GridSearchCV(svr, svr_params, cv=5, scoring='neg_mean_squared_error', n_jobs=4, verbose=1)\nclf_svr.fit(df_to_modelling, y)\nprint(clf_svr.best_estimator_)\nprint(clf_svr.best_score_)\n# (C=0.001, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lvr = LinearSVR(C=0.001, random_state=42)\nmodel = svr.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['SVR'] = score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators  =  [\n    ('rf', RandomForestRegressor(max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)),\n    ('svr', LinearSVR(C=0.001, random_state=42)),\n    ('ada', AdaBoostRegressor(random_state=42, learning_rate=0.0001, n_estimators=100)),\n    ('lgb', lgb.LGBMRegressor(random_state = 42, learning_rate=0.001, max_depth=6, n_estimators=50)),\n    \n]\nclf = StackingRegressor(\n    estimators=estimators, final_estimator=xgboost.XGBRegressor(gamma=1, random_state=42, max_depth=5, max_features=10,learning_rate=0.1, n_estimators=100)\n)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['STACK'] = score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nestimators  =  [\n    ('rf', RandomForestRegressor(max_features=10, min_samples_leaf=15, n_estimators=500, random_state=42)),\n    ('svr', LinearSVR(C=0.001, random_state=42)),\n    ('ada', AdaBoostRegressor(random_state=42, learning_rate=0.0001, n_estimators=100)),\n    ('lgb', lgb.LGBMRegressor(random_state = 42, learning_rate=0.001, max_depth=6, n_estimators=50)),\n    ('xgb', xgboost.XGBRegressor(gamma=1, random_state=42, max_depth=5, max_features=10,learning_rate=0.1, n_estimators=100))\n]\nclf = VotingRegressor(\n    estimators=estimators\n)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, pred)))\nscore = 100* max(0, 1-mean_squared_error(y_test, pred))\nprint(score)\nscores['VOLTING'] = score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame([])\nresult['model'] = list(scores.keys())\nresult['score'] = list(scores.values())\nresult = result.sort_values(['score'], ascending=False)\nresult.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Give your feedback to improve this kernel :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}