{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5338b645-6f2b-eda2-6ab5-c6920b77bc34"},"source":"Predicting IMDB rating"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50fbf513-ea2c-2598-f33c-3ba7061dbd4b"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"../input/movie_metadata.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"94d86168-cd58-54ed-fb39-90005ae2a792"},"source":"First step, I want to clean the \"nan\" in dataframe. One way is to fill them, one way is to drop. Filling them with 0 or some number will most likely make those samples become outlier, so I'll drop all \"nan\"."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"933435df-3393-c236-a402-7f2b6586eae1"},"outputs":[],"source":"print(data.shape)\nclean_data = data.dropna(axis = 0)\nprint(clean_data.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"89a342fa-d37e-9a68-25f3-382572410526"},"source":"About 1/4 of the samples are dropped, but 3756 is still large enough for training and testing. Next step, I want to check the data distribution and start with simple linear regression model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4924e7d3-1b7d-069d-75e4-b3ddc30a2d32"},"outputs":[],"source":"plt.hist(clean_data['imdb_score'], bins=25)\nplt.title(\"Distribution of IMDB score\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33b50b35-8dc4-46c0-e750-77344d1e7061"},"outputs":[],"source":"x_list = ['movie_facebook_likes','director_facebook_likes','cast_total_facebook_likes',\n          'actor_1_facebook_likes','actor_2_facebook_likes','actor_3_facebook_likes','duration',\n          'num_critic_for_reviews','num_voted_users','num_user_for_reviews','budget','gross']\n\nplt.figure(figsize=(7,10))\nfor i in range(len(x_list)):\n    plt.subplot(6,2,i+1)\n    plt.title(x_list[i])\n    plt.hist(clean_data[x_list[i]],bins=50)\n    plt.grid(True)\nplt.tight_layout()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2cb37721-9e6f-67b8-17cb-1693f6185e40"},"source":"IMDB score looks normal, but some independent variables are highly skewed. There might be missing data in them, so I'll count the zeros in each independent variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e49fa92-999e-e1de-04e9-878c3e3f0307"},"outputs":[],"source":"count = pd.DataFrame({'zero count':[0]*len(x_list)},index = x_list)\nfor element in x_list:\n    count.ix[element,'zero count'] = sum(np.array(clean_data[element])==0)\nprint(count)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d1693b16-4169-9bd2-22ea-c8e15e962bcc"},"source":"Because of too many 0 in \"director_facebook_likes\" and \"movie_facebook_likes\", I'm not going to use them as independent variables. For the remaining variables, I'll separate them into a test set and training set, then check for multicollinearity."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3c5d04b-d36a-b56e-4068-3edac20ddb1d"},"outputs":[],"source":"if \"director_facebook_likes\" in x_list: x_list.remove(\"director_facebook_likes\")\nif \"movie_facebook_likes\" in x_list: x_list.remove(\"movie_facebook_likes\")\n    \nfrom sklearn.cross_validation import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(clean_data.ix[:,x_list], clean_data['imdb_score'], \n                                                    test_size=0.25, random_state=0)\n\nx_corr = np.corrcoef(x_train,rowvar = 0)\neigvl, eigvt = np.linalg.eig(x_corr)\nprint(eigvl)\n\nplt.imshow(x_corr, interpolation='nearest', cmap=plt.cm.Blues, extent=(0,10,0,10))\nplt.colorbar()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"77181b21-1337-cad9-afcb-c4cb53c928b6"},"source":"There's a very tiny eigenvalue, which can represent multicollinearity among independent variables. Also from the correlation plot, element 0 and element 1 are highly correlated, which might be the source of multicollinearity."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42c2542b-c9fb-5656-7a27-c35b353e32b4"},"outputs":[],"source":"\nmin_eigen = pd.DataFrame({'min eigen':[0]*len(x_list)},index = x_list)\nfor element in x_list:\n    x_temp = x_train.drop(element,1)\n    x_corr = np.corrcoef(x_temp,rowvar = 0)\n    eigvl, eigvt = np.linalg.eig(x_corr)\n    min_eigen.ix[element,'min eigen'] = min(eigvl)\nprint(min_eigen)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3668005a-4c37-0c02-48fc-6c1240018e1d"},"source":"As we expected, by dropping \"cast_total_facebook_likes\" or \"actor_1_facebook_likes\", eigen value significantly increased. One solution is to do PCA, another is to drop \"cast_total_facebook_likes\". We'll start with PCA."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6534f45-6bc0-8143-0ae4-06ba78af77a0"},"outputs":[],"source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 10)\npca.fit(x_train)\nprint(np.cumsum(pca.explained_variance_ratio_))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5fb4379a-eabd-c14b-91b2-594465685e66"},"source":"Two principal components are sufficient to explain most of variability in the training data, so we'll keep 2. And now let's start with linear regression!!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1050da1c-cbd5-2f3c-2603-5b068b3c594e"},"outputs":[],"source":"pca = PCA(n_components = 2)\npca.fit(x_train)\n\nx_train_new = pca.transform(x_train)\n\nfrom sklearn import linear_model\nimport pylab\nimport scipy.stats as stats\n\ndef plot_model(x,y,model):\n    print(\"R^2: %f\" % mod.score(x,y))\n    \n    y_fitted = model.predict(x)\n    residual = y - y_fitted\n\n    plt.figure(figsize=(7,5))\n\n    plt.subplot(221)\n    plt.scatter(y_fitted, y)\n    plt.title(\"fitted value vs actual value\")\n    plt.xlabel(\"fitted value\")\n    plt.ylabel(\"actual value\")\n\n    plt.subplot(222)\n    plt.hist(residual, bins=50)\n    plt.title(\"residual histogram\")\n\n\n    plt.subplot(223)\n    stats.probplot(residual, dist=\"norm\", plot=pylab)\n\n\n    plt.subplot(224)\n    plt.scatter(y_fitted,residual)\n    plt.title(\"fitted value vs residual\")\n    plt.xlabel(\"fitted value\")\n    plt.ylabel(\"residual\")\n\n    plt.tight_layout()\n    plt.show()\n    \n    \nmod = linear_model.LinearRegression()\nmod.fit(x_train_new, y_train)\nplot_model(x_train_new,y_train,mod)\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"51bb176b-c4a7-5bc1-d29c-48931cef9b63"},"source":"The residual histogram and qqplot suggests that residual is approximate normal with some left skew. R square is very low, and fitted value vs residual does not look random. Residual should not be predictable but we do see some pattern in the plot. So let's try dropping the element instead of using PCA."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78ecd20a-02e9-3358-549e-a746597a625a"},"outputs":[],"source":"if \"cast_total_facebook_likes\" in x_list: x_list.remove(\"cast_total_facebook_likes\")\n\nx_train, x_test, y_train, y_test = train_test_split(clean_data.ix[:,x_list], clean_data['imdb_score'], \n                                                    test_size=0.25, random_state=0)\n\nmod = linear_model.LinearRegression()\nmod.fit(x_train, y_train)\n\nplot_model(x_train,y_train,mod)"},{"cell_type":"markdown","metadata":{"_cell_guid":"978b7564-5ce2-c8dd-8902-e76832ce2f64"},"source":"There's no improvement in residual after we drop \"cast_total_facebook_likes\". One solution is to add more independent variables, such as some categorical variables. The other solution is to try some other models. Let's start with categorical variables.\n\nTwo sets of categorical variables can be added, one is \"key words\", one is \"genres\". However we need to clean the data first."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e916aa1-a514-3de1-6db9-add2fb68c65f"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef token(text):\n    return(text.split(\"|\"))\n\ncv_kw=CountVectorizer(max_features=50,tokenizer=token )\nkeywords = cv_kw.fit_transform(clean_data[\"plot_keywords\"])\nkeywords_list = [\"kw_\" + i for i in cv_kw.get_feature_names()]\n\ncv_ge=CountVectorizer(tokenizer=token )\ngenres = cv_ge.fit_transform(clean_data[\"genres\"])\ngenres_list = [\"genres_\"+ i for i in cv_ge.get_feature_names()]\n\nnew_clean_data = np.hstack([clean_data.ix[:,x_list],keywords.todense(),genres.todense()])\nnew_coeff_list = x_list+keywords_list+genres_list\n\nx_train, x_test, y_train, y_test = train_test_split(new_clean_data, clean_data['imdb_score'], \n                                                    test_size=0.25, random_state=0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f74e6013-2630-3149-1d88-bd7221ca06de"},"source":"Linear Regression Again."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9ee91c1-40ab-be82-0d16-cd94a02defd6"},"outputs":[],"source":"mod = linear_model.LinearRegression()\nmod.fit(x_train, y_train)\nplot_model(x_train,y_train,mod)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f20a0894-3db3-0bc2-b589-a98adcd4ebdc"},"outputs":[],"source":"y_test_fitted = mod.predict(x_test)\nplt.scatter(y_test_fitted, y_test)\nplt.title(\"predicted value vs actual value\")\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"actual value\")\nplt.show()\n\nprint(\"Score: %f\" % mod.score(x_test,y_test))\nprint(\"SSE: %f\" % sum((y_test_fitted-y_test)**2))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ff26e53-d298-6f99-a768-2823257f2bc1"},"source":"By testing the model in out sample, score is consistent, and we can see a trend in plot of predicted value vs actual value. Also, residual qq plot is more normal like than before. However, to prevent overfitting, I'll use Lasso to eliminate some independent variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7af49175-fb27-7f76-5ecc-a971c3c75dda"},"outputs":[],"source":"model_aic = linear_model.LassoLarsIC(criterion='aic')\nmodel_aic.fit(x_train, y_train)\n\ndef plot_ic_criterion(model, name, color):\n    alpha_ = model.alpha_\n    alphas_ = model.alphas_\n    criterion_ = model.criterion_\n    plt.plot(-np.log10(alphas_), criterion_, '--', color=color,\n             linewidth=3, label='%s criterion' % name)\n    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,\n                label='alpha: %s estimate' % name)\n    plt.xlabel('-log(alpha)')\n    plt.ylabel('criterion')\n    plt.show()\n\nplot_ic_criterion(model_aic, 'AIC', 'b')\nprint(model_aic.alpha_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"916ef38a-0090-ce53-bf4d-82fce3743dfc"},"source":"Based on AIC, our optimal alpha is 0.000236"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1ab27a7-74b1-b8c9-4346-07f8311ceab9"},"outputs":[],"source":"mod = linear_model.Lasso(alpha = model_aic.alpha_)\nmod.fit(x_train, y_train)\nplot_model(x_train,y_train,mod)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be634ef2-f318-a57e-29bd-532451708588"},"outputs":[],"source":"y_test_fitted = mod.predict(x_test)\n\nplt.scatter(y_test_fitted, y_test)\nplt.title(\"predicted value vs actual value\")\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"actual value\")\nplt.show()\n\nprint(\"Score: %f\" % mod.score(x_test,y_test))\nprint(\"SSE: %f\" % sum((y_test_fitted-y_test)**2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"148ebcf3-c860-bae8-0353-0758c6f32c95"},"outputs":[],"source":"coeff_value = pd.DataFrame(list(mod.coef_),index = new_coeff_list)\nprint(sum(coeff_value[0]==0))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1d35bb5d-3954-a4e2-336e-527ee9a218cc"},"source":"Lasso has eliminated 7 independent variables which does not have much explanatory power in predicting. And our testing result has improved by a little. (so little......)\n\nNow, instead of predicting the exact rating, I want to see if we could classify the movie into top 33% or bottom 33%, which means, is this movie good or bad?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ed41b5d-fd5a-cfe1-c629-1fd0684e1bbb"},"outputs":[],"source":"imdb_score = np.array(clean_data['imdb_score'])\npercent25 = np.percentile(imdb_score,33)\npercent75 = np.percentile(imdb_score,67)\n\nclean_list = (imdb_score>percent75) + (imdb_score<percent25)\nclassifier_clean_data = new_clean_data[clean_list]\nclassifier_coeff_list = new_coeff_list\n\nimdb_level = list(clean_data['imdb_score'][clean_list]>percent75)\nimdb_level = [int(i) for i in imdb_level]\n\nx_train, x_test, y_train, y_test = train_test_split(classifier_clean_data, imdb_level, \n                                                    test_size=0.25, random_state=0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"63eab75e-5e74-d0f0-da54-5c4e0a49b872"},"source":"I first start with decision tree, and will use auc and confusion matrix in out-sample to test the predicting power of this model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d073b9fc-6dad-8c43-ab24-14ae43c80e23"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis],2)\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a4a10059-c639-7e5a-8982-11de210a835d"},"source":"In order to maximize the depth of decision tree, I use cross validation and compare the average of scores. The depth level with highest score will be the optimal choice."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85fce6b7-db1d-11be-441d-2cb8ddb6d8c5"},"outputs":[],"source":"from sklearn.model_selection import cross_val_score\n\navg_score_list = []\nfor i in range(1,20):\n    mod = DecisionTreeClassifier(max_depth = i)\n    scores = cross_val_score(mod, x_train, y_train, cv=20)\n    avg_score_list.append(np.mean(scores))\n    \nplt.plot(range(1,20),avg_score_list,'--',linewidth=3)\nplt.axvline(avg_score_list.index(max(avg_score_list))+1, linewidth=3)\nplt.show()\n\nprint(\"max score reached with depth %d\" % (avg_score_list.index(max(avg_score_list))+1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23bdd616-a95a-5c61-da9c-e373c3e39c80"},"outputs":[],"source":"def plot_test(x,y,model):\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y, [i[1] for i in model.predict_proba(x)])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.plot(false_positive_rate, true_positive_rate, \"b\", label='AUC %0.2f' % (roc_auc))\n    plt.title(\"AUC Curve\")\n    plt.show()\n\n    auc_score = roc_auc_score(y, [i[1] for i in mod.predict_proba(x)])\n    cm = confusion_matrix(y,mod.predict(x))\n    plot_confusion_matrix(cm,classes = [\"bad movie\",\"good movie\"],normalize=False)\n    print(\"AUC Score: %f\" % auc_score)\n    print(\"Accuracy: %f\" % (sum(mod.predict(x) == y)/float(len(y))))\n\n\nmod = DecisionTreeClassifier(max_depth = 7)\nmod.fit(x_train,y_train)\nplot_test(x_test,y_test,mod)"},{"cell_type":"markdown","metadata":{"_cell_guid":"04d70869-f684-4fbd-3648-06ada690252e"},"source":"Random Forest, using CV for parameter selection."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"459128cc-7319-8e7a-7bbe-d16508cdb468"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\n\navg_score_matrix = []\nfor i in range(1,16):\n    temp_score = []\n    for j in range(1,11):\n        mod = RandomForestClassifier(n_estimators=j*10,max_depth = i)\n        scores = cross_val_score(mod, x_train, y_train, cv=10)\n        temp_score.append(np.mean(scores))\n    avg_score_matrix.append(temp_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ae9c459-543a-a5d8-00f9-2e4ed386c5b0"},"outputs":[],"source":"plt.imshow(np.matrix(avg_score_matrix), interpolation='nearest', cmap=plt.cm.Blues, aspect='auto', extent=(0,100,15,0))\nplt.ylabel(\"depth\")\nplt.xlabel(\"tree number\")\nplt.colorbar()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12516dd8-c09e-ad47-a757-3bf76d41b155"},"outputs":[],"source":"score_matrix = np.matrix(avg_score_matrix)\ni,j = np.unravel_index(score_matrix.argmax(), score_matrix.shape)\nprint(i,j)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a009c98c-9a8e-e749-e2dd-47ccb1cf4f85"},"source":"optimal tree number and max depth is approximate 60 and 15"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c50cfa11-978d-752f-484e-5cee393fe950"},"outputs":[],"source":"mod = RandomForestClassifier(n_estimators=60,max_depth = 15)\nmod.fit(x_train,y_train)\nplot_test(x_test,y_test,mod)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f29ddb7-ce60-e80e-559d-5991124d1584"},"source":"Boosting tree, using CV for parameter selection."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0078769b-ab79-f6e7-7faf-dc7a5f66f2b3"},"outputs":[],"source":"from sklearn.ensemble import GradientBoostingClassifier\n\navg_score_matrix = []\nfor i in range(1,6):\n    temp_score = []\n    for j in range(1,11):\n        mod = GradientBoostingClassifier(n_estimators=j*20, learning_rate=1.0, max_depth = i, random_state=0)\n        scores = cross_val_score(mod, x_train, y_train, cv=10)\n        temp_score.append(np.mean(scores))\n    avg_score_matrix.append(temp_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"735aa631-5870-f0cf-648c-a255fd84df82"},"outputs":[],"source":"plt.imshow(np.matrix(avg_score_matrix), interpolation='nearest', cmap=plt.cm.Blues, aspect='auto', extent=(0,200,5,0))\nplt.ylabel(\"depth\")\nplt.xlabel(\"tree number\")\nplt.colorbar()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8cab26a-2bcf-3e1c-9fdd-6e538d937c89"},"outputs":[],"source":"score_matrix = np.matrix(avg_score_matrix)\ni,j = np.unravel_index(score_matrix.argmax(), score_matrix.shape)\nprint(i,j)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18afb51c-2b97-9a66-c566-df9b3cfae87f"},"outputs":[],"source":"mod = GradientBoostingClassifier(n_estimators=80, learning_rate=1.0, max_depth=1, random_state=0)\nmod.fit(x_train,y_train)\nplot_test(x_test,y_test,mod)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06c4c61e-dcb0-8964-7e88-90f0f0f1a4ef"},"outputs":[],"source":"from sklearn.ensemble import AdaBoostClassifier\n\navg_score_matrix = []\nfor i in range(1,2):\n    temp_score = []\n    for j in range(1,11):\n        mod = AdaBoostClassifier(n_estimators=j*20)\n        scores = cross_val_score(mod, x_train, y_train, cv=10)\n        temp_score.append(np.mean(scores))\n    avg_score_matrix.append(temp_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67adb35e-feea-5509-0c72-f8f96fde3c1e"},"outputs":[],"source":"avg_score_list = []\nfor i in range(1,11):\n    mod = AdaBoostClassifier(n_estimators=i*20)\n    scores = cross_val_score(mod, x_train, y_train, cv=10)\n    avg_score_list.append(np.mean(scores))\n    \nplt.plot(range(1,11),avg_score_list,'--',linewidth=3)\nplt.axvline(avg_score_list.index(max(avg_score_list))+1, linewidth=3)\nplt.show()\n\nprint(\"max score reached with depth %d\" % (avg_score_list.index(max(avg_score_list))+1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0222df4e-e154-f256-f2c2-354232466ec5"},"outputs":[],"source":"mod = AdaBoostClassifier(n_estimators=120)\nmod.fit(x_train,y_train)\nplot_test(x_test,y_test,mod)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1d0604de-79b7-73b0-525e-9a7a56605ab3"},"source":"I noticed that when I'm cleaning data, I usually use the full data set. An ideal process is to use only training data to keep myself unbiased. The only reason I'm doing in this way is: I'm SOOOOOOOOO LAZY!!!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}