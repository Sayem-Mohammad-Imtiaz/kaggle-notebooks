{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"_is_fork":false,"language_info":{"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python"},"_change_revision":0},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_cell_guid":"38c0c58e-4eff-0a10-e433-b497813e0d98","collapsed":true,"_uuid":"8ddff52a36e7d94f27ebafeb1f8028260229f71a"},"source":"#Importing packages\nimport pandas as pd\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport gensim\nfrom gensim.corpora import Dictionary\nfrom gensim.models import ldamodel\nfrom gensim import corpora\n\nimport numpy\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)  # To ignore all warnings that arise here to enhance clarity","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2ac9d2c4-b8c8-43d9-a794-7a61f620cf77","collapsed":true,"_uuid":"7a0f126a1e50f769c621353d6e45ba8656e232f2"},"source":"#Reading data\ndata=pd.read_csv(\"fake.csv\")\ndata.head()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"#Checking to see the labels\ndata['type'].unique()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"#Checking the languages\ndata['language'].unique()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"#Filtering the english language news and taking only text column\nEnNews=data[data['language']=='english']\ntexts=EnNews['text']\ntest=texts.tolist()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"63c722a3-abe1-459e-dd81-121df2e8526a","collapsed":true,"_uuid":"322061f90f35bb7f3207e96f5df28101016c6dff"},"source":"#Checking 2 first rows of data\ntest[:2]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"texts=[]\nfor text in test:\n    text=str(text)\n    texts.append(text)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"documents = [re.sub(\"[^a-zA-Z]+\", \" \", text) for text in texts]\ntexts = [[word for word in text.lower().split() ] for text in documents]\n# stemming words: having --> have; friends --> friend\nlmtzr = WordNetLemmatizer()\ntexts = [[lmtzr.lemmatize(word) for word in text ] for text in texts]\n# tokenize\n# remove common words \nstoplist = stopwords.words('english')\ntexts = [[word for word in text if word not in stoplist] for text in texts]\n#remove short words\ntexts = [[ word for word in tokens if len(word) >= 3 ] for tokens in texts]\nextra_stopwords = ['will', 'need', 'think', 'well','going', 'can', 'know', 'com', 'get','make','www','http', 'want',\n                'like','say','got','said','something','now', 'news','back','want', \n                'many','along','things','day','also','first', 'great', 'take', 'good', 'much', 'would', 'thing',\n                'talk', 'talking', 'thank', 'does', 'give']\nextra_stoplist = extra_stopwords\ntexts = [[word for word in text if word not in extra_stoplist] for text in texts]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"# this is text processing required for topic modeling with Gensim\ndictionary = Dictionary(texts)\n\n## Remove rare and common tokens.\n# ignore words that appear in less than 5 documents or more than 80% documents (remove too frequent & infrequent words) - an optional step\ndictionary.filter_extremes(no_below=2, no_above=0.4) \ndictionary.save('fakedata.dict')  # store the dictionary, for future reference\n\n# convert words to vetors or integers\ncorpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize('fakedata.mm', corpus)  # store to disk, for later use","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"numpy.random.seed(1) # setting random seed to get the same results each time. \nmodel = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=35, passes=20)\n# Result of the model\nmodel.show_topics()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"#Finding top 5 topics in terms of coherence\nnum_topics = 35\ntop_topics = model.top_topics(corpus, topn=5)\n# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\navg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\n\nfrom pprint import pprint\npprint(top_topics)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"#LSI model\nfrom gensim.models import lsimodel\nLSImodel = lsimodel.LsiModel(corpus, id2word=dictionary, num_topics=35)\nLSImodel.show_topics()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"# NMF model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF\ntfidf_vectorizer = TfidfVectorizer(max_df=0.40, min_df=2, stop_words='english')\ntf = tfidf_vectorizer.fit_transform(documents)\nnmf = NMF(n_components=35, random_state=1, alpha=.1, l1_ratio=.5).fit(tf)\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, 5)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"3dbb0539-e7f5-acd3-7915-5ee708bf68a4","collapsed":true,"_uuid":"224d0331c5337ba32e763d94a4c2547c0c7e080c"},"source":"### Between methods, LSI is the fastest one! Since these methods are unsupervised, there is no such thing like accuracy! ","cell_type":"markdown"}]}