{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"''' 1. Importing Libraries'''\nimport numpy as np \nimport pandas as pd \nimport os\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.metrics import categorical_accuracy\nfrom keras.models import model_from_json\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import *\nfrom keras.layers.normalization import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"''' 2. get the data'''\nprint(os.listdir('../input/fer2013'))\nprint(os.listdir('/kaggle/working'))\nfilname = '../input/fer2013/fer2013.csv'\nlabel_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\nnames=['emotion','pixels','usage']\ndf=pd.read_csv('../input/fer2013/fer2013.csv',names=names, na_filter=False)\nim=df['pixels']\ndf.head(10)\n\ndef getData(filname):\n    # images are 48x48\n    # N = 10000\n    Y = []\n    X = []\n    first = True\n    for line in open(filname):\n        if first:\n            first = False\n        else:\n            row = line.split(',')\n            Y.append(int(row[0]))\n            X.append([int(p) for p in row[1].split()])\n\n    X, Y = np.array(X) / 255.0, np.array(Y)\n    return X, Y\n\nX, Y = getData(filname)\nnum_class = len(set(Y))\nprint(num_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' 3.  Keras with tensorflow backend'''\nN, D = X.shape\nprint(X.shape)\nX = X.reshape(N, 48, 48, 1)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\ny_train = (np.arange(num_class) == y_train[:, None]).astype(np.float32)\ny_test = (np.arange(num_class) == y_test[:, None]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' 4. Defining the custom model'''\nfrom keras import initializers\n\ndef my_model():\n    model = Sequential()\n    input_shape = (48,48,1)\n    model.add(Conv2D(64, (3, 3), input_shape=input_shape,activation='relu', padding='same',\n                     kernel_initializer=keras.initializers.he_uniform(seed=None)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',\n                     kernel_initializer=keras.initializers.he_uniform(seed=None)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',\n                     kernel_initializer=keras.initializers.he_uniform(seed=None)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(7))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n   \n    return model\n\nmodel=my_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ''' 5. Fit the model to the data'''\n    \npath_model='/kaggle/working/custom_model_filter.h5'\nK.tensorflow_backend.clear_session() # destroys the current graph and builds a new one\nmodel=my_model() \nK.set_value(model.optimizer.lr,1e-3)\n\ndatagen = ImageDataGenerator(\n        # set input mean to 0 over the dataset\n        featurewise_center=False,\n        # set each sample mean to 0\n        samplewise_center=False,\n        # divide inputs by std of dataset\n        featurewise_std_normalization=False,\n        # divide each input by its std\n        samplewise_std_normalization=False,\n        # apply ZCA whitening\n        zca_whitening=False,\n        # epsilon for ZCA whitening\n        zca_epsilon=1e-06,\n        # randomly rotate images in the range (deg 0 to 180)\n        rotation_range=0,\n        # randomly shift images horizontally\n        width_shift_range=0.1,\n        # randomly shift images vertically\n        height_shift_range=0.1,\n        # set range for random shear\n        shear_range=0.,\n        # set range for random zoom\n        zoom_range=0.,\n        # set range for random channel shifts\n        channel_shift_range=0.,\n        # set mode for filling points outside the input boundaries\n        fill_mode='nearest',\n        # value used for fill_mode = \"constant\"\n        cval=0.,\n        # randomly flip images\n        horizontal_flip=True,\n        # randomly flip images\n        vertical_flip=False,\n        # set rescaling factor (applied before any other transformation)\n        rescale=None,\n        # set function that will be applied on each input\n        preprocessing_function=None,\n        # image data format, either \"channels_first\" or \"channels_last\"\n        data_format=None,\n        # fraction of images reserved for validation (strictly between 0 and 1)\n        validation_split=0.0)\n\n    # Compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\ndatagen.fit(X_train)\n\n    # Fit the model on the batches generated by datagen.flow().\nh = model.fit_generator(datagen.flow(X_train, y_train, batch_size=64), \n                        steps_per_epoch=len(X_train) / 32,\n                        validation_data=(X_test, y_test),\n                        epochs=300, verbose=1, workers=4,shuffle=True,\n                        callbacks=[ModelCheckpoint(filepath=path_model)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(h.history['accuracy'])\nplt.plot(h.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}