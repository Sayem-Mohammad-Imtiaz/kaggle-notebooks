{"cells":[{"metadata":{"_uuid":"818b7297508f313f548091247ae5b5f76bd22167","_cell_guid":"e72a07c7-f17d-4652-92b0-d61fc59ff02f"},"cell_type":"markdown","source":"**Regularized Linear Models: Lasso**\n\nTo test kaggle datasets and kernels we will briefly explore the dataset \"Hitters\" and use the sklearn package to fit some Lasso (or LASSO, least absolute shrinkage and selection operator) models in order to predict the salary of baseball players. \n\nThis kernel is partly based on R. Jordan Crouser's Python adaptation (Smith College, Spring 2016) of page 251-254 (Ridge Regression) of “Introduction to Statistical Learning with Applications in R” (ISLR) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. \n\n\n\n\n**1. Read data set and explore data structure**","outputs":[],"execution_count":null},{"metadata":{"_uuid":"081d6fd0996ffaae083d41c97a7cb8999ad596d1","_cell_guid":"99fc62bf-3829-4f27-b14a-daf03ea1fdbf","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\n# import necessary functions\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV\nfrom sklearn import cross_validation\nfrom sklearn.metrics import mean_squared_error\n\n# read data set and drop missing values \nHitters = pd.read_csv(\"../input/Hitters.csv\").dropna() \nHitters.info()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"449b84af4123b8cea09c8516ede63430650a1194","_cell_guid":"4d04dd3c-dca5-43be-9ee1-9e50c22d2285"},"cell_type":"markdown","source":"**2. Prepare sklearn-style objects  X (features) and y (target) **\n\nHere we adopt to the model definition style which is commonly used in Python and more generally in Machine Learning.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"940ae403b30d3d5a109daa8e785bf2a72dfaeb5b","_cell_guid":"f3dc86b0-6170-4fc8-9ead-761fcd8374bc","trusted":true,"scrolled":true},"cell_type":"code","source":"y = Hitters.Salary\n# Drop Salary (target) and columns for which we created dummy variables\nX_ = Hitters.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n# Define the feature set X.\ndummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\nX = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dc235f2b32d8f5e1ba03abdb0053f20182152ab","_cell_guid":"7f55cd39-e871-469e-9a17-830620d8b6b8"},"cell_type":"markdown","source":"**3. Fit a Set of Lasso Models **\n\nWe use the Lasso() function to perform regularized liear regression. The Lasso() function has an alpha argument (elsewhere called λ) that is used to tune the model. \n\nWe’ll generate an array of alpha values ranging from very big to very small, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit.\n\nWe standardize the data and fit Lasso models for each value of alpha.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b4f1fde76662793c2b042dbcc0bb9ddbc453e9a0","_cell_guid":"e6927dde-9413-423c-bddb-4cb05c19c912","trusted":true},"cell_type":"code","source":"alphas = 10**np.linspace(6,-2,50)*0.5\nalphas","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99eda127ab220362ec4314d814f8b18f03dce7f1","_cell_guid":"c2f313bc-c727-470a-b1a1-82525773b3c9","trusted":true},"cell_type":"code","source":"lasso = Lasso(max_iter=10000, normalize=True)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X, y)\n    coefs.append(lasso.coef_)\n    \nnp.shape(coefs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60a9b6feca173d8a201f4b36bf97999e28ae48c5","_cell_guid":"d873bfe1-10fe-4de7-847a-71f8d9b8eb9a"},"cell_type":"markdown","source":"**4. Plot Lasso tuning parameter alpha **\n\nNow we plot the relationship between alpha and the weights (regression parameters), a line for each features.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ec772a4afcbdfbd5fbb0b2cc98bd6616803d01d6","_cell_guid":"3ff7f8f0-3d4b-4106-9802-459d87022102","trusted":true},"cell_type":"code","source":"ax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03d265fe680ec2adb21f6623232d93cd5b610ae9","_cell_guid":"12c85aac-92d6-408b-8347-69b3ff3faa87"},"cell_type":"markdown","source":"On the right hand side we can see the null model, containing only the intercept. This is caused by the very high penalty. On the left hand side there is almost no penalty. Notice that in the coefficient plot (depending on the choice of tuning parameter) some of the coefficients are exactly equal to zero. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"76e877d72c5054346c1cd1f575dba05d9ee6144b"},"cell_type":"markdown","source":"\n**5. Cross Validation Lasso **\n\nWe now split the samples into a training set and a test set in order to estimate the test error. We  perform 10-fold cross-validation to choose the best alpha, refit the mode, compute the associated test error and print the best models coefficients .","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"90214d58668cf5c5549cc31778a82f70834cd5c2"},"cell_type":"code","source":"# Use the cross-validation package to split data into training and test sets\nX_train, X_test , y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.5, random_state=1)\n\nlassocv = LassoCV(alphas=None, cv=10, max_iter=100000, normalize=True)\nlassocv.fit(X_train, y_train)\nlasso.set_params(alpha=lassocv.alpha_)\nprint(\"Alpha=\", lassocv.alpha_)\nlasso.fit(X_train, y_train)\nprint(\"mse = \",mean_squared_error(y_test, lasso.predict(X_test)))\nprint(\"best model coefficients:\")\npd.Series(lasso.coef_, index=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"873fdb1d5275d8d6ba931f111049dd35294e2485"},"cell_type":"markdown","source":"We notice that 13 of the 19 coefficient estimates are exactly zero.  The Lasso thus has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. To complete the picture we need the  coresponding cross validation results of ridge regression.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"adb4f7f41adf5f02709b13f3b0f16be9921004a8"},"cell_type":"markdown","source":"**6. Ridge Regression, Cross Validated **\n\nAgain we perform 10-fold cross-validation to choose the best alpha, refit the mode, compute the associated test error and print the best models coefficients .\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"113da53348b000e38056e91f17e0cb11507c32fc","_cell_guid":"58ebbe1a-348d-42e2-b795-4a64ca26e511","trusted":true},"cell_type":"code","source":"ridgecv = RidgeCV(alphas=alphas, normalize=True)\nridgecv.fit(X_train, y_train)\nprint(\"Alpha=\", ridgecv.alpha_)\nridge6 = Ridge(alpha=ridgecv.alpha_, normalize=True)\nridge6.fit(X_train, y_train)\nprint(\"mse = \",mean_squared_error(y_test, ridge6.predict(X_test)))\nprint(\"best model coefficients:\")\npd.Series(ridge6.coef_, index=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49d1bc77ad9d6940cb7b96983c141219cb1ed77d"},"cell_type":"markdown","source":"The models performance at a glance:","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3bf1ceaf43000a90f3df91d2d0227c432854cd9d"},"cell_type":"code","source":"print(\"mse ridge = \",mean_squared_error(y_test, ridge6.predict(X_test)))\nprint(\"mse lasso = \",mean_squared_error(y_test, lasso.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac7930be5f4e0c50b46609768024b3e8b069b97c","_cell_guid":"4c420efe-2199-4691-a07d-f1e8b078ef18"},"cell_type":"markdown","source":"\n** Summary **\n\nThis is an adaption of a fairly popular R-code (and text book). It is shown how the Lasso and cross validation can be performed on the Hitters dataset using a kaggle kernel.\n\nWith alpha chosen by cross-validation in this example the test MSE of the Lasso is a litte worse than the test MSE of ridge regression. This should not be generalized,  see discussion in the forementioned book \"ISLR\" , p. 223-224.\n\nThe lasso has a major advantage over ridge regression, in that it produces simpler and more interpretable models that involve only a subset of the predictors. \n\n\nPlease feel free to fork this kernel and play around with different parameters. \n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5317970db99ae0c1c2881dd736e5a2efcad1f820"},"cell_type":"markdown","source":"","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}