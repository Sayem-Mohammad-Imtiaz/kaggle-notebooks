{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IDENTIFY THE DANCE FORM\n\n**Identify the dance form is a machine learning competition hosted by hackerearth: a platform where you can learn coding and machine learning. I'll use the dataset of this competition for my course project 'Deep Learning with Pytorch: Zero to GANs' : a free course provided by jovian.ml and freecode camp. For more information visit.** ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem statement\n\nThis International Dance Day, an event management company organized an evening of Indian classical dance performances to celebrate the rich, eloquent, and elegant art of dance. Post the event, the company planned to create a microsite to promote and raise awareness among the public about these dance forms. However, identifying them from images is a tough nut to crack.\n\nYou have been appointed as a Machine Learning Engineer for this project. Build an image tagging Deep Learning model that can help the company classify these images into eight categories of Indian classical dance.\n\n**The dataset consists of 364 images belonging to 8 categories, namely manipuri, bharatanatyam, odissi, kathakali, kathak, sattriya, kuchipudi, and mohiniyattam.**\n\nDataset link : https://www.hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-identify-dance-form/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**In this notebook I will usedifferent artchiture given below and measure their effectiveness:**\n1. Use feedword neural network\n2. Transfer learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing the libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fastai2","execution_count":null,"outputs":[]},{"metadata":{"id":"7CfKNJBpif3n","trusted":true},"cell_type":"code","source":"from fastai2.vision.all import *\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport torchvision.transforms as T\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_name = 'Dance-Classifier'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring The Dataset","execution_count":null},{"metadata":{"id":"-GH_vpKxiiQP","trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/indian-danceform-classification/dataset'\n\nTRAIN_DIR = DATA_DIR + '/train'                           # Contains training images\nTEST_DIR = DATA_DIR + '/test'                             # Contains test images\n\nTRAIN_CSV = DATA_DIR + '/train.csv'                       # Contains real labels for training images\nTEST_CSV =  DATA_DIR + '/test.csv'                        # Contains dummy labels for test image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the given training images are at TRAIN_DIR directory. Let's look at few: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(TRAIN_DIR)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image.open(TRAIN_DIR+'/234.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image.open(TRAIN_DIR+'/287.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Q: What is the total no of images?**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(os.listdir(TRAIN_DIR))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To train a classifier we need labels which is given in TRAIN_CSV file. Each images is mapped with their labels in this file. Let's look at this file.**","execution_count":null},{"metadata":{"id":"DjqLymq9i6Mt","outputId":"017711b4-aebe-4e63-8495-b4a4479c07f1","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Q: what is the total no of images belongs to each class?**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset and Dataloader\n\nI will use fastai datablock to make dataset and dataloader which I've learn recently through fastbook. If you want to learn Machine Learning this is the best resourse. Visit [fast.ai](http://fast.ai) for more information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_x(r): return DATA_DIR+'/train/'+r['Image']  # Image Directory\ndef get_y(r): return r['target']                    # Getting the label\ndblock = DataBlock(\n    blocks=(ImageBlock,CategoryBlock),\n    splitter=RandomSplitter(),\n    get_x = get_x,\n    get_y = get_y,\n    item_tfms = Resize(330),\n    batch_tfms=aug_transforms(mult=2))\n\ndls = dblock.dataloaders(train_df)\n\ntrain_dl = dls.train\nvalid_dl = dls.valid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's look a batch of dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.train.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.valid.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's save the work to jovian\n!pip install jovian --upgrade -q\nimport jovian\njovian.commit(project = project_name, environment=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\nLet's create a base model class, which contains everything except the model architecture i.e. it wil not contain the __init__ and __forward__ methods. We will later extend this class to try out different architectures. In fact, you can extend this model to solve any image classification problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Feedforward Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class DanceModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, 512)  # first linear layer\n        self.linear2 = nn.Linear(512, 128)          # second linear layer\n        self.linear3 = nn.Linear(128, output_size)  # third linear layer\n\n        \n    def forward(self, xb):\n        # Flatten images into vectors\n        out = xb.view(xb.size(0), -1)\n        # Apply layers & activation functions\n        out = self.linear1(out)\n        out = F.relu(out)\n        \n        out = self.linear2(out)\n        out = F.relu(out)\n        \n        out = self.linear3(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model on GPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_default_device()\nprint(device)\n\n# image size is 224x224x3 \n# output class = 8\ninput_size = 330*330*3\noutput_size = 8\nmodel = to_device(DanceModel(), device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before you train the model, it's a good idea to check the validation loss & accuracy with the initial set of weights.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = [evaluate(model, valid_dl)]\nhistory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train the model using the fit function to reduce the validation loss & improve accuracy.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history += fit(5, 1e-3, model, train_dl, valid_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history += fit(5, 1e-2, model, train_dl, valid_dl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us also define a couple of helper functions for plotting the losses & accuracies.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs. No. of epochs');\n    \ndef plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_losses(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_accuracies(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recoding your results\n\nAs we perform multiple experiments, it's important to record the results in a systematic fashion, so that we can review them later and identify the best approaches that we might want to reproduce or build upon later. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**list of artchitures**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"arch1 = \"4 layers (1024, 512, 128, 8)\"\narch2 = '3 layers (512, 128, 8)'\narch = [arch1, arch2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**List of learning rates**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lrs1 = [1e-2, 1e-3]\nlrs2 = [1e-2, 1e-3]\nlrs = [lrs1, lrs2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**No of epoch used while training**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch1 = [5, 5]\nepoch2 = [5, 5]\nepochs = [epoch1, epoch2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final validation accuracy and loss**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_acc = [14.8, 24]\nvalid_loss = [2.10, 2.10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'dance-feed-forward.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clear previously recorded hyperparams & metrics\njovian.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_hyperparams(arch=arch, \n                       lrs=lrs, \n                       epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_metrics(valid_loss=valid_loss, valid_acc=valid_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.commit(project=project_name, outputs=['dance-feed-forward.pth'], environment=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Transfer Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class DanceResnet(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n#         self.network = models.resnet34(pretrained=True)\n        self.network = models.resnet50(pretrained=True)\n        \n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 10)\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))\n    \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.network.parameters():\n            param.require_grad = False\n        for param in self.network.fc.parameters():\n            param.require_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.network.parameters():\n            param.require_grad = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model on GPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_default_device()\nprint(device)\nmodel = to_device(DanceResnet(), device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train the model using the fit function to reduce the validation loss & improve accuracy.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = [evaluate(model, valid_dl)]\nhistory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nmax_lr =  1e-3\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory += fit_one_cycle(5, 1e-4, model, train_dl, valid_dl, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_losses(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_accuracies(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recoding your results\n\nAs we perform multiple experiments, it's important to record the results in a systematic fashion, so that we can review them later and identify the best approaches that we might want to reproduce or build upon later. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**list of artchitures**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"arch1 = 'resnet 34'\narch2 = 'resnet 50'\narch3 = 'resnet 50: replaced RandomResized224 to Resize224'\narch3 = 'resnet 50: replaced Resized224 to Resize330'\narch = [arch1, arch2, arch3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**List of learning rates**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lrs1 = [1e-4, 1e-4]\nlrs2 = [1e-3, 1e-4]\nlrs = [lrs1, lrs2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**No of epoch used while training**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch1 = [5, 5]\nepoch2 = [5, 5]\nepochs = [epoch1, epoch2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final validation accuracy and loss**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_acc = [64, 71]\nvalid_loss = [1.76, 1.68]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'dance-resnet50.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clear previously recorded hyperparams & metrics\njovian.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_hyperparams(arch=arch, \n                       lrs=lrs, \n                       epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_metrics(valid_loss=valid_loss, valid_acc=valid_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.commit(project=project_name, outputs=['dance-resnet50.pth'], environment=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}