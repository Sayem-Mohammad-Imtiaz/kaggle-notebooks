{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook mainly cover image classification, and simplified input using Kaggle's data (no need to download, so it is fast and a lot in use).\n\nLinke to [CheatSheet 2 (Recurrent Networks, Time Series and Natural Language Processing)](https://www.kaggle.com/yyyaaan/my-cheatsheet-tensorflow-keras-2)\n\n> Compiled with GPU enabled. A global callback is enabled so that model training stops when accuracy exceeds 97%.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.preprocessing import image\nfrom keras import layers\n\nopt_verbose = 1 if 'runtime' in get_ipython().config.IPKernelApp.connection_file else 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use of callbacks to stop training once it is good enough","metadata":{}},{"cell_type":"markdown","source":"Regardless of `epochs` parameter, model training can be stopped if its accuracy or loss is good enough.\n\nThis section use in-memory data [Fashio MNIST from Kaggle](https://www.kaggle.com/zalando-research/fashionmnist), and it is also bundled in `tf.keras.datasets.fashion_mnist`. [Data Source on Github](https://github.com/zalandoresearch/fashion-mnist). The labels include 10 classes.","metadata":{}},{"cell_type":"code","source":"fashion_mnist_train = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\nfashion_mnist_test = pd.read_csv('../input/fashionmnist/fashion-mnist_test.csv')\n# Pop will also remove column from original dataset\nfashion_mnist_train_label = fashion_mnist_train.pop('label')\nfashion_mnist_test_label = fashion_mnist_test.pop('label')\n\nfashion_mnist_train.shape, fashion_mnist_train_label.shape, fashion_mnist_test.shape, fashion_mnist_test_label.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape and Standardize to 0-1\nfashion_mnist_train1=np.array(fashion_mnist_train).reshape(60000, 28, 28, 1)\nfashion_mnist_test1=np.array(fashion_mnist_test).reshape(10000, 28, 28, 1)\nfashion_mnist_train1, fashion_mnist_test1 = fashion_mnist_train1/255., fashion_mnist_test1/255.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define myCallback Class","metadata":{}},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.97):\n      print(\"--> Callback: Reached 97% accuracy. Cheers and relax! <--\")\n      self.model.stop_training = True\n\ncallbacks = myCallback()\nmax_epochs = 15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run model with callback and validation split","metadata":{}},{"cell_type":"code","source":"model1 = Sequential([\n  layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n  layers.MaxPooling2D(2, 2),\n  layers.Conv2D(64, (3,3), activation='relu'),\n  layers.MaxPooling2D(2,2),\n  layers.Flatten(),\n  layers.Dense(512, activation=tf.nn.relu),\n  layers.Dense(10, activation=tf.nn.softmax),    \n])\n\nmodel1.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhisfit=model1.fit(\n    fashion_mnist_train1, \n    fashion_mnist_train_label, \n    epochs=max_epochs, \n    validation_split=0.1,\n    callbacks=[callbacks], \n    verbose=opt_verbose,\n)\n\n\nhisevl=model1.evaluate(fashion_mnist_test1, fashion_mnist_test_label, verbose=opt_verbose)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing, flow and augmentation","metadata":{}},{"cell_type":"markdown","source":"`ImageDataGenerator` does not load data into memory. With `flow_from_directory()`, it is possible to resize, rotation, flip, shift and rescale. Augmentation is done in-memory.\n\nData Source: [Horses or Humans on Kaggle](https://www.kaggle.com/sanikamal/horses-or-humans-dataset) and orginally available [here](http://www.laurencemoroney.com/horses-or-humans-dataset/). Label is binary.\n\nReferences: [Colab1](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%208%20-%20Lesson%202%20-%20Notebook.ipynb) | [Colab2](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%204%20-%20Lesson%204%20-%20Notebook.ipynb)","metadata":{}},{"cell_type":"markdown","source":"## Define a flow from directory","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1/255.,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',    \n)\n\nvalidation_datagen = ImageDataGenerator(rescale=1/255.)\n\ntrain_generator = train_datagen.flow_from_directory(\n    '../input/horses-or-humans-dataset/horse-or-human/train/',\n    target_size=(300, 300), \n    batch_size=128,\n    class_mode='binary',\n)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    '../input/horses-or-humans-dataset/horse-or-human/validation/',\n    target_size=(300, 300), \n    batch_size=128,\n    class_mode='binary',\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A vision model with RMSprop and validation\n\nLearning rate tuning is discussed in [Cheatsheet 2](https://www.kaggle.com/yyyaaan/my-cheatsheet-tensorflow-keras-2)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\nmodel2 =Sequential([\n    layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D(2,2),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D(2,2),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D(2,2),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D(2,2),\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001), metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hisfit=model2.fit(\n    train_generator, \n    steps_per_epoch=8,\n    epochs=max_epochs,\n    validation_data=validation_generator,\n    validation_steps=8,\n    callbacks=[callbacks],\n    verbose=opt_verbose,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transferred learning","metadata":{}},{"cell_type":"markdown","source":"Keyword: Multi-class, pre-trained, dropout, split, \n\nReference: [Colab1](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%206%20-%20Lesson%203%20-%20Notebook.ipynb#scrollTo=Blhq2MAUeyGA) | [Colab2](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%208%20-%20Lesson%202%20-%20Notebook%20(RockPaperScissors).ipynb) | [Data Source](http://www.laurencemoroney.com/rock-paper-scissors-dataset/)","metadata":{}},{"cell_type":"markdown","source":"## Load pre-trained, set non-trainable and last layer","metadata":{}},{"cell_type":"code","source":"!wget --no-check-certificate -q\\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O ./inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\npre_trained_model = InceptionV3(input_shape = (300, 300, 3), include_top = False, weights = None)\npre_trained_model.load_weights('./inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\nfor layer in pre_trained_model.layers:\n  layer.trainable = False\n\n# specify which layer to be the last layer\nlast_layer = pre_trained_model.get_layer('mixed7')\nlast_output = last_layer.output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Continue Model with own layers","metadata":{}},{"cell_type":"code","source":"x = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x) \nx = layers.Dense(3, activation='softmax')(x)\n\nmodel3 = tf.keras.Model(pre_trained_model.input, x) \n\n# LOSS is different with binary\nmodel3.compile(optimizer=RMSprop(lr=0.0001), loss='categorical_crossentropy', metrics = ['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split use data generator\n\nFor folders not organized in training and validation, it is possible to split. First use `validation_split=0.2` and then in flow use `subset=\"training/validation\"`. Remeber never augment picture in validation.","metadata":{}},{"cell_type":"code","source":"# the validation_split here, and subset below\nbetter_datagen = ImageDataGenerator(\n    validation_split=0.2,\n    rescale=1/255.,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',    \n)\n\n# excluding some files/directories using classes\ntrain_generator = better_datagen.flow_from_directory(\n    '../input/rockpaperscissors',\n    target_size=(300, 300), \n    batch_size=64,\n    classes=('paper', 'rock', 'scissors'),\n    class_mode='categorical',\n    subset=\"training\",\n)\n\nvalidation_generator = better_datagen.flow_from_directory(\n    '../input/rockpaperscissors',\n    target_size=(300, 300), \n    batch_size=64,\n    classes=('paper', 'rock', 'scissors'),\n    class_mode='categorical',\n    subset='validation',\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hisfit=model3.fit(\n    train_generator,\n    steps_per_epoch = train_generator.samples // 64,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // 64,\n    epochs = 10,\n    callbacks=[callbacks], \n    verbose=opt_verbose,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities for quick actions","metadata":{}},{"cell_type":"markdown","source":"## Plot loss and accuracy","metadata":{}},{"cell_type":"code","source":"def plot_history(history):\n    import matplotlib.image  as mpimg\n    import matplotlib.pyplot as plt\n\n    epochs=range(len(history['accuracy'])) # Get number of epochs\n\n    plt.plot(epochs, history['accuracy'], 'r', \"Training Accuracy\")\n    plt.plot(epochs, history['val_accuracy'], 'b', \"Validation Accuracy\")\n    plt.title('Training and validation accuracy')\n    \n    plt.figure()\n    plt.plot(epochs, history['loss'], 'r', \"Training Loss\")\n    plt.plot(epochs, history['val_loss'], 'b', \"Validation Loss\")\n    plt.title('Training and validation loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nplot_history(hisfit.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Serving a vision model\n\nIt is a pseudo example! change URL to get your own.","metadata":{}},{"cell_type":"code","source":"# download here (always with name tmppic)\n!wget -q -O ../tmppic https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Rock-paper-scissors_%28scissors%29.png/900px-Rock-paper-scissors_%28scissors%29.png\n\npath=\"../tmppic\"\nsize=300\n\nimg = image.load_img(path, target_size=(size, size))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimages = np.vstack([x])\nclasses = model3.predict(images, batch_size=10)\n\n\nplt.imshow(img)\nprint(f\"Predict: {np.argmax(classes, axis=1)}. Probabiilty: {classes}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}