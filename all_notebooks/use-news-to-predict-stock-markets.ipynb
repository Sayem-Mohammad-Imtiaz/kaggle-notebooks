{"cells":[{"metadata":{"_cell_guid":"63de4277-c079-1630-2e47-44d138f3e7bf"},"cell_type":"markdown","source":"# Library Import #"},{"metadata":{"_cell_guid":"193a91a4-7404-a01b-2c78-6b7385979b9c","trusted":true},"cell_type":"code","source":"import re\nimport nltk\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nenglish_stemmer=nltk.stem.SnowballStemmer('english')\n\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport random\nimport itertools\n\nimport sys\nimport os\nimport argparse\nfrom sklearn.pipeline import Pipeline\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport six\nfrom abc import ABCMeta\nfrom scipy import sparse\nfrom scipy.sparse import issparse\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_X_y, check_array\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.preprocessing import normalize, binarize, LabelBinarizer\nfrom sklearn.svm import LinearSVC\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Lambda\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM, SimpleRNN, GRU\nfrom keras.preprocessing.text import Tokenizer\nfrom collections import defaultdict\nfrom keras.layers.convolutional import Convolution1D\nfrom keras import backend as K\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections as col","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f98318d0-c122-24a1-a9a6-62ab93985428"},"cell_type":"markdown","source":"# Data Import"},{"metadata":{"_cell_guid":"c39b53ac-2080-bfb4-124d-5e775af2cec2"},"cell_type":"markdown","source":"we'll use all of the dates up to the end of 2014 as our training data and everything after as testing data."},{"metadata":{"_cell_guid":"28217d5a-e580-fa92-fdc2-45a4842812e0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/Combined_News_DJIA.csv')\ntrain = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"djia = pd.read_csv('../input/upload_DJIA_table.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_merged = djia.merge(data, on='Date')\n\ndata_merged['Open_shift'] = data_merged['Open'].shift(1)\ndata_merged = data_merged.dropna()\n\ndata_merged['Label_adj'] = (data_merged['Open_shift'] >= data_merged['Open']).astype('int')\n\ndata_merged = data_merged.drop(['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close', 'Label', 'Open_shift'], axis=1)\n\ntrain = data_merged[data_merged['Date'] < '2015-01-01']\ntest = data_merged[data_merged['Date'] > '2014-12-31']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[0]['Top1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### mean len of news description"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_len = []\nfor i in range(len(train)):\n    mean_len.append(np.mean([len(i) for i in train.loc[:, 'Top1':'Top25'].iloc[i]]))\n    \nsns.distplot(mean_len, kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### labels distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['Label_adj'])\nsns.distplot(test['Label_adj'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Daw jones dynamics"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(djia['Date'], djia['Adj Close'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d372c26-3c4c-7890-3089-c8ddb17ca81a"},"cell_type":"markdown","source":"# Data Process"},{"metadata":{"_cell_guid":"b43212e0-9344-a216-b86b-37cfbfb37752"},"cell_type":"markdown","source":"First, we transform the string of news into the  number of words as input."},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text, remove_stopwords = True):\n    \n    text = text.lower()\n    \n    # Replace contractions with their longer forms \n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'0,0', '00', text) \n    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    text = re.sub(r'\\$', ' $ ', text)\n    text = re.sub(r'u s ', ' united states ', text)\n    text = re.sub(r'u n ', ' united nations ', text)\n    text = re.sub(r'u k ', ' united kingdom ', text)\n    text = re.sub(r'j k ', ' jk ', text)\n    text = re.sub(r' s ', ' ', text)\n    text = re.sub(r' yr ', ' year ', text)\n    text = re.sub(r' l g b t ', ' lgbt ', text)\n    text = re.sub(r'0km ', '0 km ', text)\n    \n    # Optionally, remove stop words\n    if remove_stopwords:\n        text = text.split()\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n        text = \" \".join(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a87f0d3d-80f3-af90-9368-dedde8c1154f","trusted":true},"cell_type":"code","source":"trainheadlines = []\nfor row in range(0,len(train.index)):\n    trainheadlines.append(clean_text(' '.join(str(x) for x in train.iloc[row,1:26])))\n    \ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(clean_text(' '.join(str(x) for x in test.iloc[row,2:27])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_all = []\nfor headlines in trainheadlines:\n    tokens = nltk.word_tokenize(headlines)\n    tokens_all += tokens\n    \ncounter_train = col.Counter(tokens_all)\n\ntokens_all = []\nfor headlines in testheadlines:\n    tokens = nltk.word_tokenize(headlines)\n    tokens_all += tokens\n    \ncounter_test = col.Counter(tokens_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train = np.array(counter_train.most_common(20))\nc_test = np.array(counter_test.most_common(20))\n\nplt.barh(c_train[:, 0], c_train[:, 1].astype('int'))\nplt.title(\"train most freq\")\nplt.show()\nplt.barh(c_test[:, 0], c_test[:, 1].astype('int'))\nplt.title(\"test most freq\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66913298-12f3-da89-030a-90ba38b1e11b","trusted":true},"cell_type":"code","source":"basicvectorizer = CountVectorizer()\nbasictrain = basicvectorizer.fit_transform(trainheadlines)\nprint(basictrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label = train['Label_adj']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_label = test['Label_adj']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4647ab33-fd7e-0d98-9afd-7ea21eb02cf8"},"cell_type":"markdown","source":"## Logic Regression"},{"metadata":{"_cell_guid":"ee566614-2f26-df99-3c54-700a07cf83bd"},"cell_type":"markdown","source":"### Logic Regression 1"},{"metadata":{"_cell_guid":"dc0ac1c7-f4fe-3bbe-18fc-f578947626b1"},"cell_type":"markdown","source":"Algorithm: Logic Regression"},{"metadata":{"_cell_guid":"5a6e98cc-ec7f-3dd2-5dce-2c2c04cff260"},"cell_type":"markdown","source":"Input: the counts of single words"},{"metadata":{"_cell_guid":"5adfb2da-7467-505b-d44b-0a028e60227b","trusted":true},"cell_type":"code","source":"basicmodel = LogisticRegression()\nbasicmodel = basicmodel.fit(basictrain, train_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56d1ea73-641b-6fdf-698f-32518fe99dec","trusted":true},"cell_type":"code","source":"basicmodel = LogisticRegression(C=1.0)\nbasicmodel = basicmodel.fit(basictrain, train_label)\n\nbasictest = basicvectorizer.transform(testheadlines)\npreds1 = basicmodel.predict(basictest)\nacc_test=accuracy_score(test_label, preds1)\nacc_train=accuracy_score(train_label, basicmodel.predict(basictrain))\n\nprint('Logic Regression 1 accuracy test: ',acc_test )\nprint('Logic Regression 1 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc13f327-70bd-2f8f-4ab7-6a9dfae356e9","trusted":true},"cell_type":"code","source":"print('Logic Regression 1 accuracy test: ',acc_test )\nprint('Logic Regression 1 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c08a6f88-d723-97b5-dd09-a61b1f3f7829"},"cell_type":"markdown","source":"The accuracy is only 0.42."},{"metadata":{"_cell_guid":"2303559d-4ad8-7d30-ff3d-ee3bb91440ae","trusted":false},"cell_type":"code","source":"basicwords = basicvectorizer.get_feature_names()\nbasiccoeffs = basicmodel.coef_.tolist()[0]\ncoeffdf = pd.DataFrame({'Word' : basicwords, \n                        'Coefficient' : basiccoeffs})\ncoeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\ncoeffdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"949e33a5-f63d-0059-2ee0-17e625aede36","trusted":false},"cell_type":"code","source":"coeffdf.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6aa2253b-0cf6-4853-4ff5-6143dae71fb1"},"cell_type":"markdown","source":"### Logic Regression 2"},{"metadata":{"_cell_guid":"65239831-43f2-290b-8e21-b4a9f82e5a54"},"cell_type":"markdown","source":"Algorithm: Logic Regression"},{"metadata":{"_cell_guid":"a108baea-371d-d93f-7d65-213ecd4337ec"},"cell_type":"markdown","source":"Input: the counts of phrases with two connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"},{"metadata":{"_cell_guid":"15df5336-f6f9-cd66-b020-6072aff3083b"},"cell_type":"markdown","source":"We delete phrases of which frequency lower than 0.03 or higher than 0.97"},{"metadata":{"_cell_guid":"dbdf7b4f-4673-49f1-d510-377026c95611","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b6bf747c-021c-eb7f-cd50-2240138ed555","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b3421b9-b326-4ef5-4235-2e764e5cd3a3","trusted":true},"cell_type":"code","source":"advancedmodel = LogisticRegression(C=0.25)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds2 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds2)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('Logic Regression 2 accuracy test: ',acc_test )\nprint('Logic Regression 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f5a4ca4-75a0-6a24-2c34-61855d2b102a"},"cell_type":"markdown","source":"The accuracy is higher than input of single words."},{"metadata":{"_cell_guid":"3af2a0fb-a8ef-84f2-2a80-3568d327baec","trusted":false},"cell_type":"code","source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1fd8c2b-23fd-7dda-cf8d-0381c3d0736f","trusted":false},"cell_type":"code","source":"advcoeffdf.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39b0d7a4-cedc-d51c-11f4-d3a1d454682f"},"cell_type":"markdown","source":"### Logic Regression 3"},{"metadata":{"_cell_guid":"4cf79ce1-930d-474a-f07b-dd397f9a72eb"},"cell_type":"markdown","source":"Algorithm: Logic Regression"},{"metadata":{"_cell_guid":"a62cefba-3f14-ce21-49a5-1fdd09556dde"},"cell_type":"markdown","source":"Input: the counts of phrases with three connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"},{"metadata":{"_cell_guid":"886f33fe-d47f-d545-1b73-29f1e3618e01","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.0039, max_df=0.1, max_features = 200000, ngram_range = (3, 3))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5437284a-b14e-96cd-e6d5-97c41790d42a","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e29bfca-0041-5b86-fdff-9c502f4aa1ad","trusted":true},"cell_type":"code","source":"advancedmodel = LogisticRegression()\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d5717cfb-3f8c-1af4-665b-6a22e6e9c9ba","trusted":true},"cell_type":"code","source":"advancedmodel = LogisticRegression(C=0.15)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds3 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds3)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('Logic Regression 2 accuracy test: ',acc_test )\nprint('Logic Regression 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_profit(model, t):\n    preds_proba = model.predict_proba(advancedtest)\n\n    preds = (preds_proba[:, 1] > t) * 1 + (preds_proba[:, 1] < (1 - t)) * 0 + (preds_proba[:, 1] >= (1-t)) * (preds_proba[:, 1] <= t) * -1\n\n    test_merged = test.merge(djia, on='Date')\n    test_merged['preds'] = preds\n\n    test_merged['Open_shift'] = test_merged['Open'].shift(1)\n    test_merged = test_merged.dropna()\n    test_merged['profit'] = (np.abs((test_merged['Open_shift'] - test_merged['Open'])) * (test_merged['Label_adj'] == test_merged['preds']) - np.abs((test_merged['Open_shift'] - test_merged['Open'])) * (test_merged['Label_adj'] != test_merged['preds'])) * (test_merged['preds'] != -1)\n    # test_merged = test_merged.sort_values(by='Date', ascending=True)\n    test_merged['cum_profit'] = test_merged['profit'].cumsum()\n\n    plt.plot(test_merged['cum_profit'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b700896c-936b-004d-2d70-50fefee2129c","trusted":true},"cell_type":"code","source":"show_profit(advancedmodel, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2a438254-8acb-4db7-4f38-bb07be962dc2"},"cell_type":"markdown","source":"The accuracy is lower than input of phrases with two connected words."},{"metadata":{"_cell_guid":"66fee024-03b4-5964-f925-cfb3746a4741","trusted":false},"cell_type":"code","source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c50f97d8-a6f7-0a60-9edf-42bb8b7aa492","trusted":false},"cell_type":"code","source":"advcoeffdf.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a4229efa-49d5-82fb-5a1d-e1ca4001e0ac"},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"_cell_guid":"5ccc2e71-28c6-f425-f229-f1395293bffc"},"cell_type":"markdown","source":"### NBayes 1"},{"metadata":{"_cell_guid":"476ba54c-4b47-6029-e0de-29e6fcb64845","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ce37cf1-5c09-9fe7-09ec-a23d6d3b749e","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4726d110-1821-ea71-dc5c-7a21ad628dd1","trusted":true},"cell_type":"code","source":"advancedmodel = MultinomialNB(alpha=0.1)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds4 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds4)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('Logic Regression 2 accuracy test: ',acc_test )\nprint('Logic Regression 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff1f469d-1c12-5a3d-1aed-148179c26824","trusted":true},"cell_type":"code","source":"show_profit(advancedmodel, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"02a9958e-ab18-bf74-58f1-160c5b47cbff","trusted":false},"cell_type":"code","source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6223b466-48eb-0a0a-5d36-c082e50b1fe5","trusted":false},"cell_type":"code","source":"advcoeffdf.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dcc814d2-014d-5cb9-b106-52ff648d9c18"},"cell_type":"markdown","source":"### NBayes 2"},{"metadata":{"_cell_guid":"dfe60202-6348-d724-c860-c3b46c66c447","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28b21c32-28de-1bc4-161e-dcf63737caca","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23ab2ed6-8e9a-4291-2f9b-6fc2563f33ca","trusted":true},"cell_type":"code","source":"advancedmodel = MultinomialNB(alpha=0.0001)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds5 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds5)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('Logic Regression 2 accuracy test: ',acc_test )\nprint('Logic Regression 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa7d48c3-60cc-0895-7ed3-ddbf796339ff","trusted":false},"cell_type":"code","source":"print('NBayes 2 accuracy: ', acc5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f49cb3c4-b80b-48a8-69be-ac6c579e2ed4","trusted":false},"cell_type":"code","source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"087f9b18-c50b-17b5-60b6-2cf2dada7c3c","trusted":false},"cell_type":"code","source":"advcoeffdf.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4bfcc58e-1ed4-18d5-a321-e58a4744dd8c"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"_cell_guid":"a77b2afd-5654-aaff-74a9-0013fe4e8164"},"cell_type":"markdown","source":"### RF 1"},{"metadata":{"_cell_guid":"91fe0b03-37a0-eb19-2632-cdc04067b12f","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.99, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"878756f6-f57d-88cb-fe82-5181c1e40901","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b561af90-db7d-ef58-26a2-5721d5432615","trusted":true},"cell_type":"code","source":"advancedmodel = RandomForestClassifier(max_depth=3, n_estimators=200)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds6 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds6)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('RF 1 accuracy test: ',acc_test )\nprint('RF 1 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91fe3c5f-b005-97f0-da7c-597c29521877","trusted":true},"cell_type":"code","source":"show_profit(advancedmodel, 0.545)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76a971f3-878d-33e2-249a-a2b063aa5460"},"cell_type":"markdown","source":"### RF 2"},{"metadata":{"_cell_guid":"1db5461b-1ee9-2196-6725-66a6c2376a19","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (1, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1fb8019-c981-9d18-8f54-2750387ec071","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bcddf034-4a10-2026-55ea-dd3d969a297b","trusted":true},"cell_type":"code","source":"advancedmodel = RandomForestClassifier(max_depth=4)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds7 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds7)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('Logic Regression 2 accuracy test: ',acc_test )\nprint('Logic Regression 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e395e3a6-7bc8-a434-16af-be13aebbabbd","trusted":true},"cell_type":"code","source":"show_profit(advancedmodel, 0.53)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a12329a-12c7-aa90-a138-025087dd76ba"},"cell_type":"markdown","source":"## Gradient Boosting Machines"},{"metadata":{"_cell_guid":"ea1681ab-166f-20a6-3c29-f34f84438a36"},"cell_type":"markdown","source":"### GBM 1"},{"metadata":{"_cell_guid":"4ec11f0e-6c50-8149-93c3-acd17d1e7caa","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c5d312d-555f-f353-606e-2f560631033a","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f9d72ff-fb8e-5d1a-6016-3ebdb76a2ffd","trusted":true},"cell_type":"code","source":"advancedmodel = GradientBoostingClassifier(n_estimators=100, max_depth=2)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds8 = advancedmodel.predict(advancedtest.toarray())\nacc_test=accuracy_score(test_label, preds8)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('GB 1 accuracy test: ',acc_test )\nprint('GB 1 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba8ae675-7e71-e24e-69bf-99d5d19590d8","trusted":true},"cell_type":"code","source":"show_profit(advancedmodel, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e39653db-00b9-d1e0-d82e-87105e0eba1e"},"cell_type":"markdown","source":"### GBM 2"},{"metadata":{"_cell_guid":"d35d6a94-69f4-5370-6d0f-5b98c91d2565","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.02, max_df=0.175, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b83bc122-5059-665a-28c1-85145787155e","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e98ca989-15e6-b785-f282-4e59f849cd1f","trusted":true},"cell_type":"code","source":"advancedmodel = GradientBoostingClassifier()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds9 = advancedmodel.predict(advancedtest.toarray())\nacc9 = accuracy_score(test['Label'], preds9)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6df5f34f-a315-c1c4-fdbe-c54127468d5e","trusted":false},"cell_type":"code","source":"print('GBM 2 accuracy: ', acc9)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b7bd5500-e077-8225-31b5-a9c3f1015ca5"},"cell_type":"markdown","source":"## Stochastic Gradient Descent Classifier"},{"metadata":{"_cell_guid":"8f139d81-25c5-ba11-8924-e68b6e2e55f6"},"cell_type":"markdown","source":"### SGDClassifier 1"},{"metadata":{"_cell_guid":"d071bb2f-df6a-1657-1af8-5d31ee8e4a19","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24d0156a-2705-fece-001e-0426da57d383","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"30e4b846-af7e-1eda-dade-f1550d6e043b","trusted":true},"cell_type":"code","source":"advancedmodel = SGDClassifier(loss='modified_huber', random_state=0, shuffle=True)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds10 = advancedmodel.predict(advancedtest.toarray())\nacc_test=accuracy_score(test_label, preds10)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('SGD 1 accuracy test: ',acc_test )\nprint('SGD 1 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79a91d72-1cd5-4308-9b0e-9810314cdd1a","trusted":false},"cell_type":"code","source":"print('SGDClassifier 1: ', acc10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f46cdfe-20b2-9bc6-103a-1f44e2172789"},"cell_type":"markdown","source":"### SGDClassifier 2"},{"metadata":{"_cell_guid":"8ccd7ace-7326-4dfe-b93d-a8377e1a3ba6","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c012b95d-8575-ef34-d699-d240ade631f0","trusted":true},"cell_type":"code","source":"print(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e4f2994-e199-976d-f54b-0e676b6b11be","trusted":true},"cell_type":"code","source":"advancedmodel = SGDClassifier(loss='modified_huber', random_state=0, shuffle=True, alpha=100)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\n\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds11 = advancedmodel.predict(advancedtest.toarray())\nacc_test=accuracy_score(test_label, preds11)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('SGD 2 accuracy test: ',acc_test )\nprint('SDG 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9d8499e-ae6a-6197-c86f-c31bf73f3f98","trusted":true},"cell_type":"code","source":"show_profit(advancedmodel, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6aa0178-e7f3-6427-f88f-aa6e436fc87d"},"cell_type":"markdown","source":"## Naive Bayes SVM"},{"metadata":{"_cell_guid":"cd1391ca-1b70-13c0-3a62-151fc84f6069","trusted":true},"cell_type":"code","source":"class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n\n    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.C = C\n        self.svm_ = [] # fuggly\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y, 'csr')\n        _, n_features = X.shape\n\n        labelbin = LabelBinarizer()\n        Y = labelbin.fit_transform(y)\n        self.classes_ = labelbin.classes_\n        if Y.shape[1] == 1:\n            Y = np.concatenate((1 - Y, Y), axis=1)\n\n        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n        # so we don't have to cast X to floating point\n        Y = Y.astype(np.float64)\n\n        # Count raw events from data\n        n_effective_classes = Y.shape[1]\n        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n                                 dtype=np.float64)\n        self._compute_ratios(X, Y)\n\n        # flugglyness\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n            Y_i = Y[:,i]\n            svm.fit(X_i, Y_i)\n            self.svm_.append(svm) \n\n        return self\n\n    def predict(self, X):\n        n_effective_classes = self.class_count_.shape[0]\n        n_examples = X.shape[0]\n\n        D = np.zeros((n_effective_classes, n_examples))\n\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            D[i] = self.svm_[i].decision_function(X_i)\n        \n        return self.classes_[np.argmax(D, axis=0)]\n        \n    def _compute_ratios(self, X, Y):\n        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n        if np.any((X.data if issparse(X) else X) < 0):\n            raise ValueError(\"Input X must be non-negative\")\n\n        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n        check_array(self.ratios_)\n        self.ratios_ = sparse.csr_matrix(self.ratios_)\n\n        #p_c /= np.linalg.norm(p_c, ord=1)\n        #ratios[c] = np.log(p_c / (1 - p_c))\n\n\ndef f1_class(pred, truth, class_val):\n    n = len(truth)\n\n    truth_class = 0\n    pred_class = 0\n    tp = 0\n\n    for ii in range(0, n):\n        if truth[ii] == class_val:\n            truth_class += 1\n            if truth[ii] == pred[ii]:\n                tp += 1\n                pred_class += 1\n                continue;\n        if pred[ii] == class_val:\n            pred_class += 1\n\n    precision = tp / float(pred_class)\n    recall = tp / float(truth_class)\n\n    return (2.0 * precision * recall) / (precision + recall)\n\n\ndef semeval_senti_f1(pred, truth, pos=2, neg=0): \n\n    f1_pos = f1_class(pred, truth, pos)\n    f1_neg = f1_class(pred, truth, neg)\n\n    return (f1_pos + f1_neg) / 2.0;\n\n\ndef main(train_file, test_file, ngram=(1, 3)):\n    print('loading...')\n    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    # to shuffle:\n    #train.iloc[np.random.permutation(len(df))]\n\n    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    print('vectorizing...')\n    vect = CountVectorizer()\n    classifier = NBSVM()\n\n    # create pipeline\n    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n    params = {\n        'vect__token_pattern': r\"\\S+\",\n        'vect__ngram_range': ngram, \n        'vect__binary': True\n    }\n    clf.set_params(**params)\n\n    #X_train = vect.fit_transform(train['text'])\n    #X_test = vect.transform(test['text'])\n\n    print('fitting...')\n    clf.fit(train['text'], train['label'])\n\n    print('classifying...')\n    pred = clf.predict(test['text'])\n   \n    print('testing...')\n    acc = accuracy_score(test['label'], pred)\n    f1 = semeval_senti_f1(pred, test['label'])\n    print('NBSVM: acc=%f, f1=%f' % (acc, f1))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f0e000a-ca1a-5611-0ca8-c8bc40eafecb"},"cell_type":"markdown","source":"### NBSVM 1"},{"metadata":{"_cell_guid":"e054e5ea-c8be-cb98-90a1-2512b3478b6e","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b57a2b30-d69f-cf0a-05d0-8a065d5a758b","trusted":true},"cell_type":"code","source":"advancedmodel = NBSVM(C=0.01)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds12 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds12)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('NBSVM 2 accuracy test: ',acc_test )\nprint('NBSVM 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aca410f2-118d-f973-6b14-a63048444f26"},"cell_type":"markdown","source":"### NBSVM 2"},{"metadata":{"_cell_guid":"663e5e98-7ccd-29a4-40a8-10a46327c06b","trusted":true},"cell_type":"code","source":"advancedvectorizer = TfidfVectorizer( min_df=0.031, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c67fed4-8110-2a52-bb33-1da6f294b592","trusted":true},"cell_type":"code","source":"advancedmodel = NBSVM(C=0.01)\nadvancedmodel = advancedmodel.fit(advancedtrain, train_label)\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds13 = advancedmodel.predict(advancedtest)\nacc_test=accuracy_score(test_label, preds13)\nacc_train=accuracy_score(train_label, advancedmodel.predict(advancedtrain))\n\nprint('NBSVM 2 accuracy test: ',acc_test )\nprint('NBSVM 2 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"803861e3-d740-c2ed-2b2e-57221ba9b8c8","trusted":false},"cell_type":"code","source":"print('NBSVM 2: ', acc13)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"682e897f-169c-8694-d05d-66e9fc844158"},"cell_type":"markdown","source":"## Deep Learning"},{"metadata":{"_cell_guid":"1de5a061-52d5-2578-9cb0-2602499ad87b"},"cell_type":"markdown","source":"### MLP"},{"metadata":{"_cell_guid":"ac32628f-8080-5d53-e3a2-3d6986431f15","trusted":true},"cell_type":"code","source":"batch_size = 32\nnb_classes = 2\n# advancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.5, max_features = 200000, ngram_range = (2, 2))\nadvancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.5, max_features = 200000, ngram_range = (1, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(clean_text(' '.join(str(x) for x in test.iloc[row,1:26])))\nadvancedtest = advancedvectorizer.transform(testheadlines)\nprint(advancedtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8","trusted":true},"cell_type":"code","source":"X_train = advancedtrain.toarray()\nX_test = advancedtest.toarray()\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\ny_train = np.array(train_label)\ny_test = np.array(test_label)\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\n# pre-processing: divide by max and substract mean\nscale = np.max(X_train)\nX_train /= scale\nX_test /= scale\n\nmean = np.mean(X_train)\nX_train -= mean\nX_test -= mean\n\ninput_dim = X_train.shape[1]\n\n# Here's a Deep Dumb MLP (DDMLP)\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=input_dim))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\n# we'll use categorical xent for the loss, and RMSprop as the optimizer\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nprint(\"Training...\")\nmodel.fit(X_train, Y_train, epochs=1, batch_size=16, validation_split=0.05)\n\nprint(\"Generating test predictions...\")\npreds14 = model.predict_classes(X_test, verbose=0)\nacc14 = accuracy_score(test_label, preds14)\n\nprint('prediction accuracy: ', acc14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds14_proba = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = 0.55\n\npreds14 = (preds14_proba[:, 1] > t) * 1 + (preds14_proba[:, 1] < (1 - t)) * 0 + (preds14_proba[:, 1] >= (1-t)) * (preds14_proba[:, 1] <= t) * -1\n\ntest_merged = test.merge(djia, on='Date')\ntest_merged['preds'] = preds14\n\ntest_merged['Open_shift'] = test_merged['Open'].shift(1)\ntest_merged = test_merged.dropna()\ntest_merged['profit'] = (np.abs((test_merged['Open_shift'] - test_merged['Open'])) * (test_merged['Label_adj'] == test_merged['preds']) - np.abs((test_merged['Open_shift'] - test_merged['Open'])) * (test_merged['Label_adj'] != test_merged['preds'])) * (test_merged['preds'] != -1)\n# test_merged = test_merged.sort_values(by='Date', ascending=True)\ntest_merged['cum_profit'] = test_merged['profit'].cumsum()\n\nplt.plot(test_merged['cum_profit'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"143c5d6d-4259-c945-f1dd-b07a4c388526"},"cell_type":"markdown","source":"### LSTM"},{"metadata":{"_cell_guid":"db232b70-f3a0-0952-e7ab-9c0b768e8025","trusted":true},"cell_type":"code","source":"max_features = 10000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.1\nmaxlen = 500\nbatch_size = 32\nnb_classes = 2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e5f6e70a-325d-5a94-0c16-c852f951d475","trusted":true},"cell_type":"code","source":"# vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(trainheadlines)\nsequences_train = tokenizer.texts_to_sequences(trainheadlines)\nsequences_test = tokenizer.texts_to_sequences(testheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9224d6a-3bca-ef84-9817-a72b41241f54","trusted":true},"cell_type":"code","source":"print('Pad sequences (samples x time)')\nX_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(X_test.ravel()).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basicmodel = LogisticRegression(C=1.0)\nbasicmodel = basicmodel.fit(X_train, Y_train[:, 0])\n\npreds1 = basicmodel.predict(X_test)\nacc_test=accuracy_score(Y_test[:, 0], preds1)\nacc_train=accuracy_score(Y_train[:, 0], basicmodel.predict(X_train))\n\nprint('Logic Regression 1 accuracy test: ',acc_test )\nprint('Logic Regression 1 accuracy train: ',acc_train )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basicmodel.coef_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29359f0d-8d42-2038-c04f-2cd5bc87bc93","trusted":true},"cell_type":"code","source":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(LSTM(128, dropout=0.5)) \nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, Y_train, batch_size=batch_size, epochs=2) #,\n         #validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n\nprint(\"Generating test predictions...\")\npreds15 = model.predict_classes(X_test, verbose=0)\nacc15 = accuracy_score(test_label, preds15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds15_proba = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = 0.55\n\npreds15 = (preds15_proba[:, 1] > t) * 1 + (preds15_proba[:, 1] < (1 - t)) * 0 + (preds15_proba[:, 1] >= (1-t)) * (preds15_proba[:, 1] <= t) * -1\n\ntest_merged = test.merge(djia, on='Date')\ntest_merged['preds'] = preds15\n\ntest_merged['Open_shift'] = test_merged['Open'].shift(1)\ntest_merged = test_merged.dropna()\ntest_merged['profit'] = (np.abs((test_merged['Open_shift'] - test_merged['Open'])) * (test_merged['Label_adj'] == test_merged['preds']) - np.abs((test_merged['Open_shift'] - test_merged['Open'])) * (test_merged['Label_adj'] != test_merged['preds'])) * (test_merged['preds'] != -1)\n# test_merged = test_merged.sort_values(by='Date', ascending=True)\ntest_merged['cum_profit'] = test_merged['profit'].cumsum()\n\nplt.plot(test_merged['cum_profit'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8afb0d07-7d0d-dd18-b19c-be603f301b52"},"cell_type":"markdown","source":"## CNN"},{"metadata":{"_cell_guid":"7271a1d1-cde0-a26c-de99-edfb7dfbf918","trusted":false},"cell_type":"code","source":"nb_filter = 120\nfilter_length = 2\nhidden_dims = 120\nnb_epoch = 2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c071e2fc-2106-4bc5-2e2f-dde4a84dabdf","trusted":false},"cell_type":"code","source":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\n# we add a Convolution1D, which will learn nb_filter\n# word group filters of size filter_length:\nmodel.add(Convolution1D(nb_filter=nb_filter,\n                        filter_length=filter_length,\n                        border_mode='valid',\n                        activation='relu',\n                        subsample_length=1))\n\ndef max_1d(X):\n    return K.max(X, axis=1)\n\nmodel.add(Lambda(max_1d, output_shape=(nb_filter,)))\nmodel.add(Dense(hidden_dims)) \nmodel.add(Dropout(0.2)) \nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61e07990-43ef-ff27-3b36-dca146f8b4a3","trusted":false},"cell_type":"code","source":"print('Train...')\nmodel.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n          validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n\nprint(\"Generating test predictions...\")\npreds16 = model.predict_classes(X_test, verbose=0)\nacc16 = accuracy_score(test['Label'], preds16)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49e9abbe-d224-b757-c17b-bd4cfa5da818","trusted":false},"cell_type":"code","source":"print('prediction accuracy: ', acc16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outcomes\n\n- The market is arguably to be a random walk. Although there is some direction to its movements, there is still quite a bit of randomness to its movements.\n- The news that we used might not be the most relevant. Perhaps it would have been better to use news relating to the 30 companies that make up the Dow.\n- More information could have been included in the model, such as the previous day(s)'s change, the previous day(s)'s main headline(s).\n- Many people have worked on this task for years and companies spend millions of dollars to try to predict the movements of the market, so we shouldn't expect anything too great considering the small amount of data that we are working with and the simplicity of our model."},{"metadata":{"_cell_guid":"21b86295-1a3b-a1b8-78cf-b032b9d74eaa","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91aa05fb-b8fe-fe99-d191-c08b8dc5620c","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60fa6ce4-1658-11d2-c0ad-3b43a542fff0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03273578-b025-4e87-6088-a993fce31e2f","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}