{"cells":[{"metadata":{},"cell_type":"markdown","source":"# README\n## TLDR;\n* Dear, beginner\n* who want to explore EDA from basic\n* who want to do Data Analysis process from scratch\n* who are beginner like me :)"},{"metadata":{"_cell_guid":"b72f3424-c0c2-47e8-b18b-bc89253eab3f","_execution_state":"idle","_uuid":"bbc2abb85c63b8c357f5c2e0140105ed195f7fb4"},"cell_type":"markdown","source":"![](https://preview.ibb.co/bKsv9k/k.jpg)\n# INTRODUCTION\n* 본 노트북은 <a href=\"https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization\">Feature Selection and Data Visualization</a> 노트북을 번역한 노트북입니다\n    * Kaggle Notebook Grandmaster의 노트북을 필사하며 데이터 분석의 기본을 다질 수 있습니다\n    * 데이터 분석을 우리가 좋아하는 pub(🍺)에 비유하는 등 분석의 재미를 느낄 수 있습니다!\n    \n    \n* 본 노트북에서는 다른 노트북과 달리 feature visualization과 selection에 초점을 맞춥니다\n    * Visualization\n        * Violin plot, Joint plot, pair grid, swarm plot, heatmap 등을 통해 데이터에서 insight를 얻습니다\n    * feature selection\n        * correlation, univariate feature selection, recursive feature elimination, recursive feature elimination with cross validation, random forest와 함께 tree based feature selection을 사용합니다\n\n\n* 한편 주성분 분석을 위해 PCA를 사용합니다 \n\n> **Enjoy your data analysis!!!**<br>\n> 본 노트북을 통해 데이터 분석을 즐기시길 바랍니다!\n\n"},{"metadata":{"_cell_guid":"8d7fdae8-1d31-4873-a021-d553e2c4087c","_execution_state":"idle","_uuid":"c28fa7775a99901a882aee31e890ea99fe796d91"},"cell_type":"markdown","source":"# Data Analysis"},{"metadata":{"_cell_guid":"52942f7b-e58d-4275-86f0-ced1bcea06f9","_execution_state":"idle","_uuid":"d7dc365d2933b6675c57c98d438356e4cc1e6125","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns # data visualization library\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory\n# e.g. running the below will list the files in the input directory\nimport time\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a65810840012b075b5a359994931bec8acf9ab0","_cell_guid":"c9bd4680-5a5d-4ce5-8b85-1820d2e478d0","_execution_state":"idle","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48131d2a-cb21-4c1f-8213-f0e78647287c","_execution_state":"idle","_uuid":"81d7851cbda2bc774e989259005e98999b84c0b2"},"cell_type":"markdown","source":"> Before making anything like feature selection,feature extraction and classification, firstly we start with basic data analysis. <br>\nLets look at features of data.<br>\n\n🖐️  Notice! (시작 전!)<br>\n* feature selection, feature extraction, classification을 하기 전에 해야 할 것은 무엇일까?\n* feature selection, feature extraction, classification을 하기 전에 우선 데이터의 feature부터 살펴보겠습니다!\n"},{"metadata":{"_cell_guid":"d30f1486-bb97-40d7-9125-67e6f15286dc","_execution_state":"idle","_uuid":"3e01972c830afa1ce55025c0b7a202d4b204dd1d","trusted":true},"cell_type":"code","source":"data.head()  # head method show only first 5 rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"데이터를 한 번 살펴보자. 어떤 insight를 얻을 수 있을까?"},{"metadata":{"_cell_guid":"d3b8329c-20eb-4f00-b900-ffc9278cd82a","_execution_state":"idle","_uuid":"16fbbd5b380761a5e22478133c1f8ead52ca7abb"},"cell_type":"markdown","source":"**There are 4 things that take my attention**\n\n1) There is an **id** that cannot be used for classificaiton<br>\n2) **Diagnosis** is our class label<br>\n3) **Unnamed: 32** feature includes NaN so we do not need it.<br>\n4) I do not have any idea about other feature names actually I do not need because machine learning is awesome **:)<br>**\n\n\nTherefore, drop these unnecessary features. However do not forget this is not a feature selection. This is like a browse a pub, we do not choose our drink yet !!!\n\n\n1) 분류에서는 사용되지 않는 id feature가 있다<br>\n2) Diagnosis는 우리의 target label이다<br>\n3) Unnamed: 32 feature는 NaN값만 포함하고 있다. 분석 단계에서는 필요없을 것 같다<br>\n4) 다른 feature들에 대해서는 하나씩 살펴볼 필요가 있을 것 같다"},{"metadata":{"_cell_guid":"60308baf-344a-41fb-8580-cef707ce5aa8","_execution_state":"idle","_uuid":"54600377cdbec016505dcb970bb1988afbc260a2","trusted":true},"cell_type":"code","source":"# feature names as a list\ncol = data.columns\nprint(col)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8764b4cf-9963-4c1a-b449-059de8153e4c","_execution_state":"idle","_uuid":"94ea75618315ac7af54cf80a501c42b40e77ecbc","trusted":true},"cell_type":"code","source":"# y includes our labels and x includes our features\ny = data.diagnosis\nlist = ['id', 'diagnosis', 'Unnamed: 32']\nx = data.drop(list, axis=1)\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31ec8d06-ea25-4b34-84ca-c322b3d8a10f","_execution_state":"idle","_uuid":"71fecf26e957a2d670182d607aca5a7b92b4a3b6","trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(12, 6))\n\ndata.diagnosis.value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0])\nsns.countplot('diagnosis', data=data, order=data['diagnosis'].value_counts().index, ax=ax[1])\nax[0].legend()\nax[1].legend()\nB, M = y.value_counts()\nprint(\"Number of Benign: \", B)\nprint(\"Number of Malignant: \", M)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c7d5b2b-445c-45ec-81da-5b3853617711","_execution_state":"idle","_uuid":"af412eff1a8fa83f1f9fb6daaeac78cf498d589a"},"cell_type":"markdown","source":"* 이제 우리는 feature를 갖게 되었다. 그런데 저러한 feature들이 무엇을 의미하는지 어떻게 알 수 있을까? feature들에 대해 얼마나 알아야 할까?\n    * 정답은 모든 feature에 대해 알 필요는 없다는 것이다\n    * 대신에 feature의 variance, stardard deviation, number of sample 또는 min/max value 등에 대해서는 반드시 알아야 한다\n* variance, stardard deviation, number of sample, min/max 등의 정보는 데이터에 대해 이해하는 것을 도와준다\n    * Pandas Dataframe에서는 친절하게도 .describe()메서드를 통해 데이터에 대한 통계치를 제공해준다\n"},{"metadata":{"_cell_guid":"c92292c2-d999-42f3-8618-73b231c163e6","_execution_state":"idle","_uuid":"7d909ed445dd83306413a72986cebc17d1814cc7","trusted":true},"cell_type":"code","source":"x.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* area_mean과 smoothness_mean을 보면 최대값이 각각 2501과 0.163400으로 크게 차이나는 것을 확인할 수 있다\n* 따라서 시각화와 feature selection, feature extraction, classification을 위해서는 standardization과 normalization을 해줘야 한다"},{"metadata":{},"cell_type":"markdown","source":"* Standardization vs Normalization\n    * > The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1.\n    * by Googling\n    * Standardization은 데이터가 mean 0, standard deviation 1을 갖도록 데이터를 변형해주는 것입니다\n    * 반면 Normalization은 변수 값의 범위를 0에서 1사이로 스케일 해주는 것입니다"},{"metadata":{"_cell_guid":"6179a010-0819-481e-8095-72ba1021fdcd","_execution_state":"idle","_uuid":"3edac4b24f82f00d32efe9d812aed40fb06fdbed"},"cell_type":"markdown","source":"# Visualization\n\n* seaborn plot을 사용해 시각화를 할 것이다\n* 시각화 단계에서 주의할 점은 '시각화'와 'feature selection'이 다르다는 것이다"},{"metadata":{"_cell_guid":"fa50c6cf-ccb6-49fa-b4bb-e7f14a3c4a09","_execution_state":"idle","_uuid":"d3d8066df83b9a30087610eed09782c1dec7c4cf"},"cell_type":"markdown","source":"Before violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better.\n\n\n* violin plot과 swarm plot을 그리기 전에 normalization과 standardization을 해줘야 한다\n    * 왜 해야할까?\n        * normalization을 하지 않고 plot을 그려보면 normalization과 standardization이 '필수'라는 사실을 확인할 수 있을 것이다"},{"metadata":{},"cell_type":"markdown","source":"* normalization을 하기 전/후로 비교해보겠습니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dia = y\ndata = x\n# don't nomalize features\n\ndata = pd.concat([data_dia, data.iloc[:, 0:10]], axis=1)\nprint(data.shape)\n\ndata = pd.melt(data, id_vars=\"diagnosis\",\n                      var_name=\"features\",\n                      value_name=\"value\")\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 위의 도표에서 볼 수 있는 것처럼 normalization을 하지 않으면 시각화를 통해 데이터를 파악하기가 매우 어렵다\n* 이제 noramization을 한 뒤에 graph를 해석해보자"},{"metadata":{"_cell_guid":"d58052d6-9e8c-46f4-a2d5-b9e82b247f27","_execution_state":"idle","_uuid":"d640909614b5ff561e35b33e555458df70b22486","trusted":true},"cell_type":"code","source":"# first ten features\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) / data.std()  # pd.std(): return sample standardization over requested axis\n\ndata = pd.concat([data_dia, data_n_2.iloc[:, 0:10]], axis=1)\nprint(data.shape)\n# data.head()\ndata = pd.melt(data, id_vars=\"diagnosis\",\n                      var_name=\"features\",\n                      value_name=\"value\")\nprint(data.head())\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 해석해보자\n    * testure_mean에서 Malignant(악성)과 Benign의 median(중위값)은 분류되는 것 처럼 보인다. 해당 feature를 분류기를 만들때 고려하면 좋을 것 같다\n    * 반면 fractal_dimension_mean feature에서 Malignant와 Benign의 median(중위값)은 악성과 정상을 구분하지 못하는 것처럼 보인다\n \n* 다른 feature들도 살펴보도록 한다"},{"metadata":{"_cell_guid":"46ee71d3-93c1-4c2d-bb00-6995f7a1c816","_execution_state":"idle","_uuid":"0a18301387ce26b58a68e5a2d340b39e86c1f5e0","trusted":true},"cell_type":"code","source":"# second ten features\ndata = pd.concat([y, data_n_2.iloc[:, 10:20]], axis=1)\ndata = pd.melt(data, id_vars=\"diagnosis\",\n                      var_name=\"features\",\n                      value_name=\"value\")\nplt.figure(figsize=(10, 10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58f17ef1-6530-4db8-bcd9-32691363e8a9","_execution_state":"idle","_uuid":"d1c4e84c3d6bda4b9ff9e284d8c790dd46980c31","trusted":true},"cell_type":"code","source":"# Third ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54c8d1570b21a65ea707c21440d90f6701b15e2b"},"cell_type":"code","source":"# As an alternative of violin plot, box plot can be used\n# box plots are also useful in terms of seeing outliers\n# I do not visualize all features with box plot\n# In order to show you lets have an example of box plot\n# If you want, you can visualize other features as well.\n\n\n# violin plot의 대체재로 box plotdl 사용될 수 있다\nplt.figure(figsize=(10, 10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 도표를 해석해보자\n    * box plot을 보면 concavity_worst와 concave point_worst가 비슷해보인다\n    * 그렇다면 실제로 상관관계가 있는지 어떻게 알 수 있을까?\n    * joint plot을 통해 feature를 조금 더 깊게 살펴보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x.loc[:, 'concavity_worst'], x.loc[:, 'concave points_worst'], kind=\"regg\", color=\"#ce1414\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* joint plot을 통해 살펴본 결과 해당 feature들이 실제로 상관관계가 있는 것을 확인할 수 있었다\n* pearson value는 correlation 값으로 1이면 높은 양의 상관관계를 의미한다\n    * 해당 도표에서 볼 수 있듯이 pearson value가 0.86으로 매우 높은 것을 확인할 수 있다"},{"metadata":{},"cell_type":"markdown","source":"> 그렇다면 3개 이상의 변수는 어떻게 비교할 수 있을까?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\ndf = x.loc[:, ['radius_worst', 'perimeter_worst', 'area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")  # 아래쪽에는 KDE plot을 그려준다\ng.map_upper(plt.scatter)  # 위쪽에는 scatter plot을 그려준다\ng.map_diag(sns.kdeplot, lw=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* pair plot을 살펴보면 세 변수가 높은 상관관계가 있다는 것을 확인할 수 있다"},{"metadata":{"_cell_guid":"c9ef0921-19bc-4d40-aa72-0bc2333bd4b4","_execution_state":"idle","_uuid":"f8355f83ac16e414e3b88e67b77d33ef31c3574d"},"cell_type":"markdown","source":"Up to this point, we make some comments and discoveries on data already. If you like what we did, I am sure swarm plot will open the pub's door :) "},{"metadata":{"_cell_guid":"03abd05a-d67a-4bfc-b951-6394de8c6fc9","_execution_state":"idle","_uuid":"c3807ef7f6e17b33ae383349bdee7ebfced2a847"},"cell_type":"markdown","source":"In swarm plot, I will do three part like violin plot not to make plot very complex appearance\n\n### swarm plot\n* Draw a categorical scatterplot with non-overlapping points.\n* categorical scatter plot을 포인트가 겹치지 않게 그려줍니다\n    * 여기서 포인트가 겹치지 않게 그려준다 함을 다시 한번 생각해보시면 좋을 것 같습니다!"},{"metadata":{"_cell_guid":"ef378d49-8aed-4b9e-96e8-e7d2458fdd89","_execution_state":"idle","_uuid":"85a2413b70c1b3d69f26a2c122c22d55f930e774","trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia = y\ndata = x\ndata_n_2 = (data-data.mean()) / (data.std())  # standardization\n\ndata = pd.concat([y, data_n_2.iloc[:, 0:10]], axis=1)\ndata = pd.melt(data, id_vars='diagnosis',\n                      var_name=\"features\",\n                      value_name='value')\nplt.figure(figsize=(10, 10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"428c75b6-b5d0-47e3-a568-17b5d1896c0c","_execution_state":"idle","_uuid":"75dfd5e9e50adceb1d42dd000ce779e79b069cce","trusted":true},"cell_type":"code","source":"data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee64bbc8-0431-482a-b08f-cdca43e41390","_execution_state":"idle","_uuid":"209e9e9120d6e889696d2d1190e663b5c1885a82","trusted":true},"cell_type":"code","source":"data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\ntoc = time.time()\nplt.xticks(rotation=90)\nprint(\"swarm plot time: \", toc-tic ,\" s\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c9efa5e-4938-477e-ab9a-4b084cc0b870","_execution_state":"idle","_uuid":"d23e2f9b040a92feb0d7ceb8e01e74c758f5dbc3"},"cell_type":"markdown","source":"They looks cool right. And you can see variance more clear. Let me ask you a question, **in these three plots which feature looks like more clear in terms of classification.** In my opinion **area_worst** in last swarm plot looks like malignant and benign are seprated not totaly but mostly. Hovewer, **smoothness_se** in swarm plot 2 looks like malignant and benign are mixed so it is hard to classfy while using this feature."},{"metadata":{},"cell_type":"markdown","source":"* variance를 보다 선명하게 관측할 수 있게 되었다\n* 마지막 plot의 area_worst는 malignant와 benign을 대부분 분류해주는 것을 볼 수 있다\n* 두 번째 plot의 smoothness_se의 경우 malignant와 benign을 제대로 분류하지 못하는 것을 확인할 수 있다"},{"metadata":{"_cell_guid":"c4c68f34-e876-4e5a-a4a7-09c07381425a","_execution_state":"idle","_uuid":"b46f98eb7ca8d36dc7bf1516895599524bab694d"},"cell_type":"markdown","source":"**What if we want to observe all correlation between features?** Yes, you are right. The answer is heatmap that is old but powerful plot method."},{"metadata":{"_cell_guid":"9e1e7d8a-bbf2-4aab-90e7-78d4c4ccf416","_execution_state":"idle","_uuid":"0eeb70ddffc8ac332ee076f2f6b2833a6ffddd2d","trusted":true},"cell_type":"code","source":"# correlation map\nf, ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidth=.5, fmt='.1f', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ee2e02b-9fc6-42f7-8a83-f6dd77df6c11","_execution_state":"idle","_uuid":"b4e3afecb204a8262330d22e4539554b2af7975a"},"cell_type":"markdown","source":"Well, finaly we are in the pub and lets choose our drinks at feature selection part while using heatmap(correlation matrix)."},{"metadata":{},"cell_type":"markdown","source":"> 드디어 pub에 도착했다\n\n* 🍺  우리가 원하는 드링크를 고르자!\n* 'feature selection'을 하자는 말이다!"},{"metadata":{"_cell_guid":"6786734a-40a9-46b6-a13a-97ee9c569636","_execution_state":"idle","_uuid":"84b145dd0c13a3a0d4dd4f9b9fd1bd782e11fcf8"},"cell_type":"markdown","source":"# Feature Selection and Random Forest Classification\n\n>Today our purpuse is to try new cocktails. For example, we are finaly in the pub(🍺) and we want to drink different tastes. Therefore, we need to compare ingredients of drinks. If one of them includes lemon, after drinking it we need to eliminate other drinks which includes lemon so as to experience very different tastes."},{"metadata":{},"cell_type":"markdown","source":"* 오늘 우리는 pub에서 새로운 cocktail을 시도해볼 것이다\n    * 다른 종류의 drink를 시도할 것이다\n    * drink의 성분을 비교해서 lemen을 먹었으면 다음 음료로는 lemon이 제외된 음료를 시키는 식으로 말이다"},{"metadata":{"_cell_guid":"c7b2df4e-270e-4c94-8789-177f5e90ac46","_execution_state":"idle","_uuid":"a042df90ef7138d6f101463e93936119176bdc0d"},"cell_type":"markdown","source":"> In this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict. "},{"metadata":{},"cell_type":"markdown","source":"* 이번 파트에서는 다른 방법들로부터 feature를 선택할 것이다\n  * correlation, univariate feature selection, recursive feature elimination(RFE), recursive feature elimination with cross validation(EFECV), tree based feature selection 등을 알아보자\n  * 모델의 학습은 random forest classifier를 사용할 것이다"},{"metadata":{"_cell_guid":"94d217e3-b2b3-4016-b72e-f8d521d17af7","_execution_state":"idle","_uuid":"39003c7b75f265bf0826f407433e65923c4dd017"},"cell_type":"markdown","source":"## 1) Feature selection with correlation and random forest classification"},{"metadata":{},"cell_type":"markdown","source":"* heatmap을 보면 raidus_mean feature와 perimeter_mean, area_mean은 높은 양의 상관관계를 보여주고 있다\n* 이중에서 필자는 radius_mean을 직관적으로 선택했다고 한다\n    * Why?\n        * swarm plot을 살펴보면 radius_mean feature가 clear하게 malignant와 benign을 구분해줘서라고 한다\n        * (내 생각엔) perimeter_mean과 area_mean도 마찬가지인 것 같은데...ㅎㅎ\n"},{"metadata":{},"cell_type":"markdown","source":"* 몇 개의 feature를 더 살펴보면...\n    * Compactness_mean과 concavity_mean, concave points_mean에서 concavity_mean을 선택 (swarm plot)을 보자\n    * radius_se, perimeter_se, area_se로부터 area_se 선택 (swarm plot을 보자)\n    * radius_wrost, perimeter_worst, area_worst에서 area_worst 선택\n    * concavity_worst, concavity_worst, concave point_worst에서 oncavity_worst 선택\n    * ...\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \nx_1.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6de99062-7a5a-4b70-879c-54d6c8a4a7e2","_execution_state":"idle","_uuid":"1ab3852ed7fbeba8718e6722e8a40521033bdf29"},"cell_type":"markdown","source":"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it."},{"metadata":{},"cell_type":"markdown","source":"* correlation heatmap을 통해 상관관계가 높은 feature들을 제거해주었다\n* 상관관계가 높은 feature들을 제거해준 뒤 다시 heatmap을 그려보자"},{"metadata":{"_cell_guid":"733f0784-4a3f-410c-a220-f98591825f2e","_execution_state":"idle","_uuid":"eec5424039036e1af43ba0795b76393805308f97","trusted":true},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6551fe9-f3a8-4738-ace3-b0427be4aeb4","_execution_state":"idle","_uuid":"0eaf4ae8e33c2d6352a862953c1fe5eecf46ed27"},"cell_type":"markdown","source":"Well, we choose our features but **did we choose correctly ?** Lets use random forest and find accuracy according to chosen features."},{"metadata":{},"cell_type":"markdown","source":"* 우리는 상관관계가 높은 feature들을 제거해주는 방식으로 feature를 선택해주었다 (feature selection)\n* 이러한 방법이 효과적인지 random forest 분류기를 통해 확인해보자"},{"metadata":{"_cell_guid":"111af932-96f8-4105-8deb-ba1172edd203","_execution_state":"idle","_uuid":"c7a6af60a44959f81593d788934a49c9259d8b43","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70% and test 30%\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n# random forest classifier with n_esitimate=10(default)\nclf_rf = RandomForestClassifier(random_state=43)\nclf_rf = clf_rf.fit(x_train, y_train)\n\nac = accuracy_score(y_test, clf_rf.predict(x_test))\nprint(\"Accuracy is: \", ac)\ncm = confusion_matrix(y_test, clf_rf.predict(x_test))\nsns.heatmap(cm, annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1503384d-ca2b-4b52-82b5-f1131b014269","_execution_state":"idle","_uuid":"21cda299619940f7b22acc9a804ee56bff71d3e7"},"cell_type":"markdown","source":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. \nNow lets see other feature selection methods to find better results."},{"metadata":{"_cell_guid":"3eed9ac3-e601-4e16-85bc-26a1a6fff850","_execution_state":"idle","_uuid":"decd86422aee506b061c905e8573abb3612734e4"},"cell_type":"markdown","source":"## 2) Univariate feature selection and random forest classification\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features.\n<http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest>"},{"metadata":{"_cell_guid":"f053659d-9dfe-4858-a220-ef327df3bc36","_execution_state":"idle","_uuid":"f681583a7b20e4fb86e557b910021a263573cf18"},"cell_type":"markdown","source":"> In this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? The answer is only trying or intuitively. I do not try all combinations but I only choose k = 5 and find best 5 features."},{"metadata":{},"cell_type":"markdown","source":"* correlation이 높은 feature를 제거하는 feature selection에 이어 다른 feature selection 방법에 대해 알아보자\n* Univariate feature selection\n    * 'univariable analysis' 두 변수 사이의 관계를 밝히는 방법\n    * SelctKBest를 사용한다\n    * \"Select features according to the k highest scores\"\n    * k개의 높은 스코어를 갖는 feature를 선택한다\n    * 이곳 <a href='https://datascienceschool.net/03%20machine%20learning/14.03%20%ED%8A%B9%EC%A7%95%20%EC%84%A0%ED%83%9D.html'>link</a>에서 자세한 내용을 알아볼 수 있다"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9684618-06fe-4b0a-835f-ceea46da397c","_execution_state":"idle","_uuid":"d9dcd1495cbf33c190a0d1211df4bac5e79bc4e5","trusted":true},"cell_type":"code","source":"print(\"Score list: \", select_feature.scores_)  # scores of features\nprint(\"Feature list: \", x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0c426ef-3072-4f6e-bf24-b5d927a98316","_execution_state":"idle","_uuid":"0f46ea1a9b1282a377549406d1c5e093380954b6"},"cell_type":"markdown","source":"Best 5 feature to classify is that **area_mean, area_se, texture_mean, concavity_worst and concavity_mean**. So lets se what happens if we use only these best scored 5 feature."},{"metadata":{"_cell_guid":"efc70e04-bc9c-4f93-bcd3-b1d7160d0d5c","_execution_state":"idle","_uuid":"9a2bd21537f7c600f3c9baaf833c001084d6ba00","trusted":true},"cell_type":"code","source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n\n# random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()\nclf_rf_2 = clf_rf_2.fit(x_train_2, y_train)\nac_2 = accuracy_score(y_test, clf_rf_2.predict(x_test_2))\nprint(\"Accuracy is: \", ac_2)\ncm_2 = confusion_matrix(y_test, clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2, annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 성능이 좋은 변수를 10개 추출한다면 모델의 성능이 더 좋아질까?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=10).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n# random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()\nclf_rf_2 = clf_rf_2.fit(x_train_2, y_train)\nac_2 = accuracy_score(y_test, clf_rf_2.predict(x_test_2))\nprint(\"Accuracy is: \", ac_2)\n# also print confusion matrix\ncm_2 = confusion_matrix(y_test, clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2, annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 성능이 더 좋아졌다!\n* 하이퍼 파라미터 서치를 통해 가장 좋은 성능의 모델을 만들수도 있을 것 같다\n    * 하이퍼 파라미터 서치도 한 번 연습해보자"},{"metadata":{"_cell_guid":"d8888dc1-b50b-46b4-b202-4e33c2630406","_execution_state":"idle","_uuid":"575005da62c41d12bbb3999b3e26148e12930ce3"},"cell_type":"markdown","source":"\nAccuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar.\nNow lets see other feature selection methods to find better results."},{"metadata":{"_cell_guid":"702ad2b3-5b12-4d15-93b1-e7d62dfd1040","_execution_state":"idle","_uuid":"7a3c3050dd9d694e52962c7c712b1ea16aab6fdf"},"cell_type":"markdown","source":"## 3) Recursive feature elimination (RFE) with random forest\n> <http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html>\nBasically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features"},{"metadata":{"_cell_guid":"8a34a801-c568-4598-8a07-85fc20ad0386","_execution_state":"idle","_uuid":"3ea45c46bb231c767160fe13ad3b21a70f0d0375"},"cell_type":"markdown","source":"> Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method."},{"metadata":{"_uuid":"c384a5240d1c1e9e2a6750e5d218dadaf24d2035","_cell_guid":"8df88bb5-8003-4696-9efe-63ebf8d609a5","_execution_state":"idle","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()\nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51d63d0b-4e00-4dc1-816c-809287b60806","_execution_state":"idle","_uuid":"29ba35a98954d0ae686ce46295179d1f1a27b74c","trusted":true},"cell_type":"code","source":"print(\"Chosen best 5 feature by rfe: \", x_train.columns[rfe.support_])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Modeling it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_3 = x_train.loc[:, x_train.columns[rfe.support_]]\nx_test_3 = x_test.loc[:, x_test.columns[rfe.support_]]\n\nclf_rf_3 = RandomForestClassifier()\nclf_rf_3.fit(x_train_3, y_train)\n\nac_3 = accuracy_score(y_test, clf_rf_3.predict(x_test_3))\nprint(\"Accuracy: \", ac_3)\n# confusion matrics\ncm_3 = confusion_matrix(y_test, clf_rf_3.predict(x_test_3))\nsns.heatmap(cm_3, annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92aa6013-3e16-4005-ab1b-b7ce53e78bd3","_execution_state":"idle","_uuid":"ce670f778a661e8ddc3b7b21a43ccb48a551581a"},"cell_type":"markdown","source":"* Chosen 5 best features by rfe is **texture_mean, area_mean, concavity_mean, area_se, concavity_worst**. \n* They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. \n* Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is **5**. \n* Maybe if we use best 2 or best 15 feature we will have better accuracy. \n* Therefore lets see how many feature we need to use with rfecv method."},{"metadata":{},"cell_type":"markdown","source":"* selectKBest mothod와 RFE를 통해 5개의 feature를 추출할 수 있었다\n* 두 방법 모두 같은 feature를 추출해줬으며 RandomForest Classifier를 통해 95%의 성능을 달성할 수 있었다\n* 하지만 여기에는 한 가지 문제점이 존재한다\n    * 5개의 feature가 최적의 feature라고 할 수 있을까?\n    * rfecv 방법을 통해 최적의 feature를 추출해보자!"},{"metadata":{"_cell_guid":"22a4f840-2a37-4047-9804-129e7f68f74a","_execution_state":"idle","_uuid":"42a8c3f2ef0e5978b620eea737e6e234dc79cfe8"},"cell_type":"markdown","source":"## 4) Recursive feature elimination with cross validation and random forest classification\n> <http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html>\nNow we will not only **find best features** but we also find **how many features do we need** for best accuracy."},{"metadata":{},"cell_type":"markdown","source":"* 이제는 최적의 feature뿐만아니라 몇 개의 feature가 필요한지도 알아보도록 하자\n* 최고의 성능을 위한 feature의 개수는 몇 개인지 알아보자!"},{"metadata":{"_cell_guid":"7a5d4d69-7734-4465-89cc-f46b4af4c548","_execution_state":"idle","_uuid":"0d7803966979745a8bdbdbc44a1927558485640a","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be0f7ce5-55c0-48c9-a125-616750f13943","_execution_state":"idle","_uuid":"cef5972f5e9fa830e92bae00ca5dd7d2b0ac8c58"},"cell_type":"markdown","source":"Finally, we find best 11 features that are **texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst** for best classification. Lets look at best accuracy with plot.\n"},{"metadata":{"_cell_guid":"5b69144b-72e4-4ac3-b8a8-c9ebbf8ffa3b","_execution_state":"idle","_uuid":"f362bfa341032f2bb1bacc1c50675a1916f5c536","trusted":true},"cell_type":"code","source":"# Plot number of features vs cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected feature\")\nplt.plot(range(1, len(rfecv.grid_scores_) +1), rfecv.grid_scores_)\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"580f19b6-1182-43d5-932c-21e2fb5bb2d9","_execution_state":"idle","_uuid":"f071ec67bd63d5c458c2cb6303fe6f54458db57b"},"cell_type":"markdown","source":"Lets look at what we did up to this point. Lets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make **feature selection and understanding data.** Then last make our last feature selection method."},{"metadata":{"_cell_guid":"2637e8bc-d986-41c0-acef-ce76afc4c350","_execution_state":"idle","_uuid":"8bc3105398fc618e19deec4de957950cfb45c054"},"cell_type":"markdown","source":"## 5) Tree based feature selection and random forest classification\n> <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>\nIn random forest classification method there is a **feature_importances_** attributes that is the feature importances (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.**\n"},{"metadata":{},"cell_type":"markdown","source":"* tree 기반의 feature selection과 randomforest classifier\n* randomforest classifier에서 제공하는 feature_importance 방법을 사용하려면 학습 데이터에 상관관계가 있는 feature가 있어서는 안된다\n"},{"metadata":{"_cell_guid":"df8abc8d-3279-4c31-a6b6-e4f272ca0b47","_execution_state":"idle","_uuid":"31d4b248f723930ff7120ffaff2c260f07e3f0fc","trusted":true},"cell_type":"code","source":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"00008678-9012-4e50-aaab-010b3353ac98","_execution_state":"idle","_uuid":"760b045b33388f6fb7b53acdf931e8204eea80cd"},"cell_type":"markdown","source":"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them. "},{"metadata":{"_cell_guid":"21ef3e97-1eba-4f30-9714-cc45a3c1a594","_execution_state":"idle","_uuid":"c3fd1de4be5be26252a4105501a217a026d116b1"},"cell_type":"markdown","source":"# Feature Extraction\n> <http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>\nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.\n "},{"metadata":{},"cell_type":"markdown","source":"* feature extraction을 위해 PCA를 사용할 것이다\n* PCA의 성능을 높이기 위해서는 데이터를 normalize 해줘야 한다"},{"metadata":{"_cell_guid":"aa440ca8-8282-4cc3-9683-d5ebdd992140","_execution_state":"idle","_uuid":"cf72d82fea5d8330db8ec324fb18abc6e969bac6","trusted":true},"cell_type":"code","source":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_N)\n\nplt.figure(1, figsize=(14, 13))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"264e1dba-1f40-4a28-bd4b-5261a7df185b","_execution_state":"idle","_uuid":"a50cb8de4ec6e2e8727a2b3b021ee42c4ce21b29"},"cell_type":"markdown","source":"According to variance ration, 3 component can be chosen."},{"metadata":{"_cell_guid":"224c9c75-256d-4650-af3e-c4c39b565661","_execution_state":"idle","_uuid":"5e0f4ba2b385fe9312ced2455eed4fb87d39a0b8"},"cell_type":"markdown","source":"# Conclusion\n* 본 노트북에서는 feature selection과 data visualization을 상세히 다뤘습니다\n* 처음 우리가 맞이한 데이터는 33개의 feature를 가지고 있었지만 feature selection을 통해 feature의 수를 33개에서 5개까지 줄일 수 있었습니다. 이 때의 정확도는 95%에 달했습니다\n* 오늘 배운 방법으로 데이터 분석을 시작하는 계기가 되었으면 좋겠습니다. 감사합니다!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}