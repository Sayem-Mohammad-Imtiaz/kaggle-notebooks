{"cells":[{"metadata":{"id":"6X8A-SDW0W2X","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"n5ka4SsKkGe5"},"cell_type":"markdown","source":"#**Data** **Preprocessing**"},{"metadata":{"id":"h2FP1n7VB91d","trusted":true},"cell_type":"code","source":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nimport random\nfrom sklearn.svm import SVC\nimport sklearn.metrics as sk\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"id":"8BA8YEtm7jBG","outputId":"fcbbfa0c-81ea-4f71-beb3-8288448d6a2b","trusted":true},"cell_type":"code","source":"#change the dataset location\ndf1 = pd.read_csv('/kaggle/input/gpu-runtime/sgemm_product.csv')\ndf = df1.sample(frac=0.4) #reducing data size for faster computation\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"ryDK-1c40mHh","trusted":true},"cell_type":"code","source":"#creating Runtime, target variable by taking average of Run1, Run2, Run3, Run4\ndf['Runtime']=df[['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']].mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"STEMeQrf0sPb","outputId":"e6651bc7-d9fd-45ec-88f3-d61cbe86d336","trusted":true},"cell_type":"code","source":"#viewing data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SKWVMRHz0uNQ","outputId":"c184691e-8425-4a99-f1ce-460e84b9c968","trusted":true},"cell_type":"code","source":"#drop other Run time variables\ndf1=df.drop(columns =['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)'], axis = 1)\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"eS3kDX710wSt","outputId":"f43f6e42-e117-4eb0-f149-0155369675cd","trusted":true},"cell_type":"code","source":"#checking descriptive stats\ndf1.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"1dGcQI5V0ykI","outputId":"ef97cbfa-9ccf-4af0-80c2-508d605065f3","trusted":true},"cell_type":"code","source":"#checking for NULL values\ndf1.isnull().sum() #no NULL values","execution_count":null,"outputs":[]},{"metadata":{"id":"1PeHDptF00dF","outputId":"7dbaa0bb-1565-4f68-fb26-f5791df55adc","trusted":true},"cell_type":"code","source":"#checking for outliers\nplt.figure(figsize=(10,6))\nsns.boxplot(df1['Runtime']);","execution_count":null,"outputs":[]},{"metadata":{"id":"T5cvJJ6W02WX","outputId":"7286450b-c946-488d-ac12-034182c4e193","trusted":true},"cell_type":"code","source":"#removing outliers\nQ1=df1['Runtime'].quantile(0.25)\nQ2=df1['Runtime'].quantile(0.75)\nIQR = Q2 - Q1\nLL=Q1-1.5*IQR\nUL=Q2+1.5*IQR\ndf2 = df1[(df1.Runtime>LL) & (df1.Runtime<UL)]\ndf2.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"rX5Ag9qwi41Q","outputId":"c792bdc0-747c-4982-eec7-f4bea4685392","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.boxplot(df2['Runtime']);","execution_count":null,"outputs":[]},{"metadata":{"id":"bdNWm8k504VI","outputId":"ac82d4c5-9cac-400e-d414-afa2d7794203","trusted":true},"cell_type":"code","source":"#checking variable distribution\nfor index in range(10):\n   df2.iloc[:,index] = (df2.iloc[:,index]-df2.iloc[:,index].mean()) / df2.iloc[:,index].std();\ndf2.hist(figsize= (14,16));","execution_count":null,"outputs":[]},{"metadata":{"id":"aRMa1e-k06v-","outputId":"ddcb0f52-3213-4a9f-d355-8a08da05c7f3","trusted":true},"cell_type":"code","source":"#plotting the distribution of Runtime\nsns.distplot(df2['Runtime'])","execution_count":null,"outputs":[]},{"metadata":{"id":"CPblhA6U6co0","outputId":"b8c933d3-4453-4562-ee25-78dc8adcebb1","trusted":true},"cell_type":"code","source":"df2['target']=np.log(df2.Runtime)\nsns.distplot(df2['target'])","execution_count":null,"outputs":[]},{"metadata":{"id":"SzM8EedS1ClZ","outputId":"93825b2e-0ebf-45c0-c4b0-ae16c7c59b72","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,14))\nsns.set(font_scale=1)\nsns.heatmap(df2.corr(),cmap='GnBu_r',annot=True, square = True ,linewidths=.5);\nplt.title('Variable Correlation')","execution_count":null,"outputs":[]},{"metadata":{"id":"brHw-bqF8tgf","outputId":"1587277d-839c-4bb4-a3db-b6b004c6e430","trusted":true},"cell_type":"code","source":"#Creating binary classification target variable\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\ndf_features=df2.drop(columns=['target','Runtime'],axis=1).values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"-kXBeN88gt0y","trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nx1_train = sc.fit_transform(x1_train)\nx1_test = sc.transform(x1_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"BBw_RnuwOkRK"},"cell_type":"markdown","source":"#Run SVM\n"},{"metadata":{"id":"SPvhfLpfN2Fo","outputId":"1151fada-9e86-4026-9dfa-0c23a67799fc","trusted":true},"cell_type":"code","source":"#Linear SVM\nprint('Linear Model',end='\\n')\nlsvclassifier = SVC(kernel='linear')\nlsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = lsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_linear=accuracies.mean()\nstd_svm_linear=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_linear*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_linear*100,end='\\n')\n\n#Predict SVM\ny_predl = lsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predl))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predl))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predl, normalize=True, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"id":"emcij_ZBhFN6","outputId":"aaa19839-b970-4835-a26a-887cdfbc77a8","trusted":true},"cell_type":"code","source":"#Polynomial SVM\nprint('Polynomial Model',end='\\n')\npsvclassifier = SVC(kernel='poly')\npsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = psvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_poly=accuracies.mean()\nstd_svm_poly=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_poly*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_poly*100,end='\\n')\n\n#Predict SVM\ny_predp = psvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predp))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predp))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predp, normalize=True, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"id":"b4USM3HPhE2c","outputId":"1d243bf8-0968-480e-f45c-85e65e33527e","trusted":true},"cell_type":"code","source":"#RBF SVM\nprint('RBF Model',end='\\n')\nrsvclassifier = SVC(kernel='rbf')\nrsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = rsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_rbf=accuracies.mean()\nstd_svm_rbf=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_rbf*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_rbf*100,end='\\n')\n\n#Predict SVM\ny_predr = rsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predr))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predr))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predr, normalize=True, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"id":"e3xxKlpcmUbb","outputId":"e930064e-deb6-4f4e-944e-0c864d1b5658","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n  \n\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs,scoring='accuracy', train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\n\ntitle = r\"Learning Curves (SVM, RBF kernel, $\\gamma=auto$)\"\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n#estimator = SVC(kernel = 'rbf', random_state = 0,gamma='auto')\nplot_learning_curve(rsvclassifier, title, df_features, df_target, (0.8, 1.1), cv=cv)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(rsvclassifier, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kKdh-uZ97gDL"},"cell_type":"markdown","source":"#Decision Trees\n"},{"metadata":{"id":"ZYSAjus1OT3H","outputId":"3ecb2b26-46da-4e88-cb21-0d770929b2d4","trusted":true},"cell_type":"code","source":"#Entropy Model\neclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\neclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = eclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_e=accuracies.mean()\nstd_dt_e=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_e*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_e*100,end='\\n')\n\n#predict y\ny_pred = eclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"DpE_zf08fYXm","outputId":"654ae45a-ae3e-4878-cb6e-f0fd618b1b16","trusted":true},"cell_type":"code","source":"#Gini Model\ngclassifier = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\ngclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = gclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_g=accuracies.mean()\nstd_dt_g=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_g*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_g*100,end='\\n')\n\n#predict y\ny_pred = gclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"QdQsxmUQlnyZ","outputId":"10c75ada-8810-408a-b77d-6f9799b98dd8","trusted":true},"cell_type":"code","source":"#Pruning the better tree - Gini Tree\nparameters = [{'criterion': ['gini'],'min_samples_leaf':[5,10,20,30,50,100],'max_depth':[1,5,10,20,50,100]}] \ngrid_search = GridSearchCV(estimator = gclassifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"BtlTpMbfT9HB","outputId":"cc58a1a7-2873-4c7e-a4a3-fe9b50abcca6","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(gclassifier, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"rBZUogCY-ozF"},"cell_type":"markdown","source":"#Boosting"},{"metadata":{"id":"5B0XdXhY-oXH","outputId":"e0dd9692-491a-4cdb-eb63-4e5948b859ff","trusted":true},"cell_type":"code","source":"# Boosting via Gradient Boost\nfrom sklearn.ensemble import GradientBoostingClassifier\nclassifiergb = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\nclassifiergb.fit(x1_train, y1_train)\n\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifiergb, X = x1_train, y = y1_train, cv = 10,n_jobs=-1)\nmean_boosting=accuracies.mean()\nstd_boosting=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_boosting*100,end='\\n')\nprint('Standard deviation of Accuracies',std_boosting*100,end='\\n')\n\n# Predicting the Test set results\ny_predgb = classifiergb.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_predgb))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_predgb))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_predgb))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2990EUaI-uXO","outputId":"6e12c973-0529-454a-94dd-05d48fd1f163","trusted":true},"cell_type":"code","source":"#playing around with the pruning to get the best boosting tree\n# Applying Grid Search to find the best model and the best parameters\nfrom sklearn.ensemble import AdaBoostClassifier\nclassifier_AdaBoost = AdaBoostClassifier(random_state=1)\nclassifier_AdaBoost.fit(x1_train, y1_train)\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators': [50,100,200,300,500,1000,1500]}] \ngrid_search = GridSearchCV(estimator = classifier_AdaBoost,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"mSfpuV3P-x-c","outputId":"1e2b70d3-83f5-4643-fa76-6b6f099cb376","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(classifier_AdaBoost, df_features, df_target,cv=3,n_jobs=-1)\ntrain_sizes \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\n\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\nplt.legend(loc=\"best\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier_AdaBoost, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"ML_assignment_2_GPU.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}