{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Fake News Categorization\nForked from Starter","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# package for regular expressions\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0. Helper functions\nFrom Starter : The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. get dataset\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# identify files\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# view a sample\nnRowsRead = None # 1000 # specify 'None' if want to read whole file\n# fake_or_real_news.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n\ndf = pd.read_csv('/kaggle/input/fake_or_real_news.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'fake_or_real_news.csv'\ndf.columns = ['rec', 'title', 'text', 'label']\ndf = df.set_index('rec')\n\n# shuffle\nr,c = df.shape\ndf1 = df.sample( r )\n\n# view 10 first listed\ndf1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize data set label values\nplotPerColumnDistribution(df1, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"# 2. sample and explore data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = 5000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"- TODO :\n-- (in)exclude numbers\n-- exclude words if len(words) > ##","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_special_char(text) : \n    cleaned = text\n    cleaned = re.sub(r'[\\n]+', ' ', cleaned)\n    cleaned = re.sub(r'\\x91\\x92\\x96', '', cleaned)\n    \n    # substitute anything not ([^...]) alpha (A-Za-z) with '' \n    cleaned = re.sub('[^A-Za-z ]+', '', cleaned)\n    # OR\n    # substitute anything not ([^...]) alpha (A-Za-z) numeric (0-9) with ''\n    # cleaned = re.sub('[^A-Za-z0-9 ]+', '', cleaned)\n    \n    return cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gets unique words from sampled text\ndef get_unique_words(df, field, num_words=1000, verbose=False) :\n\n    # get texts\n    data_word_lists = df[field].apply(lambda x : np.unique(x.split()[:num_words])).values\n\n    # get cleaned words\n    data_word_list = [  clean_special_char(y) for x in data_word_lists for y in x]\n    # verify\n    if verbose : print( 'total words found:', len(data_word_list) )\n\n    # get unique words\n    unique_words = np.unique(np.array(data_word_list))\n    print('unique words found : ', len(unique_words))\n    \n    # verify\n    if verbose : print( '100 of the unique words :', unique_words[:100] )\n    \n    return data_word_lists, data_word_list, unique_words\n\n# bar graph of number of occurances of set of 'words' in list of list of title/text words\ndef bar_word_occ(text, words, occ_threshold) :\n    \n    # get number of occurances of unique words (not including spaces/blanks)\n    wrd_cnt = [ [x, text.count(x)] for x in words if text.count(x) > occ_threshold if x != '']\n\n    # extract words and counts\n    wrd = np.array([ t[0] for t in wrd_cnt ])\n    cnt = np.array([ t[1] for t in wrd_cnt ])\n\n    # get sort order by count\n    sort_order = np.argsort(cnt)\n\n    # get ordered lists of words and counts\n    w = wrd[sort_order]\n    c = cnt[sort_order]\n\n    # plot it\n    plt.figure(figsize=(70,8))\n    plt.bar(w, c)\n    \n    # labels\n    plt.title('Real/Fake News Word Occurance (>' + str(occ_threshold) + ')' )\n    \n# histogram of the frequency of unique words in passed in list of lists\ndef hist_text_lengths(lolist, title_prefix='') :\n    # get word counts\n    word_lengths = np.array([ len(x) for x in lolist ] )\n\n    # # verify\n    # print( 'first 100\\'s word count :', word_lengths[:100])\n        \n    twlhist = plt.hist(word_lengths)\n    plt.title(title_prefix + 'Word Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1. what real news looks like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample real news entries\nreals = df1[df1.label=='REAL'][['title', 'text']].iloc[:sample_size]\nreals","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.1 what real news titles look like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('real news title - unique words')\nreal_titles, real_title_words, real_title_uniques = get_unique_words(reals, 'title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar_word_occ(real_title_words, real_title_uniques, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_text_lengths(real_titles, 'Real Title ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2. what real news text looks like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('real news text - unique words')\nreal_texts, real_text_words, real_text_uniques = get_unique_words(reals, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To reduce computational complexity : \n# sample onlyt 2000 texts from the original sample\nbar_word_occ(real_text_words[:2000], real_text_uniques, 5)\n\n# get actual word count for sample (requires lots of computation)\n# bar_word_occ(real_text_words, real_text_uniques, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_text_lengths(real_texts, 'Real Texts ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. what fake news looks like","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1 what fake news titles look like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fakes = df1[df1.label=='FAKE'][['title', 'text']]\nfakes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('fake news title - unique words')\nfake_titles, fake_title_words, fake_title_uniques = get_unique_words(fakes, 'title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get number of occurances of unique words\nbar_word_occ(fake_title_words, fake_title_uniques, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_text_lengths(fake_titles, 'Fake Titles ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2. what fake news text looks like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('fake news text - unique words')\nfake_texts, fake_text_words, fake_text_uniques = get_unique_words(fakes, 'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_text_lengths(fake_texts, 'Fake Texts ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3. fake news exclusive words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# real vs fake title uniques\nfakes_title_exclusives = [ x for x in fake_title_uniques if x not in real_title_uniques ]\nprint( 'fake title exclusive unique words found: {}'.format(len(fakes_title_exclusives)) )\nprint( '100 of the fake title exclusive unique words :', fakes_title_exclusives[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# real vs fake text uniques\nfakes_text_exclusives = [ x for x in fake_text_uniques if x not in real_text_uniques ]\nprint( 'fake text exclusive words found:', len(fakes_text_exclusives))\nprint( '100 of the fake text exclusive words :', fakes_text_exclusives[:100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Model\nusing SciKitLearn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make dictionary of unique words and [0,1] or [0, len(unique words)] index words \ndef make_uniques_dict(word_dictionary, normalized=False) :\n    \n    wdic = {}\n    i = 0\n    d = len(word_dictionary)\n\n    for w in word_dictionary :\n        \n        value = i/d if normalized else d\n        \n        wdic.update({w: i})\n        i += 1\n        \n    return wdic\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_one_hot(news_feat, dic, verbose=False) :\n    \n    x_matrix = np.zeros((len(news_feat), len(dic)))\n\n    for t in range(len(news_feat)) :\n        for w in news_feat[t] :\n    \n            w = clean_special_char(w)\n            \n            try :\n                x_matrix[t, dic[w] ] = 1\n            except:\n                x_matrix[t, 0 ] = 1\n                    \n        if verbose : print(t)\n    \n    return x_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get target values\ny_values = np.array(df.label.apply(lambda x : 1 if x==\"REAL\" else 0).values)\n\n# split\nx_train, x_test, y_train, y_test = train_test_split(df[['title', 'text']], y_values, test_size=0.2, random_state=3611)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1. predict with title only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1.1. encode","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get title words and uniques\ntitles, title_words, title_uniques = get_unique_words(x_train, 'title')\n\n# build title unique word dictionary\ntw_dic = make_uniques_dict(title_uniques)\n\n# encode\nx_train_titles = encode_one_hot(titles, tw_dic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # verify\n# print('num records\"', len(x_titles) )\n# print('first ten titles and [01] encoded titles:')\n# # verify\n# for i in x_titles[:10] :\n# #     print(titles[00], end=' : ')\n#     for j in i :\n#         if j > 0 : \n#             print(np.round(j, 3), end=', ')\n#     print('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1.2. train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make classifier\n# X, y = make_classification(n_samples=100, random_state=1)\n# x_train, x_test, y_train, y_test = train_test_split(x_titles, y_, test_size=0.2, random_state=3611)\n\nstime = time.process_time()\nclf = MLPClassifier(random_state=1, batch_size=(2110), max_iter=1000, verbose=True ).fit(x_train_titles, y_train)\n# clf = MLPClassifier(random_state=1, hidden_layer_sizes=(1000), batch_size=(2110), max_iter=1000, verbose=True ).fit(x_train, y_train)\nprint('proc time:', time.process_time() - stime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.1.3. test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#test\n\n# assume new titles are unknown\ntitles, title_words, title_uniques = get_unique_words(x_test, 'title')\n\n# do not build text unique word dictionary. use training set dictionary.\n\n# encode\nx_test_titles = encode_one_hot(titles, tw_dic)\n\n# test (with unknowns)\nclf.score(x_test_titles, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. predict with text\\[:300\\] only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.2.1. encode","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To reduce computational complexity : \n# use (maximum) only the first 100 words of a news text\nx_train['text_cropped'] = x_train['text'].apply(lambda x : x[:300])\n\n# build text word dictionary\ntexts, texts_words, texts_uniques = get_unique_words(x_train, 'text_cropped')\n\n# build text unique word dictionary\nxw_dic = make_uniques_dict(texts_uniques)\n\n# encode\nx_train_text = encode_one_hot(texts, xw_dic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2.2. train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stime = time.process_time()\nclf = MLPClassifier(random_state=1, batch_size=(100), max_iter=1000, verbose=True ).fit(x_train_text, y_train)\nprint('proc time:', time.process_time() - stime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.2.3. test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To reduce computational complexity : \n# use (maximum) only the first 100 words of a news text\nx_test['text_cropped'] = x_test['text'].apply(lambda x : x[:300])\n\n# assume new titles are unknown\ntexts, texts_words, texts_uniques = get_unique_words(x_test, 'text_cropped')\n\n# do not build text unique word dictionary. use training set dictionary.\n\n# encode\nx_test_texts = encode_one_hot(texts, xw_dic)\n\n# test\nclf.score(x_test_texts, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. predict with title+text_words\\[:100\\] only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.3.1. encode","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatonate news title and text to new column 'titletext'\nx_train['titletext'] = x_train.title + ' ' + x_train.text\n# df1['titletext'] = df1.title.apply(lambda x : x.split()) + df1.text.apply(lambda x : x.split()[:100])\n\n# build title+text word dictionary\ntitlestexts, titlestexts_words, titlestexts_uniques = get_unique_words(x_train, 'titletext', 100)\n\n# build title+text unique word dictionary\ntitlestextsw_dic = make_uniques_dict(titlestexts_uniques)\n\n# encode\nx_train_titlestexts = encode_one_hot(titlestexts, titlestextsw_dic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.2. train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stime = time.process_time()\nclf = MLPClassifier(random_state=1, batch_size=(2110), max_iter=1000, verbose=True ).fit(x_train_titlestexts, y_train)\n# clf = MLPClassifier(random_state=1, hidden_layer_sizes=(1000), batch_size=(2110), max_iter=1000, verbose=True ).fit(x_train, y_train)\nprint('proc time:', time.process_time() - stime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.3.3. test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatonate news title and text to new column 'titletext'\nx_test['titletext'] = x_test.title + ' ' + x_test.text\n\n# assume new titles are unknown\ntitlestexts, titlestexts_words, titlestexts_uniques = get_unique_words(x_test, 'titletext')\n\n# do not build text unique word dictionary. use training set dictionary.\n\n# encode\nx_test_titlestexts = encode_one_hot(titlestexts, titlestextsw_dic)\n\nclf.score(x_test_titlestexts, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}