{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import *\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\n\n%matplotlib inline\n\nfrom keras.preprocessing.text import Tokenizer,  text_to_word_sequence\nfrom keras.engine.topology import Layer\nfrom keras import initializers as initializers, regularizers, constraints\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Dropout\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras.models import Model\nimport sys\nfrom sklearn.metrics import roc_auc_score\nfrom nltk import tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/mbti-type/mbti_1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords \nfrom nltk import word_tokenize\n\nstemmer = PorterStemmer()\nlemmatiser = WordNetLemmatizer() \ncachedStopWords = stopwords.words(\"english\")\n\ndef pre_process_data(data, remove_stop_words=True):\n    list_posts = []\n    len_data = len(data)\n    i=0   \n    for row in data.iterrows():\n        i+=1\n        if i % 500 == 0:\n            print(\"%s | %s rows\" % (i, len_data))\n        posts = row[1].posts\n        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', posts) #remove urls\n        temp = re.sub(\"[^a-zA-Z.]\", \" \", temp) #remove all punctuations except fullstops \n        temp = re.sub(' +', ' ', temp).lower() \n        temp=re.sub(r'\\.+', \".\", temp) #remove multiple fullstops\n        if remove_stop_words:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n        else:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n        list_posts.append(temp)\n\n    text = np.array(list_posts)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text = pre_process_data(data, remove_stop_words=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['clean_text']=clean_text\ndata = data[['clean_text', 'type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types=data['type']\ntext=data['clean_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tps=data.groupby('type')\nprint(\"total types:\",tps.ngroups)\nprint(tps.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paras = []\nlabels = []\ntexts = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_lens = []  # will contain lengths of all sentences in complete dataset\nsent_nums = []  # will contain number of sentences in each instance \nfor idx in range(data.clean_text.shape[0]):\n    temp_text = data.clean_text[idx]\n    texts.append(temp_text)\n    sentences = tokenize.sent_tokenize(temp_text)\n    valid_sentences=[]  \n    for sent in sentences:\n        temp_len=len(text_to_word_sequence(sent))\n        if ((temp_len>8) and (temp_len<16)):\n            valid_sentences.append(sent) \n    sent_nums.append(len(valid_sentences))\n    for valid_sent in valid_sentences:\n        sent_lens.append(len(text_to_word_sequence(valid_sent)))\n    paras.append(valid_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features=200000 # maximum number of unique words that should be included in the tokenized word index\nmax_senten_len=16   # maximum number of words in each sentence\nmax_senten_num=6  # maximum number of sentences in one document\nembed_size=100      # vector size of word embedding\nVALIDATION_SPLIT = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features, oov_token=True)\ntokenizer.fit_on_texts(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using_instances=1000\ndata_2 = np.zeros((1000, max_senten_num, max_senten_len), dtype='int32')\nfor i, sentences in enumerate(paras[:1000]):\n    for j, sent in enumerate(sentences):\n        if j< max_senten_num:\n            wordTokens = text_to_word_sequence(sent)\n            k=0\n            for _, word in enumerate(wordTokens):\n                if k<max_senten_len and tokenizer.word_index[word]<max_features:\n                    data[i,j,k] = tokenizer.word_index[word]\n                    k=k+1\n                else:\n                    print(word)\n                    pass\n    print(\"Indexing done for Instance \"+str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Total %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = pd.get_dummies(types[:1000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of data tensor:', data_2.shape)\nprint('Shape of labels tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This implementation is taken from Github Repository DeepResearch, Author: Heet Sankesara\n\ndef dot_product(x, kernel):\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        \n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n                'W_regularizer': self.W_regularizer,\n                'u_regularizer': self.u_regularizer,\n                'b_regularizer': self.b_regularizer,\n                'W_constraint': self.W_constraint,\n                'u_constraint': self.u_constraint,\n                'b_constraint': self.b_constraint,\n                'bias': self.bias,\n        })\n        return config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.arange(data_2.shape[0])\nnp.random.shuffle(indices)\ndata_2 = data_2[indices]\nlabels = labels.iloc[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data_2.shape[0])\n\nx_train = data_2[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data_2[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]\nprint('Number of positive and negative reviews in traing and validation set')\nprint(y_train.columns.tolist())\nprint(y_train.sum(axis=0).tolist())\nprint(y_val.sum(axis=0).tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"REG_PARAM = 1e-13\nl2_reg = regularizers.l2(REG_PARAM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nGLOVE_DIR = \"../input/glove-6b-100d/glove.6B.100d.txt\"\nembeddings_index = {}\nf = open(GLOVE_DIR,encoding=\"utf8\")\nfor line in f:\n    try:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    except:\n        print(word)\n        pass\nf.close()\nprint('Total %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\nabsent_words = 0\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        absent_words += 1\nprint('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)), '% of total words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index) + 1,embed_size,weights=[embedding_matrix], input_length=max_senten_len, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_input = Input(shape=(max_senten_len,), dtype='float32')\nword_sequences = embedding_layer(word_input)\nword_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)\nword_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\nword_att = AttentionWithContext()(word_dense)\nwordEncoder = Model(word_input, word_att)\n\nsent_input = Input(shape=(max_senten_num, max_senten_len), dtype='float32')\nsent_encoder = TimeDistributed(wordEncoder)(sent_input)\nsent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\nsent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\nsent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\npreds = Dense(16, activation='softmax')(sent_att)\nmodel = Model(sent_input, preds)\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('1000_instances_model.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='auto') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, batch_size=512, callbacks=[checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('mbti_han.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}