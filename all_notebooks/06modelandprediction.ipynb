{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basic Statistical Models\n\nThis notebook demonstrates how to estimate a regression model with the module [statmodels][statmodels]. The techniques introduced here can also be applied using [scikit-learn](https://www.google.com/search?client=firefox-b-d&q=scikit-learn), one of the more popular Python modules for machine learning. \n\n\n## Linear Regression\n\nThe basic idea of a linear regression model is a model of the form \n\n$$ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_K x_{Ki} + u_i \\qquad i = 1, \\ldots, N $$\n\nwhere $y_i$ denotes the dependent (or response) variables and $x_{ki}$ denotes the $k^{th}$ covariate or explanatory variable with $k=1,\\ldots, K$. \n\nGiven the data of the response and the co-variate, the aim is to obtain some estimates of the coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_K$. A standard method of estimate these coefficients is the **Ordinary Least Squares Estimator** (OLS). \n\nUpon obtaining the estimate of each coefficient, denotes $\\hat{\\beta}_{k}$, $k=0,\\ldots,K$, we can make a prediction of $y_{i+1}$ by \n\n$$ \\hat{y}_{j} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1j} + \\hat{\\beta}_2 x_{2j} + \\cdots + \\hat{\\beta}_K x_{Kj}, \\qquad j=N+1,\\ldots,  $$\n\nWe won't go through the mathemtics here but for a comprehensive treatment of Linear Regression model see [here](https://www.ssc.wisc.edu/~bhansen/econometrics/). Instead, we will focus on how to use [statmodels][statmodels].\n\n\n[statmodels]: https://www.statsmodels.org/stable/index.html\n\n## Economic Return to Schooling \n\nThe example we will be using to demonstrate this is based on the paper [Z. Griliches (1976)](https://www.jstor.org/stable/1831103?seq=1#metadata_info_tab_contents) which examined the economic return to formal schooling. \n\nFirst we import the necessary modules and the data ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport statsmodels.api as sm \nimport sklearn.linear_model as skl\nimport sklearn.metrics as skm \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [12,9]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T02:58:05.730313Z","iopub.execute_input":"2021-06-02T02:58:05.730898Z","iopub.status.idle":"2021-06-02T02:58:07.459948Z","shell.execute_reply.started":"2021-06-02T02:58:05.730769Z","shell.execute_reply":"2021-06-02T02:58:07.45903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"We next import the data","metadata":{}},{"cell_type":"code","source":"school = pd.read_csv('../input/schooling/griliches.csv', header=0)\nschool.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:58:07.461198Z","iopub.execute_input":"2021-06-02T02:58:07.46151Z","iopub.status.idle":"2021-06-02T02:58:07.502772Z","shell.execute_reply.started":"2021-06-02T02:58:07.461449Z","shell.execute_reply":"2021-06-02T02:58:07.501763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is a brief data dictionary:\n\n1. $year_i$: year of observatrion. This is the base year for when the individual $i$ entered the sample. \n2. $rns_i$: dummy variable, residency in South.\n3. $mrt_i$: dummy variable, martial status (=1 if married). \n4. $smsa_i$: dummy variable, reside metro area. \n5. $med_i$: Mother's education measured in years. \n6. $iq_i$: IQ score. \n7. $kww_i$: Knowledge of the World of Work test. \n8. $age_i$: age in years. \n9. $s_i$: completed years of schooling.\n10. $expr_i$: work experience measured in years. \n11. $tenure_i$: tenure measured in years. \n12. $lw_i$: log of wage. \n\nVariables with $80$ as suffix measure the same variable again in 1980 for the same individauls. It can be seen as *follow up* measurements. \n\nThe linear regression model considered is \n\n$$ lw_i = \\beta_0 + \\beta_1 rns_i + \\beta_2 mrt_i + \\beta_3 smsa_i + \\beta_4 iq_i + \\beta_5 kww_i + \\beta_6 age_i + \\beta_7 s_i + \\beta_8 expr_i + \\beta_9 tenure_i + u_i. $$\n\n**Exercise:** <a style=\"color:red\">IMPORTANT</a> Conduct some preliminary Exploratory Data Analysis before proceeding to the regression estimation. What types of EDA do you think are appropriate in this case? ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To estimate the coefficients, we first need to identity the response variable and the covariate","metadata":{}},{"cell_type":"code","source":"covariate = ['rns', 'mrt', 'smsa', 'iq', 'kww', 'age', 's', 'expr', 'tenure']\ny = school['lw']\n#X = school.loc[:, covariate] or\nX = school[covariate]\nX = sm.add_constant(X) # We want to include the intercept","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:07:00.260604Z","iopub.execute_input":"2021-06-02T03:07:00.260952Z","iopub.status.idle":"2021-06-02T03:07:00.274378Z","shell.execute_reply.started":"2021-06-02T03:07:00.260919Z","shell.execute_reply":"2021-06-02T03:07:00.273375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given these, we can then get *statsmodels* to estimate the coefficients. \n\nWe split the process here into three steps:\n\n1. We specify the model\n2. We estimate (fit) the model\n3. We use the data with 80 prefix for prediction. ","metadata":{}},{"cell_type":"code","source":"model = sm.OLS(y,X) #specify the model\nest = model.fit() #estimate (fit) the coefficients\nest.summary() #disply the result","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:08:17.020744Z","iopub.execute_input":"2021-06-02T03:08:17.021686Z","iopub.status.idle":"2021-06-02T03:08:17.095085Z","shell.execute_reply.started":"2021-06-02T03:08:17.021642Z","shell.execute_reply":"2021-06-02T03:08:17.093673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"testcovariate = ['rns80', 'mrt80', 'smsa80', 'iq', 'kww', 'age80', 's80', 'expr80', 'tenure80']\ntest = school.loc[:, testcovariate]\ntest = sm.add_constant(test)\npredict80 = model.predict(params=est.params, exog=test)\npredict80","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:17:34.021655Z","iopub.execute_input":"2021-06-02T03:17:34.022196Z","iopub.status.idle":"2021-06-02T03:17:34.040993Z","shell.execute_reply.started":"2021-06-02T03:17:34.022161Z","shell.execute_reply":"2021-06-02T03:17:34.04023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add the predicted values into the dataframe","metadata":{}},{"cell_type":"code","source":"school['predicted80'] = predict80","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:17:52.09257Z","iopub.execute_input":"2021-06-02T03:17:52.093059Z","iopub.status.idle":"2021-06-02T03:17:52.09835Z","shell.execute_reply.started":"2021-06-02T03:17:52.093027Z","shell.execute_reply":"2021-06-02T03:17:52.097396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting predicted values against actual ","metadata":{}},{"cell_type":"code","source":"school.plot.scatter(y='lw80', x='predicted80')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:18:08.352323Z","iopub.execute_input":"2021-06-02T03:18:08.352661Z","iopub.status.idle":"2021-06-02T03:18:08.71482Z","shell.execute_reply.started":"2021-06-02T03:18:08.352634Z","shell.execute_reply":"2021-06-02T03:18:08.71411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three criteria are typically used to measure predictive performance namely Mean Squares Error (MSE), Mean Absolute Error and Mean Absolute Percentage Error. They are defined as \n\n$$ \n\\begin{align}\n     MSE =& \\frac{1}{N} \\sum^N_{i=1} ( \\hat{y}_i - y_i )^2 \\\\\n    MAE =& \\frac{1}{N} \\sum^N_{i=1} | \\hat{y}_i - y_i | \\\\\n    MAPE =& \\frac{1}{N} \\sum^N_{i=1} \\frac{| \\hat{y}_i - y_i |}{y_i} \n\\end{align}\n$$\n\nwhere $N$ is the number of prediction. \n\nThe *metrics* submodule in scikit-learn can generate all these. ","metadata":{}},{"cell_type":"code","source":"mse = skm.mean_squared_error(school['lw80'], school['predicted80'])\nmad = skm.mean_absolute_error(school['lw80'], school['predicted80'])\nmape = skm.mean_absolute_percentage_error(school['lw80'], school['predicted80'])\nmse, mad, mape","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:20:27.7915Z","iopub.execute_input":"2021-06-02T03:20:27.791847Z","iopub.status.idle":"2021-06-02T03:20:27.801476Z","shell.execute_reply.started":"2021-06-02T03:20:27.791819Z","shell.execute_reply":"2021-06-02T03:20:27.800771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For readability, we can put these criteria into a dataframe","metadata":{}},{"cell_type":"code","source":"error = pd.DataFrame([mse, mad, mape], index=['MSE', 'MAD', 'MAPE'], columns=['error'])\nerror","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:21:26.426071Z","iopub.execute_input":"2021-06-02T03:21:26.426719Z","iopub.status.idle":"2021-06-02T03:21:26.44574Z","shell.execute_reply.started":"2021-06-02T03:21:26.426676Z","shell.execute_reply":"2021-06-02T03:21:26.444891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function\n\nThis section can be skipped but it would be useful if you wish to learn how to automate a sequence of actions in Python by definining a *function*. \n\nLet's say we need to calculate these errors over and over again for different models. We may not necessarily want to cut-and-paste the code above over and over again. Ideally we want to *automate* this process up to a point. Put all these into a function is one way to achieve this.\n\nThe idea of a function is that you provide certain inputs (or arguments) and Python will manipulate those inputs and produce some outputs. We have been utilising various functions already, but we are yet to introduce how to create one for yourself. \n\nThe basic syntax to define a function is by writting \n\ndef functioname(list of arguments) \n\n    \"\"\"\n    Some doc-strings to explain what the function does and what inputs and outputs should the user expect. \n    \"\"\"\n\n    *what code you want Python to perform.*\n    \n    ......\n    \n    *return something*\n    \nNote that indentation is important in Python. The indented lines are the actions which you wish Python to perform. The function definition ends when you no longer indenting the line. \n\nFor example, if we wish to develop a function that takes the actual and predicted values from above and returns a dataframe that contains MSE, MAE and MAPE, we can consider ","metadata":{}},{"cell_type":"code","source":"def getForecastCriteria(y, yhat):\n    \"\"\"\n    A fucntion that returns MSE, MAE and MAPE based on the actual and predicted values. The function requires scikit-learn module.\n    Inputs:\n        y: (T,) array containting the actual values of the variable. \n        yhat: (T,) array containing the predicted values of the variable. \n    Output:\n        fcTable: a dataframe containing the MSE, MAE and MAPE. \n    \n    \"\"\"\n    mse = skm.mean_squared_error(y, yhat)\n    mad = skm.mean_absolute_error(y, yhat)\n    mape = skm.mean_absolute_percentage_error(y, yhat)\n    fcTable = pd.DataFrame([mse, mad, mape], index=['MSE', 'MAD', 'MAPE'], columns=['error'])\n    return fcTable","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:24:29.558656Z","iopub.execute_input":"2021-06-02T03:24:29.559019Z","iopub.status.idle":"2021-06-02T03:24:29.56444Z","shell.execute_reply.started":"2021-06-02T03:24:29.558977Z","shell.execute_reply":"2021-06-02T03:24:29.563534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if the function works. ","metadata":{}},{"cell_type":"code","source":"getForecastCriteria(school['lw80'], school['predicted80'])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:24:40.349517Z","iopub.execute_input":"2021-06-02T03:24:40.349838Z","iopub.status.idle":"2021-06-02T03:24:40.361282Z","shell.execute_reply.started":"2021-06-02T03:24:40.34981Z","shell.execute_reply":"2021-06-02T03:24:40.360243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now, you can utilise the getForecastCriteria function over and over again for different response and/or predicted values. ","metadata":{}},{"cell_type":"markdown","source":"## Discrete outcome \n\nLinear Regression Model is great when the response variable is continuous (and unbounded). When the response variable is binary or discrete, such as Male or Female, Yes or no, or contain several choices, such as transportation choices, then standard linear regression model may not be the most appropriate approach.\n\nTo demonstrate the idea, let's consider the following scenario. \n\nA retail bank would like to hire you to build a credit default model for their credit card portfolio. The bank expects the model to identify the consumers who will default on their credit card payments over the next 12 months and as such reduce their losses. The bank is willing to provide you with the data (credit_data.txt) that they can currently extract from their systems. This data set consists of 13,444 records with 14 fields. In addition to the default indicator for each observation, the remaining fields capture customer attributes and their credit history. A brief data dictionary can be found below:\n\n  - Cardhldr = Dummy variable, 1 if application for credit card accepted, 0 if not\n  - Default = 1 if defaulted 0 if not (observed when Cardhldr = 1, 10,499 observations),\n  - Age = Age in years plus twelfths of a year,\n  - Adepcnt = 1 + number of dependents,\n  - Acadmos = months living at current address,\n  - Majordrg = Number of major derogatory reports,\n  - Minordrg = Number of minor derogatory reports,\n  - Ownrent = 1 if owns their home, 0 if rent\n  - Income = Monthly income (divided by 10,000),\n  - Selfempl = 1 if self employed, 0 if not,\n  - Inc_per = Income divided by number of dependents,\n  - Exp_Inc = Ratio of monthly credit card expenditure to yearly income,\n  - Spending = Average monthly credit card expenditure (for Cardhldr = 1),\n  - Logspend = Log of spending.\n  \nIn this case, the response variable, *Default*, is a binary variable which value can only be either 0 or 1. So one way to approach this is to model the *log odd ratio* rather than the variable itself. The log odd ratio is \n\n$$z_i =  \\log \\frac{\\Pr (Default_i = 1) }{\\Pr (Default_i=0)} $$\n\nwhere $\\Pr(A)$ denotes the probability of an event $A$ to occur. The main idea is that while $Defualt$ is a binary variable, the log-odd ratio $z_i$ is a continuous variable ranges from $-\\infty$ to $\\infty$. To see this note\n\n1. $\\Pr (Default_i = 1)$ ranges from 0 to 1. \n2. $\\Pr (Default_i = 1)/\\Pr (Default_i = 0)$ ranges from 0 to $\\infty$\n3. $\\log \\Pr (Default_i = 1)/\\Pr (Default_i = 0)$ ranges from $-\\infty$ to $\\infty$\n\nSince $z_i$ is continuous and not bounded, we can then model $z_i$ as a linear model similar to the regression model, that is, \n\n$$z_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_K x_{Ki}$$.\n\nThe objective is therefore to obtain the coefficients from data $\\beta_0, \\ldots, \\beta_K$. \n\nWe will demonstrate how to estimate these coefficients using [statsmodels](https://www.statsmodels.org/stable/index.html). First we import the data. \n\nNote that in this data, there are *missing values*, so we need to tell Pandas how missing values were represented using the input argument *na_values*. In this particular dataset, the missing values are simply empty entries without any whitespace. ","metadata":{}},{"cell_type":"code","source":"credit = pd.read_csv(\"../input/credit/creditData.csv\", header=0, na_values='')\ncredit.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:30:55.153201Z","iopub.execute_input":"2021-06-02T03:30:55.153579Z","iopub.status.idle":"2021-06-02T03:30:55.215376Z","shell.execute_reply.started":"2021-06-02T03:30:55.153545Z","shell.execute_reply":"2021-06-02T03:30:55.214266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exercise** Please conduct some Exploratory Data Analysis before proceeding to the estimation of a logit model. \n\nNote that we only want those with a credit card i.e. CARDHLDR=1 and we also need to drop all the rows with missing values for purposes of estimation. \n\n<a style=\"color:red\">IMPORTANT NOTE:</a> Dropping missing values is not always the appropriate method. There is a literature on how to handle missing values based on the objectives of the analysis. This includes the possibility of actually estimating the missing values using [Expectation-Maximisation](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) (EM) algorithm. The detials are beyond the scope of this notebook, so we simply just drop them for now. \n\nThe method *dropna* can be used to drop the missing values from the dataframe. ","metadata":{}},{"cell_type":"code","source":"creditClean = credit.loc[credit['CARDHLDR']==1,:].dropna()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:33:06.158939Z","iopub.execute_input":"2021-06-02T03:33:06.159301Z","iopub.status.idle":"2021-06-02T03:33:06.18994Z","shell.execute_reply.started":"2021-06-02T03:33:06.159273Z","shell.execute_reply":"2021-06-02T03:33:06.18895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code above essentially does two things sequentially. The first '.' extracts the individuals with credit card using *conditional slicing* and the second '.' removes the missing values. \n\nThe next step is to specify our model. We follow the steps below\n\n1. Identify and extract the data of the response (endogenous) variable. \n2. Identify and extract the data of the co-variate (exogenous variables). \n3. Add a constant term to the dataframe of the covariates. \n4. Specify our logit model. \n4. Estimate (fit) our logit model. ","metadata":{}},{"cell_type":"code","source":"y = creditClean['DEFAULT'] # define the response variable.\ncredit_covariates = creditClean.columns[2:-1] #Grab all the covariates except for the first two columns.\nX = creditClean[credit_covariates] # define the covariates. \nX = sm.add_constant(X) # adding a constant\nModel0 = sm.Logit(endog=y, exog=X) # Define a logit model. \nModel0_fit = Model0.fit() #estimate (fit) our model. ","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:36:00.420595Z","iopub.execute_input":"2021-06-02T03:36:00.420916Z","iopub.status.idle":"2021-06-02T03:36:00.501942Z","shell.execute_reply.started":"2021-06-02T03:36:00.420888Z","shell.execute_reply":"2021-06-02T03:36:00.500838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To show our result, we can use the *summary* method. ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T03:59:07.065705Z","iopub.execute_input":"2021-05-26T03:59:07.06608Z","iopub.status.idle":"2021-05-26T03:59:07.073026Z","shell.execute_reply.started":"2021-05-26T03:59:07.06605Z","shell.execute_reply":"2021-05-26T03:59:07.07129Z"}}},{"cell_type":"code","source":"Model0_fit.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:36:23.826052Z","iopub.execute_input":"2021-06-02T03:36:23.826401Z","iopub.status.idle":"2021-06-02T03:36:23.93115Z","shell.execute_reply.started":"2021-06-02T03:36:23.826373Z","shell.execute_reply":"2021-06-02T03:36:23.93018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}