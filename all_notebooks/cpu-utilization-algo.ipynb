{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"95915776039525fd803970bbdc2991a59a1d5c54","_cell_guid":"962a601f-605d-4096-a95c-8dab9caecd2d","_execution_state":"idle"},"execution_count":null,"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n# convert an array of values into a dataset matrix\n\nnp.random.seed(7)\n\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n\ndataframe = pd.read_csv('../input/cpu-data-cleaned/CPU-Data.csv', usecols=[1])\n\ndataset = dataframe.values\ndataset = dataset.astype('float32')\n# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n\nlook_back = 1\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n\n# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(64,activation ='tanh', input_shape=(1, look_back), return_sequences=True))\nmodel.add(LSTM(64, activation ='tanh', input_shape=(1, look_back), return_sequences=False))\n#model.add(LSTM(32,activation ='tanh', input_shape=(1, look_back), return_sequences=False))\nmodel.add(Dense(1, activation ='relu',))\nmodel.compile(loss='mean_squared_error', optimizer='RMSProp', metrics=['accuracy'])\nmodel.fit(trainX, trainY, epochs=20, batch_size=32, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n\n# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()\n","outputs":[]}],"nbformat_minor":1}