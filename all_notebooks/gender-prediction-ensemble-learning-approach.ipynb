{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Predicting gender with ensemble learning approach: VottingClassifier**","metadata":{}},{"cell_type":"markdown","source":"![](https://scx2.b-cdn.net/gfx/news/hires/2018/gender.jpg)","metadata":{}},{"cell_type":"markdown","source":"# ***== Import libraries ==***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix, roc_curve, auc\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***== Data reading & cleaning ==***","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/gender-classification-dataset/gender_classification_v7.csv')\nprint(data.shape)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the 'gender' variable: 0 for 'Male & 1 for 'Female'\ncode = {'Male':0, 'Female':1}\ndata['gender'] = data['gender'].map(code)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be observed, most of the features are actually categorical, only 'forehead_width_cm' and 'forehead_height_cm' are numerical. ","metadata":{}},{"cell_type":"code","source":"# Turn the features into the right data type\ncategories = [i for i in data.columns if data[i].dtype == 'int64']\nfor i in categories:\n  data[i] = data[i].astype('category')\n\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Investigate the number of unique values in each column\nfor i in data.columns:\n  print(f'The column \"{i}\" has {len(data[i].value_counts())} unique values.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***== Exploratory data analysis (EDA) ==***","metadata":{}},{"cell_type":"code","source":"print(data['gender'].value_counts())\n\npie, ax = plt.subplots(figsize=[15,10])\nlabels = [1, 0]\ncolors = ['#7b77ff', '#7df691']\nplt.pie(x = data['gender'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors)\nplt.title('Gender distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,12))\nmy_pall={1:'#7b77ff', 0:'#7df691'}\nsns.boxplot(x='gender', y=\"forehead_width_cm\", data=data, palette=my_pall)\nplt.title('Boxplot of forehead_width_cm by gender')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,12))\nmy_pall={1:'#7b77ff', 0:'#7df691'}\nsns.boxplot(x='gender', y=\"forehead_height_cm\", data=data, palette=my_pall)\nplt.title('Boxplot of forehead_height_cm by gender')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genders_diff = data.groupby('gender')[['forehead_width_cm','forehead_height_cm' ]].mean()\n\nlabels = ['forehead width', 'forehead height']\nm_means =[genders_diff.iloc[0, 0], genders_diff.iloc[0,1]]\nf_means = [genders_diff.iloc[1, 0], genders_diff.iloc[1,1]]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(12,8))\nrects1 = ax.bar(x - width/2, m_means, width, label='0', color = '#7df691')\nrects2 = ax.bar(x + width/2, f_means, width, label='1', color = '#7b77ff')\n\n\nax.set_ylabel('Mean')\nax.set_title(\"Forehead's MEAN width & height by gender\")\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"males = data[data['gender'] == 0]\nfemales = data[data['gender'] == 1]\n# HAIR comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Long', 'Not-Long']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['long_hair'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males hair type')\nax[1].pie(x = females['long_hair'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females hair type')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOSE WIDE comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Wide', 'Non-wide']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['nose_wide'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males wide / non-wide nose')\nax[1].pie(x = females['nose_wide'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females wide / non-wide nose')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOSE LONG comparison \npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Long', 'Non-long']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['nose_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males long / non-long nose')\nax[1].pie(x = females['nose_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females long / non-long nose')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LIPS comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Thin', 'Non-thin']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['lips_thin'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males thin / non-thin lips')\nax[1].pie(x = females['lips_thin'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females thin / non-thin lips')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LIPS-NOSE distance comparison\npie, ax = plt.subplots(1,2, figsize=[15,10])\nlabels = ['Long', 'Short']\ncolors_m = ['#35f154', '#adf9b9']\ncolors_f = ['#4a44ff', '#adaaff']\nax[0].pie(x = males['distance_nose_to_lip_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_m)\nax[0].set_title('Males distance between nose and lips')\nax[1].pie(x = females['distance_nose_to_lip_long'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors_f)\nax[1].set_title('Females distance between nose and lips')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***== Data preparation ==***","metadata":{}},{"cell_type":"code","source":"# Separate features and target\nfeatures = data.iloc[:, :-1]\ntarget = data.iloc[:, -1]\n\n# Split them into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 123, shuffle = True, stratify = target)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***== Modelling ==***","metadata":{}},{"cell_type":"markdown","source":"## Initialize classifiers and hyperparameters","metadata":{}},{"cell_type":"code","source":"# Initiate classifiers\nLR = LogisticRegression()\nSGDC = SGDClassifier()\nSVC = SVC()\nKNN = KNeighborsClassifier()\nDT = DecisionTreeClassifier()\n\n# Initiate hyperparameters for classifiers\nparam_LR = {'C':[0.01, 0.1, 1, 10], 'penalty':['l1', 'l2']}\nparam_SGDC = {'alpha':[0.01, 0.1, 1, 10], 'loss':['hinge', 'log'], 'penalty':['l1', 'l2']}\nparam_SVC = {'C':[0.01, 0.1, 1, 10], 'gamma':[0.01, 0.1, 1, 10]}\nparam_KNN = {'n_neighbors':[2,3,4,5,6]}\nparam_DT = {'criterion':['gini', 'entropy'], 'max_depth': [3,4,5,6], 'min_samples_leaf':[0.1, 0.5, 1, 1.5, 2]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters tuning","metadata":{}},{"cell_type":"code","source":"# Logistic egression\nsearch_LR = GridSearchCV(LR, param_LR)\nsearch_LR.fit(X_train, y_train)\nprint(f'Best CV params {search_LR.best_params_}')\nprint(f'Best CV accuracy {search_LR.best_score_}')\nprint(f'Test accuracy of best hypers {search_LR.score(X_test, y_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SGDClassifier\nsearch_SGDC = GridSearchCV(SGDC, param_SGDC)\nsearch_SGDC.fit(X_train, y_train)\nprint(f'Best CV params {search_SGDC.best_params_}')\nprint(f'Best CV accuracy {search_SGDC.best_score_}')\nprint(f'Test accuracy of best hypers {search_SGDC.score(X_test, y_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVC\nsearch_SVC = GridSearchCV(SVC, param_SVC)\nsearch_SVC.fit(X_train, y_train)\nprint(f'Best CV params {search_SVC.best_params_}')\nprint(f'Best CV accuracy {search_SVC.best_score_}')\nprint(f'Test accuracy of best hypers {search_SVC.score(X_test, y_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN\nsearch_KNN = GridSearchCV(KNN, param_KNN)\nsearch_KNN.fit(X_train, y_train)\nprint(f'Best CV params {search_KNN.best_params_}')\nprint(f'Best CV accuracy {search_KNN.best_score_}')\nprint(f'Test accuracy of best hypers {search_KNN.score(X_test, y_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DecisionTree\nsearch_DT = GridSearchCV(DT, param_DT)\nsearch_DT.fit(X_train, y_train)\nprint(f'Best CV params {search_DT.best_params_}')\nprint(f'Best CV accuracy {search_DT.best_score_}')\nprint(f'Test accuracy of best hypers {search_DT.score(X_test, y_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Learning","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Re-initiate the models with their best hyperparameters\nLR = LogisticRegression(C=0.1, penalty='l2')\nSGDC = SGDClassifier(alpha=0.01, loss='hinge', penalty='l2')\nSVC = SVC(C=10, gamma=0.1)\nKNN = KNeighborsClassifier(n_neighbors=5)\nDT = DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_leaf=2)\n\n# Define a list with tuples that contains classifier's name & classifier\nclassifiers = [('Logistic Regression', LR),\n               ('SGDClassifier', SGDC),\n               ('SVC', SVC),\n               ('KNN', KNN),\n               ('Decision Tree', DT)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c_name, c in classifiers:\n    c.fit(X_train, y_train)\n    preds = c.predict(X_test)\n    print(f'{c_name} accuracy: {accuracy_score(y_test, preds)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiate Voting Classifier\nVC = VotingClassifier(estimators=classifiers)\nVC.fit(X_train, y_train)\npreds = VC.predict(X_test)\nprint(f'Voting Classifier score: {accuracy_score(y_test, preds)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the ensemble learning, the accuracy was not improved.\n\nThe best performancce was achieved by the DecisionTreeClassifier.","metadata":{}},{"cell_type":"code","source":"DT.fit(X_train, y_train)\npreds = DT.predict(X_test)\nconfusion_matrix(y_test, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(DT, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of 1001 samples, the DecisionTree missclassified 27.\n","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = DT.predict_proba(X_test)\npred = probs[:,1]\nfpr, tpr, threshold = roc_curve(y_test, pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(12,8))\nplt.title('ROC')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}