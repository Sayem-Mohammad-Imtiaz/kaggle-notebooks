{"cells":[{"source":"# Kaggle\n\n## Human Activity Recogniton with Smartphones\n-----\n### Description\n\nThe Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.\n\n### Activities\n\nEach person performed six activities (WALKING, WALKING_UPSTAIRS,  WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist, more detail follow this [link](https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones).\n\nFirst we should load libraries necessary to work. \n\nDependencies:\n- Python 3\n- numpy \n- pandas \n- sklearn\n- jupyter notebook\n- matplotlib","metadata":{"_cell_guid":"3203b17a-d1ea-4eef-8edf-32d268d672fa","_uuid":"be785312941844f9fe475ec462f605ef9dbb9b5a"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"d53230d9-c97e-4ef9-a24a-6bc6c908c736","_uuid":"c5c314e93e58701065fc1381a8f774c3030ba62f","collapsed":true},"execution_count":null,"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time"},{"source":"Download the database from previous link and add to the folder where you will run this notebook. ","metadata":{"_cell_guid":"aa89fc68-d380-49c8-b988-58a495cc1bf4","_uuid":"4a72e05d239bbfe41672677c745aa1d9d5129ddd"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"550db63b-2def-4f79-80d5-4009f15c67ac","_uuid":"171580bdc563bf65cf1fcd4236cbf38c2e1e1e95"},"execution_count":null,"cell_type":"code","source":"test  = pd.read_csv(\"./test.csv\")\ntrain = pd.read_csv(\"./train.csv\")"},{"source":"In the documentation of the database, it describes a _561-feature_ vector with time and frequency domain variables. The last two colums of database are Subject ID and Activity. There are two approaches I can take for the model classification **user-dependent**, or **user-independent**. User-dependent train and test a model using data collect from one subject only and user-independet apply the model over data from various subjects. Dependent model can achieve good accuracy in classification but lack on generalization over new subjects data. I will train over all data so it will be a **_user-independent_** model. Therefore Subject column will be dropped and Activity column will be our labels for training and testing.","metadata":{"_cell_guid":"e08f1d73-f529-4c3f-bb2f-8c554130d411","_uuid":"c68a16f2a1424c9afdeaf50ae0a3c8f53a73329c"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"fd3f6d3f-4a8b-4b45-9b7e-17273d65637b","_uuid":"c71c1f2f307b1e5010b09bd1c8595341723c7699","collapsed":true},"execution_count":null,"cell_type":"code","source":"print(\"Number of features in Train : \", train.shape[1])\nprint(\"Number of records  in Train : \",train.shape[0])\nprint(\"Number of features in Test  : \",test.shape[1])\nprint(\"Number of records  in Test  : \",test.shape[0])\n\ntrainData  = train.drop(['subject','Activity'] , axis=1).values\ntrainLabel = train.Activity.values\n\ntestData  = test.drop(['subject','Activity'] , axis=1).values\ntestLabel = test.Activity.values\n\nprint(\"Train Data shape  : \",trainData.shape)\nprint(\"Train Label shape : \",trainLabel.shape)\nprint(\"Test Data  shape  : \",testData.shape)\nprint(\"Test Label shape  : \",testLabel.shape)\n\nprint(\"Label examples: \")\nprint(np.unique(trainLabel))"},{"source":"-----\nOne problem that appears is the data in Label are strings and should be categorical. I solve that with next lines\n\n-----\n","metadata":{"_cell_guid":"a781bcaf-cda3-4a7d-bd33-056a46619f40","_uuid":"3a9347659ca16d40caa659a095862ca30c0189c6"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"ebf93f8e-a42b-4a3e-93b1-ea5e048d0253","_uuid":"134c6b519af582da0dec0d0d2aaf9bc23800d45e","collapsed":true},"execution_count":null,"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn import utils\n\nltrain = preprocessing.LabelEncoder()\nltest = preprocessing.LabelEncoder()\n\ntrainLabel = ltrain.fit_transform(trainLabel)\ntestLabel  = ltest.fit_transform(testLabel)\n\nprint(np.unique(trainLabel))\nprint(np.unique(testLabel))\nprint(\"Train Label shape : \",trainLabel.shape)\nprint(\"Test Label shape  : \",testLabel.shape)\nprint(utils.multiclass.type_of_target(testLabel))"},{"source":"Originally a lot of features can give us a good classififer, but it can ended overfitting our model and not be able to generalize new data. One of the basic apporaches followed is feature reduction and there are a lot of options available, you can follow this link to [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) but there is a lot of material you can search on web.\n\n- SVM: A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n![Fig 1]( http://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_0011.png \"Logo Title Text 1\")\n\n- RFECV: A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.","metadata":{"_cell_guid":"f1866bff-51f6-4361-aef9-dfb700bf35f4","_uuid":"f4f71a8c0f3d1550f9dae14a2f06cef26291b1dd"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"18e52562-717f-40d0-a774-55195d1dbcd3","_uuid":"748bab100deb71141be2824bef4fdd3454532785","collapsed":true},"execution_count":null,"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.utils import shuffle"},{"outputs":[],"metadata":{"_cell_guid":"9badbda9-a570-44e9-9458-8e9b650e5d4a","_uuid":"761162e1d89c2fe5d3f4c5cc5d378092a6967826","collapsed":true},"execution_count":null,"cell_type":"code","source":"t0 = time.clock()\n# Create the RFE object and compute a cross-validated score.\nsvc = SVC(kernel=\"linear\")\n# The \"accuracy\" scoring is proportional to the number of correct\n# classifications\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(6),\n              scoring='accuracy')\n# Before training the data it is convenient to shuffle the data in training\nnp.random.seed(1)\nprint(\"Labels before Shuffle\",testLabel[0:5])\ntestData,testLabel = shuffle(testData,testLabel)\ntrainData,trainLabel = shuffle(trainData,trainLabel)\nprint(\"Labels after Shuffle\",testLabel[0:5])"},{"outputs":[],"metadata":{"_cell_guid":"3bcb4234-e67d-4b64-b927-55384c7e3f10","_uuid":"47a4c4ee35ff66c65ba4be5015746d1e1f21624a","collapsed":true},"execution_count":null,"cell_type":"code","source":"# train and fit data in the model\nrfecv.fit(trainData, trainLabel)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\nprint(\"Processing time sec \",time.clock() - t0)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()"},{"source":"After ~3500 seconds (~one hour in all train data) an SVM model have been trained optimizing the features in the database. Selecting these features can reduce the time of trainig of the model so instead of trainig with **561** features it is necessary only **373**. ","metadata":{"_cell_guid":"69f2f52b-2c61-480f-9451-2a7b8d23695a","_uuid":"1823c4e1ba77803062369b661eac3c2ee704878e"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"872775dc-ed77-46a9-bbf5-8045991f7937","_uuid":"2efc7a0ebc7ef5810da37292895a43e77a9dded5","collapsed":true},"execution_count":null,"cell_type":"code","source":"print('Accuracy of the SVM model on test data is ', rfecv.score(testData,testLabel) )\nprint('Ranking of features starting from the best estimated \\n',rfecv.ranking_)\n# if we mask the features to get only the best we get this\nbest_features = []\nfor ix,val in enumerate(rfecv.support_):\n    if val==True:\n        best_features.append(testData[:,ix])"},{"source":"## Visualize\n\nFrom previous result, model obtain an accuracy of **96.4%** using **373** features from database. Some previous result to these was Accuracy of _94%_ using _150_ features and _95%_ using _163_ features. Different choose of features will give us different results, so there is the work to be done. Now just a visualization of the correlation between some features, it can be changed inside the iloc functions.","metadata":{"_cell_guid":"beb6aabf-ebe8-46b8-a6f0-9cf0bbdd978c","_uuid":"01679725d9cb632484afb1b7398605cfd774c44e"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"4d5588bf-0aee-4228-adf0-79a64446fa9b","_uuid":"42aa079acbec8b7b696c1af7b8a143461dd9cdcd","collapsed":true},"execution_count":null,"cell_type":"code","source":"from pandas.tools.plotting import scatter_matrix\nvisualize = pd.DataFrame(np.asarray(best_features).T)\nprint(visualize.shape)\nscatter_matrix(visualize.iloc[:,0:5], alpha=0.2, figsize=(6, 6), diagonal='kde')"},{"source":"## Conclusion\n\nA classification model based on SVM, applied to statistical and frecuency features from \"Human Activity Recognition with Smartphones\" database. Accuracy  obtained an acceptable 96% in the test data. Options to improve and change the algoritm could be try different kernels on SVM usig sklearn or different feature extraction methods. Improving algorithms maybe can tell the difference between one subject and another. Also another option could be implement a batch learning process.","metadata":{"_cell_guid":"a409f0ca-c214-4f72-ad35-cb3dfe440b66","_uuid":"021befc94911773c2075f935f1342c4078f74d2b"},"cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"401e7c92-20fa-4e18-8193-1fd0cae0e498","_uuid":"4a4e819c5a9da4469e997c77906bd9c00af6a20b","collapsed":true},"execution_count":null,"cell_type":"code","source":""}],"metadata":{"language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","file_extension":".py","version":"3.6.1","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":2,"nbformat":4}