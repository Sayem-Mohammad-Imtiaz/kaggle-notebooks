{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome!","metadata":{}},{"cell_type":"markdown","source":"> whatever you study, you will never learn until you see how it works practically. It's very important to work on real-life example on whatever you are studying.","metadata":{}},{"cell_type":"markdown","source":"* In this notebook, i will try to walk you through a real machine learning project.","metadata":{}},{"cell_type":"markdown","source":"Steps in ML project.\n1. Get the data.\n2. Discover and visualize the data to gain insights.\n3. Prepare the data for Machine Learning algorithms.\n4. Select a model and train it.\n5. Fine-tune your model.\n6. Present your solution.","metadata":{}},{"cell_type":"markdown","source":"In this notebook we are using California's housing data to predict housing prices.\nAs you can see it's a regression task.\n\n# Get the Data","metadata":{}},{"cell_type":"code","source":"# Download the data\nimport os\nimport tarfile\nimport urllib.request\n\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.547501Z","iopub.execute_input":"2021-06-04T12:46:44.547927Z","iopub.status.idle":"2021-06-04T12:46:44.55953Z","shell.execute_reply.started":"2021-06-04T12:46:44.547838Z","shell.execute_reply":"2021-06-04T12:46:44.558667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fetch_housing_data()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.560953Z","iopub.execute_input":"2021-06-04T12:46:44.56125Z","iopub.status.idle":"2021-06-04T12:46:44.804986Z","shell.execute_reply.started":"2021-06-04T12:46:44.561212Z","shell.execute_reply":"2021-06-04T12:46:44.804026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.807417Z","iopub.execute_input":"2021-06-04T12:46:44.807844Z","iopub.status.idle":"2021-06-04T12:46:44.813597Z","shell.execute_reply.started":"2021-06-04T12:46:44.807798Z","shell.execute_reply":"2021-06-04T12:46:44.812414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a quick look at the data and it's stats.\nhousing= load_housing_data()\nhousing.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.815721Z","iopub.execute_input":"2021-06-04T12:46:44.816541Z","iopub.status.idle":"2021-06-04T12:46:44.881761Z","shell.execute_reply.started":"2021-06-04T12:46:44.81649Z","shell.execute_reply":"2021-06-04T12:46:44.880749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to get quick description of data.\nhousing.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.883591Z","iopub.execute_input":"2021-06-04T12:46:44.884018Z","iopub.status.idle":"2021-06-04T12:46:44.905077Z","shell.execute_reply.started":"2021-06-04T12:46:44.88397Z","shell.execute_reply":"2021-06-04T12:46:44.903942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. total_bedrooms attribute have missing values.\n2. ocen_proximity if a categorical attribute.","metadata":{}},{"cell_type":"code","source":"# number of categories that exists in ocean_proximity\nhousing['ocean_proximity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.906434Z","iopub.execute_input":"2021-06-04T12:46:44.906743Z","iopub.status.idle":"2021-06-04T12:46:44.919841Z","shell.execute_reply.started":"2021-06-04T12:46:44.906712Z","shell.execute_reply":"2021-06-04T12:46:44.918695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary of numerical attributes.\nhousing.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.921828Z","iopub.execute_input":"2021-06-04T12:46:44.92237Z","iopub.status.idle":"2021-06-04T12:46:44.979271Z","shell.execute_reply.started":"2021-06-04T12:46:44.922323Z","shell.execute_reply":"2021-06-04T12:46:44.978246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's take a quick look at the data distribution.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:44.980861Z","iopub.execute_input":"2021-06-04T12:46:44.981278Z","iopub.status.idle":"2021-06-04T12:46:47.129341Z","shell.execute_reply.started":"2021-06-04T12:46:44.981235Z","shell.execute_reply":"2021-06-04T12:46:47.128598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> we notice a few things in this:\n* First, the median income attribute does not look like it is expressed in US dollars(USD). The data is scaled.The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about 30,000)\n* The housing median age and the median house value were also capped.Your Machine Learning algorithms may learn that prices never go beyond that limit.\n* Finally, many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.","metadata":{}},{"cell_type":"code","source":"# Creation of training and test set.\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:47.131634Z","iopub.execute_input":"2021-06-04T12:46:47.132018Z","iopub.status.idle":"2021-06-04T12:46:47.610953Z","shell.execute_reply.started":"2021-06-04T12:46:47.131989Z","shell.execute_reply":"2021-06-04T12:46:47.60991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:47.612728Z","iopub.execute_input":"2021-06-04T12:46:47.613014Z","iopub.status.idle":"2021-06-04T12:46:47.632715Z","shell.execute_reply.started":"2021-06-04T12:46:47.612987Z","shell.execute_reply":"2021-06-04T12:46:47.631691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing['median_income'].hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:47.633899Z","iopub.execute_input":"2021-06-04T12:46:47.634184Z","iopub.status.idle":"2021-06-04T12:46:47.814337Z","shell.execute_reply.started":"2021-06-04T12:46:47.634156Z","shell.execute_reply":"2021-06-04T12:46:47.813461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s look at the median income histogram more closely most median income values are clustered around 1.5 to 6 (i.e.15,000–60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the pd.cut() function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than 15,000), category 2 from\n1.5 to 3, and so on.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:47.815694Z","iopub.execute_input":"2021-06-04T12:46:47.815985Z","iopub.status.idle":"2021-06-04T12:46:47.824172Z","shell.execute_reply.started":"2021-06-04T12:46:47.815957Z","shell.execute_reply":"2021-06-04T12:46:47.823225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing[\"income_cat\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:47.825538Z","iopub.execute_input":"2021-06-04T12:46:47.825912Z","iopub.status.idle":"2021-06-04T12:46:47.840799Z","shell.execute_reply.started":"2021-06-04T12:46:47.825879Z","shell.execute_reply":"2021-06-04T12:46:47.839827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing['income_cat'].hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:47.842371Z","iopub.execute_input":"2021-06-04T12:46:47.842788Z","iopub.status.idle":"2021-06-04T12:46:48.021164Z","shell.execute_reply.started":"2021-06-04T12:46:47.842756Z","shell.execute_reply":"2021-06-04T12:46:48.020085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit= StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:48.022788Z","iopub.execute_input":"2021-06-04T12:46:48.023118Z","iopub.status.idle":"2021-06-04T12:46:48.050886Z","shell.execute_reply.started":"2021-06-04T12:46:48.023087Z","shell.execute_reply":"2021-06-04T12:46:48.049777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# https://www.kaggle.com/dhirajnirne/stratified-sampling\n* visit here to know the importance and use of stratified sampling.","metadata":{}},{"cell_type":"code","source":"# lets see if it worked or not\nstrat_test_set['income_cat'].value_counts()/ len(strat_test_set)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:48.052382Z","iopub.execute_input":"2021-06-04T12:46:48.052861Z","iopub.status.idle":"2021-06-04T12:46:48.067833Z","shell.execute_reply.started":"2021-06-04T12:46:48.052805Z","shell.execute_reply":"2021-06-04T12:46:48.066682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now you should remove the income_cat attribute so the data is back to its original state.\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:48.069349Z","iopub.execute_input":"2021-06-04T12:46:48.069688Z","iopub.status.idle":"2021-06-04T12:46:48.082886Z","shell.execute_reply.started":"2021-06-04T12:46:48.069642Z","shell.execute_reply":"2021-06-04T12:46:48.081657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discover and visualize the data to gain insights","metadata":{}},{"cell_type":"code","source":"# let's create copy of the dataset to play with it\nhousing= strat_train_set.copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:48.084163Z","iopub.execute_input":"2021-06-04T12:46:48.08465Z","iopub.status.idle":"2021-06-04T12:46:48.095784Z","shell.execute_reply.started":"2021-06-04T12:46:48.084602Z","shell.execute_reply":"2021-06-04T12:46:48.094756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:48.09708Z","iopub.execute_input":"2021-06-04T12:46:48.097601Z","iopub.status.idle":"2021-06-04T12:46:48.280275Z","shell.execute_reply.started":"2021-06-04T12:46:48.097556Z","shell.execute_reply":"2021-06-04T12:46:48.279523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it's hard to see any pattern here let's reduce alpha\nhousing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:48.281377Z","iopub.execute_input":"2021-06-04T12:46:48.281854Z","iopub.status.idle":"2021-06-04T12:46:48.457081Z","shell.execute_reply.started":"2021-06-04T12:46:48.281806Z","shell.execute_reply":"2021-06-04T12:46:48.456328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's make it clearer\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n             sharex=False)\nplt.legend()\nplt.show()\n# The radius of each circle represents the district’s population (option s), and the color represents the price (option c).\n# We will use a predefined color map (option cmap) called jet, which ranges from blue(low values) to red (high prices).","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:48.458224Z","iopub.execute_input":"2021-06-04T12:46:48.458543Z","iopub.status.idle":"2021-06-04T12:46:49.090998Z","shell.execute_reply.started":"2021-06-04T12:46:48.458511Z","shell.execute_reply":"2021-06-04T12:46:49.089928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density.\n","metadata":{}},{"cell_type":"code","source":"# let's look for correlations\ncorr_matrix= housing.corr()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.092236Z","iopub.execute_input":"2021-06-04T12:46:49.092561Z","iopub.status.idle":"2021-06-04T12:46:49.102448Z","shell.execute_reply.started":"2021-06-04T12:46:49.092529Z","shell.execute_reply":"2021-06-04T12:46:49.101126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see the correlation with median_house_value\ncorr_matrix['median_house_value'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.104061Z","iopub.execute_input":"2021-06-04T12:46:49.104424Z","iopub.status.idle":"2021-06-04T12:46:49.112807Z","shell.execute_reply.started":"2021-06-04T12:46:49.104344Z","shell.execute_reply":"2021-06-04T12:46:49.112017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\nthere is a strong positive correlation; for example, the median house value tends to go\nup when the median income goes up. When the coefficient is close to –1, it means\nthat there is a strong negative correlation; you can see a small negative correlation\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\ngo down when you go north).","metadata":{}},{"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.114006Z","iopub.execute_input":"2021-06-04T12:46:49.114466Z","iopub.status.idle":"2021-06-04T12:46:49.311887Z","shell.execute_reply.started":"2021-06-04T12:46:49.114418Z","shell.execute_reply":"2021-06-04T12:46:49.311084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now, you have seen correlations between different features. But, sometimes what happens is that a attribute may not have corrletion with the target but a combination of two or more attributes could have a impact on the target so now look for such combinations:","metadata":{}},{"cell_type":"code","source":"# EXPERIMENTING WITH ATTRIBUTE COMBINATIONS\n# the total number of rooms in a district is not very useful if you don’t know how many households there are.\n# What you really want is the number of rooms per household.\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.313126Z","iopub.execute_input":"2021-06-04T12:46:49.313588Z","iopub.status.idle":"2021-06-04T12:46:49.322129Z","shell.execute_reply.started":"2021-06-04T12:46:49.313541Z","shell.execute_reply":"2021-06-04T12:46:49.321343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now lets look at the correlation matrix\ncorr_matrix= housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.323465Z","iopub.execute_input":"2021-06-04T12:46:49.323941Z","iopub.status.idle":"2021-06-04T12:46:49.350935Z","shell.execute_reply.started":"2021-06-04T12:46:49.323909Z","shell.execute_reply":"2021-06-04T12:46:49.349921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The new bedrooms_per_room attribute is much more correlated with\nthe median house value than the total number of rooms or bedrooms. Apparently\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\nrooms per household is also more informative than the total number of rooms in a\ndistrict—obviously the larger the houses, the more expensive they are.","metadata":{}},{"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.35525Z","iopub.execute_input":"2021-06-04T12:46:49.35556Z","iopub.status.idle":"2021-06-04T12:46:49.646411Z","shell.execute_reply.started":"2021-06-04T12:46:49.355532Z","shell.execute_reply":"2021-06-04T12:46:49.645457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.64817Z","iopub.execute_input":"2021-06-04T12:46:49.648478Z","iopub.status.idle":"2021-06-04T12:46:49.70616Z","shell.execute_reply.started":"2021-06-04T12:46:49.648442Z","shell.execute_reply":"2021-06-04T12:46:49.705169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the data for Machine Learning Algorithm","metadata":{}},{"cell_type":"code","source":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.707273Z","iopub.execute_input":"2021-06-04T12:46:49.707559Z","iopub.status.idle":"2021-06-04T12:46:49.713784Z","shell.execute_reply.started":"2021-06-04T12:46:49.707531Z","shell.execute_reply":"2021-06-04T12:46:49.712809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"you should compute the median value on the training set, and\nuse it to fill the missing values in the training set, but also don’t forget to save the\nmedian value that you have computed. You will need it later to replace missing values\nin the test set when you want to evaluate your system, and also once the system goes\nlive to replace missing values in new data.","metadata":{}},{"cell_type":"code","source":"# DATA Cleaning\n# we will fill the the numerical missing values with their medians.\n# Scikit-Learn provides a handy class to take care of missing values: SimpleImputer\nfrom sklearn.impute import SimpleImputer\nimputer= SimpleImputer(strategy='median')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.715106Z","iopub.execute_input":"2021-06-04T12:46:49.715493Z","iopub.status.idle":"2021-06-04T12:46:49.769502Z","shell.execute_reply.started":"2021-06-04T12:46:49.715459Z","shell.execute_reply":"2021-06-04T12:46:49.768581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA Cleaning\n# we will fill the the numerical missing values with their medians.\n# Scikit-Learn provides a handy class to take care of missing values: SimpleImputer\nfrom sklearn.impute import SimpleImputer\nimputer= SimpleImputer(strategy='median')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.770554Z","iopub.execute_input":"2021-06-04T12:46:49.770822Z","iopub.status.idle":"2021-06-04T12:46:49.77749Z","shell.execute_reply.started":"2021-06-04T12:46:49.770796Z","shell.execute_reply":"2021-06-04T12:46:49.7765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#since median can only be computed on numerical attributes.\nhousing_num= housing.drop('ocean_proximity', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.778494Z","iopub.execute_input":"2021-06-04T12:46:49.77887Z","iopub.status.idle":"2021-06-04T12:46:49.793059Z","shell.execute_reply.started":"2021-06-04T12:46:49.778841Z","shell.execute_reply":"2021-06-04T12:46:49.792087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer.fit(housing_num)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.796975Z","iopub.execute_input":"2021-06-04T12:46:49.797265Z","iopub.status.idle":"2021-06-04T12:46:49.825868Z","shell.execute_reply.started":"2021-06-04T12:46:49.797238Z","shell.execute_reply":"2021-06-04T12:46:49.824693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer.statistics_","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.827584Z","iopub.execute_input":"2021-06-04T12:46:49.828007Z","iopub.status.idle":"2021-06-04T12:46:49.834784Z","shell.execute_reply.started":"2021-06-04T12:46:49.827964Z","shell.execute_reply":"2021-06-04T12:46:49.833969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking if it is same as the median\nhousing_num.median().values","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.835932Z","iopub.execute_input":"2021-06-04T12:46:49.836345Z","iopub.status.idle":"2021-06-04T12:46:49.853336Z","shell.execute_reply.started":"2021-06-04T12:46:49.836316Z","shell.execute_reply":"2021-06-04T12:46:49.852158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= imputer.transform(housing_num)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.854946Z","iopub.execute_input":"2021-06-04T12:46:49.855308Z","iopub.status.idle":"2021-06-04T12:46:49.864287Z","shell.execute_reply.started":"2021-06-04T12:46:49.855274Z","shell.execute_reply":"2021-06-04T12:46:49.863115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HANDLING CATEGORICAL ATTRIBUTES\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.865877Z","iopub.execute_input":"2021-06-04T12:46:49.866593Z","iopub.status.idle":"2021-06-04T12:46:49.881014Z","shell.execute_reply.started":"2021-06-04T12:46:49.866548Z","shell.execute_reply":"2021-06-04T12:46:49.879669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# By default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method \n# or by setting 'sparse' attribute to False\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.88235Z","iopub.execute_input":"2021-06-04T12:46:49.882678Z","iopub.status.idle":"2021-06-04T12:46:49.90533Z","shell.execute_reply.started":"2021-06-04T12:46:49.882648Z","shell.execute_reply":"2021-06-04T12:46:49.903653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although Scikit-Learn provides many useful transformers, you will need to write\nyour own.\n\nLet's create a custom transformer to add extra attributes:","metadata":{}},{"cell_type":"code","source":"#CUSTOM TRANSFORMATIONS\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.906847Z","iopub.execute_input":"2021-06-04T12:46:49.907302Z","iopub.status.idle":"2021-06-04T12:46:49.929219Z","shell.execute_reply.started":"2021-06-04T12:46:49.907258Z","shell.execute_reply":"2021-06-04T12:46:49.928251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRANSFORMATION PIPELINES\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n# The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a StandardScaler,\n# which is a transformer, so the pipeline has a transform() method that applies all the transforms to the data in sequence \n#(and of course also a fit_transform() method, which is the one we used).","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.930729Z","iopub.execute_input":"2021-06-04T12:46:49.931129Z","iopub.status.idle":"2021-06-04T12:46:49.963375Z","shell.execute_reply.started":"2021-06-04T12:46:49.931086Z","shell.execute_reply":"2021-06-04T12:46:49.962386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to \n# handle all columns, applying the appropriate transformations to each column.\nfrom sklearn.compose import ColumnTransformer\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:49.964851Z","iopub.execute_input":"2021-06-04T12:46:49.965269Z","iopub.status.idle":"2021-06-04T12:46:50.010863Z","shell.execute_reply.started":"2021-06-04T12:46:49.965225Z","shell.execute_reply":"2021-06-04T12:46:50.010048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing_prepared","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:50.012296Z","iopub.execute_input":"2021-06-04T12:46:50.012977Z","iopub.status.idle":"2021-06-04T12:46:50.020718Z","shell.execute_reply.started":"2021-06-04T12:46:50.012932Z","shell.execute_reply":"2021-06-04T12:46:50.019499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select and Train a Model\n","metadata":{}},{"cell_type":"markdown","source":"You have imported the data, explored it, sampled a training and a test set, and you wrote transformation pipelines to clean-up and prepare your data for machine learning algorithms.","metadata":{}},{"cell_type":"code","source":"# Let's train a linear regression model\nfrom sklearn.linear_model import LinearRegression\nlin_reg= LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:50.022106Z","iopub.execute_input":"2021-06-04T12:46:50.022708Z","iopub.status.idle":"2021-06-04T12:46:50.047544Z","shell.execute_reply.started":"2021-06-04T12:46:50.022657Z","shell.execute_reply":"2021-06-04T12:46:50.046426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's measure RSME(root mean squared error) of our model\n\nfrom sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:50.049447Z","iopub.execute_input":"2021-06-04T12:46:50.049896Z","iopub.status.idle":"2021-06-04T12:46:50.069778Z","shell.execute_reply.started":"2021-06-04T12:46:50.049852Z","shell.execute_reply":"2021-06-04T12:46:50.068674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not at all a good score, the predictions are off by $68,628. we can do better than this!\n\n\nThis is a classiic case of model underfitting: it means the features do not provide enough information for good prediction.\n\nTo overcome this we will use a complex model!","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ntree_reg= DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:50.071978Z","iopub.execute_input":"2021-06-04T12:46:50.072439Z","iopub.status.idle":"2021-06-04T12:46:50.403237Z","shell.execute_reply.started":"2021-06-04T12:46:50.072378Z","shell.execute_reply":"2021-06-04T12:46:50.402241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's evatuale on training set\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:50.404374Z","iopub.execute_input":"2021-06-04T12:46:50.40469Z","iopub.status.idle":"2021-06-04T12:46:50.417982Z","shell.execute_reply.started":"2021-06-04T12:46:50.40466Z","shell.execute_reply":"2021-06-04T12:46:50.416713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WWWhhhaaattt! no error!!!!! Impossible\nThis is a classic case of model overfitting!\nNow we will use 'k-fold cross validation' for better evaluation.","metadata":{}},{"cell_type":"code","source":"# cross validation use the train_test_split function to split the training set into a\n# smaller training set and a validation set, then train your models against the smaller training \n#set and evaluate them against the validation set.\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\n\n# (Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a \n# cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value),\n# which is why the preceding code computes -scores before calculating the square root)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:50.419207Z","iopub.execute_input":"2021-06-04T12:46:50.419608Z","iopub.status.idle":"2021-06-04T12:46:52.701534Z","shell.execute_reply.started":"2021-06-04T12:46:50.419572Z","shell.execute_reply":"2021-06-04T12:46:52.700527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see the scores\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:52.702892Z","iopub.execute_input":"2021-06-04T12:46:52.703179Z","iopub.status.idle":"2021-06-04T12:46:52.710361Z","shell.execute_reply.started":"2021-06-04T12:46:52.703151Z","shell.execute_reply":"2021-06-04T12:46:52.709385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems to perform worse than the Linear Regression model!\nThe Decision Tree has ascore of approximately 71,407, generally ±2,439. You would not have this information\nif you just used one validation set. But cross-validation comes at the cost of training\nthe model several times, so it is not always possible.","metadata":{}},{"cell_type":"code","source":"# let's look for scores for linear regression:\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n# the Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model.","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:52.711512Z","iopub.execute_input":"2021-06-04T12:46:52.71178Z","iopub.status.idle":"2021-06-04T12:46:52.841443Z","shell.execute_reply.started":"2021-06-04T12:46:52.711752Z","shell.execute_reply":"2021-06-04T12:46:52.840461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's try Random Forest Regressor\n# (Random Forests work by training many Decision Trees on random subsets of the features,\n# then averaging out their predictions)\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:46:52.843117Z","iopub.execute_input":"2021-06-04T12:46:52.843857Z","iopub.status.idle":"2021-06-04T12:47:08.429422Z","shell.execute_reply.started":"2021-06-04T12:46:52.843808Z","shell.execute_reply":"2021-06-04T12:47:08.428513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:47:08.430729Z","iopub.execute_input":"2021-06-04T12:47:08.431014Z","iopub.status.idle":"2021-06-04T12:47:08.902121Z","shell.execute_reply.started":"2021-06-04T12:47:08.430986Z","shell.execute_reply":"2021-06-04T12:47:08.90097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:47:08.903795Z","iopub.execute_input":"2021-06-04T12:47:08.904222Z","iopub.status.idle":"2021-06-04T12:49:27.889437Z","shell.execute_reply.started":"2021-06-04T12:47:08.904176Z","shell.execute_reply":"2021-06-04T12:49:27.888224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tune your model\n","metadata":{}},{"cell_type":"markdown","source":"assume that you now have a shortlist of promising models. You now need to\nfine-tune them. Let’s look at a few ways you can do that.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3×4) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2×3) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:49:27.890992Z","iopub.execute_input":"2021-06-04T12:49:27.891457Z","iopub.status.idle":"2021-06-04T12:50:21.261495Z","shell.execute_reply.started":"2021-06-04T12:49:27.89141Z","shell.execute_reply":"2021-06-04T12:50:21.260795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameters\ngrid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:50:21.262677Z","iopub.execute_input":"2021-06-04T12:50:21.263182Z","iopub.status.idle":"2021-06-04T12:50:21.268183Z","shell.execute_reply.started":"2021-06-04T12:50:21.263148Z","shell.execute_reply":"2021-06-04T12:50:21.267268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* When you have no idea what value a hyperparameter should have, a simple approach is to try out consecutive powers of 10 (or a smaller number if you want a more fine-grained search, as shown in this example with the n_estimators hyperparameter).\n* This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict, then try all 2 × 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True.\n* All in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor hyperparameter values, and it will train each model five times (since we are using five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90 rounds of training! It may take quite a long time, but when it is done you can get the best combination of parameters.","metadata":{}},{"cell_type":"code","source":"# Let's look at the score of each hyperparameter combination tested during the grid search:\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:50:21.269343Z","iopub.execute_input":"2021-06-04T12:50:21.269667Z","iopub.status.idle":"2021-06-04T12:50:21.290433Z","shell.execute_reply.started":"2021-06-04T12:50:21.269637Z","shell.execute_reply":"2021-06-04T12:50:21.289369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RANDOMIZED SEARCH\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:50:21.291839Z","iopub.execute_input":"2021-06-04T12:50:21.292146Z","iopub.status.idle":"2021-06-04T12:53:29.761624Z","shell.execute_reply.started":"2021-06-04T12:50:21.292117Z","shell.execute_reply":"2021-06-04T12:53:29.760608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at the score of each hyperparameter combination tested\ncvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T12:53:29.762715Z","iopub.execute_input":"2021-06-04T12:53:29.762996Z","iopub.status.idle":"2021-06-04T12:53:29.771337Z","shell.execute_reply.started":"2021-06-04T12:53:29.762969Z","shell.execute_reply":"2021-06-04T12:53:29.770067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If you have any questions, please comment down below.**\n\n**Kindly upvote it if you find this informative!**","metadata":{}},{"cell_type":"markdown","source":"# THANK YOU!","metadata":{}},{"cell_type":"markdown","source":"credits: hands-on machine learning(book)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}