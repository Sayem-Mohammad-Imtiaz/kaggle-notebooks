{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are going to predict the height and weight of the person. We are taking height and gender is the input attributes and weight going to output attribute.\n\nTable Of Content:\n\n*   **1. Feature Enineering/Data Pre Processing**\n  \n *      1(a). Import Dataset\n *      1(b). Describing Descriptive Statistics\n *      1(c). Visualising Descriptive Statistics\n *      1(d). Checking Null or Empty Values (Data Cleaning)\n *      1(e). Label Encoder/One Hot Encoder\n *      1(f). Handle Outliers\n *      1(g). Feature Split\n *      1(h). Resample Evaluate performance model\n     \n*    **2. Modeling**\n   \n *      2(a). Regression Models Without Feature Scale.\n *      2(b). Regression Models With Feature Scale.\n *      2(c). Regularisation Tuning For Top 2 Regression Algorithms.\n *      2(d). Ensemble and Boosting Regression Algorithms With Feature Scale.\n *      2(e). Regularisation Tuning For Top 2 Ensemble and Boosting Regression Algorithms.\n *      2(f). Compare All 4 Tunned Algorithms And Selecting The Best Algorithm\n *      2(g). Fit and Predict The Best Algorithm.\n *      2(h). Accuracy Of An Algorithm."},{"metadata":{},"cell_type":"markdown","source":"# 1. Feature Enineering/Data Pre Processing\n\n# 1(a). Import Dataset"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Importing the Dataset\nimport pandas as pd\ndataset = pd.read_csv(\"../input/weight-height/weight-height.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1(b). Describing Descriptive Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the head and tail of the dataset\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the shape and datatype for each attribute\nprint(dataset.shape)\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the describe of each attribute\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per above min value is clear that height and weight not starting from zero. it is starting around 50+..."},{"metadata":{},"cell_type":"markdown","source":"# 1(c). Visualising Descriptive Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram Visualisation For Height Attribute with distribution plot\n\nimport seaborn as sb\nsb.distplot(dataset['Height'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wowww Perfect we got good normal distribution...Now we are going to check weight attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.distplot(dataset['Weight'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per above plot weight is not a normal distribution but it is almost simlar to normal..\n\nIf we want best prediction we need to apply feature scale then only we will get better accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the correlation between input and output attributes.\ncorr_value=dataset.corr()\nsb.heatmap(corr_value,square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1(d). Checking Null or Empty Values (Data Cleaning)\n\nChecking the null or empty values and applying data cleaning to our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the Null or empty values \ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the Null or empty values sum\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dont have any missing values so now safely we can go ahead..."},{"metadata":{},"cell_type":"markdown","source":"# 1(e). Label Encoder/One Hot Encoder\n\nEncoding the categorical value into numerical value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoding gender column\ndataset['Gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Gender']=dataset['Gender'].map({'Male':0,'Female':1})\ndataset['Gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying first 5 rows\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1(f). Handle Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the outliers with each input attribute to output attribute.\n\nplt.plot(dataset['Gender'],dataset['Weight'])\nplt.title(\"Checking Outliers\")\nplt.xlabel(\"Gender\")\nplt.ylabel('Weight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dont have any outliers as per above plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the outliers with Height input attribute and Weight output attribute.\n\nplt.plot(dataset['Height'],dataset['Weight'])\nplt.title(\"Checking Outliers\")\nplt.xlabel(\"Height\")\nplt.ylabel('Weight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So finally we dont have any outliers in our dataset, So now we can safely go ahead."},{"metadata":{},"cell_type":"markdown","source":"# 1(g). Feature Split\n\nSplitting the input and output attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=dataset['Weight'].values\nx=dataset.drop(['Weight'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1(h). Resample Evaluate performance model\n\nNow we are going to split the input and output attribute as training and test set to evaluate model performance.."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting dataset into train and test split.\n\ntrain_size=0.80\ntest_size=0.20\nseed=5\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=train_size,test_size=test_size,random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Modeling\n\n#  2(a). Regression Models Without Feature Scale."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot Checking and Comparing Algorithms Without Feature Scale\nmodels=[]\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nmodels.append(('linear_reg',LinearRegression()))\nmodels.append(('knn',KNeighborsRegressor()))\nmodels.append(('SVR',SVR()))\nmodels.append((\"decision_tree\",DecisionTreeRegressor()))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nerror='neg_mean_squared_error'\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got accuracy very highly now we are going to applying feature scale to same code and lets check how well it performed.\n\n# 2(b). Regression Models With Feature Scale."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Pipeline with Standardization Scale and models\n# Standardize the dataset\nfrom sklearn.pipeline import Pipeline\nfrom sklearn. preprocessing import MinMaxScaler\npipelines=[]\npipelines.append(('scaler_lg',Pipeline([('scaler',MinMaxScaler()),('lg',LinearRegression())])))\npipelines.append(('scale_KNN',Pipeline([('scaler',MinMaxScaler()),('KNN',KNeighborsRegressor())])))\npipelines.append(('scale_SVR',Pipeline([('scaler',MinMaxScaler()),('SVR',SVR())])))\npipelines.append(('scale_decision',Pipeline([('scaler',MinMaxScaler()),('decision',DecisionTreeRegressor())])))\n\n# Evaluate Pipelines\npredictions=[]\nnames=[]\nfor name, model in pipelines:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg='%s : %f (%f)'%(name,result.mean(),result.std())\n    print(msg)\n    \n#Visualize the compared algorithms\nfig=plt.figure()\nfig.suptitle(\"Algorithms Comparisions\")\nplt.boxplot(predictions)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We didn't get good accuracy because we dont have complex values we have only gender and height as input attributes.\n\nLinear Regression Accuracy : -100.963799 (3.484180)\n\nKNN Regressor accuracy: : -121.553066 (4.666317)\n\nSVR accuracy :-104.947244 (3.553198)\n\nDecision Tree : : -200.952203 (7.381938)\n\n\n# 2(c). Regularisation Tuning For Top 2 Regression Algorithms.\n\nTop 2 algorithms Linear regression and SVR so now we are applying tuning to this algorithms.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVR Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nkernel=['linear','poly','rbf','sigmoid']\nc=[0.2,0.4,0.6,0.8,1.0]\nparam_grid=dict(C=c,kernel=kernel)\nmodel=SVR()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression Algorithm tuning\n\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nparam_grid=dict()\nmodel=LinearRegression()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2(d). Ensemble and Boosting Regression Algorithms With Feature Scale.\n\nEnsemble and boosting algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensemble and Boosting algorithm to improve performance\n\n\n# Boosting methods\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Ensemble Bagging methods\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nensembles=[]\nensembles.append(('scaledAB',Pipeline([('scale',MinMaxScaler()),('AB',AdaBoostRegressor())])))\nensembles.append(('scaledGBR',Pipeline([('scale',MinMaxScaler()),('GBR',GradientBoostingRegressor())])))\nensembles.append(('scaledRF',Pipeline([('scale',MinMaxScaler()),('rf',RandomForestRegressor(n_estimators=10))])))\nensembles.append(('scaledETR',Pipeline([('scale',MinMaxScaler()),('ETR',ExtraTreesRegressor(n_estimators=10))])))\nensembles.append(('scaledRFR',Pipeline([('scale',MinMaxScaler()),('RFR',RandomForestRegressor(n_estimators=10))])))\n# Evaluate each Ensemble Techinique\nresults=[]\nnames=[]\nfor name,model in ensembles:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    results.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the compared Ensemble Algorithms\nfig=plt.figure()\nfig.suptitle('Ensemble Compared Algorithms')\nplt.boxplot(results)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ada Boost Regression Algorithm : -110.605590 (5.951022)\n\nGradient Boosting Regression Algorithm : -102.850162 (3.451911)\n\nWe are going to apply tuning to this algorithms\n\n# 2(e). Regularisation Tuning For Top 2 Ensemble and Boosting Regression Algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"# GradientBoostingRegressor Tuning\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nlearning_rate=[0.1,0.2,0.3,0.4,0.5]\nn_estimators=[5,10,15,20,25,30,40,50,100,200]\nparam_grid=dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel=GradientBoostingRegressor()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoostRegressor Tuning\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nlearning_rate=[0.1,0.2,0.3,0.4,0.5]\nn_estimators=[5,10,15,20,25,30,40,50,100,200]\nparam_grid=dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel=AdaBoostRegressor()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After Applying tuning to those algorithm we are accuracy like wise\n\nGradientBoostingRegressor Tuning :-102.648845 using {'learning_rate': 0.1, 'n_estimators': 50} \n\nAdaBoostRegressor Tuning : -109.112158 using {'learning_rate': 0.5, 'n_estimators': 200} "},{"metadata":{},"cell_type":"markdown","source":"# 2(f). Compare All 4 Tunned Algorithms And Selecting The Best Algorithm\n\n1. SVR Tuning -104.664036 using {'C': 1.0, 'kernel': 'linear'} \n2. Linear Regression algorithm -100.963799 using {} \n3. GradientBoostingRegressor Tuning :-102.648845 using {'learning_rate': 0.1, 'n_estimators': 50} \n4. AdaBoostRegressor Tuning : -109.112158 using {'learning_rate': 0.5, 'n_estimators': 200} "},{"metadata":{},"cell_type":"markdown","source":"# 2(g). Fit and Predict The Best Algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finalize Model\n# we will finalize the gradient boosting regression algorithm and evaluate the model for house price predictions.\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nscaler=MinMaxScaler().fit(x_train)\nscaler_x=scaler.transform(x_train)\nmodel=GradientBoostingRegressor(random_state=5,n_estimators=50,learning_rate=0.1)\nmodel.fit(scaler_x,y_train)\n\n#Transform the validation test set data\nscaledx_test=scaler.transform(x_test)\ny_pred=model.predict(scaledx_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2(h). Accuracy Of An Algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy of algorithm\nfrom math import sqrt\nmse=mean_squared_error(y_test,y_pred)\nrmse=np.sqrt(mse)\nprint(\"rmse\",rmse)\nr2=r2_score(y_test,y_pred)\nprint(\"mse\",mse)\nprint(\"r2_score\",r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perfect we got root mean square error around 10 we done..\n\nIf any questions please let me know..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}