{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/bank-marketing-dataset/bank.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[345: 500]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['default'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://medium.com/analytics-vidhya/building-classification-model-with-python-9bdfc13faa4b","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf_bank = pd.read_csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\n\n# Drop 'duration' column\ndf_bank = df_bank.drop('duration', axis=1)\n\n# print(df_bank.info())\nprint('Shape of dataframe:', df_bank.shape)\ndf_bank.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Class Distribution\nAnother important thing to make sure before feeding our data into the model is the class distribution of the data. In our case where the expected class are divided into two outcome, ‘yes’ and ‘no’, a class distribution of 50:50 can be considered ideal.\n","metadata":{}},{"cell_type":"code","source":"df_bank['deposit'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check missing values","metadata":{}},{"cell_type":"code","source":"df_bank.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Scale Numeric Data**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Copying original dataframe\ndf_bank_ready = df_bank.copy()\n\nscaler = StandardScaler()\nnum_cols = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\ndf_bank_ready[num_cols] = scaler.fit_transform(df_bank[num_cols])\n\ndf_bank_ready.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Categorical Data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False)\ncat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n\n# Encode Categorical Data\ndf_encoded = pd.DataFrame(encoder.fit_transform(df_bank_ready[cat_cols]))\ndf_encoded.columns = encoder.get_feature_names(cat_cols)\n\n# Replace Categotical Data with Encoded Data\ndf_bank_ready = df_bank_ready.drop(cat_cols ,axis=1)\ndf_bank_ready = pd.concat([df_encoded, df_bank_ready], axis=1)\n\n# Encode target value\ndf_bank_ready['deposit'] = df_bank_ready['deposit'].apply(lambda x: 1 if x == 'yes' else 0)\n\nprint('Shape of dataframe:', df_bank_ready.shape)\ndf_bank_ready.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Dataset for Training and Testing","metadata":{}},{"cell_type":"code","source":"# Select Features\nfeature = df_bank_ready.drop('deposit', axis=1)\n\n# Select Target\ntarget = df_bank_ready['deposit']\n\n# Set Training and Testing Data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(feature , target, \n                                                    shuffle = True, \n                                                    test_size=0.2, \n                                                    random_state=1)\n\n# Show the Training and Testing Data\nprint('Shape of training feature:', X_train.shape)\nprint('Shape of testing feature:', X_test.shape)\nprint('Shape of training label:', y_train.shape)\nprint('Shape of training label:', y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n\nThis step will be done after model has been generated","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, x_test, y_test):\n    from sklearn import metrics\n\n    # Predict Test Data \n    y_pred = model.predict(x_test)\n\n    # Calculate accuracy, precision, recall, f1-score, and kappa score\n    acc = metrics.accuracy_score(y_test, y_pred)\n    prec = metrics.precision_score(y_test, y_pred)\n    rec = metrics.recall_score(y_test, y_pred)\n    f1 = metrics.f1_score(y_test, y_pred)\n    kappa = metrics.cohen_kappa_score(y_test, y_pred)\n\n    # Calculate area under curve (AUC)\n    y_pred_proba = model.predict_proba(x_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n\n    # Display confussion matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n\n    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa, \n            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Decision tree","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\n\n# Building Decision Tree model \ndtc = tree.DecisionTreeClassifier(random_state=0)\ndtc.fit(X_train, y_train)\n\n# Evaluate Model\ndtc_eval = evaluate_model(dtc, X_test, y_test)\n\n# Print result\nprint('Accuracy:', dtc_eval['acc'])\nprint('Precision:', dtc_eval['prec'])\nprint('Recall:', dtc_eval['rec'])\nprint('F1 Score:', dtc_eval['f1'])\nprint('Cohens Kappa Score:', dtc_eval['kappa'])\nprint('Area Under Curve:', dtc_eval['auc'])\nprint('Confusion Matrix:\\n', dtc_eval['cm'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes\n","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n \n#Calling the Class\nnaive_bayes = GaussianNB()\n \n#Fitting the data to the classifier\nnaive_bayes.fit(X_train , y_train)\n \n#Predict on test data\ny_predicted = naive_bayes.predict(X_test)\n\ndtc_eval = evaluate_model(naive_bayes, X_test, y_test)\n\n# Print result\nprint('Accuracy:', dtc_eval['acc'])\nprint('Precision:', dtc_eval['prec'])\nprint('Recall:', dtc_eval['rec'])\nprint('F1 Score:', dtc_eval['f1'])\nprint('Cohens Kappa Score:', dtc_eval['kappa'])\nprint('Area Under Curve:', dtc_eval['auc'])\nprint('Confusion Matrix:\\n', dtc_eval['cm'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}