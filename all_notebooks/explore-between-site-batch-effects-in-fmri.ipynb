{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport numpy.linalg as la\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport patsy\nimport sklearn as sk # scikit-learn\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport sys\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":61,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"c8c4cbc247d1b80db4863ebd794365e4e354913d","collapsed":true},"cell_type":"code","source":"def adjust_nums(numerical_covariates, drop_idxs):\n    # if we dropped some values, have to adjust those with a larger index.\n    if numerical_covariates is None: return drop_idxs\n    return [nc - sum(nc < di for di in drop_idxs) for nc in numerical_covariates]\n\ndef design_mat(mod, numerical_covariates, batch_levels):\n    # require levels to make sure they are in the same order as we use in the\n    # rest of the script.\n    design = patsy.dmatrix(\"~ 0 + C(batch, levels=%s)\" % str(batch_levels),\n                                                  mod, return_type=\"dataframe\")\n\n    mod = mod.drop([\"batch\"], axis=1)\n    numerical_covariates = list(numerical_covariates)\n    sys.stderr.write(\"found %i batches\\n\" % design.shape[1])\n    other_cols = [c for i, c in enumerate(mod.columns)\n                  if not i in numerical_covariates]\n    factor_matrix = mod[other_cols]\n    design = pd.concat((design, factor_matrix), axis=1)\n    if numerical_covariates is not None:\n        sys.stderr.write(\"found %i numerical covariates...\\n\"\n                            % len(numerical_covariates))\n        for i, nC in enumerate(numerical_covariates):\n            cname = mod.columns[nC]\n            sys.stderr.write(\"\\t{0}\\n\".format(cname))\n            design[cname] = mod[mod.columns[nC]]\n    sys.stderr.write(\"found %i categorical variables:\" % len(other_cols))\n    sys.stderr.write(\"\\t\" + \", \".join(other_cols) + '\\n')\n    return design\n\n\ndef combat(data, batch, model=None, numerical_covariates=None):\n    \"\"\"Correct for batch effects in a dataset\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        A (n_features, n_samples) dataframe of the expression or methylation\n        data to batch correct\n    batch : pandas.Series\n        A column corresponding to the batches in the data, with index same as\n        the columns that appear in ``data``\n    model : patsy.design_info.DesignMatrix, optional\n        A model matrix describing metadata on the samples which could be\n        causing batch effects. If not provided, then will attempt to coarsely\n        correct just from the information provided in ``batch``\n    numerical_covariates : list-like\n        List of covariates in the model which are numerical, rather than\n        categorical\n    Returns\n    -------\n    corrected : pandas.DataFrame\n        A (n_features, n_samples) dataframe of the batch-corrected data\n    \"\"\"\n    if isinstance(numerical_covariates, str):\n        numerical_covariates = [numerical_covariates]\n    if numerical_covariates is None:\n        numerical_covariates = []\n\n    if model is not None and isinstance(model, pd.DataFrame):\n        model[\"batch\"] = list(batch)\n    else:\n        model = pd.DataFrame({'batch': batch})\n\n    batch_items = model.groupby(\"batch\").groups.items()\n    batch_levels = [k for k, v in batch_items]\n    batch_info = [v for k, v in batch_items]\n    n_batch = len(batch_info)\n    n_batches = np.array([len(v) for v in batch_info])\n    n_array = float(sum(n_batches))\n\n    # drop intercept\n    drop_cols = [cname for cname, inter in  ((model == 1).all()).iteritems() if inter == True]\n    drop_idxs = [list(model.columns).index(cdrop) for cdrop in drop_cols]\n    model = model[[c for c in model.columns if not c in drop_cols]]\n    numerical_covariates = [list(model.columns).index(c) if isinstance(c, str) else c\n            for c in numerical_covariates if not c in drop_cols]\n\n    design = design_mat(model, numerical_covariates, batch_levels)\n\n    sys.stderr.write(\"Standardizing Data across genes.\\n\")\n    B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T)\n    grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch,:])\n    var_pooled = np.dot(np.array(((data - np.dot(design, B_hat).T)**2),dtype=np.float32), np.ones((int(n_array), 1)) / int(n_array))\n\n    stand_mean = np.dot(grand_mean.T.reshape((len(grand_mean), 1)), np.ones((1, int(n_array))))\n    tmp = np.array(design.copy())\n    tmp[:,:n_batch] = 0\n    stand_mean  += np.dot(tmp, B_hat).T\n\n    var_pooled = np.array(var_pooled,dtype=np.float32)\n    s_data = ((data - stand_mean) / np.dot(np.sqrt(var_pooled), np.ones((1, int(n_array)))))\n\n    sys.stderr.write(\"Fitting L/S model and finding priors\\n\")\n    batch_design = design[design.columns[:n_batch]]\n    gamma_hat = np.dot(np.dot(la.inv(np.dot(batch_design.T, batch_design)), batch_design.T), s_data.T)\n\n    delta_hat = []\n\n    for i, batch_idxs in enumerate(batch_info):\n        #batches = [list(model.columns).index(b) for b in batches]\n        delta_hat.append(s_data[batch_idxs].var(axis=1))\n\n    gamma_bar = gamma_hat.mean(axis=1) \n    t2 = gamma_hat.var(axis=1)\n\n\n    a_prior = list(map(aprior, delta_hat))\n    b_prior = list(map(bprior, delta_hat))\n\n    sys.stderr.write(\"Finding parametric adjustments\\n\")\n    gamma_star, delta_star = [], []\n    for i, batch_idxs in enumerate(batch_info):\n        #print '18 20 22 28 29 31 32 33 35 40 46'\n        #print batch_info[batch_id]\n\n        temp = it_sol(s_data[batch_idxs], gamma_hat[i],\n                     delta_hat[i], gamma_bar[i], t2[i], a_prior[i], b_prior[i])\n\n        gamma_star.append(temp[0])\n        delta_star.append(temp[1])\n\n    sys.stdout.write(\"Adjusting data\\n\")\n    bayesdata = s_data\n    gamma_star = np.array(gamma_star)\n    delta_star = np.array(delta_star)\n\n\n    for j, batch_idxs in enumerate(batch_info):\n\n        dsq = np.sqrt(delta_star[j,:])\n        dsq = dsq.reshape((len(dsq), 1))\n        denom =  np.dot(dsq, np.ones((1, n_batches[j])))\n        numer = np.array(bayesdata[batch_idxs] - np.dot(batch_design.loc[batch_idxs], gamma_star).T)\n\n        bayesdata[batch_idxs] = numer / denom\n\n    vpsq = np.sqrt(var_pooled).reshape((len(var_pooled), 1))\n    bayesdata = bayesdata * np.dot(vpsq, np.ones((1, int(n_array)))) + stand_mean\n\n    return bayesdata,gamma_star,delta_star\n\ndef it_sol(sdat, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001):\n    n = (1 - pd.isnull(sdat)).sum(axis=1)\n    g_old = g_hat.copy()\n    d_old = d_hat.copy()\n\n    change = 1\n    count = 0\n    while change > conv:\n        #print g_hat.shape, g_bar.shape, t2.shape\n        g_new = postmean(g_hat, g_bar, n, d_old, t2)\n        sum2 = ((sdat - np.dot(g_new.values.reshape((g_new.shape[0], 1)), np.ones((1, sdat.shape[1])))) ** 2).sum(axis=1)\n        d_new = postvar(sum2, n, a, b)\n\n        change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max())\n        g_old = g_new #.copy()\n        d_old = d_new #.copy()\n        count = count + 1\n    adjust = (g_new, d_new)\n    return adjust \n\n    \n\ndef aprior(gamma_hat):\n    m = gamma_hat.mean()\n    s2 = gamma_hat.var()\n    return (2 * s2 +m**2) / s2\n\ndef bprior(gamma_hat):\n    m = gamma_hat.mean()\n    s2 = gamma_hat.var()\n    return (m*s2+m**3)/s2\n\ndef postmean(g_hat, g_bar, n, d_star, t2):\n    return (t2*n*g_hat+d_star * g_bar) / (t2*n+d_star)\n\ndef postvar(sum2, n, a, b):\n    return (0.5 * sum2 + b) / (n / 2.0 + a - 1.0)\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"# Load spatial QC features\nqap_spatial = pd.read_csv('../input/ABIDE_qap_functional_spatial.csv')\nqap_spatial.columns","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fabb57bd7bcdd3594c568e7f0179b1adb1de25b6","collapsed":true},"cell_type":"code","source":"# Load temporal QC features\nqap_temporal = pd.read_csv('../input/ABIDE_qap_functional_temporal.csv')\nqap_temporal.columns","execution_count":5,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"2313e1d99b88621565d9b6450e2a642e6937d19a","collapsed":true},"cell_type":"code","source":"# Combine both spatial and temporal features\nqap_intersection = pd.merge(qap_spatial, qap_temporal, on=['subject','site','scan'], how='inner')\nqap_functional = qap_intersection.T.drop_duplicates(keep=False).T\nqap_boxplot = qap_functional.boxplot(column=['ghost_x','ghost_y'],by='site',rot=60,fontsize=16,figsize=(18,6),layout=(1,2))\nqap_boxplot = qap_functional.boxplot(column=['dvars','mean_fd'],by='site',rot=60,fontsize=16,figsize=(18,6),layout=(1,2))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3fda21f5626a34699aa79525665981d9e4e143","_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"# Run combat to re-center all features\ndm = qap_functional.drop(['subject','scan','site','ghost_z'],axis=1)\ndm_corrected,gamma,delta = combat(dm.T, qap_functional['site'], None)\ndm_corrected = dm_corrected.append(qap_functional['site'].T).T\n# Uncomment to check parameters\n# print('Gamma (combat): {}'.format(gamma.T[:][np.nonzero(dm_corrected.T.columns.contains('dvars'))]))\n# print('Delta (combat): {}'.format(delta.T[:][np.nonzero(dm_corrected.T.columns.contains('dvars'))]))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3650fbde56f7b77368d680b63331f4cf0e11a1c9","collapsed":true},"cell_type":"code","source":"qap_combat_boxplot = dm_corrected.boxplot(column=['ghost_x','ghost_y'],by='site',rot=60,fontsize=16,figsize=(18,6),layout=(1,2))\nqap_combat_boxplot = dm_corrected.boxplot(column=['dvars','mean_fd'],by='site',rot=60,fontsize=16,figsize=(18,6),layout=(1,2))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90bc385b57f11b4e82092ba71f5d93ec51fb8c0f","collapsed":true},"cell_type":"code","source":"def scale_df(df):\n    scaler = preprocessing.StandardScaler()\n    x = df.values.astype(float)\n    x_scaled = scaler.fit_transform(x)\n    return pd.DataFrame(x_scaled,columns=df.columns)\n\ndm = dm.apply(lambda col:pd.to_numeric(col, errors='coerce'))\nsns.heatmap(scale_df(dm),cmap='RdYlBu',robust=True,yticklabels=False)\nplt.show()\n\ndm_corrected = dm_corrected.apply(lambda col:pd.to_numeric(col, errors='coerce'))\nsns.heatmap(scale_df(dm_corrected.dropna(axis=1)),cmap='RdYlBu',robust=True,yticklabels=False)\nplt.show()","execution_count":82,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}