{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## Libraries Needed:\nimport numpy as np                        ## Matrix functions\nimport matplotlib.pyplot as plt           ## PLotting\nimport pandas as pd                       ## To Work WIth Dataframes \nimport plotly.express as px               ## For Interactive Visualization\nimport plotly.graph_objects as go         ## For Detailed visual plots\nfrom collections import Counter         \nfrom plotly.subplots import make_subplots ## To Plot Subplots\nfrom wordcloud import WordCloud           ## To Generate Wordcloud\nfrom datetime import datetime             ## Work with timeseries data\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MetaData","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata = pd.read_csv(\"../input/zomato-restaurants-hyderabad/Restaurant names and Metadata.csv\")\nprint(\"MetaData Shape:\", metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linked attribute will be of no use in our analysis. Also, Cost attribute should be of float type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.drop(['Links'], axis=1, inplace=True)\nmetadata['Cost'] = metadata['Cost'].apply(lambda x : float(x.replace(',', '')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cost Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cost = metadata[['Name', 'Cost']]\n\nbins = pd.DataFrame(pd.cut(cost['Cost'], bins= 10))\nbins.columns = ['bins']\nbins['bins'] = bins['bins'].astype(str)\n\nbins = bins['bins'].value_counts().reset_index()\nbins.columns = ['Bin', 'Count']\nbins[\"Cumsum\"] = bins['Count'].cumsum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(name = \"Restaurants in Range\", x = bins['Bin'], y=bins['Count']))\nfig.add_trace(go.Scatter(name = \"Restaurants below or in Range\", x = bins['Bin'], y=bins['Cumsum']))\nfig.update_layout(title=\"No Of Restaurents by Price Range\",\n                 xaxis_title = \"Price Range\",\n                 yaxis_title = \"No Of Restaurants\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\ntemp = cost.sort_values(by='Cost')\n\nfig.add_trace(go.Bar(name = \"Cheapest Restaurant\", x = temp.head()['Name'], y=temp.head()['Cost']))\nfig.add_trace(go.Bar(name=\"Expensive Restaurent\", x = temp.tail()['Name'], y=temp.tail()['Cost']))\nfig.update_layout(title = \"Least and Most Expensive Restaurants:\",\n                 xaxis_title = \"Restaurant Name\",\n                 yaxis_title = \"Cost\")\nfig.show()\ndel temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cuisins","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cuisines = metadata['Cuisines']\ncuisines = cuisines.apply(lambda x : x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_cuisines = ', '.join(i for i in cuisines.tolist())\nall_cuisines = Counter(all_cuisines.split(', '))\nall_cuisines = pd.DataFrame.from_dict(all_cuisines, orient='index', dtype='int')\nall_cuisines.columns = ['No Of Restaurents']\nall_cuisines.sort_values(by='No Of Restaurents', ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuisines = cuisines.apply(lambda x : x.split(', '))\ncuisines = pd.DataFrame(cuisines)\n\nfor i in all_cuisines.index.tolist():\n    cuisines['{}'.format(i)] = cuisines['Cuisines'].apply(lambda x : i in x)\n\ncuisines.drop('Cuisines', axis=1, inplace=True)\ncuisines = pd.concat([metadata, cuisines], axis=1)\ncuisines.drop(['Collections', 'Cuisines', 'Timings'], axis=1, inplace=True)\ncuisines = pd.melt(cuisines, id_vars=['Name', 'Cost'], var_name='Cuisine')\ncuisines = cuisines[cuisines['value']]\ncuisines.drop(['value'], axis=1, inplace=True)\ndel all_cuisines","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = cuisines['Cuisine'].value_counts().reset_index()\n\nfig = px.bar(x = temp['index'], y=temp['Cuisine'])\nfig.update_layout(title = \"Cuisines availability\",\n                 xaxis_title = \"Cusisine\",\n                 yaxis_title = \"No of restaurants cuisine available at\")\nfig.show()\ndel temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Value_counts() functions returns in descending order. So we don't need to sort expliitly.\ntop_cuisines = cuisines['Cuisine'].value_counts().reset_index()\ntop_cuisines = top_cuisines['index'].tolist()[:8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = cuisines[cuisines['Cuisine'].isin(top_cuisines)]\n\nfig = px.histogram(data_frame=temp, x='Cost',\n            facet_col = 'Cuisine', facet_col_wrap=4,\n            title = \"Price Distribution amongst most popular cuisines:\")\nfig.show()\ndel temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_cost = cuisines.groupby(by='Cuisine')['Cost'].mean().reset_index()\nmean_cost.sort_values(by='Cost', ascending=False, inplace=True)\n\nfig = px.bar(mean_cost, x='Cuisine', y='Cost')\nfig.update_layout(title = \"Average Cost by Cuisine\",\n                 xaxis_title = \"Cuisine (Most to Least Expensive)\",\n                 yaxis_title = \"Avg Cost of Cuisine\")\nfig.show()\ndel mean_cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuisine_offered = cuisines.groupby(by='Name')['Cuisine'].count().reset_index()\ncuisine_offered.columns = ['Name', 'Cuisine_Offered']\n\nmetadata = pd.merge(metadata, cuisine_offered, left_on='Name', right_on = 'Name')\n\ndel cuisine_offered","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Collections","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"collections = metadata['Collections'].dropna().tolist()\ncollections = ', '.join(i for i in collections)\n\nwc = WordCloud(background_color=\"white\", max_words=200, \n               width=800, height=600, random_state=1).generate(collections)\nprint(\"Most Common Taggs:\")\nplt.imshow(wc)\ndel collections","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reviews","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = pd.read_csv(\"../input/zomato-restaurants-hyderabad/Restaurant reviews.csv\")\nprint(\"Reviews Shape:\", reviews.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = reviews[reviews.Reviewer.isnull()].Restaurant.unique()\nprint(\"These are the restaurants where we have missing values:\", temp, sep = '\\n')\ndel temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Picture attribute will not be used in our analysis, we better get rid of it. Also, number of null values is ignorable considering size of the dataset. We're gonna drop null values too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews.drop('Pictures', axis = 1, inplace=True)\nreviews.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews['Rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews.loc[reviews['Rating']=='Like', 'Rating'] = 3.5\nreviews['Rating'] = reviews['Rating'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Metadada attribute consistes of followers and reviews. It is better to extract these differently and use in out analysis. We'll rename the reviews as thread review as we already have an attribute named as reviews that are Original Review. Also, After extracting these differently, we'll have no use of the \"Metadata\" attribute itself. So, we'll drop that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_followers(x):\n    x = x.split(\", \")\n    try :\n        x = x[1].split()[0]\n    except:\n        x = 0\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews['Thread Review'] = reviews['Metadata'].apply(lambda x : x.split(\", \")[0].split()[0])\nreviews['Followers'] = reviews['Metadata'].apply(get_followers)\n\nreviews['Thread Review'] = reviews['Thread Review'].astype('int')\nreviews['Followers'] = reviews['Followers'].astype('int')\n\nreviews.drop('Metadata', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Time is in the form of string, for better analysis we can convert this to datetime object.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews['Time'] = reviews['Time'].apply(lambda x : datetime.strptime(x, '%m/%d/%Y %H:%M'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### No Review Restaurant","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews['Restaurant'].value_counts().nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 100 reviews for each restaurant, Which restaurants have not been reviewd?\n\ntemp = set(metadata['Name'].tolist()) - set(reviews['Restaurant'].tolist())\n\nprint(\"Restaurants which have no reviews.\", temp, sep = '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Details of the restaurants that have not been reviewd.\")\nmetadata[metadata['Name'].isin(temp)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reviews Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviewers = reviews['Reviewer'].value_counts().reset_index()\nreviewers.columns = ['Reviewer', 'Reviews']\n\nfig = px.histogram(reviewers, 'Reviews')\nfig.update_layout(title = \"Distribution in no of reviews:\",\n                 xaxis_title = \"No of Reviews\",\n                 yaxis_title = \"Given By users\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that most users have only given reviews once.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = reviewers.head()['Reviewer'].tolist()\nprint(\"People who have posted most reviews are :\", temp)\n\ndel temp, reviewers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's see if we'll encounter these names again in our analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Restaurants and Ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_ratings = reviews.groupby('Restaurant')['Rating'].mean().reset_index()\nmean_ratings.columns = ['Restaurant', 'Avg. Rating']\nreviews = pd.merge(reviews, mean_ratings, left_on = 'Restaurant', right_on = 'Restaurant')\nmean_ratings.sort_values(by='Avg. Rating', ascending = False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(name = \"Highest Avg. Ratings\",\n                     x = mean_ratings.head()['Restaurant'], y = mean_ratings.head()['Avg. Rating']))\nfig.add_trace(go.Bar(name = \"Lowest Avg. Ratings\",\n                     x = mean_ratings.tail()['Restaurant'], y = mean_ratings.tail()['Avg. Rating']))\n\nfig.update_layout(title = \"Restaurents with highest and lowest avg. ratings:\",\n                 xaxis_title = \"Restaurant Name\",\n                 yaxis_title = \"Avg. Rating\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time and Reviews","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews['Hour'] = reviews['Time'].dt.hour\nreviews['Month'] = reviews['Time'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour_counts = reviews['Hour'].value_counts().reset_index()\nhour_counts.columns = ['Hour', 'Count']\nhour_counts.sort_values(by = 'Hour')\nfig = px.bar(hour_counts, 'Hour', 'Count')\nfig.update_layout(title = \"Reviews submissions by day Hours:\",\n                 xaxis_title = \"Day Hour\",\n                 yaxis_title = \"No Of Reviews\")\nfig.show()\ndel hour_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see an obvious pattern in day-hour and no of reviews recorded during that hour. No one just reviews a restaurant as soon as they wake up.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"month_counts = reviews['Month'].value_counts().reset_index()\nmonth_counts.columns = ['month', 'Count']\nmonth_counts.sort_values(by = 'month')\nfig = px.bar(month_counts, 'month', 'Count')\nfig.update_layout(title = \"Reviews submissions by months:\",\n                 xaxis_title = \"Month\",\n                 yaxis_title = \"No Of Reviews\")\nfig.show()\ndel month_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We have an unusual pattern here, Recorded reviews are extremely less in the month of June as compared to other months. What chould be the reason?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = reviews.groupby(by='Hour')['Rating'].mean().reset_index()\nprint(temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We found No Significance difference here, So there is no point plotting it and going deep in it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews['Weekday'] = reviews['Time'].dt.weekday\nday_map = dict(zip(range(7), [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]))\nreviews['Weekday'] = reviews['Weekday'].map(day_map)\ndel day_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekday_count = reviews.groupby(by='Weekday')['Review'].count().reset_index()\n\nfig = go.Figure(data=[\n    go.Pie(labels = weekday_count['Weekday'],\n           values = weekday_count['Review'],\n          )\n])\n\nfig.update_traces(hoverinfo='label+value', textinfo='percent', textfont_size=20,\n                  marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title = \"No of Reviews by Week-Day:\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see a useual pattern here. People tend to go out on weekends more than on weekdays, this is why we have more reviews on weekends than on weekdays.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data_frame=reviews, x='Rating',\n            facet_col = 'Weekday', facet_col_wrap=4,\n            title = \"Rating Distribution amongst weekdays:\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Followers and Thread Reviews","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(reviews, x = 'Thread Review', y='Followers')\nfig.update_layout(title = \"Relationship b/w Threads and Followers\",\n                 xaxis_title = \"No Of Threads\",\n                 yaxis_title = \"No Of Followers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reviewers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviewers = reviews.groupby(by='Reviewer')['Followers', 'Thread Review'].sum().reset_index()\nreviewers.sort_values(by = ['Followers'], ascending = False, inplace=True)\n\nmost_followers = reviewers.head()\n\nreviewers.sort_values(by = ['Thread Review'], ascending = False, inplace=True)\n\nmost_threads = reviewers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows = 1, cols = 2, subplot_titles = ['Most Followers', 'Most Threads'])\n\nfig.add_trace(go.Bar(name=\"Followers\", x = most_followers['Reviewer'], y = most_followers['Followers']), 1,1)\nfig.add_trace(go.Bar(name=\"Threads\", x = most_followers['Reviewer'], y = most_followers['Thread Review']), 1,1)\n\nfig.add_trace(go.Bar(name=\"Followers\", x = most_threads['Reviewer'], y = most_threads['Followers']), 1,2)\nfig.add_trace(go.Bar(name=\"Threads\", x = most_threads['Reviewer'], y = most_threads['Thread Review']), 1,2)\n\n\nfig.update_xaxes(title_text=\"Reviewer\", row=1, col=1)\nfig.update_xaxes(title_text=\"Reviewer\", row=1, col=2)\n\nfig.update_yaxes(title_text=\"Sum\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sum\", row=1, col=2)\n\nfig.update_layout(title = \"Reviewers with:\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Did you notice something? 'Parijat Ray' in the list of Reviewer with most threads is from our list of people who posted most reviews.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Combining Datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cost vs Rating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(cuisines, reviews, left_on = 'Name', right_on = 'Restaurant')\ndf.drop(['Name', 'Time', 'Hour', 'Month'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = px.scatter(df, 'Cost', 'Avg. Rating', trendline = 'ols')\nfig.update_layout(title = \"Relationship between Cost and Avg. Raing of the restaurant\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see a positive correlation between cost and Avg. Rating of a Cuisine.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del metadata, reviews, cuisines","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Reviews (Text)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review = pd.read_csv(\"../input/zomato-restaurants-hyderabad/Restaurant reviews.csv\")\nreview = review[['Review', 'Rating']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review['Review']= review['Review'].apply(lambda x : x.replace('\\n', ' '))\nreview['Review']= review['Review'].apply(lambda x : x.lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ratings distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review.groupby(by='Rating')['Review'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Very few reviews have rating like, 1.5, 2.5, 3.5 and 4.5. (We have lesser data for some categories and more data for other categories). This will make classification process difficult.\n- We should merge them to get better performance. \n- For Analysis as well, this will be more helpful as this few records are similar to outliers.\n- 'Like' is a data redunduncy, better drop this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review = review[review['Rating']!='Like']\nreview['Rating']= review['Rating'].astype('float')\nreview['Rating'] = review['Rating'].apply(lambda x : int(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.groupby(by='Rating')['Review'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting meaningful words from reviews","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\nreview['Words'] = review['Review'].apply(word_tokenize)\n\nfrom nltk.corpus import stopwords \n\nStopWords = set(stopwords.words('english'))\n\ndef clean_words(x):\n    words = []\n    for i in x:\n        if i.isalnum() and i not in StopWords:\n            words.append(i)\n    return words\n\nreview['Words'] = review['Words'].apply(clean_words)\nreview['Word Count'] = review['Words'].apply(lambda x : len(x))\ndel StopWords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Review length by Rating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review.groupby(by='Rating')['Word Count'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(review, x='Word Count', color='Rating',\n            barmode = 'overlay', nbins=50, marginal = 'box')\nfig.update_layout(title = \"Word Count Distribution in Reviews by Ratings.\",\n                 xaxis_title = \"Word Count\",\n                 yaxis_title = \"No of Reviews\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.drop('Word Count', axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most common words by rating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common = dict()\n\nfor group, data in review.groupby(by='Rating'):\n    words = []\n    for i in data['Words'].tolist():\n        words.extend(i)\n    words = nltk.FreqDist(words)\n    words = words.most_common(10)\n    most_common['{}'.format(group)] = words\nprint(\"Most Common Words by ratings and their word-counts:\")\npd.DataFrame(most_common)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parts Of Speech","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"review['POS'] = review['Words'].apply(nltk.pos_tag)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adjectives","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_adjective(x):\n    adj = set(['JJ', 'JJR', 'JJS'])\n    word = []\n    for i in x:\n        if i[1] in adj:\n            word.append(i[0])\n    return word\n\nreview['ADJ'] = review['POS'].apply(get_adjective)\n\nmost_common = dict()\nfor group, data in review.groupby(by='Rating'):\n    words = []\n    for i in data['ADJ'].tolist():\n        words.extend(i)\n    words = nltk.FreqDist(words)\n    words = words.most_common(10)\n    most_common['{}'.format(group)] = words\nprint(\"Most Common Adjectives by ratings:\")\npd.DataFrame(most_common)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nouns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_noun(x):\n    noun = set(['NN', 'NNS', 'NNP', 'NNPS'])\n    word = []\n    for i in x:\n        if i[1] in noun:\n            word.append(i[0])\n    return word\n\nreview['Noun'] = review['POS'].apply(get_noun)\n\nreview.drop('POS', axis = 1, inplace = True)\n\nmost_common = dict()\nfor group, data in review.groupby(by='Rating'):\n    words = []\n    for i in data['Noun'].tolist():\n        words.extend(i)\n    words = nltk.FreqDist(words)\n    words = words.most_common(10)\n    most_common['{}'.format(group)] = words\nprint(\"Most Common Nouns by ratings:\")\npd.DataFrame(most_common)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A bigram (group of two words) better describes feelings than a single word. Let's see the bigrams","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Common Bigrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common = dict()\nfor group, data in review.groupby(by='Rating'):\n    words = []\n    for i in data['Words'].tolist():\n        words.extend(i)\n    bigram = list(nltk.bigrams(words))\n    bigram = nltk.FreqDist(bigram)\n    bigram = bigram.most_common(10)\n    most_common['{}'.format(group)] = bigram\n\nprint(\"Most Common Bi-grams by Ratings:\")\npd.DataFrame(most_common)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now, these bigrams have started making proper sense. Like, \"Worst Experiance\" is th most common bigram for rating 1, whereas \"Must Try\" is the most common bigram for rating 5. These bigram mean the same as their ratings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del most_common","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Polarity And Subjectivity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\nreview['Subjectivity'] = review['Review'].apply(lambda x : TextBlob(x).sentiment.subjectivity)\nreview['Polarity'] = review['Review'].apply(lambda x : TextBlob(x).sentiment.polarity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(review, x='Subjectivity', barmode='overlay', color='Rating')\nfig.update_layout(title = \"Subjectivity distribution in reviews of different ratings.\",\n                 xaxis_title = \"Subjectivity\",\n                 yaxis_title = \"Number of Reviews\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(review, x='Polarity', barmode='overlay', color='Rating')\n\nfig.update_layout(title = \"Polarity distribution in reviews of different ratings.\",\n                 xaxis_title = \"Subjectivity\",\n                 yaxis_title = \"Number of Reviews\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We do see difference in polarity distribution of reviews of different ratings. Which is how this should be. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Tf-idf","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text  import TfidfVectorizer\ntf = TfidfVectorizer(stop_words = 'english', ngram_range = (1,2),\n                    min_df = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = review['Review']\ny = review['Rating']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 1)\n\ntf_x_train = tf.fit_transform(x_train)\ntf_x_test = tf.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nperformance = {'Model' : [],\n              'Accuracy Score' : [],\n              'Precision Score' : [],\n              'Recall Score' : [],\n              'f1 Score' : []}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr= LogisticRegression()\nlr.fit(tf_x_train, y_train)\npred = lr.predict(tf_x_test)\n\nperformance['Model'].append('LogisticRegression')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(tf_x_train, y_train)\npred = sgd.predict(tf_x_test)\n\nperformance['Model'].append('SGD')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(tf_x_train, y_train)\npred = mnb.predict(tf_x_test)\n\nperformance['Model'].append('Multinomial NB')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\n\nbnb = BernoulliNB()\nbnb.fit(tf_x_train, y_train)\npred = bnb.predict(tf_x_test)\n\nperformance['Model'].append('Bernoulli NB')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(tf_x_train, y_train)\npred = svc.predict(tf_x_test)\n\nperformance['Model'].append('SVC')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(tf_x_train, y_train)\npred = linear_svc.predict(tf_x_test)\n\nperformance['Model'].append('Linear SVC')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nrfc.fit(tf_x_train, y_train)\npred = rfc.predict(tf_x_test)\n\nperformance['Model'].append('Random Forest')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(performance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Voted Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statistics import mode\n\nclass voted_classifier():\n    def __init__(self):\n        self.classifiers = [lr, sgd, mnb, bnb, svc, linear_svc, rfc]\n        \n    def classify(self, features):\n        names = ['lr', 'sgd', 'mnb', 'bnb', 'svc', 'linear_svc', 'rfc']\n        i = 0 \n        votes = pd.DataFrame()\n        for classifier in self.classifiers:\n            pred = classifier.predict(features)\n            votes[names[i]] = pred\n            i+=1\n        return votes.mode(axis = 1)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc = voted_classifier()\npred = vc.classify(tf_x_test)\n\nperformance['Model'].append('Voted Classifier')\nperformance['Accuracy Score'].append(accuracy_score(y_test, pred))\nperformance['Precision Score'].append(precision_score(y_test, pred, average='macro'))\nperformance['Recall Score'].append(recall_score(y_test, pred, average='macro'))\nperformance['f1 Score'].append(f1_score(y_test, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(performance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Thank you. Do share your thoughts and suggestions in the comments. (Work on progress to ahieve better accuracy.)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}