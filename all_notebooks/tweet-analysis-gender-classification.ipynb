{"cells":[{"metadata":{"id":"SbrhJjL04R2z"},"cell_type":"markdown","source":"## Exploratory Data Analysis and Sample ML Models on the Twitter Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The dataset is obtained from https://www.kaggle.com/darkknight98/twitter-data . The objective is to determine which gender commits more typos on twitter, based on approx. 20000 user tweets and other tweet/user related meta-data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Read on to see boiler-plate implementations of different ML algorithms applied on the dataset. I've provided implementations using just the text and without using text. Models tested include an ensemble of simple classifiers, logistic regression,Random Forest, SVC and MultinomialNB along with a basic Neural network.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Brief Contents\n\n\n* [Understanding the data](#under)\n* [Nulls Checking & Removal](#nulls)\n* [Regexp based text preprocessing](#regexp)\n* [Encoding Cat-Features](#encoding)\n* [Data Visualizations](#dv)\n* [Gender Analysis with Just Text](#text)\n* [Gender Analysis without Text](#notext)\n    - [Ensembling](#ensembling)\n    - [Simple NN](#nn)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"P.S: Also find the regexp based vectorization of text, and preprocessing of tweet-text so as to make it a useful feature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"under\"></a>\n## Understanding the Data","execution_count":null},{"metadata":{"id":"yinVfTDk4R20"},"cell_type":"markdown","source":"###  Importing the required modules ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"zZH5GXKZ4R20","outputId":"e43e6cf4-884a-4625-f95a-6581d7f153c0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"w9p80xfZ4R24"},"cell_type":"markdown","source":"### Loading the dataset with suitable encoding","execution_count":null},{"metadata":{"id":"G7WPPx-o4R25","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/tweet_data.csv\",encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"id":"YiyekS9k4R27","outputId":"90893884-c72b-4ac4-a2a5-6e8d814baf6d","trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"QCjsWGFc4R2-"},"cell_type":"markdown","source":"### Dropping the columns which are of less significance.","execution_count":null},{"metadata":{"id":"aEAa8rND4R2-","trusted":true},"cell_type":"code","source":"col=[\"profileimage\",\"tweet_location\",\"user_timezone\",\"sidebar_color\",\"tweet_coord\",\"link_color\",\"fav_number\",\"tweet_id\",\"_last_judgment_at\",\"created\",\"tweet_created\"]\ndf.drop(col,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"YpkyVoYg4R3B","outputId":"b4cac5b9-2674-4d44-81e0-54f90fe1b2d1","trusted":true},"cell_type":"code","source":"df\n## Sanity Check ##","execution_count":null,"outputs":[]},{"metadata":{"id":"VK4ux9Nq4R3D"},"cell_type":"markdown","source":"### Removing duplicates from the dataframe","execution_count":null},{"metadata":{"id":"35oXrj8p4R3E","trusted":true},"cell_type":"code","source":"df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"nulls\"></a>\n## Nulls Checking and removal","execution_count":null},{"metadata":{"id":"URM1TcbI4R3G"},"cell_type":"markdown","source":"### Checking for null values across the columns","execution_count":null},{"metadata":{"id":"hTTlhY8c4R3G","outputId":"88a831b7-1986-4b68-de73-f33499679b90","trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"3I97DkwC4R3J"},"cell_type":"markdown","source":"### Dropping null values in gender and gender:confidence columns as they are of no significance with gender being the dependant variable","execution_count":null},{"metadata":{"id":"O3hc3h_F4R3J","trusted":true},"cell_type":"code","source":"c=[\"gender\",\"gender:confidence\"]\ndf.dropna(subset=c,how=\"any\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"M8GguM0z4R3L","outputId":"e14c66d5-9737-481a-aa9d-958141040d42","trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"id":"UIlt7akq4R3N"},"cell_type":"markdown","source":"### Checking null values that still exist in the entire dataframe","execution_count":null},{"metadata":{"scrolled":true,"id":"wvrkK0Xt4R3N","outputId":"0580d41a-7ad2-496c-e1cc-1ff323a85fe5","trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"TV0LQKFk4R3P"},"cell_type":"markdown","source":"### Replacing null values in text column with empty string","execution_count":null},{"metadata":{"id":"ztGrXUWx4R3Q","trusted":true},"cell_type":"code","source":"df[\"text\"].fillna(\"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"v3YiVr-04R3R"},"cell_type":"markdown","source":"### Replacing null values in description column with empty string","execution_count":null},{"metadata":{"id":"bq56VOTA4R3S","trusted":true},"cell_type":"code","source":"df[\"description\"].fillna(\"\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"regexp\"></a>\n# Reg-exp based text preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Heads up: The two cells below take long to run to completion","execution_count":null},{"metadata":{"id":"gpEmPasY4R3U"},"cell_type":"markdown","source":"### Removing the special characters and hyperlinks in text column using Regular Expressions","execution_count":null},{"metadata":{"id":"Hxt0C5Yw4R3U","trusted":true},"cell_type":"code","source":"o=list(df[\"text\"])\nimport re\n\nl=[]\nk=[]\nfor s in df[\"text\"] :\n    a=re.sub(r\"http://t.co/[a-zA-Z0-9]*\",\" \",str(s))\n    b=re.sub(r\"https://t.co/[a-zA-Z0-9]*\",\" \",str(s))\n    \n    l.append(a)\n    k.append(b)\n    \ndf.replace(inplace=True, to_replace=o, value=l)\no=list(df[\"text\"])\ndf.replace(inplace=True, to_replace=o, value=k)\n    \ndf[\"text\"].replace(regex=True, inplace=True, to_replace=r'[,!.; -@!%^&*)(]', value=' ')\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"I6xzS0s44R3W"},"cell_type":"markdown","source":"### Removing the special characters and hyperlinks in description column using regular Expressions","execution_count":null},{"metadata":{"id":"U3-hZdLJ4R3X","trusted":true},"cell_type":"code","source":"o=list(df[\"description\"])\nimport re\n\nl=[]\nk=[]\nfor s in df[\"description\"] :\n    s=re.sub(r\"http://t.co/[a-zA-Z0-9]*\",\" \",str(s))\n    s=re.sub(r\"https://t.co/[a-zA-Z0-9]*\",\" \",str(s))\n    \n    l.append(s)\n    k.append(s)\n    \ndf.replace(inplace=True, to_replace=o, value=l)\ndf.replace(inplace=True, to_replace=o, value=k)\n\ndf[\"description\"].replace(regex=True, inplace=True, to_replace=r'[,!.; -@!%^&*)(]', value=' ')","execution_count":null,"outputs":[]},{"metadata":{"id":"wQpeLWvg4R3Z","outputId":"b673b0d1-554f-4142-a483-d904af396291","trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"BKmqIErD4R3c","outputId":"2e8836a8-a38a-4a4a-d281-daef4ab3f58d","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"XcuXPCgy4R3e","outputId":"7a59970b-fbde-447d-9429-5e033fc47a13","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"-GlfmW6E4R3f","outputId":"90ae3f14-ceaf-4813-ab19-3558f8ed5f7f","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"9pMUB7Yp4R3h","outputId":"952a4830-7303-4d39-e55e-0c9088785158","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the number of unique values of columns","execution_count":null},{"metadata":{"id":"4yQg4PTJ4R3j","outputId":"176aeab2-69d9-45db-fe68-aec0eed236cf","trusted":true},"cell_type":"code","source":"df.gender.nunique()","execution_count":null,"outputs":[]},{"metadata":{"id":"vlt1-6M24R3k","outputId":"64258b79-665d-4a7b-a5f9-44091db0876c","trusted":true},"cell_type":"code","source":"df.gender.unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"DomfY40A4R3m","outputId":"89868f5a-13cd-4bb4-e93e-37eb9e88bb3e","trusted":true},"cell_type":"code","source":"df._golden.unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"6n2npk_-4R3o","outputId":"82e72d32-5638-4bcb-9184-4c04512c33a2","trusted":true},"cell_type":"code","source":"df._unit_state.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"encoding\"></a>\n## Encoding the Cat-Features","execution_count":null},{"metadata":{"id":"gWjbsla-4R3q"},"cell_type":"markdown","source":"### Converting Categorical data to Numerical data","execution_count":null},{"metadata":{"id":"A4LIgKFD4R3q","outputId":"4740bac5-d9f1-4cb8-dff4-0eec538c411a","trusted":true},"cell_type":"code","source":"num_col=df.select_dtypes(include=np.number).columns\nprint(\"Numerical Columns :\\n\",num_col)\ncat_col=df.select_dtypes(exclude=np.number).columns\nprint(\"Categorical Columns :\\n\",cat_col)","execution_count":null,"outputs":[]},{"metadata":{"id":"M0WTo7pU4R3s","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndf['gender']=le.fit_transform(df['gender'])#Converts brand=0,female=1,male=2,unknown=3\ndf['_golden']=le.fit_transform(df['_golden'])#Converts true as 1 and false as 0\ndf['_unit_state']=le.fit_transform(df['_unit_state'])#converts finalized as 0 and golden as 1","execution_count":null,"outputs":[]},{"metadata":{"id":"XgKzV91b4R3u","outputId":"6794c488-76d8-4cd9-ec23-42babba214ed","trusted":true},"cell_type":"code","source":"df.gender.unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"J4_KvAP54R3v","outputId":"d8771ddc-c63e-42e1-d4d3-edfefef3b39c","trusted":true},"cell_type":"code","source":"df._golden.unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"WiCo4JqX4R3x","outputId":"93124c8a-8792-4b2d-92c1-eddc88302051","trusted":true},"cell_type":"code","source":"df._unit_state.unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"scu2sQut4R3z","outputId":"aa1282a5-4bfd-4b7c-f83b-3b6ec2d732a0","trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"wix6NZLC4R31","outputId":"8dafada0-2d48-4494-f252-7d0db4880655","trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"Gm0rnmUx4R33"},"cell_type":"markdown","source":"<a id=\"dv\"></a>\n## Data Visualization","execution_count":null},{"metadata":{"id":"e7Bv0nA-4R33"},"cell_type":"markdown","source":"### Histogram of gender ","execution_count":null},{"metadata":{"id":"82MmnuPk4R34","outputId":"37c750fd-cf80-478d-ccb3-6963a3155bdc","trusted":true},"cell_type":"code","source":"df.gender.plot(kind='hist')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"lkes5YgV4R35"},"cell_type":"markdown","source":"fig=plt.figure(figsize=(10,5))\nplt.bar(df.gender,df.tweet_count,color='maroon',width=0.4)\nplt.show()","execution_count":null},{"metadata":{"id":"4Xi2A5qc4R36"},"cell_type":"markdown","source":"### Bar plot against gender and tweet count","execution_count":null},{"metadata":{"id":"SRDn7huL4R36","outputId":"a670c8fb-53b2-42d6-eab0-8dab9c0607d4","trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(10,5))\nplt.bar(df.gender,df.tweet_count,color='maroon',width=0.4)\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Tweet count\")\nplt.title(\"Tweet count based on gender\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"9y6uHsTJ4R39"},"cell_type":"markdown","source":"###  Seaborn heatmap for a correlation matrix","execution_count":null},{"metadata":{"id":"l8aO882v4R39","outputId":"4cc5790a-fa73-4f25-f3b7-315690fa2452","trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr(),annot=True,fmt='.1g',cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"SROkSIrb4R4A","outputId":"0a335aa6-4a87-46d1-aca7-014f644a96d1","trusted":true},"cell_type":"code","source":"matrix=np.triu(df.corr())\nsns.heatmap(df.corr(),annot=True,mask=matrix)","execution_count":null,"outputs":[]},{"metadata":{"id":"eEz1L0HQd7zv"},"cell_type":"markdown","source":"<a id=\"text\"></a>\n## Gender Analysis with just Text","execution_count":null},{"metadata":{"id":"zhhyF4Km4R4C","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"id":"B7dYLUtw4R4E","outputId":"8b3777fd-dc52-481d-a932-0e189525e121","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"bYWBW24EH6dC","trusted":true},"cell_type":"code","source":"def normalize_text(s):\n    # just in case\n    s = str(s)\n    s = s.lower()\n    \n    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W\\s',' ',s)\n    \n    # make sure we didn't introduce any double spaces\n    s = re.sub('\\s+',' ',s)\n    \n    return s\ndf['text_norm'] = [normalize_text(s) for s in df['text']]\ndf['description_norm'] = [normalize_text(s) for s in df['description']]","execution_count":null,"outputs":[]},{"metadata":{"id":"TFrob_Tx4R4G","outputId":"ed7eb14d-1097-4435-8cca-e8462044b5f0","trusted":true},"cell_type":"code","source":"df['all_features'] = df['text_norm'].str.cat(df['description_norm'], sep=' ')\ndf_confident = df[df['gender:confidence']==1] ## Choosing only the one's with confidence\ndf_confident.shape #Now we have approx 14000 entries.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This is where we vectorize the tweets such that they are now a candidate for a feature","execution_count":null},{"metadata":{"id":"EEnfkVLXJBTO","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nx = vectorizer.fit_transform(df_confident['text_norm'])\nencoder = LabelEncoder()\ny = encoder.fit_transform(df_confident['gender'])","execution_count":null,"outputs":[]},{"metadata":{"id":"C0jFu0GYgjWJ","outputId":"315a3a47-c718-4a67-dbf0-a91e4d6d4394","trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom catboost import CatBoostClassifier\n#nb = CatBoostClassifier(silent = True)\n\n#### Multi-Nomial Naive Bayes was found to give considerably good peroformance with this data ####\n\nnb = MultinomialNB(alpha = 0.6,fit_prior = True)\nnb.fit(x_train, y_train)\n\nprint(nb.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy's between 54 - 59 % can be obtained with suitable tuning","execution_count":null},{"metadata":{"id":"rzcul69Qha9F","outputId":"7f0d0395-dd85-4802-cd42-315baf0ee8fc","trusted":true},"cell_type":"code","source":"### Just to illustrate how the text data looks like ###\ndf_just_text = pd.DataFrame(x)\ndf_just_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"notext\"></a>\n## Gender Analysis without Text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Analysis using important features without text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The following section examines predictive modelling of typos even without using the 'Text' of the tweet. Let's see how linear models and GBDTs perform in this scenario.","execution_count":null},{"metadata":{"id":"fz0zX_PO4R4I","trusted":true},"cell_type":"code","source":"X=df[['_unit_id','_golden','_unit_state','_trusted_judgments','gender:confidence','profile_yn:confidence','retweet_count','tweet_count']]","execution_count":null,"outputs":[]},{"metadata":{"id":"1GpFpO0MJifH","outputId":"22f5892d-cf0a-4416-d982-a4db54b3eecd","trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"XTOqzmCm4R4K","outputId":"777a6deb-4aac-40be-95e1-60ef3ea07ef7","trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"id":"FOE6inob4R4M","trusted":true},"cell_type":"code","source":"Y=df[['gender']]","execution_count":null,"outputs":[]},{"metadata":{"id":"YNZHC2XEZiZ5","outputId":"a19cef11-6809-4539-8add-6c34699a50bb","trusted":true},"cell_type":"code","source":"df_conf = df[df['gender:confidence']==1]\ndf_conf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"ohO1v7Zt4R4O","trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"ukxIye567XT9"},"cell_type":"markdown","source":"<a id=\"ensembling\"></a>\n## Ensembling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I have used the following code in several of my notebooks, and i suggest the reader to use this as a template for any other ensembling tasks to save your time! I have provided a boiler plate, Naive (non-optimized) ensemble of 20+ algos here.","execution_count":null},{"metadata":{"id":"LX4ZMDeN78iv","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process,model_selection\nimport xgboost\nfrom xgboost import XGBClassifier\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    #svm.SVC(probability=True),\n    #svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost\n    XGBClassifier()    \n    ]\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nMLA_predict = Y['gender']\nrow_index = 0\nX1 = X.copy()\nfor alg in MLA:\n    #print(row_index)\n    X = X1\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    print('Examining ',MLA_name)\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    cv_results = model_selection.cross_validate(alg, X, Y, cv  = cv_split)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    alg.fit(X, Y)\n    MLA_predict[MLA_name] = alg.predict(X)\n    row_index+=1\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the accuracies are lesser than the models that use text data (which is kind of obvious), Seems like GBClassifier and XGBoost does the best on this data!","execution_count":null},{"metadata":{"id":"aJi7rpGjAeDE","trusted":true},"cell_type":"code","source":"#### Taking 4 Ensembles ###\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process,model_selection\nimport xgboost\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    XGBClassifier(),\n    CatBoostClassifier(verbose = False)    ## Just to see how it does! ##\n    ]\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nMLA_predict = Y['gender']\nrow_index = 0\nX1 = X.copy()\nfor alg in MLA:\n    X = X1\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    print(MLA_name)\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    cv_results = model_selection.cross_validate(alg, X, Y, cv  = cv_split)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    alg.fit(X, Y)\n    MLA_predict[MLA_name] = alg.predict(X)\n    row_index+=1\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"id":"AJ3FGcBvCEIu","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Uncomment and run whichever you feel like!","execution_count":null},{"metadata":{"id":"ARs0_95l4R4Q","trusted":true},"cell_type":"code","source":"#a=XGBClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n#a=GradientBoostingClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n#a=CatBoostClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n#a=AdaBoostClassifier(num_rounds = 150,min_split_leaf = 10,max_depth = 3,random_state=100)\n\na = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"id":"mzmlae6M4R4R","outputId":"faf92024-1698-4760-b30e-bee0911234e9","trusted":true},"cell_type":"code","source":"a.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"sf8LbFvs4R4T","trusted":true},"cell_type":"code","source":"y_pred=a.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Nj_EGA0J4R4W","outputId":"31d40643-2fbe-4e8e-ac6a-c01b8e662fd5","trusted":true},"cell_type":"code","source":"score=accuracy_score(Y_test,y_pred)\nscore*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"nn\"></a>\n## Just trying out a simple NN for completeness","execution_count":null},{"metadata":{"id":"sOjs3BH64R4Z","trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"id":"gK7x4sED4R4b","trusted":true},"cell_type":"code","source":"model=tf.keras.Sequential([\n    tf.keras.layers.Dense(units=8,input_dim=X_train.shape[1],activation='relu'),\n     tf.keras.layers.LeakyReLU(0.3),\n    tf.keras.layers.Dense(units=1,activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"id":"ibyBh61t4R4d","trusted":true},"cell_type":"code","source":"model.compile(loss = 'mean_squared_error',optimizer = 'adam',metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"XbdA_ru34R4h","outputId":"31c368c6-6e31-4a34-ab04-b4b0d9d3b28a","trusted":true},"cell_type":"code","source":"model.fit(X_train,Y_train,epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"id":"wr3_ioGv4R4i","trusted":true},"cell_type":"code","source":"y_pred=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"XXcE8auq4R4k","trusted":true},"cell_type":"code","source":"score=accuracy_score(Y_test,y_pred)\nscore*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, 2 Layered-NN ? Not that great I guess!, But there's still scope for improving it. But I'll stop here as i feel i have covered enough for you to take it from here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## So we have seen several implementations for analysis on twitter data with different algorithms using text and without the text (using just meta-data)too. Hope it was helpful!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}