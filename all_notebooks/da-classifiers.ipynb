{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the required packages \nimport numpy as np \nimport pandas as pd \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report\n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.cluster import KMeans\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nimport matplotlib.pyplot as plt\nAccuracy={}\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"CSV_COLUMN_NAMES = ['Date','Time','NO2(GT)','T','RH','AH','Weekday','hour']\ntrain_path=\"../input/train.csv\"\nlabel_name='NO2(GT)'\ntrain = pd.read_csv(filepath_or_buffer=train_path,\n                        names=CSV_COLUMN_NAMES,  # list of column names\n                        header=0,  # ignore the first row of the CSV file.\n                        skipinitialspace=True,\n                        #skiprows=1\n                       )\n    \ntrain.pop('Time')\ntrain.pop('Date')\ntrain.pop('T')\ntrain['hour'] = train['hour'].astype(str)\ntrain.Weekday= train.Weekday.astype(str)\n# train['T']= train['T'].astype(float)\ntrain_features, train_label = train, train.pop(label_name)\n\nprint(train_features.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ccb5337ce02d9706f9a09a3a5a13c98e8751673"},"cell_type":"code","source":"CSV_COLUMN_NAMES = ['Date','Time','NO2(GT)','T','RH','AH','Weekday','hour']\ntest_path=\"../input/test.csv\"\nlabel_name='NO2(GT)'\ntest = pd.read_csv(filepath_or_buffer=test_path,\n                        names=CSV_COLUMN_NAMES,  # list of column names\n                        header=0,  # ignore the first row of the CSV file.\n                        skipinitialspace=True,\n                        #skiprows=1\n                       )\n    \ntest.pop('Time')\ntest.pop('Date')\ntest.pop('T')\ntest['hour'] = test['hour'].astype(str)\ntest.Weekday= test.Weekday.astype(str)\n# test['T']= test['T'].astype(float)\ntest_features, test_label = test, test.pop(label_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35a21255668cf4efc9b27105d0e52c9d1cf54fb3"},"cell_type":"code","source":"clf = DecisionTreeClassifier()\nclf = clf.fit(train_features, train_label)\npredict_label=clf.predict(test_features)\nprint(\"Results of Decison tree\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'DecisionTree':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d541b2366a3c7d0555773087e0202659998b8d64"},"cell_type":"code","source":"clf = RandomForestClassifier()\nclf = clf.fit(train_features, train_label)\npredict_label=clf.predict(test_features)\nprint(\"Results of Random Forest\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'RandomForest':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06f37cacbaf35c95cc0b534489bd7a9c6bb857f8"},"cell_type":"code","source":"clf = svm.SVC(gamma='auto')\nclf = clf.fit(train_features, train_label)\npredict_label=clf.predict(test_features)\nprint(\"Results of SVM Classifier\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'SVM':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc3c2cef0b877a52a973b539ad4e40a58802cbe2"},"cell_type":"code","source":"clf = GaussianNB()\nclf = clf.fit(train_features, train_label)\npredict_label=clf.predict(test_features)\nprint(\"Results of Naive Bayes\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'Naive_Bayes':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13f93b0945b03799e32d0371379dc2df82edbbd8"},"cell_type":"code","source":"clf = AdaBoostClassifier()\nclf = clf.fit(train_features, train_label)\npredict_label=clf.predict(test_features)\nprint(\"Results of AdaBoost\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'AdaBoost':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"394b4704affd0de18f48db03d1257b5ac02d3117"},"cell_type":"code","source":"clf = GradientBoostingClassifier()\nclf = clf.fit(train_features, train_label)\npredict_label=clf.predict(test_features)\nprint(\"Results of GradientBoosting\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'GradientBoosting':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8215891644bbf5f6c78448ed2087a101eb2996a"},"cell_type":"code","source":"clf = KMeans(n_clusters=2)\nclf = clf.fit(train_features, train_label)\npredict_label=clf.predict(test_features)\nprint(\"Results of Kmeans Clustering\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'Kmeans_Clustering':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16e40788bb256351f22f90e0f844c310123426ca"},"cell_type":"code","source":"#creating normal features and crossed features for the nn model\n\nWeekday = tf.feature_column.categorical_column_with_vocabulary_list(\n    'Weekday', ['0', '1', '2', '3', '4','5','6'])\nhour = tf.feature_column.categorical_column_with_vocabulary_list(\n    'hour', [ '0', '1', '2', '3', '4','5','6','7','8','10','12','13','14','15','16','17','18','19','20','21','22','23'])\n# T = tf.feature_column.numeric_column(key='T',dtype=tf.float64)\nRH = tf.feature_column.numeric_column(key='RH',dtype=tf.float64)\nAH = tf.feature_column.numeric_column(key='AH',dtype=tf.float64)\n\nbase_columns = [\n    tf.feature_column.indicator_column(Weekday),\n    tf.feature_column.indicator_column(hour),\n#     T,\n    RH,\n    AH\n]\n\nWeekday_x_hour = tf.feature_column.crossed_column(\n    ['Weekday', 'hour'], hash_bucket_size=1000)\n\ncrossed_columns = [\n     tf.feature_column.indicator_column(Weekday_x_hour)\n]\n\n#running Dnn classifer model with the features designed above and with and input layer with 4 nodes\nclassifier = tf.estimator.DNNClassifier(feature_columns=base_columns+crossed_columns,hidden_units=[5],n_classes=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc40cf1887179c56e6d4c808f465cbf5d2f9b28a"},"cell_type":"code","source":"#training the nnmodel\n\ndef train_input_fn(features, labels, batch_size):\n        dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n        \n        dataset = dataset.shuffle(buffer_size=1000).repeat(count=None).batch(batch_size)\n        \n        return dataset.make_one_shot_iterator().get_next()\n\n\nclassifier.train(\n        input_fn=lambda:train_input_fn(train_features, train_label, 50 ),\n        steps=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d69cd473b0e84c0807c98c03180681fcd06108c6"},"cell_type":"code","source":"#eval_input_fn() is similar to train_input_fn()  it helps in shuffling data and providing input as batches \ndef eval_input_fn(features, labels=None, batch_size=None):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n\n    # Convert inputs to a tf.dataset object.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n    # Batch the examples\n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n\n    # Return the read end of the pipeline.\n    return dataset.make_one_shot_iterator().get_next()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d42b781c7fd6d20b95d98d2b8548126fecd61a70"},"cell_type":"code","source":"test_df=pd.read_csv(\"../input/test.csv\",parse_dates=True)\n\npredict_x= { \n#     'T':[],\n    'Weekday':[],\n    'hour':[ ],\n    'RH':[ ],\n    'AH':[ ],\n}\n\n\n# predict_x['T'] = test_df['T'].astype(float)\npredict_x['RH'] = test_df['RH'].astype(float)\npredict_x['AH'] = test_df['AH'].astype(float)\npredict_x['hour']= test_df.hour.astype(str)\npredict_x['Weekday']= test_df.Weekday.astype(str)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6f099b4c4f086c7b85ef1876f171ea7b4cf3af5"},"cell_type":"code","source":"#predicting using classifier. \npredictions = classifier.predict(\n    input_fn=lambda:eval_input_fn(predict_x,\n                                  labels=None,\n                                  batch_size=50))\n\ni=0\npredict_label=[]\nfor pred_dict in zip(predictions):\n    if pred_dict[0]['classes'] == [b'1']:      \n        predict_label.append(1)\n        \n    else:\n        predict_label.append(0)\n    i=i+1\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b24c6344a902d770aadb1aa296dfa75215fa33ec"},"cell_type":"code","source":"print(\"Results of Neural Network\")\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix( test_label, predict_label)) \n      \nprint (\"Accuracy : \")\nprint(accuracy_score(test_label,predict_label)*100) \nAccuracy.update({'DNN':accuracy_score(test_label,predict_label)*100})\n\nprint(\"Report : \")\nprint(classification_report(test_label, predict_label)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e27c5e7895ec402be6efffc4300d24ecc295850c"},"cell_type":"code","source":"y_pos = np.arange(len(Accuracy.keys()))\nplt.barh(y_pos,list(Accuracy.values()), align='center', alpha=0.5)\nplt.yticks(y_pos, list(Accuracy.keys()))\n\nplt.xlabel('Accuracy')\nplt.title('Classifiers Comparsion')\nplt.xlim(0, 100) \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4245d9bcf59ce6c918a396f4742e72056fc0f839"},"cell_type":"code","source":"\"\"\"\n1) All ensemble learning methods (such as AdaBoost, Gradient Boost, Random forest) have performed better \nwith an accuracy around 80%. With Gradient boost being the best with accuracy of 79.00326%. \n\n2) K Mean clustering with only 2  clusters has an accuracy of  42.40196%. The reason for low accuracy \ncould be explained by “Curse of Increase Dimensionality”. Since KNN performs poorly as no of input \nfields increases, five input features  has decreased the accuracy and accuracy  continues to decreases\nif we increase the no of clusters.\n\n\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}