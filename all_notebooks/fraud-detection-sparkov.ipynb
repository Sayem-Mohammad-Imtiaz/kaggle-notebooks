{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install dgl\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pd.unique(df.cc_num))\n\nprint(df.cc_num.value_counts())\nsns.countplot(df.cc_num)\n\nlen(pd.unique(df.merchant))\n\nprint(df.merchant.value_counts())\nsns.countplot(df.merchant)\n\nprint(df.is_fraud.value_counts())\nsns.countplot(df.is_fraud)\n\n# customer_node = set(df['cc_num'])\n# merchant_node = set(df['merchant'])\n\n# print(f'Intersection: {len(customer_node.intersection(merchant_node))}')\n# print(f'Src diff: {len(customer_node.difference(merchant_node))}')\n# print(f'Des diff: {len(merchant_node.difference(customer_node))}')\n\n# mapping = dict(zip(customer_node, range(len(df))))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"user_transaction = df[['cc_num', 'trans_num', 'category', 'amt', 'is_fraud']]\nuser_transaction.to_csv('user_transaction.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchant_transaction = df[['merchant', 'trans_num', 'category', 'amt', 'is_fraud']]\nmerchant_transaction.to_csv('merchant_transaction.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merchant_transaction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NodeClassicication Heterogenous","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install dgl\nimport seaborn as sns\nimport dgl\nimport torch\nimport dgl.nn as dglnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport seaborn as sns\nfrom matplotlib import rcParams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndf1 = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv')\ndf2 = pd.read_csv('/kaggle/input/fraud-detection/fraudTest.csv')\n\ndf1['train_mask'] = 1\ndf1['test_mask'] = 0\ndf2['train_mask'] = 0\ndf2['test_mask'] = 1\ndf = pd.concat([df1, df2])\n\n\ndef encode_data(input):\n\n    customer_node = set(df['cc_num'])\n    merchant_node = set(df['merchant'])\n    transaction_node = set(df['trans_num'])\n    category = set(df['category'])\n    city = set(df['city'])\n\n    mapping_c = dict(zip(customer_node, range(len(customer_node))))\n    mapping_m = dict(zip(merchant_node, range(len(merchant_node))))                 \n    mapping_t = dict(zip(transaction_node, range(len(df))))\n    mapping_cat = dict(zip(category, range(len(df))))\n    mapping_city = dict(zip(city, range(len(df))))\n    \n    output = input.copy()\n    output['customer_id'] = output.cc_num.apply(lambda x: mapping_c[x])\n    output['merchant_id'] = output.merchant.apply(lambda x: mapping_m[x])\n    output['transaction_id'] = output.trans_num.apply(lambda x: mapping_t[x])\n    output['category_id'] = output.category.apply(lambda x: mapping_cat[x])\n    output['city_id'] = output.city.apply(lambda x: mapping_city[x])\n\n    return output\n\nencoded_df = encode_data(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df1.shape[0])\nprint(df2.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of customers', pd.unique(encoded_df.customer_id).shape[0])\nprint('Number of customers in train', pd.unique(encoded_df[encoded_df['train_mask'] == 1].customer_id).shape[0])\nprint('Number of customers in test', pd.unique(encoded_df[encoded_df['test_mask'] == 1].customer_id).shape[0])\n\nprint('Number of merchants', pd.unique(encoded_df.merchant_id).shape[0])\nprint('Number of merchants in train', pd.unique(encoded_df[encoded_df['train_mask'] == 1].merchant_id).shape[0])\nprint('Number of merchants in test', pd.unique(encoded_df[encoded_df['test_mask'] == 1].merchant_id).shape[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_train = set(pd.unique(encoded_df[encoded_df['train_mask'] == 1].customer_id))\nc_test = set(pd.unique(encoded_df[encoded_df['test_mask'] == 1].customer_id))\n\nprint(\"Number of customers in train but not in test\", len(c_train.difference(c_test)))\n\nm_train = set(pd.unique(encoded_df[encoded_df['train_mask'] == 1].merchant_id))\nm_test = set(pd.unique(encoded_df[encoded_df['test_mask'] == 1].merchant_id))\n\nprint(\"Number of merchants in train but not in test\", len(m_train.difference(m_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df.is_fraud.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Percent of fraud {n_not_fraud/(n_not_fraud + n_fraud) * 100:.2f}')\nprint(f'Percent of fraud {n_fraud/(n_not_fraud + n_fraud) * 100:.2f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(encoded_df.is_fraud.value_counts())\n# sns.catplot(encoded_df.is_fraud)\n\n\ng = sns.catplot( y='is_fraud', kind='bar', data=encoded_df)\ng.ax.set_ylim(0,100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nprint(encoded_df.is_fraud.value_counts())\nsns.countplot(encoded_df.is_fraud)\n\n# customer_node = set(df['cc_num'])\n# merchant_node = set(df['merchant'])\n\n# print(f'Intersection: {len(customer_node.intersection(merchant_node))}')\n# print(f'Src diff: {len(customer_node.difference(merchant_node))}')\n# print(f'Des diff: {len(merchant_node.difference(customer_node))}')\n\n# mapping = dict(zip(customer_node, range(len(df))))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_relation(encoded_df):\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud_item = encoded_df[encoded_df['is_fraud'] == 1]\nnon_fraud_item = encoded_df[encoded_df['is_fraud'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Edge Classicication Heterogenous","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install dgl\nimport seaborn as sns\nimport dgl\nimport torch\nimport dgl.nn as dglnn\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndf1 = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv')\ndf2 = pd.read_csv('/kaggle/input/fraud-detection/fraudTest.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['train_mask'] = 1\ndf1['test_mask'] = 0\ndf2['train_mask'] = 0\ndf2['test_mask'] = 1\ndf = pd.concat([df1, df2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_data(input):\n\n    customer_node = set(df['cc_num'])\n    merchant_node = set(df['merchant'])\n    transaction_node = set(df['trans_num'])\n    category = set(df['category'])\n    city = set(df['city'])\n\n    mapping_c = dict(zip(customer_node, range(len(customer_node))))\n    mapping_m = dict(zip(merchant_node, range(len(merchant_node))))                 \n    mapping_t = dict(zip(transaction_node, range(len(df))))\n    mapping_cat = dict(zip(category, range(len(df))))\n    mapping_city = dict(zip(city, range(len(df))))\n    \n    output = input.copy()\n    output['customer_id'] = output.cc_num.apply(lambda x: mapping_c[x])\n    output['merchant_id'] = output.merchant.apply(lambda x: mapping_m[x])\n    output['transaction_id'] = output.trans_num.apply(lambda x: mapping_t[x])\n    output['category_id'] = output.category.apply(lambda x: mapping_cat[x])\n    output['city_id'] = output.city.apply(lambda x: mapping_city[x])\n\n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df = encode_data(df)\n# tuples = [tuple(x) for x in encoded_df[['customer_id', 'merchant_id']].to_numpy()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud_item = encoded_df[encoded_df['is_fraud'] == 1]\nnon_fraud_item = encoded_df[encoded_df['is_fraud'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud_buy_edge = [tuple(x) for x in fraud_item[['customer_id', 'merchant_id']].to_numpy()]\nnon_buy_edge = [tuple(x) for x in non_fraud_item[['customer_id', 'merchant_id']].to_numpy()]\nfraud_sell_edge = [tuple(x) for x in fraud_item[['merchant_id', 'customer_id']].to_numpy()]\nnon_sell_edge = [tuple(x) for x in non_fraud_item[['merchant_id', 'customer_id']].to_numpy()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hetero_graph = dgl.heterograph({('client', 'is_fraud_buy', 'merchant'):fraud_buy_edge,\n                     ('client', 'not_fraud_buy', 'merchant'): non_buy_edge,\n                     ('merchant', 'is_fraud_sell', 'client'): fraud_sell_edge,\n                     ('merchant', 'not_fraud_sell', 'client'): non_sell_edge})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client_node = encoded_df.copy()\nclient_amt = client_node.groupby(['customer_id']).sum().amt\nclient_node = client_node.groupby(['customer_id']).max()\nclient_node['amt'] = client_amt\n\nmerchant_node = encoded_df.copy()\nmerchant_amt = merchant_node.groupby(['merchant_id']).sum().amt\nmerchant_node = merchant_node.groupby(['merchant_id']).max()\nmerchant_node['amt'] = merchant_amt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hetero_graph.nodes['client'].data['feature'] = torch.tensor(client_node[['is_fraud', 'amt']].to_numpy()).float()\nhetero_graph.nodes['merchant'].data['feature'] = torch.tensor(merchant_node[['is_fraud', 'amt']].to_numpy()).float()\n# hetero_graph.nodes['client'].data['h'] = torch.from_numpy(f_c).float()\n# hetero_graph.nodes['merchant'].data['h'] = torch.from_numpy(f_m).float()\n\n\n# hetero_graph.edges['is_fraud_buy'].data['h'] = torch.Tensor([ 1 for i in range(fraud_item.shape[0])]).bool()\n# hetero_graph.edges['not_fraud_buy'].data['h'] = torch.Tensor([ 0 for i in range(non_fraud_item.shape[0])]).bool()\n# hetero_graph.edges['is_fraud_sell'].data['h'] = torch.Tensor([ 1 for i in range(fraud_item.shape[0])]).bool()\n# hetero_graph.edges['not_fraud_sell'].data['h'] = torch.Tensor([ 0 for i in range(non_fraud_item.shape[0])]).bool()\n                                                              \n                                                              \nhetero_graph.edges['is_fraud_buy'].data['train_mask'] = torch.Tensor(fraud_item['train_mask'].to_numpy()).bool()\nhetero_graph.edges['is_fraud_buy'].data['test_mask'] = torch.Tensor(fraud_item['test_mask'].to_numpy()).bool()\nhetero_graph.edges['not_fraud_buy'].data['train_mask'] = torch.Tensor(non_fraud_item['train_mask'].to_numpy()).bool()\nhetero_graph.edges['not_fraud_buy'].data['test_mask'] = torch.Tensor(non_fraud_item['test_mask'].to_numpy()).bool()\nhetero_graph.edges['is_fraud_sell'].data['train_mask'] = torch.Tensor(fraud_item['train_mask'].to_numpy()).bool()\nhetero_graph.edges['is_fraud_sell'].data['test_mask'] = torch.Tensor(fraud_item['test_mask'].to_numpy()).bool()\nhetero_graph.edges['not_fraud_sell'].data['train_mask'] = torch.Tensor(non_fraud_item['train_mask'].to_numpy()).bool()\nhetero_graph.edges['not_fraud_sell'].data['test_mask'] = torch.Tensor(non_fraud_item['test_mask'].to_numpy()).bool()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hetero_graph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_graph = hetero_graph['client', :, 'merchant']\nedge_label = dec_graph.edata[dgl.ETYPE]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a Heterograph Conv model\n\nclass RGCN(nn.Module):\n    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n        super().__init__()\n\n        self.conv1 = dglnn.HeteroGraphConv({\n            rel: dglnn.GraphConv(in_feats, hid_feats)\n            for rel in rel_names}, aggregate='sum')\n        self.conv2 = dglnn.HeteroGraphConv({\n            rel: dglnn.GraphConv(hid_feats, out_feats)\n            for rel in rel_names}, aggregate='sum')\n\n    def forward(self, graph, inputs):\n        # inputs are features of nodes\n        h = self.conv1(graph, inputs)\n        h = {k: F.relu(v) for k, v in h.items()}\n        h = self.conv2(graph, h)\n        return h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HeteroMLPPredictor(nn.Module):\n    def __init__(self, in_dims, n_classes):\n        super().__init__()\n        self.W = nn.Linear(in_dims * 2, n_classes)\n\n    def apply_edges(self, edges):\n        x = torch.cat([edges.src['h'], edges.dst['h']], 1)\n        y = self.W(x)\n        return {'score': y}\n\n    def forward(self, graph, h):\n        # h contains the node representations for each edge type computed from\n        # the GNN for heterogeneous graphs defined in the node classification\n        # section (Section 5.1).\n        with graph.local_scope():\n            graph.ndata['h'] = h   # assigns 'h' of all node types in one shot\n            graph.apply_edges(self.apply_edges)\n            return graph.edata['score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features, rel_names):\n        super().__init__()\n        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n        self.pred = HeteroMLPPredictor(out_features, len(rel_names))\n    def forward(self, g, x, dec_graph):\n        h = self.sage(g, x)\n        return self.pred(dec_graph, h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\nmodel = Model(2, 4, 5, hetero_graph.etypes)\nclient_feats = hetero_graph.nodes['client'].data['feature']\nmerchant_feats = hetero_graph.nodes['merchant'].data['feature']\nnode_features = {'client': client_feats, 'merchant': merchant_feats}\n\ntest_mask = dec_graph.edata['test_mask']\ntrain_mask = dec_graph.edata['train_mask']\n\nopt = torch.optim.Adam(model.parameters())\ncounter = 1\nfor epoch in range(50):\n    model.train()\n    logits = model(hetero_graph, node_features, dec_graph)\n    loss = F.cross_entropy(logits[train_mask], edge_label[train_mask])\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    print(loss.item())\n    \n    model.eval()\n    with torch.no_grad():\n        out = model(hetero_graph, node_features, dec_graph)\n        pred = out.argmax(1)\n        print(\"Predicted\")\n        pred[pred==0] = 1\n        pred[pred==1] = 0\n        pred[pred==2] = 1\n        pred[pred==3] = 0\n\n    #     pred = model(hetero_graph, node_features)['transaction']\n    #     result = hetero_graph.nodes['transaction'].data['label']\n        train_acc = (pred[train_mask] == edge_label[train_mask]).float().mean()\n        test_acc = (pred[test_mask] == edge_label[test_mask]).float().mean()\n        print(counter)\n        print('Test Accuracy: %.2f%%' % (test_acc.item() * 100))\n        print(f1_score(pred[test_mask], edge_label[test_mask])*100)\n        print(roc_auc_score(edge_label[test_mask], pred[test_mask])*100)\n        print('*'*100)\n        counter+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" hetero_graph.etypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\n\ntest_mask = dec_graph.edata['test_mask']\ntrain_mask = dec_graph.edata['train_mask']\nmodel.eval()\nwith torch.no_grad():\n    out = model(hetero_graph, node_features, dec_graph)\n    pred = out.argmax(1)\n    print(\"Predicted\")\n    pred[pred==0] = 1\n    pred[pred==1] = 0\n    pred[pred==2] = 1\n    pred[pred==3] = 0\n        \n#     pred = model(hetero_graph, node_features)['transaction']\n#     result = hetero_graph.nodes['transaction'].data['label']\n    train_acc = (pred[train_mask] == edge_label[train_mask]).float().mean()\n    test_acc = (pred[test_mask] == edge_label[test_mask]).float().mean()\n    print('Test Accuracy: %.2f%%' % (test_acc.item() * 100))\n    \n    print(f1_score(pred[test_mask], edge_label[test_mask])*100)\n    print(roc_auc_score(edge_label[test_mask], pred[test_mask]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Edge Classicication Homogenous","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install dgl\nimport seaborn as sns\nimport dgl\nimport torch\nimport dgl.nn as dglnn\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndf1 = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv')\ndf2 = pd.read_csv('/kaggle/input/fraud-detection/fraudTest.csv')\ndf = pd.concat([df1, df2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_data(input):\n\n    customer_node = set(df['cc_num'])\n    merchant_node = set(df['merchant'])\n    transaction_node = set(df['trans_num'])\n    category = set(df['category'])\n    city = set(df['city'])\n\n\n    mapping_c = dict(zip(customer_node, range(len(customer_node))))\n    mapping_m = dict(zip(merchant_node, range(len(customer_node), len(customer_node)+len(merchant_node))))\n                         \n    print(len(customer_node), len(merchant_node))\n                         \n    mapping_t = dict(zip(transaction_node, range(len(df))))\n    mapping_cat = dict(zip(category, range(len(df))))\n    mapping_city = dict(zip(city, range(len(df))))\n\n    \n    output = input.copy()\n    output['customer_id'] = output.cc_num.apply(lambda x: mapping_c[x])\n    output['merchant_id'] = output.merchant.apply(lambda x: mapping_m[x])\n    output['transaction_id'] = output.trans_num.apply(lambda x: mapping_t[x])\n    output['category_id'] = output.category.apply(lambda x: mapping_cat[x])\n    output['city_id'] = output.city.apply(lambda x: mapping_city[x])\n\n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuples = [tuple(x) for x in fraud_item[['customer_id', 'merchant_id']].to_numpy()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create data for Gephi","metadata":{}},{"cell_type":"code","source":"encoded_df = encode_data(df)\nfraud_item = encoded_df[encoded_df['is_fraud'] == 1]\nnon_fraud_item = encoded_df[encoded_df['is_fraud'] == 0]\n\nmerchant_node = encoded_df.merchant_id.unique()\nclient_node = encoded_df.customer_id.unique()\n\nfraud_tuples = [tuple(x) for x in fraud_item[['customer_id', 'merchant_id']].to_numpy()]\nnon_fraud_tuples = [tuple(x) for x in non_fraud_item[['customer_id', 'merchant_id']].to_numpy()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(fraud_tuples)\nlen(non_fraud_tuples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nG = nx.MultiGraph()\nG.add_nodes_from(client_node, client=1)\nG.add_nodes_from(merchant_node, client=0)\nG.add_edges_from(fraud_tuples, fraud=True)\nG.add_edges_from(non_fraud_tuples, fraud=False)\n# nx.write_gexf(G, \"Sparkov.gexf\")\n# print(G.number_of_nodes(), G.number_of_edges())\n# import os\n# os.chdir(r'/kaggle/working')\n# %cd /kaggle/working\n# from IPython.display import FileLink\n# FileLink(r'Sparkov.gexf')\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(G.number_of_nodes(), G.number_of_edges())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from networkx.algorithms import bipartite\na_node_list = list()\na_node_list.extend(pd.unique(fraud_item.customer_id))\na_node_list.extend(pd.unique(fraud_item.merchant_id))\na_node_list.extend(pd.unique(non_fraud_item.customer_id))\na_node_list.extend(pd.unique(non_fraud_item.merchant_id))\nnode_list = list(set(a_node_list))\nbdegress = bipartite.degrees(G, node_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from networkx.algorithms import bipartite\n# b_avg_cc = bipartite.average_clustering(G)\nclient_degrees = 0\nclient_count = 0\n\nmerchant_degrees = 0\nmerchant_count = 0\nfor i in bdegress:\n    client_degrees += i[1]\n    client_count += 1\n    \n    merchant_degrees += i[0]\n    merchant_count += 1\n\nclient_degress_real = client_degrees / client_count\nmerchant_degress_real = merchant_degrees / merchant_count\n\nprint(client_degress_real)\nprint(merchant_degress_real)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_avg_cc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_fraud_item","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_node_list = list()\na_node_list.extend(pd.unique(fraud_item.customer_id))\na_node_list.extend(pd.unique(fraud_item.merchant_id))\na_node_list.extend(pd.unique(non_fraud_item.customer_id))\na_node_list.extend(pd.unique(non_fraud_item.merchant_id))\nnode_list = list(set(a_node_list))\nbdegress = bipartite.degrees(G, node_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(node_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bdegress = bipartite.degrees(G, node_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client_degrees = 0\nclient_count = 0\n\nmerchant_degrees = 0\nmerchant_count = 0\nfor i in bdegress:\n    client_degrees += i[1]\n    client_count += 1\n    \n    merchant_degrees += i[0]\n    merchant_count += 1\n\nclient_degress_real = client_degrees / client_count\nmerchant_degress_real = merchant_degrees / merchant_count\n\nprint(client_degress_real)\nprint(merchant_degress_real)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nG = nx.nx.MultiGraph()\nG.add_nodes_from(client_node, bipartite=1)\nG.add_nodes_from(merchant_node, bipartite=0)\nG.add_edges_from(fraud_tuples, fraud=True)\nG.add_edges_from(non_fraud_tuples, fraud=False)\nprint(G.number_of_nodes(), G.number_of_edges())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud_node_list = list()\nfraud_node_list.extend(pd.unique(fraud_item.customer_id))\nfraud_node_list.extend(pd.unique(fraud_item.merchant_id))\nfraud_node_list = list(set(fraud_node_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from networkx.algorithms import bipartite\nb_avg_cc = bipartite.average_clustering(G)\n# bdegress = bipartite.degrees(G, fraud_node_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_avg_cc = bipartite.average_clustering(G)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_avg_cc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_sb = bipartite.spectral_bipartivity(G)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_sb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client_degrees = 0\nclient_count = 0\n\nmerchant_degrees = 0\nmerchant_count = 0\nfor i in bdegress:\n    client_degrees += i[1]\n    client_count += 1\n    \n    merchant_degrees += i[0]\n    merchant_count += 1\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client_degress_real = client_degrees / client_count\nmerchant_degress_real = merchant_degrees / merchant_count\n\nprint(client_degress_real)\nprint(merchant_degress_real)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(client_degress_real)\nprint(merchant_degress_real)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(nx.clustering(G))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bipartite.average_clustering(G)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from networkx.algorithms import bipartite\nbipartite.clustering(G)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from networkx.algorithms import community\ncommunity.kernighan_lin_bisection(G)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gmc = community.greedy_modularity_communities(G)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(gmc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gmc[0]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g1 = G.subgraph(gmc[0])\ng2 = G.subgraph(gmc[1])\ng3 = G.subgraph(gmc[2])\ng4 = G.subgraph(gmc[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(G.number_of_nodes(), G.number_of_edges())\nprint(g1.number_of_nodes(), g1.number_of_edges())\nprint(g2.number_of_nodes(), g2.number_of_edges())\nprint(g3.number_of_nodes(), g3.number_of_edges())\nprint(g4.number_of_nodes(), g4.number_of_edges())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nw_graph = list()\n# for g in community.girvan_newman(G):\n#     nw_graph.append(g)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\n%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'Sparkov.gexf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graph = dgl.graph(tuples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat = pd.concat([encoded_df[['customer_id', 'is_fraud']].groupby(['customer_id']).mean(), encoded_df[['merchant_id', 'is_fraud']].groupby(['merchant_id']).mean()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graph.ndata['feature'] = torch.tensor(feat.to_numpy())\ngraph.edata['feature'] = torch.tensor(encoded_df[['amt']].to_numpy())\ngraph.edata['label'] = torch.tensor(encoded_df.is_fraud.to_numpy())\ngraph.edata['train_mask'] = torch.cat((torch.zeros(df1.shape[0], dtype=torch.bool), torch.ones(df2.shape[0], dtype=torch.bool)), 0)\ngraph.edata['test_mask'] = torch.cat((torch.ones(df1.shape[0], dtype=torch.bool), torch.zeros(df2.shape[0], dtype=torch.bool)), 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Contruct a two-layer GNN model\nimport dgl.nn as dglnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass SAGE(nn.Module):\n    def __init__(self, in_feats, hid_feats, out_feats):\n        super().__init__()\n        self.conv1 = dglnn.SAGEConv(\n            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n        self.conv2 = dglnn.SAGEConv(\n            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n\n    def forward(self, graph, inputs):\n        # inputs are features of nodes\n        h = self.conv1(graph, inputs)\n        h = F.relu(h)\n        h = self.conv2(graph, h)\n        return h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dgl.function as fn\nclass DotProductPredictor(nn.Module):\n    def forward(self, graph, h):\n        # h contains the node representations computed from the GNN defined\n        # in the node classification section (Section 5.1).\n        with graph.local_scope():\n            graph.ndata['h'] = h\n            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n            return graph.edata['score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLPPredictor(nn.Module):\n    def __init__(self, in_features, out_classes):\n        super().__init__()\n        self.W = nn.Linear(in_features * 2, out_classes)\n\n    def apply_edges(self, edges):\n        h_u = edges.src['h']\n        h_v = edges.dst['h']\n        score = self.W(torch.cat([h_u, h_v], 1))\n        return {'score': score}\n\n    def forward(self, graph, h):\n        # h contains the node representations computed from the GNN defined\n        # in the node classification section (Section 5.1).\n        with graph.local_scope():\n            graph.ndata['h'] = h\n            graph.apply_edges(self.apply_edges)\n            return graph.edata['score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features):\n        super().__init__()\n        self.sage = SAGE(in_features, hidden_features, out_features)\n        self.pred = MLPPredictor(in_features, out_features)\n    def forward(self, g, x):\n        h = self.sage(g, x)\n        return self.pred(g, h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"node_features = graph.ndata['feature'].float()\nedge_label = graph.edata['label'].float()\ntrain_mask = graph.edata['train_mask']\nmodel = Model(1, 20, 1)\nopt = torch.optim.Adam(model.parameters())\nfor epoch in range(10):\n    pred = model(graph, node_features)\n    loss = ((pred[train_mask] - edge_label[train_mask]) ** 2).mean()\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    print(loss.item())","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_node = customer_node.union(merchant_node)\nprint(f\"Node numbers {len(all_node)}\")\nmapping = dict(zip(all_node, range(len(all_node))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['customer_id'] = df.cc_num.apply(lambda x: mapping[x])\ndf['merchant_id'] = df.merchant.apply(lambda x: mapping[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"members = pd.DataFrame(mapping.items(), columns=['name', 'id'])\nmembers.to_csv('./members.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# interactions = pd.concat([df[['nameOrig', 'customer_id', 'merchant_id', 'is_fraud']], X_val[['customer_id', 'merchant_id', 'amt', 'is_fraud']]], axis=0)\ninteractions = df[['customer_id', 'merchant_id', 'amt', 'is_fraud']]\ninteractions.to_csv('interactions.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Contruct a two-layer GNN model\nimport dgl.nn as dglnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass SAGE(nn.Module):\n    def __init__(self, in_feats, hid_feats, out_feats):\n        super().__init__()\n        self.conv1 = dglnn.SAGEConv(\n            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n        self.conv2 = dglnn.SAGEConv(\n            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n\n    def forward(self, graph, inputs):\n        # inputs are features of nodes\n        h = self.conv1(graph, inputs)\n        h = F.relu(h)\n        h = self.conv2(graph, h)\n        return h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dgl.function as fn\nclass DotProductPredictor(nn.Module):\n    def forward(self, graph, h):\n        # h contains the node representations computed from the GNN defined\n        # in the node classification section (Section 5.1).\n        with graph.local_scope():\n            graph.ndata['h'] = h\n            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n            return graph.edata['score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"node_features = graph.ndata['feat']\nnode_labels = graph.ndata['label']\ntrain_mask = graph.ndata['train_mask']\nvalid_mask = graph.ndata['val_mask']\ntest_mask = graph.ndata['test_mask']\nn_features = node_features.shape[1]\nn_labels = int(node_labels.max().item() + 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, graph, features, labels, mask):\n    model.eval()\n    with torch.no_grad():\n        logits = model(graph, features)\n        logits = logits[mask]\n        labels = labels[mask]\n        _, indices = torch.max(logits, dim=1)\n        correct = torch.sum(indices == labels)\n        return correct.item() * 1.0 / len(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SAGE(in_feats=n_features, hid_feats=100, out_feats=n_labels)\nopt = torch.optim.Adam(model.parameters())\n\nfor epoch in range(10):\n    model.train()\n    # forward propagation by using all nodes\n    logits = model(graph, node_features)\n    # compute loss\n    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n    # compute validation accuracy\n    acc = evaluate(model, graph, node_features, node_labels, valid_mask)\n    # backward propagation\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    print(loss.item())\n\n    # Save model if necessary.  Omitted in this example.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dgl\nfrom dgl.data import DGLDataset\nimport torch\nimport os\nimport pandas as pd\n\nclass PlaysimDataset(DGLDataset):\n    def __init__(self):\n        super().__init__(name='Sparkov')\n\n    def process(self):\n        nodes_data = pd.read_csv('/kaggle/input/sparkov/members.csv')\n        edges_data = pd.read_csv('/kaggle/input/sparkov/interactions.csv')[:30000]\n#         nodes_data = pd.read_csv('./members.csv')\n#         edges_data = pd.read_csv('./interactions.csv')\n#         node_features = torch.from_numpy(nodes_data['name'].astype('category').cat.codes.to_numpy()).float()\n        node_features = torch.from_numpy(nodes_data[['id']].to_numpy()).float()\n        edge_features = torch.from_numpy(edges_data[['amt']].to_numpy())\n        edge_labels = torch.from_numpy(edges_data['is_fraud'].to_numpy())\n        edges_src = torch.from_numpy(edges_data['customer_id'].to_numpy())\n        edges_dst = torch.from_numpy(edges_data['merchant_id'].to_numpy())\n\n        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=nodes_data.shape[0])\n        self.graph.ndata['feature'] = node_features\n#         self.graph.edata['feature'] = edge_features\n        self.graph.edata['label'] = edge_labels\n\n        # If your dataset is a node classification dataset, you will need to assign\n        # masks indicating whether a node belongs to training, validation, and test set.\n        n_edges = edges_data.shape[0]\n        n_train = int(n_edges * 0.6)\n        n_val = int(n_edges * 0.2)\n        \n        print(n_edges, n_train, n_val)\n        train_mask = torch.zeros(n_edges, dtype=torch.bool)\n        val_mask = torch.zeros(n_edges, dtype=torch.bool)\n        test_mask = torch.zeros(n_edges, dtype=torch.bool)\n        \n        train_mask[:n_train] = True\n        val_mask[n_train:n_train + n_val] = True\n        test_mask[n_train + n_val:] = True\n        \n        self.graph.edata['train_mask'] = train_mask\n        self.graph.edata['val_mask'] = val_mask\n        self.graph.edata['test_mask'] = test_mask\n\n    def __getitem__(self, i):\n        return self.graph\n\n    def __len__(self):\n        return 1\n\ndataset = PlaysimDataset()\ngraph = dataset[0]\n\nprint(graph)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Contruct a two-layer GNN model\nimport dgl.nn as dglnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass SAGE(nn.Module):\n    def __init__(self, in_feats, hid_feats, out_feats):\n        super().__init__()\n        self.conv1 = dglnn.SAGEConv(\n            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n        self.conv2 = dglnn.SAGEConv(\n            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n\n    def forward(self, graph, inputs):\n        # inputs are features of nodes\n        h = self.conv1(graph, inputs)\n        h = F.relu(h)\n        h = self.conv2(graph, h)\n        return h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dgl.function as fn\nclass DotProductPredictor(nn.Module):\n    def forward(self, graph, h):\n        # h contains the node representations computed from the GNN defined\n        # in the node classification section (Section 5.1).\n        with graph.local_scope():\n            graph.ndata['h'] = h\n            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n            return graph.edata['score']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, in_features, hidden_features, out_features):\n        super().__init__()\n        self.sage = SAGE(in_features, hidden_features, out_features)\n        self.pred = DotProductPredictor()\n    def forward(self, g, x):\n        h = self.sage(g, x)\n        return self.pred(g, h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"node_features = graph.ndata['feature']\nedge_label = graph.edata['label']\ntrain_mask = graph.edata['train_mask']\nmodel = Model(1, 100, 2)\nopt = torch.optim.Adam(model.parameters())\nfor epoch in range(200):\n    pred = model(graph, node_features)\n    loss = ((pred[train_mask] - edge_label[train_mask]) ** 2).mean()\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    print(f'{epoch}: {loss.item()}')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import roc_auc_score\n\n# def evaluate(model, graph, features, labels, mask):\n#     model.eval()\n#     with torch.no_grad():\n#         logits = model(graph, features)\n#         logits = logits[mask]\n#         labels = labels[mask]\n#         _, indices = torch.max(logits, dim=1)\n#         correct = torch.sum(indices == labels)\n#         return correct.item() * 1.0 / len(labels)\n    \n# # compute validation accuracy\n# acc = evaluate(model, graph, node_features, edge_label, graph.edata['test_mask'])\n# acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nmodel.eval()\nwith torch.no_grad():\n    pred = model(graph, node_features)\n    print(roc_auc_score(graph.edata['label'][graph.edata['test_mask']], pred[graph.edata['test_mask']]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heterogeneous Graph","metadata":{}},{"cell_type":"code","source":"!pip install dgl\nimport dgl\nimport tensorflow as tf\nimport torch as th\nimport dgl.nn as dglnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport torch\nimport dgl\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_data(input_df1, input_df2):\n    df = pd.concat([input_df1, input_df2])\n    customer_node = set(df['cc_num'])\n    merchant_node = set(df['merchant'])\n    transaction_node = set(df['trans_num'])\n    category = set(df['category'])\n    city = set(df['city'])\n\n    mapping_c = dict(zip(customer_node, range(len(df))))\n    mapping_m = dict(zip(merchant_node, range(len(df))))\n    mapping_t = dict(zip(transaction_node, range(len(df))))\n    mapping_cat = dict(zip(category, range(len(df))))\n    mapping_city = dict(zip(city, range(len(df))))\n\n    output_df1 = input_df1.copy()\n    output_df1['customer_id'] = output_df1.cc_num.apply(lambda x: mapping_c[x])\n    output_df1['merchant_id'] = output_df1.merchant.apply(lambda x: mapping_m[x])\n    output_df1['transaction_id'] = output_df1.trans_num.apply(lambda x: mapping_t[x])\n    output_df1['category_id'] = output_df1.category.apply(lambda x: mapping_cat[x])\n    output_df1['city_id'] = output_df1.city.apply(lambda x: mapping_city[x])\n\n    output_df2 = input_df2.copy()\n    output_df2['customer_id'] = output_df2.cc_num.apply(lambda x: mapping_c[x])\n    output_df2['merchant_id'] = output_df2.merchant.apply(lambda x: mapping_m[x])\n    output_df2['transaction_id'] = output_df2.trans_num.apply(lambda x: mapping_t[x])\n    output_df2['category_id'] = output_df2.category.apply(lambda x: mapping_cat[x])\n    output_df2['city_id'] = output_df2.city.apply(lambda x: mapping_city[x])\n\n    \n    return output_df1, output_df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndf1 = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv')\ndf2 = pd.read_csv('/kaggle/input/fraud-detection/fraudTest.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df1, encoded_df2 = encode_data(df1, df2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = encoded_df1[['customer_id','merchant_id','category_id','city_id','unix_time','merch_lat','merch_long','zip','lat','long', 'amt']]\ny1 = encoded_df1[['is_fraud']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_resampled, y_resampled = SMOTE().fit_resample(X1, y1)\nX_resampled.assign(is_fraud = y_resampled.is_fraud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df1_up = X_resampled.assign(is_fraud = y_resampled.is_fraud)\nselected_df2 = encoded_df2[['customer_id','merchant_id','category_id','city_id','unix_time','merch_lat','merch_long','zip','lat','long', 'amt', 'is_fraud']]\ndf = pd.concat([encoded_df1_up, selected_df2])\ndf = df.assign(transaction_id = list(range(df.shape[0])))\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a Heterograph Conv model\nclass RGCN(nn.Module):\n    def __init__(self, in_feats, hid_feats, hid_feats2, out_feats, rel_names):\n        super().__init__()\n\n        self.conv1 = dglnn.HeteroGraphConv({\n            rel: dglnn.GraphConv(in_feats, hid_feats)\n            for rel in rel_names}, aggregate='sum')\n        self.conv2 = dglnn.HeteroGraphConv({\n            rel: dglnn.GraphConv(hid_feats, hid_feats2)\n            for rel in rel_names}, aggregate='sum')\n        self.conv3 = dglnn.HeteroGraphConv({\n            rel: dglnn.GraphConv(hid_feats2, out_feats)\n            for rel in rel_names}, aggregate='sum')\n\n    def forward(self, graph, inputs):\n        # inputs are features of nodes\n        h = self.conv1(graph, inputs)\n        h = {k: F.relu(v) for k, v in h.items()}\n        h = self.conv2(graph, h)\n        h = {k: F.relu(v) for k, v in h.items()}\n        h = self.conv3(graph, h)\n        return h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"id นับไปเป็นฟีเจอเลยหรือเปล่า","metadata":{}},{"cell_type":"code","source":"customer_feat = df[['customer_id', 'city_id', 'lat', 'long']]\nmerchant_feat = df[['merchant_id', 'category_id', 'merch_lat', 'merch_long']]\ntrans_feat = df[['transaction_id', 'is_fraud', 'amt']]\n\n\ncustomer_feat = df[['customer_id', 'is_fraud']]\nmerchant_feat = df[['merchant_id', 'is_fraud']]\ntrans_feat = df[['transaction_id', 'is_fraud']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a heterograph with 3 node types and 1 edges types.\n\ngraph_data = {\n   ('customer', 'make', 'transaction'): (df.customer_id.to_numpy(), df.transaction_id.to_numpy()),\n   ('merchant', 'create', 'transaction'): (df.merchant_id.to_numpy(), df.transaction_id.to_numpy()),\n   ('transaction', 'make-by', 'customer'): (df.transaction_id.to_numpy(), df.customer_id.to_numpy()),\n   ('transaction', 'create_by', 'merchant'): (df.transaction_id.to_numpy(), df.merchant_id.to_numpy())\n}\nhetero_graph = dgl.heterograph(graph_data)\nprint(hetero_graph)\n#ยังไม่น่ใจว่า assigne แบบนี้ถูกไหม\n\n#ต้องกลับมาแก้ feature ของ customer และ merchangte\nhetero_graph.nodes['customer'].data['feature'] = torch.tensor(customer_feat.groupby(['customer_id']).max().to_numpy())\nhetero_graph.nodes['merchant'].data['feature'] = torch.tensor(merchant_feat.groupby(['merchant_id']).max().to_numpy())\nhetero_graph.nodes['transaction'].data['feature'] = torch.tensor(trans_feat.groupby(['transaction_id']).max().to_numpy())\nhetero_graph.nodes['transaction'].data['label'] = torch.tensor(trans_feat.sort_values(by=['transaction_id']).is_fraud.to_numpy())\n\n#randomly generate training masks on user nodes and click edges\nhetero_graph.nodes['transaction'].data['train_mask'] = torch.cat((torch.ones(encoded_df1_up.shape[0], dtype=torch.bool), torch.zeros(df2.shape[0], dtype=torch.bool)), 0) # Train + Test\n# hetero_graph.nodes['transaction'].data['train_mask'] = torch.zeros(len(hetero_graph.nodes('transaction')), dtype=torch.bool).bernoulli(0.3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RGCN(1, 20, 10, len(pd.unique(df.is_fraud)), hetero_graph.etypes)\nc_feats = hetero_graph.nodes['customer'].data['feature'].float()\nm_feats = hetero_graph.nodes['merchant'].data['feature'].float()\nt_feats = hetero_graph.nodes['transaction'].data['feature'].float()\nlabels = hetero_graph.nodes['transaction'].data['label']\ntrain_mask = hetero_graph.nodes['transaction'].data['train_mask']\ntest_mask = train_mask.logical_not()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"node_features = {'customer': c_feats, 'merchant': m_feats, 'transaction': t_feats}\nh_dict = model(hetero_graph, {'customer': c_feats, 'merchant': m_feats, 'transaction': t_feats})\nh_cus = h_dict['customer']\nh_mer = h_dict['merchant']\nh_tran = h_dict['transaction']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = torch.optim.Adam(model.parameters())\n\ncount = 0\nfor epoch in range(50):\n    model.train()\n    # forward propagation by using all nodes and extracting the user embeddings\n    logits = model(hetero_graph, node_features)['transaction']\n    # compute loss\n    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n    # Compute validation accuracy.  Omitted in this example.\n    # backward propagation\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    count+=1\n#     if count % 10 == 0:\n#         print(loss.item())\n    model.eval()\n    with torch.no_grad():\n        pred = model(hetero_graph, node_features)['transaction']\n        result = hetero_graph.nodes['transaction'].data['label']\n\n\n        train_acc = (pred[test_mask].argmax(1) == labels[test_mask]).float().mean()\n        test_acc = (pred[test_mask].argmax(1) == labels[test_mask]).float().mean()\n        print('*'* 50)\n        print('Loss', loss.item())\n        print('Test Accuracy: %.2f%%' % (test_acc.item() * 100))\n        print('Recall', recall_score(labels[test_mask], pred[test_mask].argmax(1)) * 100)\n        print('F1Score', f1_score(labels[test_mask], pred[test_mask].argmax(1))  * 100)\n        print('ROC', roc_auc_score(labels[test_mask], pred[test_mask].argmax(1)) * 100)\n    # Save model if necessary.  Omitted in the example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nG = nx.fast_gnp_random_graph(1000, 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}