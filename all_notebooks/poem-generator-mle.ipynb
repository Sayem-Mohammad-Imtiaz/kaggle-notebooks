{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Poem Generator\n___\nThe goal of this notebook is to generate new poems using a probabilistic model.\n\n\n![poems](https://upload.wikimedia.org/wikipedia/commons/5/5b/Power-of-words-by-antonio-litterio-creative-commons-attribution-share-alike-3-0.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# Probabilistc model? Wut?\n\n![wut cat](https://upload.wikimedia.org/wikipedia/commons/2/22/Lolwut_cat.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## Probability!\n\nThat's it! We are going to create a new sentence using the probability of every word appearing next to another. Take a look:\n\nUsing the dataset that we have, we can calculate the probability of any word occurring. Let's say that we have the corpus:\n\ncorpus = [ ['I like the weather'] , \n  ['the weather is nice'] ,\n  ['I don't like the weather'] ]\n  \n  \n Using the corpus we can calculate the probability of each word appearing, considering the 7 different words from our corpus.\n <br>\n \n|   Word  \t| Frequency \t| Probability formula \t|    Result   \t|\n|:-------:\t|:---------:\t|:-------------------:\t|:-----------:\t|\n|    I    \t|     2     \t|         2/7         \t| 0,285714286 \t|\n|   like  \t|     2     \t|         2/7         \t| 0,285714286 \t|\n|   the   \t|     3     \t|         3/7         \t| 0,428571429 \t|\n| weather \t|     3     \t|         3/7         \t| 0,428571429 \t|\n|    is   \t|     1     \t|         1/7         \t| 0,142857143 \t|\n|   nice  \t|     1     \t|         1/7         \t| 0,142857143 \t|\n|  don't  \t|     1     \t|         1/7         \t| 0,142857143 \t|\n\n<br>\n\nSo the probability of the word **' I '** occuring is 0,28, but what is the probabilty of **' I like '** occuring? To solve this we need to understand the chain rule of probability."},{"metadata":{},"cell_type":"markdown","source":"## Chain rule of probability\n\n![image.png](attachment:image.png)\n\nSuper easy to understand right? If not let's break it down using our example.\n\np(i,like) = p(like) * p(i)\n\np(i,like) = 0,285714286 * 0,285714286\n\np(i,like) = 0,081632653\n\nBut what if we wanted to calculate something more meaningful, maybe 'I like the':\n\np(i,like,the) = p(the) * p(i,like)\n\np(i,like,the) = 0,428571429 * 0,081632653\n\np(i,like,the) = 0,034985423\n","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{},"cell_type":"markdown","source":"## Generating Poems using MLE\n\nUsing the chain rule of probability we can calculate any probabilty of combination of words, from now on we're going to call the combination of words N-gram (where N is the number of words) i.e. 2-gram (bigram) is (i,like) and a 3-gram (trigram) is (i,like,the).\n\nWe can define the probability of a combination of word happening and the ratio between that phrase and n-1 of that phrase.\n\n> p(i, like, the) = how many times ' i like the ' happened / how many time ' i like ' happened.\n\nWith this estimative we can find the probability of ' the ' being the next word after ' i like ' . If we do this for our whole dataset of poems we can generate a model that choose a word based on the probability distribution of the next word based on the two word before. Sound complicated I know, but let's try."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## importing our libraries\nimport pandas as pd\nimport numpy as np\n\n## we are goingo to use bigram and trigrams to separate our dataset in combinations of two and three words\nfrom nltk.util import bigrams,trigrams\n\n## we are going to use Counter to counter the bigrams and trigrams of our dataset.\nfrom collections import Counter\n\n## let's use regular expressions to treat our dataset\nimport re\n\n## let's use random to choose the words of our poem based on a probability distribution\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## reading the dataset\ndf = pd.read_csv('/kaggle/input/poems-in-portuguese/portuguese-poems.csv')\n\n## droping any NA values from content (that's the columns where the content of the poem is written)\ndf.dropna(subset=['Content'],inplace=True)\n\n## reseting the index\ndf.reset_index(drop=True,inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking the distribution of authors\n\ndf.Author.value_counts(normalize=True)[:10].plot.bar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since more than 14% of our dataset is composed of Fernando Pessoa's poems our model will probably be biased to his style. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's generate our corpus\n\ndef generate_corpus(df):\n    \n    corpus = []\n    \n    for i in range(len(df)):\n        s = df.Content[i].casefold().replace('\\r\\n', ' ').replace('\\n',' ')\n        s = re.sub('([.,!?():-])', r' \\1 ', s)\n        s = re.sub('\\s{2,}', ' ', s).split(' ')\n        corpus.append(s)\n        \n    return corpus\n\n## creating the tags for our corpus\n\ndef create_tags(corpus):\n    \n    words = []\n    \n    for sentence in corpus:\n        sentence.insert(0, '<s>')\n        sentence.insert(0, '<s>')\n        sentence.append('</s>')\n        words.extend(sentence)\n        \n    return words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've added some tags to mark the start and the end of each sentence. So we can calculate the probablity of the sentence starting with each word ... the initial bigram will be (' < s > ' , ' < s > ')."},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating the corpus\n\ncorpus = generate_corpus(df)\nprint(corpus[0:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## inserting the tags and gerenating a single string to divide our dataset in bigrams and trigrams\n\nsingle_string = create_tags(corpus)\nprint(single_string[0:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating the bigrams of each combination of word\n\nwords_bigram = bigrams(single_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating the trigram of each combination of word\n\nwords_trigram = trigrams(single_string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have bigrams and trigramns we have to count them."},{"metadata":{"trusted":true},"cell_type":"code","source":"## counting bigrams\n\nbigram_count = Counter(words_bigram)\nbigram_count[('<s>', '<s>')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## counting trigrams\n\ntrigram_count = Counter(words_trigram)\ntrigram_count[('<s>', '<s>','eu')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 15541 bigrams initiating with (' < s > ' , ' < s > '), that means we have 15541 poems! Since we've added these two markers in the begginning of each document. However analysing the trigram (' < s > ' , ' < s > ', ' eu ' ) we can see that of all the 15541 poems only 319 of them started with the word ' eu '."},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating a list of bigramns and trigrams so we can calculate the probilities\n\nbigram_key = list(bigram_count.keys())\ntrigram_key = list(trigram_count.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_bigram = []\nlist_lastword = []\nlist_probs = []\n\n## for each trigram t\n\nfor t in trigram_key:\n    \n    ## create a bigram using the first two words of the trigram\n    key_bigram = (t[0], t[1])\n    \n    ## find how many times the trigram happened and divide it by the number of times that the bigram happened\n    prob = trigram_count[t] / bigram_count[key_bigram]\n    \n    ## append the lists above\n    list_bigram.append(key_bigram)\n    list_lastword.append(t[2])\n    list_probs.append(prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating a dataframe with the results\n\nmodel_df = pd.DataFrame({'bigram': list_bigram, 'lastword': list_lastword, 'prob': list_probs})\nmodel_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do some of it by hand!"},{"metadata":{"trusted":true},"cell_type":"code","source":"## how many time <s><s> appeared\n\nbigram_count[('<s>', '<s>')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## how many time <s><s><eu> appeared\n\ntrigram_count[('<s>', '<s>','eu')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## probability of the word 'eu' appearing next to <s><s>\n\n319 / 15541","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## taking the proof\n\nmodel_df.iloc[0,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use the dataframe created to find all the possible words that come after (' < s > ' , ' < s > ')."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = model_df.loc[model_df['bigram'] == ('<s>', '<s>')]\ntest_df.sort_values('prob',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe above means that the word ' a ' is the most probable to occour after (' < s > ' , ' < s > '). But what can occur after it? Just take a look at the dataframe below."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = model_df.loc[model_df['bigram'] == ('<s>', 'a')]\ntest_df.sort_values('prob',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating the poems"},{"metadata":{},"cell_type":"markdown","source":"Now we can use the distribution of all the probabilities in model_df to choose what will be the last word given a bigram. We're not going to use only the word with the highest probability but the probability of each one using random.choices."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_sents = 2\ncurrent_bigram = ('<s>', '<s>')\ni = 0\nwhile i < num_sents:\n    df = model_df.loc[model_df['bigram'] == current_bigram]\n    words = df['lastword'].values\n    probs = df['prob'].values\n    last_word = random.choices(words, probs)[0]\n    \n    current_bigram = (current_bigram[1], last_word)\n    \n    if last_word == '</s>':\n        i+=1\n    \n    if last_word != '<s>' and last_word != '</s>':\n        print(last_word, end=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's simple! We can even go further by querying at the beggining only the name of one particular author. I hope you've learned something here, any doubts please comment and leave a like if you actually like the work."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}