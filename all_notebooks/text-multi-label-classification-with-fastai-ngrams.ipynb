{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text multiclassification with fastai applying N-grams ( ͡° ͜ʖ ͡°) #"},{"metadata":{},"cell_type":"markdown","source":"### Motivation ###\nWhile there are many examples of text classification in which there are only two classes, I decide to deal with a multiclassification problem. The data I used are user stories from stack-overflow with tags assigned to them."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.text import *\nfrom nltk.corpus import stopwords\nimport re\nfrom bs4 import BeautifulSoup\nfrom functools import partial \nimport io \nimport os\nimport sklearn.feature_extraction.text as sklearn_text\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function wrote to clean our texts a bit"},{"metadata":{"trusted":true},"cell_type":"code","source":"REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/stack-overflow-user-stories/stack-overflow-data.csv')\ndf = df[pd.notnull(df['tags'])]\ndf = df[pd.notnull(df['post'])]\ndf['post'] = df['post'].apply(clean_text)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of rows in our dataset is almost 80 000. Let's do a train, validation split, making sure the samples are balanced. A *stratify* argument from scikit.learn *train_test_split* function provides this."},{"metadata":{"trusted":false},"cell_type":"code","source":"df_trn, df_val = train_test_split(df, stratify = df['tags'],  test_size = 0.2, random_state = 12)\ndf_trn['tags'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to create databunch. The creation of a databunch should consist the following two steps:\n\n1. **Tokenization** - it takes words from description and converts them into a standard form of tokens. Basically each token represents a word.\n\n2. **Numericalization** - The next thing we do is we take a complete unique list of all of the possible tokens﹣ that's called the vocab which gets created for us. So here is every possible token (the first ten of them) that appear in our all of the descriptions. We then replace every description with a list of numbers.\n\nSo through tokenization and numericalization, this is the standard way in NLP of turning a document into a list of numbers. Fortunately, this can be easily done with fast.ai"},{"metadata":{"trusted":false},"cell_type":"code","source":"labels = list(df['tags'].unique())\ndata = (TextList.from_df(df_trn, cols='post')\n                .split_by_rand_pct(0.2)\n                .label_from_df(classes=labels)\n                .databunch(bs=48))\ndata.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In NLP, a token is the basic unit of processing. Here, the tokens mostly correspond to words or punctuation, as well as several special tokens, corresponding to unknown words, capitalization, etc.\n\nAll those tokens starting with \"xx\" are fastai special tokens. You can see the list of all of them and their meanings in the fastai docs.\n\nLet's see the string-to-ints."},{"metadata":{"trusted":false},"cell_type":"code","source":"data.vocab.stoi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"..."},{"metadata":{"trusted":false},"cell_type":"code","source":"len(data.train_dl.x), len(data.valid_dl.x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A term-document matrix represents a document as a \"bag of words\", that is, we don't keep track of the order the words are in, just which words occur (and how often). This is the implementation. Here we use the most common sparse storage format - compressed sparse row (CSR)"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_term_doc_matrix(label_list, vocab_len):\n    j_indices = []\n    indptr = []\n    values = []\n    indptr.append(0)\n\n    for i, doc in enumerate(label_list):\n        feature_counter = Counter(doc.data)\n        j_indices.extend(feature_counter.keys())\n        values.extend(feature_counter.values())\n        indptr.append(len(j_indices))\n\n    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n                                   shape=(len(indptr) - 1, vocab_len),\n                                   dtype=int)\n\n\n\nval_term_doc = get_term_doc_matrix(data.valid_dl.x, len(data.vocab.itos))\ntrn_term_doc = get_term_doc_matrix(data.train_dl.x, len(data.vocab.itos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x= trn_term_doc\ny=data.train_dl.y.items\nval_y = data.valid_dl.y.items","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression ###\nLet's start with a simple logistic regreeion. The C paramterer is already tuned"},{"metadata":{"trusted":false},"cell_type":"code","source":"m = LogisticRegression(C=0.03, dual=False)\nm.fit(x, y)\npreds = m.predict(val_term_doc)\n(preds==val_y).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binarized Logistic Regression ###\nHow about binarized version? Here we take care inly about if the word occurs in a document or not. The frequency does not really matter."},{"metadata":{"trusted":false},"cell_type":"code","source":"m = LogisticRegression(C=0.03, dual=False)\nm.fit(trn_term_doc.sign(), y)\npreds = m.predict(val_term_doc.sign())\n(preds==val_y).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, the accuracy here is a bit better. How about taking as a token a pair or a triple of words? We will check out **ngrams**. Let's fit normalized logistic regression where the features are the trigrams. We will use *CountVectorizer* from *sklearn.feature_extraction.text* "},{"metadata":{"trusted":false},"cell_type":"code","source":"veczr =  CountVectorizer(ngram_range=(1,3), preprocessor=noop, tokenizer=noop, max_features=800000)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we get words from the train data description, transfer them into ngrams and add all unigrams, bigrams and trigrams to a vocabulary of our vectorizer."},{"metadata":{"trusted":false},"cell_type":"code","source":"docs = data.train_dl.x","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_words = [[docs.vocab.itos[o] for o in doc.data] for doc in data.train_dl.x]\ntrain_ngram_doc = veczr.fit_transform(train_words)\ntrain_ngram_doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"veczr.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply analogical steps to validation data descriptions."},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_words = [[docs.vocab.itos[o] for o in doc.data] for doc in data.valid_dl.x]\nval_ngram_doc = veczr.transform(valid_words)\nval_ngram_doc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to create full vocabulary of ngrams"},{"metadata":{"trusted":false},"cell_type":"code","source":"vocab = veczr.get_feature_names()\nvocab[100000:100005]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binarized Logistic Regression with Ngrams ###\n\nLet's extend a model adding our bigrams and trigrams to it"},{"metadata":{"trusted":false},"cell_type":"code","source":"y=data.train_dl.y\nvalid_labels = data.valid_dl.y.items","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"m = LogisticRegression(C=0.03, dual=True)\nm.fit(train_ngram_doc.sign(), y.items);\npreds = m.predict(val_ngram_doc.sign())\n(preds.T==valid_labels).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see this is a bit better than a model with only unigrams. Let's tune paramter C."},{"metadata":{"trusted":false},"cell_type":"code","source":"a_list = []\ni_list = []\nfor i in range (1,100):\n    m = LogisticRegression(C=i/100, dual=True)\n    m.fit(train_ngram_doc.sign(), y.items);\n    preds = m.predict(val_ngram_doc.sign())\n    a = (preds.T==valid_labels).mean()\n    a_list.append(a)\n    i_list.append(i)\nplt.plot(i_list, a_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So what is the best parameter C?"},{"metadata":{"trusted":false},"cell_type":"code","source":"best_c = i_list[np.argmax(a_list)]/100\nbest_c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we calculate chosen model."},{"metadata":{"trusted":false},"cell_type":"code","source":"m = LogisticRegression(C=best_c, dual=True)\nm.fit(train_ngram_doc.sign(), y.items);\npreds = m.predict(val_ngram_doc.sign())\n(preds.T==valid_labels).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally plot the results."},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=2)\n#predictions = model.predict(X_test, batch_size=1000)\n\nLABELS = df['tags'].unique()\n\nconfusion_matrix = metrics.confusion_matrix(valid_labels, preds)\n\nplt.figure(figsize=(35, 15))\nsns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\nplt.title(\"Confusion matrix\", fontsize=20)\nplt.ylabel('True label', fontsize=20)\nplt.xlabel('Predicted label', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks for reading! ( ͡ᵔ ͜ʖ ͡ᵔ )"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}