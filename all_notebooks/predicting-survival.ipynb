{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predict outcome\n\nPredict whether or not a horse can survive based upon past medical conditions.\n\nNoted by the \"outcome\" variable in the data. "},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade scikit-learn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nimport pprint\nimport seaborn as sb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.inspection import permutation_importance # scikit-learn >=0.22 requires custom install\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(d):\n    binary_cols = [\"surgery\", \"surgical_lesion\", \"cp_data\", ]\n    d[binary_cols] = d[binary_cols].replace({\"yes\":1,\"no\":0})\n    d[\"age\"] = d[\"age\"].replace({\"young\":0, \"adult\":1})\n    d[\"capillary_refill_time\"] = d[\"capillary_refill_time\"].replace({1:0, 2:1})\n    d[\"outcome\"] = d[\"outcome\"].replace({\"died\":0, \"euthanized\":1, \"lived\": 2})\n    \n    # One hot encode variables\n    d = pd.get_dummies(d, dummy_na=True)\n    \n    # Drop those where we don't know the outcome\n    d = d[d.outcome.isnull()==False]\n    \n    # Rename outcome to torget (just to make it clearer)\n    d[\"target\"] = d[\"outcome\"]\n    del d[\"outcome\"]\n\n    y = d[\"target\"]\n    del d[\"target\"]\n    X = d\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    # Replace na's - do it WITHOUT using the test data or we're cheating\n    # Add isnull bool column to signal the value was actually unknown\n    # Median - these cols are more or less normally distributed/have a bell shape\n    nan_cols = [\"packed_cell_volume\", \"pulse\", \"rectal_temp\"]\n    for c in nan_cols:\n        med = X_train[c].median()\n        X_train[f\"{c}_nan\"] = X_train[c].isnull().astype(\"int\")\n        X_train.loc[:, c] = X_train[c].fillna(med)\n        X_test[f\"{c}_nan\"] = X_test[c].isnull().astype(\"int\")\n        X_test.loc[:, c] = X_test[c].fillna(med)\n    \n    # Mode - not normally distributed, just use the most frequent value\n    nan_cols = [\"respiratory_rate\",\"nasogastric_reflux_ph\", \n                \"abdomo_protein\", \"total_protein\"]\n    \n    for c in nan_cols:\n        med = X_train[c].mode(dropna=True)[0]\n        X_train[f\"{c}_nan\"] = X_train[c].isnull().astype(\"int\")\n        X_train.loc[:, c] = X_train[c].fillna(med)\n        X_test[f\"{c}_nan\"] = X_test[c].isnull().astype(\"int\")\n        X_test.loc[:, c] = X_test[c].fillna(med)\n    \n    d[\"target\"] = y\n    return X_train, X_test, y_train, y_test, d","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/horse-colic/horse.csv')\nX_train, X_test, y_train, y_test, df = preprocess(df)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensure no remaining nan's\nX_train.isnull().sum().sum() #sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Good candidates for feature selection\n# Get correlated features\ncorred_cols = list(df.corr().target.abs().sort_values(ascending=False).head(n=30).index)\ncorred_cols = corred_cols[1:]\n#df.corr().target.abs().sort_values(ascending=False).head(n=30)#.index\ncorred_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(random_state=0, n_estimators=100)\nclf.fit(X_train[corred_cols], y_train)\n#\n# Use the forest's predict method on the test data\ny_pred = clf.predict(X_test[corred_cols])\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Accuracy: %s' % metrics.accuracy_score(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred, digits=3))\ncm = metrics.confusion_matrix(y_pred, y_test)\nsb.set(font_scale=1.3)\nsb.heatmap(cm, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pprint.pprint(dict(zip(corred_cols, clf.feature_importances_)))\n\nimportances = clf.feature_importances_\nfeat_importances = pd.Series(clf.feature_importances_, index=X_train[corred_cols].columns).sort_values(ascending=False)\nfeat_importances.nlargest(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = permutation_importance(clf, X_test[corred_cols], y_test, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots(figsize=(20,10))\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_test[corred_cols].columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how **pulse** is important in rf but not when using permutation importance. Maybe random forest is using that to overfit the data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove pulse\ncols = corred_cols.copy()\ncols.remove(\"pulse\")\n#cols.remove(\"peripheral_pulse_normal\")\ncols.remove(\"nasogastric_reflux_ph\")\nclf = RandomForestClassifier(random_state=42, n_estimators=100)\nclf.fit(X_train[cols], y_train)\n#\n# Use the forest's predict method on the test data\ny_pred = clf.predict(X_test[cols])\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Accuracy: %s' % metrics.accuracy_score(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred, digits=3))\ncm = metrics.confusion_matrix(y_pred, y_test)\nsb.set(font_scale=1.3)\nsb.heatmap(cm, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It did increase the accuracy :)"},{"metadata":{},"cell_type":"markdown","source":"## LGBMClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_valid, x_test, y_valid, y_test = train_test_split(X_train, y_train, test_size=0.5, random_state=1)\nclf = lgb.LGBMClassifier(device='cpu',  \n                         num_threads=6,\n                         bagging_freq=5,\n                         bagging_fraction= 0.9,\n                         feature_fraction= 0.80,\n                         learning_rate=0.05,\n                         min_data_in_leaf=2,\n                         num_leaves=81,\n                         random_state=42\n                        )\n\n\nclf.fit(X_train[corred_cols], y_train)\ny_pred = clf.predict(X_test[corred_cols])\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Accuracy: %s' % metrics.accuracy_score(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred, digits=3))\ncm = metrics.confusion_matrix(y_pred, y_test)\nsb.set(font_scale=1.3)\nsb.heatmap(cm, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use cols (no pulse)\nclf = lgb.LGBMClassifier(device='cpu',  \n                         num_threads=6,\n                         bagging_freq=5,\n                         bagging_fraction= 0.7, # changed\n                         feature_fraction= 0.80,\n                         learning_rate=0.05,\n                         min_data_in_leaf=4, # changed\n                         #num_leaves=81\n                         random_state=42\n                        )\n\n\nclf.fit(X_train[cols], y_train)\ny_pred = clf.predict(X_test[cols])\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Accuracy: %s' % metrics.accuracy_score(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred, digits=3))\ncm = metrics.confusion_matrix(y_pred, y_test)\nsb.set(font_scale=1.3)\nsb.heatmap(cm, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}