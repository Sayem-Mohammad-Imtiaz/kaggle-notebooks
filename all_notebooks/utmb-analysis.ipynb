{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Reading and preparing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nfrom IPython.display import display","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Reading csv files","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A first inspection of the downloaded files is made, without reading them, and it is verified that there are 15 csv files ranging from the year 2003 to 2017. The one corresponding to the year 2003 is particularly small compared to the others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_folder = '/kaggle/input/ultratrail-du-montblanc-20032017'\nfiles = sorted(os.listdir(raw_folder))\n\ndef full_file(dirname, filename):\n    return os.path.join(dirname, filename)\n\nprint(files)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to load each file as a dataframe without having to explicitly define a variable for each one of them, I will store them in a dictionary whose key will be the year of the file and the value, the corresponding dataframe. Likewise, all concatenated annual dataframes will be stored in a single dataframe, so that all data can be inspected together. A variable with the corresponding year is added to each dataframe. I declare an auxiliary function to be able to read the files as it will be reused later. For this part of the work it will be enough to read only a couple of rows.****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function to load csv. If nrows is less than 0 it returns all rows\n\n\ndef read_csvs(nrows, show=True):\n    dic = dict()\n    df = pd.DataFrame()\n    for file in files:\n        year = file[5:9]\n        if nrows > 0:\n            args = {'filepath_or_buffer': full_file(raw_folder, file), 'nrows': nrows}\n        else:\n            args = args = {'filepath_or_buffer': full_file(raw_folder, file)}\n        dic[year] = pd.read_csv(**args)\n        dfd = dic[year]\n        dfd['Year'] = year\n        if show:\n            print(year, list(dfd))\n        df = pd.concat([df, dfd], sort=False)\n    return dic, df\n\n\ndf_dict, df = read_csvs(nrows=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Data preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We see that the number and name of columns of each dataframe is quite variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the global dataframe we have 136! columns whose names are listed below, it must be taken into account that those with the same name in the source files are only seen once, a hard work awaits us ... I begin by listing the columns in alphabetical order:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = np.array(list(df))\ncolumns.sort()\ncolumns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are basically two things observed:\n  1. Some of them are repeated, but the capitalization of the letters changes: `'Champ' - 'champ'`.\n  2. Many of them look alike eg: `'Champex La', 'Champex Lac'`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We convert all column names to lowercase and retrieve unique values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = np.char.lower(np.array(list(df)))\ncolumns = np.lib.arraysetops.unique(columns)\ncolumns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We verify that we have managed to reduce the number of columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a first look at the content of the global dataframe to see what data it stores. 10 random observations are displayed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# option to show all columns of the dataframe\npd.set_option ('display.max_columns', df.shape [1])\n# we choose 10 at random\ndf.sample (10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most of them are passing times and their names, for sure, the place where the runner has been timed. There are also other data such as number, name, team, category, position and the like.\n#### To continue, each column must be mapped with a new name that groups its \"reasonable resemblances\" in a relation n to 1. For example, it is logical to relate `'bert', 'berton', 'bertone'` with `'Bertone'`. For this task, each name must be identified with a place crossed by the route. *Behind the scenes*, using the race website, Google Maps and some patience, a table with the relationships has been assembled. Of all of them, it has not been possible to identify five.\n#### To distinguish the time columns, the unknown and the remaining ones, a qualifying field is added. Two iterations have been made in this process since in the final concatenation of dataframes the year 2012 failed - it has two fields `'conta'` and`' coun r'` that were assigned to `'Contamines'` - and the year 2015 - has two fields `'courm1'` and`' courm2'` that mapped to `'Courmayeur'`â€“.\n#### A convenience field `Order` has also been added by which the columns of the final dataframe will be ordered. All the time columns are located at the end of the dataframe starting at index 8 and their position is relative to the race course.\n#### In the dataframe there are two fields: `'Cham', 'cham'` which can be Champex or Chamoniz, in the list of the dataframe above it is verified that their times are higher than those of passage through Champex and similar to those of the arrival in Chamonix. They are therefore assigned to the latter and the name of the field will be `Arrivee` respecting the original French for arrival but without accentuation signs, for the rest of the fields they have also been eliminated, as well as the spaces and hyphens, keeping only the characters of the alphabet English.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" #### The result of the work described is saved in a csv file that is retrieved in the lower cell.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file = 'columns.csv'\ndata_folder = '/kaggle/input/utmbcolumns'\ncols_map = pd.read_csv(full_file(data_folder, file))\ncols_map[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The unknown variables are:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unknowns = cols_map.loc[cols_map['type'] == 'Unknown']\nunknowns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### To continue, this time we will load all the data without displaying it on the screen using the auxiliary function created in cell 4.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dict, df = read_csvs(nrows=-1, show=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now you have to rename the columns of each dataframe. It is important to bear in mind that this part has to be done in each individual dataframe, and then concatenate them; if it were done in the global dataframe, the new names of the columns with different capitalization would become identical. The global dataframe in which all the annual dataframes are concatenated is initialized.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the global dataframe\ndf = pd.DataFrame()\n# we go through the dictionary\nfor k, dfd in df_dict.items():\n     # create a dataframe of a column with the column names in lowercase\n    cols_dfd = pd.DataFrame(np.char.lower(np.array(list(dfd))))\n    # we put the same name as its equivalent column on the map\n    cols_dfd.columns = ['old']\n     # merge the map and the original columns by 'old'\n    cols_new = pd.merge(cols_dfd, cols_map)['new'].tolist()\n     # we rename the columns in the original\n    dfd.columns = cols_new\n     # add the corrected annual dataframe to the global dataframe\n    df = pd.concat([df, dfd], sort=False)\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We check its dimensions and see that the variables have gone from 136 to 53. Not bad ...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We sort the columns according to the column map.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### We sort the columns according to the column map. # Order is repeated in the new columns that group the original ones\n# the minimum value is taken, but the maximum or the average would be equal\ngroup = cols_map.groupby ('new') ['order']. min (). sort_values ()\n# we save the ordered columns\ncolumns_order = list (group.index.values)\n# the global dataframe is reindexed with ordered columns\ndf = df.reindex (columns = columns_order)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we check if there are variables with all null observations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"empties = df.columns[df.isna().all()].tolist()\nempties\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One appears and we delete it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=empties, inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data in the chrono columns are `timedeltas`. We will make sure that everyone is properly trained before finishing the cleaning.\nIf we look at the data in `Timediff` we see that there are observations of the form` mm: ss.0` and the function expects `hh: mm: ss.0`. We are going to pass this data to the `hh: mm: ss` format, also valid for` pd.to_timedelta () `.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Timediff'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we reset the index since we are going to use masks to act only\n# on affected observations\ndf = df.reset_index(drop=True)\n# we create a mask with the values that end in .0 ignoring the non-null\nmask = df['Timediff'].str.endswith(\".0\") & pd.notnull(df['Timediff'])\n# the replacement is done using regex syntax\ndf['Timediff'] = df['Timediff'].loc[mask].str.replace('\\\\.0', '', 1)\n# the hh: part is added at the beginning\ndf['Timediff'] = df['Timediff'].loc[mask].apply(lambda x: '00:' + x)\ndf['Timediff'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We verify that all values accept the conversion to timedelta.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cronos_list = list(df)[8:]\ncronos_df = df[df.columns.intersection(cronos_list)]\nfor crono in cronos_list:\n    print(crono)\n    pd.to_timedelta(cronos_df[crono])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', df.shape[1])\ndf[:3]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Column type conversion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Columns with the chrono label are located from index 8. We retrieve them directly without going through the column map. At the end we verify that we can operate with the values correctly with any subtraction of times.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Year'] = df['Year'].astype(int)\n\ncronos = list(df)[8:]\nfor crono in cronos:\n    print(crono)\n    td = pd.to_timedelta(df[crono])\n    df[crono] = td\n\ndf.Time[30] - df.Time[29]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Verifications","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We will do some basic checks. We start by checking the `Arrivee` variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('Year')['Arrivee'].min().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Three abnormal values are seen\n  1. 2012 Arrival time is half of the other reported years.\n  2. 2003 There is no arrival time.\n  3. 2010 There is no arrival time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We prepare an auxiliary function to retrieve only the data of the columns with information, the varied scheme of each year means that in many variables there is no data; in this way we can perform the analysis with less noise.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# auxiliary function to recover a year by eliminating\n# columns with all nans values\n\ndef get_year(df, year):\n    year_df = df.loc[df.Year == year]\n    notnas = year_df.columns[year_df.notna().any()].tolist()\n    return year_df[notnas]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Year 2012","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_year(df, 2012)[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For 2012 we see that the data is correct and seems reasonable, but the passing points and times indicate that it is another race. We will not be able to use this year to contrast it with the others. Since we don't even know what race it is, we are going to exclude it directly from the rest of the analysis. You have to delete the observations whose year is 2012 and then delete the exclusive variables of that year knowing that they have no data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we exclude the observations from the year 2012 (it's another race)\ndf = df[df.Year != 2012]\n# we locate your unique variables knowing that they will be nans\nempties = df.columns[df.isna().all()].tolist()\ndisplay(empties)\n# we delete them from the dataframe\ndf.drop(columns=empties, inplace=True)\ndisplay(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Year 2003","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_year(df, 2003)[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In 2003 it is verified that there are no intermediate passage times, but only the total time. Its observations can inform the analysis. We will update the `Arrivee` column with the value of` Time`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = df.Year == 2003\ndf.loc[mask, 'Arrivee'] = df.loc[mask, 'Time']\nget_year(df, 2003)[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Year 2010","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_year(df, 2010)[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2010 is not a great year for our analysis task, there are only two passing columns and no total time. We will keep the observations because they can be useful for the analysis of participants when times are not required.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Finally, we examine every year to check their quality - at least those in the first rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"years = np.arange(2003, 2018)\nfor y in years:\n    if y != 2012:\n        display(get_year(df, y).head(3))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### New kid in town! Everything seems correct except a negative time of passage in 2005 for the runner with number 1 in Contamines. Let's review the negative times.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we retrieve the time data by position\n# knowing they are consecutive\ncronos = df[list(df)[8:]]\n\n\ndef check_less_than_0(cronos):\n    # we filter negative times and not nans\n    less_than_0 = (cronos < pd.Timedelta(0)) & pd.notna(cronos)\n    # we add the result to count them (sum of booleans)\n    totals_lt0 = less_than_0.sum().copy()\n    # we save those that have at least one negative value in an orderly fashion\n    totals_gt0 = totals_lt0[totals_lt0 > 0].sort_values(ascending=False)\n    display(totals_gt0)\n    return totals_gt0.index.values\n\n\ncols_lt0 = check_less_than_0(cronos)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are not many, but it is not correct to leave them in the dataframe. We set them to `nan`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cols_lt0:\n    mask = (cronos[col] < pd.Timedelta(0)) & pd.notna(cronos[col])\n    df.loc[mask, col] = np.nan\n\ncronos = df[list(df)[8:]]\ncheck_less_than_0(cronos)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's now see the quality of the Nationality variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Nationality = df.Nationality.str.lower()\n\ncountries = df.groupby('Nationality')[\n    'Nationality'].count().sort_values(ascending=False)\n\ndisplay(countries[:5])\n\ndisplay(countries[-5:])\n\ndisplay(countries.index.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### **Caution!**, there is a country with a blank space as a code. We will take it into account later.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We review the categories. In the listings above you could see some categories with spaces.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we pass all values to lowercase\ndf.Category = df.Category.str.lower()\n# we remove the spaces\ndf.Category = df.Category.str.replace(' ', '')\n# we group, count and order the countries\ncategories = df.groupby('Category')['Category'].count()\n# we show the first ones\ndisplay(categories[:5])\n# we show the latest ones\ndisplay(categories[-5:])\n# we show all values\ndisplay(categories.index.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The category variable is made up of the category and gender. Let's separate them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Sex'] = df.Category.str[2]\ndisplay(df['Sex'][:3])\n\ndf['Category'] = df.Category.str[:2]\n\ncols = list(df)\ncols.remove('Sex')\ncols.insert(8, 'Sex')\ndf = df[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ### Settings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nsns.set_context('notebook', font_scale=1.3, rc={'lines.linewidth': 2})\nplot_width = 12\nplot_height = 8\nplt.rcParams.update({'figure.max_open_warning': 0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Data visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Let's start by looking at the evolution of the number of participants per year showing those who managed to finish the race. Remember that for 2010 we do not have arrival data, so we exclude it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fdf = df[df.Year != 2010]\n\nrunners = pd.DataFrame(fdf.groupby('Year')['Id'].count())\nrunners.rename(columns={'Id': 'Runners'}, inplace=True)\nrunners.reset_index(level=0, inplace=True)\n\nfinishers = pd.DataFrame(\n    fdf.loc[df.Arrivee.notna()].groupby('Year')['Id'].count())\nfinishers.rename(columns={'Id': 'Finishers'}, inplace=True)\nfinishers.reset_index(level=0, inplace=True)\n\nf, ax = plt.subplots(figsize=(plot_width, plot_height))\n\nsns.set_color_codes(\"pastel\")\nsns.barplot(data=runners, x='Year', y='Runners',\n            label=\"Runners\", color='b')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(data=finishers, x='Year', y='Finishers',\n            label=\"Finishers\", color='b')\n\nax.set(xlabel='Year', ylabel='Runners')\nax.legend(ncol=1, loc=\"upper left\", frameon=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We see that the first year they all finished, there would probably be more participants but the dataset that we have for that year must be exclusively those who finished. In 2014 barely a third of the runners arrived and in 2015 and 2016 less than half. In the rest of the years there is no regular pattern. We would have to try to find out the cause.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The winners time graph is shown below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"winners = pd.DataFrame(fdf.groupby('Year')['Arrivee'].min().dropna())\nwinners.reset_index(level=0, inplace=True)\nwinners['Hours'] = winners.Arrivee / np.timedelta64(1, 'h')\n\nf, ax = plt.subplots(figsize=(plot_width, plot_height))\n\n# sns.set_color_codes(\"pastel\")\nsns.lineplot(data=winners, x='Year', y='Hours',\n             label=\"Winner time\", color='b')\nax.set_ylim(18.5, 22.5)\n\nax.set(xlabel='Year', ylabel='Hours')\nax.legend(ncol=1, loc=\"upper left\", frameon=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The time of 2003 is the second fastest, a bit surprising; in those years mountain races were beginning and the participants were amateurs. With professional runners, the time was not lowered until 14 years later, it does not seem credible. It is more plausible that after the first edition a modification of the route was made.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Will race time be related to the percentage of runners who finish the race? In a race that runs at an altitude of more than 2,000 m for most of its length, weather conditions and the state of the terrain may influence race speed. Let's see, we have to separate 2003 from this analysis since there is no data on non-finalizers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmerged1 = pd.merge(winners, runners)\nmerged1 = pd.merge(merged1, finishers)\n\nmerged1['FinishersPerc'] = merged1.Finishers / merged1.Runners * 100\n\nmerged1 = merged1[merged1.Year != 2003]\n\nf, ax = plt.subplots(figsize=(plot_width, plot_height))\n\nsns.scatterplot(data=merged1, x=\"FinishersPerc\", y=\"Hours\",\n                s=100)\n\nax.set(xlabel='% finishers', ylabel='Hours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### If we look at the points to the right of 55% we see a negative correlation: the higher the percentage of finishers, the better the race time. But we are only looking at the time of the winners, we will extend the study to the average time of all those who arrived.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_time = pd.DataFrame(\n    fdf.loc[df.Arrivee.notna()].groupby('Year')['Arrivee'].sum())\ntotal_time.rename(columns={'Arrivee': 'TotalTime'}, inplace=True)\n\ntotal_time['TotalHours'] = total_time.TotalTime / np.timedelta64(1, 'h')\n\ntotal_time.reset_index(level=0, inplace=True)\n\nmerged2 = pd.merge(merged1, total_time)\n\nmerged2['AvgFinishersTime'] = merged2.TotalHours / merged2.Finishers\n\n\nf, ax = plt.subplots(figsize=(plot_width, plot_height))\n\nsns.scatterplot(data=merged2, x=\"FinishersPerc\", y=\"AvgFinishersTime\", s=100)\n\nax.set(xlabel='% finishers', ylabel='Hours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Well, there is a growing trend, it can be said that, in general, the higher the percentage of finishers, the average number of hours per finisher increases. The correlation between the series is 0.67:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"merged3 = merged2[['FinishersPerc', 'AvgFinishersTime']]\ncorr = merged3.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using `jointplots` the mapping looks like this","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"j = sns.jointplot(data=merged2, x='FinishersPerc',\n                  y='AvgFinishersTime', kind='reg')\nj.ax_joint.set_xlabel('% finishers')\nj.ax_joint.set_ylabel('Hours')\n\nj = sns.jointplot(data=merged2, x='FinishersPerc',\n                  y='AvgFinishersTime', kind='kde')\nj.ax_joint.set_xlabel('% finishers')\nj.ax_joint.set_ylabel('Hours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We take the opportunity to see the evolution of the average time of those who finished the race.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(plot_width, plot_height))\n\nsns.lineplot(data=merged2, x='Year', y='AvgFinishersTime',\n             label=\"Finishers average time\", color='b')\n\nax.set(xlabel='Year', ylabel='Hours')\nax.legend(ncol=1, loc=\"upper left\", frameon=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### We see that in the evolution of the average time of those who finished the race an upward trend is observed until 2011, when it begins to oscillate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### In terms of participants by country, the presence of French is overwhelming. We show two charts for easy viewing. We do the study on the complete dataframe recovering the year 2010 since we do not deal with times.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = pd.DataFrame(df.groupby('Nationality')['Nationality'].count().sort_values(ascending=False))\n\ncountries.rename(columns={'Nationality': 'Quantity'}, inplace=True)\n\ncountries.reset_index(level=0, inplace=True)\ncountries.rename(columns={'Nationality': 'Country'}, inplace=True)\n\ncountries = countries.loc[countries.Country != ' ']\n\ndisplay(countries[:10])\n\n\ncountries_above = countries.loc[countries.Quantity >= 50]\ncountries_below = countries.loc[countries.Quantity < 50]\n\n\nf, ax = plt.subplots(figsize=(plot_width, plot_width))\n\nsns.barplot(data=countries_above, x='Quantity', y='Country',\n            label=\"Runners\", color='b')\n\nax.set(title='Countries with 50 or more runners',\n       xlabel='Quantity', ylabel='Country')\nax.legend(ncol=1, loc=\"lower right\", frameon=False)\n\nf, ax = plt.subplots(figsize=(plot_width, plot_width))\n\nsns.barplot(data=countries_below, x='Quantity', y='Country',\n            label=\"Runners\", color='b')\n\nax.set(title='Countries with less than 50 runners',\n       xlabel='Quantity', ylabel='Country')\nax.legend(ncol=1, loc=\"lower right\", frameon=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's check what happens with participation by gender.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_df = df.pivot_table('Id', index='Year', columns='Sex', aggfunc='count')\n\nsex_df.reset_index(level=0, inplace=True)\ndisplay(sex_df)\n\nf, ax = plt.subplots(figsize=(plot_width, plot_height))\n\nsns.set_color_codes(\"pastel\")\nsns.barplot(data=sex_df, x='Year', y='h',\n            label=\"Men\", color='b')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(data=sex_df, x='Year', y='f',\n            label=\"Women\", color='b')\n\nax.set(xlabel='Year', ylabel='Runners')\nax.legend(ncol=1, loc=\"upper left\", frameon=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Wow, the girls turnout is only around 10% on a steady basis. Let's review the performance of the girls vs the boys.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"times_sex_df = fdf.pivot_table('Arrivee', index='Year', columns='Sex',\n                               aggfunc=[np.sum, np.min, 'count'])\n\ncols = ['tf', 'th', 'mf', 'mh', 'cf', 'ch']\ntimes_sex_df.columns = cols\n\ntimes_sex_df.tf = times_sex_df.tf / np.timedelta64(1, 'h')\ntimes_sex_df.th = times_sex_df.th / np.timedelta64(1, 'h')\ntimes_sex_df.mf = times_sex_df.mf / np.timedelta64(1, 'h')\ntimes_sex_df.mh = times_sex_df.mh / np.timedelta64(1, 'h')\ntimes_sex_df['tt'] = times_sex_df.th + times_sex_df.tf\n\n\ntimes_sex_df['af'] = times_sex_df.tf / times_sex_df.cf\ntimes_sex_df['ah'] = times_sex_df.th / times_sex_df.ch\ntimes_sex_df['at'] = times_sex_df.tt / (times_sex_df.ch + times_sex_df.cf)\n\n\ntimes_sex_df.reset_index(level=0, inplace=True)\ndisplay(times_sex_df)\n\nf, ax = plt.subplots(figsize=(plot_width, plot_height))\n\nsns.lineplot(data=times_sex_df, x='Year', y='af',\n             label=\"Avg girls\", color='g')\nsns.lineplot(data=times_sex_df, x='Year', y='ah',\n             label=\"Avg boys\", color='b')\nsns.lineplot(data=times_sex_df, x='Year', y='at',\n             label=\"Avg all\", color='r')\nsns.lineplot(data=times_sex_df, x='Year', y='mf',\n             label=\"Min girls\", color='m')\nsns.lineplot(data=times_sex_df, x='Year', y='mh',\n             label=\"Min boys\", color='y')\n\nax.set(xlabel='Year', ylabel='Hours')\nax.legend(ncol=1, loc='best', frameon=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Well! the difference in the last three years is around half an hour and in 2011 we are talking about minutes. It also stands out that the average of all the runners is highly influenced by the great difference in participation between boys and girls, being almost parallel to that of the first. On the other hand, the distance by sex between winners and between the averages is remarkable: the overall of the girls is close to that of the boys, but between the numbers one the lines are much more separated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(\"Year\", data=df, aspect=4.0, kind='count',\n                   hue='Category', order=range(2003, 2018))\ng.set_xlabels('Year')\ng.set_ylabels('Runners by category')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Regarding the comparative evolution of the categories, it can be seen that the pattern of participants is almost clonic in recent years.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### In order to see the evolution at each crossing point of the first n runners of each race, an auxiliary function has been defined that accepts as parameters the desired year and the number of runners - although, for clarity, it is not recommended to put more than 10-. The function is able to generate the waypoints dynamically, showing those corresponding to each year. We launched the run for 10 male and female runners. Remember that for 2003 there were no stages, so straight lines are shown. In no case are big comebacks seen.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_topN(df, year, n=5, sex='all'):\n\n    def get_year(df, year, sex):\n        # filtra el aÃ±o\n        year_df = df.loc[df.Year == year]\n        \n        if sex == 'h' or sex == 'f':\n            year_df = year_df.loc[year_df.Sex == sex]\n        \n        notnas = year_df.columns[year_df.notna().any()].tolist()\n        return year_df[notnas]\n\n    \n    ydf = get_year(df, year, sex)\n    \n    n = min(ydf.shape[0], n)\n\n    \n    topN = ydf.sort_values(by='Arrivee')[:n]\n    \n    cols = list(topN)[10:]\n    \n    cronos = topN[cols] / np.timedelta64(1, 'h')\n    \n    cronos['Name'] = topN.Name\n    \n    cronos.set_index('Name', inplace=True)\n\n    \n    plt.subplots(figsize=(plot_width, plot_height))\n    plt.tight_layout(pad=2)\n    \n    y = np.arange(0, len(cols))\n    \n    for i in np.arange(0, n):\n    \n        x = cronos.iloc[i]\n    \n        plt.yticks(y, cols)\n        plt.plot(x, y, label=x.name)\n    plt.legend(framealpha=1, frameon=True)\n    \n    fig = plt.gcf()\n    if sex == 'f':\n        who = 'Girls'\n    elif sex == 'h':\n        who = 'Boys'\n    else:\n        who = 'Girls & Boys'\n    fig.suptitle('Evolution of the top ' + str(n) +\n                 ' .Year: ' + str(year) + '. ' + who + '.')\n\n\nfor x in np.arange(2003, 2018):\n\n    if not (x == 2010 or x == 2012):\n        plot_topN(df, x, 10, 'f')\n        plot_topN(df, x, 10, 'h')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Boys**. In 2005 there are a few nulls. In 2006 there is a crossing point with no data and a timing in Tseppes by runner Vincent Delebarre clearly wrong. In general, it is seen that the separation between the first and the tenth is more than three hours, except in 2016 when it does not reach two hours: the lines at the arrival are very close.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### **Girls**. In 2004, 2007 and 2008 there are also a few nulls. In this case, the separation between the first and the tenth is much wider, except in 2016 when it is less than two hours: the lines at the finish are very close.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}