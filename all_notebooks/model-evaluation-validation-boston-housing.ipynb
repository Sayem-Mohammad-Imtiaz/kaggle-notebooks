{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n###########################################\n\nimport matplotlib.pyplot as pl\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, learning_curve, validation_curve, KFold, cross_val_score, GridSearchCV\nimport seaborn as sns\nimport pandas as pd\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error,make_scorer,r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pretty display for notebooks\n%matplotlib inline\n\n# Load the Boston housing dataset\nfilename = (\"../input/housing.csv\")\nnames = ['CRIME', 'ResZoNe', 'N-Retail Bussiness', 'CHARS River', 'NitrOxd', 'Rooms/dwell', 'AGE', 'Dis to comp', 'Dist to highway',\n         'TAX', 'PTRATIO', 'B','LSTAT', 'MedV']\ndataset = pd.read_csv(filename, delim_whitespace=True, names=names)\nprices = dataset['MedV']\nfeatures = dataset.drop('MedV', axis = 1)\n# Success\nprint(\"Boston housing dataset has {} data points with {} variables each.\".format(*dataset.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms\ndataset.hist(figsize=(9,7),grid=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that some variables have an exponential distribution like B,Crime, Chars river, res zone. And Dist to highway, tax had bimodal dist\nI do feel medv, rooms/dwell, PTRatio & lstat are important removing other noise"},{"metadata":{"trusted":true},"cell_type":"code","source":"features=features.drop(['CRIME', 'ResZoNe', 'CHARS River', 'NitrOxd', 'AGE', 'Dis to comp',  'TAX', 'B',], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(prices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prices are deviated from normal distribution have appreciable positive skewness and showed peakedness"},{"metadata":{},"cell_type":"markdown","source":"# Correlation HeatMap"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=dataset.corr()\npl.figure(figsize=(10, 10))\nsns.heatmap(corr, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\npl.title('Correlation between features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables having strong Corelation with target varible MEDV are RM (.7), LSTAT(-.74), PTRATIO(-.51). dropping others"},{"metadata":{"trusted":true},"cell_type":"code","source":"features=features.drop(['N-Retail Bussiness','Dist to highway'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Feature observation\npl.figure(figsize=(20, 5))\nfor i, col in enumerate(features.columns):\n    pl.subplot(1, 13, i+1)\n    pl.plot(dataset[col], prices, 'o')\n    pl.title(col)\n    pl.xlabel(col)\n    pl.ylabel('prices')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, prices,test_size=0.2, random_state=31)\n\nprint(\"Training and testing split was successful.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test options and evaluation metric using Root Mean Square error method\nnum_folds = 10\nseed = 7\nRMS = 'neg_mean_squared_error'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Each Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Regressors"},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot():\n    fig = pl.figure()\n    fig.suptitle('Algorithm Comparison')\n    ax = fig.add_subplot(111)\n    pl.boxplot(results)\n    ax.set_xticklabels(names)\n    pl.show()\nboxplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardize the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Scaled Regressors"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Various Ensemblor regressors"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensembles = []\nensembles.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor(n_neighbors=7))])))\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor(n_estimators=100))])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=RMS)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning scaled Gradient Boost Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=range(21,41,1))\nmodel_train = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model_train, param_grid=param_grid, scoring=RMS, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PlotLearningCurve(X, y):\n    \"\"\" Calculates the performance of several models with varying sizes of training data.\n        The learning and testing scores for each model are then plotted. \"\"\"\n\n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Generate the training set sizes increasing by 50\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n\n    # Create the figure window\n    fig = pl.figure(figsize=(10,7))\n\n    # Create three different models based on n_estimators\n    for k, n_est in enumerate([20,25,30,35]):\n        \n        regressor=GradientBoostingRegressor(n_estimators=n_est)\n        # Calculate the training and testing scores\n        sizes, train_scores, test_scores = learning_curve(regressor, X, y, \\\n            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n\n        # Find the mean and standard deviation for smoothing\n        train_std = np.std(train_scores, axis = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n\n        # Subplot the learning curve\n        ax = fig.add_subplot(2, 2, k+1)\n        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n        ax.fill_between(sizes, train_mean - train_std, \\\n            train_mean + train_std, alpha = 0.15, color = 'r')\n        ax.fill_between(sizes, test_mean - test_std, \\\n            test_mean + test_std, alpha = 0.15, color = 'g')\n\n        # Labels\n        ax.set_title('n_estimators = %s'%(n_est))\n        ax.set_xlabel('Number of Training Points')\n        ax.set_ylabel('Score')\n        ax.set_xlim([0, X.shape[0]*0.8])\n        ax.set_ylim([-0.05, 1.05])\n\n    # Visual aesthetics\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n    fig.suptitle('Learning Algorithm LC Performances', fontsize = 16, y = 1.03)\n    fig.tight_layout()\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PLotComplexityCurve(X, y):\n    \"\"\" Calculates the performance of the model as model complexity increases.\n        The learning and testing errors rates are then plotted. \"\"\"\n\n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Vary the n_estimators parameter from 1 to 100\n    n_estimators = np.arange(1,100)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = validation_curve(GradientBoostingRegressor(), X, y, \\\n        param_name = \"n_estimators\", param_range = n_estimators, cv = cv, scoring = 'r2')\n\n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot the validation curve\n    pl.figure(figsize=(7, 5))\n    pl.title('Learning algorithm Complexity Performance')\n    pl.plot(n_estimators, train_mean, 'o-', color = 'r', label = 'Training Score')\n    pl.plot(n_estimators, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    pl.fill_between(n_estimators, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    pl.fill_between(n_estimators, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n\n    # Visual aesthetics\n    pl.legend(loc = 'lower right')\n    pl.xlabel('N Estimators')\n    pl.ylabel('Score')\n    pl.ylim([-0.05,1.05])\n    pl.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Produce learning curves for varying training set sizes and number of regressors"},{"metadata":{"trusted":true},"cell_type":"code","source":"PlotLearningCurve(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Produce complexity curves for varying training set sizes and number of regressors"},{"metadata":{"trusted":true},"cell_type":"code","source":"PLotComplexityCurve(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=28)\nmodel.fit(rescaledX, y_train)\n# transform the validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=predictions.astype(int)\nsubmission = pd.DataFrame({\n        \"Org House Price\": y_test,\n        \"Pred House Price\": predictions\n    })\n\nprint(submission.head(10))","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}