{"cells":[{"metadata":{"_uuid":"929bb7239d67aa71cd065830de8680218023d9ae"},"cell_type":"markdown","source":"In this notebook, I'll use the dataset \"StackSample: 10% of Stack Overflow Q&A\", I'll only use the questions and the tags. \nI will implement a tag suggestion system. I'll both try machine learning models and deep learning models like Word2Vec. I'll then compare the performance of both approaches. \n\nThis notebook will be divided in 2 parts:\n* PART 1 : Cleaning data and EDA\n* PART 2 : Classical classifiers implemented (SGC classifier, MultiNomial Naive Bayes Classifier, Random Forest Classfier, ...\n","execution_count":null},{"metadata":{"_uuid":"a7cf70d54413735fe632aaa5df43f34e353e0ff4"},"cell_type":"markdown","source":"**PART 1: Cleaning Data and Exploratory Data Analysis**","execution_count":null},{"metadata":{"_uuid":"abe58b6f63ad6fc975f6b62c1a47d3af54a943e5"},"cell_type":"markdown","source":"**1.1 Setting up the dataset for later training**","execution_count":null},{"metadata":{"_uuid":"6ddb1e9c1032972f4c87204650195ee00fb3bb29"},"cell_type":"markdown","source":"Importing useful libraries at first","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport seaborn as sns\n\nimport warnings\n\nimport pickle\nimport time\n\nimport re\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.tokenize import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import hamming_loss\n\n\nimport logging\nfrom scipy.sparse import hstack\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('bmh')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Setting a random seed in order to keep the same random results each time I run the notebook\nnp.random.seed(seed=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc1c92bf4166de1e2ce4bfb229b40b32eb94374a"},"cell_type":"code","source":"import os \nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"120edf8b2306a6391ad992cf3f48598739ec1940"},"cell_type":"code","source":"# Importing the database \n\ndf = pd.read_csv(\"../input/Questions.csv\", encoding=\"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40c3e09d38aecf03ce13fd0d88809e80a67d3d1a"},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9beed1bea78dd1ed930caeef2923aef03420e1cc"},"cell_type":"code","source":"tags = pd.read_csv(\"../input/Tags.csv\", encoding=\"ISO-8859-1\", dtype={'Tag': str})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f4a7e6a7ee2745fc207f19fccb9c0dbc3a571a4"},"cell_type":"code","source":"tags.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab191660e879734a6c26d1c1f2afd0e1218a50fc"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b71d45e2c86152d95f22679f01d8f855b042a62e"},"cell_type":"code","source":"tags.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec6a03e07edd3ce3de1f20f073bdf18ffca87f4c"},"cell_type":"markdown","source":"First, what I want to do is to merge both dataframes. In order to do that, I'll have to group tags by the id of the post since a post can have multiple tags. I'll just use the groupeby function and then merge the dataframes on the id. ","execution_count":null},{"metadata":{"trusted":true,"_uuid":"933761c8b2bdb668e0bca3c9ea9cab36dd1a4391"},"cell_type":"code","source":"tags['Tag'] = tags['Tag'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267d7c8f01a3b427ab139d689ce5d13ab8820913","_kg_hide-output":true},"cell_type":"code","source":"grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4096130e47ec904f9bbe27822d1ddf403a675dc"},"cell_type":"code","source":"grouped_tags.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"a9ecae8a6445c671d9c0a1b207df8b530bae70ab"},"cell_type":"code","source":"grouped_tags.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8da81a33552cd273c80e159e0c1cc8dddc98399c"},"cell_type":"code","source":"grouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags.values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9968760c364a005b7de54bd908cfc612fc9defe8"},"cell_type":"code","source":"grouped_tags_final.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2612dea031562d420f8dac14801d167721d2bcc9"},"cell_type":"code","source":"df.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd794a28df6e19818c754702ed5f3745262d866f"},"cell_type":"code","source":"df = df.merge(grouped_tags_final, on='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"132c4501747d2832eaf8d95f5d9cb3acaadc97f4","scrolled":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df4a7b2e88af524f4bd778c223d580de76c57cf7"},"cell_type":"markdown","source":"Now, I'll take only quesions witha score greater than 5. I'm doing that for 2 reasons:\n* 1- I'll require less computational resources from kaggle.\n* 2- The posts will probably be with a better quality and will be better tagged since they have lots of upvotes. \n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ecebe90606c70e84e4e87e1661e49a11c37e529f"},"cell_type":"code","source":"df = df[df['Score']>5].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e462e56ec2d84a0f3810d53bee3715cb570907f1"},"cell_type":"markdown","source":"**1.2 Cleaning Data**","execution_count":null},{"metadata":{"trusted":true,"_uuid":"1550d62d082219a8c8564f8a95afb121d9096c67","scrolled":true},"cell_type":"code","source":"print('Dupplicate entries: {}'.format(df.duplicated().sum()))\ndf.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"097a6864ce893217365a3dca22bb68b442746740"},"cell_type":"markdown","source":"This is a very good dataset since there are no missing valeus or dupplicate values. ","execution_count":null},{"metadata":{"trusted":true,"_uuid":"6ed21b85891cb57be8273aa41443e6c8198c0d0c"},"cell_type":"code","source":"df.drop(columns=['Id', 'Score'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3a6dc67414c2faf61fc32adf821d5d6c5a00e4e"},"cell_type":"markdown","source":"### Tags","execution_count":null},{"metadata":{"_uuid":"7d52b54ca515eb7ef5a9545b9f722090abc4c683"},"cell_type":"markdown","source":"Let's do some cleaning on the tags' column. Furthermore, I decided to keep the 100 most popular tags because I'll be easier to predict the right tag from 100 words than from 14,000 and because we want to keep macro tags and not be too specific since it's only a recommendation for a post, the user can add more specific tags himself. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f52f8b1079382f88516dd1df91aeaebe6d4f1dc4"},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d112c6cca7ac7b8fecfafdecfe3c75820a8b6f8c","_kg_hide-output":true},"cell_type":"code","source":"df['Tags'] = df['Tags'].apply(lambda x: x.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84505cde455a6d528b08e72688caa6c092893388"},"cell_type":"code","source":"all_tags = [item for sublist in df['Tags'].values for item in sublist]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79be7667451d41ac7cd9ef52b7edae19a8c9e846"},"cell_type":"code","source":"len(all_tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a7e3c76664f2bbcbab52d1ecbf445e8451d877d"},"cell_type":"code","source":"my_set = set(all_tags)\nunique_tags = list(my_set)\nlen(unique_tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = Counter(all_tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(counts.most_common(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56cecaeb6a313c81202a08fbcca65e33da95b3be"},"cell_type":"code","source":"frequencies_words = counts.most_common(20)\ntags_features = [word[0] for word in frequencies_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a97a5e5d2e1031394382558d4dc3b54530400005"},"cell_type":"code","source":"print(tags_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dabce5e92dc9314e7042384add46fc2c31f3112"},"cell_type":"code","source":"def most_common(tags):\n    tags_filtered = []\n    for i in range(0, len(tags)):\n        if tags[i] in tags_features:\n            tags_filtered.append(tags[i])\n    return tags_filtered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b1f8eefaf9b94b5bffdbd295b5770fae7b2984"},"cell_type":"code","source":"df['Tags'] = df['Tags'].apply(lambda x: most_common(x))\ndf['Tags'] = df['Tags'].apply(lambda x: x if len(x)>0 else None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6aae9b05c9d212cb26adb487df1476e6040c48"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f9ffd9e9e0dfe7c2e4c595e31559e2f5ec513f4"},"cell_type":"code","source":"df.dropna(subset=['Tags'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"608e36814e0b2befb41bb9dc6eca073edf48fbe4"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8864db3bccbe04abb7656fa580fbbb87b0528c43"},"cell_type":"markdown","source":"We are here loosing 10000 rows but the it's for the greater good. ","execution_count":null},{"metadata":{"trusted":true,"_uuid":"0240dc179aeabfdd8bbd6638d0e1b3ae5cb56877"},"cell_type":"markdown","source":"**1.2.2 Body**","execution_count":null},{"metadata":{"_uuid":"598869f86fd4f9a6fe1e41f9acce43d392cb50aa"},"cell_type":"markdown","source":"In the next two columns: Body and Title, I'll use lots of text processing:\n* Removing html format \n* Lowering text\n* Transforming abbreviations \n* Removing punctuation (but keeping words like c# since it's the most popular tag)\n* Lemmatizing words\n* Removing stop words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import preprocess_kgptalkie as ps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_clean(x):\n    x = str(x).lower().replace('\\\\', '').replace('_', ' ')\n    x = ps.cont_exp(x)\n    x = ps.remove_emails(x)\n    x = ps.remove_urls(x)\n    x = ps.remove_html_tags(x)\n    x = ps.remove_accented_chars(x)\n    x = ps.remove_special_chars(x)\n    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Body'] = df['Body'].apply(lambda x: get_clean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Title'] = df['Title'].apply(lambda x: get_clean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Text'] = df['Title'] + \" \" +  df['Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"119c8c35293cccd9627b58544acaaf06b4cb32d2"},"cell_type":"markdown","source":"**PART 2: Classical classifiers**","execution_count":null},{"metadata":{"_uuid":"4d7d89ca6323645d6bf722bd572852edc2b9571c"},"cell_type":"markdown","source":"**2.1 Data preparation**","execution_count":null},{"metadata":{"_uuid":"ccbc4065cdf68e9ef58fcad2889fc7454372425a"},"cell_type":"markdown","source":"Now our data is almost ready to be put into a classifier. I just need to:\n* Binarize the tags\n* Use a TFIDF for body and Title\nThe parameters in the TFIDF are very important for the performance of our tags since we don't want him to delete words like c# or.net. To do that we need to use the following pattern : token_pattern=r\"(?u)\\S\\S+\"","execution_count":null},{"metadata":{"trusted":true,"_uuid":"8640de349b18ea297f15c3d3eed3836d3ed19e14"},"cell_type":"code","source":"y = df['Tags']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4669f647cb5a147710294f1116ac4e2f29f50d7"},"cell_type":"code","source":"multilabel = MultiLabelBinarizer()\ny = multilabel.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8fd01469710da8a69fc45de00d37901d570814a"},"cell_type":"code","source":"tfidf = TfidfVectorizer(analyzer = 'word', max_features=1000)\nX = tfidf.fit_transform(df['Text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1219d6cdc1f1d27c90910659f1be21a917f9288e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) # Do 70/30 split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9ba6707c5aef3d55a7a5a8d33182c87d83af703"},"cell_type":"markdown","source":"Now it's finally ready. ","execution_count":null},{"metadata":{"_uuid":"6755c0843211713e0b4bf90cb4618cfdaa9fef85"},"cell_type":"markdown","source":"**2.2 One vs Rest**","execution_count":null},{"metadata":{"_uuid":"c4448315a460e141e1c8b079b809c1040e4fc050"},"cell_type":"markdown","source":"To evaluate our models, I'll use the jacard score since it's the best fitted for multi label classification. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"03e10593770e8b27cb2a9574efcc9cd8b6dabbd6"},"cell_type":"code","source":"def avg_jacard(y_true,y_pred):\n    '''\n    see https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n    https://www.oreilly.com/library/view/mastering-machine-learning/9781788299879/87b63eb8-f52c-496a-b73b-42f8aef549fb.xhtml\n    '''\n    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n    \n    return jacard.mean()*100\n\ndef print_score(y_pred, clf):\n    print(\"Clf: \", clf.__class__.__name__)\n    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n    print(\"---\")    ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6fb3326bc2c812cd991ca969762e54190b548150"},"cell_type":"code","source":"sgd = SGDClassifier()\nlr = LogisticRegression()\nsvc = LinearSVC()\n\nfor classifier in [sgd, lr, svc]:\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print_score(y_pred, classifier)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c14aca676e8fe5e65beba7b40ce7d6295e8a4242"},"cell_type":"markdown","source":"**2.6 Confusion matrix**","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"444d6f85d6c9c121c3ecf4654cb7a37e21974321"},"cell_type":"code","source":"for i in range(y_train.shape[1]):\n    print(multilabel.classes_[i])\n    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Store","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Text', 'Tags']].to_csv('stackoverflow.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import hamming_loss\n\n\n# warnings.filterwarnings(\"ignore\")\n# plt.style.use('bmh')\n# %matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport ast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('./stackoverflow.csv', index_col = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Tags'] = df['Tags'].apply(lambda x: ast.literal_eval(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Tags']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Tags'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel = MultiLabelBinarizer()\ny = multilabel.fit_transform(df['Tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df['Text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token = Tokenizer()\ntoken.fit_on_texts(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(token.word_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(token.word_index) + 1 #https://keras.io/api/layers/core_layers/embedding/\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = ['i love i rt the']\n# x = [1, 2, 3, 4, 6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token.texts_to_sequences(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_text = token.texts_to_sequences(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 100\nX = pad_sequences(encoded_text, maxlen=max_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_size = 50\ndef get_model():\n  model = Sequential()\n  model.add(Embedding(vocab_size, vec_size, input_length=max_length))\n\n  model.add(Conv1D(32, 2, activation = 'relu'))\n  model.add(MaxPooling1D(2))\n  model.add(Dropout(0.2))\n\n  model.add(Conv1D(64, 3, activation = 'relu'))\n  model.add(MaxPooling1D(2))\n  model.add(Dropout(0.3))\n\n#   model.add(Dense(128, activation='relu'))\n#   model.add(Dropout(0.2))\n\n  model.add(Dense(128, activation='relu'))\n\n  model.add(GlobalMaxPooling1D())\n\n  model.add(Dense(y.shape[1], activation='softmax'))\n\n  return model\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### jacard_score Multilabel Classification Evaluation Metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\ndef avg_jacard(y_true,y_pred):\n    '''\n    see https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n    '''\n    jacard = K.sum(K.minimum(y_true,y_pred)) / K.sum(K.maximum(y_true,y_pred))\n    \n    return K.mean(jacard)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model()\nmodel.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = [avg_jacard])\nmodel.fit(X_train, y_train, epochs = 1, validation_data = (X_test, y_test), batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = \"I have some HTML tables which I extract from a third party program which I'd like to show without using a javascript the user gets to see 4 categories and each category has multiple options. From each category only 1 item can be selected\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_clean(x):\n    x = str(x).lower().replace('\\\\', '').replace('_', ' ')\n    x = ps.cont_exp(x)\n    x = ps.remove_emails(x)\n    x = ps.remove_urls(x)\n    x = ps.remove_html_tags(x)\n    x = ps.remove_accented_chars(x)\n    x = ps.remove_special_chars(x)\n    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_encoded(x):\n#   x = get_clean(x)\n  x = token.texts_to_sequences([x])\n  x = pad_sequences(x, maxlen=max_length, padding = 'post')\n  return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coded = get_encoded(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict_classes(coded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel.inverse_transform(model.predict_classes(coded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel.classes_[11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multilabel.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}