{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This case study will help us to understand the These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. We will focus on the following stages namely -\n\n\n* Data Preparation\n* Feature Engineering\n* Feature Subset Selection\n* Model Training\n* Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"**Dataset Data**:\n\nhttps://www.kaggle.com/sadeghjalalian/wine-customer-segmentation"},{"metadata":{},"cell_type":"markdown","source":"**Possible Work to be done** :\n\n* Write a Data Science Proposal for achieving the objective mentioned.\n* Perform exploratory analysis on the data.\n* Perform data wrangling / pre-processing.\n* Apply any 2 features engineering technique.\n* Plot top 10 features.\n* Identification of the performance parameters to be improved, for the given problem statement.\n* Design Machine Learning models – Logistic regression and Decision tree to predict.\n* Compare the performance of selected feature engineering techniques.\n* Compare the performance of the 2 classifiers – Logistic regression and Decision tree to predict.\n* 10.Conclusions."},{"metadata":{},"cell_type":"markdown","source":"**Package Imports**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom statsmodels.tools.tools import add_constant\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import auc,roc_curve,f1_score,recall_score,precision_score\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"> Reading data from Wine Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"data =pd.read_csv(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confirm the imports**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the columns are numeric in nature"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Checking for unique data values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_unique_values(data_frame):\n    print(\"Unique value for dataset attributes :\\n\")\n    for column in data_frame.columns:\n        print(column, \" \" ,data_frame[column].unique(), \"\\n\")   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique_values(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Missing Values**\n> Lets see how the missing values can be replaced in the dataset. First check whereall the missing values are present.\n> \n> Take a closer look at the actual missing value count. 'False' means cell has a value whereas 'True\" means cell is missing value. Output the count for different attributes of dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_missing_values(data):\n    missing_data = data.isnull()\n    for column in missing_data.columns.values.tolist():\n        print(column)\n        print (missing_data[column].value_counts())\n        print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_missing_values(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the values is missing."},{"metadata":{},"cell_type":"markdown","source":"**Duplicate Records Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verifing duplicate records in wine datasets\nduplicate = data[data.duplicated()] \nduplicate \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There is no duplicate Data"},{"metadata":{},"cell_type":"markdown","source":"**Noisy Data Verification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\ndata.iloc[:,0:13].boxplot(figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#box plot\nsns.set(style=\"whitegrid\")\nsns.set(rc={'figure.figsize':(4,2)})\nsns.boxplot(x=data['Ash'])\nplt.show()\nsns.boxplot(x=data['Ash_Alcanity'])\nplt.show()\nsns.boxplot(x=data['Magnesium'])\nplt.show()\nsns.boxplot(x=data['Proanthocyanins'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There some outliers in the following features.**\n \n* Ash\n* Ash_Alcanity\n* Magnesium\n* Proanthocyanins"},{"metadata":{},"cell_type":"markdown","source":"**Verify Skewness in data**\n* Skewness is a measure of asymmetry of a distribution.\n* \n* If the skewness is between -0.5 and 0.5, the data are fairly symmetrical\n* If the skewness is between -1 and — 0.5 or between 0.5 and 1, the data are moderately skewed\n* If the skewness is less than -1 or greater than 1, the data are highly skewed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# skewness along the index axis \ndata.skew(axis = 0, skipna = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features:\n\nMagnesium\n\nMalic_Acid\nare highly skewed and hence should be transformed"},{"metadata":{},"cell_type":"markdown","source":"**Verify target class imbalance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Customer_Segment'].value_counts(sort = False, normalize = True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Customer Segment looks like balanced with 3 different types. No imabalance treatment required"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:,0:13]  #independent columns\ny = data.iloc[:,-1]    #target column i.e price range","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Subset Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_top_univariate_filters(data, score_func, top_k):\n    X = data.iloc[:,0:13]  #independent columns\n    y = data.iloc[:,-1]    #target column i.e price range\n\n    if score_func == \"chi2\":\n        func = chi2\n    elif score_func == \"f_classif\":\n        func = f_classif\n    elif score_func == \"mutual_info_classif\":\n        func = mutual_info_classif\n    \n    #apply SelectKBest class to extract top k best features\n    bestfeatures = SelectKBest(score_func=func, k=top_k)\n    fit = bestfeatures.fit(X,y)\n\n    dfscores = pd.DataFrame(fit.scores_)\n    dfcolumns = pd.DataFrame(X.columns)\n\n    #concat two dataframes for better visualization \n    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n    featureScores.columns = ['features','Score']  #naming the dataframe columns\n    print(featureScores.nlargest(top_k,'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_top_univariate_filters(data, 'chi2', 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_top_univariate_filters(data, 'f_classif', 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation Matrix with Heatmap**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\n\n#plot heat map\nplt.figure(figsize=(20,20))\nsns.heatmap(data[top_corr_features].corr(), annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Few observations : Following features are correlated when correlation >0.7\n\n* Total_Phenols and Flavanoids\n* Total_Phenols and OD280\n* Flavanoids and OD280"},{"metadata":{},"cell_type":"markdown","source":"**Variance inflation factor**"},{"metadata":{},"cell_type":"markdown","source":"* Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"VIF = pd.Series([variance_inflation_factor(X.values, i) \n               for i in range(X.shape[1])], \n              index=X.columns).to_frame()\n\n\nVIF.rename(columns={VIF.columns[0]: 'VIF'},inplace = True)\nVIF[~VIF.isin([np.nan, np.inf, -np.inf]).any(1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping Flavanoids column\nX.drop(\"Flavanoids\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transform skewed entities**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['Magnesium'] = np.log(X['Magnesium'])\nX['Malic_Acid'] = np.log(X['Malic_Acid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.skew(axis = 0, skipna = True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Subset Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_top_decition_classifier_feature(data, classifier, top_k):\n\n    if classifier == \"ExtraTreesClassifier\":\n        classifier = ExtraTreesClassifier\n    elif classifier == \"DecisionTreeClassifier\":\n        classifier = DecisionTreeClassifier\n       \n    model = classifier()\n    model.fit(X,y)\n\n    #use inbuilt class feature_importances of tree based classifiers\n    print(model.feature_importances_) \n\n    #plot graph of feature importances for better visualization\n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    feat_importances.nlargest(top_k).plot(kind='barh')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_top_decition_classifier_feature(data, \"DecisionTreeClassifier\", 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_top_decition_classifier_feature(data, \"ExtraTreesClassifier\", 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = LogisticRegression()\nlogistic_fit=logistic.fit(X_train,y_train)\ny_pred_test=logistic_fit.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Logistic model  Test Accuracy: {:.2f}%'.format(accuracy_score(y_test, y_pred_test) * 100))\n\nprint('Logistic model Classification report:\\n\\n', classification_report(y_test, y_pred_test))\n\nprint('Logistic model Training set score: {:.2f}%'.format(logistic_fit.score(X_train, y_train) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_test)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm)\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test Accuracy:',accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Accuracy:',accuracy_score(y_train,clf.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyper-parameter Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n'criterion':['gini','entropy'],\n'max_depth':[4,6,8,12]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_dt = GridSearchCV(clf,param_grid=param_grid,n_jobs=-1,cv=5,scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_dt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_dt.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_dt.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_dt = DecisionTreeClassifier(criterion='gini',max_depth=12)\nf_dt.fit(X_train,y_train)\ny_pred = f_dt.predict(X_test)\nprint('Test Accuracy:',accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Accuracy:',accuracy_score(y_train,f_dt.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test,y_pred)\ncm\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_prob1 = logistic_fit.predict_proba(X_test)\npred_prob2 = f_dt.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\nplt.plot(fpr2, tpr2, linestyle='--',color='green', label='Decision tree')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo_list=[]\nalgo_list=[\"Logistic Regression\",\"Decision Tree\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_list=[]\nscore_list.append(accuracy_score(y_test, y_pred_test) * 100)\nscore_list.append(accuracy_score(y_test, y_pred) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprecision=[]\nprecision.append(precision_score(y_test, y_pred_test, average='macro'))\nprecision.append(precision_score(y_test, y_pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrecall=[]\nrecall.append(recall_score(y_test, y_pred_test, average='macro'))\nrecall.append(recall_score(y_test, y_pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nf1score=[]\nf1score.append(f1_score(y_test, y_pred_test, average='macro'))\nf1score.append(f1_score(y_test, y_pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score={\"Algorithims\":algo_list,\"Scores\":score_list,\"precision_score\":precision,\"recall_score\":recall,\"f1_score\":f1score}\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(score)\ndf","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}