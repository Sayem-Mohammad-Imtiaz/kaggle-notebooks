{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Getting the feet wet on ML with the Titanc Dataset, by *Pau Orti*"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\n\n# CSV loaderfunction\ndef load_data(path, file_name):\n    csv_path = os.path.join(path, file_name)\n    return pd.read_csv(csv_path)\n\n# Extract X_test\nX_test = load_data('/kaggle/input/titanic-machine-learning-from-disaster','test.csv')\n\n# y_test\ny_test = load_data('/kaggle/input/y-test-titanic','y_test.csv')\n\n# Extract X_train\nX_train = load_data('/kaggle/input/titanic-machine-learning-from-disaster','train.csv')\n\n# Extract predictions (y_train) from X_train\ny_train = X_train[['Survived']]\ny_train.append(y_test[['Survived']])\n\n# Drop the predictions from the training set\nX_train.drop('Survived', axis=1, inplace=True)\n\n# Merge x_train and x_test\nX_merged = X_train.append(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n\n# COMBINE AND CLEAN UP DATA\n\nclass Modifier():\n    \n    def transform(self, df=X_merged, \n                  company=True, title=True, \n                  cabin_letter=True, ticket_num=True,\n                  age=True, pipeline=True):\n        if company:\n            df['Company'] = df['SibSp'] + df['Parch']\n        if title:\n            df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.')\n            \n            df['Title'] = df['Title'].replace(\n                                            ['Lady','Countess',\n                                             'Capt','Col',\n                                             'Don','Dr',\n                                             'Major','Rev',\n                                             'Sir','Jonkheer',\n                                             'Dona'],\n                                                    'Unusual')\n            \n            df['Title'] = df['Title'].replace('Mlle','Miss')\n            df['Title'] = df['Title'].replace('Ms','Miss')\n            df['Title'] = df['Title'].replace('Mme','Mrs')\n        if cabin_letter:\n            df['Cabin_letter'] = df.Cabin.str.extract('([A-Z])') #Takes the letter of the Cabin\n        if ticket_num:\n            df['Ticket_num'] = df.Ticket.str.extract('([0-9]+)') #Takes the number on the ticket\n        if age:\n            df['Age_strat'] = pd.cut(df['Age'],\n                                     bins=[\n                                             0, 11, \n                                             18, 22,\n                                             27, 33,\n                                             40, 66,\n                                             np.inf\n                                                     ],\n                                     \n                                     labels=[i for i in range(1,9)]\n                                    )\n            \n        return df\n    \n# ATTRIBUTES\n\n# Numeric\nnum_attr = ['Fare']\n\n# Categorical Alphabetic\ncat_attr = ['Embarked', 'Title',\n            'Cabin_letter','Sex']\n\n# Categorical Numeric\nord_attr = ['Pclass','Company','Age_strat']\n\n\n#PIPELINE TO CLEAN AND TRANSFORM THE DATA \n\n# For numeric attributes\nnum_pipeline = Pipeline(\n    [\n        ('imputer', SimpleImputer(strategy='median')),\n        ('StdScaler', StandardScaler())\n    ]\n)\n\n# For categorical alphabetic attributes\ncat_pipeline = Pipeline(\n    [\n        ('imputer', SimpleImputer(strategy='constant', fill_value = 'Z')),\n        ('OneVeryHot', OneHotEncoder())\n    ]\n)\n\n# For categorical alphabetic numeric\nord_pipeline = Pipeline(\n    [\n        ('imputer', SimpleImputer(strategy='constant',\n                                  fill_value = 0)),\n        ('OneVeryHot', OneHotEncoder())\n    ]    \n)\n\n\n# MERGE THE THREE PIPELINES IN ONE\n\nfull_pipeline = ColumnTransformer(\n    [('num', num_pipeline, num_attr),\n    ('cat', cat_pipeline, cat_attr),\n    ('ord', ord_pipeline, ord_attr)]\n)\n\n# Instanciateing\nattr_mod = Modifier()\nmod_df = attr_mod.transform(ticket_num=False) #ticket_num set to false since it is probably not useful for the algorithm\n\n# Our dataframe transformed and output as a spare matrix --- READY TO BE TO PASS IT TO THE ALGORITHM \nX = full_pipeline.fit_transform(mod_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into training data and test data \n\nX, X_test = X[:891], X[891:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Machine Learning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to nicely print cross val results\ndef cross_val_results(cross_val_score):\n    print('---------CROSS VALIDATION - THREE-FOLD---------\\n')\n    print('Scores: \\t', [round(i,3) for i in cross_val_score])\n    print('Mean:   \\t', round(cross_val_score.mean(),2))\n    print('Std dev:\\t', round(cross_val_score.std(),6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOGISTIC REGRESSION CLASSIFIER"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# List of attributes\nlist_of_attr = [\n                    'Fare','Embarked_S','Embarked_C',\n                    'Embarked_Q','Embarked_Nan' ,'Mr',\n                    'Mrs','Miss','Master','Unusual',\n                    'Cabin_NaN','Cabin_C','Cabin_E', \n                    'Cabin_G','Cabin_D','Cabin_A',\n                    'Cabin_B','Cabin_F','Cabin_T',\n                    'male','female','Class_3',\n                    'Class_1','Class_2','Company_1',\n                    'Company_0','Company_4','Company_2',\n                    'Company_6','Company_5','Company_3',\n                    'Company_7','Company_10','Age_3',\n                    'Age_6', 'Age_4', 'Age_NaN','Age_7',\n                    'Age_1','Age_2','Age_5','Age_8'\n                                                   ]\n\n# Create a function that returns a data frame of which its columns are: attributes, coeficients and Odds Ratio\ndef coef(model, list_of_attr):\n    list_of_coef = list(model.coef_[0,:])\n    \n    coef_df = pd.DataFrame(\n            {\n            'Attributes':list_of_attr, \n            'Coefficients': list_of_coef, \n            'Odds_Ratio': [np.exp(i) for i in list_of_coef]\n            }\n        )\n    \n    return coef_df\n\nlog_reg = LogisticRegression()\nlog_reg_cross_val = cross_val_score(\n                                log_reg, \n                                X,\n                                np.ravel(y_train),\n                                cv=3\n                                    )\n\n# Print cross val results\ncross_val_results(log_reg_cross_val)\n\nlog_reg.fit(X, np.ravel(y_train))\n\ncoef_df = coef(log_reg, list_of_attr)\n\n# Print positive coefficients, in descending order\ncoef_df.loc[coef_df['Coefficients'] < 0].sort_values(\n                                                    ascending=False,\n                                                    by='Coefficients'\n                                                            ).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE CLASSIFIER"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(max_depth=4, max_leaf_nodes=7)\ntree_clf.fit(X, np.ravel(y_train))\n\ncross_val_results(\n    cross_val_score(\n        tree_clf,\n        X,\n        np.ravel(y_train),\n        cv=3\n    )\n)\n\n# Extract decision tree as .dot file\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(\n    tree_clf,\n    out_file='/kaggle/working/tree_clf.dot',\n    feature_names=list_of_attr,\n    class_names= ['Died','Survived'],\n    rounded=True,\n    filled=True\n)\n\n# Convert the decision tree .dot file into an easy-to-read image .png\n\nimport pydot\n\n(graph,) = pydot.graph_from_dot_file('tree_clf.dot')\ngraph.write_png('tree_clf.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Titanic Decision Tree**\n![sklearn.tree.export_graphviz](tree_clf.png)"},{"metadata":{},"cell_type":"markdown","source":"# Ensamble Methods\n\n# SOFT VOTING CLASSIFIER - Logit, RandomForest, SVC, SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom  sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\n\n# Instanciateing\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier(n_estimators=300,\n                                 max_leaf_nodes=15,\n                                 n_jobs=-1)\nsvm_clf = SVC(probability=True)\n\nvoting_clf = VotingClassifier(\n                    estimators=[\n                                    ('lr',log_clf),\n                                    ('rf',rnd_clf),\n                                    ('svc',svm_clf)                         \n                                ],\n                    voting='soft'\n)\n\ncross_val_results(\n    cross_val_score(\n                    voting_clf,\n                    X,\n                    np.ravel(y_train),\n                    cv=3)\n                            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM FOREST CLASSIFIER"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {\n     'n_estimators': [200, 300, 500],\n     'max_leaf_nodes': [15,17,23]\n    }\n]\n\nrnd_clf = RandomForestClassifier(n_jobs=-1, oob_score=True) #All available CPU's\n\n\ngrid_search = GridSearchCV(rnd_clf, param_grid, cv=3, return_train_score=True)\n\ngrid_search.fit(X, np.ravel(y_train))\ngrid_search.best_params_\n\ncvres = grid_search.cv_results_\n\nfor scores, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(scores, params)\n\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_2 = RandomForestClassifier(n_estimators=300, max_leaf_nodes=15, n_jobs=-1) \n\ncross_val_results(\n    cross_val_score(\n                    rfc_2,\n                    X,\n                    np.ravel(y_train),\n                    cv=3)\n                            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXTREMELY RANDOMIZED TREES"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\netc_clf = ExtraTreesClassifier(n_jobs=-1, oob_score=True) #All available CPU's\n\n\ngrid_search_2 = GridSearchCV(\n                             etc_clf,\n                             param_grid,\n                             cv=3,\n                             return_train_score=True\n                                )\n\ngrid_search.fit(X, np.ravel(y_train))\n\ncvres = grid_search.cv_results_\n\nfor scores, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(scores, params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TESTING MODEL'S ACCURRACY ON TEST SET"},{"metadata":{"trusted":true},"cell_type":"code","source":"# RANDOM FOREST CLASSIFIER\n\nrandom_forest = RandomForestClassifier(\n                                        n_estimators=300,\n                                        max_leaf_nodes=15,\n                                        n_jobs=-1\n                                                    )\nrandom_forest.fit(X, np.ravel(y_train))\ny_pred = random_forest.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_pred, y_test['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\nprecision = precision_score(y_pred, y_test['Survived'])\nrecall = recall_score(y_pred, y_test['Survived'])\n\nprint('Precision: {}\\nRecall: {}'.format(round(precision,2),\n                                         round(recall,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EXTREMELY RANDOMIZED TREES \n\netc = ExtraTreesClassifier(max_leaf_nodes=17, n_estimators=200, n_jobs=-1)\netc = etc.fit(X, np.ravel(y_train))\nex_tr_cl_pred = etc.predict(X_test)\n\naccuracy_score(ex_tr_cl_pred, y_test['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SOFT VOTING CLASSIFIER \n\nvoting_clf.fit(X, np.ravel(y_train))\np = voting_clf.predict(X_test)\n\naccuracy_score(p, y_test['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BEST MODEL (RandomForestClassifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SUBMISSION\n\nsubmission = pd.DataFrame(\n                    {\n                        'PassengerId': y_test['PassengerId'],\n                        \"Survived\": y_pred\n                    }\n                            )\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n* [Predicting the Survival of Titanic Passengers, by Niklas Donges](http://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8)\n* [Titanic Data Science Solutions, by Manav Sehgal](https://www.kaggle.com/startupsci/titanic-data-science-solutions)\n* [Hands-on machine learning with scikit-learn keras and tensorflow, by Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}