{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Unsupervised Learning"},{"metadata":{},"cell_type":"markdown","source":"## Data Description\nThe data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. \nThis particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars."},{"metadata":{},"cell_type":"markdown","source":"## Domain\nObject Recognition"},{"metadata":{},"cell_type":"markdown","source":"## Context\nThe purpose is to classify a given silhouette as one of three types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles."},{"metadata":{},"cell_type":"markdown","source":"#### Import neccessary libaries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Import dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.read_csv('../input/vehicle2/vehicle-2.csv')\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We have **846 rows** and <font color='red'>**19 columns**</font> including target column(*class*)."},{"metadata":{},"cell_type":"markdown","source":"#### Lets look at the sample of the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All the attibutes seems to be quantitative in nature only the target varibale(class) is categorical in nature."},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info() # Check for the info, to get overall understanding what are the datatypesof featured and presence of null values.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can clearly see that there are null values in cicularity and other features also.\n- class has dtype as object, which genrally represent string values."},{"metadata":{},"cell_type":"markdown","source":"#### Description of features of data"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_info=pd.read_csv('../input/attributes-vehicle-silhouettecsv/attributes_vehicle_silhouette.csv')\ndata_info['name']=data.columns\ndata_info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PreProcessing"},{"metadata":{},"cell_type":"markdown","source":"#### Checking and removing NAN values"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Checking if null values are present\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Filling the NAN values\n# Fill the values by median of the data.\nfor col in data.columns[0:-1]:\n    data[col].fillna(value=data[col].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We do know that we have a categorical target variable, we need to convert it into numeric value.\n- We have different ways to do that, one-hot encoding, label enconding and few others.\n    - Let go with label enconding, as it doesn't increase the dataset size and we already have 19 columns."},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert target variable to label encoding\nlabel_encoder=LabelEncoder()\ndata['class'] = label_encoder.fit_transform(data['class'])\ndata['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 0 is Car\n- 1 is Bus\n- 2 is Van"},{"metadata":{},"cell_type":"markdown","source":"Let check head of the data with null value removed and target variable as numeric values."},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understanding the features"},{"metadata":{"trusted":false},"cell_type":"code","source":"data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- scaled_variance.1 has high stdandard deviation **(176)**.\n- We can explore all other variables properties later on."},{"metadata":{},"cell_type":"markdown","source":"### Correlation Analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"corr = data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 3.5})\nplt.figure(figsize=(18,7))\n\n#create a mask so we only see the correlation values onces\n#making a array of shape corr with all values as 0\nmask = np.zeros_like(corr)\n\n# marking the all elements from one diagonal above the main diagonal as True\nmask[np.triu_indices_from(mask,1)] = True\n\n#mask of sns.heatmap\n#If passed, data will not be shown in cells where ``mask`` is True.\n#Cells with missing values are automatically masked.\na= sns.heatmap(corr, mask=mask, annot=True, fmt='.2f')\nrotx= a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty= a.set_yticklabels(a.get_yticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Elongatedness is highly correleated to compactness, cicularity, distance_circularity, radius_ratio, scatter_ratio, px.axis_rectangularity, max.length_rectuangularity, scaled_variance, scaled_variance.1, scaled_radius_of_gyration\n2. Hollows_ratio is highly correlated to skweness_about_2.\n3. Target variable doesn't have high correlation to any other variable."},{"metadata":{},"cell_type":"markdown","source":"#### Features to be explored more\n1. **Elongatedness**: This feature is highly correlated to at least 10 other features, that means the variance explained by all these other features is covered by this single feature. It is highly correlated to target variable in respect to all others.\n2. **Hollows Ratio**\n3. **pr.axis_aspect_ratio**\n4. **max.length_aspect_ratio**\n5. **scaled_radius_of_gyration.1**\n6. **skewness_about**\n7. **skewness_about.1**"},{"metadata":{"trusted":false},"cell_type":"code","source":"features= ['pr.axis_aspect_ratio', 'max.length_aspect_ratio', 'elongatedness',\n       'scaled_radius_of_gyration.1', 'skewness_about', 'skewness_about.1',\n       'hollows_ratio']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#helper function\ndef i_j_counter(rows,columns):\n    i=0\n    j=0\n    indexes=[]\n    while(1>0):\n        indexes.append([i,j])\n        if((j+1)%n_columns==0):\n            j=0\n            i=i+1\n            if(i%n_rows == 0):\n                break;\n        else:\n            j=j+1\n    return indexes;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Detecting Outliers"},{"metadata":{"trusted":false},"cell_type":"code","source":"# using a box plot to see the presence of outliers.\n# are only seeing the box plot of the features we think is important to us.\nplt.figure(figsize=(18,7))\nsns.boxplot(data=data[features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Even though the data is not scaled, we can clearly see the there are outlier to columns which we are concern with. We need to clean these.**"},{"metadata":{},"cell_type":"markdown","source":"### Cleaning outliers"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Assigning outliers to there wiskers postion\nfor col in data.columns:\n    Q1=data[col].quantile(0.25)\n    Q3=data[col].quantile(0.75)\n    IQR = Q3-Q1\n    IQR\n    c1=Q1-(1.5*IQR)\n    c2=Q3+(1.5*IQR)\n    data.loc[data[col] < c1, col] = c1\n    data.loc[data[col] > c2, col] = c2","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Distribution plots help us to understand the distribution\nskew = pd.DataFrame(data[features].skew(),columns=['value'])\nn_rows=4\nn_columns=2\nfig, axes = plt.subplots(n_rows,n_columns,figsize=(20,15))\nfor col,index,skew in zip(features,i_j_counter(n_rows,n_columns),skew['value']):\n    sns.distplot(data[col],ax=axes[index[0],index[1]],color='c', label=f'skew : {skew : .2f}')\n    axes[index[0],index[1]].legend(loc ='upper right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **pr.axis_aspect_ratio** : Normally distributed graph with postive skewness **(0.26)**.\n2. **max.length_aspect_ratio**: Bi-modal values distribution with postive skewness **(0.28)**.\n3. **elongatedness** : Clear bi-modal value peaks at 32 and 45 approx with postive skewness **(0.05)**.\n4. **scaled_radius_of_gyration.1**: Mostly normally distributed with a slight high on right hand side with postive skewnesss **(0.56)**.\n5. **skewness_about** : Bi-modal values with psotive skewness **(0.71)**. \n6. **skewness_about.1** : Normally distributed with postive skewness **(0.69)**.\n7. **Hollows Ratio** : Bi-modal values with negativeskewness **(-0.23)**."},{"metadata":{},"cell_type":"markdown","source":"### Groupby Analysis on orginal given groups"},{"metadata":{},"cell_type":"markdown","source":"We have four groups\n- Cars (two groups further inside)\n- Bus\n- Vans"},{"metadata":{"trusted":false},"cell_type":"code","source":"groupby=data.groupby('class')[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"groupby.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"groupby.skew()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"n_rows=2\nn_columns=4\nfig, axes = plt.subplots(n_rows,n_columns,figsize=(20,10))\nfor col,index in zip(features,i_j_counter(n_rows,n_columns)):\n    sns.kdeplot(data[data['class']==0][col],ax=axes[index[0],index[1]],legend=False)\n    sns.kdeplot(data[data['class']==1][col],ax=axes[index[0],index[1]],legend=False)\n    sns.kdeplot(data[data['class']==2][col],ax=axes[index[0],index[1]],legend=False)\n    axes[index[0],index[1]].set_xlabel(f'{col}')\nprint('Blue is Cars');\nprint('Orange is Bus');\nprint('Green is Van');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **pr.axis_aspect_ratio** - buses have a normal distribution with a mena near 60, cars and van show bi-modal values.\n2. **max.length_aspect_ratio** - multi modal value for cars might be due very types are in data, buses and vans have almost same variance.\n3. **elongatedness** - buses have a lower value but cars and vans share same peak value around 45.\n4. **scaled_radius_of_gyration.1** - highest count hold by buses cars still shows that two cluster should be there.\n5. **skewness_about** - cars have highest count around value of 5, all other vechiles also share there peak near same point.\n6. **skewness_about.1** - buses have bi-modal values, car and vans share similar distribution\n7. **hollows_ratio** - car have two peaks near 185 and 205, buses and vans have similar value where occurs near 185 and 200."},{"metadata":{},"cell_type":"markdown","source":"### Scaling the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"y=data['class']\nX=data.drop(['class'],axis=1)\nscaler=StandardScaler()\nscaled_data=pd.DataFrame(scaler.fit_transform(X),columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(data, diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are many features which shows linear releationship with other features or we can say high collinearity.\n- We have already identified those and store the features which can explain the maximum variance of data.\n- We also see that most of the diagonal have atleast 2 peaks in them and suggesting 2 there are two clusters atleast.\n- we have our target variable with three peaks."},{"metadata":{},"cell_type":"markdown","source":"### Splitting in test and train data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Spliting the whole scaled data with 18 columns into train and test data with test size as 30%.\n# Keep the random state same in the future split same to get exactly same records division.\nX_train,X_val, y_train,y_val = train_test_split(scaled_data,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training a SVM"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc= SVC()\nsvc.fit(X_train,y_train)\npredict= svc.predict(X_val)\nprint(f'Train set acccuracy {svc.score(X_train,y_train) *100 : .4f}%')\nprint(f'Test set accuracy {svc.score(X_val,y_val)*100: .4f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We see very high train and test score with SVM."},{"metadata":{},"cell_type":"markdown","source":"### Cross-Validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold = KFold(n_splits=10, random_state=1, shuffle=True)\nresult = cross_val_score(svc,scaled_data,y,cv=kfold,scoring='accuracy')\nprint(f'Mean KFold accuracy score: {result.mean()*100 : .4f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The KFold on whole data also gives the similar result as the SVC for a particular set of data."},{"metadata":{},"cell_type":"markdown","source":"### PCA [Principal Component Analysis]"},{"metadata":{"trusted":false},"cell_type":"code","source":"# We want PCA to extract feature which can explain 95% of the variance of data.\npca = PCA(n_components=.95)\npca.fit(scaled_data)\nfeature_ratio = pd.DataFrame(pca.explained_variance_ratio_*100, columns=['Variance Explained %(precentage)'])\n#print(pca.explained_variance_ratio_)\nXpca95 = pd.DataFrame(pca.transform(scaled_data))\nfeature_ratio","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(Xpca95, diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train SVM with PCA"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Spliting the data into train and test data with test size=30%\nX_train_pca, X_val_pca, y_train_pca, y_val_pca = train_test_split(Xpca95,y, test_size=0.3, random_state=1)\nsvc_pca=SVC()\nsvc_pca.fit(X_train_pca,y_train_pca)\npredict= svc_pca.predict(X_val_pca)\nprint(f'Train set acccuracy {svc_pca.score(X_train_pca,y_train_pca)*100: .4f}%')\nprint(f'Test set accuracy{svc_pca.score(X_val_pca,y_val_pca)*100: .4f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross-validation with PCA"},{"metadata":{"trusted":false},"cell_type":"code","source":"kfold = KFold(n_splits=10, random_state=1, shuffle=True)\nresult = cross_val_score(svc,Xpca95,y,cv=kfold,scoring='accuracy')\nprint(f'Mean KFold accuracy is: {result.mean()*100: .4f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is loss of approx 4% in accuracy of the data while using the PCA method, but we have reduced the no. of feature to be considered from 19 to 7, with 95% variance explained."},{"metadata":{},"cell_type":"markdown","source":"### Optional Task"},{"metadata":{},"cell_type":"markdown","source":"### KMeans Clustering"},{"metadata":{"trusted":false},"cell_type":"code","source":"# KMeans clustering\n\n\n# Using elbow method to determine the best number of clusters.\n# 1.Select the range in which you want to search for clusters, from the above analysis for orginal data we have 2-3 cluster \n# and in the Xpca95 data we have seen there could be 2 clusters. So, let a take range from 2-5.\nclusters = np.arange(2,6);\nmean_distortions=[]\nfor cluster in clusters:\n    kmeans = KMeans(n_clusters=cluster)\n    kmeans.fit(Xpca95)\n    mean_distortions.append(sum(np.min(cdist(Xpca95,kmeans.cluster_centers_),axis=1))/Xpca95.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(clusters,mean_distortions,'bx-')\nplt.title('Elbow Estimatior')\nplt.xlabel('No. of clusters')\nplt.ylabel('Mean Distortion')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above graph we can cleary see that elbow point exist on **3**. So, we will go with 3 as no. of clusters."},{"metadata":{},"cell_type":"markdown","source":"### KMean clustering with n_clusters=3"},{"metadata":{"trusted":false},"cell_type":"code","source":"kmean_pca95 = KMeans(n_clusters=3)\nkmean_pca95.fit(X_train_pca)\nXpca_train_labels = kmean_pca95.labels_\nXpca_val_labels = kmean_pca95.predict(X_val_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the SVM to evaluate with new labels from clustering"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc_k = SVC()\nsvc_k.fit(X_train_pca, Xpca_train_labels)\npredict = svc_k.predict(X_val_pca)\ntrain_score= svc_k.score(X_train_pca, Xpca_train_labels)\ntest_score = svc_k.score(X_val_pca, Xpca_val_labels)\nprint(f'Traing score is : {train_score*100: .4f}% Test score is: {test_score*100: .4f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The score has has a signinficant improvement against the orignial data labels.\n- It also implies that cluster formed by KMeans algorithm are pretty good, but there could be a difference between what they contain from the real world, the real world cluster might share some overlapping, but that might be less while identifying via algorthim as it only looks at the very specific features given to us. Instead classification might required more than that, in real life Maruti Omni could be car for someone and van some other or can be overlapping even in the features given."},{"metadata":{},"cell_type":"markdown","source":"## The END"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}