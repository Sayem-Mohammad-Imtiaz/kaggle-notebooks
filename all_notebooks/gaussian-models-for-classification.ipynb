{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Gaussian Models for Classification\n### From Naive Bayes to Gaussian Mixture Model\n\nContents:\n- Maximum Likelihood and Maximum a posterior classifiers\n- Gaussian Naive Bayes\n- Linear Discriminant Analysis\n- Quadratic Discriminant Analysis\n- Gaussian Mixture Model\n- How to improve the model?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset\nIn this notebook I wanna use MAGIC Gamma Telescope Dataset. It's a Binary classification problem that has 10 real valued features. we want to classify every item as Gamma(signal) or Hadron.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Reading the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [ 'fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist' ]\nraw_data = pd.read_csv('../input/magic-gamma-telescope-dataset/telescope_data.csv', names=features + ['class'], skiprows=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode class labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(raw_data['class'])\nraw_data['class'] = le.transform(raw_data['class'])\nX = raw_data[features].values\ny = raw_data['class'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking class imbalance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['class'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see dataset is not balanced, hence for evaluating the model in addition to accuracy we'll also check the f1-score","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Create the train and test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is better to write a function for easier evaluation of the classifiers. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_clf(X_train, X_test, y_train, y_test, clf, name=\"Classifier\"):\n  from sklearn.metrics import f1_score, accuracy_score\n  # fit the classifier\n  clf.fit(X_train, y_train)\n  pred = clf.predict(X_test)\n  # evaluate prediction using acc and f1 score\n  score_f1 = f1_score(y_test, pred)\n  score_acc = accuracy_score(y_test, pred)\n  print('{} acc-score: {}'.format(name, score_acc))\n  print('{} f1-score: {}'.format(name, score_f1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define The Enemy!\nDecision Tree is an Excellent classifier. First we'll evaluate it's performance then try to beat it by using Gaussian models.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nevaluate_clf(X_train, X_test, y_train, y_test, DecisionTreeClassifier(), \"Decision Tree\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Maximum Likelihood and Maximum a Posterior Classifiers\nIf you can find a proper probability distribution for every class, then you can calculate the likelihood of a new data item and find the class for which the likelihood is maximum. Sometimes you have a prior knowledge about you classes and you can encode it as prior distributions. In this notebook we use Multivariate Normal or Gaussian Distribution for modeling density of classes. We will start by a simple model later we'll improve it until out performing the Decision Tree classifier.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes Classifier\nLet's start by a simple Gaussian model. In this model we assume that the features are independent. That means a diagonal covariance matrix for each Gaussian distribution. Because of the assumption of feature inedpendence it's called Naive. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"Image('../input/gaussiannotebookimg/2.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nevaluate_clf(X_train, X_test, y_train, y_test, GaussianNB(), \"Gaussian NB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, in comparison to Decision Tree result it's disappointing. But we can improve it. Before that, we must know what's the problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Why Gaussian Naive Bayes doesn't perform well?\nIn this case that's mainly because of the assumption that indicates features are independent.\nFrom the geometrical point of view it means the elipsoid of Normal ditribution can not rotate and only can be scaled along it's axes.\nTo understand better, let's check the correlation of features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data[features].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some features are highly correlated. for example see the scatter plot for **fSize**\nand **fConc**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data[features].plot.scatter('fSize', 'fConc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How to fix the feature correlation problem?\nIn Naive Bayes model, Covariance Matrix of Normal ditributions of classes was diagonal. Instead, we can use a full Covariance Matrix, But for now let's use a unique Covariance Matrix for each class. Indeed we assume that the correlation of features for every class is same. Such a model is called *LDA* or *Linear Discriminant Analysis*.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linear Discriminant Analysis (LDA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/gaussiannotebookimg/3.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Performing LDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nevaluate_clf(X_train, X_test, y_train, y_test, LinearDiscriminantAnalysis(), \"LDA\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better than Naive Bayes but still worse than Decision Tree.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Why not to use different Covariance Matrices?\nWe can do so, However if we have many classes, it will increase the number of parameters significantly and can lead to overfittig. This method is called *Quadratic Discriminant Analysis*  or *QDA*.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/gaussiannotebookimg/4.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### QDA Result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nevaluate_clf(X_train, X_test, y_train, y_test, QuadraticDiscriminantAnalysis(), \"QDA\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Why LDA result was better?\nIt seems for this dataset, increaing flexibility of Normal distribution doesn't have a big impact on performance and instead caused overfitting. Unfortunately Decision Tree is still better. let's find another way.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### So, Now how to improve the model?\nBy far, for every class we used one Normal Distribution. If that's not enough let's use more of them. We can use a convex combination of them! this is called Gaussian Mixture Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/gaussiannotebookimg/5.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Expectation Maximization Algorithm\nEstimating parameters of a single Gaussian distribution is trivial and includes only calculation of mean and covariance.However parameter estimation for a Gaussian Mixture model is not so trivial. Also optimizing parameters of these models for minimizing the negative log likelihood is not easy for gradient-based optimizers. that's because the Covariance Matrix should be Positive-definite which is not an easy to handle constraint for many optimizers [2]. Instead we can use the Expectation Maximization Algorithm. Usually used for estimating parameters of Graphical models who use latent variables (in our case coefficients of gaussian distributions). Fortunately EM algorithm for Gaussian Mixture model has been implemented in Scikit-learn, hence there is no need to implement it manually, However implementing EM is not hard at all. for details of EM see the reference [1]. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating a Scikit-Learn classifier based on Gaussian Mixture \nNow we use scikit implementation of EM Algorithm for Gaussian Mixture models to create a custom estimator for classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.mixture import GaussianMixture\n\nclass GaussianMixtureClassifier(BaseEstimator):\n  \n  def __init__(self, n_components=1):\n    self.n_components = n_components\n\n  def fit(self, X, y):\n    # find number of classes\n    self.n_classes = int(y.max() + 1)\n    # create a GM for each class\n    self.gm_densities = [GaussianMixture(self.n_components, covariance_type='full') for _ in range(self.n_classes)]\n    # fit the Mixture densities for each class\n    for c in range(self.n_classes):\n      # find the correspond items\n      temp = X[np.where(y == c)]\n      # estimate density parameters using EM\n      self.gm_densities[c].fit(temp)\n\n  def predict(self, X):\n    # calculate log likelihood for each class\n    log_likelihoods = np.hstack([ self.gm_densities[c].score_samples(X).reshape((-1, 1)) for c in range(self.n_classes) ])\n    # return the class whose density maximizes the log likelihoods\n    log_likelihoods = log_likelihoods.argmax(axis=1)\n    return log_likelihoods","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we create a Gaussian Mixture Classifier with a mixture of 2 Gaussian distributions per class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_clf(X_train, X_test, y_train, y_test, GaussianMixtureClassifier(n_components=2), \"Gaussian Mixture\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finally we beated the Decision Tree!!!\nNow you can see the mixture model outperforms the Decision Tree.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## How to improve the result further?\n- Encode your domain knowledge about the problem as prior distributions.\n- Try optimizing hyperparameters of the model, number of Gaussians for each class, Covariance Matrix type for each class.\n- Everything is not Gaussian. again use your domain knowledge and find proper distribution for each feature.\n- Do a little feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## References\nThese resources help me a lot to write this notebook.  \n- [1] Machine Learning: A Probabilistic Perspective, Kevin P. Murphy  \nSee Chapter 4 for Gaussian Models and Chapter 11 for Mixture Models and EM Algorithm  \n- [2] Coursera, Advanced Machine Learning Specialization, Bayesian Methods for Machine Learning Course","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will be happy to know your comments :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}