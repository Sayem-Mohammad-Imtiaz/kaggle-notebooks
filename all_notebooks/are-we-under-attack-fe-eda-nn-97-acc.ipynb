{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this kernel, we will be looking at the NSL-KDD training and testing sets which provide significant information on internet traffic record data. These data sets contain the records of the internet traffic seen by a simple intrusion detection network and is considered to be a benchmark for modern-day internet traffic.\n\nLet's first take a look at what is in these datasets."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"init_train_df = pd.read_csv('../input/nslkdd/kdd_train.csv')\ninit_test_df = pd.read_csv('../input/nslkdd/kdd_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that in both, training and testing, sets there are 42 features per record, with 41 of the features referring to the traffic input itself and one label feature referring  to what type of activity the record is.\n\nWe also see that there are appear two be some distinct \"groups\" of features and give us some hints as how we could manipulate our data for model building; Specifically:\n* Categorical Features: Some features have named values identifying someting in the feature, such as *protocol_type* which tells us protocol is happening in the oberservation or *flag* which identifies what flag occurs during this record. For these features, we will most likely need to one-hot encode them.\n* Numeric Count Features: Features like *duration*, *src_bytes*, *dst_bytes*, etc seem to be integer counts of what they track, respectively. So far, we're not sure of the true scope of them, but they could have a high spanning distribution if we take *dst_bytes* as an example; From the head of the feature, we see it can be anywhere from 0 to 8153 or beyond. We will most likely have to normalize these features somehow.\n* Numeric Rate Features: Features with *_rate* as a tail seem to be float values ranging from 0 to 1.0, obviously signifying the rate of something they respectively represent. As these features are in a *benchmark* dataset, we will assume that these will not need much, if any, touch-up.\n\nLet's confirm or refute our initial observations by doing some exploratory data analysis."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n## Dataset Quality\n\nFirst things first, let's see if there are any null or missing values in our datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"init_train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems we're all good with the training set!"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...And we're good with the testing set as well!"},{"metadata":{},"cell_type":"markdown","source":"## Types of Features and Scope"},{"metadata":{},"cell_type":"markdown","source":"To verify our ideas on the types of feature groups we have, we look at the summary of our dataframes and number of unique values in their features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"init_train_df.info()\ninit_train_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_test_df.info()\ninit_test_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that our initial impressions had some truth behind them!\n\nBoth sets possess only four object/categorical features all with relatively reasonable amounts of levels. Specifically, these features are *duration*, *protocol_type*, *service* and *labels.* We also see two concerns: \n* There is a some degree of mismatch in the levels of the *service* and *labels* features between our testing set. We will need to reconcile these disparities somehow, especially if we do one-hot encoding, to maintain dimensional fidelity during model training and predicting.\n* The *service* feature has up to 70 unique values. This level of cardinality may be problematic, but we will choose whether to minimize it or not depending on how well our model performs.\n\nBoth sets contain certain integer features with large ranges. Specifically, these features are:\n* *duration*\n* *src_bytes*\n* *dst_bytes*\n* *hot*\n* *num_compromised*\n* *num_root*\n* *count*\n* *srv_count*\n* *dst_host_count*\n* *dst_host_srv_count*\n\nWe also see we might be right about the *rate* features, as they have up to 101 features (zero to one?). \n\nLet's dig deeper by visualizing our data."},{"metadata":{},"cell_type":"markdown","source":"## Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For our first visualization, we'll graph the counts of each target label in our datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(25, 7.5), dpi=100)\n\nfig.suptitle(f'Counts of Observation Labels', fontsize=25)\n\nsns.countplot(x=\"labels\", \n            palette=\"OrRd_r\", \n            data=init_train_df, \n            order=init_train_df['labels'].value_counts().index,\n            ax=ax1)\n\nax1.set_title('Train Set', fontsize=20)\nax1.set_xlabel('label', fontsize=15)\nax1.set_ylabel('count', fontsize=15)\nax1.tick_params(labelrotation=90)\n\nsns.countplot(x=\"labels\", \n            palette=\"GnBu_r\", \n            data=init_test_df, \n            order=init_test_df['labels'].value_counts().index,\n            ax=ax2)\n\nax2.set_title('Test Set', fontsize=20)\nax2.set_xlabel('label', fontsize=15)\nax2.set_ylabel('count', fontsize=15)\nax2.tick_params(labelrotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Immediately we see that we have a significant skew in our data, which is mostly observations of normal behavior and neptune attacks, which is an attack where the attacker exploits flaws in the three-way-handshake of the TCP protocol and continuously sends a large number of successive spoofed SYN packets.\n\nWhen we do our preprocessing, we will downsample these observations so that we use a number more inline with number of other types of attacks in our dataset.\n\nNow we will visualize our integer type features with large ranges in histograms. To make it easier for us, we write up a simple plot/subplot function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(df, cols, title):\n    grid = gridspec.GridSpec(10, 2, wspace=0.5, hspace=0.5) \n    fig = plt.figure(figsize=(15,25)) \n    \n    for n, col in enumerate(df[cols]):         \n        ax = plt.subplot(grid[n]) \n\n        ax.hist(df[col], bins=20) \n        #ax.set_ylabel('Count', fontsize=12)\n        ax.set_title(f'{col} distribution', fontsize=15) \n    \n    fig.suptitle(title, fontsize=20)\n    grid.tight_layout(fig, rect=[0, 0, 1, 0.97])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_cols = [ 'duration', 'src_bytes', 'dst_bytes', 'hot', 'num_compromised', 'num_root', 'count', 'srv_count', 'dst_host_count', 'dst_host_srv_count']\n    \nplot_hist(init_train_df, hist_cols, 'Distributions of Integer Features in Training Set')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_cols = [ 'duration', 'src_bytes', 'dst_bytes', 'hot', 'num_compromised', 'num_root', 'count', 'srv_count', 'dst_host_count', 'dst_host_srv_count']\n    \nplot_hist(init_test_df, hist_cols, 'Distributions of Integer Features in Testing Set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although these initial plots are a bit hard to read for certain ranges (due to the skewed overall distribution), we clearly see how these features have bimodal (even multimodal) distributions wtih a large range. It's clear that to make these features more manageable, we will need to normalize them.\n\nWe repeat the same type of visualizations for our rate columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"rate_cols = [ 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n\nplot_hist(init_train_df, rate_cols, 'Distributions of Rate Features in Training Set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rate_cols = [ 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n\nplot_hist(init_test_df, rate_cols, 'Distributions of Rate Features in Testing Set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, in all these features, we see the same bimodal distributions we saw in the integer type features. Although, in this case, we did verify that the ranges of our rate features are from 0 to 1."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\nIts obvious that with the current states of our sets, we will not be able to get any meaningful insight from visualizations or other methods until we remove the skew from our data.\n\nSo first we will downsample our *normal* and *neptune* observations in our training set, so that there are only 5000 observations, and in our testing set, so that there are only 1000 observations, of both."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\n \nproc_train_df = init_train_df.copy()                                                                      # create a copy of our initial train set to use as our preproccessed train set.\nproc_test_df = init_test_df.copy()                                                                        # create a copy of our initial test set to use as our preproccessed test set.\n\nproc_train_normal_slice = proc_train_df[proc_train_df['labels']=='normal'].copy()                         # get the slice of our train set with all normal observations\nproc_train_neptune_slice = proc_train_df[proc_train_df['labels']=='neptune'].copy()                       # get the slice of our train set with all neptune observations\n\nproc_test_normal_slice = proc_test_df[proc_test_df['labels']=='normal'].copy()                            # get the slice of our test set with all normal observations\nproc_test_neptune_slice = proc_test_df[proc_test_df['labels']=='neptune'].copy()                          # get the slice of our test set with all neptune observations\n\nproc_train_normal_sampled = proc_train_normal_slice.sample(n=5000, random_state=random_state)             # downsample train set normal slice to 5000 oberservations\nproc_train_neptune_sampled = proc_train_neptune_slice.sample(n=5000, random_state=random_state)           # downsample train set neptune slice to 5000 oberservations\n\nproc_test_normal_sampled = proc_test_normal_slice.sample(n=1000, random_state=random_state)               # downsample test set normal slice to 1000 oberservations\nproc_test_neptune_sampled = proc_test_neptune_slice.sample(n=1000, random_state=random_state)             # downsample test set neptune slice to 5000 oberservations\n\nproc_train_df.drop(proc_train_df.loc[proc_train_df['labels']=='normal'].index, inplace=True)              # drop initial train normal slice\nproc_train_df.drop(proc_train_df.loc[proc_train_df['labels']=='neptune'].index, inplace=True)             # drop initial train neptune slice\n\nproc_test_df.drop(proc_test_df.loc[proc_test_df['labels']=='normal'].index, inplace=True)                 # drop initial test normal slice\nproc_test_df.drop(proc_test_df.loc[proc_test_df['labels']=='neptune'].index, inplace=True)                # drop initial test neptune slice\n\nproc_train_df = pd.concat([proc_train_df, proc_train_normal_sampled, proc_train_neptune_sampled], axis=0) # add sampled train normal and neptune slices back to train set\nproc_test_df = pd.concat([proc_test_df, proc_test_normal_sampled, proc_test_neptune_sampled], axis=0)     # add sampled test normal and neptune slices back to test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(25, 7.5), dpi=100)\n\nfig.suptitle(f'Counts of Observation Labels', fontsize=25)\n\nsns.countplot(x=\"labels\", \n            palette=\"OrRd_r\", \n            data=proc_train_df, \n            order=proc_train_df['labels'].value_counts().index,\n            ax=ax1)\n\nax1.set_title('Train Set', fontsize=20)\nax1.set_xlabel('label', fontsize=15)\nax1.set_ylabel('count', fontsize=15)\nax1.tick_params(labelrotation=90)\n\nsns.countplot(x=\"labels\", \n            palette=\"GnBu_r\", \n            data=proc_test_df, \n            order=proc_test_df['labels'].value_counts().index,\n            ax=ax2)\n\nax2.set_title('Test Set', fontsize=20)\nax2.set_xlabel('label', fontsize=15)\nax2.set_ylabel('count', fontsize=15)\nax2.tick_params(labelrotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking much better!\n\nHowever, we see that some target labels have very few observations and that some types of attacks are only observed in the test set. We will now kill two birds with one stone, by keeping the labels of attacks, with enough observations, as they are and changing the label values of all other attacks to *Other.*\n\nSpecifically, we will hold onto the *normal*, *neptune*, *satan*, *ipsweep*, *portsweep*, *smurf*, *nmap*, *back*, *teardrop*, *warezclient* values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_labels = ['normal', 'neptune', 'satan', 'ipsweep', 'portsweep', 'smurf', 'nmap', 'back', 'teardrop', 'warezclient']\n\nproc_train_df['labels'] = proc_train_df['labels'].apply(lambda x: x if x in keep_labels else 'other')\nproc_test_df['labels'] = proc_test_df['labels'].apply(lambda x: x if x in keep_labels else 'other')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(25, 7.5), dpi=100)\n\nfig.suptitle(f'Counts of Observation Labels', fontsize=25)\n\nsns.countplot(x=\"labels\", \n            palette=\"OrRd_r\", \n            data=proc_train_df, \n            order=proc_train_df['labels'].value_counts().index,\n            ax=ax1)\n\nax1.set_title('Train Set', fontsize=20)\nax1.set_xlabel('label', fontsize=15)\nax1.set_ylabel('count', fontsize=15)\nax1.tick_params(labelrotation=90)\n\nsns.countplot(x=\"labels\", \n            palette=\"GnBu_r\", \n            data=proc_test_df, \n            order=proc_test_df['labels'].value_counts().index,\n            ax=ax2)\n\nax2.set_title('Test Set', fontsize=20)\nax2.set_xlabel('label', fontsize=15)\nax2.set_ylabel('count', fontsize=15)\nax2.tick_params(labelrotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we were successful in our discretization, however, we see that there are significantly more *Other* observations in the test set than the train set. We can solve this easily, however, by taking a sample of the *Other* observations in the test set and transferring them to the train set (specifically 80%)!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nseed_random = 718\n\nproc_test_other_slice = proc_test_df[proc_test_df['labels']=='other'].copy()\n\nproc_train_other_sampled, proc_test_other_sampled = train_test_split(proc_test_other_slice, test_size=0.2, random_state=seed_random)\n\nproc_test_df.drop(proc_test_df.loc[proc_test_df['labels']=='other'].index, inplace=True)\n\nproc_train_df = pd.concat([proc_train_df, proc_train_other_sampled], axis=0)\nproc_test_df = pd.concat([proc_test_df, proc_test_other_sampled], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(25, 7.5), dpi=100)\n\nfig.suptitle(f'Counts of Observation Labels', fontsize=25)\n\nsns.countplot(x=\"labels\", \n            palette=\"OrRd_r\", \n            data=proc_train_df, \n            order=proc_train_df['labels'].value_counts().index,\n            ax=ax1)\n\nax1.set_title('Train Set', fontsize=20)\nax1.set_xlabel('label', fontsize=15)\nax1.set_ylabel('count', fontsize=15)\nax1.tick_params(labelrotation=90)\n\nsns.countplot(x=\"labels\", \n            palette=\"GnBu_r\", \n            data=proc_test_df, \n            order=proc_test_df['labels'].value_counts().index,\n            ax=ax2)\n\nax2.set_title('Test Set', fontsize=20)\nax2.set_xlabel('label', fontsize=15)\nax2.set_ylabel('count', fontsize=15)\nax2.tick_params(labelrotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything looks good!\n\nNow, we will normalize any integer type feature with more than 10 unique values by applying a log transform on them."},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_cols = [ 'duration', 'src_bytes', 'dst_bytes', 'hot', 'num_compromised', 'num_root', 'num_file_creations', 'count', 'srv_count', 'dst_host_count', 'dst_host_srv_count']\n\nfor col in norm_cols:\n    proc_train_df[col] = np.log(proc_train_df[col]+1e-6)\n    proc_test_df[col] = np.log(proc_test_df[col]+1e-6)\n    \nplot_hist(proc_train_df, norm_cols, 'Distributions in Processed Training Set')\nplot_hist(proc_test_df, norm_cols, 'Distributions in Processed Testing Set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome! Although these features still have multimodal distributions, they are now in more manageable ranges! \n\nAs this dataset is a *benchmark* one, we will not do anything with the rate features to avoid any potential information loss from whatever manipulations we make.\n\nNow, we will one-hot encode our object/categorical training features. Although we were able to reconcile our target feature levels mismatch by discretizing them, we will reconcile our training features level mismatch in a more naive way. We will first join our train and test sets, encode the totality of our object/categoical features, and then split the joined dataset back to the individual training and testing sets.\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"proc_train_df['train']=1                                                                       # add train feature with value 1 to our training set\nproc_test_df['train']=0                                                                        # add train feature with value 0 to our testing set\n\njoined_df = pd.concat([proc_train_df, proc_test_df])                                           # join the two sets\n \nprotocol_dummies = pd.get_dummies(joined_df['protocol_type'], prefix='protocol_type')          # get one-hot encoded features for protocol_type feature\nservice_dummies = pd.get_dummies(joined_df['service'], prefix='service')                       # get one-hot encoded features for service feature\nflag_dummies = pd.get_dummies(joined_df['flag'], prefix='flag')                                # get one-hot encoded features for flag feature\n\njoined_df = pd.concat([joined_df, protocol_dummies, service_dummies, flag_dummies], axis=1)    # join one-hot encoded features to joined dataframe\n\nproc_train_df = joined_df[joined_df['train']==1]                                               # split train set from joined, using the train feature\nproc_test_df = joined_df[joined_df['train']==0]                                                # split test set from joined, using the train feature\n\ndrop_cols = ['train', 'protocol_type', 'service', 'flag']                                      # columns to drop\n\nproc_train_df.drop(drop_cols, axis=1, inplace=True)                                            # drop original columns from training set\nproc_test_df.drop(drop_cols, axis=1, inplace=True)                                             # drop original columns from testing set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proc_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proc_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we were successful!\n\n# Model Building\n\nWith a fully preprocessed dataset, we are now able to move onto model building. As we have two sets with 122 features and we want to predict what each observation is out of 11 target values, we think that the best model to take on this challenge is a Neural Network model as deep learning models are good at handling large numbers of parameters to provide generalized results.\n\nFirst, we'll turn our training and testing dataframes into the x_features, y_target format."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_buffer = proc_train_df['labels'].copy()\nx_buffer = proc_train_df.drop(['labels'], axis=1)\n\ny_test = proc_test_df['labels'].copy()\nx_test = proc_test_df.drop(['labels'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also split our training set into a training and validation set, so that we can use both resultant sets during model training to ensure it converges on adequate optima."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nseed_random = 315\n\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y_buffer)\n\nx_train, x_val, y_train, y_val = train_test_split(x_buffer, y_buffer, test_size=0.3, random_state=seed_random)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for our model, we will construct a 5-layer model. I've constructed this model using a rule-of-thumb I've developed from prior exercises, where the first layer has twice as many nodes as the number of features in our sets and every subsequent layer has half as many until we reach the number of output values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.optimizers import Adam, Nadam\nfrom keras.layers import Dense, Dropout\n\ninput_size = len(x_train.columns)\n\ndeep_model = Sequential()\ndeep_model.add(Dense(256, input_dim=input_size, activation='softplus'))\n#deep_model.add(Dropout(0.2))\ndeep_model.add(Dense(128, activation='relu'))\ndeep_model.add(Dense(64, activation='relu'))\ndeep_model.add(Dense(32, activation='relu'))\n#deep_model.add(Dense(18, activation='softplus'))\ndeep_model.add(Dense(11, activation='softmax'))\n\ndeep_model.compile(loss='categorical_crossentropy', \n                   optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=True),\n                   metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a sequential neural network to perform categorical classification, we will also need to one-hot encode our target set. We do so below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_econded = label_encoder.transform(y_train)\ny_val_econded = label_encoder.transform(y_val)\ny_test_econded = label_encoder.transform(y_test)\n\ny_train_dummy = np_utils.to_categorical(y_train_econded)\ny_val_dummy = np_utils.to_categorical(y_val_econded)\ny_test_dummy = np_utils.to_categorical(y_test_econded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can finally train our model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_model.fit(x_train, y_train_dummy, \n               epochs=50, \n               batch_size=2500,\n               validation_data=(x_val, y_val_dummy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've got a good feeling about how well this model will perform when we use it to make predictions! We see how, by epoch 50, it was able to achieve a validation accuracy of 0.9747 and overall accuracy of 0.9649! Not only are those numbers impressive, but they also indicate the model was able to reach an optima!\n\nSo, we make some predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_val_pred = deep_model.predict_classes(x_val)\ndeep_val_pred_decoded = label_encoder.inverse_transform(deep_val_pred)\n\ndeep_test_pred = deep_model.predict_classes(x_test)\ndeep_test_pred_decoded = label_encoder.inverse_transform(deep_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"To help us better understand our model performance, we introduce a function to visualize the confusion matrix between our predictions and our targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer \n\n# Showing Confusion Matrix\n# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    #y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cm(y_val, deep_val_pred_decoded, 'Confusion matrix for predictions on the validation set')\nf1_score(y_val, deep_val_pred_decoded, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cm(y_test, deep_test_pred_decoded, 'Confusion matrix for predictions on the testing set')\nf1_score(y_test, deep_test_pred_decoded, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nWow! We achieved really impressive results with our model predictions on the validation and testing sets! We got F1 Scores of 95.873% and 95.330% respectively! \n\nFor all types of attacks, except for *nmap* and *other* attacks, we got >95% accuracies. For *other* attacks, this is not suprising; That label is a conglomerate of various attacks, some with significantly few observations to adequately train on. If we wish to acheive better results with this label, or even predicting the specific attack, we just need to capture and add more such observations to our training set.\n\nFor the nmap attack, it is probably due to the fact that it has similar enough metrics to other attacks in the dataset, specifically *ipsweep*. This is understandable as both *nmap* and *ipsweep* activities, aim to figure out how many live *IP's* there are.\n\nI hope this kernel helps you better understand the data and gives you some inspiration on how to tackle it yourself!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}