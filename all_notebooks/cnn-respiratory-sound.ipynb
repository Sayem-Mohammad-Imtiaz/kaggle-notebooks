{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Dense, Flatten, Dropout, SeparableConv1D\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"diagnosis_df = pd.read_csv('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv', names=['Patient number', 'Diagnosis'])\ndiagnosis_df.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(diagnosis_df['Diagnosis'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_diagnosis = pd.read_csv('../input/respiratory-sound-database/demographic_info.txt', names = \n                 ['Patient number', 'Age', 'Sex' , 'Adult BMI (kg/m2)', 'Child Weight (kg)' , 'Child Height (cm)'],\n                 delimiter = ' ')\ndf_no_diagnosis.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =  df_no_diagnosis.join(diagnosis_df.set_index('Patient number'), on = 'Patient number', how = 'left')\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root = '../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\nfilenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]\ndef Extract_Annotation_Data(file_name, root):\n    tokens = file_name.split('_')\n    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    return (recording_info, recording_annotations)\ni_list = []\nrec_annotations = []\nrec_annotations_dict = {}\nfor s in filenames:\n    (i,a) = Extract_Annotation_Data(s, root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s] = a\nrecording_info = pd.concat(i_list, axis = 0)\nrecording_info.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Diagnosis():\n    def __init__ (self, id, diagnosis, image_path):\n        self.id = id\n        self.diagnosis = diagnosis \n        self.image_path = image_path   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_wav_files():\n    audio_path = '../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\n    files = [f for f in listdir(audio_path) if isfile(join(audio_path, f))]  #Gets all files in dir\n    wav_files = [f for f in files if f.endswith('.wav')]  # Gets wav files \n    wav_files = sorted(wav_files)\n    return wav_files, audio_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def diagnosis_data():\n    diagnosis = pd.read_csv('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv')\n  \n    wav_files, audio_path = get_wav_files()\n    diag_dict = { 101 : \"URTI\"}  \n    diagnosis_list = []\n  \n    for index , row in diagnosis.iterrows():\n        diag_dict[row[0]] = row[1]     \n\n    c = 0\n    for f in wav_files:\n        diagnosis_list.append(Diagnosis(c, diag_dict[int(f[:3])], audio_path+f))  \n        c+=1  \n\n    return diagnosis_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import librosa\nimport librosa.display\n\ndef audio_features(filename): \n    sound, sample_rate = librosa.load(filename)\n    stft = np.abs(librosa.stft(sound))  \n \n    mfccs = np.mean(librosa.feature.mfcc(y=sound, sr=sample_rate, n_mfcc=40),axis=1)\n    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate),axis=1)\n    mel = np.mean(librosa.feature.melspectrogram(sound, sr=sample_rate),axis=1)\n    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate),axis=1)\n    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(sound), sr=sample_rate),axis=1)\n    \n    concat = np.concatenate((mfccs,chroma,mel,contrast,tonnetz))\n    return concat\n\ndef data_points():\n    labels = []\n    images = []\n\n    to_hot_one = {\"COPD\":0, \"Healthy\":1, \"URTI\":2, \"Bronchiectasis\":3, \"Pneumonia\":4, \"Bronchiolitis\":5, \"Asthma\":6, \"LRTI\":7}\n\n    #count = 0\n    for f in diagnosis_data():\n        #print(count)\n        labels.append(to_hot_one[f.diagnosis]) \n        images.append(audio_features(f.image_path))\n        #count+=1\n\n    return np.array(labels), np.array(images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DataFrame for Diagnosis and audio","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/filename_differences.txt'\n\ndiff = pd.read_csv(path, sep=\" \", header=None, names=['file_names'])\ndiff.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =  diff.join(diagnosis_df,how = 'left')\ndf.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mel spectrograms and MFCCs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = audio_features('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/101_1b1_Al_sc_Meditron.wav')\nS = librosa.feature.melspectrogram(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nplt.subplot(2,2,1)\nlibrosa.display.specshow(librosa.power_to_db(S, ref=np.max))\nplt.title('URTI Mel spectrogram')\nplt.tight_layout()\n\nT = librosa.feature.mfcc(x)\nplt.subplot(2,2,2)\nlibrosa.display.specshow(librosa.power_to_db(T, ref=np.max))\nplt.title('URTI MFCC')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# COPD Mel Spectrograms and MFCCs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = audio_features('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/104_1b1_Al_sc_Litt3200.wav')\nS1 = librosa.feature.melspectrogram(x1)\n\nplt.figure(figsize=(10, 7))\nplt.subplot(2,2,1)\nlibrosa.display.specshow(librosa.power_to_db(S1, ref=np.max))\nplt.title('COPD Mel spectrogram')\nplt.tight_layout()\n\nT1 = librosa.feature.mfcc(x1)\nplt.subplot(2,2,2)\nlibrosa.display.specshow(librosa.power_to_db(T1, ref=np.max))\nplt.title('COPD MFCC')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Healthy Mel Spectrogram and MFCCs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x2 = audio_features('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/102_1b1_Ar_sc_Meditron.wav')\nS2 = librosa.feature.melspectrogram(x2)\n\nplt.figure(figsize=(10, 7))\nplt.subplot(2,2,1)\nlibrosa.display.specshow(librosa.power_to_db(S2, ref=np.max))\nplt.title('Healthy Mel spectrogram')\nplt.tight_layout()\n\nT2 = librosa.feature.mfcc(x2)\nplt.subplot(2,2,2)\nlibrosa.display.specshow(librosa.power_to_db(T2, ref=np.max))\nplt.title('Healthy MFCC')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef preprocessing(labels, images):    \n\n  # Remove Asthma and LRTI\n    images = np.delete(images, np.where((labels == 7) | (labels == 6))[0], axis=0) \n    labels = np.delete(labels, np.where((labels == 7) | (labels == 6))[0], axis=0)      \n\n  # Split data\n    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=10)\n\n  # Hot one encode the labels\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)  \n\n  # Format new data\n    y_train = np.reshape(y_train, (y_train.shape[0], 6))\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    y_test = np.reshape(y_test, (y_test.shape[0], 6))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_train.shape[1],  1))\n\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nfrom tensorflow.keras.utils import plot_model,to_categorical\n\nlabels, images = data_points()\nX_train, X_test, y_train, y_test = preprocessing(labels, images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Depthwise Separable CNN model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=(193, 1)))\n\nmodel.add(Conv1D(128, kernel_size=5, activation='relu'))\nmodel.add(MaxPooling1D(2)) \n\nmodel.add(SeparableConv1D(256, kernel_size=5, activation='relu'))\nmodel.add(MaxPooling1D(2)) \n\nmodel.add(SeparableConv1D(256, kernel_size=5, activation='relu'))\nmodel.add(MaxPooling1D(2)) \n\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation='relu'))   \nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=70, batch_size=200, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_training(history, lw = 3):\n    plt.figure(figsize=(10,6))\n    plt.plot(history.history['accuracy'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_accuracy'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Training Accuracy vs Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend(fontsize = 'x-large')\n    plt.show()\n\n    plt.figure(figsize=(10,6))\n    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Training Loss vs Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(fontsize = 'x-large')\n    plt.show()\nvisualize_training(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nmatrix_index = [\"COPD\", \"Healthy\", \"URTI\", \"Bronchiectasis\", \"Pneumoina\", \"Bronchiolitis\"]\n\npreds = model.predict(X_test)\nclasspreds = np.argmax(preds, axis=1) # predicted classes \ny_testclass = np.argmax(y_test, axis=1) # true classes\n\ncm = confusion_matrix(y_testclass, classpreds)\nprint(classification_report(y_testclass, classpreds, target_names=matrix_index))\n\n# Get percentage value for each element of the matrix\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm / cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n\n# Display confusion matrix \ndf_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nfig, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(df_cm, annot=annot, fmt='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Architecture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}