{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Liberary Import**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets install extra liberary\n! pip install autocorrect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" ---- NLP text cleaning ----\"\"\"\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n# import en_core_web_sm\n# nlp = en_core_web_sm.load()\nimport re\nimport string\nimport unidecode\n# from pycontractions import Contractions\nfrom autocorrect import Speller\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Load DataSet**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"messages=pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding=\"latin-1\")\nmessages.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1, inplace=True)\nmessages.columns = [\"category\", \"text\"]\nprint(messages.shape)\nmessages.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # let's add some columns feature\ndef get_avg_word_len(x):\n    words=x.split()\n    word_len=0\n    for word in words:\n        word_len=word_len+len(word)\n    return word_len/len(words)\n\nmessages['msg_len'] = messages['text'].apply(len)\nmessages['word_count'] = messages['text'].apply(lambda x :  len(x.split())  )\nmessages['avg_word_len'] = messages['text'].apply(lambda x :  get_avg_word_len(x)  )\nmessages['class']=messages['category'].apply(lambda x: 1 if x=='ham' else 0) # let's give ham=1 and spam=0 class\n# # messages['class']=np.where(messages['category']=='spam',0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **BASIC Text Cleaning**\n\n\n1. Remove tags, accented_chars and white space.\n2. Expand Contractions\n3. Make text all lower case\n4. Remove punctuation and numerical values\n5. Remove common non-sensical text (/n)\n6. Tokenize text, Lemmatize / Stemming text\n7. Spell check\n8. Remove stop words","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Reference:\n1. https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae [pre-process ekbana]\n2. https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79 [Pre-Process towardsdatascience]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\"))\nspell_check = Speller(lang='en')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"remove accented characters, to lower case, Remove numerical\"\"\"\ndef text_process_1(text):\n    # remove accented characters from text, e.g. café \n    text = unidecode.unidecode(text)\n    # change to lower case\n    text = text.lower()\n    # remove tags\n    text=re.sub('<[^<]+?>','', text)\n    # Remove numerical like  1996, 6 ,6df\n    text=''.join(c for c in text if not c.isdigit())\n    # return\n    return text\n\n\n\"\"\" token & lemmatizer -- using spaCy liberary \"\"\" \n\"\"\" spacy lematizer  also Expand Contractions words\"\"\"\ndef text_process_2(text):\n    text=nlp(text)\n    text=[token.lemma_ if token.lemma_ != \"-PRON-\" else token.lower_ for token in text ]\n    # return\n    return ' '.join(text) #as we are joining the list value so need a ' ' sinle space between them \n\n\n\"\"\" Remove stopword & punctuation & single character\"\"\"\ndef text_process_3(text):\n    # Check characters to see if they are in punctuation then remove them\n    text=''.join([char for char in text if char not in string.punctuation])\n    # Remove stopword and single character \n    text = [word for word in word_tokenize(text) if word not in stop_words and len(word )>1]\n    # return\n    return ' '.join(text) #as we are joining the list value so need a ' ' sinle space between them \n\n\n\"\"\"autocorrect\"\"\"\ndef text_process_4(text):\n    # spell check autocorrect\n    text=[spell_check(w) for w in text.split() ]\n    # Again\n    # Remove stopword and single character if generated\n    text = [word for word in text if word not in stop_words and len(word )>1]\n    # return\n    return ' '.join(text) #as we are joining the list value so need a ' ' sinle space between them \n\n\n\"\"\"Detect number in word if present and remove Eg: five, three  \"\"\"\n\"\"\" using spacy \"\"\"\ndef text_process_5(text):\n    text = nlp(text)\n    text = [token.text for token in text if token.pos_ != 'NUM'  ]\n    #text = [w2n.word_to_num(token.text) if token.pos_ == 'NUM' else token.text for token in text]\n    return ' '.join(text)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msg=\" ...18u..  âñ don't <h1>HELLO!!</h1> the??/ him he functions fna is a great  going go 66s ain’t wif ac acc early Available otw fiev hundrade \"\nmsg1=text_process_1(msg)\nmsg2=text_process_2(msg1)\nmsg3=text_process_3(msg2)\nmsg4=text_process_4(msg3)\nmsg5=text_process_5(msg4)\n\n\nprint(msg1)\nprint(msg2)\nprint(msg3)\nprint(msg4)\nprint(msg5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Apply Text Process**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_process_1 : remove accented characters, to lower case, Remove numerical\nmessages[\"text\"] = messages[\"text\"].apply(text_process_1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_process_2 : token & lemmatizer & Expand Contractions words\nmessages[\"text\"] = messages[\"text\"].apply(text_process_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_process_3 : Remove stopword & punctuation & single character\nmessages[\"text\"] = messages[\"text\"].apply(text_process_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_process_4 : autocorrect words\nmessages[\"text\"] = messages[\"text\"].apply(text_process_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_process_5 : Detect number in word if present and remove Eg: five, three\nmessages[\"text\"] = messages[\"text\"].apply(text_process_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" let's know how many of the data are spam and ham","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"messages[\"category\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), \n                                     autopct = '%1.1f%%', shadow = True)\nplt.ylabel(\"Spam vs Ham\")\nplt.legend([\"Ham\", \"Spam\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, 86.6% of data are spam and remaining 13.4% are only spam","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see on bar graph\nmessages[\"category\"].value_counts().plot(kind = 'bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"show the msg_len ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(messages['msg_len'], bins=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" let's see the msg length from both spam and ham msg","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (20, 6))\n\nsns.distplot(messages[messages[\"category\"] == \"spam\"][\"msg_len\"], bins = 20, ax = ax[0])\nax[0].set_xlabel(\"Spam Message Word Length\")\n\nsns.distplot(messages[messages[\"category\"] == \"ham\"][\"msg_len\"], bins = 20, ax = ax[1])\nax[1].set_xlabel(\"Ham Message Word Length\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of words counts for each msg","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(messages['word_count'], bins=40)\nplt.title(\"word count on messgaes\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"msg word counts for both spam and ham ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (20, 6))\n\nsns.distplot(messages[messages[\"category\"] == \"spam\"][\"word_count\"], bins = 20, ax = ax[0])\nax[0].set_xlabel(\"Spam Message Word count\")\n\nsns.distplot(messages[messages[\"category\"] == \"ham\"][\"word_count\"], bins = 20, ax = ax[1])\nax[1].set_xlabel(\"Ham Message Word count\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's see the WordCloud diagream**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to draw the wordCloud from the text msg-paragraph \ndef show_word_cloud(Msg):\n  text=' '\n  for words in Msg:\n    text+=\" \"+words\n\n  #word cloud\n  wordcloud = WordCloud(width=600, \n                        height=400,\n                        background_color = 'black'\n                        ).generate(text.lower())\n  plt.figure( figsize=(10,8),\n             facecolor='k')\n  plt.imshow(wordcloud, interpolation = 'bilinear')\n  plt.axis(\"off\")\n  plt.tight_layout(pad=0)\n  plt.show()\n  del text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_word_cloud(messages['text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to return the top 10 common words with freq\ndef feature_bow(msg):\n    cv=CountVectorizer()\n    bow=cv.fit_transform(msg)\n    features_df=pd.DataFrame(bow.toarray(), columns=cv.get_feature_names())\n    words = cv.get_feature_names()\n    feature_df = pd.DataFrame(\n        data =list(zip(words, features_df[words].sum())),\n        columns = ['feature','freq']\n        )\n    #sort the df according to freq\n    feature_df.sort_values(by='freq',ascending=False, inplace=True)\n    feature_df.reset_index(drop=True, inplace=True)\n    # most occuring 10 words\n    return feature_df.head(10)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_freq=feature_bow(messages['text'])\nfeature_freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.barplot(x='feature',y='freq',data=feature_freq)\nplt.title(\"Top 10 feature words and frequency from whole dataset\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Lets study individual Spam/ham words\nspam_messages = messages[messages[\"category\"] == \"spam\"][\"text\"]\nham_messages = messages[messages[\"category\"] == \"ham\"][\"text\"]\nprint(f\"spam len:{len(spam_messages)}\")\nprint(f\"ham len:{len(ham_messages)}\")\nprint(f\"spam+ham: {len(spam_messages)+len(ham_messages)}\")\nprint(f\"total len:{messages.shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's see Spam Messages**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show_word_cloud(spam_messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_freq=feature_bow(spam_messages)\nfeature_freq\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.barplot(x='feature',y='freq',data=feature_freq)\nplt.title(\"Top 10 feature words and frequency from spam_messages\")\nplt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**let's see ham_messages**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show_word_cloud(ham_messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_freq=feature_bow(ham_messages)\nfeature_freq\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.barplot(x='feature',y='freq',data=feature_freq)\nplt.title(\"Top 10 feature words and frequency from ham_messages\")\nplt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, \n1. In spam message the are call, free, txt words present mostly\n2. In ham message there are get, go , come  words present mostly","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##  ---- To be continue ---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}