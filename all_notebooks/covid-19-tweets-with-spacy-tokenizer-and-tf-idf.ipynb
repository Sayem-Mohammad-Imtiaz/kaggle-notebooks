{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KDTree\nfrom sklearn.decomposition import PCA\n\nimport re;\nimport logging;\nimport sqlite3;\nimport time;\nimport sys;\nimport multiprocessing;\nimport matplotlib.pyplot as plt;\n\n\n# Tokenisation & TF-IDF\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"../input/covidvaccine-tweets/covidvaccine.csv\")\n#df = pd.read_csv(\"covidvaccine.csv\")\ndf.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## There is no row with is_retweet = True","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.is_retweet.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"#We create a pandas dataframe as follows:\ndata = pd.DataFrame(data=df.text)\ndata = data.rename(columns={'text' : 'Tweets'})\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We display the first 10 elements of the dataframe:\npd.set_option('max_colwidth',170)\ndisplay(data.head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs=df.text.head(1000).values\ntype(docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs_clean = []\nfor doc in docs:\n    doc_2 = re.sub(r':.*$', \":\", doc)\n    docs_clean.append(doc_2)\n\ndocs_clean[:20]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs2=docs_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove punctuations\npunctuationChars = '!@#$%^&*(){}{}|;:\",./<>?' # you might choose different charcters to drop\nfor i in punctuationChars:\n    docs2 = np.char.replace(docs2, i, ' ')\n# remove apostrophe's (single quotes)\ndocs2 = np.char.replace(docs2,\"'\",' ')\n# remove line feeds\ndocs2 = np.char.replace(docs2,\"\\n\",' ')\n# remove 'http:'\ndocs2 = np.char.replace(docs2,\"https:\",' ')\ndocs2 = np.char.replace(docs2,\"https\",' ')\n\n# make lower case\nfor i,s in enumerate(docs2):\n    docs2[i] = s.lower()\n    \n# Show the cleaned data\n# Show the beginning of each document\n\n#for i in range(len(docs2)):\n#        print(f'\\ndoc{i}: {docs2[i]}') \n\nfor i in range(100):\n       print(f'\\ndoc{i}: {docs2[i]}') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(docs2[0:10])\nfeature_names = vectorizer.get_feature_names()\ndense = vectors.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Spacy Tokenizer","metadata":{}},{"cell_type":"code","source":"def spacy_tokenizer(document):\n    tokens = nlp(document)\n    tokens = [token for token in tokens if (\n        token.is_stop == False and \\\n        token.is_punct == False and \\\n        token.lemma_.strip()!= '')]\n    tokens = [token.lemma_ for token in tokens]\n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data to see what spacy tokenizer can do.\nexample_corpus = [\n    \"Monsters are bad. They likes to eat geese. I saw one goose flying away\", \\\n    \"I saw a monster yesterday. The meaning is so obvious!\", \\\n    \"Why are we talking about bad monsters? They are meanness.\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\n# test\ncorpus=example_corpus\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult_test = tfidf_vector.fit_transform(corpus)\nresult_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Successfully extraxt intended meaning of the words. 14 tokens for the example corpus.","metadata":{}},{"cell_type":"code","source":"dense = result_test.todense()\ndenselist = dense.tolist()\ndf_test = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply Spacy tokenizer, TF-IDF, K-means for our first 1000 tweets.","metadata":{}},{"cell_type":"code","source":"tfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\ncorpus = docs2\n\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult = tfidf_vector.fit_transform(corpus)\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Itâ€™s a sparse matrix with 1000 reviews and 3191 terms, out of those 3191000 possible numbers there are 9169 non-zero TF-IDF values. We can check which terms are actually considered from the sentences with the get_feature_names method:","metadata":{}},{"cell_type":"code","source":"# We can check which terms are actually considered from the sentences with the get_feature_names method:\ntfidf_vector.get_feature_names()[1:500]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sparse matrix format is an efficient way to store this information, but you might want to convert it to a more readable, dense matrix format using the todense method. \nTo create a pandas DataFrame from the results, you can use the following code:","metadata":{}},{"cell_type":"code","source":"dense = result.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see the weights for words contained in the first Tweet.","metadata":{}},{"cell_type":"code","source":"df[[\"australia\", \"manufacture\", \"covid-19\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### check the cosine similarity","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import linear_kernel\ncos_df = pd.DataFrame(columns=df.index)\nfor i in range(999):\n    curr_cos_sim = linear_kernel(result[i:i+1], result).flatten()\n    cos_df[i] = curr_cos_sim\n    \ncos_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the clustering table","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans_models = {}\nfor i in range(2,13+1):\n    current_kmean = KMeans(n_clusters=i).fit(result)\n    kmeans_models[i] = current_kmean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_df = pd.DataFrame()\ncluster_df['Review Texts'] = docs\nfor i in range(2, 13+1):\n    col_name = str(i) +'means_label'\n    cluster_df[col_name] = kmeans_models[i].labels_\ncluster_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Elbow Method to determine the best K","metadata":{}},{"cell_type":"code","source":"Sum_of_squared_distances = []\nK = range(1,18)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(result)\n    Sum_of_squared_distances.append(km.inertia_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choose K = 10 to do the experiment","metadata":{}},{"cell_type":"code","source":"cluster10 = cluster_df.iloc[:,[0,9]]\ncluster10_0 = cluster10.loc[cluster10[\"10means_label\"] == 0]\ncluster10_0.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster10_1 = cluster10.loc[cluster10[\"10means_label\"] == 1]\ncluster10_1.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cluster_2 focus on topics related to Russia Vaccine.","metadata":{}},{"cell_type":"code","source":"cluster10_2 = cluster10.loc[cluster10[\"10means_label\"] == 2]\ncluster10_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CLuster_3 contains more rumors and negtive reactions.","metadata":{}},{"cell_type":"code","source":"cluster10_3 = cluster10.loc[cluster10[\"10means_label\"] == 3]\ncluster10_3.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster10_4 = cluster10.loc[cluster10[\"10means_label\"] == 4]\ncluster10_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster10_5 = cluster10.loc[cluster10[\"10means_label\"] == 5]\ncluster10_5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What's Next?\n### 1. Using KNN or cosine similarity to classify the new tweets\n### 2. Based on the insight we get from the existing clusters, extract the useful Information that related to the topic you are interested.","metadata":{}}]}