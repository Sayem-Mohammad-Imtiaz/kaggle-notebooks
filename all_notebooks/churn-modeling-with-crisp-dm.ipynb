{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kernel's Methodolgy\n\nIn this kernel aims to find most suitable model via CRISP-DM strategy for Bank customer which could churn. CRISP-DM is basically data mining methodology but nowadays it use to data science project. Although different approaches have been developed in the field of data science over the years, at the last point reached, where a data science project can be started, which steps should be followed, the outputs of the phases of the project and the measurable steps during the project can be managed with the method shortened as CRISP-DM.\n\n# What is CRISP-DM\nCRISP-DM (Cross Industry Standard Process for Data Mining) bir veri madenciliği metodolojisidir. Bu yöntemde bir proje altı parçaya bölünerek süreç ilerletilir.\n    1. Business Understanding: This is the understanding of the business and the understanding of the business being processed.\n    2. Data Understanding: It is the phase of having information about the data structure.\n    3. Data Preparation: This is the data preparation phase.\n    4. Modeling: Creating a model with data is the stage.\n    5. Evaluation: This is the evaluation phase of the model.\n    6. Deployment: Application is the phase of action. After the model is created, the application is started by programming.\n    \n## 1. Business Understanding\nBasically, expectation of the bank, which customer could be churn and how modelling data of customer of the bank. In line with this expectation, main objective detects customers that could be leave from there.\n\n## 2. Data Understanding\nFirst of all importing all libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\"\"\"Data Preparation Library\"\"\"\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"Models Library\"\"\"\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\n\"\"\"Model Evaluation\"\"\"\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n\"\"\"Other\"\"\"\nimport os\nimport warnings\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category = ConvergenceWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dataset\ndt = pd.read_csv(\"../input/churn-modelling/Churn_Modelling.csv\")\n\n# First 5 rows of data\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset belongs to the bank that is hidden its name because of data security. The dataset consists of 13 attributes and 10,000 rows. The following shows description of attributes.\n1.\tCustomer ID: This attribute is unique and assume that primary key\n2.\tSurname: it belongs to surname of customer and string values\n3.\tGeography: it shows country of customer \n4.\tGender: male/female \n5.\tCredit Score: it gives credit score of customers. That score calculates interbank system. High score shows that the customer debt high repayment capacity. \n6.\tAge: age of customers\n7.\tTenure: The number of ages the customer is in the bank.\n8.\tBalance: Customer's money in the bank.\n9.\tNumber of Products: Number of products owned by the customer.\n10.\tCredit Card: Whether the customer has a credit card\n11.\tActive Status: Customer's presence in the bank\n12.\tEstimated Salary: Customer's estimated salary\n13.\tExited: Churn or not\n\nRemove unneeded columns which are RowNumber, CustomerId and Surnmae\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = dt.drop(columns=[\"RowNumber\",\"CustomerId\",\"Surname\"])\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is unique 10.000 customers. Geography and gender are categorical variables. Geography consist of France (%50), Germany (%25) and Spain (%25). Also, gender contain %54 male and %46 female.\n\nBasic description for continuous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above table (basic statical description)shows descriptive statistics of continuous variables from original data. Credit score is between 350 and 850. The average age of the customers is 39. Tenure is maximum ten years which assume age of the bank. Minimum value and first quantiles of Balance are equal 0 which means the distribution may not be normal."},{"metadata":{},"cell_type":"markdown","source":"Missing values checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(dt.isnull().sum(),columns=[\"Count\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exited column that is flag to define for customers whether churn (%80) or not (%20). Thus, target feature is determined. Other features allow the predictor to classify the value of the target variable. For this reason, the relationship between the target column and other columns is examined in the following visualizations "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exited -- CreditScore\nsns.violinplot( x=dt[\"Exited\"], y=dt[\"CreditScore\"], linewidth=5)\nplt.title(\"Credit Score Distribution of Churn (Exited)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exited -- Age\nsns.violinplot( x=dt[\"Exited\"], y=dt[\"Age\"], linewidth=5)\nplt.title(\"Age of Customers Distribution of Churn (Exited)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exited -- Tenure\nsns.violinplot( x=dt[\"Exited\"], y=dt[\"Tenure\"], linewidth=5)\nplt.title(\"Tenure of Customers Distribution of Churn (Exited)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exited -- Balance\nsns.violinplot( x=dt[\"Exited\"], y=dt[\"Balance\"], linewidth=5)\nplt.title(\"Balance of Customers Distribution of Churn (Exited)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balance boxplot\ndt[[\"Balance\"]].boxplot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exited -- NumOfProducts\nsns.violinplot( x=dt[\"Exited\"], y=dt[\"NumOfProducts\"], linewidth=5)\nplt.title(\"Number of Products of Customers Distribution of Churn (Exited)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exited -- EstimatedSalary\nsns.violinplot( x=dt[\"Exited\"], y=dt[\"EstimatedSalary\"], linewidth=5)\nplt.title(\"Estimated Salary of Customers Distribution of Churn (Exited)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above figures shows the relationship between six continuous variables and target variable in the form of violin graph. Balance, Tenure, Estimated Salary and Credit score almost appear to be irregular for both churn and not churn. Customers who churn higher age than other. Churn customers When the product numbers are examined; could be interpreted by looking at the graph that customers reduce their products before leaving."},{"metadata":{},"cell_type":"markdown","source":"The following figure in the correlation between the six variables, there is no significant value between any two variables. Only a negative relationship exists between Balance and Number of Product "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Correlation Matrix\ncorrelationColumns = dt[[\"CreditScore\",\"Age\",\"Tenure\"\n    ,\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"]]\n\nsns.set()\ncorr = correlationColumns.corr()\nax = sns.heatmap(corr\n                 ,center=0\n                 ,annot=True\n                 ,linewidths=.2\n                 ,cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. DATA PREPARATION\nSince there is a target variable in data of the Bank, classification is made by following the supervised learning method. First, to define which target variable is the model, the target variable and the other variables are separated from each other (Exited and other). Customer ID, Row Number and Surname variables are excluded from the data set because they cannot be input for the model,"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decomposition predictors and target\npredictors = dt.iloc[:,0:10]\ntarget = dt.iloc[:,10:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The characters in the gender variable are replaced with 0 or 1. "},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    predictors['isMale'] = predictors['Gender'].map({'Male':1, 'Female':0})\nexcept:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dummy variables was reconstructed as 1 or 0 for the three values in the Geography data. Therefore, three different variables were formed. However, the third variable was excluded from the data since two variables included in all three cases. "},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # Geography one shot encoder\n    predictors[['France', 'Germany', 'Spain']] = pd.get_dummies(predictors['Geography'])\n    # Removal of unused columns.\n    predictors = predictors.drop(columns=['Gender','Geography','Spain'])\nexcept:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling preparation applies transformation methodology. Three variables (Credit Score, Estimated Salary and Balance) were transformed by normalizing. All values in the variables are represented between 1 and 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"normalization = lambda x:(x-x.min()) / (x.max()-x.min())\ntransformColumns = predictors[[\"Balance\",\"EstimatedSalary\",\"CreditScore\"]]\npredictors[[\"Balance\",\"EstimatedSalary\",\"CreditScore\"]] = normalization(transformColumns)\n\n# All Predictors Columns\npredictors.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to measure the accuracy rate in the modeling, the data set was divided into test and train."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test splitting\nx_train,x_test,y_train,y_test = train_test_split(predictors,target,test_size=0.25, random_state=0)\npd.DataFrame({\"Train Row Count\":[x_train.shape[0],y_train.shape[0]],\n              \"Test Row Count\":[x_test.shape[0],y_test.shape[0]]},\n             index=[\"X (Predictors)\",\"Y (Target)\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After all these preparations, the dataset is made ready for modeling which is another step of CRISP-DM method."},{"metadata":{},"cell_type":"markdown","source":"## 4. MODELING\nAfter the pre-processing of the data, one or multiple specific modelling techniques, which are connected to the data mining goal, are selected and data could be modelled. In order to test cogency and the quality of the model, a procedure should be created before the model is built.  Afterward, in order to produce one or more models, the modelling tool could start running on the ready set of data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Numpy excaptions handle\ny_train = y_train.values.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train)\ny_pred_dtc = dtc.predict(x_test)\ndtc_acc = accuracy_score(y_test,y_pred_dtc)\n\n# Logistic Regression\nlogr = LogisticRegression()\nlogr.fit(x_train,y_train)\ny_pred_logr = logr.predict(x_test)\nlogr_acc = accuracy_score(y_test,y_pred_logr)\n\n# Naive Bayes\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\ny_pred_gnb = gnb.predict(x_test)\ngnb_acc = accuracy_score(y_test,y_pred_gnb)\n\n# K Neighbors Classifier\nknn = KNeighborsClassifier( metric='minkowski')\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\nknn_acc = accuracy_score(y_test,y_pred_knn)\n\n# Random Forrest\nrfc = RandomForestClassifier()\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)\nrfc_acc = accuracy_score(y_test,y_pred_rfc)\n\n# Neural Network\nnnc = MLPClassifier()\nnnc.fit(x_train,y_train)\ny_pred_nnc = nnc.predict(x_test)\nnnc_acc = accuracy_score(y_test,y_pred_nnc)\n\n# Xgboost Classifier\nxgboast = xgb.XGBClassifier()\nxgboast.fit(x_train, y_train)\nxgboast = xgboast.score(x_test,y_test)\n\npd.DataFrame({\"Algorithms\":[\"Decision Tree\",\"Logistic Regression\",\"Naive Bayes\",\"K Neighbors Classifier\",\"Random Ferest\",\"Neural Network\",\"Xgboost Classifier\"],\n              \"Scores\":[dtc_acc,logr_acc,gnb_acc,knn_acc,rfc_acc,nnc_acc,xgboast]})\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When the accuracy of all models is compared, it is seen that XGBoost algorithm is higher (%86)"},{"metadata":{},"cell_type":"markdown","source":"## 5. EVALUATION\nIn the evaluation stage, the obtained model obtained should evaluated more carefully and the steps while building the model should review in order to be sure that the model appropriately achieves the business objectives."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation test\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('xgboast', XGBClassifier()))\n\n# evaluate each model in turning kfold results\nresults_boxplot = []\nnames = []\nresults_mean = []\nresults_std = []\np,t = predictors.values, target.values.ravel()\nfor name, model in models:\n    cv_results = cross_val_score(model, p,t, cv=10)\n    results_boxplot.append(cv_results)\n    results_mean.append(cv_results.mean())\n    results_std.append(cv_results.std())\n    names.append(name)\npd.DataFrame({\"Algorithm\":names,\n                                \"Accuracy Mean\":results_mean,\n                                \"Accuracy\":results_std})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results_boxplot)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the cross-validation method, different train and test sets were created and the model re-run iteratively and the result was increased from 86% to 87%. The accuracy of different algorithms was compared with the application of cross validation method."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid Seach for XGboast\nparams = {\n        'min_child_weight': [1, 2, 3],\n        'gamma': [1.9, 2, 2.1, 2.2],\n        'subsample': [0.4,0.5,0.6],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3,4,5]\n        }\ngd_sr = GridSearchCV(estimator=XGBClassifier(),\n                     param_grid=params,\n                     scoring='accuracy',\n                     cv=5,\n                     )\ngd_sr.fit(predictors, target.values.ravel())\nbest_parameters = gd_sr.best_params_\npd.DataFrame(best_parameters.values(),best_parameters.keys(),columns=[\"Best Parameters\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score is: \",gd_sr.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. DEPLOYMENT\nThe deployment phase requires the consequences of the evaluation to verify a strategy for deployment within a particular company. When the results of the project will be used widely, it is significant that the business should take required actions to use definitely the models. At this phase, final report and presentation of the found results are produced."},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you saved model, you can use Pickle file.\n# Pickle cound use \"import pickle\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CONCLUSION\nAs a result, using CRISP-DM method, the data set was handled through various processes. This continued at each stage by completing the previous stage. XGBoost has the best accuracy of 87%."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}