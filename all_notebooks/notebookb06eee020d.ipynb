{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Tehran Divar](https://divar.ir/s/tehran/) website data analysis","metadata":{}},{"cell_type":"markdown","source":"# Tehran housing price data analysis  \nI will present an analysis about renting a house price in this notebook. Housing data is generated from [Tehran Divar](https://divar.ir/s/tehran/) website and the dataset contains data about renting house in Theran at the end of 1399. I use different types of regression for evaluating model and comparing the results of those types. \n\nThis dataset is available in [kaggle](https://www.kaggle.com/amiralimadadi/tehran-housing) and you can also check the scrap project for generating this dataset [here](https://github.com/amiralimadadi/Divar_WebScrap).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n!pip install Unidecode\nfrom unidecode import unidecode\n\n!pip install arabic-reshaper\n!pip install python-bidi\nfrom bidi.algorithm import get_display\nfrom arabic_reshaper import reshape","metadata":{"execution":{"iopub.status.busy":"2021-07-11T06:47:12.624941Z","iopub.execute_input":"2021-07-11T06:47:12.62532Z","iopub.status.idle":"2021-07-11T06:50:34.459475Z","shell.execute_reply.started":"2021-07-11T06:47:12.625217Z","shell.execute_reply":"2021-07-11T06:50:34.458298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Why Tehran housing price\nThere are some major reasons for me to do this research:\n1.   **Population**: Tehran is the capital and major city in Iran. It also hosts more than 9 million population in 2021, which makes it the second populated city in the middle east. \n2.   **Housing transactions**: More than 83,000 housing transactions are being made per year in Tehran.\n3.   **Housing price**: Housing price has rappidly grown during the last decade in Tehran (notice the graph bellow).\n3.   **Districts**: Tehran consists of 22 districts and many neighbourhoods, so finding a good and cheap place to live seems difficault.\n\nAs I said, prices are growing up sharply (for both rent and buy a house), there are too many transactions, there are many neighbourhoods in Tehran and many people are requesting to buy or rent a house per month. Therefore, studying this field and making a comprehensive dataset seems necessary for both landloards and tenants.\n\nI selected [Tehran Divar](https://divar.ir/s/tehran/) website as my reference, because it is the greatest website for second hand stuffs deal in Tehran. More than 90 percentages of all housing advertisements are provided in [Tehran Divar](https://divar.ir/s/tehran/).","metadata":{}},{"cell_type":"code","source":"df_bachground = pd.read_csv('TehranHousingPriceBackground.csv', encoding=\"utf-8\")  \n\nplt.figure(figsize= (20, 5))\nsns.pointplot(data=df_bachground, x=\"Month\", y=\"Price\")\nplt.ylabel(get_display(reshape('Price')))\nplt.xlabel(get_display(reshape('Month')))\nplt.title('Average Price Per Meter in Tehran (from 1395 to 1399)')\nplt.xticks(rotation = \"vertical\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data\nData stored in csv file has **12,383** records containing **9** features. Features are:\n1.   **total_value**: is the overall value of the house. It is calculated based on deposit and monthly rent.\n2.   **neighborhood**: is filled based on the house position in Tehran.\n3.   **area**: is the area of house in squared meter.\n4.   **year**: is the year that the house is built.\n5.   **deposit**: is the deposit of house in Iran currency (Tooman).\n6.   **rent**: is the monthly rent of house in Iran currency (Tooman).\n7.   **elavator**: indicates that house has elavator (1 for has and 0 for dose not have).\n8.   **parking**: indicates that house has parking lot (1 for has and 0 for dose not have).\n9.   **warehouse**: indicates that house has  warehouse (1 for has and 0 for dose not have).","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('Data.csv', encoding=\"utf-8\")  \ndf.drop_duplicates(subset =None, keep = 'first', inplace = True)\n\ndf['neighborhood'] = df['neighborhood'].astype(pd.StringDtype())\n\ndf.shape","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data summarization\nAs you will see, the data set has 8 useful and clean features and 1 goal feature which is **total_value**. All columns of dataset has defined types so that we can work with them easily.\n\nAs I said before, **total_value** has been calculated based on **deposit** and **rent**, therefore I need to remove those 2 columns. At last dataset will have **6  features** and **1 goal feature**.\n\nI grouped data by *neighborhood* feature and get a summary for the *total_value* field. You can see the maximum, minimm, median, count and variance of *total_value* in each *neighborhood*. You may also notice, there are **314 unique neighborhoods** in the dataset.","metadata":{}},{"cell_type":"code","source":"# df_main.drop(columns=['rent','deposit'], inplace=True, axis=1)\n\ndf.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('neighborhood')[['total_value']].agg([np.min ,np.max, np.mean, np.var, 'count']).reset_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features correlation and distribution\nTo have a deep view of data, I calculate correlation between features. As you can see in figure, the goal feature (**total_value**) has the highest correlation to area. To omit the affect of neighbourhood on correlation calculation, I calculate the correlation for records of 1 neighbourhood (*سعادت آباد*). You can sense the affect of neighbourhood on **total_value** in the figure too.\n\nTherefore, it seems that **area** and **neighbourhood** are the most important features in the dataset and focusing on them will help us to generate a better predicting model. ","metadata":{}},{"cell_type":"code","source":"fig , axes = plt.subplots(1,2,figsize = (20, 10))\n\ndf_temp = df[(df['neighborhood'].str.contains('سعادت')) & (df['total_value']<5e10) & (df['total_value']!=0)]\ndf_temp = df_temp[['total_value','year','area','elavator','parking']]\ng1 = sns.heatmap(df_temp.corr(),annot=True, annot_kws={\"size\": 13},cbar=False,ax=axes[0])\ng1.set(title='Correlation value of features');\n# plt.setp(axes[0].get_xticklabels(), rotation=90)\n\n\ndf_temp = df.groupby('neighborhood')[['total_value']].mean().reset_index()\ndf_temp = df_temp.sort_values('total_value')\ng2 = sns.lineplot(data=df_temp, x = [get_display(reshape(label)) for label in df_temp['neighborhood']]\n                  , y=\"total_value\",linewidth = 5,ax=axes[1])\n\nfor ind, label in enumerate(g2.get_xticklabels()):\n    if ind % 15 == 0:  # every 10th label is kept\n        label.set_visible(True)\n    else:\n        label.set_visible(False)\ng2.set(title='Average Price in Neighbourhoods in Tehran');\ng2.set(ylabel='');\n# g2.set(xlabel='Neighbourhood')\nplt.setp(axes[1].get_xticklabels(), rotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To test data distribution, I use scatter plot. The main numeric feature of dataset is **area** and the goal feature is **total_value**. So plotting **area** as x vector and **total_value** as y vector, will be useful. The key point is, I should plot data for each neighbourhood separatedly.This will result in correct analysis. I picked the neighbourhood with the most number of samples for plotting and it is *سعادت آباد*.\n\nAs you can see below, there is a relation between **area** and **total_value**. As *area* increases, *total_value* increases too. So it seems that *regression* algorithms may result in good predictions.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize= (18,5))\n\ndf_temp = df[(df['neighborhood'].str.contains('سعادت')) & (df['total_value']<5e10) & (df['total_value']!=0)]\n\nsns.scatterplot(x = \"area\", y = \"total_value\", data = df_temp,)\nplt.xlabel('Area (in Meter)');\nplt.ylabel('Value (in Tooman)');\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing useless records (records with 0 value)\nI found some logically bad records in the dataset. Some bad records and features were removed in preparation step before, but there are some logical bad records in the dataset yet. \n\nFor example, I found some records with the value of 0 for *total_value*. It means that this house has the value of 0!. I guess these records have been inserted into dataset, beacuse of user faults, so  I decide to remove them all. There are **1536** records with this situation in the dataset.","metadata":{}},{"cell_type":"code","source":"# Houses with both rent and deposit equal 0 should be removed\ndf = df[(df['total_value'] > 0) ]\ndf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing useless records (records with very small sample count)\nI also noticed the *neighborhoods* with less than 10 samples. These *neighborhoods* will lead into generating too many columns in dummy transformation. I convert the their *neighborhood* to other *سایر*, to make my model better and more simple.\n\nFinally, dataset contains **10847** acceptable records.","metadata":{}},{"cell_type":"code","source":"# Neibourhoods with more than 10 samples are acceptable\ndf_temp = pd.DataFrame(df['neighborhood'].value_counts(sort = True))\ndf_temp = df_temp[df_temp['neighborhood'] < 10]\n\ndf.loc[(df['neighborhood'].isin(df_temp.index)), 'neighborhood'] = \"سایر\"\n# df = df[df['neighborhood'].isin(df_head_provinces.index)]\n\ndf.reset_index(drop=True)\n\ndf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show top 15 neighbourhoods according to ads count\n\ndf_head_provinces = pd.DataFrame(df['neighborhood'].value_counts(sort = True))\ndf_head_provinces = df_head_provinces[df_head_provinces['neighborhood'] > 10]\n\ndf_top_15 = df_head_provinces.head(15)\nplt.figure(figsize= (20, 5))\nsns.barplot(data = df_top_15, x = [get_display(reshape(label)) for label in df_top_15.index]\n            , y = df_top_15.values.ravel(), palette= sns.color_palette(\"hls\", 18))\n# plt.xlabel(get_display(reshape('محله ')))\n# plt.ylabel(get_display(reshape('تعداد')))\n# plt.title(get_display(reshape('15 محله پر آگهی')))\n\nplt.xlabel('Neighbourhood');\nplt.ylabel('Count');\nplt.title('Top 15 Neighbourhoods (according to ads count)');\n\nplt.xticks(rotation = \"vertical\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing useless records (outliers)\nSome houses have very unusual *total_value* or *area*. I consider them as outliers and remove them all and show them in graph below.","metadata":{}},{"cell_type":"code","source":"fig , axes = plt.subplots(2,1,figsize = (18, 10))\ng1 = sns.scatterplot(x = [get_display(reshape(label)) for label in df['neighborhood']]\n                     , y = \"total_value\", data = df, ax=axes[0])\n# g1.set(title=get_display(reshape('ارزش ملک در محله ها')))\n# g1.set(ylabel=get_display(reshape('ارزش ملک')))\ng1.set(title='Comparision the Value of Houses in Neighbourhoods');\ng1.set(ylabel='Value (in Tooman)');\ng1.set(xlabel=None)\nplt.setp(axes[0].get_xticklabels(), rotation=90)\n\n\ng2 = sns.scatterplot(x = [get_display(reshape(label)) for label in df['neighborhood']]\n                     , y = \"area\", data = df, ax=axes[1])\n# g2.set(title=get_display(reshape('متراژ ملک در محله ها')))\n# g2.set(ylabel=get_display(reshape('متراژ ملک')))\ng2.set(title='Comparision the Area of Houses in Neighbourhoods');\ng2.set(ylabel='Area (in Meter)');\ng2.set(xlabel=None)\nplt.setp(axes[1].get_xticklabels(), rotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you look at the data frame bellow, you will find out that houses with the *total_value* more than 40,000,000,000 are outliers (**1 record**). You may notice the houses with the amount of *area* more than 100,000 squared meter are outliers too (**3 records**).\n\nThese records seem to be unusual due to user fault. For example, there is a house with just *50* squared meter and *50,000,000,000*. This amount is almot twice the maximum of *total_amount* in dataset. Therefore, I can omit them all.","metadata":{}},{"cell_type":"code","source":"df_temp = df[(df['total_value'] > 30000000000) | (df['area'] >= 100000)]\ndf_temp.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing outliers\ndf_temp = df[df['total_value'] > 30000000000]\n# print (df_temp)\nprint('Outliers count the field of total_value:',df_temp['neighborhood'].count())\ndf = df[df['total_value'] < 30000000000]\n\ndf_temp = df[df['area']  >= 100000]\nprint('Outliers count the field of area:',df_temp['neighborhood'].count())\n# print(df_temp)\ndf = df[df['area'] < 100000]\n\nprint('Final valid data count:',df['neighborhood'].count())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final valid data\nAt the end of data purification, dataset will consist of **10843** records. You can see distribution of data in top 15 neighbourhoods (according to sample counts), in the following graph.\n","metadata":{}},{"cell_type":"code","source":"# Data distribution in top 15 neighbourhoods according to ads count\nplt.figure(figsize= (18, 8))\n\ndf_head_provinces = pd.DataFrame(df['neighborhood'].value_counts(sort = True))\ndf_top_15 = df[df['neighborhood'].isin(df_head_provinces.head(15).index)]\n\nsns.boxplot(x = [get_display(reshape(label)) for label in df_top_15['neighborhood']] \n            , y = \"total_value\", data = df_top_15)\n# plt.title(get_display(reshape('توزیع داده در 15 محله پر آگهی')),fontsize=\"15\")\nplt.title('Data Distribution in Top 15 Neiboughrhoods (according to ads count)',fontsize=\"15\")\nplt.xlabel('Neighbourhood');\nplt.ylabel('Value (in Tooman)');\nplt.xticks(rotation = \"vertical\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n!pip install xgboost\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Input Array and Output  Vector\nIn order to build my model, I need to get dummies of *neighborhood* and change the type of data set to numpy array.\n\nThe input array will have 5276 records and 96 features and the output vector will have 5276 elements at the end.","metadata":{}},{"cell_type":"code","source":"df_main = pd.get_dummies(df, columns=[\"neighborhood\"])\n\ndf_main.drop(columns=['rent','deposit'], inplace=True, axis=1)\n\ntemp_data = df_main.to_numpy()\nX = temp_data[:,1:]\ny = temp_data[:,0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing Different Algorithms of Regression\nAt first, I prepare train and test sets for inputs and outputs. I use cross validation to ensure that the score and error values are more accurate.\n\nI compare different algorithms of regression to pick the best one as my model. For this purpose, I declare some lists and fuctions to calculate and show the performance of each regression algorithm.","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model = []\nRMSE = []\nMAE = []\nMSE = []\nR_Square = []\nadj_rsquared = []\nCV = []\nScore = []\n\nnames = [\"ExtraTrees Regressor\", \"Gradient Boosting Regressor\", \"Random Forest Regressor\",  \n         \"Bagging Regressor\",\"XGB Regressor\",\"Linear Regression\", \"Ridge Regression\", \n         \"Lasso Regression\", \"Decision Tree Regressor\", \"XGBRF Regressor\", \"Adaboost Regressor\"]\nmodels = [ExtraTreesRegressor(), GradientBoostingRegressor(), RandomForestRegressor(),  \n          BaggingRegressor(), XGBRegressor(), LinearRegression(), Ridge(), \n          Lasso(), DecisionTreeRegressor(), XGBRFRegressor(), AdaBoostRegressor()]\n\nnames_to_plot = [\"XGBRF Regressor\", \"Random Forest Regressor\"]\nmodels_to_plot = [XGBRFRegressor(),  RandomForestRegressor()]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(true, predicted, variable_of_model):\n    MAE.append(metrics.mean_absolute_error(true, predicted))\n    MSE.append(metrics.mean_squared_error(true, predicted))\n    RMSE.append(np.sqrt(metrics.mean_squared_error(true, predicted)))\n    R_Square.append(metrics.r2_score(true, predicted))\n    n= X_test.shape[0]\n    p= X_test.shape[1] - 1\n    adj_rsquared.append(1 - (1 - R_Square[-1]) * ((n - 1)/(n-p-1)))\n    cv_accuracies = cross_val_score(estimator = variable_of_model, X = X_train, y = y_train.ravel(), cv = 5,verbose = 1)\n    CV.append(cv_accuracies.mean())\n    Score.append(variable_of_model.score(X_test, y_test.ravel()))\n\ndef pred_vis(name, y_test_vis, y_pred_vis):\n    if y_test_vis.shape[0] > 300:\n        y_test_vis = y_test_vis[:300]\n        y_pred_vis = y_pred_vis[:300]\n        \n    y_test_m_vis = y_test_vis\n    plt.figure(figsize=(18,5))\n    plt.title(\"{} Prediction\" .format(name))\n    plt.plot(y_test_m_vis, c=\"steelblue\", alpha=1)\n    plt.plot(y_pred_vis, c=\"darkorange\", alpha=.7,linestyle='dashed')\n    legend_list = [\"y_test\", \"y_pred\"]\n#     plt.xlabel(\"Var\")\n    plt.ylabel(\"Output\")\n    plt.legend(legend_list, loc=1,fontsize=\"10\")\n    plt.grid(True)\n    plt.show()\n\ndef fit_and_predict(name, model):\n    variable_of_model = model\n    variable_of_model.fit(X_train, y_train.ravel())\n    pred = variable_of_model.predict(X_test)\n    evaluate(y_test, pred, variable_of_model)\n    if name in names_to_plot :\n        pred_vis(name, y_test, pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor name, model in zip(names, models):\n    fit_and_predict(name, model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result\nAfter applying fit and predict to all regression algorithms, I can get a good result of each algorithm.\n\nSince, it is a regression task, I should pay attention to the *Score* and error values like *MSE* simultaneously. ","metadata":{}},{"cell_type":"code","source":"evaluation_dataframe = pd.DataFrame({\"Model\": names,\n                                     \"MAE\": MAE,\n                                     \"MSE\": MSE,\n                                     \"RMSE\": RMSE,\n                                     \"R Squared\": R_Square,\n                                     \"adj R Squared\": adj_rsquared,\n                                     \"Cross Validation\": CV,\n                                     \"Score\" : Score})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection\nAs you can see, there are several regression algorithms return acceptable results. Based on *MSE* and *Score* values, ***XGBRFRegressor***, ***Gradient Boosting Regressor*** and ***Random Forest Regressor*** are good models.\n\n\n### Tips\n1.   Almost, all types of regression return acceptable good results.\n2.   The performance of each regression type depends on dataset size. For example, if you decrease the size of dataset to half or less, some types of regression like **Linear** or **Laso** retrun better results.\n3.   By removing the neighbourhood *سایر*, different results may be generated.\n4.   Most of the regression algorithms worked well in this task.","metadata":{}},{"cell_type":"markdown","source":"## Polynomial Regression\nSince polynomial regression is not a redefined regression structure, I have to implement and analyze it separetely. I make a pipline with polynomial features and linear regression to build a polynomial regression structure. \n\nBy plotting the **validation_curve** and also **GridSearchCV**, it is obvious that the **1th degree polynomial** is the optimal point. This means that polynomial regression works as well as linear regression. \n\nTherefore, I decide to test polynomial on just one neighbourhood, because it seems that polynomial may work better in a neighbourhood.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef PolynomialRegression(degree=2, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree),\n                         LinearRegression(**kwargs))\n\ndf_polynomial = df\ntemp_data = df_polynomial.to_numpy()\n\nX = temp_data[:,2].reshape(-1, 1)\ny = temp_data[:,0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import validation_curve\n\nplt.figure(figsize= (12,5))\n\ndegree = range(15)\n\ntrain_score, val_score = validation_curve(PolynomialRegression(), X, y,\n                                          'polynomialfeatures__degree', degree, cv=5)\n\nplt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('Degree')\nplt.ylabel('Score');\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'polynomialfeatures__degree': np.arange(15),\n              'linearregression__fit_intercept': [True, False],\n              'linearregression__normalize': [True, False]}\n\ngrid = GridSearchCV(PolynomialRegression(), param_grid, cv=5)\ngrid.fit(X, y);\ngrid.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying polynomial on one neighbourhood\nAs I said before, it seems that polynomial regression may work better in 1 neighbourhood. I select **سعادت آباد** as the input array due to its sample counts.\n\nBy plotting the **validation_curve** and also **GridSearchCV**, it is obvious that the **5th degree polynomial** is the optimal point. The *MSE* value has been reduced comparing to the result of all applying polynomial regression to all neighbourhoods. So it seems that polynomial regression may be a good choice for analysing neighbourhoods separately.","metadata":{}},{"cell_type":"code","source":"df_polynomial =  df[df['neighborhood'].str.contains('سعادت')]\ntemp_data = df_polynomial.to_numpy()\n\nX = temp_data[:,2].reshape(-1, 1)\ny = temp_data[:,0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize= (12,5))\n\ndegree = range(15)\n\ntrain_score, val_score = validation_curve(PolynomialRegression(), X, y,\n                                          'polynomialfeatures__degree', degree, cv=5)\n\nplt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('Degree')\nplt.ylabel('Score');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'polynomialfeatures__degree': np.arange(15),\n              'linearregression__fit_intercept': [True, False],\n              'linearregression__normalize': [True, False]}\n\ngrid = GridSearchCV(PolynomialRegression(), param_grid, cv=5)\ngrid.fit(X, y);\ngrid.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nmodel = PolynomialRegression(5)\nmodel.fit(X, y)\ny_model = model.predict(X_test)\nprint('Best Score:', model.score(X_test, y_test.ravel()))\nprint('Best MSE:', metrics.mean_squared_error(y_test, y_model))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conculusions\nIn a nutshell, there are some tips that I am going to point here.\n1.  It seems that input data features are enough for getting good results.\n2.  It is obvious that almost all regression algorithms are good solutions for this study.\n3.  Polynomial regression works well on each neighbourhood, but it fails on the whole neighbourhoods data.\n4.  I guess Neural networks can generate good results in this case and they can be good model for further research in the field of housing price.\n","metadata":{}}]}