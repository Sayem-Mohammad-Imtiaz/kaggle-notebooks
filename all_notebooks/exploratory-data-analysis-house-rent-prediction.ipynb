{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Model Building Pipeline: Data Analysis\n\nIn this following notebook, we will go through the Data Analysis step in the Machine Learning model building pipeline. There will be a notebook for each one of the Machine Learning Pipeline steps:\n\n1. Data Analysis\n2. [Feature Engineering](https://www.kaggle.com/rkb0023/feature-engineering-house-rent-prediction)\n3. [Model Building](https://www.kaggle.com/rkb0023/model-building-house-rent-prediction)\n\n**This is the notebook for step 1: Data Analysis**\n\nThe dataset can be found in [iNeuron](https://challenge-ineuron.in/mlchallenge.php#) ML Challenge 2.\n\n\n## Predicting Rent Price of Houses\n\nThe aim of the project is to build a machine learning model to predict the rent price of homes based on different explanatory variables describing aspects of residential houses. \n\n\n### What is the objective of the machine learning model?\n\nWe aim to minimise the difference between the real rent and the rent estimated by our model. We will evaluate model performance using the mean squared error (mse) and the root squared of the mean squared error (rmse).\n\n<br>\n<hr>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will analyse the dataset to identify:\n\n1. Data Description\n2. Missing values\n3. Numerical variables\n4. Distribution of the numerical variables\n5. Outliers\n6. Categorical variables\n7. Cardinality of the categorical variables\nPotential relationship between the variables and the target: price","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## House Rent dataset: Data Analysis\n\nIn the following cells, we will analyse the variables of the House Rent Dataset from iNeuron. We will go through the different aspects of the analysis of the variables.\n\nLet's go ahead and load the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline\n\n# to display all the columns of the dataframe in the notebook\npd.pandas.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Import","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/houseRent/housing_train.csv')\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The house price dataset contains 265,190 rows, i.e., houses, and 22 columns, i.e., variables. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### *Show data header*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Data Information*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 13 numerical features and 9 categorical features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### *Data columns*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Show statistical analysis of our dataset*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's show min, max, std, and count of each numerical variables in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values\n\nLet's go ahead and find out which variables of the dataset contain missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### *Show if there are missing datapoints*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().mean().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables containing missing values:-\n- parking_options (36%)\n- laundry_options (20%)\n- lat (0.5%)\n- long (0.5%)\n- description & state (nominal percentage)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Heat Map**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data.isnull(), ax=ax, cmap=\"YlGnBu\", center=0).set(\n            title = 'Missing Data', \n            xlabel = 'Columns', \n            ylabel = 'Data Points');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a list of the variables that contain missing values\nvars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]\ndata[vars_with_na].isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset contains a few variables with missing values. We need to account for this in our following notebook, where we will engineer the variables for use in Machine Learning Models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Relationship between values being missing and price\n\nLet's evaluate the price of the house in those observations where the information is missing, for each variable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Bar Plot**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def analyse_na_value(df, var):\n    df = df.copy()\n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    df[var] = np.where(df[var].isna(), 1, 0)\n    grs = df.groupby(var)['price'].median().reset_index()\n    plt.figure(figsize=(10,6))\n    sns.barplot(x=grs[var], y=grs['price'])\n    plt.title(var)\n    plt.show()\n\n\n# let's run the function on each variable with missing data\nfor var in vars_with_na:\n    analyse_na_value(data, var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average rent price in houses where the information is missing, differs from the average rent price in houses where information exists. \n\nWe will capture this information when we engineer the variables in our next pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Categorical variables","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# make a list of the categorical variables that contain missing values\n\nvars_with_na = [\n    var for var in data.columns\n    if data[var].isnull().sum() > 0 and data[var].dtypes == 'O'\n]\nprint(vars_with_na)\ndata[vars_with_na].isna().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[vars_with_na].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Description***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.description[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While going through the description for the house records, I found some interesting information that can be used as a feature in determining the target. Like info about grilling, pool, fireplace etc can be a useful feature. We will look about this more in data cleaning pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***State***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['region','state']].head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('region')['state'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The state is highly related to the region. So filing the missing values of state with the mode value of state for that region.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***laundry_options***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('type')['laundry_options'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling in the missing laundry_options with the mode value of laundry_options for the type of the house.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***parking_options***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('type')['parking_options'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same as landry_options, filling in the missing parking_options with the mode value of parking_options for the type of the house.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will fill in the missing values in the notebook for Feature Engineering Pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Numerical variables\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a list with the numerical variables that contain missing values\nvars_with_na = [\n    var for var in data.columns\n    if data[var].isnull().sum() > 0 and data[var].dtypes != 'O'\n]\nprint(vars_with_na)\n# print percentage of missing values per variable\ndata[vars_with_na].isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***lat***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('region')['lat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***long***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('region')['long'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we know lattitudes and longitudes tends to correspond to the region. So it will be appropriate to fill missing lattitudes and longitudes with the mode value for the region.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Boolean Variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Extracting the boolean variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bool_vars = [var for var in data if data[var].nunique() == 2]\n\ndata[bool_vars].head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# make list of numerical variables\nnum_vars = [var for var in data.columns if data[var].dtypes != 'O' and var not in bool_vars]\n\nprint('Number of numerical variables: ', len(num_vars))\n\n# visualise the numerical variables\ndata[num_vars].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above view of the dataset, we notice the variable id, which is an indicator of the house. We will not use this variable to make our predictions, as there is one different value of the variable per each row, i.e., each house in the dataset. See below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of House Id labels: ', len(data.id.unique()))\nprint('Number of Houses in the Dataset: ', len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same goes for url and image_url, each house have different set of values for these features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Geographical variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Plotting lattitude and longitude to get more insights","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Scatter Plot**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=data['long'], y=data['lat'],alpha=0.01)\nplt.xlim(right=-50)\nplt.ylim(bottom=20,top=60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Shapely geometry**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from shapely.geometry import Point\nimport geopandas as gpd\nfrom geopandas import GeoDataFrame\n\n\ngeometry = [Point(xy) for xy in zip(data['long'], data['lat'])]\ngdf = GeoDataFrame(data, geometry=geometry)   \n\n#this is a simple map that goes with geopandas\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\ngdf.plot(ax=world.plot(figsize=(10, 6)), marker='o', color='red', markersize=15);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above map it is clear that most of the houses are from the united states. \nWhich are in between longitudes -130 to -50 and lattitudes 20 to 50","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Discrete variables\n\nLet's go ahead and find which variables are discrete, i.e., show a finite number of values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  let's male a list of discrete variables\ndiscrete_vars = [var for var in num_vars if len(\n    data[var].unique()) < 20 and var not in ['id', 'price']]\n\n\nprint('Number of discrete variables: ', len(discrete_vars))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's visualise the discrete variables\n\ndata[discrete_vars].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These discrete variables refer to the number of rooms and bathrooms.\nLet's go ahead and analyse their contribution to the house price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def analyse_discrete(df, var):\n    df = df.copy()\n    grs = df.groupby(var)['price'].median().reset_index()\n    plt.figure(figsize=(10,6))\n    sns.barplot(x=grs[var], y=grs['price'])\n    plt.title(var.upper())\n    plt.show()\n    \n    \nfor var in discrete_vars:\n    analyse_discrete(data, var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There tend to be a relationship between the variables values and the price, but this relationship is not always monotonic. \n\nFor example, for beds, there is a monotonic relationship: the higher the quantity, the higher the price.  \n\nHowever, for baths, the relationship is not monotonic. Clearly, some baths number, like 8.5, correlate with higher sale prices, but higher values do not necessarily do so. We need to be careful on how we engineer these variables to extract maximum value for a linear model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Continuous variables\n\nLet's go ahead and find the distribution of the continuous variables. We will consider continuous variables to all those that are not temporal or discrete variables in our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make list of continuous variables\ncont_vars = [\n    var for var in num_vars if var not in discrete_vars+['id']]\n\nprint('Number of continuous variables: ', len(cont_vars))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's visualise the continuous variables\n\ndata[cont_vars].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dist Plot before log transformation**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's go ahead and analyse the distributions of these variables\ndef analyse_continuous(df, var):\n    df = df.copy()  \n    df = df.dropna(axis=0)\n    plt.figure(figsize=(10,6))\n    sns.set_style(\"darkgrid\")\n    sns.distplot(df[var], hist=True)\n    plt.legend(['Skewness={:.2f} Kurtosis={:.2f}'.format(\n            data[var].skew(), \n            data[var].kurt())\n        ],\n        loc='best')\n    plt.title(var)\n    plt.show()\n\nfor var in cont_vars:\n    analyse_continuous(data, var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variables are not normally distributed, including the target variable 'price'. \n\nTo maximise performance of linear models, we need to account for non-Gaussian distributions. We will transform our variables in the next lecture / video, during our feature engineering step.\n\nLet's evaluate if a logarithmic transformation of the variables returns values that follow a normal distribution:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Dist Plot after log transformation**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's go ahead and analyse the distributions of these variables\n# after applying a logarithmic transformation\ndef analyse_transformed_continuous(df, var):\n    df = df.copy()\n    df = df.dropna(axis=0)\n\n    # log does not take 0 or negative values, so let's be\n    # careful and skip those variables\n    if var == 'lat' or var == 'long':\n        pass\n    else:\n        # log transform the variable\n        df[var] = np.log1p(df[var])\n    plt.figure(figsize=(10,6))\n    sns.set_style(\"darkgrid\")\n    sns.distplot(df[var], hist=True)\n    plt.legend(['Skewness={:.2f} Kurtosis={:.2f}'.format(\n            data[var].skew(), \n            data[var].kurt())\n        ],\n        loc='best')\n    plt.title(var)\n    plt.show()\n\n\nfor var in cont_vars:\n    analyse_transformed_continuous(data, var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get a better spread of the values for most variables when we use the logarithmic transformation. This engineering step will most likely add performance value to our final model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the previous plots, we observe some monotonic associations between price and the variables to which we applied the log transformation, for example 'sqfeet'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Outliers\n\nExtreme values may affect the performance of a linear model. Let's find out if we have any in our variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Box Plot**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Violin plots are similar to box plots, except that they also show the probability density of the data at different values, usually smoothed by a kernel density estimator.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# let's make boxplots to visualise outliers in the continuous variables\n\n\ndef find_outliers(df, var):\n    df = df.copy()\n\n    # log does not take negative values, so let's be\n    # careful and skip those variables\n    if var == 'lat' or var == 'long':\n        pass\n    else:\n        # log transform the variable\n        df[var] = np.log1p(df[var])\n    ax = sns.boxplot(x=data[var], palette=\"muted\", orient=\"vertical\")\n    plt.title(var)\n    plt.ylabel(var)\n    plt.show()\n\n\nfor var in cont_vars:\n    find_outliers(data, var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of the continuous variables seem to contain outliers. Outliers tend to affect the performance of linear model. So it is worth spending some time understanding if removing outliers will add performance value to our  final machine learning model.","execution_count":null},{"metadata":{"_cell_guid":"465043f2-d687-4b1f-a6b4-1036859dfeb0","_execution_state":"idle","_uuid":"32b12bca723c5e867f7d7a7e179ff934a5fcdf30"},"cell_type":"markdown","source":"### Let's explore these outliers\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Methods for exploring outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### IQR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def out_iqr(df , column):\n    global lower,upper\n    q25, q75 = np.quantile(df[column], 0.25), np.quantile(df[column], 0.75)\n    # calculate the IQR\n    iqr = q75 - q25\n    # calculate the outlier cutoff\n    cut_off = iqr * 1.5\n    # calculate the lower and upper bound value\n    lower, upper = q25 - cut_off, q75 + cut_off\n    print('The IQR is',iqr)\n    print('The lower bound value is', lower)\n    print('The upper bound value is', upper)\n    # Calculate the number of records below and above lower and above bound value respectively\n    df1 = df[df[column] > upper]\n    df2 = df[df[column] < lower]\n    return print('Total number of outliers are', df1.shape[0]+ df2.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standard Deviation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def out_std(df, column):\n    global lower,upper\n    # calculate the mean and standard deviation of the data frame\n    data_mean, data_std = df[column].mean(), df[column].std()\n    # calculate the cutoff value\n    cut_off = data_std * 3\n    # calculate the lower and upper bound value\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    print('The lower bound value is', lower)\n    print('The upper bound value is', upper)\n    # Calculate the number of records below and above lower and above bound value respectively\n    df1 = df[df[column] > upper]\n    df2 = df[df[column] < lower]\n    return print('Total number of outliers are', df1.shape[0]+ df2.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring outliers in the variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots()\nax.scatter(x = data['sqfeet'], y = data['price'])\nplt.ylabel('price', fontsize=13)\nplt.xlabel('sqfeet', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see at the bottom right two with extremely large sqfeet that are of a low price. Also one at the top left with extremely small sqfeet that are of high price. These values are huge oultliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### IQR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_iqr(data, 'price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STD","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_std(data,'price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sqfeet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots()\nax.scatter(x = data['sqfeet'], y = data['price'])\nplt.ylabel('price', fontsize=13)\nplt.xlabel('sqfeet', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see at the bottom right two with extremely large sqfeet that are of a low price. Also one at the top left with extremely small sqfeet that are of high price. These values are huge oultliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### IQR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_iqr(data, 'sqfeet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STD","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_std(data,'sqfeet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##   beds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots()\nax.scatter(x = data['sqfeet'], y = data['beds'])\nplt.ylabel('beds', fontsize=13)\nplt.xlabel('sqfeet', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### IQR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_iqr(data, 'beds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STD","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_std(data,'beds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## baths","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### IQR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_iqr(data, 'baths')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STD","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_std(data,'baths')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## lat","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### IQR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_iqr(data.dropna(axis=0), 'lat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## long","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### IQR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_iqr(data.dropna(axis=0), 'long')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be using interquartile range to remove the outliers. As with other methods the upper and lower bounds were irrelevant.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Categorical variables\n\nLet's go ahead and analyse the categorical variables present in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# capture categorical variables in a list\ncat_vars = [var for var in data.columns if data[var].dtypes == 'O']\n\nprint('Number of categorical variables: ', len(cat_vars))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's visualise the values of the categorical variables\ndata[cat_vars].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of labels: cardinality\n\nLet's evaluate how many different categories are present in each of the variables.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data[cat_vars].nunique().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[cat_vars].nunique() / len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables like url, image_url, description has high cardinality. It is worth mentioning that each of houses may have differt values for these variables. Hence the high cardinality. So it okay to remove these. Also region_url contains the region, so dropping region_url will not affect our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# recapture categorical variables in a list\ncat_vars = [var for var in cat_vars if var not in ['url', 'image_url', 'description', 'region_url']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[cat_vars].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the categorical variables show low cardinality(except region), this means that they have only few different labels. That is good as we won't need to tackle cardinality during our feature engineering lecture.\n\n#### Rare labels:\n\nLet's go ahead and investigate now if there are labels that are present only in a small number of houses:","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def analyse_rare_labels(df, var, rare_perc):\n    df = df.copy()\n\n    # determine the % of observations per category\n    tmp = df.groupby(var)['price'].count() / len(df)\n\n    # return categories that are rare\n    return tmp[tmp < rare_perc]\n\n# print categories that are present in less than\n# 1 % of the observations\n\n\nfor var in cat_vars:\n    print(analyse_rare_labels(data, var, 0.01))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the categorical variables show multiple labels that are present in less than 1% of the houses. We will engineer these variables in our next notebook. Labels that are under-represented in the dataset tend to cause over-fitting of machine learning models. That is why we want to remove them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### frequent labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_frequent_labels(df, var, rare_perc):\n    # function finds the labels that are shared by more than\n    # a certain % of the houses in the dataset\n    df = df.copy()\n    tmp = df.groupby(var)['price'].count() / len(df)\n    return tmp[tmp > rare_perc].index.values\n\nfrequent_ls = {}\nfor var in cat_vars:\n    frequent_ls[var] = find_frequent_labels(data, var, 0.01)\n    \nfrequent_ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the frequent labels for the categorical variables that we are going to keep.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## type","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***Pie Plot***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grdsp = data.groupby([\"type\"])[[\"price\"]].mean().reset_index()\n\nfig = px.pie(grdsp,\n             values=\"price\",\n             names=\"type\",\n             template=\"seaborn\")\nfig.update_traces(rotation=90, pull=0.05, textinfo=\"percent+label\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the majority of the houses are of type apartment with around 50.2% of the total records.<br>\nThe mean price for the type apartment is around 14,544.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## state","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['state'].value_counts().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Scatter Plot***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[((data['long']>-125) & (data['long']<-45)) & ((data['lat']>30) & (data['lat']<45))]\ndf = df[df.price<2400]\ndf.plot(kind=\"scatter\", x=\"lat\", y=\"long\", alpha=0.4, \n        s=df[\"state\"].value_counts()[1]/100, label=\"no_of_houses\", \n        c=\"price\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n       figsize=(12,12))\nplt.title('House Rent Across State')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The radius of each circle represents the state’s house count (option s), and the color represents the price (option c). The range is from blue (low values) to red (high prices):","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## region","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['region'].value_counts().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Scatter Plot***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[((data['long']>-125) & (data['long']<-45)) & ((data['lat']>30) & (data['lat']<45))]\ndf = df[df.price<2400]\ndf.plot(kind=\"scatter\", x=\"lat\", y=\"long\", alpha=0.4, \n        s=df[\"region\"].value_counts()[1]/100, label=\"no_of_houses\", \n        c=\"price\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n        figsize=(12,12))\nplt.title('House Rent Across Region')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The radius of each circle represents the region’s house count (option s), and the color represents the price (option c). The range is from blue (low values) to red (high prices):","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Correlation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***Correlation Heatmap***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = data.corr()\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\nfig, ax = plt.subplots(figsize=(12,12)) \n\nsns.heatmap(corr_matrix, \n            annot=True, \n            mask=mask,\n            ax=ax, \n            cmap='BrBG').set(\n    title = 'Feature Correlation', xlabel = 'Columns', ylabel = 'Columns')\n\nax.set_yticklabels(corr_matrix.columns, rotation = 0)\nax.set_xticklabels(corr_matrix.columns)\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's hardly any correlation among the independent and dependent features.\n\nSome insights from the above correlation heatmap:\n\n1. expected stronge correlation between beds and baths\n2. unexpected correlation between smoking_allowed and lat\n3. unexpected correlation between smoking_allowed and infant_mortality\n4. expected stronge correlation between cats_allowed and dogs_allowed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In these notebook we did exploratory data analysis on the dataset from iNeuron ML Challenge-2.\n\nThe task of the EDA were:\n1. Data Description\n2. Missing values\n3. Numerical variables\n4. Distribution of the numerical variables\n5. Outliers\n6. Categorical variables\n7. Potential relationship between the variables and the target: price","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Description:\n\nThe house price dataset contains 265,190 rows, i.e., houses, and 22 columns, i.e., variables. <br>\nThere are 13 numerical features and 9 categorical features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Missing Values:\nFollowing variables consist missing values.\n- parking_options (36%)\n- laundry_options (20%)\n- lat (0.5%)\n- long (0.5%)\n- description & state (nominal percentage)\n\n**Imputate missing values:**\n\nvariables | imputation \n--- | ---\nparking_options | mode value of the parking_options for the respective house type\nlaundry_options | mode value of the laundry_options for the respective house type\nlat | mode value of the lattitude for the respective house region\nlong | mode value of the longitude for the respective house region\nstate | drop records\ndescription | drop records\n\n\n***Note***: Description column can be explored more to get intriguing new features. For example: having pool, fireplace, grilling place, gym nearby etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Numerical variables:\n\nNumerical variables are: <br>\n    \n    ['price', 'sqfeet', 'beds', 'baths', 'lat', 'long']\n\nBoolean variables are: <br>\n    \n    ['cats_allowed', 'dogs_allowed', 'smoking_allowed', 'wheelchair_access', 'electric_vehicle_charge', \n    'comes_furnished']\n\nDiscrete variables are: <br>\n    \n    ['beds', 'baths']\n\nContinuous variables are: <br>\n    \n    ['price', 'sqfeet', 'lat', 'long']","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Distribution of Numerical Variables\n\nAll the numerical variables, except lat and long are skewed. Log transformation found to be useful while analysis the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. Outliers\n\nAll the numerical variables contain huge outliers. While doing EDA, we decided to remove the outliers with the help of interquartile range.\n\nvariables | no of outliers | upper bound | lower bound \n--- | --- | --- | ---\nprice | 13423 | 2400 | 1\nsqfeet | 11212 | 1762 | 146\nbeds | 10017 | 3 | 1\nbaths | 1459 | 3 | 1\nlat | 3148 | 53 | 23\nlong | 3110 | -45 | -142","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6. Categorical Variables\n\nThe categorical features are: <br>\n\n    ['region', 'type', 'laundry_options', 'parking_options', 'state']\n       \nThese variables contain all text: <br>\n    \n    ['url', 'region_url', 'image_url', 'description']\n\nThe frequent labels for the categorical variables are: <br>\n\n    {\n    'region': \n        ['denver', 'fayetteville', 'jacksonville', 'omaha / council bluffs', 'rochester'],\n     'type': \n        ['apartment', 'condo', 'duplex', 'house', 'manufactured', 'townhouse'],\n     'laundry_options': \n        ['laundry in bldg', 'laundry on site', 'w/d hookups', 'w/d in unit'],  \n     'parking_options': \n        ['attached garage', 'carport', 'detached garage', 'off-street parking', 'street parking'],\n     'state': \n        ['al', 'ar', 'az', 'ca', 'co', 'ct', 'fl', 'ga', 'ia', 'id', 'il', 'in', 'ks', 'ky', 'la', \n        'ma', 'md', 'mi', 'mn', 'ms', 'nc', 'nd', 'ne', 'nj', 'nm', 'nv', 'ny', 'oh']\n    }","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 7. Relationship between independent and dependent variable\n\nWe did not notice much relationship among the features. We look into this topic in our next notebook, i.e. Feature Engineering Pipelin.\n\nIn the next notebook, we will transform these strings / labels into numbers, so that we capture this information and transform it into a monotonic relationship between the category and the house price.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}