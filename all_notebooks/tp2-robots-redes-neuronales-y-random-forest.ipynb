{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>(7570) SIST.DE PROG. NO CONVENCIONAL DE ROBOTS\n### <center>Trabajo Práctico 02: Redes Neuronales y Random Forest</center>\n### <center>Marco Luis Fleres, Padrón 93174</center>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargamos el dataset:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATAFILE=\"/kaggle/input/mushroom-classification/mushrooms.csv\"\nDATASET_SIZE=8124\nnp.set_printoptions(precision=3, suppress=True)\nN_CLASSES=1 # Edible/Poisonous\nN_INPUTS=22 # Columnas del CSV usadas para clasificar\n\nfull_dataset = tf.data.experimental.make_csv_dataset(DATAFILE, batch_size=1, label_name=\"class\", shuffle=True)\n# Ya que hay una sola clase \"edible/poisonous\", codificamos el label de los rows como 0/1. Luego codificaremos los features.\nfull_dataset = full_dataset.map(lambda features, label: (features, 0 if label==\"e\" else 1))\n\nfor features, label in full_dataset.take(1):\n    print(\"Label:\", label.numpy())\n    tf.print(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separamos los datos en los conjuntos de entrenamiento y validación.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = full_dataset.take(round(DATASET_SIZE/3*2))\ntest_data = full_dataset.skip(round(DATASET_SIZE/3*2)).take(round(DATASET_SIZE/3)-1)\n\n\nprint(tf.data.experimental.cardinality(train_data).numpy())\nprint(tf.data.experimental.cardinality(test_data).numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Construimos el modelo del perceptrón multicapa. Empezamos por codificar los valores de las columnas como números. Ya que construiremos un modelo de keras, usamos feature_columns para codificar las columnas:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import feature_column\n\nfeature_columns=[]\n\nVOCABULARY={\n    'cap-shape': ['b', 'c', 'x', 'f', 'k', 's'],\n    'cap-surface': ['f', 'g', 'y', 's'],\n    'cap-color': ['n', 'b', 'c', 'g', 'r', 'p', 'u', 'e', 'w', 'y'],\n    'bruises': ['t', 'f'],\n    'odor': ['a', 'l', 'c', 'y', 'f', 'm', 'n', 'p', 's'],\n    'gill-attachment': ['a', 'd', 'f', 'n'],\n    'gill-spacing': ['c', 'w', 'd'],\n    'gill-size': ['b', 'n'],\n    'gill-color': ['k', 'n', 'b', 'h', 'g', 'r', 'o', 'p', 'u', 'e', 'w', 'y'],\n    'stalk-shape': ['e', 't'],\n    'stalk-root': ['b', 'c', 'u', 'e', 'z', 'r', '?'],\n    'stalk-surface-above-ring': ['f', 'y', 'k', 's'],\n    'stalk-surface-below-ring': ['f', 'y', 'k', 's'],\n    'stalk-color-above-ring': ['n', 'b', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n    'stalk-color-below-ring': ['n', 'b', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n    'veil-type': ['p', 'u'],\n    'veil-color': ['n', 'o', 'w', 'y'],\n    'ring-number': ['n', 'o', 't'],\n    'ring-type': ['c', 'e', 'f', 'l', 'n', 'p', 's', 'z'],\n    'spore-print-color': ['k', 'n', 'b', 'h', 'r', 'o', 'u', 'w', 'y'],\n    'population': ['a', 'c', 'n', 's', 'v', 'y'],\n    'habitat': ['g', 'l', 'm', 'p', 'u', 'w', 'd'],\n}\n\nfor header in VOCABULARY:\n    feature_columns.append(feature_column.indicator_column(feature_column.categorical_column_with_vocabulary_list(header, VOCABULARY[header])))\n\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observamos el preprocesado:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for features, label in train_data.take(1):\n    print(\"Label:\", label.numpy())\n    print(\"Parametros:\")\n    tf.print(features)\n    print(\"Parametros codificados:\", feature_layer(features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Construimos el pipeline del modelo:\nHabiendo codificado los features mediante categorical_column_with_vocabulary_list, terminamos con 126 neuronas de entrada.\nFunción de activación: relu\nFunción de inicialización de pesos: Función Normal Estándar truncada\nUsamos dos layer ocultas con 20 neuronas cada una.\n\nPara la Backpropagation, usamos un optimizador de Gradient Descent con una función de error de Cuadrados Mínimos. Realizamos 10 ciclos de entrenamiento.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\nfrom keras.optimizers import SGD\n\nmodel = tf.keras.Sequential([\n  #tf.keras.Input(shape=(126)),\n  feature_layer,\n  layers.Dense(20, activation='relu', kernel_initializer='he_normal', name=\"layer1\"),\n  layers.Dense(20, activation='relu', kernel_initializer='he_normal', name=\"layer1\"),\n  layers.Dense(1, name=\"output\")\n])\n\nmodel.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='mse', metrics=['accuracy'])\n#print(model.summary())\n\n#train_data = train_data.batch(100)\nmodel.fit(train_data.batch(100), epochs=10, batch_size=100, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Probamos el Predictor con algunos datos:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for features, label in test_data.take(20):\n    print(\"Label/Prediction:\", label.numpy(), model.predict_classes(features)[0][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Obtenemos el Error Cuadrático Medio, la precisión, recall y f1 del modelo con los datos de prueba, tanto con el puntaje provisto por Keras como el evaluado con sklearn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\n\n# Evaluamos usando el conjunto de datos de test\nloss, accuracy = model.evaluate(test_data.batch(1))\nprint('MSE: %.3f' % (loss))\nprint('Accuracy: %.3f' % (accuracy))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n#predictions = test_data.map(lambda features, labels: (labels, tf.map_fn(lambda f:model.predict_classes(f), features)))\npredictionPairs = []\n\nfor features, label in test_data.take(10):\n    predictionPairs.append( (label.numpy(), model.predict_classes(features)[0][0]) )\n\ny_true, y_pred = zip(*predictionPairs)\n    \nprint({\n    \"accuracy\": accuracy_score(y_true, y_pred),\n    \"precision\": precision_score(y_true, y_pred),\n    \"recall\": recall_score(y_true, y_pred),\n    \"f1\": f1_score(y_true, y_pred)\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataPd = pd.read_csv(DATAFILE)\ndataPd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nY = dataPd['class']\nX = dataPd.drop(['class'], axis=1)\n\n# Necesitamos codificar los features como números para alimentar el Random Forest\nX = OneHotEncoder().fit_transform(X).toarray()\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.33, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parámetros del Random Forest:\nCantidad de árboles (n_estimators): 50\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nRF_model = RandomForestClassifier(bootstrap=True, n_estimators=50)\nRF_model.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, f1_score\n\nreal = list(test_y.tolist())\nRF_predictions = list(RF_model.predict(test_X))\n\nprint({\n    \"accuracy\": accuracy_score(real, RF_predictions),\n    \"precision\": precision_score(real, RF_predictions, pos_label=\"e\"),\n    \"recall\": recall_score(real, RF_predictions, pos_label=\"e\"),\n    \"f1\": f1_score(real, RF_predictions, pos_label=\"e\")\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusiones\n\nTanto la red neuronal como el RandomForest tuvieron una precisión muy alta. Es posible que se haya dado overfitting, aunque los RandomForest son menos suceptibles a esto. Harían falta mas datos para comprobarlo. La Red Neuronal si bien converge rápidamente, no es tan rápida de entrenar como el RandomForest, y requiere más hiperparámetros, por lo que a igualdad de resultados, preferiremos el RandomForest.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}