{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Sentiment Classification"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4334c9e7401e84862defa71ba574b05742f7ec1f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"041d67561bef921c9dd5c63abba9e6be0866d3e9"},"cell_type":"markdown","source":"## Read data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"96b662c0f35802f5df118bfc6b3a504558136741"},"cell_type":"code","source":"data_train = pd.read_csv('../input/train.csv')\ndata_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"758f67199e7b44e9e0bcb033d3a4a0374d80453b","collapsed":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b257204326ce866dd18c049e3bc38835cc92288","collapsed":true},"cell_type":"code","source":"print(data_train.dtypes)\nprint(data_train.describe())\nprint(data_train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a6605f7649d775843c89860e7a2c733feb72b7f","collapsed":true},"cell_type":"code","source":"data_train.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40ecd8c391b00bda0acd8fb8d01e925761672188","collapsed":true},"cell_type":"code","source":"print(data_train.shape, data_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bf021ab34e4b5d9d54ea3459e95ae1556c51c530"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c1319aaf93949758855b48757c9998d0c6bb15d"},"cell_type":"code","source":"vectorizer = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb07a49a6a7329003f0ce1f98e408989072b2f40","collapsed":true},"cell_type":"code","source":"train_vector = vectorizer.fit_transform(data_train.sentence)\ntest_vector = vectorizer.transform(data_test.sentence)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"946b467deb2cbd61f0f7c0090a3cd6ab573eadf8"},"cell_type":"markdown","source":"### Visualize Word Frequency"},{"metadata":{"trusted":true,"_uuid":"fe772f9ecc8c35eebaea170ca9f4fce929ab488f","collapsed":true},"cell_type":"code","source":"WordFrequency = pd.DataFrame({'Word': vectorizer.get_feature_names(), 'Count': train_vector.toarray().sum(axis=0)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a9c8c566778a9fc659cfac1204b8a9284af23f6","collapsed":true},"cell_type":"code","source":"WordFrequency['Frequency'] = WordFrequency['Count'] / WordFrequency['Count'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5b62a8671ea2e2bcc734ca39628ef43eee8871c","collapsed":true},"cell_type":"code","source":"plt.plot(WordFrequency.Frequency)\nplt.xlabel('Word Index')\nplt.ylabel('Word Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef992c9c3de08b6ba270eda009baf8d63a877a38"},"cell_type":"markdown","source":"### Sort WordFrequency in descending order"},{"metadata":{"trusted":true,"_uuid":"a413341ae01ad8893a27cebd748c073812319f9d","collapsed":true},"cell_type":"code","source":"WordFrequency_sort = WordFrequency.sort_values(by='Frequency', ascending=False)\nWordFrequency_sort.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3146956a755f1ae0e7b109d9e35d7a43d36f357"},"cell_type":"markdown","source":"## Model 1: Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"0d5ae6288059d5134e35a440e9f2de8628301808","collapsed":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.cross_validation import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"619a2a88fed6b2f3815463654239172112ed9cbb"},"cell_type":"code","source":"clf1 = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cb6a3b886f6ed036e88ddcc6e9b8aec51d55d4e","collapsed":true},"cell_type":"code","source":"cross_val_acc = cross_val_score(clf1, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc)\nprint(cross_val_acc.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4e639564345e297260052eeedcdd7b0c5b96c828"},"cell_type":"code","source":"clf1.fit(train_vector, data_train.label.values)\npredictions = clf1.predict(test_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82ee61dfeae31ce1d0c0675421f3e3310d107273","collapsed":true},"cell_type":"code","source":"solution1 = pd.DataFrame(list(zip(data_test.sentence, predictions)), columns=['sentence', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4625ef83a189e87dccf07ab258d563c5d53cf15e","collapsed":true},"cell_type":"code","source":"solution1.to_csv('./solution1_naive_bayes.csv', index=False)\n# Accuracy in testing data: 0.97461","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34ed7ad5a9458eebe8f345bd82903e5a0a9fa152"},"cell_type":"markdown","source":"## Model 2: Random Forest"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"24a470e239cdaf99a0444a8ea50fdde3b878773f"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf2 = RandomForestClassifier(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff815c805119f787aabfd8d044eb00286011d3c4","collapsed":true},"cell_type":"code","source":"cross_val_acc2 = cross_val_score(clf2, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc2)\nprint(cross_val_acc2.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8bcfc3a8906d489ce45fd4b043e988803ecf5a38"},"cell_type":"code","source":"clf2.fit(train_vector, data_train.label.values)\nprediction2 = clf2.predict(test_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4fdd8049f3865af3cf5b9a57cc4ea30051475b28"},"cell_type":"code","source":"solution2 = pd.DataFrame(list(zip(data_test.sentence, prediction2)), columns=['sentence','label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9361c321e14c816588f224f4d3cf044a6f04ec8e","collapsed":true},"cell_type":"code","source":"solution2.to_csv('./solution2_random_forest.csv', index=False)\n# Accuracy in testing data: 0.97884","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f78381e2b0f8d7b02e99f3f3ac37e932bd4bb3a"},"cell_type":"markdown","source":"### Use GridSearchCV"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8d563af431eb70565fbce21825fb7bfaf62a2fe"},"cell_type":"code","source":"from pprint import pprint\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b81404dff4aba009844f00ffc1c8983af5411df0","collapsed":true},"cell_type":"code","source":"pprint(clf2.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f88dfbc47012ff732ce767d41e3f34cfd20de5d","collapsed":true},"cell_type":"code","source":"param_grid = {\n             'class_weight': ['balanced', None],\n             'criterion': ['gini', 'entropy'],\n             'max_depth': [None, 1, 5, 10],\n             'max_features': ['auto', 'log2', None],\n             'n_estimators': [5, 10, 20]}\ncv_clf2 = GridSearchCV(estimator=clf2, param_grid=param_grid, scoring='accuracy', verbose=0, n_jobs=-1)\ncv_clf2.fit(train_vector, data_train.label.values)\nbest_parameters = cv_clf2.best_params_\nprint('The best parameters for using RF model is: ', best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4f0c0a2cab00bd0540cb6150dbc5645a9906e65a"},"cell_type":"code","source":"clf2_balanced_gini = RandomForestClassifier(class_weight='balanced', n_estimators=20)\nclf2_entropy = RandomForestClassifier(criterion='entropy', n_estimators=20)\nclf2_gini = RandomForestClassifier(n_estimators=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90cd3706c92c29e8d6f7ec66db932f093f238747","collapsed":true},"cell_type":"code","source":"RF_score1 = cross_val_score(clf2_balanced_gini, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(RF_score1)\nprint(RF_score1.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f37abaae7af64dd2b2350ff9953f3de0c3e20474","collapsed":true},"cell_type":"code","source":"RF_score2 = cross_val_score(clf2_entropy, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(RF_score2)\nprint(RF_score2.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61df55b303b4300e6fc4598d4430b5f7b50ccfe5","collapsed":true},"cell_type":"code","source":"RF_score3 = cross_val_score(clf2_gini, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(RF_score3)\nprint(RF_score3.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0333efdd3ab761bfc02a6f145abef4ca8ec983ac"},"cell_type":"code","source":"clf2_balanced_gini.fit(train_vector, data_train.label.values)\nprediction2_tuned = clf2_balanced_gini.predict(test_vector)\nsolution2_tuned = pd.DataFrame(list(zip(data_test.sentence, prediction2_tuned)), columns=['sentence', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cbbe7e50e827263869c66f613e24026f77ba85b","collapsed":true},"cell_type":"code","source":"solution2_tuned.to_csv('./solution2_RF_tuned.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2757fd2650473b6b51ea5d33d35d7af31cbb2f6"},"cell_type":"markdown","source":"## Model 3: Logistic Regression (Use GridSearchCV to tune hyper-parameters)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"edb47ae537a6a8734743b9f5fd04925cb295f25f"},"cell_type":"code","source":"# Use Logistic Regression directly\nfrom sklearn.linear_model import LogisticRegression\nclf3_1 = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7105467f4990b9214646a192ebab48a0d90cc46b","collapsed":true},"cell_type":"code","source":"cross_val_acc3_1 = cross_val_score(clf3_1, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc3_1)\nprint(cross_val_acc3_1.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe76d02cf05004ee75476625892788c109797247"},"cell_type":"markdown","source":"### Use GridSearchCV"},{"metadata":{"trusted":true,"_uuid":"b1cd6968dfe141d06f29c549cdaa7e3482cba4c7","collapsed":true},"cell_type":"code","source":"pprint(clf3_1.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd0704a169654659c2e3052c13febe1f5f819437","collapsed":true},"cell_type":"code","source":"param_grid = {'penalty': ['l1', 'l2'],\n             'class_weight': ['balanced', None],\n             'C': [0.1, 1, 10]\n             }\nclf3_2 = GridSearchCV(estimator=clf3_1, param_grid=param_grid, scoring='accuracy', verbose=1, n_jobs=-1)\nclf3_2.fit(train_vector, data_train.label.values)\nbest_param = clf3_2.best_params_\nprint('The best parameters for using LR model is: ', best_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"108c74f172a1fc8e6887ff52cc6ce12db23268fd","collapsed":true},"cell_type":"code","source":"clf3_2 = LogisticRegression(C=9.4)\ncross_val_acc3_2 = cross_val_score(clf3_2, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc3_2)\nprint(cross_val_acc3_2.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24a816450ffb933389bc310f23b11716f4fbade8","collapsed":true},"cell_type":"code","source":"clf3_1.fit(train_vector, data_train.label.values)\nclf3_2.fit(train_vector, data_train.label.values)\nprediction3_1 = clf3_1.predict(test_vector)\nprediction3_2 = clf3_2.predict(test_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"978a17935247f73b99b20c358ec69500b9cbd148"},"cell_type":"code","source":"solution3_origin_LR = pd.DataFrame(list(zip(data_test.sentence, prediction3_1)), columns=['sentence', 'label'])\nsolution3_CV_LR = pd.DataFrame(list(zip(data_test.sentence, prediction3_2)), columns=['sentence', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10a74c7530339f640865b8eda2d61a7b39440d8c","collapsed":true},"cell_type":"code","source":"solution3_origin_LR.to_csv('./solution3_origin_LR.csv', index=False)\n# Accuracy in testing data: 0.99083","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6cca0daedb19c5d14c4ee481e45a4fc91b3b786","collapsed":true},"cell_type":"code","source":"solution3_CV_LR.to_csv('./solution3_CV_LR.csv', index=False)\n# Accuracy in testing data:0.99083","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4eb50d789e5281f02f816506f384cd54207203fd"},"cell_type":"markdown","source":"## Model 4: RNN"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aacc3b1c354f4d149ee30e6ec765eb743fd620fb"},"cell_type":"code","source":"import collections\nimport tensorflow as tf\nimport os\nimport nltk\nfrom keras.preprocessing import sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d95a23809838f7b1f1af6781cbe2b830f12145f6"},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f38fb74b02ae806015e63ccf5effc0b77210d836"},"cell_type":"code","source":"num_sentences = len(data_train)\nprint(num_sentences)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adb3ad2bc81b34e5b67004cb40f17b6c7da41e15"},"cell_type":"markdown","source":"### Applied nltk.word_tokenize to do words spliting"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"406914026121c4ac31fa617acaad32a25d8d8ee1"},"cell_type":"code","source":"maxLength = 0\nword_frequency = collections.Counter()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8c2fe0d45da3d40597eb62de3cfa7b4392ad238"},"cell_type":"code","source":"for idx, row in data_train.iterrows():\n    words = nltk.word_tokenize(row['sentence'].lower())\n    if len(words) > maxLength:\n        maxLength = len(words)\n    for word in words:\n        word_frequency[word] += 1\nprint(len(word_frequency))\nprint(maxLength)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6088881ecc4d2a089ab3baeb7f0f9f90dd57f524"},"cell_type":"code","source":"maxFeatures = 2074\nvocab_size = maxFeatures + 2\nword2index = {x[0]: i+2 for i, x in enumerate(word_frequency.most_common(maxLength))}\n\nword2index['PAD'] = 0\nword2index['UNK'] = 1\nindex2word = {i:w for w, i in word2index.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3de61e01abe1d77590c9359cb27329b1b145bd9a"},"cell_type":"code","source":"data_X_in = np.empty((num_sentences, ), dtype=list)\ndata_y = np.zeros((num_sentences, ))\ni = 0\n\nfor index, row in data_train.iterrows():\n    words = nltk.word_tokenize(row['sentence'].lower())\n    seqs = []\n    for word in words:\n        if word in word2index:\n            seqs.append(word2index[word])\n        else:\n            seqs.append(word2index[\"UNK\"])\n    data_X_in[i] = seqs\n    data_y[i] = int(row['label'])\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d1bdba71898e973b42a1e44542cf938a533df56b"},"cell_type":"code","source":"data_X_in = sequence.pad_sequences(data_X_in, padding='post', value=word2index['PAD'], maxlen=maxLength)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"767a33ba5eb8226cead75a152ded3eae5a858b6d"},"cell_type":"code","source":"print(data_X_in[:5])\nprint(data_X_in.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc5fe90eb706e6ff94a25e0668001dfd730a3e23"},"cell_type":"code","source":"print(data_train.sentence.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eae2ea40e34858ae9ef912b9a3f3b2094706bbc"},"cell_type":"code","source":"print(data_y[:5])\nprint(data_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d88092a79fead6465f3669ee19b4a8ddf5319969"},"cell_type":"code","source":"def data_generator(batch_size):\n    while True:\n        for i in range(0,len(data_X_in),batch_size):\n            if i + batch_size < len(data_X_in):\n                yield data_X_in[i:i + batch_size], data_y[i:i + batch_size]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c29874e3a0db9dab919ecba7da04af34882bfdd4"},"cell_type":"code","source":"batch_size = 24\nembedding_size = 100\nvocab_size = maxFeatures + 2\nnum_units = 64\nNUM_EPOCHS = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"631040e672a592602c04e3da414ef9a9c1dd5654"},"cell_type":"code","source":"import tflearn\ntf.reset_default_graph()\nconfig = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d81deeb1df035c12dc00c48373d0317c0604e3f"},"cell_type":"code","source":"with tf.device('/gpu:1'):\n    initializer = tf.random_uniform_initializer(\n        -0.08, 0.08)\n    tf.get_variable_scope().set_initializer(initializer)\n    x = tf.placeholder(\"int32\", [None, None])\n    y = tf.placeholder(\"int32\", [None])\n    x_len = tf.placeholder(\"int32\",[None])\n    \n    learning_rate = tf.placeholder(tf.float32, shape=[])\n    \n    # embedding\n    embedding_encoder = tf.get_variable(\n        \"embedding_encoder\", [vocab_size, embedding_size],dtype=tf.float32)\n    encoder_emb_inp = tf.nn.embedding_lookup(\n        embedding_encoder, x)\n    \n    # Build RNN cell\n    encoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n    \n    encoder_cell = tf.contrib.rnn.DropoutWrapper(cell=encoder_cell, output_keep_prob=0.75)\n    # Run Dynamic RNN\n    #   encoder_outputs: [max_time, batch_size, num_units]\n    #   encoder_state: [batch_size, num_units]\n    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n        encoder_cell, encoder_emb_inp,\n        sequence_length=x_len, time_major=False,dtype=tf.float32)\n    \n    model_logistic = tf.layers.dense(encoder_state[0],1)\n    model_pred = tf.nn.sigmoid(model_logistic)\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y,tf.float32),logits=tf.reshape(model_logistic,(-1,)))\n    loss = tf.reduce_mean(loss)\n    optimizer = tf.train.AdamOptimizer().minimize(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"079e91f7cf0cf7083028448715e1f2834c49e431"},"cell_type":"code","source":"session.run(tf.global_variables_initializer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e12a1609bda02e49a3fccdc71a0ef4b499cffcdf"},"cell_type":"code","source":"import os\nimport sys\nimport time\n\nclass Dataset():\n    def __init__(self,data,label):\n        self._index_in_epoch = 0\n        self._epochs_completed = 0\n        self._data = data\n        self._label = label\n        assert(data.shape[0] == label.shape[0])\n        self._num_examples = data.shape[0]\n        pass\n\n    @property\n    def data(self):\n        return self._data\n    \n    @property\n    def label(self):\n        return self._label\n\n    def next_batch(self,batch_size,shuffle = True):\n        start = self._index_in_epoch\n        if start == 0 and self._epochs_completed == 0:\n            idx = np.arange(0, self._num_examples)  # get all possible indexes\n            np.random.shuffle(idx)  # shuffle indexe\n            self._data = self.data[idx]  # get list of `num` random samples\n            self._label = self.label[idx]\n\n        # go to the next batch\n        if start + batch_size > self._num_examples:\n            self._epochs_completed += 1\n            rest_num_examples = self._num_examples - start\n            data_rest_part = self.data[start:self._num_examples]\n            label_rest_part = self.label[start:self._num_examples]\n            idx0 = np.arange(0, self._num_examples)  # get all possible indexes\n            np.random.shuffle(idx0)  # shuffle indexes\n            self._data = self.data[idx0]  # get list of `num` random samples\n            self._label = self.label[idx0]\n\n            start = 0\n            self._index_in_epoch = batch_size - rest_num_examples #avoid the case where the #sample != integar times of batch_size\n            end =  self._index_in_epoch  \n            data_new_part =  self._data[start:end]  \n            label_new_part = self._label[start:end]\n            return np.concatenate((data_rest_part, data_new_part), axis=0),np.concatenate((label_rest_part, label_new_part), axis=0)\n        else:\n            self._index_in_epoch += batch_size\n            end = self._index_in_epoch\n            return self._data[start:end],self._label[start:end]\n\nclass ProgressBar():\n    def __init__(self,worksum,info=\"\",auto_display=True):\n        self.worksum = worksum\n        self.info = info\n        self.finishsum = 0\n        self.auto_display = auto_display\n    def startjob(self):\n        self.begin_time = time.time()\n    def complete(self,num):\n        self.gaptime = time.time() - self.begin_time\n        self.finishsum += num\n        if self.auto_display == True:\n            self.display_progress_bar()\n    def display_progress_bar(self):\n        percent = self.finishsum * 100 / self.worksum\n        eta_time = self.gaptime * 100 / (percent + 0.001) - self.gaptime\n        strprogress = \"[\" + \"=\" * int(percent // 2) + \">\" + \"-\" * int(50 - percent // 2) + \"]\"\n        str_log = (\"%s %.2f %% %s %s/%s \\t used:%ds eta:%d s\" % (self.info,percent,strprogress,self.finishsum,self.worksum,self.gaptime,eta_time))\n        sys.stdout.write('\\r' + str_log)\n\ndef get_dataset(paths):\n    dataset = []\n    for path in paths.split(':'):\n        path_exp = os.path.expanduser(path)\n        classes = os.listdir(path_exp)\n        classes.sort()\n        nrof_classes = len(classes)\n        for i in range(nrof_classes):\n            class_name = classes[i]\n            facedir = os.path.join(path_exp, class_name)\n            if os.path.isdir(facedir):\n                images = os.listdir(facedir)\n                image_paths = [os.path.join(facedir,img) for img in images]\n                dataset.append(ImageClass(class_name, image_paths))\n  \n    return dataset\n\nclass ImageClass():\n    \"Stores the paths to images for a given class\"\n    def __init__(self, name, image_paths):\n        self.name = name\n        self.image_paths = image_paths\n  \n    def __str__(self):\n        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n  \n    def __len__(self):\n        return len(self.image_paths)\n\ndef split_dataset(dataset, split_ratio, mode):\n    if mode=='SPLIT_CLASSES':\n        nrof_classes = len(dataset)\n        class_indices = np.arange(nrof_classes)\n        np.random.shuffle(class_indices)\n        split = int(round(nrof_classes*split_ratio))\n        train_set = [dataset[i] for i in class_indices[0:split]]\n        test_set = [dataset[i] for i in class_indices[split:-1]]\n    elif mode=='SPLIT_IMAGES':\n        train_set = []\n        test_set = []\n        min_nrof_images = 2\n        for cls in dataset:\n            paths = cls.image_paths\n            np.random.shuffle(paths)\n            split = int(round(len(paths)*split_ratio))\n            if split<min_nrof_images:\n                continue  # Not enough images for test set. Skip class...\n            train_set.append(ImageClass(cls.name, paths[0:split]))\n            test_set.append(ImageClass(cls.name, paths[split:-1]))\n    else:\n        raise ValueError('Invalid train/test split mode \"%s\"' % mode)\n    return train_set, test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66e988df358c3a46c9b04a59c25af767194b66b3"},"cell_type":"code","source":"losses = []\nbeginning_lr = 0.1\ngen = data_generator(batch_size)\nfor one_epoch in range(0,1):\n    pb = ProgressBar(worksum=len(data_X_in))\n    pb.startjob()\n    for one_batch in range(0,len(data_X_in),batch_size):\n        batch_x,batch_y = gen.__next__()\n        batch_x_len = np.asarray([len(i) for i in batch_x])\n        batch_lr = beginning_lr \n        \n        _,batch_loss = session.run([optimizer,loss],feed_dict={\n            x:batch_x,\n            y:batch_y,\n            x_len:batch_x_len,\n            learning_rate:batch_lr,\n        })\n        pb.info = \"EPOCH {} batch {} lr {} loss {}\".format(one_epoch,one_batch,batch_lr,batch_loss)\n        pb.complete(batch_size)\n        losses.append(batch_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f148a8c8cf6ab6df5e246b5027da3eb7eb32426"},"cell_type":"code","source":"%matplotlib inline\npd.DataFrame(losses).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2e946bf62110fb137411d6935513090be7e6a0c0"},"cell_type":"code","source":"def predict_result(sent):\n    words = nltk.word_tokenize(row['sentence'].lower())\n    senttoken = [word2index.get(word,word2index['UNK']) for word in words]\n    inputx = np.asarray([senttoken])\n    inputx_len = np.asarray([len(senttoken)])\n    batch_predict = session.run(model_pred,feed_dict={\n            x:inputx,\n            x_len:inputx_len,\n        })[0]\n    return 1 if batch_predict > 0.5 else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ee27140c802d43fb3ec6098c15fcc8f6fbe3a5b"},"cell_type":"code","source":"labels = []\nfor index, row in data_test.iterrows():\n    label = predict_result(row['sentence'])\n    labels.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78951aefc394721db3f44937498c97e911a6e81f"},"cell_type":"code","source":"print(len(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c7a149f3f6042bb1a9e5b71dd4ca333a4723b2dc"},"cell_type":"code","source":"solution_RNN1 = pd.DataFrame(list(zip(data_test.sentence, labels)), columns=['sentence', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1608710f60c2f71f64a57cba2518a3a0897a1e21"},"cell_type":"code","source":"solution_RNN1.to_csv('./solution_RNN1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7995442a555e73dab16cb32f4f92a9536a9a1ada"},"cell_type":"markdown","source":"### Applied sentence.split() to do words spliting"},{"metadata":{"trusted":true,"_uuid":"0e760cb1c38471f96f459da6dd4c35a2227e7b77"},"cell_type":"code","source":"max_len = 0\nword_freq = collections.Counter()\nfor i in data_train.sentence.values:\n    words = [j.lower() for j in i.strip('\\n').split()]\n    if len(words) > max_len:\n        max_len = len(words)\n    for word in words:\n        word_freq[word] += 1\nprint(len(word_freq))\nprint(max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53adc8a6ce22959b5cc432d6fa4083512dddf583"},"cell_type":"code","source":"maxFeatures = 2673\nvocab_size = maxFeatures + 2\nword2index = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(maxLength))}\n\nword2index['PAD'] = 0\nword2index['UNK'] = 1\nindex2word = {i:w for w, i in word2index.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b04e5467e127480ca4f88ec13d535df8dc4365e9"},"cell_type":"code","source":"data_X_in = np.empty((num_sentences, ), dtype=list)\ndata_y = np.zeros((num_sentences, ))\ni = 0\n\nfor index, row in data_train.iterrows():\n    words = [j.lower() for j in row['sentence'].split()]\n    seqs = []\n    for word in words:\n        if word in word2index:\n            seqs.append(word2index[word])\n        else:\n            seqs.append(word2index[\"UNK\"])\n    data_X_in[i] = seqs\n    data_y[i] = int(row['label'])\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d88d996d654354815a2210aaab8597c69e6ebd38"},"cell_type":"code","source":"data_X_in = sequence.pad_sequences(data_X_in, padding='post', value=word2index['PAD'], maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7072c5e60263f90aeb1bc0d66f4454de4d5368eb"},"cell_type":"code","source":"print(data_X_in[:5])\nprint(data_X_in.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbb6854a87d2555698602594875d396c66f9af1e"},"cell_type":"code","source":"print(data_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"428619910f1ad499323cde1059da9c24d5d339c5"},"cell_type":"code","source":"print(data_y[:5])\nprint(data_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a32454b76f512f17b67d1b4145fedb4b4232a848"},"cell_type":"code","source":"tf.reset_default_graph()\nconfig = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\nconfig.gpu_options.allow_growth = True\n#config.gpu_options.per_process_gpu_memory_fraction = 0.4\nsession = tf.Session(config=config)\n\nwith tf.device('/gpu:1'):\n    initializer = tf.random_uniform_initializer(\n        -0.08, 0.08)\n    tf.get_variable_scope().set_initializer(initializer)\n    x = tf.placeholder(\"int32\", [None, None])\n    y = tf.placeholder(\"int32\", [None])\n    x_len = tf.placeholder(\"int32\",[None])\n    \n    learning_rate = tf.placeholder(tf.float32, shape=[])\n    \n    # embedding\n    embedding_encoder = tf.get_variable(\n        \"embedding_encoder\", [vocab_size, embedding_size],dtype=tf.float32)\n    encoder_emb_inp = tf.nn.embedding_lookup(\n        embedding_encoder, x)\n    \n    # Build RNN cell\n    encoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n    \n    encoder_cell = tf.contrib.rnn.DropoutWrapper(cell=encoder_cell, output_keep_prob=0.75)\n    # Run Dynamic RNN\n    #   encoder_outputs: [max_time, batch_size, num_units]\n    #   encoder_state: [batch_size, num_units]\n    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n        encoder_cell, encoder_emb_inp,\n        sequence_length=x_len, time_major=False,dtype=tf.float32)\n    \n    model_logistic = tf.layers.dense(encoder_state[0],1)\n    model_pred = tf.nn.sigmoid(model_logistic)\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y,tf.float32),logits=tf.reshape(model_logistic,(-1,)))\n    loss = tf.reduce_mean(loss)\n    optimizer = tf.train.AdamOptimizer().minimize(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"74a4124e19c9e7524cde64508511ee8f5ec09b2b"},"cell_type":"code","source":"session.run(tf.global_variables_initializer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b53bd15237a2dc4bef9bcb284d8dbfc7ff5b771f"},"cell_type":"code","source":"losses = []\nbeginning_lr = 0.1\ngen = data_generator(batch_size)\nfor one_epoch in range(0,1):\n    pb = ProgressBar(worksum=len(data_X_in))\n    pb.startjob()\n    for one_batch in range(0,len(data_X_in),batch_size):\n        batch_x,batch_y = gen.__next__()\n        batch_x_len = np.asarray([len(i) for i in batch_x])\n        batch_lr = beginning_lr \n        \n        _,batch_loss = session.run([optimizer,loss],feed_dict={\n            x:batch_x,\n            y:batch_y,\n            x_len:batch_x_len,\n            learning_rate:batch_lr,\n        })\n        pb.info = \"EPOCH {} batch {} lr {} loss {}\".format(one_epoch,one_batch,batch_lr,batch_loss)\n        pb.complete(batch_size)\n        losses.append(batch_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30c0b8bba46ded3937200fd74b20381d0232c39a"},"cell_type":"code","source":"pd.DataFrame(losses).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d0dc8b75d6db550a46b98701582f1c859a79d479"},"cell_type":"code","source":"def predict_result(sent):\n    words = [j.lower() for j in row['sentence'].split()]\n    senttoken = [word2index.get(word,word2index['UNK']) for word in words]\n    inputx = np.asarray([senttoken])\n    inputx_len = np.asarray([len(senttoken)])\n    batch_predict = session.run(model_pred,feed_dict={\n            x:inputx,\n            x_len:inputx_len,\n        })[0]\n    return 1 if batch_predict > 0.5 else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1af04abba0fc75ec9b4b57ac0a40eb0e0fa9bd76"},"cell_type":"code","source":"labels = []\nfor index, row in data_test.iterrows():\n    label = predict_result(row['sentence'])\n    labels.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a37c1252d2c739d18d7310365252770fb5329852"},"cell_type":"code","source":"print(len(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e5c6e1737293cf0e0d6b8518cd4570d862ecc67"},"cell_type":"code","source":"COLUMN_NAMES = ['sentence', 'label']\nsolution_RNN2 = pd.DataFrame(columns=COLUMN_NAMES)\n\nsolution_RNN2['sentence'] = data_test['sentence']\nll = pd.Series(labels)\nsolution_RNN2['label'] = ll.values\nprint(solution_RNN2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b59e40a80eee5240b44657cac412bfe0913bc195"},"cell_type":"code","source":"solution_RNN2.to_csv('./solution_RNN2.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}