{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport cv2\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-28T13:35:31.893431Z","iopub.execute_input":"2021-05-28T13:35:31.893786Z","iopub.status.idle":"2021-05-28T13:35:36.596868Z","shell.execute_reply.started":"2021-05-28T13:35:31.89371Z","shell.execute_reply":"2021-05-28T13:35:36.596106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2021-05-28T12:54:04.490451Z","iopub.execute_input":"2021-05-28T12:54:04.490835Z","iopub.status.idle":"2021-05-28T12:54:04.498264Z","shell.execute_reply.started":"2021-05-28T12:54:04.490803Z","shell.execute_reply":"2021-05-28T12:54:04.49727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 256 #16 * tpu_strategy.num_replicas_in_sync","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vanilla_df=pd.read_csv('../input/chestxray8-dataframe/train_df.csv').drop(['Image Index','Patient ID'],axis=1) #.drop_duplicates('Patient ID','last')\n# if used drop duplicates then we can only work with those classes : Effusion,Infiltration, Mass, Nodule ,Atelectasis                                                        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vanilla_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df= vanilla_df[vanilla_df['No Finding'] !=1 ]\nillness_df['Normal']=illness_df['No Finding']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df= illness_df[ illness_df['Hernia'] !=1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df= illness_df[ illness_df['Pneumonia'] !=1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.drop(['No Finding','Hernia','Pneumonia'], inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vanilla_df.drop(['Hernia','Pneumonia'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vanilla_df['Normal']=vanilla_df['No Finding']\nvanilla_df.drop( ['No Finding'] , axis=1 , inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_df=vanilla_df[ vanilla_df['Normal'] ==1 ].loc[ 0:7200,:] # taking only 4000 images with normal conditions\nnormal_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effusion_df=vanilla_df[ vanilla_df['Effusion'] ==1 ].loc[ 0:40000,:] # taking only 4000 images with conditions\neffusion_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infiltration_df=vanilla_df[ vanilla_df['Infiltration'] ==1 ].loc[ 0:22000,:] # taking only 3200 images with conditions\ninfiltration_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"atelectasis_df=vanilla_df[ vanilla_df['Atelectasis'] ==1 ].loc[ 0:33000,:] # taking only 3100 images with normal conditions\natelectasis_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.drop(index= illness_df[illness_df['Effusion']==1].index , axis=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## thats what we want now, to remove the excess rows\n","metadata":{}},{"cell_type":"code","source":"illness_df.drop(index= illness_df[illness_df['Infiltration']==1].index , axis=0, inplace=True)\nillness_df.drop(index= illness_df[illness_df['Atelectasis']==1].index , axis=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"illness_df.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_df= illness_df.append([normal_df,atelectasis_df,infiltration_df,effusion_df])\npath=balanced_df['FilePath']\nbalanced_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col= ['Cardiomegaly','Emphysema','Effusion','Infiltration',\n      'Mass','Nodule','Atelectasis','Pneumothorax',\n      'Pleural_Thickening','Fibrosis','Edema','Consolidation','Normal']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_df.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_df.drop_duplicates('FilePath', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_df.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Great now that our dataset is kind of balanced, we can proceed","metadata":{}},{"cell_type":"code","source":"balanced_df= balanced_df.sample(frac=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## reducing the batch size as SGD consumes a large chunk of memo","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE=64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_gen= tf.keras.preprocessing.image.ImageDataGenerator(\n                                                            samplewise_center=True,\n                                                            samplewise_std_normalization=True,\n                                                            rotation_range=0.2,\n                                                            zca_whitening=True,\n                                                            width_shift_range=0.1,\n                                                            height_shift_range=0.1,\n                                                            shear_range=0.0,\n                                                            zoom_range=0.2,\n                                                            horizontal_flip=True,\n                                                            rescale=1/255.,\n                                                            validation_split=0.1)\n\n\ntrain_data= tmp_gen.flow_from_dataframe(  dataframe= balanced_df ,\n                                          directory= None ,\n                                          x_col='FilePath' ,\n                                          y_col= col ,\n                                          class_mode=\"raw\" ,\n                                          batch_size= BATCH_SIZE ,\n                                          shuffle= True ,\n                                          target_size= (224,224),\n                                          subset=\"training\"\n                                       )\n\nval_data= tmp_gen.flow_from_dataframe(  dataframe= balanced_df ,\n                                         directory= None ,\n                                         x_col= 'FilePath' ,\n                                         y_col= col ,\n                                         class_mode= \"raw\" ,\n                                         batch_size= BATCH_SIZE ,\n                                         shuffle= True ,\n                                         target_size= (224,224),\n                                         subset= 'validation'\n                                      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.optimizers import Adam, RMSprop, Adadelta, Adagrad\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-05-28T13:36:06.883824Z","iopub.execute_input":"2021-05-28T13:36:06.884176Z","iopub.status.idle":"2021-05-28T13:36:07.321075Z","shell.execute_reply.started":"2021-05-28T13:36:06.884146Z","shell.execute_reply":"2021-05-28T13:36:07.32035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating index to class dictionary\nidx_class={i:c for i,c in enumerate(col)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_CAM(processed_image, actual_label, layer_name='conv2d_127'):\n    model_grad = Model( [model.inputs] ,   [model.get_layer(layer_name).output , model.output]  )\n    \n    with tf.GradientTape() as tape:\n        conv_output_values, predictions = model_grad(processed_image)\n\n        # watch the conv_output_values\n        tape.watch(conv_output_values)\n\n        ## Use binary cross entropy loss\n        ## actual_label is 0 if cat, 1 if dog\n        # get prediction probability of dog\n        # If model does well, \n        # pred_prob should be close to 0 if cat, close to 1 if dog\n        pred_prob = predictions[:,1] # [ batch , (cat_prob , dog_prob) ]\n        # we tale only one prbability to be able to use binary_crossentropy_loss not sparse_categorical_loss\n        \n        # make sure actual_label is a float, like the rest of the loss calculation\n        actual_label = tf.cast( actual_label , dtype=tf.float32 )\n        \n        # add a tiny value to avoid log of 0\n        smoothing = 0.00001 \n        \n        # Calculate loss as binary cross entropy\n        # we can use tf.keras in that too\n        # bce = tf.keras.losses.BinaryCrossentropy()\n        # bce(y_true, y_pred).numpy()\n\n\n        loss = -1 * ( actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing) )\n        print(f\"binary loss: {loss}\")\n    \n    # get the gradient of the loss with respect to the outputs of the last conv layer\n    grads_values = tape.gradient(loss , conv_output_values)\n    grads_values = tf.keras.backend.mean(grads_values , axis=(0,1,2)) # mean over batch , hight , width --> num of channels\n    \n    conv_output_values = np.squeeze( conv_output_values.numpy() ) # will remove the 1 valued dimention which is the batch  --> (h , w )\n    grads_values = grads_values.numpy()\n    print(conv_output_values.shape)\n    # weight the convolution outputs with the computed gradients\n    for i in range(128): # num of filter channels\n        conv_output_values[ : , : , i ] *= grads_values[i] # multiply the gradient of the channels by the channels values\n    heatmap = np.mean(conv_output_values, axis=-1)# taking the mean over the channels , --> ( h , w )\n    \n    heatmap = np.maximum(heatmap, 0) # taking only the positive values\n    heatmap /= heatmap.max()# regularizing the pixel values\n    \n    del model_grad, conv_output_values, grads_values, loss\n   \n    return heatmap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_sample():\n    \n\n    images, labels= next(val_data)\n    sample_image = images[0]  # batch 0 so that returns ( h , w , c) for the image, without the batch dimention\n    sample_label = labels[0] # takes batch of xs and ys # x= train_data.next() -> x[0].shape -> 32,224,224,3\n    \n    sample_image_processed = np.expand_dims(sample_image, axis=0) # adding back the batch dimention\n    \n    activations = vis_model.predict(sample_image_processed) # the output of each layer -features-\n    \n    pred_label = np.argmax( model.predict(sample_image_processed) , axis=-1 )[0]\n    pred_label = idx_class[pred_label]\n    \n    print(activations[0].shape)\n    sample_activation = activations[0] [0 , : , : , -1] # taking the first output , for image of batch 0, and for the last layer #16 , --> (h,w)\n    \n    sample_activation-=sample_activation.mean()\n    sample_activation/=sample_activation.std()\n    \n    sample_activation *=255\n    sample_activation = np.clip( sample_activation , 0 , 255 ).astype(np.uint8)\n    \n    heatmap = get_CAM(sample_image_processed , sample_label )\n    heatmap = cv2.resize( heatmap, ( sample_image.shape[0], sample_image.shape[1 ]) )\n    heatmap = heatmap *255\n    heatmap = np.clip( heatmap , 0 , 255 ).astype(np.uint8)\n    heatmap = cv2.applyColorMap( heatmap , cv2.COLORMAP_HOT )\n    converted_img = sample_image\n    super_imposed_image = cv2.addWeighted( converted_img, 0.8, heatmap.astype('float32'), 2e-3, 0.0 )\n    \n    sample_label = idx_class[np.argmax(sample_label)]\n    \n    f,ax = plt.subplots(2,2, figsize=(15,8))\n\n    ax[0,0].imshow(sample_image)\n    ax[0,0].set_title(f\"True label: {sample_label} \\n Predicted label: {pred_label}\")\n    ax[0,0].axis('off')\n    \n    ax[0,1].imshow(sample_activation)\n    ax[0,1].set_title(\"Random feature map\")\n    ax[0,1].axis('off')\n    \n    ax[1,0].imshow(heatmap)\n    ax[1,0].set_title(\"Class Activation Map\")\n    ax[1,0].axis('off')\n    \n    ax[1,1].imshow(super_imposed_image)\n    ax[1,1].set_title(\"Activation map superimposed\")\n    ax[1,1].axis('off')\n    plt.tight_layout()\n    plt.show()\n  \n    return activations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## As we can see i'm only refining the last layer only, 14k params to train","metadata":{}},{"cell_type":"code","source":"def build_model():\n  # load the base VGG16 model\n  base_model = load_model('../input/chet-xray-encoder-model/encoder_model.h5')\n  \n  # build on top of AE\n  #output = layers.GlobalAveragePooling2D()(base_model.output)\n  output=layers.Flatten()(base_model.output)\n  output = layers.BatchNormalization()(output)\n  output = layers.Dense(64, activation='relu')(output)\n  output = layers.Dropout(0.4)(output)\n#   output = layers.Dense(32, activation='relu')(output)\n#   output = layers.BatchNormalization()(output)\n  output = layers.Dense( len(col) , activation='sigmoid')(output)\n\n  # set the inputs and outputs of the model\n  model = Model( base_model.input , output )\n\n  # freeze the earlier layers and leave the last 4 layers to train\n    \n  for layer in base_model.layers[:]:\n       layer.trainable=False\n\n  # choose the optimizer\n  #optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  # configure the model for training\n\n  model.compile(loss='binary_crossentropy', \n                optimizer= 'adam',#RMSprop( 0.001 , momentum=0.98 ), #Adam(0.004 ), #Adadelta(),\n                metrics=[tf.keras.metrics.AUC()])\n  \n  # display the summary\n  model.summary()\n  \n  return model\n\nmodel=build_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***let's Plot the outputs before model training***","metadata":{}},{"cell_type":"code","source":"# select all the layers for which you want to visualize the outputs and store it in a list\noutputs = [ layer.output for layer in model.layers[1:] ] # all layers except the input layer\n\n# Define a new model that generates the above output\nvis_model = Model(model.input , outputs)\n\n# store the layer names we are interested in\nlayer_names = []\nfor layer in outputs:\n    layer_names.append( layer.name.split(\"/\")[0] )\n\n    \nprint(\"Layers that will be used for visualization: \")\nprint(layer_names)\n# Choose an image index to show, or leave it as None to get a random image\nactivations = show_sample()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"# defining our call backs\ncb= tf.keras.callbacks.ModelCheckpoint( \"my_model.h5\" , save_best_only=True  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history= model.fit( train_data , validation_data= val_data , \n                    epochs= 50 , callbacks= [cb] \n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using a non completed training model to continue the training","metadata":{}},{"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\nmodel = load_model('../input/nih-13classes-pretrained-model/13-class-model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T13:36:27.924922Z","iopub.execute_input":"2021-05-28T13:36:27.925265Z","iopub.status.idle":"2021-05-28T13:36:30.984532Z","shell.execute_reply.started":"2021-05-28T13:36:27.925236Z","shell.execute_reply":"2021-05-28T13:36:30.983744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history= model.fit( train_data , validation_data= val_data , \n                    epochs= 15 , callbacks= [cb] \n                  )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1)\n\nfig.suptitle('Train vs Valid')\n\nax1.plot(range(12), model.history.history['loss'],color='b', label='loss')\nax1.plot(range(12), model.history.history['val_loss'],color='r', label='val_loss')\nax1.set_ylabel('Loss')\n\n\nax2.plot(range(12), model.history.history['auc'], label='auc')\nax2.plot(range(12), model.history.history['val_auc'], label='val_auc')\nax2.set_ylabel('auc')\n\nax2.set_xlabel('Epochs')\n\n\nax2.set_ylim([0,1])\nax1.set_ylim([0,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's train all the layers to see an improvements\n","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for l in model.layers :\n    l.trainable= True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history= model.fit( train_data , validation_data= val_data , \n                    epochs= 15 , callbacks= [cb] \n                  )","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd_model= Model(inputs= model.inputs , outputs= model.output )\n\nsgd_model.compile(loss='binary_crossentropy', \n                optimizer= tf.keras.optimizers.SGD(0.005 , 0.9) ,#RMSprop( 0.001 , momentum=0.98 ), #Adam(0.004 ), #Adadelta(),\n                metrics=[tf.keras.metrics.AUC() , 'accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.Model.save(sgd_model, './my_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history= sgd_model.fit( train_data , validation_data= val_data , \n                    epochs= 20 , callbacks= [cb] \n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.Model.save(sgd_model, './my_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting after returning to train back and forth, which will show nothing useful ","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1)\n\nfig.suptitle('Train vs Valid')\n\nax1.plot(range(12), model.history.history['loss'],color='b', label='loss')\nax1.plot(range(12), model.history.history['val_loss'],color='r', label='val_loss')\nax1.set_ylabel('Loss')\n\n\nax2.plot(range(12), model.history.history['auc'], label='auc')\nax2.plot(range(12), model.history.history['val_auc'], label='val_auc')\nax2.set_ylabel('auc')\n\nax2.set_xlabel('Epochs')\n\n\nax2.set_ylim([0,1])\nax1.set_ylim([0,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select all the layers for which you want to visualize the outputs and store it in a list\noutputs = [ layer.output for layer in model.layers[1:] ] # all layers except the input layer\n\n# Define a new model that generates the above output\nvis_model = Model(model.input , outputs)\n\n# store the layer names we are interested in\nlayer_names = []\nfor layer in outputs:\n    layer_names.append( layer.name.split(\"/\")[0] )\n\n    \nprint(\"Layers that will be used for visualization: \")\nprint(layer_names)\n# Choose an image index to show, or leave it as None to get a random image\nactivations = show_sample()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport sklearn\ntest_generator= val_data.next()\ny_predict =np.argmax( model.predict(test_generator[0]),axis=1)\n\n#tn, fp, fn, tp = np.max( confusion_matrix( test_generator.labels , y_predict ) , axis=1)\nmatrix=confusion_matrix(np.argmax(test_generator[1], axis=1) , y_predict)\nprint(matrix)\n# Confusion matrix Plotting\nimport seaborn as sns\n#classes=['covid', 'normal', 'pnumonia']\nsns.heatmap(matrix, annot=True, xticklabels=col, yticklabels=col ,cmap='Blues')#YlGnBu_r or Blues or twilight_shifted_r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\n#2- setting the path of the image\npath='../input/nawwar/1.jpeg'\n#3- uploading the image into a variable\n\nimg= image.load_img( path , target_size=( 224,224 ) )\n# don't forget the target size the model is expecting\n#4- processing the image variable to suit the model\n\nx= image.img_to_array( img )\nx= np.expand_dims( x , axis=0 )\nimages= np.vstack( [x] )\n\nplt.imshow(img) # to show the image\n# to predict the image\nprint('Class is: ', idx_class[np.argmax(model.predict(x))] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classify(image):\n    \n    sample_image = image  # batch 0 so that returns ( h , w , c) for the image, without the batch dimention\n    #sample_label = label # takes batch of xs and ys # x= train_data.next() -> x[0].shape -> 32,224,224,3\n    \n    sample_image_processed = np.expand_dims(sample_image, axis=0) # adding back the batch dimention\n    \n    activations = vis_model.predict(sample_image_processed) # the output of each layer -features-\n    \n    pred_label = np.argmax( model.predict(sample_image_processed) , axis=-1 )[0]\n    pred_label = idx_class[pred_label]\n    \n    print(activations[0].shape)\n    sample_activation = activations[0] [0 , : , : , :3] # taking the first output , for image of batch 0, and for the last layer #16 , --> (h,w)\n    \n    sample_activation-=sample_activation.mean()\n    sample_activation/=sample_activation.std()\n    \n    sample_activation *=255\n    sample_activation = np.clip( sample_activation , 0 , 255 ).astype(np.uint8)\n    \n    f,ax = plt.subplots(1,2, figsize=(15,8))\n\n    ax[0].imshow(sample_image)\n    ax[0].set_title(f\"Predicted label: {pred_label}\")\n    ax[0].axis('off')\n    \n    ax[1].imshow(sample_activation)\n    ax[1].set_title(\"Random feature map\")\n    ax[1].axis('off')\n \n    plt.tight_layout()\n    plt.show()\n  \n    return activations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\n#2- setting the path of the image\npath='../input/nawwar/1.jpeg'\n#3- uploading the image into a variable\n\nimg= image.load_img( path , target_size=( 224,224 ) )\n# don't forget the target size the model is expecting\n#4- processing the image variable to suit the model\n\nx= image.img_to_array( img )\n\nc=classify(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lite_model=tf.lite.TFLiteConverter.from_keras_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T13:39:15.723221Z","iopub.execute_input":"2021-05-28T13:39:15.72354Z","iopub.status.idle":"2021-05-28T13:39:15.727525Z","shell.execute_reply.started":"2021-05-28T13:39:15.72351Z","shell.execute_reply":"2021-05-28T13:39:15.726597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lite_model","metadata":{"execution":{"iopub.status.busy":"2021-05-28T13:39:20.692325Z","iopub.execute_input":"2021-05-28T13:39:20.692636Z","iopub.status.idle":"2021-05-28T13:39:20.70208Z","shell.execute_reply.started":"2021-05-28T13:39:20.692607Z","shell.execute_reply":"2021-05-28T13:39:20.700928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tflite_convert  --keras_model_file=../input/nih-13classes-pretrained-model/13-class-model.h5  --output_file=./litemodel.tflite","metadata":{"execution":{"iopub.status.busy":"2021-05-28T13:49:59.548855Z","iopub.execute_input":"2021-05-28T13:49:59.549219Z","iopub.status.idle":"2021-05-28T13:50:07.35632Z","shell.execute_reply.started":"2021-05-28T13:49:59.549187Z","shell.execute_reply":"2021-05-28T13:50:07.355339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}