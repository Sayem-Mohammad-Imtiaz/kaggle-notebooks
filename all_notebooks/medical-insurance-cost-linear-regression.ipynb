{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Medical Insurance Cost\n## Using Linear Regression model","metadata":{}},{"cell_type":"markdown","source":"### I. Aim:\n\nOur aim here is to predict the medical insurance cost of individuals using various parameters like\n    <ul>\n        <li>age - of the primary beneficiary</li>\n        <li>sex - male or female</li>\n        <li>bmi - a measure of the health condition for given height and weight</li>\n        <li>children - number of dependents covered by the insurance</li>\n        <li>smoker - yes or no</li>\n        <li>region - region in US namely northeast, northwest, southeast and southwest</li>\n    </ul>\n\n### II. Model:\nTowards this end we will be using the Linear regression model and test its suitability in this context.     ","metadata":{}},{"cell_type":"markdown","source":"### III. Imports:","metadata":{}},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.382441Z","iopub.execute_input":"2021-06-02T17:23:36.382889Z","iopub.status.idle":"2021-06-02T17:23:36.388784Z","shell.execute_reply.started":"2021-06-02T17:23:36.382834Z","shell.execute_reply":"2021-06-02T17:23:36.387764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.390878Z","iopub.execute_input":"2021-06-02T17:23:36.391199Z","iopub.status.idle":"2021-06-02T17:23:36.402466Z","shell.execute_reply.started":"2021-06-02T17:23:36.391167Z","shell.execute_reply":"2021-06-02T17:23:36.401403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### IV. Loading the data","metadata":{}},{"cell_type":"code","source":"MedicalInsurance = pd.read_csv('../input/insurance/insurance.csv')\nMedicalInsurance.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.403816Z","iopub.execute_input":"2021-06-02T17:23:36.404134Z","iopub.status.idle":"2021-06-02T17:23:36.436302Z","shell.execute_reply.started":"2021-06-02T17:23:36.404104Z","shell.execute_reply":"2021-06-02T17:23:36.435191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### V. Splitting the dataset into training and test sets\n\nHere based on the training set we will train our model and also take various model tuning decisions and hence it will comprise of both the training and validation sets down the line. The test set here on the other hand is meant only for a final verdict as to how good our output performs (in terms of confidence intervals say) in real world. In a way it's like a meausure of the user experience once an application has been delivered.","metadata":{}},{"cell_type":"code","source":"TrainSet, TestSet = train_test_split(MedicalInsurance, test_size=0.25)\nprint('Size of Training set:', len(TrainSet))\nprint('Size of Test set:', len(TestSet))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.439198Z","iopub.execute_input":"2021-06-02T17:23:36.439515Z","iopub.status.idle":"2021-06-02T17:23:36.44813Z","shell.execute_reply.started":"2021-06-02T17:23:36.439484Z","shell.execute_reply":"2021-06-02T17:23:36.446201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VI. Are there any null values?","metadata":{}},{"cell_type":"code","source":"MedicalInsurance.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.450032Z","iopub.execute_input":"2021-06-02T17:23:36.45035Z","iopub.status.idle":"2021-06-02T17:23:36.463826Z","shell.execute_reply.started":"2021-06-02T17:23:36.450318Z","shell.execute_reply":"2021-06-02T17:23:36.462741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So good news, no null entry is there to worry about.**","metadata":{}},{"cell_type":"markdown","source":"### VII. A look at the categorical features","metadata":{}},{"cell_type":"markdown","source":"The training set will further be divided into training and validation set. Prior to that we will look at the categorical variables in the data set to identify how many different categories/classes per feature is there and what is the relative frequency of each class.","metadata":{}},{"cell_type":"code","source":"# A plot to show the category-wise data distribution for categorical features \nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(14,5))\n\nTrainSet['sex'].value_counts(normalize=True).plot(kind='bar',ax=ax1,title='sex',color=['b','g'],alpha=0.7)\nTrainSet['smoker'].value_counts(normalize=True).plot(kind='bar',ax=ax2,title='smoker',color=['brown','violet'],alpha=0.7)\nTrainSet['region'].value_counts(normalize=True).plot(kind='bar',ax=ax3,title='region',color=['crimson','cyan','orange','indianred'],alpha=0.7);","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.46588Z","iopub.execute_input":"2021-06-02T17:23:36.466191Z","iopub.status.idle":"2021-06-02T17:23:36.922579Z","shell.execute_reply.started":"2021-06-02T17:23:36.466162Z","shell.execute_reply":"2021-06-02T17:23:36.921057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question: For features where the distribution of data is very imbalanced among the possible feature values, leaving the training-validation set split (or training-test split for that matter) to random splitting might lead to even more drastic imbalances. For example, in the above scenario the data count for *smoker=yes* is significantly lower than *smoker=no*. If it is too low, then during the split it might happen that one of the splits (training/test/validation) ends up with no or too low *smoker=yes* records compared to the other splits. In such scenarios we need to stratify during splitting. This will be checked later if such issues have occured and what effect they have had. In fact if such issues do really occur then this sort of distribution study needs to be done for all features in general and corrective measures (like stratifying) taken for them. For large number of features though this might be tedious and impractical. This trade-off discussion needs to be taken up in detail later.**\n","metadata":{}},{"cell_type":"markdown","source":"So we will use -\n<ul>\n    <li>sex - 1-level encoder with male=0, female=1</li>\n    <li>smoker - 1-level encoder with no=0, yes=1</li>\n    <li>region - 4-level one-hot encoder (or 3-level <i>because there are no regions outside the 4</i>)</li>\n    <ol>\n        <li>northeast - 1000</li>\n        <li>northwest - 0100</li>\n        <li>southeast - 0010</li>\n        <li>southwest - 0001</li>\n    </ol>\n    <b>Question: Which is better 4-level or 3-level encoding?</b>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"### VIII. Breaking the training set into training and validation sets","metadata":{}},{"cell_type":"code","source":"FinalTrainSet,ValSet = train_test_split(TrainSet,test_size=0.25)\nFinalTrainSet.index = np.arange(len(FinalTrainSet))\nValSet.index =np.arange(len(ValSet))\nprint('Final Training set size:', len(FinalTrainSet))\nprint('Validation set size:', len(ValSet))\nX_train,y_train = FinalTrainSet.iloc[:,:-1],FinalTrainSet.iloc[:,-1]\nX_val,y_val = ValSet.iloc[:,:-1],ValSet.iloc[:,-1]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.924563Z","iopub.execute_input":"2021-06-02T17:23:36.925077Z","iopub.status.idle":"2021-06-02T17:23:36.938682Z","shell.execute_reply.started":"2021-06-02T17:23:36.925024Z","shell.execute_reply":"2021-06-02T17:23:36.937474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A plot to show the category-wise data distribution for categorical features \nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(8,5))\n\nFinalTrainSet['smoker'].value_counts(normalize=True).plot(kind='bar',ax=ax1,title='Training set smoker distribution',color=['brown','violet'],alpha=0.7)\n\nValSet['smoker'].value_counts(normalize=True).plot(kind='bar',ax=ax2,title='Validation set smoker distribution',color=['brown','violet'],alpha=0.7);","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:36.940384Z","iopub.execute_input":"2021-06-02T17:23:36.941032Z","iopub.status.idle":"2021-06-02T17:23:37.197286Z","shell.execute_reply.started":"2021-06-02T17:23:36.94098Z","shell.execute_reply":"2021-06-02T17:23:37.196306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the feature about which we were worried has been tackled automatically and no further corrective measures needed.","metadata":{}},{"cell_type":"markdown","source":"### IX. Encoding the categoricals","metadata":{}},{"cell_type":"code","source":"# Encoding the sex and smoker columns\nsexSmokerEncoder = preprocessing.OrdinalEncoder(categories=[['male','female'],['no','yes']],dtype=int)\nX_train[['sex','smoker']] = sexSmokerEncoder.fit_transform(X_train[['sex','smoker']])\nX_train.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T17:23:37.200054Z","iopub.execute_input":"2021-06-02T17:23:37.200368Z","iopub.status.idle":"2021-06-02T17:23:37.21944Z","shell.execute_reply.started":"2021-06-02T17:23:37.200336Z","shell.execute_reply":"2021-06-02T17:23:37.218674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding the region column\nregionEncoder = preprocessing.OneHotEncoder(categories=[['northeast','northwest','southeast','southwest']],sparse=False,dtype=int)\nX_train[['NE','NW','SE','SW']] = regionEncoder.fit_transform(X_train[['region']])\nX_train.drop(columns=['region'],inplace=True)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.220486Z","iopub.execute_input":"2021-06-02T17:23:37.220931Z","iopub.status.idle":"2021-06-02T17:23:37.252685Z","shell.execute_reply.started":"2021-06-02T17:23:37.220835Z","shell.execute_reply":"2021-06-02T17:23:37.251983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similar transformations done for validation set as well\nX_val[['sex','smoker']] = sexSmokerEncoder.transform(X_val[['sex','smoker']])\nX_val[['NE','NW','SE','SW']] = regionEncoder.transform(X_val[['region']])\nX_val.drop(columns=['region'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.255224Z","iopub.execute_input":"2021-06-02T17:23:37.255633Z","iopub.status.idle":"2021-06-02T17:23:37.270699Z","shell.execute_reply.started":"2021-06-02T17:23:37.25559Z","shell.execute_reply":"2021-06-02T17:23:37.269738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### X. Fitting the Linear Regression model measuring its fit on validation set","metadata":{}},{"cell_type":"code","source":"linrgr = lin.LinearRegression()\nlinrgr.fit(X_train,y_train)\ndf = pd.DataFrame(columns=['Features','Coefficients'])\ndf['Features']=X_train.columns\ndf['Coefficients']=linrgr.coef_\ndf.loc[len(df)]=['intercept',linrgr.intercept_]\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.272136Z","iopub.execute_input":"2021-06-02T17:23:37.272535Z","iopub.status.idle":"2021-06-02T17:23:37.319914Z","shell.execute_reply.started":"2021-06-02T17:23:37.272505Z","shell.execute_reply":"2021-06-02T17:23:37.318741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point the coefficients are displayed for the sake of display and they do not make much sense on their own.","metadata":{}},{"cell_type":"code","source":"trainfitscore = np.around(linrgr.score(X_train,y_train),3)\ntrainMAE = np.around(metrics.mean_absolute_error(y_true=y_train,y_pred=linrgr.predict(X_train)),3)\nvalfitscore = np.around(linrgr.score(X_val,y_val),3)\nvalMAE = np.around(metrics.mean_absolute_error(y_true=y_val,y_pred=linrgr.predict(X_val)),3)\n\nprint('Training set R-squared:', trainfitscore)\nprint('Validation set R-squared:', valfitscore)\nprint('\\n\\nTraining set MAE:', trainMAE)\nprint('Validation set mean absolute error percentage:', valMAE)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.321104Z","iopub.execute_input":"2021-06-02T17:23:37.321376Z","iopub.status.idle":"2021-06-02T17:23:37.341364Z","shell.execute_reply.started":"2021-06-02T17:23:37.321349Z","shell.execute_reply":"2021-06-02T17:23:37.340363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: Everytime we do some tuning we will require to retrain the Linear Regression model and evaluate its fit to training and validation sets. Then we will see if there was some improvement obtained due to the tuning. To streamline this process we will just define a function that -\n<ol>\n    <li>will take in the training set and validation set</li>\n    <li>run plain linear regression on it</li>\n    <li>measure and record the fit to the training and validation set respectively along with the reason for the retraining</li>\n</ol>","metadata":{}},{"cell_type":"code","source":"# The table to recod the progression of the model fit for each reason or tuning\nmodelprogress = pd.DataFrame(columns=['Reason','Training R-squared','Validation R-squared','Training MAE', 'Validation MAE'])\n\ndef fitandrecord(X_train,y_train,X_val,y_val,reason,modelprogress,returncoefficients=False):\n    linrgr = lin.LinearRegression()\n    linrgr.fit(X_train,y_train)\n    trainfitscore = np.around(linrgr.score(X_train,y_train),3)\n    trainMAE = np.around(metrics.mean_absolute_error(y_true=y_train,y_pred=linrgr.predict(X_train)),3)\n    valfitscore = np.around(linrgr.score(X_val,y_val),3)\n    valMAE = np.around(metrics.mean_absolute_error(y_true=y_val,y_pred=linrgr.predict(X_val)),3)\n    modelprogress.loc[len(modelprogress)] = [reason,trainfitscore,valfitscore,trainMAE,valMAE]\n    \n    if returncoefficients:\n        df = pd.DataFrame(columns=['Features','Coefficients'])\n        df['Features']=X_train.columns\n        df['Coefficients']=linrgr.coef_\n        df.loc[len(df)]=['intercept',linrgr.intercept_]\n        return (linrgr,modelprogress,df)\n    return linrgr,modelprogress","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.34267Z","iopub.execute_input":"2021-06-02T17:23:37.343127Z","iopub.status.idle":"2021-06-02T17:23:37.356165Z","shell.execute_reply.started":"2021-06-02T17:23:37.34309Z","shell.execute_reply":"2021-06-02T17:23:37.355037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing the streamlining once\n\nmodel, modelprogress=fitandrecord(X_train,y_train,X_val,y_val,'Initial',modelprogress)\nmodelprogress","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.357698Z","iopub.execute_input":"2021-06-02T17:23:37.358125Z","iopub.status.idle":"2021-06-02T17:23:37.396055Z","shell.execute_reply.started":"2021-06-02T17:23:37.358074Z","shell.execute_reply":"2021-06-02T17:23:37.394782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XI. Looking for unwanted collinearity","metadata":{}},{"cell_type":"code","source":"# Displaying the correlation matrix\nfig, ax = plt.subplots()\ncax = ax.matshow(np.absolute(X_train.corr()), cmap='coolwarm')\nfig.colorbar(cax)\nax.set_xticks(np.arange(len(X_train.columns)))\nax.set_xticklabels(X_train.columns, rotation=45, ha='left')\nax.set_yticks(np.arange(len(X_train.columns)))\nax.set_yticklabels(X_train.columns)\nax.set_title('Correlation matrix');","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.397765Z","iopub.execute_input":"2021-06-02T17:23:37.398208Z","iopub.status.idle":"2021-06-02T17:23:37.760349Z","shell.execute_reply.started":"2021-06-02T17:23:37.398164Z","shell.execute_reply":"2021-06-02T17:23:37.75898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no appreciable correlation amongst any two features. However there might be between more than 2 features. For that we will need to measure the ***VIF (variance inflation factor)*** for each variable. It is essentially measuring how good one feature can be measured in terms of the others. Its measure is $\\frac{1}{1-{R^2}_{X_j|X_{j-1}}}$ where ${R^2}_{X_j|X_{j-1}}$ is the $R^2$ from the regression (I guess linear) of $X_j$ onto all other features. This is from *ISLR by James, Witten, Hastie, Tibshirani*.","metadata":{}},{"cell_type":"code","source":"def showmulticollinearity(X_train,y_train):\n    multicollinearitytable = pd.DataFrame(columns=['Features','VIF'], dtype='float64')\n    multicollinearitytable['Features'] = X_train.columns\n    for i, feature in enumerate(X_train.columns):\n        concernedFeature = X_train[feature]\n        remainingFeatures = X_train[X_train.columns.difference([feature])]\n        featureRegressor = lin.LinearRegression()\n        featureRegressor.fit(remainingFeatures,concernedFeature)\n        r_squared_score = featureRegressor.score(remainingFeatures,concernedFeature)\n        VIFscore = np.around(1/(1-r_squared_score),3)\n        multicollinearitytable.loc[i,'VIF'] = VIFscore\n    return multicollinearitytable\n\nshowmulticollinearity(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.762155Z","iopub.execute_input":"2021-06-02T17:23:37.762625Z","iopub.status.idle":"2021-06-02T17:23:37.842616Z","shell.execute_reply.started":"2021-06-02T17:23:37.762578Z","shell.execute_reply":"2021-06-02T17:23:37.841595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note: The 4-level encoding of region clearly produces exact collinearity amongst the 4-features. This introduces uncertainty in the model parameters and we might end up with bad fit in the test set. We hence remove one of the 4-features, say region_southwest, and see if that improves the training and validation set fit.**","metadata":{}},{"cell_type":"code","source":"X_traincollinearity = X_train.copy()\nX_traincollinearity.drop(columns=['SW'],inplace=True)\nX_valcollinearity = X_val.copy()\nX_valcollinearity.drop(columns=['SW'],inplace=True)\n\nshowmulticollinearity(X_traincollinearity,y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T17:23:37.843651Z","iopub.execute_input":"2021-06-02T17:23:37.843934Z","iopub.status.idle":"2021-06-02T17:23:37.909933Z","shell.execute_reply.started":"2021-06-02T17:23:37.843907Z","shell.execute_reply":"2021-06-02T17:23:37.908932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the issue of multicollinearity is solved; how about the fit?","metadata":{}},{"cell_type":"code","source":"model, modelprogress = fitandrecord(X_traincollinearity,y_train,X_valcollinearity,y_val,'Removing multicollinearity',modelprogress)\nmodelprogress","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T17:23:37.911185Z","iopub.execute_input":"2021-06-02T17:23:37.911722Z","iopub.status.idle":"2021-06-02T17:23:37.944484Z","shell.execute_reply.started":"2021-06-02T17:23:37.911678Z","shell.execute_reply":"2021-06-02T17:23:37.943432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Strange that the fit does not at all improve. Does multicollinearity not affect the fit at all?** See my blog <a href=\"https://einchako.medium.com/does-feature-collinearity-affect-model-fit-4618c5dc79ea\"> here </a> where I have investigated this issue further and reached a solution.","metadata":{}},{"cell_type":"code","source":"# Finalizing the decision to drop SW column\nX_train = X_traincollinearity\nX_val = X_valcollinearity","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.947909Z","iopub.execute_input":"2021-06-02T17:23:37.948336Z","iopub.status.idle":"2021-06-02T17:23:37.952683Z","shell.execute_reply.started":"2021-06-02T17:23:37.948306Z","shell.execute_reply":"2021-06-02T17:23:37.951705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XII.  Looking for any non-linearity","metadata":{}},{"cell_type":"markdown","source":"Our aim here will be to look at the residuals (studentized by expressing them as fraction of **mean absolute error MAE**) and check for any patterns in there. These patterns show some non-linearity not captured by our model.","metadata":{}},{"cell_type":"code","source":"y_trainpredict = model.predict(X_train)\ntrainMAE = modelprogress.loc[len(modelprogress.index)-1,'Training MAE']\ntrainresiduals = y_train-y_trainpredict\ntrainStudentizedResiduals = trainresiduals/trainMAE\n\nfig, ax = plt.subplots(figsize = (9,6))\nax.plot(y_train,trainStudentizedResiduals,'.g',label='Original responses')\nax.axhline(y=trainStudentizedResiduals.mean(),ls='--',c='r',label='Mean of studentized residuals')\nax.set_xlabel('Charges')\nax.set_ylabel('Studentized residuals')\nax.set_title('y vs residuals')\nax.legend();","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:37.954091Z","iopub.execute_input":"2021-06-02T17:23:37.95451Z","iopub.status.idle":"2021-06-02T17:23:38.197623Z","shell.execute_reply.started":"2021-06-02T17:23:37.954463Z","shell.execute_reply":"2021-06-02T17:23:38.196845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot above shows for lower y(charges) our linear fit predictions systematically overestimate, while at higher y values it underestimates. So we are tempted to introduce some form of second-degree non-linearity here.","metadata":{}},{"cell_type":"markdown","source":"### XIII. Adding non-linearity via polynomial regression","metadata":{}},{"cell_type":"code","source":"poly = preprocessing.PolynomialFeatures(degree=2)\n\nX_trainpolynomial = poly.fit_transform(X_train)[:,1:]\nX_valpolynomial = poly.transform(X_val)[:,1:]\n\nmodel, modelprogress = fitandrecord(X_trainpolynomial,y_train,X_valpolynomial,y_val,'2-degree polynomial fit',modelprogress)\nmodelprogress","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:38.198848Z","iopub.execute_input":"2021-06-02T17:23:38.199344Z","iopub.status.idle":"2021-06-02T17:23:38.268762Z","shell.execute_reply.started":"2021-06-02T17:23:38.199311Z","shell.execute_reply":"2021-06-02T17:23:38.267572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the studentized residuals once more","metadata":{}},{"cell_type":"code","source":"y_trainpredict = model.predict(X_trainpolynomial)\ntrainMAE = modelprogress.loc[len(modelprogress.index)-1,'Training MAE']\ntrainresiduals = y_train-y_trainpredict\ntrainStudentizedResiduals = trainresiduals/trainMAE\n\ny_valpredict = model.predict(X_valpolynomial)\nvalMAE = modelprogress.loc[len(modelprogress.index)-1,'Validation MAE']\nvalresiduals = y_val-y_valpredict\nvalStudentizedResiduals = valresiduals/valMAE\n\nfig, (ax1,ax2) = plt.subplots(2,1,figsize = (9,14))\n\nax1.plot(y_train,trainStudentizedResiduals,'.g',label='Original responses')\nax1.axhline(y=trainStudentizedResiduals.mean(),ls='--',c='r',label='Mean of studentized residuals')\nax1.set_xlabel('Charges')\nax1.set_ylabel('Studentized residuals')\nax1.set_title('Training set')\nax1.legend()\n\nax2.plot(y_val,valStudentizedResiduals,'.g',label='Original responses')\nax2.axhline(y=valStudentizedResiduals.mean(),ls='--',c='r',label='Mean of studentized residuals')\nax2.set_xlabel('Charges')\nax2.set_ylabel('Studentized residuals')\nax2.set_title('Validation set')\nax2.legend();","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:38.27056Z","iopub.execute_input":"2021-06-02T17:23:38.271029Z","iopub.status.idle":"2021-06-02T17:23:38.741429Z","shell.execute_reply.started":"2021-06-02T17:23:38.270981Z","shell.execute_reply":"2021-06-02T17:23:38.738973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still some non-linearity seems to be present and introducing a 3-degree polynomial may turn out to be worse as the number of features would go up to around 92 $\\left( {8 \\choose 1} + {8 \\choose 2} + {8 \\choose 3} \\right)$. Given the amount of training data we have, ~750 records, that is too many features. So we can stop here with Linear regression.","metadata":{}},{"cell_type":"markdown","source":"### XIV. How well does it then fit the final test set?","metadata":{}},{"cell_type":"code","source":"X_test,y_test = TestSet.iloc[:,:-1], TestSet.iloc[:,-1]\nX_test[['sex','smoker']] = sexSmokerEncoder.transform(X_test[['sex','smoker']])\nX_test[['NE','NW','SE','SW']] = regionEncoder.transform(X_test[['region']])\nX_test.drop(columns=['region','SW'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:38.742728Z","iopub.execute_input":"2021-06-02T17:23:38.743016Z","iopub.status.idle":"2021-06-02T17:23:38.760837Z","shell.execute_reply.started":"2021-06-02T17:23:38.742989Z","shell.execute_reply":"2021-06-02T17:23:38.759582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = poly.transform(X_test)[:,1:]\n\ntestscore = np.around(model.score(X_test,y_test),3)\ntestMAE = np.around(metrics.mean_absolute_error(y_true=y_test,y_pred=model.predict(X_test)),3)\n\nprint('Test fit:', testscore)\nprint('Test MAE:', testMAE)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:38.762409Z","iopub.execute_input":"2021-06-02T17:23:38.762818Z","iopub.status.idle":"2021-06-02T17:23:38.78143Z","shell.execute_reply.started":"2021-06-02T17:23:38.762774Z","shell.execute_reply":"2021-06-02T17:23:38.779975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelprogress","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:23:38.783255Z","iopub.execute_input":"2021-06-02T17:23:38.784035Z","iopub.status.idle":"2021-06-02T17:23:38.80992Z","shell.execute_reply.started":"2021-06-02T17:23:38.783952Z","shell.execute_reply":"2021-06-02T17:23:38.808623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The slightly larger differences in the fit scores of 2-3% between the training/validation/test sets instead of the more desired say ~1% is possibly due to the small number of samples although I cannot empirically establish it as yet.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}