{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import transformers\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom transformers import create_optimizer\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/preprocessed-tweets/precessed_test.csv')\ntrain_df = pd.read_csv('../input/preprocessed-tweets/processed_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop_duplicates(subset='tweet', keep='first', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth',200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['word_count'] = train_df.tweet.apply(lambda x:len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(4799, 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.word_count.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 30\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ninput_ids = tokenize_sentences(train_df['tweet'], tokenizer, MAX_LEN)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels =  train_df['label'].values\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n\ntrain_size = len(train_inputs)\nvalidation_size = len(validation_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = ['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertClassifier(tf.keras.Model):    \n    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        \n        super().__init__()\n        \n        self.bert = bert\n        \n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        \n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n        \n        cls_output = outputs[1]\n        \n        cls_output = self.classifier(cls_output)\n                \n        return cls_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(data_tuple, batch_size, train=True):\n    \n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    \n    dataset = dataset.repeat(1)\n    \n    if train:\n        \n        dataset = dataset.shuffle(buffer_size=10000)\n    \n    dataset = dataset.batch(batch_size)\n    \n    if train:\n        \n        dataset = dataset.prefetch(1)\n        \n    if not train:\n        \n        dataset = dataset.cache()\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(model, token_ids, masks, labels):\n    \n    labels = tf.dtypes.cast(labels, tf.float32)\n    \n    with tf.GradientTape() as tape:\n        \n        predictions = model(token_ids, attention_mask=masks)\n        \n        loss = loss_object(labels, predictions)\n    \n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    \n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), name = 'gradients')\n    \n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        \n        auc.update_state(labels[:,i], predictions[:,i])\n        \ndef validation_step(model, token_ids, masks, labels):\n    \n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    \n    v_loss = loss_object(labels, predictions)\n\n    validation_loss(v_loss)\n    \n    for i, auc in enumerate(validation_auc_metrics):\n        \n        auc.update_state(labels[:,i], predictions[:,i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\n\nTEST_BATCH_SIZE = 64\n\nNR_EPOCHS = 1\n\nMAX_LEN = 30 # try diffrent lengths\n\nthreshold = 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeds = [0 ,31 ,97,193,1001,83,42,456,21,237] # for ensembles\n\nseeds = [0]\n\nfor seed in range(len(seeds)):\n    \n    print('=' * 50, f\"CV {seed+1}\", '=' * 50)\n    \n    model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))\n    \n    labels =  train_df[label_cols].values\n    \n    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=seed, test_size = 0.2)\n\n    train_masks, validation_masks = train_test_split(attention_masks, random_state=seed, test_size=0.2)\n\n    train_size = len(train_inputs)\n\n    validation_size = len(validation_inputs)\n\n\n    train_dataset = create_dataset((train_inputs, train_masks, train_labels), batch_size=BATCH_SIZE,train=True)\n\n    validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), batch_size=BATCH_SIZE,train=False)\n    \n    \n    steps_per_epoch = train_size // (BATCH_SIZE)\n\n    #  Loss Function\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\n\n    validation_loss = tf.keras.metrics.Mean(name='val_loss')\n\n    #  Optimizer (with 1-cycle-policy)\n    warmup_steps = steps_per_epoch // 3\n\n    total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\n\n    optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n    # Gradients\n    \n    gradients = 0\n    \n    #  Metrics\n    train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n    validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n\n    for epoch in range(NR_EPOCHS):\n\n        print('=' * 50, f\"EPOCH {epoch+1}\", '=' * 50)\n\n        start = time.time()\n\n\n        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(train_dataset)):\n\n            train_step(model, token_ids, masks, labels)\n\n            if batch_no % 100 == 0:\n\n                    print(f'\\nTrain Step: {batch_no}, Loss: {train_loss.result()}')\n\n                    for i, label_name in enumerate(label_cols):\n\n                        print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n\n                        train_auc_metrics[i].reset_states()\n\n        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(validation_dataset)):\n\n            validation_step(model, token_ids, masks, labels)\n\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n\n        for i, label_name in enumerate(label_cols):\n\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n\n            validation_auc_metrics[i].reset_states()\n\n        print('\\n')\n        \n#     probs = generate_class_probablities(model,test_dataset,test_steps)\n        \n#     submission.loc[:, label_cols] = probs\n        \n#     submission.to_csv('probs'+str(seed)+'.csv')\n        \n#     class_probs += probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}