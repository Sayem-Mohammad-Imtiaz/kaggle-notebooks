{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport math\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport mxnet as mx\nimport matplotlib.pyplot as plt\n\n\nclass Environment(object):\n    def __init__(self):\n        self.data = pd.read_csv('../input/digital-coin-trx-1hour-2019-2021/trx_1h_2019_2021.csv')\n        self.data.index = pd.to_datetime(self.data['time'])\n        self.data.drop(columns=['time'], inplace=True)\n        \n        self.data['pct_change'] = self.data['close'].pct_change()\n        self.data.fillna(0, inplace=True)\n\n        self.barpos = 0\n\n        self.buy_fee_rate = 0.0008\n        self.sell_fee_rate = 0.0008\n        self.order_money = 1000  # 一次买多钱\n\n        self.init = 10000\n        self.fund = self.init\n        self.position = 0\n        self.market_value = 0\n\n        self.total_profit = 0\n        self.day_profit = 0  # Reward of every step, can tunning\n\n    def reset(self):\n        self.barpos = 0\n\n        self.init = 10000\n        self.fund = self.init  # 现金\n        self.position = 0  # 仓\n        self.market_value = 0  # 总价值\n\n        self.total_profit = 0\n        self.day_profit = 0\n\n        observation = self.data.iloc[self.barpos].tolist()\n        observation.append(self.market_value)\n        observation.append(self.position)\n        observation.append(self.fund)\n        return observation\n\n    def step(self, action):\n        current_price = self.data['close'].iloc[self.barpos]\n        self.day_profit = self.position * current_price * self.data['pct_change'].iloc[self.barpos]\n        if action == 0:\n            if self.fund > self.order_money:\n                buy_order = math.floor(self.order_money / current_price / 100) * 100\n                self.fund -= buy_order * current_price\n\n                buy_fee = buy_order * self.buy_fee_rate\n                self.position += buy_order - buy_fee\n            #     print('buy:success')\n            # else:\n            #     print('buy:not enough fund')\n\n        elif action == 1:\n            if self.position * current_price > self.order_money:\n                sell_order = math.ceil(self.order_money / current_price / 100) * 100\n                self.position -= sell_order\n                sell_fee = sell_order * current_price * self.sell_fee_rate\n                self.fund += sell_order * current_price - sell_fee\n        #         print(\"sell:success\")\n        #     else:\n        #         print('sell:not enough stock')\n\n        # else:\n        #     print('keep still')\n\n        # 重新计算持仓状况，不考虑除权除息\n        self.market_value = self.position * current_price + self.fund\n        self.total_profit = self.market_value - self.init\n        self.barpos += 1\n\n        observation = self.data.iloc[self.barpos].tolist()\n        observation.append(self.market_value)\n        observation.append(self.position)\n        observation.append(self.fund)\n\n        return (observation,\n                self.day_profit,\n                True if self.barpos == self.data.shape[0] - 1 else False)\n\n\nclass DeepQNetwork(mx.gluon.nn.Block):\n    def __init__(self, input_dims, fc1_dims, fc2_dims, n_actions, learning_rate):\n        \"\"\"\n        n_actions: buy, sell, hold\n        \"\"\"\n        super(DeepQNetwork, self).__init__()\n        self.input_dims = input_dims\n        self.fc1_dims = fc1_dims\n        self.fc2_dims = fc2_dims\n        self.n_actions = n_actions\n        self.learning_rate = learning_rate\n\n        self.fc1 = mx.gluon.nn.Dense(self.fc1_dims, activation='relu')  # an affine operation: y = Wx + b\n        self.fc2 = mx.gluon.nn.Dense(self.fc2_dims, activation='relu')\n        self.fc3 = mx.gluon.nn.Dense(self.n_actions)\n\n    def init(self):\n        self.initialize(mx.init.Xavier())\n        self.optimizer = mx.gluon.Trainer(self.collect_params(), 'adam', {'learning_rate': self.learning_rate})\n        self.loss = mx.gluon.loss.L2Loss()\n\n    def forward(self, inputs):\n        return self.fc3(self.fc2(self.fc1(inputs)))\n\n\nclass Agent(object):\n    \"\"\"\n    gamma的折扣率它必须介于0和1之间。越大，折扣越小。意味着学习，agent 更关心长期奖励。另一方面，gamma越小，折扣越大。意味着 agent 更关心短期奖励。\n    epsilon探索率ϵ。即策略是以1−ϵ的概率选择当前最大价值的动作，以ϵ的概率随机选择新动作。\n    \"\"\"\n    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions=3,\n                 max_mem_size=1000000, eps_min=0.01, eps_dec=5e-4):\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.eps_min = eps_min\n        self.eps_dec = eps_dec\n        self.lr = lr\n        self.n_actions = n_actions\n        self.mem_size = max_mem_size\n        self.batch_size = batch_size\n        self.mem_cnt = 0\n\n        self.Q_eval = DeepQNetwork(input_dims, 256, 256, self.n_actions, self.lr)\n        self.Q_eval.init()\n\n        self.state_memory = np.zeros((self.mem_size, input_dims), dtype=np.float32)\n        self.new_state_memory = np.zeros((self.mem_size, input_dims), dtype=np.float32)\n\n        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)  # 存储是否结束的bool型变量\n\n    def save_model(self):\n        params = self.Q_eval._collect_params_with_prefix()\n        model = {key: val._reduce() for key, val in params.items()}\n        joblib.dump(model, 'DNN_Params.m')\n\n    def load_model(self):\n        model = joblib.load('DNN_Params.m')\n        params = self.Q_eval._collect_params_with_prefix()\n        for name in model:\n            params[name]._load_init(model[name], mx.cpu(), cast_dtype=False, dtype_source='current')\n\n    def choose_action(self, observation):\n        \"\"\" 1-epsilon的概率执行最大价值操作, epsilon概率执行随机动作.\n\n        observation, 状态state\n        \"\"\"\n        if np.random.random() > self.epsilon:\n            state = mx.nd.array([observation])  # (1, 10)\n            # 神经网络模型得到action的Q value vector\n            actions = self.Q_eval.forward(state)\n            action = int(mx.nd.argmax(actions).asscalar())\n        else:\n            action = np.random.choice(self.n_actions)\n            # print(\"random action:\", action)\n        return action\n\n    def store_transition(self, state, action, reward, state_, done):\n        \"\"\" 存储状态变化\n        \"\"\"\n        index = self.mem_cnt % self.mem_size\n        self.state_memory[index] = state\n        self.new_state_memory[index] = state_\n        self.reward_memory[index] = reward\n        self.action_memory[index] = action\n        self.terminal_memory[index] = done\n\n        self.mem_cnt += 1\n        # print(\"store_transition index:\", index)\n\n    def learn(self):\n        \"\"\" 从记忆中抽取batch进行学习\n        \"\"\"\n        # memory counter小于一个batch_size, 等待积累数据\n        if self.mem_cnt < self.batch_size:\n            # print(\"learn:watching\")\n            return\n\n        # 得到memory大小，不超过mem_size\n        max_mem = min(self.mem_cnt, self.mem_size)\n\n        # 随机生成一个batch的memory index，不可重复抽取\n        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n        batch_index = np.arange(self.batch_size, dtype=np.int32)\n\n        # 从state memory中抽取一个batch\n        state_batch = mx.nd.array(self.state_memory[batch])\n        new_state_batch = mx.nd.array(self.new_state_memory[batch])\n        reward_batch = mx.nd.array(self.reward_memory[batch])\n        action_batch = self.action_memory[batch]\n        terminal_batch = self.terminal_memory[batch]\n\n        with mx.autograd.record():\n            # batch_index所有行，action_batch列，state_batch经过神经网络前向传播，输出(64, 3)，3个action选已经做的action对应的值为Q值\n            q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n            q_next = self.Q_eval.forward(new_state_batch).asnumpy()  # (64, 10) -> (64, 3)\n            q_next[terminal_batch] = 0.0  # 如果是最终状态，则将q值置为0\n            q_target = reward_batch + mx.nd.array(self.gamma * np.max(q_next, axis=1))\n\n            loss = self.Q_eval.loss(q_target, q_eval)\n        loss.backward()\n        self.Q_eval.optimizer.step(self.batch_size)\n\n        self.epsilon = self.epsilon - self.eps_dec \\\n            if self.epsilon > self.eps_min else self.eps_min\n\n\ndef run_dqn():\n    environ = Environment()\n    agent = Agent(gamma=0.9, epsilon=0.5, lr=0.003, input_dims=10, batch_size=64, n_actions=3, eps_min=0.03)\n    profits, eps_history = [], []\n    epochs = 100\n\n    for i in range(epochs):\n        profit = 0\n        done = False\n        # can add env_list if have multiple stocks\n        observation = environ.reset()\n        while not done:\n            # as barpos increasing\n            action = agent.choose_action(observation)\n            observation_, reward, done = environ.step(action)\n            profit = environ.total_profit\n            agent.store_transition(observation, action, reward, observation_, done)\n            agent.learn()\n            observation = observation_\n\n        # 保存一下每局的收益，最后画个图\n        profits.append(profit)\n        eps_history.append(agent.epsilon)\n        avg_profits = np.mean(profits[-100:])\n\n        print('episode', i, 'profits %.2f' % profit,\n              'avg profits %.2f' % avg_profits,\n              'epsilon %.2f' % agent.epsilon)\n\n    agent.save_model()\n    with open('profits.txt', 'w') as fd:\n        json.dump(profits, fd)\n\n    x = [i + 1 for i in range(epochs)]\n    plt.plot(x, profits)\n\nrun_dqn()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}