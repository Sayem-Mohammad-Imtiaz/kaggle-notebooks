{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Building  an Abstractive Reviews summarizer with Seq2Seq Model.\n<img src = 'https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2016/02/AMAZON-1200x537.png'> "},{"metadata":{"trusted":true},"cell_type":"code","source":"from shutil import copyfile\ncopyfile(src = \"../input/attention/attention.py\", dst = \"../working/attention.py\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Fi64aA0FFxcS","trusted":true},"cell_type":"code","source":"from attention import AttentionLayer","execution_count":null,"outputs":[]},{"metadata":{"id":"JUValOzcHtEK"},"cell_type":"markdown","source":"###### Import the Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"_Jpu8qLEFxcY","outputId":"95968e01-faac-4911-c802-9c008a4e62cf","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport re\nimport os\nimport json\nimport pickle\nfrom tqdm import tqdm\nimport glob\nimport matplotlib.pyplot as plt\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed,Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\npd.set_option(\"display.max_colwidth\", 200)\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('/kaggle/input/amazon-music-reviews/')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_review = pd.read_csv('/kaggle/input/amazon-music-reviews/Musical_instruments_reviews.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_review.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_review.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_review.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_review.drop_duplicates(subset=['reviewText'],inplace=True)  #dropping duplicates\ndf_review.dropna(axis=0,inplace=True)   #dropping na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_review[['summary','reviewText']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"qi0xD6BkIWAm"},"cell_type":"markdown","source":"# Information about dataset\n\nLet us look at datatypes and shape of the dataset"},{"metadata":{"id":"__fy-JxTFxc9","outputId":"d42c6e36-bbc8-43c2-de0e-d3effe3e8c4c","trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = ['summary','text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=50, figure_size=(15.0,15.0), \n                   title = None, title_size=20, image_color=False,color = None):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(data['summary'].values, title=\"Word Cloud of  Amazon Reviews Summary\",color = 'black')\nplot_wordcloud(data['text'].values, title=\"Word Cloud of Amazon Review\",color = 'black')","execution_count":null,"outputs":[]},{"metadata":{"id":"r0xLYACiFxdJ"},"cell_type":"markdown","source":"# Text Data Preprocessing\n\nPerforming basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move"},{"metadata":{"_kg_hide-input":true,"id":"0s6IY-x2FxdL","trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","execution_count":null,"outputs":[]},{"metadata":{"id":"2JFRXFHmI7Mj"},"cell_type":"markdown","source":"We will perform the below preprocessing tasks for our data:\n\n - Convert everything to lowercase\n\n - Remove HTML tags\n\n - Contraction mapping\n\n - Remove (â€˜s)\n\n - Remove any text inside the parenthesis ( )\n\n - Eliminate punctuations and special characters\n\n -  Remove stopwords\n\n - Remove short words\n"},{"metadata":{"id":"XZr-u3OEFxdT","trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english')) \n\ndef text_cleaner(text,num):\n    newString = text.lower()\n    newString = BeautifulSoup(newString, \"lxml\").text\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString)\n    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    newString = re.sub('[m]{2,}', 'mm', newString)\n    if(num==0):\n        tokens = [w for w in newString.split() if not w in stop_words]\n    else:\n        tokens=newString.split()\n    long_words=[]\n    for i in tokens:\n        if len(i)>1:                                                 #removing short word\n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()","execution_count":null,"outputs":[]},{"metadata":{"id":"A2QAeCHWFxdY","trusted":true},"cell_type":"code","source":"#call the function\ncleaned_text = []\nfor t in data['text']:\n    cleaned_text.append(text_cleaner(t,0)) ","execution_count":null,"outputs":[]},{"metadata":{"id":"snRZY8wjLao2"},"cell_type":"markdown","source":"Let us look at the first five preprocessed text"},{"metadata":{"id":"NCAIkhWbFxdh","outputId":"c2da1a36-4488-4e32-ef9e-fcfe496e374d","trusted":true},"cell_type":"code","source":"cleaned_text[:5]  ","execution_count":null,"outputs":[]},{"metadata":{"id":"GsRXocxoFxd-","trusted":true},"cell_type":"code","source":"#call the function\ncleaned_summary = []\nfor t in data['summary']:\n    cleaned_summary.append(text_cleaner(t,0))","execution_count":null,"outputs":[]},{"metadata":{"id":"oZeD0gs6Lnb-"},"cell_type":"markdown","source":"Let us look at the first 10 preprocessed news titles"},{"metadata":{"id":"jQJdZcAzFxee","outputId":"a1fbe683-c03f-4afb-addf-e075021c121b","trusted":true},"cell_type":"code","source":"cleaned_summary[:10]","execution_count":null,"outputs":[]},{"metadata":{"id":"L1zLpnqsFxey","trusted":true},"cell_type":"code","source":"data['cleaned_text']=cleaned_text\ndata['cleaned_summary']=cleaned_summary","execution_count":null,"outputs":[]},{"metadata":{"id":"tR1uh8xSNUma"},"cell_type":"markdown","source":"Add the **START** and **END** special tokens at the beginning and end of the summary.Add tokens at begining and end of news title  \n"},{"metadata":{"id":"EwLUH78CFxhg","trusted":true},"cell_type":"code","source":"data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x : 'sostok '+ x + ' eostok')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_text_len=30\nmax_summary_len=8","execution_count":null,"outputs":[]},{"metadata":{"id":"1GlcX4RFOh13"},"cell_type":"markdown","source":"We are getting closer to the model building part. Before that, we need to split our dataset into a training and validation set. Weâ€™ll use 90% of the dataset as the training data and evaluate the performance on the remaining 10% (holdout set):"},{"metadata":{"id":"RakakKHcFxhl","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_tr,x_val,y_tr,y_val=train_test_split(np.array(data['cleaned_text']),np.array(data['cleaned_summary']),test_size=0.1,random_state=0,shuffle=True) ","execution_count":null,"outputs":[]},{"metadata":{"id":"oRHTgX6hFxhq","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\n\n#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_tr))","execution_count":null,"outputs":[]},{"metadata":{"id":"y8KronV2Fxhx","outputId":"d2eb2f27-fbbc-4e61-9556-3c3ff5e4327b","trusted":true},"cell_type":"code","source":"thresh=4\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in x_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq/tot_freq)*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"J2giEsF3Fxh3","trusted":true},"cell_type":"code","source":"#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum lengath\nx_tr    =   pad_sequences(x_tr_seq,maxlen=max_text_len, padding='post')\nx_val   =   pad_sequences(x_val_seq,maxlen=max_text_len, padding='post')\n\n#size of vocabulary ( +1 for padding token)\nx_voc   =  x_tokenizer.num_words + 1","execution_count":null,"outputs":[]},{"metadata":{"id":"eRHqyBkBFxiJ","trusted":true},"cell_type":"code","source":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_tr))","execution_count":null,"outputs":[]},{"metadata":{"id":"yzE5OiRLFxiM","outputId":"7f7a4f89-b088-4847-8172-09e5a2383d0e","trusted":true},"cell_type":"code","source":"thresh=6\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in y_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq/tot_freq)*100)","execution_count":null,"outputs":[]},{"metadata":{"id":"0PBhzKuRSw_9"},"cell_type":"markdown","source":"Let us define the tokenizer with top most common words for summary."},{"metadata":{"id":"-fswLvIgFxiR","trusted":true},"cell_type":"code","source":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \ny_tokenizer.fit_on_texts(list(y_tr))\n\n#convert text sequences into integer sequences\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n\n#size of vocabulary\ny_voc  =   y_tokenizer.num_words +1","execution_count":null,"outputs":[]},{"metadata":{"id":"qqwDUT5oTFmn"},"cell_type":"markdown","source":"Let us check whether word count of start token is equal to length of the training data"},{"metadata":{"id":"pR8IX9FRFxiY","outputId":"b116cdbd-42c4-4ede-9f6d-46284115393e","trusted":true},"cell_type":"code","source":"y_tokenizer.word_counts['sostok'],len(y_tr)","execution_count":null,"outputs":[]},{"metadata":{"id":"LVFhFVguTTtw"},"cell_type":"markdown","source":"Delete start and end tokens"},{"metadata":{"id":"kZ-vW82sFxih","trusted":true},"cell_type":"code","source":"ind=[]\nfor i in range(len(y_tr)):\n    cnt=0\n    for j in y_tr[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_tr=np.delete(y_tr,ind, axis=0)\nx_tr=np.delete(x_tr,ind, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"cx5NISuMFxik","trusted":true},"cell_type":"code","source":"ind=[]\nfor i in range(len(y_val)):\n    cnt=0\n    for j in y_val[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_val=np.delete(y_val,ind, axis=0)\nx_val=np.delete(x_val,ind, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"wOtlDcthFxip"},"cell_type":"markdown","source":"## NLP Model Building"},{"metadata":{"id":"zXef38nBFxir","outputId":"7ae99521-46f8-4c6f-9cba-4979deffeee8","trusted":true},"cell_type":"code","source":"from keras import backend as K \nK.clear_session()\n\nlatent_dim = 300\nembedding_dim=100\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n\n# Attention layer\nattn_layer = AttentionLayer(name='attention_layer')\nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n\n# Concat attention input and decoder LSTM output\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_concat_input)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.summary() ","execution_count":null,"outputs":[]},{"metadata":{"id":"Lwfi1Fm8Fxiz","trusted":true},"cell_type":"code","source":"#converts the integer sequence to a one-hot vector using sparse_categorical_crossentropy\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"id":"p0ykDbxfUhyw"},"cell_type":"markdown","source":"Early Stopping - Our model will stop training once the validation loss increases:"},{"metadata":{"id":"s-A3J92MUljB","trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)","execution_count":null,"outputs":[]},{"metadata":{"id":"ETnPzA4OFxi3","outputId":"477e374f-7cf2-4d60-f86e-2c49c9cebedb","trusted":true},"cell_type":"code","source":"history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))","execution_count":null,"outputs":[]},{"metadata":{"id":"tDTNLAURFxjE","outputId":"e2ea6e44-3931-4014-97a1-03fa2a441228","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"sBX0zZnOFxjW","trusted":true},"cell_type":"code","source":"reverse_target_word_index=y_tokenizer.index_word\nreverse_source_word_index=x_tokenizer.index_word\ntarget_word_index=y_tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"id":"eM_nU_VvFxjq"},"cell_type":"markdown","source":"## Model Inference\n\nInferencing Encoder and Decoder for getting predicted summary for title"},{"metadata":{"id":"9QkrNV-4Fxjt","trusted":true},"cell_type":"code","source":"# Encode the input sequence to get the feature vector\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n#attention inference\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_inf_concat) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","execution_count":null,"outputs":[]},{"metadata":{"id":"6f6TTFnBFxj6","trusted":true},"cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"id":"aAUntznIFxj9","trusted":true},"cell_type":"code","source":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","execution_count":null,"outputs":[]},{"metadata":{"id":"9gM4ALyfWwA9"},"cell_type":"markdown","source":"Result for Inference is shown below with News Text,original summary and predicted summary"},{"metadata":{"id":"BUtQmQTmFxkI","outputId":"f407d9fc-e0cd-4082-98f5-bd1f562dc26f","trusted":true},"cell_type":"code","source":"for i in range(0,100):\n    print(\"Amazon Review:\",seq2text(x_tr[i]))\n    print(\"Original Review summary:\",seq2summary(y_tr[i]))\n    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"R_qIecuvY5GT"},"cell_type":"markdown","source":"### References\n - [comprehensive-guide-text-summarization-using-deep-learning](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/)\n - [a-quick-introduction-to-text-summarization-in-machine-learning](https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f)\n - [Keras Text Summarizer](https://github.com/chen0040/keras-text-summarization)"},{"metadata":{},"cell_type":"markdown","source":"<font color='blue'>If you find this kernel awesome and useful please hit upvote ðŸ˜Š .</font> <br>\n<font color='blue'>Cheers!!!! </font>"}],"metadata":{"colab":{"collapsed_sections":[],"name":"How to build own text summarizer using deep learning.ipynb","provenance":[],"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}