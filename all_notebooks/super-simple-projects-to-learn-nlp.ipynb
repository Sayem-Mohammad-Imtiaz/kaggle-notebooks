{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sqlite3\nimport regex as re\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ham is Called 'not spam' so I changed it to '0' and spam is marked as '1' **"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#create dataframe from csv\ndf = pd.read_csv('/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv',encoding='ISO-8859-1')\ndf[\"Category\"].replace({'ham': '0','spam': '1'}, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Category'] = df['Category'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Analysis"},{"metadata":{},"cell_type":"markdown","source":"Before anything, it is best to do a quick analysis of the data to eliminate duplicate rows and establish some baseline counts. I use pandas drop_duplicates to drop the duplicate rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"spam count: \" +str(len(df.loc[df.Category==1])))\nprint(\"not spam count: \" +str(len(df.loc[df.Category==0])))\nprint(df.shape)\n\ndf = df.drop_duplicates()\ndf = df.reset_index(inplace = False)[['Category','Message']]\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Word Cloud"},{"metadata":{},"cell_type":"markdown","source":"**Word clouds are a useful way to visualize text data because they make understanding word frequencies easier. Words that appear more frequently within the email text appear larger in the cloud. Word Clouds make it easy to identify “key words.”**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_desc = []\nfor w in range(len(df.Message)):\n    desc = df['Message'][w].lower()\n    \n    #remove punctuation\n    desc = re.sub('[^a-zA-Z]', ' ', desc)\n    \n    #remove tags\n    desc=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",desc)\n    \n    #remove digits and special chars\n    desc=re.sub(\"(\\\\d|\\\\W)+\",\" \",desc)\n    \n    clean_desc.append(desc)\n#assign the cleaned descriptions to the data frame\ndf['Message'] = clean_desc\n        \ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = ['is','you','your','and', 'the', 'to', 'from', 'or', 'I', 'for', 'do', 'get', 'not', 'here', 'in', 'im', 'have', 'on', 're', 'new', 'subject']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, background_color = 'black', stopwords = stop_words, max_words = 1000, min_font_size = 20).generate(str(df['Message']))\n#plot the word cloud\nfig = plt.figure(figsize = (8,8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spam Detection"},{"metadata":{},"cell_type":"markdown","source":"**This is a binary classification problem since an email can either be spam (1) or not spam (0). I want to build a machine learneing model that can identify whether or not an email is spam. I am going to use the Python library Scikit-Learn to explore tokenization, vectorization, and statistical classification algorithms.**"},{"metadata":{},"cell_type":"markdown","source":"**Import Dependencies**"},{"metadata":{},"cell_type":"markdown","source":"Import the Scikit-Learn functionality we need to transform and model the data. I will use CountVectorizer, train_test_split, ensemble models, and a couple metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble \nfrom sklearn.metrics import classification_report, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of sentences\ntext = df.Message\n#instantiate the class\ncv = CountVectorizer()\n#tokenize and build vocab\ncv.fit(text)\nprint(cv.vocabulary_)\n#transform the text\nvector = cv.transform(text)\nprint(vector.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The CountVectorizer is counting the tokens and allowing me to construct the sparse matrix containing the transformed words to numbers.**"},{"metadata":{},"cell_type":"markdown","source":"Bag of Words Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_vec = CountVectorizer().fit_transform(df['Message'])\nX_train, X_test, y_train, y_test = train_test_split(text_vec, df['Category'], test_size = 0.90, random_state = 75, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Classifier**"},{"metadata":{},"cell_type":"markdown","source":"**I am using the GradientBoostingClassifier() model from the Scikit-Learn Ensemble collection.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = ensemble.GradientBoostingClassifier(\n    n_estimators = 500, #how many decision trees to build\n    learning_rate = 2.0, #learning rate\n    max_depth = 500\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generate Predictions**"},{"metadata":{},"cell_type":"markdown","source":"Finally, we fit the data, call predict and generate the classification report. Using classification_report(), it is easy to build a text report showing the main classification metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**our model achieved 95% accuracy**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}