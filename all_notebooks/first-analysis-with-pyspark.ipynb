{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#My first attempt at appyling what I've learned in Data Science to a Big Dataset. \n# As first dataset, I will load the most recent one and try to do some statistics out of it. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql.session import SparkSession","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = SparkContext()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession(sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol2018 = spark.read.csv('../input/parking-violations-issued-fiscal-year-2018.csv', header=True, inferSchema=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyspark.sql.functions as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_per_col = []\nfor c in park_viol2018.columns:\n    missing_per_col.append(park_viol2018.filter(F.isnull(park_viol2018[c])).count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By doing this, I was able to select which are the columns I'm interested in. \n#With a first check, I will drop columns whose nonzero elements are above a threshold, then I will \n#consider dropping rows. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_per_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I would like to zip column names and relative number of missing values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_missings = list(zip(park_viol2018.columns, missing_per_col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_missings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c, val in cols_missings:\n    if val > 1.6e6:\n        park_viol2018 = park_viol2018.drop(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol2018.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol2018_sampled = park_viol2018.sample(withReplacement=None, fraction=0.02)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import timeit\nstart_time = timeit.default_timer()\npark_viol2018_sampled_pd = park_viol2018_sampled.toPandas()\nelapsed = timeit.default_timer() - start_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elapsed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol2018_sampled_pd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol2018_sampled_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of samples should be more or less uniform; I will plot some basic statistics out of this data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol2018_sampled_pd['Registration State'].value_counts().loc[lambda x: x > 1000].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The distribution is pretty obvious, since the dataset comprehends data from NY!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's plot the vehicle years ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol2018_sampled_pd['Vehicle Year'].value_counts().loc[lambda x: x>100].plot(kind='bar', rot=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The majority of the vehicle years is not reported, and apart from those values we see that newer\n#vehicles dominate the scene","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's also first plot the distribution of the violation codes, then we'll go and see \n#what violation codes represent with the provided xls files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park_viol_code_group = park_viol2018_sampled_pd.groupby('Violation Code', as_index=True)\npark_viol_code_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes = park_viol_code_group['Violation Code'].count().sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Top 10 of the violation codes! ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try to read the .xlsx containing the codes directly.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"violation_codes_df = pd.read_excel('../input/ParkingViolationCodes Nov 2018.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"violation_codes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Great, I will try to perform a join on the most violated codes. First, let's give a look at it ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes_df = pd.DataFrame(most_violated_codes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes_df = most_violated_codes_df.rename({'Violation Code': 'Count'},axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes_with_desc =pd.merge(most_violated_codes_df,violation_codes_df,\n                                        left_on=most_violated_codes_df.index,\n                                        right_on='VIOLATION CODE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_violated_codes_with_desc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see that the most common violation in this sample is the 'no parking-street cleaning', followed \n# by 'fail to display muni meter receipt'.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As a last step of this first analysis I'll group by date and check day by day the amount of violations ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"viol_by_dates_df = park_viol2018_sampled_pd.groupby('Date First Observed', as_index=False).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"viol_by_dates_df = pd.DataFrame(viol_by_dates_df[['Date First Observed','Plate ID']]).rename({'Plate ID': 'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"viol_by_dates_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"viol_by_dates_df = viol_by_dates_df.iloc[2:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"viol_by_dates_df['Date First Observed'] = pd.to_datetime(viol_by_dates_df['Date First Observed'], \n                                                        format='%Y/%m/%d')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}