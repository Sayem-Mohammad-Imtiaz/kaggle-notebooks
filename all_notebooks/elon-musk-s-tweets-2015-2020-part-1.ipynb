{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> Analysing Elon Musk's tweeting style 2015-2020"},{"metadata":{},"cell_type":"markdown","source":"I have created this notebook strictly for the exercising purposes and I hope you find it interesting and learn something about Elon Musk's tweets in 2015â€“2020. Most recently I have started the Data Science Nanodegree online course with Udacity. This post is a part of an exercise and is required for me to pass the first module. The project consists of two parts: \n- exploratory analysis of a dataset of your choice\n- Medium post with the results from the above (to be found here: https://medium.com/p/176a8279cefb)\n\nEnjoy the journey of exploring Elon's tweets which is the Kaggle dataset (https://www.kaggle.com/vidyapb/elon-musk-tweets-2015-to-2020) I've chosen for the exercise.\nI will try to answer these three questions:\n* What's Elon Musk's tweeting style?\n* What tweeting pattern yields the highest engagement?\n* What are the most popular words used in his tweets?"},{"metadata":{},"cell_type":"markdown","source":"<h2> Import all necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport nltk\nimport altair as alt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom datetime import date, datetime\nfrom pandas_profiling import ProfileReport\nfrom tabulate import tabulate\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nfrom nltk.corpus import stopwords\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List all the libraries used in the project\n#!pip3 freeze","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Create the file name and path references"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path_name = '../input/elon-musk-tweets-2015-to-2020/elonmusk.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def open_file(file_path_name):\n    \"\"\"\n    This function opens the csv file and creates the dataframe\n    :param file_path_name: Name of the path and the input file\n    :return: The pandas dataframe\n    \"\"\"\n    return pd.read_csv(file_path_name, index_col=[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Check the file content"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(open_file(file_path_name).head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Create a dataframe profile pdf (Optional)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def profile_dataframe(file_path_name):\n#     \"\"\"\n#     This function is taking the pandas dataframe and creating the profile report in the html format.\n#     :param file_path_name: Name of the path and the input file\n#     :return: The dataframe profile in the html format\n#     \"\"\"\n#     today = date.today()\n#     df = open_file(file_path_name)\n#     profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n#     profile.to_file(\"report_{0}.html\".format(today))\n\n\n# open(profile_dataframe(file_path_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Prepare the code to perform the dataframe cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_columns_with_constant_values(df):\n    return df.drop(columns=list(df.columns[df.nunique() <= 1]))\n\n\ndef check_the_head_of_constant_columns(df):\n    print(tabulate\n          (df[df.columns[df.nunique() <= 1]].head(20), headers='keys', tablefmt='psql'))\n\n    \ncolumns_to_drop = ['hashtags', 'cashtags', 'link', 'quote_url', 'urls', 'created_at']\n\n\ndef drop_redundant_columns(df, columns_to_drop):\n    return df.drop(columns=columns_to_drop, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Add new count columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_columns_with_constant_values(df):\n    \"\"\"\n    This function takes the pandas dataframe and drops the columns with the constant values.\n    :param df: The input dataframe.\n    :return: The transformed dataframe with dropped columns with the constant values.\n    \"\"\"\n    return df.drop(columns=list(df.columns[df.nunique() <= 1]))\n\n\ndef check_the_head_of_constant_columns(df):\n    \"\"\"\n    This function takes the pandas dataframe and checks the name of columns with the constant values.\n    :param df: The input dataframe.\n    :return: The names of the columns with the constant values.\n    \"\"\"\n    print(tabulate\n          (df[df.columns[df.nunique() <= 1]].head(20), headers='keys', tablefmt='psql'))\n\n\ncolumns_to_drop = ['hashtags', 'cashtags', 'link', 'quote_url', 'urls', 'created_at']\n\n\ndef drop_redundant_columns(df, columns_to_drop):\n    \"\"\"\n    This function takes the pandas dataframe and drops the specified.\n    :param df: The input dataframe.\n    :param columns_to_drop: The list of columns to drop from the dataframe.\n    :return: The transformed dataframe with dropped columns.\n\n    \"\"\"\n    return df.drop(columns=columns_to_drop, axis=0)\n\ndef add_reply_to_count(df):\n    \"\"\"\n    This function takes the pandas dataframe and adds the new column with the count of replies.\n    :param df: The input dataframe.\n    :return: The dataframe with added new reply to count column.\n    \"\"\"\n    reply_to_count_values = []\n    for i, content in df['reply_to'].items():\n        reply_to_count_values.append((int(content.count(\"{\")) - 1))\n    df['reply_to_count'] = reply_to_count_values\n    return df\n\n\ndef add_mentions_count(df):\n    \"\"\"\n    This function takes the pandas dataframe and adds the new column with the count of mentions.\n    :param df: The input dataframe.\n    :return: The dataframe with added new mentions count column.\n    \"\"\"\n    new_values = []\n    for i, content in df['mentions'].items():\n        new_values.append(int(content.count(\"'\") / 2))\n    df['mentions_count'] = new_values\n    return df\n\n\ndef add_photos_count(df):\n    \"\"\"\n    This function takes the pandas dataframe and adds the new column with the count of photos.\n    :param df: The input dataframe.\n    :return: The dataframe with added new photos count column.\n    \"\"\"\n    new_values = []\n    for i, content in df['photos'].items():\n        new_values.append(int(content.count(\"https\")))\n    df['photos_count'] = new_values\n    return df\n\n\ndef add_weekday(df):\n    \"\"\"\n    This function takes the pandas dataframe, extracts the weekday and adds it to the new column.\n    :param df: The input dataframe.\n    :return: The dataframe with added new weekday column.\n    \"\"\"\n    weekday = []\n    for i, content in df['date'].items():\n        year, month, day = map(int, content.split('-'))\n        d = date(year, month, day)\n        weekday.append(d.weekday())\n    df['weekday'] = weekday\n    return df\n\n\ndef convert_to_datetime(df):\n    \"\"\"\n    This function takes the pandas dataframe, converts to the datetime and adds it to the new column.\n    :param df: The input dataframe.\n    :return: The dataframe with added new datetime column.\n    \"\"\"\n    df['datetime'] = (df['date'] + \" \" + df['time']).astype('string')\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Add new date and time related columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_hour_minute(df):\n    \"\"\"\n    This function takes the pandas dataframe, extracts the time related values and adds it to the new columns.\n    :param df: The input dataframe.\n    :return: The dataframe with added new year, month, hour, and minute columns.\n    \"\"\"\n    year_col = []\n    month_col = []\n    hour_col = []\n    minute_col = []\n    for i, content in df['datetime'].items():\n        t1 = datetime.strptime(content, '%Y-%m-%d %H:%M:%S')\n        year_col.append(t1.year)\n        month_col.append(t1.month)\n        hour_col.append(t1.hour)\n        minute_col.append(t1.minute)\n    df['year'] = year_col\n    df['month'] = month_col\n    df['hour'] = hour_col\n    df['minute'] = minute_col\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Bring it all together"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_index(df, index_column):\n    \"\"\"\n    This function takes the pandas dataframe, the index column name and creates a new index.\n    :param df: The input dataframe.\n    :param index_column: The list of the name of an index column.\n    :return: The dataframe with a new index.\n    \"\"\"\n    return df.set_index(index_column, drop=True, inplace=False, verify_integrity=True)\n\n\ndef clean_dataframe(df, columns_to_drop):\n    \"\"\"\n    This function takes the pandas dataframe, a list of columns to drop and removes them.\n    :param df: The input dataframe.\n    :param columns_to_drop: The list of the columns to drop.\n    :return: The dataframe without dropped columns.\n    \"\"\"\n    df = drop_redundant_columns(df, columns_to_drop)\n    return df\n\n\ndef transform_dataframe(df):\n    \"\"\"\n    This function takes the pandas dataframe, and performs the above operations.\n    :param df: The input dataframe.\n    :return: The transformed dataframe.\n    \"\"\"\n    df = drop_columns_with_constant_values(df)\n    add_mentions_count(df)\n    add_weekday(df)\n    add_reply_to_count(df)\n    add_photos_count(df)\n    convert_to_datetime(df)\n    extract_hour_minute(df)\n    df = drop_redundant_columns(df, ['photos', 'date', 'mentions', 'reply_to', 'reply_to_count'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Open the file and check the datatypes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = open_file(file_path_name)\nnew_df = clean_dataframe(df, columns_to_drop)\nnew_df = transform_dataframe(new_df)\n\nprint(\"\\n----------------- DATA TYPES -------------------\")\nprint(new_df.dtypes)\n#Use the code below for the pretty print the dataframe\n# print(tabulate(new_df.loc[new_df['photos_count'] == 2], headers='keys')) #, tablefmt='psql'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Text preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count the number of tweets\nprint(new_df.count())\n\n#Normalize the tweets to be lowercase\ndf['tweet'] = df['tweet'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop duplicates tweets\nnew_df.drop_duplicates(subset=['tweet'], keep='first', inplace=True)\nprint(new_df.shape)\nprint(new_df.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove the @users\ndef remove_users(tweet, pattern1, pattern2):\n    \"\"\"\n    This function takes the tweet text and removes the regex pattern words.\n    :param tweet: The text to be searched for the pattern.\n    :pattern1: The first pattern to be removed.\n    :pattern2: The second pattern to be removed.\n    :return: The text without a removed patterns.\n    \"\"\"\n    r = re.findall(pattern1, tweet)\n    for i in r:\n        tweet = re.sub(i, '', tweet)\n        \n    r = re.findall(pattern2, tweet)\n    for i in r:\n        tweet = re.sub(i, '', tweet)\n    return tweet\n\nnew_df['tidy_tweet'] = np.vectorize(remove_users)(new_df['tweet'], \"@ [\\w]*\", \"@[\\w]*\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Descriptive statistics"},{"metadata":{},"cell_type":"markdown","source":"One we've got the dataframe cleaned and ready, we can perform some basic statictics, and start looking for the answers for the questions posed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count the number of characters and length of a tweet\ncount = new_df['tweet'].str.split().str.len()\ncount.index = count.index.astype(str) + ' words:'\ncount.sort_index(inplace=True)\n\n\ndef word_count(df):\n    \"\"\"\n    This function takes the dataframe and adds a new colun with the number of words.\n    :param df: The dataframe to be transformed.\n    :return: The transformed dataframe.\n    \"\"\"\n    words_count = []\n    for i, content in df['tweet'].items():\n        new_values =[]\n        new_values = content.split()\n        words_count.append(len(new_values))\n    df['word_count'] = words_count\n    return df\n\nnew_df = word_count(new_df)\n\nprint(\"Total number of words: \", count.sum(), \"words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Average number of words per tweet: \", round(count.mean(),2), \"words\")\nprint(\"Max number of words per tweet: \", count.max(), \"words\")\nprint(\"Min number of words per tweet: \", count.min(), \"words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df['tweet_length'] = new_df['tweet'].str.len()\n\nprint(\"Total length of a dataset: \", new_df.tweet_length.sum(), \"characters\")\nprint(\"Average length of a tweet: \", round(new_df.tweet_length.mean(),0), \"characters\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,8))\nsns.heatmap(new_df.corr(), annot=True, linewidths=1.5, fmt=\".2f\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> From the correlation matrix above we can observe a few interesting correlations. Replies and retweets count are highly (73% and 91% respectively) positively correlated with the likes count. On the other hand, mentions count is moderately negatively (-23%) correlated with the likes count (the more mentions, the less likes). Attaching the photos or videos will likely increase the number of likes too (29% and 10% respectively). The length of a tweet is slightly negatively correlated with the likes count."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(new_df.columns)\nX = new_df[['replies_count', 'retweets_count', 'mentions_count', 'weekday', \n            'photos_count', 'year', 'hour', 'minute', 'tweet_length','word_count'\n           ]]\ny = new_df[['likes_count']]\n\nX2 = sm.add_constant(X)\nest = sm.OLS(y, X2)\nest2 = est.fit()\nprint(est2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> From the table above, we can see the P>|t| value, which indicates if we can find an evidence of the statistical significance of each feature. When the value is ~<0.05, we could conclude that the feature has a statistial difference (statisticaly significant in other words). We can see that most of our features, but weekday, a minute, and a word count, are statistically significant for measureing the correlation between them and the target feature (likes count). Read more here: https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-t-values-and-p-values-in-statistics and here: https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression"},{"metadata":{},"cell_type":"markdown","source":"<h1> The Analysis"},{"metadata":{},"cell_type":"markdown","source":"<h1> What's Elon Musk's tweeting style?"},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.data_transformers.disable_max_rows()\n\nalt.Chart(new_df, width=800, height=400\n         ).mark_point(filled=True\n                     ).encode(\n    alt.X('hour:N', scale=alt.Scale(zero=True)),\n    alt.Y('month:N', scale=alt.Scale(zero=True)),\n    alt.Size('likes_count:Q'),\n    #alt.Color('photos_count:N'),\n    alt.OpacityValue(0.5),\n    tooltip=['tweet:N', 'likes_count:Q', 'photos_count:N', 'year:N']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bars = alt.Chart(new_df).mark_bar().encode(\n    y='month:O',\n    x='count_tweets:Q',\n    tooltip=['mean_tweets:N'],\n    text='mean_tweets:Q'\n).transform_aggregate(\n    count_tweets='count(id)',\n    mean_tweets='mean(count_tweets)',\n    #sum_likes='sum(likes_count)',\n    groupby=[\"month\"]\n)\n\ntext = bars.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n).encode(\n    text='count_tweets:Q'\n)\n\n(bars + text).properties(height=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bars = alt.Chart(new_df).mark_bar().encode(\n    y='weekday:O',\n    x='count_tweets:Q',\n    #tooltip=['mean_tweets:N'],\n    text='mean_tweets:N'\n).transform_aggregate(\n    count_tweets='count(id)',\n    mean_tweets='mean(count_tweets)',\n    #sum_likes='sum(likes_count)',\n    groupby=[\"weekday\"]\n)\n\ntext = bars.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n).encode(\n    text='count_tweets:Q'\n)\n\n(bars + text).properties(height=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bars = alt.Chart(new_df).mark_bar().encode(\n    y='hour:O',\n    x='count_tweets:Q',\n    #tooltip=['mean_tweets:N'],\n    text='mean_tweets:N'\n).transform_aggregate(\n    count_tweets='count(id)',\n    mean_tweets='mean(count_tweets)',\n    #sum_likes='sum(likes_count)',\n    groupby=[\"hour\"]\n)\n\ntext = bars.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n).encode(\n    text='count_tweets:Q'\n)\n\n(bars + text).properties(height=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> What tweeting pattern yields the highest engagement?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.set_theme(color_codes=True)\ng = sns.lmplot(x='hour', y='likes_count', data = new_df,# col = 'photos_count', \n              aspect = 2.5, robust=False, palette='tab5',\n              scatter_kws=dict(s=50, linewidths=.1, edgecolors='black'),\n              order=2, ci=None\n              )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> The graph above helps us understand what times are most likely to yield the highest likes count in tweeting. Times between 6-9 am and 7-9 pm are yeilding the highest results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(new_df, col=\"weekday\", height=6, aspect=.5)\ng.map(sns.barplot, \"photos_count\", \"likes_count\", order=[0, 1, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> We can see an interesting pattern on the figure above. Tweets with two photos, posted on Monday and Tuesday, will yield more likes than posted other days. Looks like tweets with one photo attached, will gain more response in likes posted from Wednesday till Sunday. Tweets with no photos will collect the lowest likes on average."},{"metadata":{"trusted":true},"cell_type":"code","source":"import altair as alt\nalt.data_transformers.disable_max_rows()\n\nbase = alt.Chart(new_df, width=600, height=400).mark_point(filled=True).encode(\n    x=alt.X('month:O'), y='likes_count:Q', tooltip=['tweet:N', 'likes_count:Q', 'photos_count:N', 'time:N']\n)\n\n# A slider filter\nyear_slider = alt.binding_range(min=2015, max=2020, step=1)\nslider_selection = alt.selection_single(bind=year_slider, fields=['year'], name=\"Tweet\")\n\nrating_color_condition = alt.condition(slider_selection,\n                      alt.Color('photos_count:N'),\n                      alt.value('lightgray'))\n\nfilter_year = base.add_selection(\n    slider_selection\n).encode(\n    color=rating_color_condition\n).transform_filter(\n    slider_selection\n).properties(title=\"Slider Filtering\")\n\nfilter_year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> What are the most popular words used in his tweets?"},{"metadata":{},"cell_type":"markdown","source":"<h3> Prepare the text blob to extract the most popular words."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_text_blob(df, text_column):\n    blob_text=[]\n    for i, content in df[text_column].items():\n        for i in content.split():\n            blob_text.append(i.lower())\n    return blob_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blob_text = create_text_blob(new_df, 'tidy_tweet')\nprint(blob_text[0:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = Counter(blob_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))  \nfiltered_sentence = [w for w in blob_text if not w in stop_words]  \nfiltered_sentence = []  \n  \nfor w in blob_text:  \n    if w not in stop_words:  \n        filtered_sentence.append(w)  \n\nprint(filtered_sentence[0:200])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = Counter(filtered_sentence)\n#Use the code below for the row-by-row print.\n# for i, n in counts.items():\n#     print(i,\":\", n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ntop_20_words = {}\n\nfor (key, value) in counts.items():\n   # Check if value is greater than 200 and add to new dictionary\n    if value > 200 :\n        top_20_words[key] = value\n    continue\n\nsorted_top_20_words = dict(sorted(top_20_words.items(), key=lambda item: item[1], reverse=False))\n\nword = sorted_top_20_words.keys()\ncount = sorted_top_20_words.values()\n\n\nfig = px.bar(y=word, x=count, text = count)\nfig.update_traces(texttemplate='%{text:}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> As we can see on the graph above, there are a few insignificant words like '&', '...', or stopwords like \"it's\", \"would\" or numbers. We will get rid of them to get a clearer picture of what Elon Musk is tweeting about."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"top_20_words_clean = {}\n\nfor (key, value) in counts.items():\n    # Check if key length is greater than 3 and value greater than 150 and add to new dictionary\n    if len(key)>2 and value > 150 :\n        top_20_words_clean[key] = value\n    continue\n\nsorted_top_20_words_clean = dict(sorted(top_20_words_clean.items(), key=lambda item: item[1], reverse=False))\n\nword = sorted_top_20_words_clean.keys()\ncount = sorted_top_20_words_clean.values()\n\nfig = px.bar(y=word, x=count, text = count)\nfig.update_traces(texttemplate='%{text:}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Looks like Elon Musk was mostly tweeting about the tesla car model 3, and used nouns as like, good, great. We can see that SpaceX was also occurring quite often."},{"metadata":{},"cell_type":"markdown","source":"The Medium article about the above analysis: https://medium.com/@lukasz.aszyk/this-is-how-5-years-of-elon-musks-tweets-look-like-part-1-176a8279cefb\n\nThe Github repo with the project above: https://github.com/asheone/Data-Science-Project-1"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}