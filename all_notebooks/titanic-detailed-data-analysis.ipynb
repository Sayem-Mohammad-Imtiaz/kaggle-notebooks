{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will analyze the data of Titanic Dataset.  \nProcess followed for the Data Analysis is as below:  \n1. Reading the Data  \n    a. Loading the data  \n    b. Using info(), describe() to view the data  \n        \n2. Data Cleaning  \n    a. Checking Null values in df  \n    b. Imputing the null values  \n        \n3. Exploratory Data Analysis  \n    a. Plotting graphs against target variable  \n    b. Plotting correlation heatmap  \n        \n4. Machine Learning - Preprocessing  \n    a. Train Test Split  \n    b. Standard Scaling for numerical variables  \n    c. One Hot Encoding for categorical variables  \n        \n5. Machine Learning - Modelling  \n    a. Logistic Regression  \n    b. Decision Tree  \n    c. Random Forrest  "},{"metadata":{},"cell_type":"markdown","source":"Importing the Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/taitanictrain/datasets_11657_16098_train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now know the dimensions of the dataset and its constitution."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this, we see that PassengerId and Name are unique for each observation.  \nThis means they would bear no impact on the data analysis.  \nHence, we can remove these columns from our dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['PassengerId','Name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we see the data distribution.  \nThe insights we can get from it are:\n - Total Number of variables - 11\n - Variables and their types  \n - Number of categorical variables (object) - 5\n - Number of numerical variables (int64 or float64) - 7\n - If any variables have null values - Age, Cabin and Embarked"},{"metadata":{},"cell_type":"markdown","source":"Knowing the above, we can delve deeper into handling null values.  \nHad the Non-Null Count been 891 for all, we would have skipped this step."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the above for better view."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.heatmap(df.isnull(), yticklabels=False, cmap='ocean');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"687/891","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the null values from Cabin column account for 77% of total number of rows.  \nAlso, inspecting the column, we see it is a categorical variable with 147 categories.  \nHence it would not be logical to keep that column for our analysis.  \nWe will drop the Cabin column from our dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('Cabin', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check if it has been executed successfully."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets handle the next column with most null values - Age.  \nAs Age is an int64 type variable, we could impute using mean or median.  \nLet us analyze it futher before making any decision."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Age.describe().round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets visualize the Age data using a boxplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data = df, x = df.Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Age.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could simply impute the missing values with the median.  \nBut before that, let us classify the Age column further with other categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data = df, x = df.Sex, y = df.Age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We realise there is no significant difference between the medians of both classifications.  \nLet us do this with another categorical vairable."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=df, x=df.Pclass, y=df.Age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can clearly see the difference between Age values for each Pclass.  \nHence, it would be logical to impute the Age according to the Pclass."},{"metadata":{},"cell_type":"markdown","source":"To confirm our visualization is correct, let us look at the numbers themselves."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Pclass']==1].Age.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Pclass']==2].Age.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Pclass']==3].Age.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We realise, that imputing Age value by mean of age w.r.t Pclass is more meaningful.  \nHence, we will impute the data accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age'] = df['Age'].fillna(df.groupby('Pclass').Age.transform('median'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have imputed the null values in Age column according to their Pclass median.  \nLets check the null values again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still have to handle the null values from Embarked column.  \nLets understand that column better to make a decision."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Embarked.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since category S constitutes a significant share of Embarked column, it is that much likely that the missing values would be in category S.  \nHence, we will replace the missing values of Embarked column with value S."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Embarked = df.Embarked.fillna('S')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if were able to successfully execute it."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Embarked.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's have an overall view of the dataframe after all the cleaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Now that we have cleaned the data, let us proceed with EDA to better understand the data."},{"metadata":{},"cell_type":"markdown","source":"We want to see the relationship of the input variables with respect to the output variable.  \nHere our output variable is 'Survived.'  \nHence we will plot our charts to show the distribution of each variable with Survived data."},{"metadata":{},"cell_type":"markdown","source":"Before that, lets look at the distribution of Survived."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data = df, x = df.Survived)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pclass"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data = df, x = df.Pclass, hue = df.Survived)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can immediately see a pattern. It seems that higher your Pclass, the greater your chance of Surviving.  \nA passenger with Pclass 3 has a high probability of not Surviving."},{"metadata":{},"cell_type":"markdown","source":"#### Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data=df, x=df.Survived, y=df.Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=df, x=df.Survived, y=df.Age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much inference can be derived from the Age column.  \nHowever, the boxplot does match with our understanding of Titanic survivors.  \nThe people who Survived were mostly either old people, children or woman.  \nYoung and middle-aged men were less prioritized due to dearth of lifeboats.\n  \nHence, it makes sense that the Age distribution of Surivived is more widspread than that of those who didn't Survive."},{"metadata":{},"cell_type":"markdown","source":"#### SibSp"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SibSp.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sibsp refers to Number of Siblings/Spouses Aboard"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df, x=df.SibSp, hue=df.Survived)\nplt.legend(bbox_to_anchor=(1.05, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To an untrained eye, there doesn't seem to be much to interpret from the above plot."},{"metadata":{},"cell_type":"markdown","source":"#### Parch"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Parch.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parch refers to Number of Parents/Children Aboard"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df, x=df.Parch, hue=df.Survived)\nplt.legend(bbox_to_anchor=(1.05, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, this plot seems very similar to the earlier plot.  \nNot much inference."},{"metadata":{},"cell_type":"markdown","source":"#### Ticket"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Ticket.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Ticket.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a dataset with lenght of 891, we have unique values of 681.  \nThere may not be much inference from this."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df, x=df.Ticket, hue=df.Survived)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, we can clearly say they is no significant pattern between passengers who have Survived and their Ticket Number."},{"metadata":{},"cell_type":"markdown","source":"We will drop the column Ticket from our dataset. We were able to come to conclusion thanks to EDA."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('Ticket', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fare"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,4))\nsns.boxplot(data=df, x=df.Fare, y=df.Survived, orient='h', palette='viridis');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data=df, x=df.Survived, y=df.Fare, palette='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plots we see that those who had paid higher Fare had more chances of Survival."},{"metadata":{},"cell_type":"markdown","source":"#### Embarked"},{"metadata":{},"cell_type":"markdown","source":"This variable refers to port of Embarkment.  \n(C = Cherbourg; Q = Queenstown; S = Southampton)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Embarked.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df, x=df.Embarked, hue=df.Survived, palette='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logically, port of Embarkment shouldn't have any correlation with Survival."},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Modelling"},{"metadata":{},"cell_type":"markdown","source":"### One Hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, drop_first = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,12))\nsns.heatmap(df.corr(), annot = True, cmap = 'Blues');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There doesn't seem to be any massively correlated variables which may cause multicollienarity problem in the analysis.  \nHence, we can proceed further."},{"metadata":{},"cell_type":"markdown","source":"\n### Splitting the dataset \ni.e. Removing the Dependent variable from the dataset into a new dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Independent Variable\nX = df.drop('Survived', axis = 1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dependent Variable\ny = df['Survived']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_plot = pd.Series(model.feature_importances_, index = X.columns)\nfeature_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_plot.nlargest(10).plot(kind = \"barh\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_sc = sc.fit_transform(X[['Age','Fare']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_df = pd.DataFrame(X_sc, columns = ['Age','Fare'])\nX_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_rem = X.drop(['Age','Fare'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fin =pd.concat([X_df, X_rem], axis = 1) \nX_fin.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_fin, y, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()\nlog_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_preds = log_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Accuracy of Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\nconfusion_matrix(y_test, log_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, log_preds).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(log_model, X_test, y_test, cmap = \"GnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, log_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Automating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def algo(algorithm):\n    model = algorithm()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    print('The accuracy of {} model is {}'.format(model, accuracy_score(y_test, preds).round(2)))\n    print(\"\\n\")\n    print(\"CLASSIFICATION REPORT\",'\\n',classification_report(y_test, preds),\"\\n\")\n    print(\"CONFUSION MATRIX\",'\\n')\n    plot_confusion_matrix(model, X_test, y_test, cmap = \"GnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nalgo(DecisionTreeClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nalgo(RandomForestClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nalgo(KNeighborsClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine (SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nalgo(SVC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nalgo(LinearSVC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nalgo(GaussianNB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nalgo(SGDClassifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Perceptron\nalgo(Perceptron)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"#### From the above results, it is clear that RandomForest Classifier is the best performing model."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Hyper Parameter Tuning on Random Forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparameters = [{'n_estimators': range(10,20), \n               'max_depth': range(2,6)}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(random_state = 0)\n\nrfc_grid = GridSearchCV(estimator = model, param_grid = parameters, cv = 10)\n\nrfc_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_grid.best_estimator_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}