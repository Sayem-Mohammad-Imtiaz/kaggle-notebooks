{"cells":[{"metadata":{},"cell_type":"markdown","source":"# News Classification\nThis notebook demonstrates how to classify news with Bidirectional LSTM"},{"metadata":{},"cell_type":"markdown","source":"### "},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n!wget https://github.com/stopwords-iso/stopwords-bn/blob/master/stopwords-bn.txt\n\n# since we are classifying, tokens like stopwords, punctuations don't really have a meaning\n# so we will remove all of these\ndef clean_and_tokenize(txt):\n    txt = re.sub(r'[^\\u0900-\\u0A7F\\u0020-u003F\\u2000-\\u206F]', '', txt) \n    txt = re.sub(r'[a-zA-Z0-9]', '', txt)\n    txt = re.sub(r'<.*?>', '', txt)\n    txt = re.sub(r'।+', '। ', txt) \n        \n    chars = ['/', ';',\n             '—', \n             '=', '%',\n             '>', '<',\n             '_', '…',\n             '–', '*', '~',\n             '}', '{',\n             \"\\\\\", \n             '[', ']',\n             '#', '+',\n             '∗', '&', '|',\n             '`', '@', '^',\n             '$', '•']\n    for char in chars:\n        txt = txt.replace(char, ' ')\n    \n    # load the stop words\n    with open('stopwords-bn.txt', 'r') as f:\n        stopwords = f.readlines()\n    \n    for word in stopwords:\n        txt = txt.replace(word, ' ')\n    \n    txt = txt.replace('\\u200c', '')\n    txt = txt.replace('\\u200d', '')\n    txt = txt.replace(' . ', ' ')\n    txt = txt.replace('.', ' ')\n    txt = re.sub(r'\\([^)]*\\)', ' ', txt)\n    txt = txt.replace('‘‘', ' ') \n    txt = txt.replace('’’', ' ') \n    txt = txt.replace('‘', \" \")\n    txt = txt.replace('’', \" \")\n    txt = txt.replace('“', ' ')\n    txt = txt.replace('”', ' ')\n    txt = txt.replace('\"', ' ')    \n    txt = txt.replace(\"'\", ' ')\n    txt = re.sub(r'।',' ',txt)\n    txt = re.sub(r',',' ',txt)\n    txt = re.sub(r'-',' ',txt)\n    txt = txt.replace('?', ' ')\n    txt = txt.replace('!', ' ')\n    txt = txt.replace(':', ' ')\n    txt = re.sub(r' +',' ',txt)\n    txt = txt.strip() \n     \n    return txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Process the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom torchtext.data import Field, LabelField, TabularDataset, BucketIterator\nimport numpy as np\n\ndata = pd.read_csv('../input/bdnews24-corpus/bdnews24.csv', encoding='utf-8', index_col=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(data['en_category'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's cherry pick some categories\ncats = {\n    'bangladesh': 0,\n    'business': 1,\n    'cricket': 2,\n    'economy': 3,\n    'politics': 4,\n    'tech': 5,\n    'sport': 6,\n    'world': 7,\n   \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat in cats.keys():\n    print(f'{cat}: {len(data[data.en_category == cat])}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only select the `contents` & `en_category` column\nfiltered = data[data.en_category.isin(cats)]\n\n# reset the indices \nfiltered = filtered.reset_index()[['contents', 'en_category']]\n\nfiltered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\n\nx = filtered['contents']\ny = filtered['en_category']\n\ntrain_x, _, train_y, _ = train_test_split(x, y, stratify=y, test_size=0.95)\n\n# clean the text\ntrain_x = train_x.apply(lambda x: clean_and_tokenize(str(x)))\n\n# convert the categories to ids\ntrain_y = train_y.apply(lambda x: cats[x])\n\n# first split the train dataset into (train, rest) set\ntrain_x, rest_x, train_y, rest_y = train_test_split(train_x, train_y, stratify=train_y, test_size=0.2, random_state=747)\n\n# then split the rest into (test, valid) set\ntest_x, valid_x, test_y, valid_y = train_test_split(rest_x, rest_y, stratify=rest_y, test_size=0.5, random_state=747)\n\nprint(len(test_y), len(valid_y), len(train_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.bincount(test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now save the splits \n\ntrain_df = pd.concat([train_x, train_y], axis=1).reset_index()[['en_category', 'contents']]\ntest_df = pd.concat([test_x, test_y], axis=1).reset_index()[['en_category', 'contents']]\nvalid_df = pd.concat([valid_x, valid_y], axis=1).reset_index()[['en_category', 'contents']]\n\ntrain_df.to_csv('train.csv', header=['classlabel', 'content'], encoding='utf-8', index=False)\ntest_df.to_csv('test.csv', header=['classlabel', 'content'], encoding='utf-8', index=False)\nvalid_df.to_csv('valid.csv', header=['classlabel', 'content'], encoding='utf-8', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torchtext import data\nfrom torchtext import datasets\nimport time\nimport random\nimport pandas as pd\nimport numpy as np\n\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 123\ntorch.manual_seed(RANDOM_SEED)\n\nVOCABULARY_SIZE = 60_000\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 32\nNUM_EPOCHS = 50\nDROPOUT = 0.5\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nEMBEDDING_DIM = 200\nBIDIRECTIONAL = True\nHIDDEN_DIM = 200\nNUM_LAYERS = 2\nOUTPUT_DIM = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef remove_exponent(d):\n    \"\"\"Remove exponent.\"\"\"\n    return d.quantize(Decimal(1)) if d == d.to_integral() else d.normalize()\n\n\ndef millify(n, precision=0, drop_nulls=True, prefixes=[]):\n    \"\"\"Humanize number.\"\"\"\n    millnames = ['', 'k', 'M', 'B', 'T', 'P', 'E', 'Z', 'Y']\n    if prefixes:\n        millnames = ['']\n        millnames.extend(prefixes)\n    n = float(n)\n    millidx = max(0, min(len(millnames) - 1,\n                         int(math.floor(0 if n == 0 else math.log10(abs(n)) / 3))))\n    result = '{:.{precision}f}'.format(n / 10**(3 * millidx), precision=precision)\n    if drop_nulls:\n        result = remove_exponent(Decimal(result))\n    return '{0}{dx}'.format(result, dx=millnames[millidx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = data.Field(sequential=True,\n#                   tokenize='spacy',\n                  include_lengths=True) # necessary for packed_padded_sequence\n\nLABEL = data.LabelField(dtype=torch.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fields = [('classlabel', LABEL), ('content', TEXT)]\n\ntrain_dataset = data.TabularDataset(\n    path=\"train.csv\", format='csv',\n    skip_header=True, fields=fields)\n\ntest_dataset = data.TabularDataset(\n    path=\"test.csv\", format='csv',\n    skip_header=True, fields=fields)\n\nvalid_dataset = data.TabularDataset(\n    path=\"valid.csv\", format='csv',\n    skip_header=True, fields=fields)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Num Train: {len(train_dataset)}')\nprint(f'Num Valid: {len(test_dataset)}')\nprint(f'Num Valid: {len(valid_dataset)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT.build_vocab(train_dataset, test_dataset, valid_dataset,\n                 min_freq=2)\nLABEL.build_vocab(train_dataset)\n\nprint(f'Vocabulary size: {len(TEXT.vocab)}')\nprint(f'Number of classes: {len(LABEL.vocab)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n    (train_dataset, valid_dataset, test_dataset), \n    batch_size=BATCH_SIZE,\n    sort_within_batch=True, # necessary for packed_padded_sequence\n    sort_key=lambda x: len(x.content),\n    device=DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train')\nfor batch in train_loader:\n    print(f'Text matrix size: {batch.content[0].size()}')\n    print(f'Target vector size: {batch.classlabel.size()}')\n    break\n    \nprint('\\nValid:')\nfor batch in valid_loader:\n    print(f'Text matrix size: {batch.content[0].size()}')\n    print(f'Target vector size: {batch.classlabel.size()}')\n    break\n    \nprint('\\nTest:')\nfor batch in test_loader:\n    print(f'Text matrix size: {batch.content[0].size()}')\n    print(f'Target vector size: {batch.classlabel.size()}')\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_dim, embedding_dim, bidirectional, hidden_dim, num_layers, output_dim, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim,\n                           num_layers=num_layers,\n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        self.fc1 = nn.Linear(hidden_dim * num_layers, 64)\n        self.fc2 = nn.Linear(64, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_length):\n\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        hidden = self.fc1(hidden)\n        hidden = self.dropout(hidden)\n        hidden = self.fc2(hidden)\n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DIM = len(TEXT.vocab)\n\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n\ntorch.manual_seed(RANDOM_SEED)\nmodel = RNN(INPUT_DIM, EMBEDDING_DIM, BIDIRECTIONAL, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM, DROPOUT, PAD_IDX)\nmodel = model.to(DEVICE)\nprint(model)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_accuracy(model, data_loader, device):\n    model.eval()\n    correct_pred, num_examples = 0, 0\n    with torch.no_grad():\n        for batch_idx, batch_data in enumerate(data_loader):\n            text, text_lengths = batch_data.content\n            if 0 in text_lengths:\n                continue\n            logits = model(text, text_lengths.to('cpu'))\n            _, predicted_labels = torch.max(logits, 1)\n            num_examples += batch_data.classlabel.size(0)\n            correct_pred += (predicted_labels.long() == batch_data.classlabel.long()).sum()\n        return correct_pred.float()/num_examples * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    for batch_idx, batch_data in enumerate(train_loader):\n        \n        text, text_lengths = batch_data.content\n        # print(text.shape, text_lengths.shape)\n        if 0 in text_lengths:\n            continue\n        \n        ### FORWARD AND BACK PROP\n        logits = model(text, text_lengths.to('cpu'))\n        cost = F.cross_entropy(logits, batch_data.classlabel.long())\n        optimizer.zero_grad()\n        \n        cost.backward()\n        \n        ### UPDATE MODEL PARAMETERS\n        optimizer.step()\n        \n        ### LOGGING\n        if not batch_idx % 50:\n            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n                   f'Cost: {cost:.4f}')\n\n    with torch.set_grad_enabled(False):\n        print(f'training accuracy: '\n              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n              f'\\nvalid accuracy: '\n              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n        \n    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n    \nprint(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\nprint(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = {\n    'bangladesh': 0,\n    'business': 1,\n    'cricket': 2,\n    'economy': 3,\n    'politics': 4,\n    'tech': 5,\n    'sport': 6,\n    'world': 7,\n   \n}\n\nmap_dict = {v: k for k, v in cats.items()}\n\ndef predict(model, sentence, device='cpu'):\n    model.eval()\n    indexed = [TEXT.vocab.stoi[token] for token in clean_and_tokenize(sentence).split()]\n#     indexed = [TEXT.vocab.stoi[i] for i in tokenized]\n    length = [len(indexed)]\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(1)\n    length_tensor = torch.LongTensor(length)\n    \n    output = model(tensor, length_tensor)\n    predictions = torch.softmax(output, dim=1)\n    \n    probs, label = predictions.max(dim=1)\n    \n    return predictions, probs.item(), label.item()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.prothomalo.com/bangladesh/%E0%A6%86%E0%A6%A8%E0%A6%BF%E0%A6%B8%E0%A7%81%E0%A6%B2-%E0%A6%B9%E0%A6%A4%E0%A7%8D%E0%A6%AF%E0%A6%BE-%E0%A6%AE%E0%A6%BE%E0%A6%AE%E0%A6%B2%E0%A6%BE%E0%A7%9F-%E0%A6%86%E0%A6%97%E0%A6%BE%E0%A6%AE-%E0%A6%9C%E0%A6%BE%E0%A6%AE%E0%A6%BF%E0%A6%A8-%E0%A6%AA%E0%A7%87%E0%A6%B2%E0%A7%87%E0%A6%A8-%E0%A6%8F%E0%A6%95-%E0%A6%9A%E0%A6%BF%E0%A6%95%E0%A6%BF%E0%A7%8E%E0%A6%B8%E0%A6%95\nnews = \"\"\"\nসিনিয়র সহকারী পুলিশ সুপার (এএসপি) আনিসুল করিমকে হত্যার অভিযোগে করা মামলায় চিকিৎসক নুসরাত ফারজানা আগাম জামিন পেয়েছেন। আগামী ৫ জানুয়ারি পর্যন্ত তাঁকে জামিন দেওয়া হয়েছে।\nজামিন চেয়ে তাঁর করা আবেদনের শুনানি নিয়ে আজ বুধবার বিচারপতি হাবিবুল গনি ও বিচারপতি মো. রিয়াজ উদ্দিন খানের সমন্বয়ে গঠিত হাইকোর্ট বেঞ্চ এ আদেশ দেন। ওই সময়ের মধ্যে ঢাকার চিফ মেট্রোপলিটন ম্যাজিস্ট্রেট আদালতে জামিননামা দিয়ে মেট্রোপলিটন সেশন জজ আদালতে আত্মসমর্পণ করতে বলা হয়।\nআদালতে উপস্থিত হয়ে আইনজীবীর মাধ্যমে আজ আগাম জামিনের আরজি জানান নুসরাত ফারজানা। আদালতে তাঁর পক্ষে শুনানি করেন আইনজীবী রুহুল কুদ্দুস। রাষ্ট্রপক্ষে শুনানি করেন সহকারী অ্যাটর্নি জেনারেল মাহফুজুর রহমান লিখন।\n\n\"\"\"\npreds, probs, label = predict(model, news, 'cuda')\n\nprint(f'Class Label: {label} -> {map_dict[label]}')\n# print(torch.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}