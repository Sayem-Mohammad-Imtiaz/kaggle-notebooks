{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Your client is a large MNC and they have 9 broad verticals across the organisation. One of the problem your client is facing is around identifying theright people for promotion (only for manager position and below) and prepare them in time. Currently the process, they are following is:**\n1. They first identify a set of employees based on recommendations/ past performance\n2. Selected employees go through the separate training and evaluation program for each vertical. These programs are based on  the required skill of each vertical.At the end of the program, based on various factors such as training performance, KPI completion (only employees with KPIs completed greater than 60% are considered) etc., employee gets promotion\n3. For above mentioned process, the final promotions are only announced after the evaluation and this leads to delay in transition to their new roles. Hence, company needs your help in identifying  the eligible candidates at a particular checkpoint so that they can expedite the entire promotion cycle. "},{"metadata":{},"cell_type":"markdown","source":"**Evaluation Metric to be Used:** \n\n**The evaluation metric for this analysis and model should be F1 Score.**"},{"metadata":{},"cell_type":"markdown","source":"### Dataset Description"},{"metadata":{},"cell_type":"markdown","source":"<table>\n  <thead>\n    <tr>\n      <th>Field</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td> Variable</td>\n      <td>Definition</td>\n    </tr>\n    <tr>\n      <td>employee_id</td>\n      <td>Unique ID for employee</td>\n    </tr>\n    <tr>\n      <td>department</td>\n      <td>Department of employee</td>\n    </tr>\n    <tr>\n      <td>region</td>\n      <td>Region of employment (unordered)</td>\n    </tr>\n    <tr>\n      <td>education</td>\n      <td>Education Level</td>\n    </tr>\n    <tr>\n      <td>gender</td>\n      <td>Gender of Employee</td>\n    </tr>\n     <tr>\n      <td>recruitment_channel</td>\n      <td>Channel of recruitment for employee</td>\n    </tr>\n     <tr>\n      <td>no_of_trainings</td>\n      <td>no of other trainings completed in previous year on soft skills, technical skills etc.</td>\n    </tr>\n    <tr>\n     <td>age</td>\n     <td>Age of Employee</td>\n    </tr>\n    <tr>\n     <td>previous_year_rating</td>\n     <td>Employee Rating for the previous year</td>\n    </tr>\n    <tr>\n     <td>length_of_service</td>\n     <td>Length of service in years</td>\n    </tr>\n    <tr>\n     <td>KPIs_met >80%</td>\n     <td>if Percent of KPIs(Key performance Indicators) >80% then 1 else 0</td>\n    </tr>\n    <tr>\n      <td>awards_won?</td>\n      <td>if awards won during previous year then 1 else 0</td>\n    </tr>\n      <tr>\n      <td>avg_training_score</td>\n      <td>Average score in current training evaluations</td>\n    </tr>\n      <tr>\n      <td>Total expenditure</td>\n      <td>General government expenditure on health as a percene of total government expenditure (%)</td>\n    </tr>\n      <tr>\n      <td>is_promoted\t(Target)</td>\n      <td>Recommended for promotion</td>\n    </tr>\n  </tbody>\n</table>"},{"metadata":{},"cell_type":"markdown","source":"#### Importing Package"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\nimport pylab as pl\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"../input/hranalyticsav/train_data.csv\")\n#test_data=pd.read_csv(\"C:\\\\Users\\\\ARPIT\\\\Desktop\\\\New folder\\\\HR Analytics\\\\test_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_data = train_data.select_dtypes(include=[np.number])\ncategorical_data = train_data.select_dtypes(exclude=[np.number])\nprint(\"Numeric_Column_Count =\", numeric_data.shape)\nprint(\"Categorical_Column_Count =\", categorical_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allna = (train_data.isnull().sum() / len(train_data))*100\nallna = allna.drop(allna[allna == 0].index).sort_values()\nNA=train_data[allna.index.to_list()]\nNAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint(f'We have :{NAcat.shape[1]} categorical features with missing values')\nprint(f'We have :{NAnum.shape[1]} numerical features with missing values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_missing=train_data.isnull().sum().sort_values(ascending=False)\npercent=(train_data.isnull().sum()/train_data.isnull().count()).sort_values(ascending=False)\nmissing_data=pd.concat([total_missing,percent],axis=1,keys=['Missing_Total','Percent'])\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(numeric_data)\ntotal = numeric_data.isnull().sum().sort_values(ascending=False)\npercent_1 = numeric_data.isnull().sum()/numeric_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total','%'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['previous_year_rating']=train_data.groupby([\"department\",\"region\"])['previous_year_rating'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['previous_year_rating'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['education']=train_data['education'].fillna(train_data['education'].mode()[0], inplace = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab( train_data.is_promoted,train_data.education,margins=True).style.background_gradient(cmap='Wistia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.style as style\nstyle.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,9))\nsize = [39078, 805,14925]\nlabels = \"Bachelor's\", \"Below Secondary\",\"Master's & above\"\ncolors = ['green','blue','yellow']\nexplode = [0, 0.2,0.3]\nplt.pie(size, labels = labels, colors = colors, explode = explode, shadow = False, autopct = \"%.2f%%\")\nplt.title('Pie Chart Representing distribution of Employess based on their Education', fontsize =20)\nplt.axis('on')\nplt.legend(bbox_to_anchor=(0.1, 1.05, 1., .80), loc='lower left',\n           ncol=3, mode=\"expand\", borderaxespad=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking dependency of different regions in promotion\ndata = pd.crosstab(train_data['region'], train_data['is_promoted'])\ndata.div(data.sum(1).astype('float'), axis = 0).plot(kind = 'bar', stacked = True, figsize = (15, 8), color = ['lightblue', 'purple'])\nplt.title('Dependency of Regions in determining Promotion of Employees', fontsize = 30)\nplt.xlabel('Different Regions of the Company', fontsize = 18)\nplt.legend(bbox_to_anchor=(0.1, 1.05, 1., .80), loc='lower left',\n           ncol=3, mode=\"expand\", borderaxespad=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dependency of awards won on promotion\ndata = pd.crosstab(train_data['awards_won?'], train_data['is_promoted'])\ndata.div(data.sum(1).astype('float'), axis = 0).plot(kind = 'bar', stacked = True, figsize = (6, 6), color = ['magenta', 'purple'])\nplt.title('Dependency of Awards in determining Promotion', fontsize = 25)\nplt.xlabel('Awards Won or Not', fontsize = 20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot between average training score and is_promoted\ndata = pd.crosstab(train_data['avg_training_score'], train_data['is_promoted'])\ndata.div(data.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (20, 9), color = ['darkred', 'lightgreen'])\nplt.title('Looking at the Dependency of Training Score in promotion', fontsize = 30)\nplt.xlabel('Average Training Scores', fontsize = 15)\nplt.legend(bbox_to_anchor=(0.1, 1.05, 1., .80), loc='lower left',\n           ncol=3, mode=\"expand\", borderaxespad=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot('recruitment_channel',hue='is_promoted',data=train_data).set_title('Promotion_Recruitment Channel')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.kdeplot(train_data[\"length_of_service\"][train_data.is_promoted == 0], color = \"magenta\", shade = True)\nsns.kdeplot(train_data[\"length_of_service\"][train_data.is_promoted == 1], color = \"blue\", shade = True)\nsns.kdeplot(train_data[\"avg_training_score\"][train_data.is_promoted == 0], color = \"green\", shade = True)\nsns.kdeplot(train_data[\"avg_training_score\"][train_data.is_promoted == 1], color = \"yellow\", shade = True)\nplt.title(\"Best Age where Employees can get Promoted and Avg training Score for Promotion\")\nplt.legend(['Promoted = 0', 'Promoted = 1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['education'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,5))\ntrain_data.age[train_data.education == \"Bachelor's\"].plot(kind='kde')    \ntrain_data.age[train_data.education == \"Master's & above\"].plot(kind='kde')\ntrain_data.age[train_data.education == \"Below Secondary\"].plot(kind='kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution with Education Qualification\")\n# sets our legend for our graph.\nplt.legend((\"Bachelor's\",\"Master's & above\",\"Below Secondary\"),loc='best') ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nc = 'y'\n# Create dictionary of keyword aruments to pass to plt.boxplot\nred_dict =  {'patch_artist': True,\n             'boxprops': dict(color=c, facecolor=c),\n             'capprops': dict(color=c),\n             'flierprops': dict(color=c, markeredgecolor=c),\n             'medianprops': dict(color=c),\n             'whiskerprops': dict(color=c)}\ntrain_data.boxplot(column=['age','length_of_service','avg_training_score'],**red_dict,grid=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px \ndf = train_data\nfig = px.box(df, x=\"is_promoted\", y=\"age\",points=\"outliers\")\nfig.update_traces(quartilemethod=\"inclusive\") \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"promoted= 'promoted'\nnot_promoted = 'not promoted'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(14, 6))\nfemale = train_data[train_data['gender']=='f']\nmale = train_data[train_data['gender']=='m']\nax = sns.distplot(female[female['is_promoted']==0].age.dropna(), bins=40, label = not_promoted, ax = axes[0], kde =False)\nax = sns.distplot(female[female['is_promoted']==1].age.dropna(), bins=18, label = promoted, ax = axes[0], kde =False)\n\nax.legend()\nax.set_title('Female')\nax = sns.distplot(male[male['is_promoted']==1].age.dropna(), bins=18, label = promoted, ax = axes[1], kde = False)\nax = sns.distplot(male[male['is_promoted']==0].age.dropna(), bins=40, label = not_promoted, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.countplot(train_data['is_promoted'])\nplt.show()\nprint('Percent of  people getting Promoted: ',len(train_data[train_data['is_promoted']==1])/len(train_data['is_promoted'])*100,\"%\")\nprint('Percent of people not getting promoted: ',len(train_data[train_data['is_promoted']==0])/len(train_data['is_promoted'])*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Min-Max Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train_data.drop(['employee_id','KPIs_met >80%','awards_won?','is_promoted','department','region','education','gender','recruitment_channel'],axis=1)\ny=train_data['is_promoted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_features = MinMaxScaler().fit_transform(x.values)\nscaled_features_df = pd.DataFrame(scaled_features,index=x.index, columns=x.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train_data.drop(['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service','avg_training_score'],axis=1,inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.concat([scaled_features_df,train_data], axis=1).reindex(train_data.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(train_data['gender'], prefix='G')\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['gender'], prefix='G')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(train_data['recruitment_channel'], prefix='R')\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['recruitment_channel'], prefix='R')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(train_data['region'], prefix='Re')\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['region'], prefix='Re')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(train_data['department'], prefix='Dep')\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['department'], prefix='Dep')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train_data.drop(['employee_id','department','Dep_Technology','region','Re_region_8','recruitment_channel','R_other','gender','G_f'],axis=1,inplace=False)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal Encoding on Education"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['education'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nordinalencoder = OrdinalEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinalencoder.fit_transform(train_data[['education']])\ncategories = pd.Categorical(train_data['education'], categories=[\"Master's & above\",\"Bachelor's\",\"Below Secondary\"], ordered=True)\n# Order of labels set for data\ncategories\n# Factorizing the column data\nlabels, unique = pd.factorize(categories, sort=True)\ntrain_data['education'] = labels\n# Encoded Income Range Data\ntrain_data['education'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for Correlated Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_corr=train_data.corr()\nplt.figure(figsize=(18,18))\nsns.set(font_scale=0.8)\nsns.heatmap(my_corr, cbar=True, annot=True, square=True, fmt='.1f', annot_kws={'size': 8},linewidth=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correlation Among Columns > 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_target =train_data.corr().abs()\nTarget_Corr = cor_target.corr()['avg_training_score'].to_frame().reset_index() #Feature Correlation related to SalePrice\nFeature_corr =cor_target.unstack().to_frame(name='Correlation') # Feature Relation\nFeature = Feature_corr[(Feature_corr['Correlation']>=0.5)&(Feature_corr['Correlation']<1)].sort_values(by='Correlation', ascending = False).reset_index()\nFeature.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### VIF"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_data.drop('is_promoted',axis=1)\nY=train_data.is_promoted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nX_vif = add_constant(X)\nvif = pd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vif.sort_values(ascending = False).head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_data.drop(['Re_region_2','Dep_Sales & Marketing','Re_region_22','avg_training_score','Re_region_7','Dep_Operations','is_promoted'],axis=1)\nY=train_data.is_promoted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"#### Stats Model for Logistic reg."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection using random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_sel=SelectFromModel(RandomForestClassifier(n_estimators=1000))\nfeat_sel_fit=feat_sel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_sel_fit.get_support()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_feat= X_train.columns[(feat_sel_fit.get_support())]\nselected_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling "},{"metadata":{},"cell_type":"markdown","source":"#### Phase1: Data Preprocessing, Data Balancing"},{"metadata":{},"cell_type":"markdown","source":"##### Creating the subset from existing DatFrame for Analysis.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df=train_data[['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service','KPIs_met >80%', 'awards_won?','G_m','R_sourcing','is_promoted']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=new_df.drop(['is_promoted'],axis=1)\ny=new_df.is_promoted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**X will contain all the Independent variables such as no_of_trainings, age, previous_year_rating,  length_of_service, KPIs_met >80%, awards_won?, G_m,  R_sourcing <br>\nY has the is_promoted i.e. the Dependent variable**"},{"metadata":{},"cell_type":"markdown","source":"##### Data Balancing using SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote=SMOTE(sampling_strategy='minority')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = smote.fit_sample(x, y)\nX.shape,Y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Splitting the dataset into the Training set and Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Phase 2: Making the Neural Network (NN)"},{"metadata":{},"cell_type":"markdown","source":"##### Importing the Keras libraries and packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential # sequential module reqd to initialize the NN\nfrom keras.layers import * # dense module reqd to build the layers of the NN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Initialising the ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"promotion_pred = Sequential()# creating object of Sequential class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Adding the input layer and the hidden layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"promotion_pred.add(Dense(input_dim=8, activation=\"relu\", kernel_initializer=\"uniform\", units=5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.add method of object classifier to add layers.<br>\n2.Dense function will take care of the first step of ANN i.e. randomly intializing weights of synapses to small number close to 0 (but not 0); done with init = 'uniform' (initialize weights based on uniform distribution) 8 input nodes we know from our dataset; hence input_dim = 8.<br>\n3.Forward-propagation by applying the activiation function. Neuron applies the activation fn to the sum of weights inputs. The closer the activation fn value is to 1, the more activated the neuron, and the more activated the neuron, the more it passes on the signal.<br>\n4.Use rectifier activation fn for hiddern layers; activation = 'relu' units i.e the output dimensions is set = 6 which is the chosen number of nodes in this hidden layer.<br>\nTIP: no rule of thumb to choosing ouput dimensions; can choose average of the number of nodes in the input layer and the number of nodes in the output layer.**"},{"metadata":{},"cell_type":"markdown","source":"##### Adding the output layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"promotion_pred.add(Dense(activation = 'sigmoid', kernel_initializer = \"uniform\", units = 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Compiling the ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"promotion_pred.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.optimizer is the algo used to find the optimal no of weights in the NN (until now weights have only been initialized); 'adam' is a type of SGD algo loss deals with the loss function within the SGD algo which needs to be optimized(minimized);<br>\n2.loss fn for SGD going to be the same as that for logistic regression (logarithmic loss); since sigmoid fn used as activation fn we use log loss fn acuracy metric ensure that accuracy increases batch by batch; metrics parameter expecting a list so 'accuracy' added in []**\n"},{"metadata":{},"cell_type":"markdown","source":"##### Fitting the ANN to the Training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"History=promotion_pred.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_data=(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.Weights upated via batch learning so batch size needs to be specified (no rule of thumb)<br>\n2.When the whole training set is passed throught the ANN, that makes an epoch. <br>\n3.Epoch size needs to be specified (no rule of thumb)**"},{"metadata":{},"cell_type":"markdown","source":"#### Phase 3: Making the predictions and evaluating the model\n"},{"metadata":{},"cell_type":"markdown","source":"##### Predicting the Test set results."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = promotion_pred.predict(X_test)# this gives us the probability of a employee getting promoted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (y_pred > 0.5) # this above syntax is equivalent to sayig if y_pred>0.5 give value 1 and if not give value 0. Binary classification","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Making the Confusion Matrix\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom yellowbrick.classifier import ConfusionMatrix\nconf_mat=confusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report , confusion_matrix , accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\ncm_test = confusion_matrix(y_test, y_pred)\nfig, ax = plot_confusion_matrix(conf_mat=conf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of the model is - \" , promotion_pred.evaluate(X_test,y_test)[1]*100 , \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=y_pred.astype(int).flatten()\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification Report**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ncls = classification_report(y_test,y_pred)\nprint(cls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(History.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.style as style\nstyle.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loss-Train-Test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(History.history['loss'],label='train')\nplt.xlabel('epochs')\nplt.plot(History.history['val_loss'],label='test')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy-Train-Test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(History.history['accuracy'],label='train')\nplt.xlabel('epochs')\nplt.plot(History.history['val_accuracy'],label='test')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}