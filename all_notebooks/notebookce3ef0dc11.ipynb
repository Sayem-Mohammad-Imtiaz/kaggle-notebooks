{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementation of ID3 decision Tree Algorithm to classify Titanic Dataset","metadata":{}},{"cell_type":"code","source":"# if the file fail to run please contact me (jovi.wang@team.telstra.com)\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# /kaggle/input/titanic/train.csv\n# /kaggle/input/titanic/test.csv\n# /kaggle/input/titanic/gender_submission.csv      ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-03T11:32:27.678346Z","iopub.execute_input":"2021-07-03T11:32:27.678696Z","iopub.status.idle":"2021-07-03T11:32:27.689281Z","shell.execute_reply.started":"2021-07-03T11:32:27.678666Z","shell.execute_reply":"2021-07-03T11:32:27.68812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About the dataset\nTitanic - Machine Learning from Disaster\n\nPredict survival on the Titanic\n\n\n```shell\nraw_train.info()\n```\n\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n\n|    | column      | non-null count | Dtype |\n| -- | ----------- | -------------- | ----- |\n| 0  | PassengerId | 891 non-null   | int64 |\n| 1  | Survived    | 891 non-null   | int64 |\n| 2  | Pclass      | 891 non-null   | int64 |\n| 3  | Name        | 891 non-null   | object |\n| 4  | Sex         | 891 non-null   | object |\n| 5  | Age         | 714 non-null   | float64 |\n| 6  | SibSp       | 891 non-null   | int64 |\n| 7  | Parch       | 891 non-null   | int64 |\n| 8  | Ticket      | 891 non-null   | object |\n| 9  | Fare        | 891 non-null   | float64 |\n| 10  | Cabin      | 204 non-null   | object |\n| 11  | Embarked   | 889 non-null   | object |\n\n## column variable notes:\n\nSurvived: 1 = Yes, 0 = No\n\npclass: 1 = Upper, 2 = Middle, 3 = Lower\n\nSibSp: # sibling & spouse\n\nParch: # parent & child\n\n","metadata":{}},{"cell_type":"markdown","source":"# Preparing dataset \n* remove columns (Name, Ticker and Cabin) that are less likely contributing to model training process \n* convert numeric data type into categorical data type\n* drop missing rows\n* PassengerId will be used when validating the survival result","metadata":{}},{"cell_type":"code","source":"def pre_process_data (data):\n    \n    # 1. based on the attribute description, remove some less significant columns, \n    if pd.Series(['Name','Ticket','Cabin']).isin(data.columns).all():\n        data = data.drop(['Name','Ticket','Cabin'], axis=1)    \n        \n    # 2. convert survived column from int into string\n    # later in the loop rename them into 'Yes' and 'No'\n    if 'Survived' in data:\n        data[\"Survived\"] = data[\"Survived\"].astype(str)\n        \n    # 3. loop through the data, rename Survived column and add new columns\n    # new column #1: (Age_group) based on age range (ref: Age Categories, Life Cycle Groupings)\n    # new column #2: (Fare_group) based on fare histogram (divide into 3 groups, based on result of raw_train['Fare'].describe())\n    \n    for index, row in data.iterrows():\n        if 'Survived' in row:\n            data.at[index, 'Survived'] = 'Yes' if row['Survived'] == '1' else 'No'\n\n        if 'Age' in row:    \n            if not np.isnan(row['Age']):\n                if row['Age'] < 15:\n                    data.at[index, 'Age_group'] = 'Children'\n                elif row['Age'] < 25:\n                    data.at[index, 'Age_group'] = 'Youth'\n                elif row['Age'] < 65:\n                    data.at[index, 'Age_group'] = 'Adults'\n                else:\n                    data.at[index, 'Age_group'] = 'Seniors'\n    \n        #     raw_train['Fare'].describe()    \n        #     count    712.000000\n        #     mean      34.567251\n        #     std       52.938648\n        #     min        0.000000\n        #     25%        8.050000\n        #     50%       15.645850\n        #     75%       33.000000\n        #     max      512.329200\n        if 'Fare' in row:   \n            if row['Fare'] < 8.05:\n                data.at[index, 'Fare_group'] = 'Low_fare'\n            elif row['Fare'] < 33:\n                data.at[index, 'Fare_group'] = 'Medium_fare'\n            else:\n                data.at[index, 'Fare_group'] = 'High_fare'\n    \n    \n    # 4. remove Age and Fare,since they are re-categorised into Age_group and Fare_group\n    if pd.Series(['Age','Fare']).isin(data.columns).all():\n        data = data.drop(['Age', 'Fare'], axis=1)    \n    # remove any null values\n    # raw_train - missing 177 Age, 2 Embarked values\n    # raw_test - missing 86 Age values     \n    data = data.dropna()\n#     data = data.sort_values(by=['Pclass', 'Sex', 'Age_group', 'Fare_group', 'SibSp', 'Parch'], ascending=True)\n    return data\n\nraw_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nraw_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nraw_test_result = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\n# raw_test.info()\n\ndf_train = pre_process_data(raw_train)\ndf_test = pre_process_data(raw_test)\ndf_test_result = pre_process_data(raw_test_result)\n\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-07-03T11:32:27.722911Z","iopub.execute_input":"2021-07-03T11:32:27.723276Z","iopub.status.idle":"2021-07-03T11:32:28.033113Z","shell.execute_reply.started":"2021-07-03T11:32:27.723244Z","shell.execute_reply":"2021-07-03T11:32:28.032119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Core function and class\n* utility function to calculate entropy (**compute_entropy**) and information gain (**compute_info_gain**)\n* **TreeNode** class to simulate the tree-like model of decisions (root node, leaf node, paths from root to leaf)\n* functions in TreeNode class to train a decision tree model using ID3 algrithom (**fit**), predict a decision (**predict**) and print the tree model structure (**pretty_print**) ","metadata":{}},{"cell_type":"code","source":"def compute_entropy(y):\n    \"\"\"\n    param y: The data samples of a discrete distribution\n    \"\"\"\n    if len(y) < 2: #  a trivial case\n        return 0\n    # y.value_counts() get unique items value counts\n    # nomalize = True converts the value into percentage (range from 0 to 1)\n    freq = np.array(y.value_counts(normalize=True))\n    # print('freq', freq)\n    # the small eps for safe numerical computation \n    return -(freq * np.log2(freq + 1e-6)).sum() \n\n    \ndef compute_info_gain(samples, attr, target, silent = False):\n    if not silent:\n        print(f'evaluate attr: {attr}', target)\n    # samples, X\n    # attr, Age_group\n    # target, y\n    values = samples[attr].value_counts(normalize=True)\n    #  print(values) -> Adults 0.594101, Youth 0.280899, Children 0.109551, Seniors 0.015449\n    \n    # total entropy (entropy of target)\n    target_ent = compute_entropy(target)\n    \n    sum_sub_ent = 0\n    # compute weighted sum of the subgroup entropies\n    for v, freq in values.iteritems():\n        # iteration 1,      iteration 2,     iteration 3,       iteration 4\n        # v = Adults        v = Youth        v = Children       v = Seniors\n        # freq = 0.594101   freq = 0.280899  freq = 0.109551    freq = 0.015449\n        \n        # get sub group indexes \n        # retieve index of samples that have Age_group == Adults\n        index = samples[attr] == v\n        # calculate sub group entropy\n        # compute target entropy of samples that have Age_group == Adults\n        sub_ent = compute_entropy(target[index])\n        # accumilate the split entropy, sum weighted entropies\n        sum_sub_ent += freq * sub_ent\n        # print(f'subgroup:{v} freq:{freq} sub_ent:{sub_ent} sum_sub_ent:{sum_sub_ent}')\n    \n    # return total entropy subtract spli_entropy as infomation gain\n    if not silent:\n        print(f'    {attr}: target_ent: {target_ent} sum_sub_ent: {sum_sub_ent}')\n        print(f'    infomation gain: {target_ent - sum_sub_ent}')\n        print('')\n    return target_ent - sum_sub_ent\n\nclass TreeNode:\n    \"\"\"\n    A recursively defined data structure to store a tree.\n    Each node can contain other nodes as its children\n    \"\"\"\n    def __init__(self):\n        self.children = {} # Sub nodes --\n        # recursive, those elements of the same type (TreeNode)\n        self.decision = None # Undecided\n        self.split_feat_name = None # Splitting feature\n\n    def pretty_print(self, prefix=''):\n        if self.split_feat_name is not None:\n            for k, v in self.children.items():\n                v.pretty_print(f\"{prefix} :When {self.split_feat_name} is {k}\")\n                #v.pretty_print(f\"{prefix}:{k}:\")\n        else:\n            print(f\"{prefix}:{self.decision}\")\n\n    def predict(self, sample):\n        # exit condition for recursion \n        if self.decision is not None:\n            print(\"Decision:\", self.decision)\n            return self.decision\n        else: \n            attr_val = sample[self.split_feat_name]\n            if attr_val in self.children:\n                \n                child = self.children[attr_val]\n                print(\"Testing \", self.split_feat_name, \"->\", attr_val)\n\n                # call it self to start the recursion\n                return child.predict(sample)\n            else:\n                # when attr_val is not a key for self.children, this means the tree model does not know this data before\n                return 'Unknown'\n\n    def fit(self, X, y):\n        \"\"\"\n        The function accepts a training dataset, from which it builds the tree \n        structure to make decisions or to make children nodes (tree branches) \n        to do further inquiries\n        :param X: [n * p] n observed data samples of p attributes\n        :param y: [n] target values\n        \"\"\"\n        if len(X) == 0:\n            # If the data is empty when this node is arrived, \n            # we just make an arbitrary decision\n            self.decision = \"Yes\"\n            return\n        else: \n            # if remaining data all have same target value, then decision is the target value \n            if len(y.unique()) == 1:\n                print(f\"all target values are unique, make decision!\\n\")\n                self.decision = y.unique()[0]\n                return\n            else:\n                info_gain_max = 0\n                # Examine each attribute and find the biggest infomation gain\n                for a in X.keys():\n                    aig = compute_info_gain(X, a, y, silent = True)\n                    # aig = compute_info_gain(X, a, y)\n                    if aig > info_gain_max:\n                        info_gain_max = aig\n                        self.split_feat_name = a\n                # when information gain is 0 for all selected attribute \n                # thus, info_gain_max = 0 and self.split_feat_name is None, self.decision will be simple majority in y\n                if info_gain_max == 0 and self.split_feat_name == None:\n                    # print(X, y)\n                    print(f\"after evaluate each attribute, split_feat_name is None and IG_max is 0, make decision!\\n\")\n                    self.decision = y.value_counts().index.tolist()[0];\n                    return\n                \n                print(f\"Split by {self.split_feat_name}, IG: {info_gain_max:.6f}\")\n                print()\n                self.children = {}\n                for v in X[self.split_feat_name].unique():\n                    print(f'loop through feature {self.split_feat_name} unique values')\n                    index = X[self.split_feat_name] == v\n                    self.children[v] = TreeNode()\n                    self.children[v].fit(X[index], y[index])","metadata":{"execution":{"iopub.status.busy":"2021-07-03T11:32:28.034785Z","iopub.execute_input":"2021-07-03T11:32:28.035111Z","iopub.status.idle":"2021-07-03T11:32:28.054941Z","shell.execute_reply.started":"2021-07-03T11:32:28.035078Z","shell.execute_reply":"2021-07-03T11:32:28.053999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build tree models and evaluate performance","metadata":{}},{"cell_type":"markdown","source":"## Tree_1 build ","metadata":{}},{"cell_type":"code","source":"# Test tree building with all attributes\nattrs_1 = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Age_group', 'Fare_group']\ndata_1 = df_train[attrs_1]\ntarget = df_train[\"Survived\"]\n\ntree_1 = TreeNode()\ntree_1.fit(data_1, target)\n# tree_1.pretty_print(prefix = 'tree_1')\n# end up getting a big tree with many branches","metadata":{"execution":{"iopub.status.busy":"2021-07-03T11:32:28.056771Z","iopub.execute_input":"2021-07-03T11:32:28.057093Z","iopub.status.idle":"2021-07-03T11:32:30.843989Z","shell.execute_reply.started":"2021-07-03T11:32:28.057063Z","shell.execute_reply":"2021-07-03T11:32:30.842996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tree_2 build","metadata":{}},{"cell_type":"code","source":"# Test tree building with less attributes (skip SibSp and Parch)\nattrs_2 = ['Pclass', 'Sex', 'Embarked', 'Age_group', 'Fare_group']\ndata_2 = df_train[attrs_2]\ntarget = df_train[\"Survived\"]\n\ntree_2 = TreeNode()\ntree_2.fit(data_2, target)\n# tree_2.pretty_print(prefix='tree_2')","metadata":{"execution":{"iopub.status.busy":"2021-07-03T11:32:30.845626Z","iopub.execute_input":"2021-07-03T11:32:30.845954Z","iopub.status.idle":"2021-07-03T11:32:32.062253Z","shell.execute_reply.started":"2021-07-03T11:32:30.845921Z","shell.execute_reply":"2021-07-03T11:32:32.061274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test tree building with less attributes (skip SibSp and Parch)\nattrs_3 = ['Pclass', 'Sex','Age_group', 'Fare_group']\ndata_3 = df_train[attrs_3]\ntarget = df_train[\"Survived\"]\n\ntree_3 = TreeNode()\ntree_3.fit(data_3, target)\n# tree_3.pretty_print(prefix='tree_3')","metadata":{"execution":{"iopub.status.busy":"2021-07-03T11:32:32.063468Z","iopub.execute_input":"2021-07-03T11:32:32.063767Z","iopub.status.idle":"2021-07-03T11:32:32.696202Z","shell.execute_reply.started":"2021-07-03T11:32:32.063736Z","shell.execute_reply":"2021-07-03T11:32:32.695062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model performance using test dataset","metadata":{}},{"cell_type":"code","source":"count_1 = 0\ncount_2 = 0\ncount_3 = 0\n\nfor i, row in df_test.iterrows():\n    pred_value_1 = tree_1.predict(row)\n    pred_value_2 = tree_2.predict(row)\n    pred_value_3 = tree_3.predict(row)\n    # pred_value is Yes, No and Unknown\n\n    passengerId = row['PassengerId']\n    test_result_value = df_test_result.loc[df_test_result['PassengerId'] == passengerId]['Survived'];\n        \n    if (test_result_value == pred_value_1).all():\n        count_1 += 1\n    if (test_result_value == pred_value_2).all():\n        count_2 += 1\n    if (test_result_value == pred_value_3).all():\n        count_3 += 1\nprint()    \nprint(f'accurate rate for t1 is: {count_1/len(df_test)}')\nprint(f'accurate rate for t2 is: {count_2/len(df_test)}')\nprint(f'accurate rate for t3 is: {count_3/len(df_test)}')\n# one result is like below\n# accurate rate for t1 is: 0.7349397590361446\n# accurate rate for t2 is: 0.7530120481927711\n# accurate rate for t3 is: 0.8734939759036144","metadata":{"execution":{"iopub.status.busy":"2021-07-03T11:32:32.697614Z","iopub.execute_input":"2021-07-03T11:32:32.698037Z","iopub.status.idle":"2021-07-03T11:32:35.500881Z","shell.execute_reply.started":"2021-07-03T11:32:32.697993Z","shell.execute_reply":"2021-07-03T11:32:35.499862Z"},"trusted":true},"execution_count":null,"outputs":[]}]}