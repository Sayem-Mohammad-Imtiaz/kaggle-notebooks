{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### **Problem Statement**\nShip or vessel detection has a wide range of applications, in the areas of maritime safety, fisheries management, marine pollution, defence and maritime security, protection from piracy, illegal migration, etc.\n\nKeeping this in mind, a Governmental Maritime and Coastguard Agency is planning to deploy a computer vision based automated system to identify ship type only from the images taken by the survey boats. You have been hired as a consultant to build an efficient model for this project.\n\nThere are 5 classes of ships to be detected.\n\n**Dataset Description**\nThere are 6252 images in train and 2680 images in test data. The categories of ships and their corresponding codes in the dataset are as follows -\n\n'Cargo' -> 1\n'Military' -> 2\n'Carrier' -> 3\n'Cruise' -> 4\n'Tankers' -> 5\n\nThere are three files provided to you, viz train.zip, test.csv and sample_submission.csv which have the following structure.\n\nVariable\t   Definition\nimage\t    :  Name of the image in the dataset (ID column)\ncategory\t:  Ship category code (target column)\n\ntrain.zip contains the images corresponding to both train and test set along with the true labels for train set images in train.csv\n\n**Evaluation Metric**\nThe Evaluation metric for this competition is weighted F1 Score"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom tqdm import tqdm\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### install some cool stuff\n! pip install albumentations    #### for augmentations that work faster\n! pip install pytorchcv         #### More pre-trained models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport time \nimport tqdm\nimport random\nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n\nimport cv2\n\nimport albumentations\nfrom albumentations import torch as AT\n#import pretrainedmodels\n\nimport scipy.special\n\nfrom pytorchcv.model_provider import get_model as ptcv_get_model\n\ncudnn.benchmark = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Need to seed everything so we can reproduce results as well\nSEED = 323\nbase_dir = '../input/'\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    os.environ['PYHTONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Taking a look at folder and file structure \n!ls '../input/'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### images are @ : '../input/train/images/' has both train+test images \n### Train Labels along with image ID is @: '../input/train/train.csv'\n### Test file with test image IDs '../input/test_ApKoW4T.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### read labels/image \nlabels = pd.read_csv(base_dir+'/train/train.csv')\nlabels.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Label Distribution:\nimport seaborn as sns\nlabels['category'].value_counts()\nsns.countplot(x='category' , data=labels)\nplt.title(\"Category dist\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## mapping the ship categories  \ncategory = {'Cargo': 1, \n'Military': 2, \n'Carrier': 3, \n'Cruise': 4, \n'Tankers': 5}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### preparing some useful stuff for easy vizualizations later on\nRev = dict((v,k) for k,v in category.items())  ### need to reverse\ncaT = pd.DataFrame(labels['category'].map(Rev))\ncaT.rename(columns={'category': \"CategoryText\"},inplace=True)\ncaT = pd.concat([labels,pd.DataFrame(caT)],1)\ncaT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import sample\nimport cv2\nimport matplotlib.image as mpimg\n\ndef plotClass(category,N):\n    categoryIdx = caT[caT['CategoryText']==category].index[:30]\n    randIdx = sample(list(categoryIdx),N)\n    jpegName = caT.iloc[randIdx,:]['image'].values\n    fig = plt.figure(figsize=(18,14))\n    for i , jpeg in enumerate(list(jpegName)):\n        plt.subplot(1,N ,i+1)\n        imgFile = mpimg.imread('../input/train/images/{}'.format(jpeg))\n        plt.imshow(imgFile)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotClass(\"Military\",4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotClass(\"Cargo\",4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotClass(\"Carrier\",4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotClass(\"Cruise\",4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotClass(\"Tankers\",4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### different sized images, vertical flipping does not make sense as these are front on/hortizontal pictures\n### left-to-right flip can be considered augmentation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n\ndef prepare_labels(y):\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y, label_encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y, le_full = prepare_labels(labels['category'])\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels['category'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### map imge names to labels\nimg_class_dict = {k:v for k, v in zip(labels.image, labels.category)}\nimg_class_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### DataLoader\nfrom PIL import Image\n\nclass ShipDataLoader(torch.utils.data.DataLoader):\n    def __init__(self,CSVfolder = '../input/train/train.csv',process='train',transform = transforms.Compose([transforms.CenterCrop(64),transforms.ToTensor()]),\n                 imgFolder='../input/train/images/',labelsDict = {},y=y):\n        self.process = process\n        self.imgFolder = imgFolder\n        self.CSVfolder = CSVfolder\n        self.y = y\n        self.FileList = pd.read_csv(self.CSVfolder)['image'].tolist()\n        self.transform = transform\n        self.labelsDict = labelsDict\n        \n        if self.process =='train':\n            self.labels = [labelsDict[i] for i in self.FileList]\n        else:\n            self.labels = [0 for i in range(len(self.FileList))]\n\n    def __len__(self):\n        return len(self.FileList)\n    \n    def __getitem__(self,idx):\n        fname =  self.FileList[idx]\n        img_name = os.path.join(self.imgFolder, fname)\n        img = cv2.imread(img_name)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        image = self.transform(image=img)\n        image = image['image']\n        if self.process == 'train':\n            label = self.y[idx]\n        else:\n            label = np.zeros((5,))\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Augmentations from albumenations\nsz = 224\n\ndata_transforms = albumentations.Compose([\n    albumentations.Resize(sz, sz),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.Transpose(p=0.5),\n    albumentations.Flip(p=0.0),\n    albumentations.OneOf([\n        albumentations.CLAHE(clip_limit=2), albumentations.IAASharpen(), albumentations.IAAEmboss(), \n        albumentations.RandomBrightness(), albumentations.RandomContrast(),\n        albumentations.JpegCompression(), albumentations.Blur(), albumentations.GaussNoise()], p=0.5), \n    albumentations.HueSaturationValue(p=0.5), \n    albumentations.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.15, rotate_limit=45, p=0.5),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n    AT.ToTensor()\n    ])\n\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(sz, sz),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n    AT.ToTensor()\n    ])\n\ndata_transforms_tta0 = albumentations.Compose([\n    albumentations.Resize(sz, sz),\n    albumentations.RandomRotate90(p=0.5),\n    albumentations.Transpose(p=0.5),\n    albumentations.Flip(p=0.5),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n    AT.ToTensor()\n    ])\n\ndata_transforms_tta1 = albumentations.Compose([\n    albumentations.Resize(sz, sz),\n    albumentations.RandomRotate90(p=1),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n    AT.ToTensor()\n    ])\n\ndata_transforms_tta2 = albumentations.Compose([\n    albumentations.Resize(sz, sz),\n    albumentations.Transpose(p=1),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n    AT.ToTensor()\n    ])\n\ndata_transforms_tta3 = albumentations.Compose([\n    albumentations.Resize(sz, sz),\n    albumentations.HorizontalFlip(p=1),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n    AT.ToTensor()\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 90-10 train-validation split\ntr, val = train_test_split(labels.category, stratify=labels.category, test_size=0.1, random_state=42)\n\n### Batchsize and parallelize\n\nBS = 64\nnum_workers = 8\n\n#### Idx for train and valid\ntrain_sampler = SubsetRandomSampler(list(tr.index)) \nvalid_sampler = SubsetRandomSampler(list(val.index))\n\n#### Train dataloader ####\ntraindataset = ShipDataLoader(transform = data_transforms,\n                              imgFolder='../input/train/images/',process='train',\n                              labelsDict = img_class_dict,y=y)\ntrain_loader = torch.utils.data.DataLoader(traindataset, batch_size=BS,sampler=train_sampler,num_workers=num_workers)\n\n#### Valid dataloader ####\n\nvaldataset = ShipDataLoader(transform = data_transforms_test,\n                              imgFolder='../input/train/images/',process='train',\n                              labelsDict = img_class_dict,y=y)\nval_loader = torch.utils.data.DataLoader(valdataset, batch_size=BS,sampler=valid_sampler,num_workers=num_workers)\n\n#### Test dataloader ####\n\ntestdataset = ShipDataLoader(transform = data_transforms_test,\n                              imgFolder='../input/train/images/',process='test',\n                              labelsDict = img_class_dict,y=y)\ntest_loader = torch.utils.data.DataLoader(testdataset, batch_size=BS,num_workers=num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nclass = labels['category'].nunique()\nnclass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modName = 'SENet154'\nmodel_conv = ptcv_get_model(modName, pretrained=True)\n\n### set all weight to be not trainable\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\ndef count_parameters(model):\n    '''\n    Count of trainable weights in a model\n    '''\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ncount_parameters(model_conv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parent_counter = 0\nfor child in model_conv.children():\n    child_counter = 0\n    for c in child.children():\n        child_counter += 1\n        print(\"Parent: \",parent_counter,\"Child: \",child_counter)\n    parent_counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### unfreeze the few last layers of SENET154 models; based only only flattened hierarchy\n\ndef unfreeze(n):\n    child_child_counter = 0\n    for child in model_conv.children():\n        for c in child.children():\n            if child_child_counter > n:\n                for param in c.parameters():\n                    param.requires_grad = True\n            child_child_counter += 1\n            \n            \ndef freeze(n):\n    child_child_counter = 0\n    for child in model_conv.children():\n        for c in child.children():\n            if (child_child_counter > n) and (child_child_counter <= 4):\n                for param in c.parameters():\n                    param.requires_grad = False\n            else:\n                break\n            child_child_counter += 1\n\nunfreeze(4)            \ncount_parameters(model_conv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### We need to replace the last FC layer and then add custom head\nmodel_conv.output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = 0.\nmodel_conv.output = nn.Sequential(nn.Dropout(d), nn.Linear(in_features=2048, out_features=1024, bias=True), nn.SELU(),\n                                     nn.Dropout(d),nn.Linear(in_features=1024, out_features=512, bias=True), nn.SELU(), nn.Dropout(d), \n                                  nn.Linear(in_features=512, out_features=nclass, bias=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(model_conv.output.children())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_parameters(model_conv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv.cuda()\ncriterion = nn.BCEWithLogitsLoss() ### numerically stable as compared to BCELoss\noptimizer = optim.Adam(model_conv.parameters(), lr=1e-4)  ### will only consider the newly added FC and the unfrozen  block weight\nscheduler = StepLR(optimizer, 5, gamma=0.2)\nscheduler_cosineAL = lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader), eta_min=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nuseCosine = True\nmodel_conv.cuda()\nn_epochs = 100\nlayerWiseFreeze = False\npatience = 25\n# current number of tests, where validation loss didn't increase\np = 0\n# whether training should be stopped\nstop = False\n\n# Show batch f1 results\nbatchRes = False\n\n## loss lists\nloss_list = []\ntrain_f1_avg=[]\nval_f1_avg = []\nval_f1_max=0\n\nfor epoch in range(1, n_epochs+1):\n    print(time.ctime(), 'Epoch:', epoch)\n    \n    if epoch == 10:\n            n = 3\n            unfreeze(n)\n            \n    if layerWiseFreeze:\n        if epoch == 5:\n            n = 3\n            unfreeze(n)\n            print(\"Unfreezing layers from {} grouping onwards\".format(n))\n        elif epoch == 10:\n            freeze(n)\n            print(\"Freezing layers from {} grouping onwards\".format(n))\n        elif epoch == 15:\n            unfreeze(n)\n            print(\"Unfreezing layers from {} grouping onwards\".format(n))\n    \n    train_loss_epoch = []\n    val_loss_epoch = []\n    train_f1_epoch = []\n    val_f1_epoch = []\n\n    if useCosine:\n        scheduler_cosineAL.step()\n    else:\n        scheduler.step()\n\n    for batch_i, (data, target) in  enumerate(train_loader):\n        data, target = data.cuda(), target.cuda()\n        optimizer.zero_grad()\n        output = model_conv(data)\n        \n        loss = criterion(output, target.float())\n        train_loss_epoch.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        \n        a = target.data.cpu().numpy()\n        b = output.data.cpu().numpy()\n        train_f1_scr_iter = f1_score(np.argmax(a,1),np.argmax(b,1),average='weighted')\n        train_f1_epoch.append(train_f1_scr_iter)\n        \n        if (batch_i+1)%25 == 0:    \n            val_loss = []\n            model_conv.eval()\n            for val_batch_i, (data, target) in enumerate(val_loader):\n                data, target = data.cuda(), target.cuda()\n                output = model_conv(data)\n                loss = criterion(output, target.float())\n                val_loss_epoch.append(loss.item()) \n                a1 = target.data.cpu().numpy()\n                b1 = output.data.cpu().numpy()\n                val_f1_scr_iter = f1_score(np.argmax(a1,1),np.argmax(b1,1),average='weighted')\n                val_f1_epoch.append(val_f1_scr_iter)\n    if batchRes:\n        try:\n            print(\"Batch Val f1\",f1_score(np.argmax(a1,1),np.argmax(b1,1),average='weighted'))\n        except:\n            pass\n        \n    print(\"Epoch: {},Train F1: {}, Val F1: {}\".format(epoch,np.mean(train_f1_epoch),np.mean(val_f1_epoch)))\n    train_f1_epoch.append(np.mean(train_f1_epoch))\n    val_f1_avg.append(np.mean(val_f1_epoch))\n    \n    if np.mean(val_f1_epoch) > val_f1_max:\n        print('Validation F1 increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        val_f1_max,\n        np.mean(val_f1_epoch)))\n        torch.save(model_conv.state_dict(), 'model.pt')\n        val_f1_max = np.mean(val_f1_epoch)\n        e  = epoch\n        p = 0\n    else:\n        p += 1\n        if p > patience:\n            print('Early stop training')\n            stop = True\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best Val F1 loss:{}, Epoch:{}\".format(val_f1_max,e))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### empty cuda cache\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_conv.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    saved_dict = torch.load('model.pt')\n    model_conv.load_state_dict(saved_dict)\nexcept:\n    model_conv.load_state_dict(model_conv.state_dict())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TTA_ITER = 10\nfrom tqdm import tqdm\n\npreds = {}\ndef getPreds(TTA_ITER):\n    for num_tta in tqdm(range(TTA_ITER)):\n        iterpreds = []\n        if num_tta==0:\n            testdataset = ShipDataLoader(CSVfolder='../input/test_ApKoW4T.csv',transform = data_transforms_test,\n                                  imgFolder='../input/train/images/',process='test',\n                                  labelsDict = img_class_dict,y=y)\n            test_loader = torch.utils.data.DataLoader(testdataset, batch_size=BS,num_workers=num_workers)\n\n        elif num_tta==1:\n            testdataset = ShipDataLoader(CSVfolder='../input/test_ApKoW4T.csv',transform = data_transforms_tta1,\n                                  imgFolder='../input/train/images/',process='test',\n                                  labelsDict = img_class_dict,y=y)\n            test_loader = torch.utils.data.DataLoader(testdataset, batch_size=BS,num_workers=num_workers,shuffle=False)\n\n        elif num_tta==2:\n            testdataset = ShipDataLoader(CSVfolder='../input/test_ApKoW4T.csv',transform = data_transforms_tta2,\n                                  imgFolder='../input/train/images/',process='test',\n                                  labelsDict = img_class_dict,y=y)\n            test_loader = torch.utils.data.DataLoader(testdataset, batch_size=BS,num_workers=num_workers,shuffle=False)\n\n        elif num_tta==3:\n\n            testdataset = ShipDataLoader(CSVfolder='../input/test_ApKoW4T.csv',transform = data_transforms_tta3,\n                                  imgFolder='../input/train/images/',process='test',\n                                  labelsDict = img_class_dict,y=y)\n            test_loader = torch.utils.data.DataLoader(testdataset, batch_size=BS,num_workers=num_workers,shuffle=False)\n\n        elif num_tta<8:\n            testdataset = ShipDataLoader(CSVfolder='../input/test_ApKoW4T.csv',transform = data_transforms_tta0,\n                                  imgFolder='../input/train/images/',process='test',\n                                  labelsDict = img_class_dict,y=y)\n            test_loader = torch.utils.data.DataLoader(testdataset, batch_size=BS,num_workers=num_workers,shuffle=False)\n\n        else:\n            testdataset = ShipDataLoader(CSVfolder='../input/test_ApKoW4T.csv',transform = data_transforms_tta0,\n                                  imgFolder='../input/train/images/',process='test',\n                                  labelsDict = img_class_dict,y=y)\n            test_loader = torch.utils.data.DataLoader(testdataset, batch_size=BS,num_workers=num_workers,shuffle=False)\n\n        for batch_i, (data, target) in enumerate(test_loader):\n            data, target = data.cuda(), target.cuda()\n            output = model_conv(data).detach()\n            sm = torch.nn.Softmax()\n            probabilities = sm(output).data.cpu().numpy()\n            iterpreds.append(probabilities)\n        preds[num_tta]=iterpreds\n        \n        \n    _1 = [j for i in preds[0] for j in i ]\n    flat_list1 = [item for sublist in _1 for item in sublist]\n    a1 =np.array(flat_list1).reshape(-1,5)\n\n    _2 = [j for i in preds[1] for j in i ]\n    flat_list2 = [item for sublist in _2 for item in sublist]\n    a2 =np.array(flat_list2).reshape(-1,5)\n\n    _3 = [j for i in preds[2] for j in i ]\n    flat_list3 = [item for sublist in _3 for item in sublist]\n    a3 =np.array(flat_list3).reshape(-1,5)\n\n    _4 = [j for i in preds[3] for j in i ]\n    flat_list4 = [item for sublist in _4 for item in sublist]\n    a4 =np.array(flat_list4).reshape(-1,5)\n\n    _5 = [j for i in preds[4] for j in i ]\n    flat_list5 = [item for sublist in _5 for item in sublist]\n    a5 =np.array(flat_list5).reshape(-1,5)\n\n    _6 = [j for i in preds[5] for j in i ]\n    flat_list6 = [item for sublist in _6 for item in sublist]\n    a6 =np.array(flat_list6).reshape(-1,5)\n\n    _7 = [j for i in preds[6] for j in i ]\n    flat_list7 = [item for sublist in _7 for item in sublist]\n    a7 =np.array(flat_list7).reshape(-1,5)\n\n    _8 = [j for i in preds[7] for j in i ]\n    flat_list8 = [item for sublist in _8 for item in sublist]\n    a8 =np.array(flat_list8).reshape(-1,5)\n\n    _9 = [j for i in preds[8] for j in i ]\n    flat_list9 = [item for sublist in _9 for item in sublist]\n    a9 =np.array(flat_list9).reshape(-1,5)\n\n    _10 = [j for i in preds[9] for j in i ]\n    flat_list10 = [item for sublist in _10 for item in sublist]\n    a10 =np.array(flat_list10).reshape(-1,5)\n    \n    temp = (a1+a2+a3+a4+a5+a6+a7+a8+a9+a10)/10\n    \n    labelMAP = {0:1,1:2,2:3,3:4,4:5}\n    labelsUPD = np.vectorize(labelMAP.get)(np.argmax(temp,1))\n    \n    testCSV = pd.read_csv('../input/sample_submission_ns2btKE.csv')\n    testCSV['category'] = labelsUPD\n    valF1 = val_f1_max\n    fname = 'AVsubmission_'+str(modName)+\"_\"+str(valF1)\n    return fname,testCSV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fname,testCSV = getPreds(TTA_ITER=10)\ntestCSV.to_csv(fname+'.csv',index=False)\nfrom IPython.display import FileLink, FileLinks\nFileLink(fname+'.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"### End of code ### "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}