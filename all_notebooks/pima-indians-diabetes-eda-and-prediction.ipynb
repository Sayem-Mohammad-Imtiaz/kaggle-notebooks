{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import the data\ndiabetes = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gets name of column\ndiabetes.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#information about the column\ndiabetes.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get basic statistics about the data\ndiabetes.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for the null values\ndiabetes.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion 1\n\nIt seems there is no columns with null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us look at first 10 rows\ndiabetes.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion 2\n\nBy looking at the columns SkinThickness,BloodPressure,Glucose and BMI, we come at a conclusion that their values can't be zeros. It means null values are represented by 0 in these columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_for_zero(columns):\n    for col in columns:\n        if 0 in diabetes[col]:\n            print(col+' has 0 in it.')\n\ncolumns = ['Glucose', 'BloodPressure', 'SkinThickness',\n           'BMI', 'DiabetesPedigreeFunction', 'Age',]\ncheck_for_zero(columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These all column has zeros in it and it can't be, so we will find a way to replace these zeros:\n1. One obvious way is to replace them with the mean, median or mode of respective columns, since they are all non categorical columns\n2. Other way can be to replace with the mean, median or mode of respective columns based on outcome columns.\n\nIn this kernel we will follow first."},{"metadata":{"trusted":true},"cell_type":"code","source":"#before doing so, we will split our X and y\nX = diabetes.drop('Outcome',axis=1)\nY = diabetes['Outcome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=0, strategy='mean')\nX[columns] = imp.fit_transform(X[columns]) #fit the imputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion 3\nTill now we have transformed our coulumns containing 0 in it. Now there is a column named Insulin which also have zeros but since I have no idea whether this column can contains zero too, so I will drop this column"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop('Insulin',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's do some EDA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(X['Pregnancies'])\nX['Pregnancies'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_dist(column):\n    plt.figure()\n    return sns.distplot(X[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns:\n    draw_dist(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us have some visualization about y\nsns.countplot(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since all columns are not on same scale so let us normalize them\n# Import the necessary modules\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier(n_neighbors=5))]\n        \n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.4, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train,y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy on training data: {}'.format(knn_scaled.score(X_train,y_train)))\nprint('Accuracy on test data: {}'.format(knn_scaled.score(X_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#implement logistic regression\n# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', LogisticRegression())]\n        \n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.4, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nlogreg_scaled = pipeline.fit(X_train,y_train)\n\n# Compute and print metrics\nprint('Accuracy on training data: {}'.format(logreg_scaled.score(X_train,y_train)))\nprint('Accuracy on test data: {}'.format(logreg_scaled.score(X_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#implement logistic regression\n# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', SVC())]\n        \n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.4, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nSvm_scaled = pipeline.fit(X_train,y_train)\n\n# Compute and print metrics\nprint('Accuracy on training data: {}'.format(Svm_scaled.score(X_train,y_train)))\nprint('Accuracy on test data: {}'.format(Svm_scaled.score(X_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}