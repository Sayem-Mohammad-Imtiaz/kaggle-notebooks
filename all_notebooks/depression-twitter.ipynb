{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Index \n<a id='index'></a>\n[Import libraries](#import_libraries) <br>\n[Import functions](#import_functions) <br>\n[Import dataset](#import_dataset) <br>\n[Add polarity and subjectivity](#add_polarity_subj) <br>\n[Look at pair plots](#pair_plot) <br>\n[Word Cloud](#word_cloud) <br>\n[Word Cloud for depressed](#word_cloud_dep) <br>\n[Word Cloud for non-depressed](#word_cloud_non-dep) <br>\n[Topic modelling on depressed](#lsa_depr) <br>\n[Topic modelling on non-depressed](#lsa_nondepr) <br>","metadata":{}},{"cell_type":"markdown","source":"### Import libraries <a id='import_libraries'></a>\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#Text manipulation\nimport nltk\nfrom textblob import TextBlob\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom gensim import matutils, models\nimport scipy.sparse\n\n#Text visualisation \nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction import text \n\n## visualisation\n#import pyLDAvis.gensim\n#from gensim.corpora import Dictionary\n#from gensim.models.coherencemodel import CoherenceModel\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nimport spacy\nspacy.load(\"en_core_web_sm\")\n\nnltk.download('stopwords')\n\n# deactivate deprecation warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import functions for tweets <a id='import_functions'></a>\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"###### Function to prepare the tweets\ndef clean_text_1(text):\n    # Lowercase\n    text = text.lower()\n    # Remove special text in brackets ([chorus],[guitar],etc)\n    text = re.sub('\\[.*?\\]', '', text)\n    # Remove punctuation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # Remove words containing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)    \n    # Remove quotes\n    text = re.sub('[‘’“”…]', '', text)\n    # Remove new line \\n \n    text = re.sub('\\n', ' ', text)\n    # Remove stop_word\n    stop_words = stopwords.words('english')\n    words = word_tokenize(text)\n    new_text = \"\"\n    for w in words:\n        if w not in stop_words and len(w) > 1:\n            new_text = new_text + \" \" + w\n    return new_text\n\n##### Function to integrate polarity and subjectivity in the tweets\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\n### Function to lemmatize the text\n\ndef lemmatize_tag(text):\n    wnl = WordNetLemmatizer()\n    lemma=[]\n    for i,j in pos_tag(word_tokenize(text)) :\n        p=j[0].lower()\n        if p in ['j','n','v']:\n            if p == 'j':\n                p = 'a'\n            lemma.append(wnl.lemmatize(i,p))\n        else :\n            lemma.append(wnl.lemmatize(i))    \n    return ' '.join(lemma)\n\n\n### Function to extract nouns\ndef nouns(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n    is_noun = lambda pos: pos[:2] == 'NN'\n    tokenized = word_tokenize(text)\n    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n    return ' '.join(all_nouns)\n\n# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\n## lemmatization with noun, adjective, verbs, adverb\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import dataset <a id='import_dataset'></a>\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv(\"/kaggle/input/sentimental-analysis-for-tweets/sentiment_tweets3.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add columns for polarity and subjectivity<a id='add_polarity_subj'></a>\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"df['polarity'] = df['message to examine'].apply(pol)\ndf['subjectivity'] = df['message to examine'].apply(sub)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unbalanced data. Less depressed","metadata":{}},{"cell_type":"code","source":"\nsns.histplot(df['label (depression result)'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pair plots <a id='pair_plot'></a>\n\nPolarity is not separating depression.\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df[['polarity','subjectivity','label (depression result)']],hue='label (depression result)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Cloud <a id='word_cloud'></a>\n\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"# Clean text\ndf_clean = pd.DataFrame(df['message to examine'].apply(clean_text_1)).copy()\ndf_clean = pd.DataFrame(df['message to examine'].apply(lemmatize_tag)).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define stop words for text cleaning\nstop_words = stopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define stop words for text cleaning\nstop_words2=[]\nfor w in stop_words:\n    stop_words2.append(w)\n\nstop_words2.extend(['http://t.',\"I'm\",'http',\"can't\",'Å','Ā','like','t','åā','www','com','https'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = WordCloud(collocations=False,stopwords=stop_words2, background_color='white', colormap='Dark2',\n               max_font_size=150, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_to_analyze = ''\nfor i in df['message to examine']:\n    text_to_analyze = text_to_analyze + ' ' + i","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = wc.generate(text_to_analyze)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'depression' is appearing at biggest","metadata":{}},{"cell_type":"code","source":"# Wordcloud plot\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Wordcloud for Tweets')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Cloud for depressed <a id='word_cloud_dep'></a>\n\nThe words 'depression' and 'axiety' are appearing as more relevant\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"#Extract tweets from depressed\ndf_depressed = df_clean[df['label (depression result)']==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_to_analyze = ''\nfor i in df_depressed['message to examine']:\n    text_to_analyze = text_to_analyze + ' ' + i\nwc = WordCloud(collocations=False,stopwords=stop_words2, background_color='white', colormap='Dark2',\n               max_font_size=150, random_state=42)\nwc = wc.generate(text_to_analyze)\n# Wordcloud plot\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Wordcloud for depressed Tweets')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Cloud for non-depressed <a id='word_cloud_non_dep'></a>\n\nWords like 'good', 'thank' and 'love' are more relevant\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"#Extract tweets from depressed\ndf_nondepressed = df_clean[df['label (depression result)']==0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words2.extend(['get'])\ntext_to_analyze = ''\nfor i in df_nondepressed['message to examine']:\n    text_to_analyze = text_to_analyze + ' ' + i\nwc = WordCloud(collocations=False,stopwords=stop_words2, background_color='white', colormap='Dark2',\n               max_font_size=150, random_state=42)\nwc = wc.generate(text_to_analyze)\n# Wordcloud plot\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Wordcloud for non-depressed Tweets')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Topic modelling on depressed <a id='lsa_depr'></a>\n\nfirst topic: 'depression', 'anxiety'\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer()#CountVectorizer(min_df=.2, max_df=.8,stop_words=stop_words2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put tweets in list\nalltweets = []\nfor i in df_depressed['message to examine']:\n    alltweets.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_cv = cv.fit_transform(alltweets)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = df_depressed.index\ndata_dtm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words=stop_words2)#min_df=.4, max_df=.8,stop_words=stop_words2)\ntfidf = vectorizer.fit_transform(alltweets)\n#len(vectorizer.get_feature_names())\ndata_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names())\ndata_tfidf.index = df_depressed.index\ndata_tfidf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# SVD represent documents and terms in vectors \nsvd_model = TruncatedSVD(n_components=5) #try with 10 topics\nsvd_model.fit(data_tfidf)\nprint(svd_model.components_.shape)\nprint(svd_model.singular_values_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"terms = vectorizer.get_feature_names()\n# Print out the topics\nfor i, comp in enumerate(svd_model.components_):\n    terms_comp = zip(terms, comp)\n    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_terms:\n        print(\"%.2f*%s \"% (t[1], t[0]) ,end='')\n    print(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Topic modelling on non-depressed <a id='lsa_nondepr'></a>\n\nfirst topic: 'good', 'day,'love', 'today'\n\n[Back to Index](#index)","metadata":{}},{"cell_type":"code","source":"# Put tweets in list\nalltweets = []\nfor i in df_nondepressed['message to examine']:\n    alltweets.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_cv = cv.fit_transform(alltweets)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = df_nondepressed.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words=stop_words2)#min_df=.4, max_df=.8,stop_words=stop_words2)\ntfidf = vectorizer.fit_transform(alltweets)\n#len(vectorizer.get_feature_names())\ndata_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names())\ndata_tfidf.index = df_nondepressed.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# SVD represent documents and terms in vectors \nsvd_model = TruncatedSVD(n_components=5) #try with 10 topics\nsvd_model.fit(data_tfidf)\nprint(svd_model.components_.shape)\nprint(svd_model.singular_values_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"terms = vectorizer.get_feature_names()\n# Print out the topics\nfor i, comp in enumerate(svd_model.components_):\n    terms_comp = zip(terms, comp)\n    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_terms:\n        print(\"%.2f*%s \"% (t[1], t[0]) ,end='')\n    print(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}