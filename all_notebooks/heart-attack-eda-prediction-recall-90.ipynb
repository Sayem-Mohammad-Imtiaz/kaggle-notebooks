{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nIn this task we have to classify whether person is likely to get heart attack, metric we want to maximize is recall (because we want to find all persons that are likely to get heart attack).\n\n    Age : Age of the patient\n\n    Sex : Sex of the patient\n\n    exang: exercise induced angina (1 = yes; 0 = no)\n\n    ca: number of major vessels (0-3)\n\n    cp : Chest Pain type chest pain type\n        Value 1: typical angina\n        Value 2: atypical angina\n        Value 3: non-anginal pain\n        Value 4: asymptomatic\n\n    trtbps : resting blood pressure (in mm Hg)\n\n    chol : cholestoral in mg/dl fetched via BMI sensor\n\n    fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n\n    rest_ecg : resting electrocardiographic results\n        Value 0: normal\n        Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n        Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n    thalach : maximum heart rate achieved\n\n    target(output) : 0= less chance of heart attack 1= more chance of heart attack\n","metadata":{}},{"cell_type":"markdown","source":"# Imports & Read Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:00.950826Z","iopub.execute_input":"2021-06-30T16:07:00.951228Z","iopub.status.idle":"2021-06-30T16:07:00.957111Z","shell.execute_reply.started":"2021-06-30T16:07:00.951191Z","shell.execute_reply":"2021-06-30T16:07:00.955664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:00.963168Z","iopub.execute_input":"2021-06-30T16:07:00.963747Z","iopub.status.idle":"2021-06-30T16:07:00.999004Z","shell.execute_reply.started":"2021-06-30T16:07:00.963711Z","shell.execute_reply":"2021-06-30T16:07:00.997758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing I want to do is to separate categorical and numerical features because later we will need to make some transformations and plot some graphs.","metadata":{}},{"cell_type":"code","source":"numerical_features = ['age','trtbps','chol','thalachh','oldpeak']\ncategorical_features = ['sex','cp','fbs','restecg','exng','slp','caa','thall']","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:01.002255Z","iopub.execute_input":"2021-06-30T16:07:01.002605Z","iopub.status.idle":"2021-06-30T16:07:01.007759Z","shell.execute_reply.started":"2021-06-30T16:07:01.002574Z","shell.execute_reply":"2021-06-30T16:07:01.006612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Univariate analysis","metadata":{}},{"cell_type":"markdown","source":"Let's look on how balanced our target variable is.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(x=df['output']);","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:01.009493Z","iopub.execute_input":"2021-06-30T16:07:01.010033Z","iopub.status.idle":"2021-06-30T16:07:01.153841Z","shell.execute_reply.started":"2021-06-30T16:07:01.009971Z","shell.execute_reply":"2021-06-30T16:07:01.15306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there is no disbalance in target variable.\n\nIt is also nice practice to look on statistics and distributions of our features.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-30T16:07:01.155183Z","iopub.execute_input":"2021-06-30T16:07:01.155789Z","iopub.status.idle":"2021-06-30T16:07:01.211183Z","shell.execute_reply.started":"2021-06-30T16:07:01.15574Z","shell.execute_reply":"2021-06-30T16:07:01.210086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From here we see some useful information about our features:\n\n    1. Min age = 29, max age = 77, mean age = 54.3, from here we see that we have no information about young people.\n    2. Sex mean 0.68 means that 68% of our observations are labeled as 1 \n    3. It also looks like we have no outliers (judging by the max-min values), maybe only the observation with chol = 564 is an outlier, but I will not remove it because it is very high but still achievable value.\n    4. We can make some other conclusions about our data but they won't be obvious for me and for most people ","metadata":{}},{"cell_type":"markdown","source":"# Bivariate analysis","metadata":{}},{"cell_type":"markdown","source":"In this section we will look on dependency between target variable and features.","metadata":{}},{"cell_type":"code","source":"_,ax = plt.subplots(1,2,figsize=(14,6))\n\nsns.histplot(data=df,x='age',hue='output',kde=True,ax=ax[0])\nsns.boxplot(data=df,y='age',x='output',ax=ax[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:01.214913Z","iopub.execute_input":"2021-06-30T16:07:01.215489Z","iopub.status.idle":"2021-06-30T16:07:01.618141Z","shell.execute_reply.started":"2021-06-30T16:07:01.215441Z","shell.execute_reply":"2021-06-30T16:07:01.616949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From graph below we see strange thing - median age of people that are more likely to have heart attack is lower then median age of people that are less likely.","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df,x='sex',hue='output')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:01.620959Z","iopub.execute_input":"2021-06-30T16:07:01.621443Z","iopub.status.idle":"2021-06-30T16:07:01.768794Z","shell.execute_reply.started":"2021-06-30T16:07:01.621396Z","shell.execute_reply":"2021-06-30T16:07:01.768035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that sex labeled as 0 is more likely to have heart attack then sex 1.","metadata":{}},{"cell_type":"code","source":"_,ax = plt.subplots(2,2,figsize=(16,10))\nfor i,x in enumerate(['trtbps','chol','thalachh','oldpeak']):\n    sns.histplot(data=df,x=x,hue='output',kde=True,ax=ax[i%2][i//2])\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:01.769971Z","iopub.execute_input":"2021-06-30T16:07:01.770461Z","iopub.status.idle":"2021-06-30T16:07:02.918333Z","shell.execute_reply.started":"2021-06-30T16:07:01.770418Z","shell.execute_reply":"2021-06-30T16:07:02.917263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graphs we see that all this features might be useful.\n\nNow let's look on distribution of categorical variables. ","metadata":{}},{"cell_type":"code","source":"_,ax = plt.subplots(4,2,figsize=(16,15))\nfor i,x in enumerate(['cp','fbs','restecg','exng','slp','caa','thall']):\n    sns.histplot(data=df,x=x,hue='output',kde=True,ax=ax[i%4][i//4])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:02.919697Z","iopub.execute_input":"2021-06-30T16:07:02.92004Z","iopub.status.idle":"2021-06-30T16:07:04.945869Z","shell.execute_reply.started":"2021-06-30T16:07:02.919985Z","shell.execute_reply":"2021-06-30T16:07:04.944956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this graphs we can make conclusion that feature fbs might not be useful. To check this we can make two models first with this feature and  second without.","metadata":{}},{"cell_type":"markdown","source":"# Missing values","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:04.947443Z","iopub.execute_input":"2021-06-30T16:07:04.947795Z","iopub.status.idle":"2021-06-30T16:07:04.95952Z","shell.execute_reply.started":"2021-06-30T16:07:04.947762Z","shell.execute_reply":"2021-06-30T16:07:04.95814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this data we don't have missing values.","metadata":{}},{"cell_type":"markdown","source":"# Scaling and Encoding","metadata":{}},{"cell_type":"markdown","source":"We want to scale our data to make all variables have similar values range. To do this we can't simply scale all data because this will lead to data leakage when we will be evaluating our model, so we have to split data on train and test set.\n\nEncoding is used to transform (categorical) data so that model can understand it, I will use OneHotEncoding since this is ont of the best choices for linear models.\n\nI will make pipeline for transformations using ColumnTransformer to do this I need to specify columns dtype to choose appropriate transformation for them.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:04.961629Z","iopub.execute_input":"2021-06-30T16:07:04.962516Z","iopub.status.idle":"2021-06-30T16:07:04.974057Z","shell.execute_reply.started":"2021-06-30T16:07:04.962371Z","shell.execute_reply":"2021-06-30T16:07:04.972845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:04.97561Z","iopub.execute_input":"2021-06-30T16:07:04.976383Z","iopub.status.idle":"2021-06-30T16:07:04.992454Z","shell.execute_reply.started":"2021-06-30T16:07:04.976329Z","shell.execute_reply":"2021-06-30T16:07:04.991427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[numerical_features]=df[numerical_features].astype('float64')\ndf[categorical_features] = df[categorical_features].astype('category')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:04.993782Z","iopub.execute_input":"2021-06-30T16:07:04.994493Z","iopub.status.idle":"2021-06-30T16:07:05.016085Z","shell.execute_reply.started":"2021-06-30T16:07:04.994453Z","shell.execute_reply":"2021-06-30T16:07:05.015271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:05.017145Z","iopub.execute_input":"2021-06-30T16:07:05.017588Z","iopub.status.idle":"2021-06-30T16:07:05.02509Z","shell.execute_reply.started":"2021-06-30T16:07:05.017558Z","shell.execute_reply":"2021-06-30T16:07:05.024032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_transformer = Pipeline(steps=[('scaler',StandardScaler())])\n\ncat_transformer = Pipeline(steps=[('onehot',OneHotEncoder(handle_unknown='ignore'))])\n\ntransformer = ColumnTransformer(transformers=[\n        ('num', num_transformer, numerical_features),\n        ('cat', cat_transformer,categorical_features)])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:05.026707Z","iopub.execute_input":"2021-06-30T16:07:05.027121Z","iopub.status.idle":"2021-06-30T16:07:05.037612Z","shell.execute_reply.started":"2021-06-30T16:07:05.026985Z","shell.execute_reply":"2021-06-30T16:07:05.035847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"Looks like our transformer is ready now we can make some models.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold,train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:05.039206Z","iopub.execute_input":"2021-06-30T16:07:05.039636Z","iopub.status.idle":"2021-06-30T16:07:05.053282Z","shell.execute_reply.started":"2021-06-30T16:07:05.039602Z","shell.execute_reply":"2021-06-30T16:07:05.05207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = train_test_split(df,random_state=1)\nX_test = df_test.drop(columns=['output'])\ny_test = df_test.output","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:05.055125Z","iopub.execute_input":"2021-06-30T16:07:05.055599Z","iopub.status.idle":"2021-06-30T16:07:05.072649Z","shell.execute_reply.started":"2021-06-30T16:07:05.055551Z","shell.execute_reply":"2021-06-30T16:07:05.07168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.drop(columns=['output'])\ny = df_train.output","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:05.074095Z","iopub.execute_input":"2021-06-30T16:07:05.074399Z","iopub.status.idle":"2021-06-30T16:07:05.090938Z","shell.execute_reply.started":"2021-06-30T16:07:05.074371Z","shell.execute_reply":"2021-06-30T16:07:05.089608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_recall(model,message=''):\n    print('-'*9)\n    print(message)\n    pipeline = Pipeline(steps=[('transformer',transformer),('model',model)])\n    print('Recall = ',round(np.mean(cross_val_score(pipeline,X,y,scoring='recall')),5))\n    print('Precision = ',round(np.mean(cross_val_score(pipeline,X,y,scoring='precision')),5))    ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:05.092674Z","iopub.execute_input":"2021-06-30T16:07:05.093027Z","iopub.status.idle":"2021-06-30T16:07:05.106173Z","shell.execute_reply.started":"2021-06-30T16:07:05.092975Z","shell.execute_reply":"2021-06-30T16:07:05.105125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_recall(LogisticRegression(C=0.1),'LogRegres, c = 0.1')\nprint_recall(LogisticRegression(C=1),'LogRegres, c = 1')\nprint_recall(LogisticRegression(C=10),'LogRegres, c = 10')\n\nprint_recall(SVC(C=0.1),'SVC, c = 0.1')\nprint_recall(SVC(C=1),'SVC, c = 1')\nprint_recall(SVC(C=10),'SVC, c = 10')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:05.107961Z","iopub.execute_input":"2021-06-30T16:07:05.108326Z","iopub.status.idle":"2021-06-30T16:07:07.125494Z","shell.execute_reply.started":"2021-06-30T16:07:05.10829Z","shell.execute_reply":"2021-06-30T16:07:07.12388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_recall(KNeighborsClassifier(n_neighbors=3),'KNN, k = 3')\nprint_recall(KNeighborsClassifier(n_neighbors=5),'KNN, k = 5')\nprint_recall(KNeighborsClassifier(n_neighbors=8),'KNN, k = 8')\n\nprint_recall(RandomForestClassifier(min_samples_leaf=1),'Forest, 1 sample per leaf')\nprint_recall(RandomForestClassifier(min_samples_leaf=3),'Forest, 3 samples per leaf')\nprint_recall(RandomForestClassifier(min_samples_leaf=5),'Forest, 5 samples per leaf')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:07:07.130131Z","iopub.execute_input":"2021-06-30T16:07:07.130504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now focus on logistic regression and try to get recall equal to 0.95","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline(steps=[('transformer',transformer),('model',LogisticRegression(C=0.1))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prob = 0.0\nfor p in np.linspace(0.5,0,100):\n    kfold = KFold()\n    recall = list()\n    precision = list()\n    for train_idx,test_idx in kfold.split(X):\n        pipeline.fit(X.iloc[train_idx],y.iloc[train_idx])\n        proba = pipeline.predict_proba(X.iloc[test_idx])\n        predictions = proba[:,1] >= p\n        recall.append(recall_score(y.iloc[test_idx],predictions))\n        precision.append(precision_score(y.iloc[test_idx],predictions))\n\n    if np.mean(recall) > 0.95:\n        prob = p\n        print('p = ', p)\n        print('Recall = ', np.mean(recall))\n        print('Precision = ', np.mean(precision))\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see we need threshold ~ 0.39 to get recall = 0.95 let's check results on test set.","metadata":{}},{"cell_type":"code","source":"pipeline.fit(X,y);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba = pipeline.predict_proba(X_test)\npredictions = proba[:,1] >= prob\nprint('Recall = ',recall_score(y_test,predictions))\nprint('Precision = ',precision_score(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, result differs, but this is because we have small dataset so the split has huge influence on the metrcis.","metadata":{}}]}