{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Modules\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nimport warnings\nwarnings.simplefilter(\"ignore\")\n# Load Data\ndata = pd.read_csv(\"../input/car-price-prediction/CarPrice_Assignment.csv\")\nprint(\"Shape of Data  :  {}\".format(data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First look data\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(\"car_ID\",axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical feat.\ncategorical = data.select_dtypes(\"object\").columns\n#numeric\nnumeric = data.select_dtypes([\"int64\",\"float\"]).columns\n###\nprint(\"Number of Categorical Data :{}\".format(len(categorical)))\nprint(\"Number of Numeric Data :{}\".format(len(numeric)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NA values \nprint(\"Number of NA Values for all of data : {}\".format(data.isna().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Target \nsns.displot(data.price,color=\"red\")\n#\nst.skew(data.price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The price variable has a skewed distribution, I will try transformation techniques to simulate a normal distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log transform\nlog_price = np.log1p(data.price)\nsns.distplot(log_price,color = \"red\")\nprint(\"P-Value : {}\".format(st.kstest(log_price,\"norm\")[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Root Square Transform\nsqrtprice = np.sqrt(data.price)\nsns.distplot(sqrtprice)\nprint(\"P-Value : {}\".format(st.kstest(sqrtprice,\"norm\")[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxcox transform\nboxcox_trans = st.boxcox(data.price)\nresults = boxcox_trans[0] #values\nlam = boxcox_trans[1] #lambda \nsns.distplot(results)\nprint(\"P-Value : {}\".format(st.kstest(results,\"norm\")[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The technique best suited to the p-value and chart is the boxcox technique. I will convert the price variable with boxcox."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.price = results\n#lambda_price = lam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distributions of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical:\n    print(\"####\",col,\"####\")\n    print(data[col].value_counts(),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical:\n    print(col,\":::\",data[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I'll take the car brands from the CarName variable, add them to the dataset and drop the CarName variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_carname = list(data.CarName.astype(\"str\"))\nsplit_car = []\nfor i in range(0,len(data_carname)):\n    split_car.append(data_carname[i].split()[0])\n    \ndata[\"brand\"] = split_car\ndata.drop(\"CarName\",axis = 1,inplace = True)\ndata.brand.replace(\"maxda\",\"mazda\",inplace=True)\ndata.brand.replace(\"maxda\",\"mazda\",inplace=True)\ndata.brand.replace(\"Nissan\",\"nissan\",inplace=True)\ndata.brand.replace(\"porcshce\",\"porsche\",inplace=True)\ndata.brand.replace(\"vokswagen\",\"volkswagen\",inplace=True)\ndata.brand.replace(\"vw\",\"volkswagen\",inplace=True)\ndata.brand.replace(\"toyouta\",\"toyota\",inplace=True)\ndata.brand.replace(\"alfa-romero\",\"alfa-romeo\",inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.brand.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = data.select_dtypes(\"object\").columns\ndummy = pd.get_dummies(data[categorical],drop_first=True)\ndata.drop(categorical,axis = 1,inplace=True)\ndata = pd.concat([dummy,data],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(\"price\",axis = 1)\ny = data.price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,PolynomialFeatures\nfrom sklearn.model_selection import KFold,cross_val_predict\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom scipy.special import inv_boxcox\nkf = KFold(shuffle=True,random_state=42,n_splits=5)\nscale = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Linear Regression\nscores = []\nlr = LinearRegression()\nfor train_index,test_index in kf.split(X):\n    X_train,X_test,y_train,y_test = (X.iloc[train_index,:],X.iloc[test_index,:],\n                                     y.iloc[train_index],y.iloc[test_index])\n    model = lr.fit(X_train,y_train)\n    pred = model.predict(X_test)\n    #pred = inv_boxcox(pred,lam)\n    scores.append(r2_score(pred,y_test))\n    plt.scatter(y_test.values,pred)\n    plt.show()\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression\nalphas = np.geomspace(0.01,20,num = 15) \nscores = []\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha,max_iter = 10000)\n    estimator = Pipeline([(\"scaler\",scale),(\"lasso_regression\",lasso)])\n    predictions = cross_val_predict(estimator,X,y,cv=kf)\n    scores.append(r2_score(y,predictions))\n\n\nplt.semilogx(alphas,scores,\"-*\")  # Alphalara karşılık R2 sonucu için grafik\nr2_lasso = pd.DataFrame(zip(alphas,scores),columns=[\"Alpha\",\"R2_Score\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso with add Polynomial Features\npf = PolynomialFeatures(degree = 2)\nscores = []\nalphas = np.geomspace(1,20,num = 5) \n\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha,max_iter = 100000)\n    estimator = Pipeline([(\"polynomial_feature\",pf),(\"scaler\",scale),(\"lasso_regression\",lasso)])\n    predictions = cross_val_predict(estimator,X,y,cv=kf)\n    scores.append(r2_score(y,predictions))\n\n\nplt.semilogx(alphas,scores,\"-*\")\npf_lasso_r2 = pd.DataFrame(list(zip(alphas,scores)),columns=[\"Alpha\",\"R2_Score\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pf_lasso_r2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression\npf = PolynomialFeatures(degree = 3)\nscores = []\nalphas = np.geomspace(0.1,20,num = 20) \n\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha,max_iter = 100000)\n    estimator = Pipeline([(\"polynomial_feature\",pf),(\"scaler\",scale),(\"ridge_regression\",ridge)])\n    predictions = cross_val_predict(estimator,X,y,cv=kf)\n    print(\"For Alpha :: {}\".format(alpha),\"----> Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y,predictions))))\n    scores.append(r2_score(y,predictions))\nplt.semilogx(alphas,scores,\"-o\")\nridge_r2 = pd.DataFrame(list(zip(alphas,scores)),columns=[\"Alpha\",\"R2_Score\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_r2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nestimator = Pipeline([(\"polynomial\",PolynomialFeatures(include_bias=False)),\n                      (\"scale\",scale),\n                      (\"ridge_regression\",Ridge())])\nparams = {\"polynomial__degree\":[1,2,3],\n          \"ridge_regression__alpha\":np.geomspace(4,20,30)}\ngrid = GridSearchCV(estimator,params,cv = kf)\ngrid.fit(X,y)\ngrid.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion \n* The R2 scores of the model established by linear regression, Lasso and Ridge regression are close to each other. However, the model with the highest score was Ridge regression and Lasso can be preferred because it works faster than regression. Optimal parameters degree = 3, alpha = 20.00000004"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nestimator = Pipeline([(\"polynomial\",PolynomialFeatures(degree = 3,include_bias=False)),\n                      (\"scale\",scale),\n                      (\"ridge_regression\",Ridge(alpha=20.000000000000004))])\nestimator.fit(X_train,y_train)\npredict = estimator.predict(X_test)\n\nprint(\"R2 Score for Ridge Regression : {}\".format(r2_score(predict,y_test)))\nprint(\"Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y_test,predict))))\nprint(\"Mean Absolute Error : {}\".format(mean_absolute_error(predict,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}