{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nThis is the code portion of my Politecnico di Milano M.Sc. thesis on electric load forecasting. It's a work in progress but will be complete around Dec 2020. "},{"metadata":{},"cell_type":"markdown","source":"# To Do\n\nLit \n- ~Review~\n\nCourses\n- ~Udacity - Intro to TensorFlow for Deep Learning~\n\nBooks\n- Géron 2019 - Hands on Machine Learning\n\nViz\n- ~Daily and weekly plots~\n- ~Hist~\n\nData Prep\n- ~Generalized ESD for outliers~ Doesn't seem to work (why?)\n- ~Basic stats analysis: autocorrelation, Augmented Dicky-Fuller~\n- ~Z-score for outlier detection~\n- ~Outlier replacement~ Same day different year\n- ~Normalize data~\n- ~Diff data for ANN~\n\nModels\n- ~ANN, CNN, and LSTM~\n- ~Predict [t+1 .. t+24h] data points (4x24=96)~ Too complex\n- ~Only predict one horizon data pt~\n- Ensemble: lower error, compare stdev of predictions\n- Consider nowcast\n- Grid/random search on hyperparameters\n- Bi-directional LSTM?\n\nFeature Engineering\n- ~Empirical mode decomposition~\n- ~Feature-timeseries cross correlation~\n- ~Triangle time-of-day index~\n- Day of wk, holidays, etc\n- ~LSTM past 2-3 days~\n- Exogenous: temp, irradiance, windspeed (check, don't *need* to include)\n\nMetrics\n- ~RMSE to penalize larger errors~\n- Benchmark: naïve persistence\n- Accuracy?\n\nTraining\n- ~Set-seed~\n- Try training on less data to start (3 months?) \n- Cross-validation"},{"metadata":{},"cell_type":"markdown","source":"# Dependencies"},{"metadata":{},"cell_type":"markdown","source":"Note that package 'emd' is not installed by default, and disappears every session refresh. It should install using `pip install emd` below."},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install emd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys \nimport warnings\nimport numpy as np\nfrom numpy import log\nimport pandas as pd\nimport itertools\nimport datetime as dt\nimport calendar\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport pylab\nimport seaborn as sns # used for plot interactive graph.\n\n#from numpy.random import seed\n\nfrom scipy import signal\nfrom scipy import stats\nfrom scipy.stats import randint\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler # for standardization of da\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.cluster import KMeans\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport statsmodels.api as sm\n\nimport keras\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\nimport tensorflow as tf\n#from tensorflow.random import set_seed #as tf_set_random_seed\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n\n#############################################\n#\n# In kaggle kernal requires 'pip install emd' \n#\n#############################################\nimport emd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\nplt.rcParams['axes.prop_cycle']\npd.set_option('precision', 2)\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read csv into dataframe\ndf = pd.read_csv('../input/hotel-load-and-solar/hotel_load_and_solar_2016-05-19_2020-09-21.csv', parse_dates=['Datetime'], index_col=['Datetime'])\nprint(df.describe())\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First Data Clean"},{"metadata":{},"cell_type":"markdown","source":"Check for null data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The solar PV connection is located behind the utility consumption meter, so we will assume all solar PV production is self consumed (the array is small compared to the load) and therefore the estimated native load (without solar) is `Meter (kW) + Solar (kW)`."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['meter','solar'] # convenient renaming (all units in kW)\ndf[df<.001] = 0\ndf['load'] = df['meter'].values + df['solar'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Viz"},{"metadata":{},"cell_type":"markdown","source":"## One Week"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2016-11-14':'2016-11-20'])\nplt.legend(['Meter (kW)','Solar (kW)','Load (kW)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All Data, Resampled Daily"},{"metadata":{},"cell_type":"markdown","source":"Hotel shuts down operations (due to covid-19) in mid-March 2020. Also notice the bad data in 2017."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfds = df.resample('D').mean()\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(dfds['load'],label='Load (kW)')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Every Day, Overlaid"},{"metadata":{},"cell_type":"markdown","source":"Note three areas of outlier-seeming data:\n1. A very small amount of data, tightly grouped, between 0 kW and ~400 kW\n2. A medium amount of less-tightly grouped data around ~500 kW\n3. A barely visible set of daily peaks in the afternoon and evening > 1750 kW "},{"metadata":{"trusted":true},"cell_type":"code","source":"delta = 4*24\nt_begin = 0\nt_end = 4*24\n\nL = df.shape[0]\nn = int(L/delta) - 1 # keeps from getting too close to the end\n\nd = df['load'][t_begin:t_end].values.reshape(delta,1)\n\nfor i in range(n):\n    t_begin += delta\n    t_end += delta\n    d_new = df['load'][t_begin:t_end].values.reshape(delta,1)\n    d = np.concatenate([d,d_new],axis=1)\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(d,alpha=0.04)\nplt.xlabel('Timesteps [15 min]')\nplt.ylabel('Load (kW)')\nplt.title('Daily Data (Beginning at 0:00)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Every Week, Overlaid"},{"metadata":{"trusted":true},"cell_type":"code","source":"delta = 4*24*7\nt_begin = 4*24*6 # first day of data is monday, so begin graphing on a sunday\nt_end = t_begin + delta\n\nL = df.shape[0]\nn = int(L/delta) - 2 # keeps from getting too close to the end\n\nd = df['load'][t_begin:t_end].values.reshape(delta,1)\n\nfor i in range(n):\n    t_begin += delta\n    t_end += delta\n    d_new = df['load'][t_begin:t_end].values.reshape(delta,1)\n    d = np.concatenate([d,d_new],axis=1)\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(d,alpha=0.08)\nplt.xlabel('Timesteps [15 min]')\nplt.ylabel('Load (kW)')\nplt.title('Weekly Data (Beginning on Sunday)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean and Standardize Data"},{"metadata":{},"cell_type":"markdown","source":"First, we know we don't want the covid-affected data (hotel closed down in March 2020)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[:'2020-01-31']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second, an error was identified in the solar data. Notes how on 2016-11-12 the solar PV power starts increasing after midday and continues well into the night, which is impossible. This is true for this day, and the previous few weeks also. So we will just throw out all the data before 2016-11-14."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.plot(df['2016-11-12':'2016-11-13']['solar'])\nplt.ylabel('Power (kW)')\nplt.title('Solar Bad Data Example')\ndf = df['2016-11-14':]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we don't need the meter and solar data anymore"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('meter',axis=1, inplace=True)\ndf.drop('solar',axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlier Detection"},{"metadata":{},"cell_type":"markdown","source":"Now use the z-score method to find the remaining outlier data"},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 3\n\nz = np.abs(stats.zscore(df)) # vector of z-scores (absolute values of..)\nzi = np.where(z>threshold)[0] # array of outlier indices\nprint ('Number of z-score outliers: ',len(zi))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visually locate any cluster of outliers (if at all). Indeed we see 7 independent clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.hist(zi, bins=range(int(min(zi)),int(max(zi)),10),color = \"skyblue\", ec=\"skyblue\")\nplt.xlabel('Index of outlier data')\nplt.ylabel('Occurrences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or to automate this we can use a simple clustering algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=7,random_state=0).fit(zi.reshape(-1,1))\ncc = km.cluster_centers_\nprint('Mean index of the outlier clusters: \\n\\n', cc,'\\n')\n\nfor i in range(cc.shape[0]):\n    print('Cluster %d mean datetime: %s' % (i,df.index[int(cc[i])]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlier Replacement"},{"metadata":{},"cell_type":"markdown","source":"Locate the outlier data and choose data from the same day-of-year of a different year or the same day-of-week from a nearby week."},{"metadata":{},"cell_type":"markdown","source":"Cluster 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2017-8-6':'2017-8-15']['load'].values,label='2017')\nplt.plot(df['2018-8-6':'2018-8-15']['load'].values,label='2018')\ndf['2017-8-7 7:00':'2017-8-14 11:00']['load'] = df['2018-8-7 7:00':'2018-8-14 11:00']['load'].values\nplt.plot(df['2017-8-6':'2017-8-15']['load'].values,label='2017 fixed')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2018-08-23':'2018-08-25'].values,label='2018')\nplt.plot(df['2019-08-23':'2019-8-25'].values,label='2019')\ndf['2018-08-24 7:00':'2018-8-24 21:00'] = df['2019-08-24 7:00':'2019-8-24 21:00'].values\nplt.plot(df['2018-08-23':'2018-08-25'].values,label='2018 fixed')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2019-12-12':'2019-12-14'].values,label='2019')\nplt.plot(df['2018-12-12':'2018-12-14'].values,label='2018') \ndf['2019-12-13 10:00':'2019-12-13 20:00'] = df['2018-12-13 10:00':'2018-12-13 20:00'].values\nplt.plot(df['2019-12-12':'2019-12-14'].values,label='2019 fixed')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2017-10-22':'2017-10-24'].values,label='2017')\nplt.plot(df['2018-10-22':'2018-10-24'].values,label='2018')\ndf['2017-10-22 15:00':'2017-10-24 17:00']['load'] = df['2018-10-22 15:00':'2018-10-24 17:00']['load'].values\nplt.plot(df['2017-10-22':'2017-10-24'].values,label='2017 fixed')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2016-12-15':'2016-12-16'].load.values,label='2016')\nplt.plot(df['2017-12-15':'2017-12-16'].load.values,label='2017')\ndf['2016-12-15 12:00':'2016-12-16 12:00'] = df['2017-12-15 12:00':'2017-12-16 12:00'].values\nplt.plot(df['2016-12-15':'2016-12-16'].load.values,label='2016 fixed')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2019-7-8'].load.values,label='2019')\nplt.plot(df['2018-7-8'].load.values,label='2018')\ndf['2019-7-8 8:00':'2019-7-8 13:00'] = df['2018-7-8 8:00':'2018-7-8 13:00'].values\nplt.plot(df['2019-7-8'].load.values,label='2019 fixed')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(df['2018-7-29':'2018-7-31'].values,label='2018')\nplt.plot(df['2019-7-29':'2019-7-31'].values,label='2019')\ndf['2018-7-30 1:00':'2018-7-30 11:00']['load'] = df['2019-7-30 1:00':'2019-7-30 11:00']['load'].values\nplt.plot(df['2018-7-29':'2018-7-31'].values,label='2018 fixed')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having removed the outliers we can visually inspect the daily data again"},{"metadata":{"trusted":true},"cell_type":"code","source":"delta = 4*24\nt_begin = 0\nt_end = 4*24\n\nL = df.shape[0]\nn = int(L/delta) - 1 # keeps from getting too close to the end\n\nd = df['load'][t_begin:t_end].values.reshape(delta,1)\n\nfor i in range(n):\n    t_begin += delta\n    t_end += delta\n    d_new = df['load'][t_begin:t_end].values.reshape(delta,1)\n    d = np.concatenate([d,d_new],axis=1)\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(d,alpha=0.04)\nplt.xlabel('Timesteps [15 min]')\nplt.ylabel('Load (kW)')\nplt.title('Daily Data (Beginning at 0:00)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize and Difference"},{"metadata":{},"cell_type":"markdown","source":"Normalized (min-max scaled) data should enable better learning neural networks, for instance if different feature sets have very different magnitudes (e.g. load power up to 1800 kW, day of week up to 7). The cleaned data does not have enormous outliers, so most of the data will fit nicely in a 0-to-1 scale. "},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\n\ndfn = df.copy(deep=True)\ndfn['load'] = scaler.fit_transform(df['load'].values.reshape(-1,1))\ndfn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Difference the normalized data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ddfn = dfn.diff().fillna(method='bfill') # witout .fillna() the first row would be NaN\nddfn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistical Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Normality"},{"metadata":{},"cell_type":"markdown","source":"See the histogram for basic visual analysis of the distribution. The data is not very Gaussian. This may be an indicator of non-stationarity."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.hist(df.values,bins=50)\nplt.xlabel('Load (kW)')\nplt.ylabel('Occurences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The normalized and time differenced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.hist(ddfn.values,bins=50)\nplt.xlabel('Load (kW)')\nplt.ylabel('Occurences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quantile-Quantile (QQ) Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 5), dpi=80)\n\nax1 = plt.subplot(121)\nres = stats.probplot(df.values.flatten(), dist=\"norm\", plot=plt)\nax1.set_title('Load QQ')\n\nax2 = plt.subplot(122)\nres = stats.probplot(ddfn.values.flatten(), dist=\"norm\", plot=plt)\nax2.set_title('Load QQ normalized and time-differenced')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stationarity"},{"metadata":{},"cell_type":"markdown","source":"Stationarity is achieved when the statistical properties of a timeseries do not change with time. A very simple test of this could be computing the rolling mean and variance for a large window size. Note that if the data is not a typical gaussian distribution then mean and variance are less meaningful summary statistics."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rm = df.rolling(35041,center=True).mean() # window weights are equal\ndf_rv = df.rolling(35041,center=True).var() # window weights are equal\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\n\nplt.subplot(121)\nplt.plot(df_rm['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Mean')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(df_rv['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Variance')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then if we do the same on the normalized and differenced timeseries. Note that differencing has the larger impact on statistical properties of the timeseries."},{"metadata":{"trusted":true},"cell_type":"code","source":"ddfn_rm = ddfn.rolling(35041,center=True).mean() # window weights are equal\nddfn_rv = ddfn.rolling(35041,center=True).var() # window weights are equal\n\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\n\nplt.subplot(121)\nplt.plot(ddfn_rm['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Mean on Normalized and Differenced Timeseries')\nplt.legend()\n\nplt.subplot(122)\nplt.plot(ddfn_rv['load'],label='Load (kW)')\nplt.title('Rolling 1 Year Variance on Normalzied and Differenced Timeseries')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Augmented Dicky-Fuller Test\n\nBased on the very low ADF p-value we can likely say the dataset is stationary (the null hypothesis is rejected). Also the ADF statistic is much lower than the critical values at 10, 5, and 1% significance.\n\nReminder: null hypothesis == non-stationarity\n- p-value <= 0.05: hypothesis rejected, \"suggests\" stationarity\n- p-value > 0.05: hypothesis cannot be rejected, \"suggests\" non-stationariy"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = adfuller(df.values)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because the cleaned data is not very normally distributed, we can also check the ADF test of the time-differenced data, which is closer to Gaussian."},{"metadata":{"trusted":true},"cell_type":"code","source":"result = adfuller(ddfn.values)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autocorrelation"},{"metadata":{},"cell_type":"markdown","source":"One week"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(df.values,lags=4*24*7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(ddfn.values,lags=4*24*7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One year"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(df.values,lags=4*24*365)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(ddfn.values,lags=4*24*365)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Data Split"},{"metadata":{},"cell_type":"markdown","source":"Save the test data to calculate the final generalizaton error "},{"metadata":{"trusted":true},"cell_type":"code","source":"split = 0.9 # 10% of data for test\ni_split = int(len(dfn)*split)\n\ndfn_test = dfn[i_split:].copy(deep=True)\ndfn = dfn[:i_split]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = 0.9 # 10% of data for test\ni_split = int(len(ddfn)*split)\n\nddfn_test = ddfn[i_split:].copy(deep=True)\nddfn = ddfn[:i_split]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Looking for useful data transformations to reveal the patterns in the load"},{"metadata":{},"cell_type":"markdown","source":"## Timekeeping Triangle Indices "},{"metadata":{},"cell_type":"markdown","source":"Time of Day"},{"metadata":{},"cell_type":"markdown","source":"Each day the triangle index starts at value 0 at time 0:00, increases at every timestep to its maximum value of 1 at time 12:00, then decreases back to value 0 at time 0:00 the next day."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn['time of day'] = (48 - np.abs(dfn.index.hour.values*4 + dfn.index.minute.values/15 - 48))/48","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddfn['time of day'] = (48 - np.abs(ddfn.index.hour.values*4 + ddfn.index.minute.values/15 - 48))/48","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Day of Week"},{"metadata":{},"cell_type":"markdown","source":"Pandas computes day of week such that monday=0 and sunday=6. The problems are that\n1. This does not seem to correlate with load (which is maximum on Friday, roughly speaking).\n2. The ramp function suggests that 0 is very far away from 6, but actually its just the day after. For this reason we prefer a triangle function, where subsequent days have similar values.\n\nTherefore we will shift the day of week index such that Friday is the maximum, and then convert it from a ramp to a triangle. Lastly we will normalize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# make tuesday=0\ndfn['day of week'] = dfn.index.dayofweek - 1\ndfn['day of week'][dfn['day of week'] == -1] += 7 \n\n# convert ramp to triangle and normalize\ndfn['day of week'] = (3 - np.abs(dfn['day of week'].values - 3))/3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make tuesday=0\nddfn['day of week'] = ddfn.index.dayofweek - 1\nddfn['day of week'][ddfn['day of week'] == -1] += 7 \n\n# convert ramp to triangle and normalize\nddfn['day of week'] = (3 - np.abs(ddfn['day of week'].values - 3))/3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the two triangle indices"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(dfn['2019-1-1':'2019-1-7']['day of week'],label='day of week')\nplt.plot(dfn['2019-1-1':'2019-1-7']['time of day'],label='time of day')\nplt.title('Timekeeping Triangle Indices')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Holidays"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n#dr = pd.date_range(start='2016-01-01', end='2020-12-31')\n#dff = pd.DataFrame()\n#dff['Date'] = dr\n\n#cal = calendar()\n#holidays = cal.holidays(start=dr.min(), end=dr.max())\n\n#dff['Holiday'] = dff['Date'].isin(holidays)*1\n#dff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ddfn['Date'] = ddfn.index.to_frame()\n#ddfn['Date'] = ddfn['Date'].apply(lambda x:x.date().strftime('%Y-%m-%d'))\n\n#cal2 = calendar()\n#holidays2 = cal2.holidays(start=ddfn.index.min(), end=ddfn.index.max())\n#ddfn['Date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ddfn['Holidays'] = ddfn['Date'].isin(holidays2)*1\n#ddfn['2018-12-25']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dayname={0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n\ndaily_max = df.resample('D').max()\ndaily_avg = df.resample('D').mean()\ndaily_min = df.resample('D').min()\ndaily_std = df.resample('D').std()\n\ndf_daily_max = pd.DataFrame()\ndf_daily_avg = pd.DataFrame()\ndf_daily_min = pd.DataFrame()\ndf_daily_std = pd.DataFrame()\n\nfor dow in range(7): # \"day of week\", 0 = monday  \n    df_daily_max[dayname[dow]] = daily_max[daily_max.index.dayofweek==dow].values[:167].flatten()\n    df_daily_avg[dayname[dow]] = daily_avg[daily_avg.index.dayofweek==dow].values[:167].flatten()\n    df_daily_min[dayname[dow]] = daily_min[daily_min.index.dayofweek==dow].values[:167].flatten()\n    df_daily_std[dayname[dow]] = daily_std[daily_std.index.dayofweek==dow].values[:167].flatten()    \n\n    \nplt.figure(num=None, figsize=(20, 10), dpi=80)\n\nplt.subplot('221')\nplt.title('daily maximums')\nplt.ylabel('Load (kW)')\ndf_daily_max.boxplot()\n\nplt.subplot('222')\nplt.title('daily averages')\nplt.ylabel('Load (kW)')\ndf_daily_avg.boxplot()\n\nplt.subplot('223')\nplt.title('daily minimums')\nplt.ylabel('Load (kW)')\ndf_daily_min.boxplot()\n\nplt.subplot('224')\nplt.title('daily std devs')\nplt.ylabel('Load (kW)')\ndf_daily_std.boxplot()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Empirical Mode Decomposition (EMD)"},{"metadata":{},"cell_type":"markdown","source":"Decompose (\"sift\") into Intrinsic Mode Functions (IMFs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"imf = emd.sift.sift(dfn['load'].values)\n\nprint('Number of different IMFs: ',imf.shape[1])\n\nfig = emd.plotting.plot_imfs(imf, scale_y=True, cmap=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imf_dif = emd.sift.sift(ddfn['load'].values)\n\nprint('Number of different IMFs: ',imf_dif.shape[1])\n\nfig = emd.plotting.plot_imfs(imf, scale_y=True, cmap=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select features based on correlation with the original timeseries: IMFs 3, 4, 5, and 10."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn_emd = dfn.copy(deep=True)\n\nfor i in range(imf.shape[1]):\n    dfn_emd['IMF%s'%(i+1)] = imf[:,i]\n    \n\nc = dfn_emd.corr()\nc.style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddfn_emd = ddfn.copy(deep=True)\n\nfor i in range(imf_dif.shape[1]):\n    ddfn_emd['IMF%s'%(i+1)] = imf_dif[:,i]\n    \n\nc = ddfn_emd.corr()\nc.style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Partially reconstruct the original timeseries using only the four IMFs, which capture most but not all the variation."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn_emd['IMFs'] = imf[:,2] + imf[:,3] + imf[:,4] + imf[:,9] + imf[:,10]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(dfn[:'2016-11-20']['load'],'b',label='Original timeseries')\nplt.plot(dfn_emd[:'2016-11-20']['IMFs'],'g--',label='IMFs 3, 4, 5, 10, 11')\nplt.ylabel('Load (kW)')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddfn_emd['IMFs'] = imf_dif[:,0] + imf_dif[:,1] + imf_dif[:,2] + imf_dif[:,3] + imf_dif[:,4]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(ddfn[:'2016-11-20']['load'],'b',label='Original timeseries')\nplt.plot(ddfn_emd[:'2016-11-20']['IMFs'],'g--',label='IMFs 1-5')\nplt.ylabel('Load (kW)')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Organize Data for Supervisory Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfn_emd.drop('IMF1',axis=1, inplace=True)\ndfn_emd.drop('IMF2',axis=1, inplace=True)\ndfn_emd.drop('IMF6',axis=1, inplace=True)\ndfn_emd.drop('IMF7',axis=1, inplace=True)\ndfn_emd.drop('IMF8',axis=1, inplace=True)\ndfn_emd.drop('IMF9',axis=1, inplace=True)\ndfn_emd.drop('IMFs',axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddfn_emd.drop('IMF6',axis=1, inplace=True)\nddfn_emd.drop('IMF7',axis=1, inplace=True)\nddfn_emd.drop('IMF8',axis=1, inplace=True)\nddfn_emd.drop('IMF9',axis=1, inplace=True)\nddfn_emd.drop('IMF10',axis=1, inplace=True)\nddfn_emd.drop('IMF11',axis=1, inplace=True)\nddfn_emd.drop('IMF12',axis=1, inplace=True)\nddfn_emd.drop('IMFs',axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Time Shifted Columns"},{"metadata":{},"cell_type":"markdown","source":"### Model Input (X)"},{"metadata":{},"cell_type":"markdown","source":"Choose which input data set (and shorten column names for compactness)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xdf = dfn.copy(deep=True)\nXdf_emd = dfn_emd\n\nXdf.rename(columns={'load': 't'},inplace=True)\nXdf.rename(columns={'time of day': 'tod'},inplace=True)\nXdf.rename(columns={'day of week': 'dow'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xddf = ddfn.copy(deep=True)\nXddf_emd = ddfn_emd\n\nXddf.rename(columns={'load': 't'},inplace=True)\nXddf.rename(columns={'time of day': 'tod'},inplace=True)\nXddf.rename(columns={'day of week': 'dow'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice the NaNs that appear. This is because we start at the beginning of the (cleaned) dataset and attempt to go back in time `n_in` data points - which isn't possible. All the NaNs will be cleared out in a final trim."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_in = 4*24*3 # number of inputs\n\nfor i in range(1,n_in):\n    Xdf.insert(0, 'IMF3 t-%s'%i,  Xdf_emd['IMF3'].shift(i), True)  \n    Xdf.insert(0, 'IMF4 t-%s'%i,  Xdf_emd['IMF4'].shift(i), True)    \n    Xdf.insert(0, 'IMF5 t-%s'%i,  Xdf_emd['IMF5'].shift(i), True)    \n    Xdf.insert(0, 'IMF10 t-%s'%i, Xdf_emd['IMF10'].shift(i), True)    \n    Xdf.insert(0, 'IMF11 t-%s'%i, Xdf_emd['IMF11'].shift(i), True)   \n    Xdf.insert(0, 'tod t-%s'%i,   Xdf_emd['time of day'].shift(i), True)   \n    Xdf.insert(0, 'dow t-%s'%i,   Xdf_emd['day of week'].shift(i), True)   \n    Xdf.insert(0, 't-%s'%i,       Xdf_emd['load'].shift(i),    True)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_in = 4*24*3 # number of inputs\n\nfor i in range(1,n_in):\n    Xddf.insert(0, 'IMF1 t-%s'%i,  Xddf_emd['IMF1'].shift(i), True)  \n    Xddf.insert(0, 'IMF2 t-%s'%i,  Xddf_emd['IMF2'].shift(i), True)    \n    Xddf.insert(0, 'IMF3 t-%s'%i,  Xddf_emd['IMF3'].shift(i), True)    \n    Xddf.insert(0, 'IMF4 t-%s'%i,  Xddf_emd['IMF4'].shift(i), True)    \n    Xddf.insert(0, 'IMF5 t-%s'%i,  Xddf_emd['IMF5'].shift(i), True)   \n    Xddf.insert(0, 'tod t-%s'%i,   Xddf_emd['time of day'].shift(i), True)   \n    Xddf.insert(0, 'dow t-%s'%i,   Xddf_emd['day of week'].shift(i), True)   \n    Xddf.insert(0, 't-%s'%i,       Xddf_emd['load'].shift(i),    True)    \n    \nXdf    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Output (Y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Ydf = pd.DataFrame(dfn['load'])\nYdf.rename(columns={'load': 't'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Yddf = pd.DataFrame(ddfn['load'])\nYddf.rename(columns={'load': 't'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = 96\nn_out = 1\nYdf['t+96'] = Ydf['t'].shift(-96)\n\n# use for predicting (say) all t+1h, t+2h.. t+24h\nif 0:\n    h = 0 # horizon, not incorporated\n    n_out = 24  # number of outputs\n\n    for i in range(1,n_out+1):\n        Ydf['t+%sh'%i]=Ydf['t'].shift(-i)\n\nYdf.drop('t',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = 96\nn_out = 1\nYddf['t+96'] = Yddf['t'].shift(-96)\n\n# use for predicting (say) all t+1h, t+2h.. t+24h\nif 0:\n    h = 0 # horizon, not incorporated\n    n_out = 24  # number of outputs\n\n    for i in range(1,n_out+1):\n        Yddf['t+%sh'%i]=Yddf['t'].shift(-i)\n\nYddf.drop('t',axis=1,inplace=True)\n\nYddf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trim the edges (NANs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xdf = Xdf[n_in-1 : -(n_out+h-1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xddf = Xddf[n_in-1 : -(n_out+h-1)]\nXddf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ydf = Ydf[n_in-1 : -(n_out+h-1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Yddf = Yddf[n_in-1 : -(n_out+h-1)]\nYddf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reduce data for hyperparamter tuning"},{"metadata":{},"cell_type":"markdown","source":"Final train with use all the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"months = 6\nk = 4*24*30*months\n\nXdf = Xdf[-k:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months = 6\nk = 4*24*30*months\n\nXddf = Xddf[-k:]\nXddf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ydf = Ydf[-k:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Yddf = Yddf[-k:]\nYddf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split train vs validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"split = 0.7\nL = Xdf.shape[0]\ni_split = int(L*split)\n\n\nXdf_train = Xdf[0:i_split]\nXdf_valid = Xdf[i_split:]\n\nYdf_train = Ydf[0:i_split]\nYdf_valid = Ydf[i_split:]\n\nprint('Xdf_train.shape: ',Xdf_train.shape)\nprint('Xdf_valid.shape: ',Xdf_valid.shape)\nprint('Ydf_train.shape: ',Ydf_train.shape)\nprint('Ydf_valid.shape: ',Ydf_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = 0.7\nL = Xddf.shape[0]\ni_split = int(L*split)\n\n\nXddf_train = Xddf[0:i_split]\nXddf_valid = Xddf[i_split:]\n\nYddf_train = Yddf[0:i_split]\nYddf_valid = Yddf[i_split:]\n\nprint('Xdf_train.shape: ',Xddf_train.shape)\nprint('Xdf_valid.shape: ',Xddf_valid.shape)\nprint('Ydf_train.shape: ',Yddf_train.shape)\nprint('Ydf_valid.shape: ',Yddf_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xdf_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ydf_valid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nbatch = 1024\nlr = 0.0001\npatience = 20\nneurons = 200\nadam = optimizers.Adam(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EMD + ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ann = Sequential()\n\nmodel_ann.add(Dense(neurons, activation='relu', input_dim=Xdf_train.shape[1]))\nmodel_ann.add(Dense(Ydf_train.shape[1]))\n\nmodel_ann.compile(loss='mse', optimizer=adam)\nmodel_ann.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dann = Sequential()\n\nmodel_dann.add(Dense(neurons, activation='relu', input_dim=Xddf_train.shape[1]))\nmodel_dann.add(Dense(Yddf_train.shape[1]))\n\nmodel_dann.compile(loss='mse', optimizer=adam)\nmodel_dann.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n\nann_history =  model_ann.fit(Xdf_train.values, Ydf_train.values, validation_data=(Xdf_valid, Ydf_valid), epochs=epochs, verbose=2, callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n\ndann_history =  model_dann.fit(Xddf_train.values, Yddf_train.values, validation_data=(Xddf_valid, Yddf_valid), epochs=epochs, verbose=2, callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training and validations losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.plot(ann_history.history['loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.plot(ann_history.history['val_loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch - 2')\nplt.legend(['training loss', 'validation loss'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 5), dpi=80)\nplt.plot(dann_history.history['loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.plot(dann_history.history['val_loss'][2:]) # first two losses can be orders of magnitudes higher\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch - 2')\nplt.legend(['training loss', 'validation loss'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{},"cell_type":"markdown","source":"Make Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_hat = model_ann.predict(Xdf_train.values)\nY_valid_hat = model_ann.predict(Xdf_valid.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Yd_train_hat = model_dann.predict(Xddf_train.values)\nYd_valid_hat = model_dann.predict(Xddf_valid.values)\nprint('Train predictions shape ',Yd_train_hat.shape)\nprint('Valid predictions shape ',Yd_valid_hat.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Root Mean Squared Error"},{"metadata":{"trusted":true},"cell_type":"code","source":"measurements_t = scaler.inverse_transform(Ydf_train.values)\nmeasurements_v = scaler.inverse_transform(Ydf_valid.values)\n\npersistence_t = scaler.inverse_transform(Xdf_train['t'].values.reshape(-1,1))\npersistence_v = scaler.inverse_transform(Xdf_valid['t'].values.reshape(-1,1))\n\npredictions_t = scaler.inverse_transform(Y_train_hat)\npredictions_v = scaler.inverse_transform(Y_valid_hat)\n\nprint('Persist train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_t, persistence_t))))\nprint('Persist valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_v, persistence_v))))\n\nprint('Train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_t, predictions_t))))\nprint('Valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(measurements_v, predictions_v))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Root Mean Squared Error (differenced)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dmeasurements_t = scaler.inverse_transform(Yddf_train.values)\ndmeasurements_v = scaler.inverse_transform(Yddf_valid.values)\n\ndpersistence_t = scaler.inverse_transform(Xddf_train['t'].values.reshape(-1,1))\ndpersistence_v = scaler.inverse_transform(Xddf_valid['t'].values.reshape(-1,1))\n\ndpredictions_t = scaler.inverse_transform(Yd_train_hat)\ndpredictions_v = scaler.inverse_transform(Yd_valid_hat)\n\nprint('Persist train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_t, dpersistence_t))))\nprint('Persist valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_v, dpersistence_v))))\n\nprint('Train rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_t, dpredictions_t))))\nprint('Valid rmse [kW]: {:.3f}'.format(np.sqrt(mean_squared_error(dmeasurements_v, dpredictions_v))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots"},{"metadata":{},"cell_type":"markdown","source":"Naïve persistence (24 h)"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nm = measurements_v[0:k]\np = persistence_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set 24 h persistence')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nm = dmeasurements_v[0:k]\np = dpersistence_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set 24 h persistence (differenced)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rolling Horizon Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nm = measurements_t[0:k]\np = predictions_t[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('training set predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nm = dmeasurements_t[0:k]\np = dpredictions_t[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('training set predictions (differenced)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nm = measurements_v[0:k]\np = predictions_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nm = dmeasurements_v[0:k]\np = dpredictions_v[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y','Predicted Y'])\nplt.title('validation set predictions (differenced)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rolling Horizon Predictions (integrated)"},{"metadata":{},"cell_type":"markdown","source":"Intregrate (we differenced the original data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nmeasurements_v_i = scaler.inverse_transform(Ydf_valid.values)\n\n#Y_valid_hat[0] = Ydf_valid.values[0]\nY_valid_hat_i = np.cumsum(Y_valid_hat).reshape(-1,1)\npredictions_v_i = scaler.inverse_transform(Y_valid_hat_i)\n\nm = measurements_v_i[0:k]\np = predictions_v_i[0:k]\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,m,t,p)\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y_valid','Predicted Y_valid'])\nplt.title('training set predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stooooppppp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EMD + CNN (not run in this version)"},{"metadata":{},"cell_type":"markdown","source":"## Reshape X and Y into 3D array for CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_3d = Xdfn_train.values.reshape(Xdfn_train.shape[0],Xdfn_train.shape[1],1)\nX_valid_3d = Xdfn_valid.values.reshape(Xdfn_valid.shape[0],Xdfn_valid.shape[1],1)\n\nY_train_3d = Ydfn_train.values.reshape(Ydfn_train.shape[0],Ydfn_train.shape[1],1)\nY_valid_3d = Ydfn_valid.values.reshape(Ydfn_valid.shape[0],Ydfn_valid.shape[1],1)\n\nprint('X_train_3d shape: ',X_train_3d.shape)\nprint('X_valid_3d shape: ',X_valid_3d.shape)\nprint('Y_train_3d shape: ',Y_train_3d.shape)\nprint('Y_valid_3d shape: ',Y_valid_3d.shape)\nprint('X_train[:3,:5,0]:\\n',X_train_3d[:3,:5,0])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cnn = Sequential()\nmodel_cnn.add(Conv1D(filters=4, kernel_size=10, activation='relu', input_shape=(X_train_3d.shape[1], 1)))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(20, activation='relu'))\nmodel_cnn.add(Dense(20, activation='relu'))\nmodel_cnn.add(Dense(Y_train_3d.shape[1]))\nmodel_cnn.compile(loss='mse', optimizer=adam)\nmodel_cnn.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_history = model_cnn.fit(X_train_3d, Y_train_3d, validation_data=(X_valid_3d, Y_valid_3d), epochs=epochs, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_train_pred = model_cnn.predict(X_train_3d)\ncnn_valid_pred = model_cnn.predict(X_valid_3d)\nprint('Ydf_train shape',Ydf_train.shape)\nprint('cnn_train_pred shape',cnn_train_pred.shape)\nprint('Train rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_train.values, cnn_train_pred))))\nprint('Validation rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_valid.values, cnn_valid_pred))))\nY_valid_hat_cnn = model_cnn.predict(X_valid_3d)\nprint('Y_valid_hat_cnn.shape:',Y_valid_hat_cnn.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots"},{"metadata":{},"cell_type":"markdown","source":"### Rolling prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4*24*7\nt=np.arange(0,k)\n\nplt.figure(num=None, figsize=(20, 10), dpi=80)\nplt.plot(t,Y_valid_3d[0:k,0],t,Y_valid_hat_cnn[0:k,0])\nplt.ylabel('kW')\nplt.xlabel('timestep')\nplt.legend(['Y_valid','Predicted Y_valid'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EMD + LSTM (not run in this version)"},{"metadata":{},"cell_type":"markdown","source":"Reshape? (no, already done above)"},{"metadata":{},"cell_type":"markdown","source":"## Build"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\nmodel_lstm.add(Dense(Y_train_3d.shape[1]))\nmodel_lstm.compile(loss='mse', optimizer=adam)\nmodel_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_history = model_lstm.fit(X_train_3d, Y_train_3d, validation_data=(X_valid_3d, Y_valid_3d), epochs=epochs, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_valid_hat_lstm = model_lstm.predict(X_valid_3d)\nY_valid_hat_lstm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_train_pred = model_lstm.predict(X_train_3d)\nlstm_valid_pred = model_lstm.predict(X_valid_3d)\nprint('Train rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_train.values, lstm_train_pred))))\nprint('Validation rmse: {:.3f}'.format(np.sqrt(mean_squared_error(Ydf_valid.values, lstm_valid_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots"},{"metadata":{},"cell_type":"markdown","source":"### Rolling 12 hr prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"k=72\nt=np.arange(0,k)\n\nplt.plot(t,Y_valid_3d[0:k,-1],t,Y_valid_hat_lstm[0:k,-1])\nplt.ylabel('kWh')\nplt.xlabel('hrs')\nplt.legend(['Y_valid','Y_valid_hat'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Single prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"t=np.arange(0,n_out)\n\nplt.plot(t,Y_valid_3d[0,:],t,Y_valid_hat_lstm[0,:])\nplt.ylabel('kWh')\nplt.xlabel('hrs')\nplt.legend(['Y_valid','Y_valid_hat'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}