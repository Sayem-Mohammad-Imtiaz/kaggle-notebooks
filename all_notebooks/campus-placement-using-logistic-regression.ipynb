{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\nfrom sklearn.compose import ColumnTransformer,make_column_transformer,make_column_selector\nfrom sklearn.model_selection import train_test_split\nimport math\n\n\n#Logistic Sigmoid function\ndef logisticSigmoid(theta,X):\n    return (1 / (1 + np.exp(-(np.dot(X,theta)))))\n\n\ndef get_acc(y_hat,y_test):\n    count = len(y_hat)\n    score = 0\n    for i,j in zip(y_hat,y_test):\n        if i == j:\n            score+=1\n    \n    return (score/count)*100\n\n#Binary Cross entropy loss for 2 distributions\n    \ndef cross_loss(p, q):\n    return -(np.mean([p[i]*math.log(q[i]) for i in range(len(p))]))\n\n#Derivative of the log likelihood\ndef loss_derivative(y_true,y_hat,X):\n    return np.dot(np.transpose(y_hat-y_true),X)\n\ndef KfoldValidation(k,X_train,y_train):\n    num_samples = len(X_train) // k\n    validation_error = []\n    training_error = []\n    for i in range(k):\n        print(\"Processing Fold #\", i)\n        val_data = X_train[i * num_samples:(i+1) * num_samples]\n        val_test = y_train[i * num_samples:(i+1) * num_samples]\n        partial_train_data = np.concatenate([X_train[:i * num_samples], X_train[(i+1) * num_samples:]],axis=0)\n        partial_train_label = np.concatenate([y_train[:i * num_samples], y_train[(i+1) * num_samples:]],axis=0)\n        theta,training_loss = gradient_descent(partial_train_data,partial_train_label,1000,0.0005,0)\n        y_hat_test = logisticSigmoid(theta,val_data)\n        y_hat_train = logisticSigmoid(theta,partial_train_data)\n        validation_loss = cross_loss(val_test,y_hat_test)\n        training_loss = cross_loss(partial_train_label,y_hat_train)\n        validation_error.append(validation_loss)\n        training_error.append(training_loss)\n    \n    return training_error,validation_error  \n\ndef gradient_descent(X,y,epoch,lr,verbose):\n    theta = np.random.randn(X.shape[1], 1) / 100\n    num_e = 1\n    loss_epoch = []\n    \n    while num_e <= epoch:\n        \n        #Get initial predictions and loss\n        y_hat = logisticSigmoid(theta,X)\n        loss = cross_loss(y,y_hat)\n        #Get derivative of loss\n        dloss = loss_derivative(y,y_hat,X)\n        #Update the parameters\n        theta = theta - lr * np.transpose(dloss)\n        \n        if verbose == 1:\n            print(\"Processing Epoch ########\" ,num_e)\n            print(\"Loss at epoch\",num_e,\" is \",loss)\n\n        loss_epoch.append(loss)\n        num_e = num_e+1\n    return theta,loss_epoch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')\n \n#since serial number and gender dont matter we remove them from the dataset\nX = dataset.iloc[:,2:-2]\ny = dataset.iloc[:,-2]\n\n#Using a column transformer method to scale the dataset and convert the certain columns to one hot encoded vectors\nct = make_column_transformer(\n       (StandardScaler(),\n        make_column_selector(dtype_include=np.number)),  # rating\n       (OneHotEncoder(),\n     make_column_selector(dtype_include=object)))\n\n\nX = ct.fit_transform(X)\n\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n#Splitting the dataset 1/3 ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\ny_train = y_train.astype('float32')\ny_test = y_test.astype('float32')\n\ny_train = y_train.reshape(y_train.shape[0],1)\ny_test = y_test.reshape(y_test.shape[0],1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#performing Kfold validation\ntraining_loss,validation_loss = KfoldValidation(10, X_train, y_train)\ntraining_loss = np.mean(training_loss)\nvalidation_loss = np.mean(validation_loss)\n\nprint(\"Training loss: \",training_loss)\nprint(\"Validation loss: \",validation_loss)\n\n#Performing gradient descent using a specified learning rate and number of epochs\ntheta,loss = gradient_descent(X_train, y_train, 1000, 0.0005,0)\n\n#Evaluating on test set\ny_pred_test = logisticSigmoid(theta, X_test)\nfor i in range(len(y_pred_test)):\n    if y_pred_test[i] > 0.5:\n        y_pred_test[i] = 1\n    else:\n        y_pred_test[i] = 0\n        \n\ntest_acc = get_acc(y_pred_test, y_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred_test)\nprint(\"Confusion Matrix:\\n\",confusion_matrix)\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}