{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forecasting\n\nAccording to wikepedia, Forecasting is the process of making predictions of the future based on past and present data and most commonly by analysis of trends. A commonplace example might be estimation of some variable of interest at some specified future date. Prediction is a similar, but more general term. Both might refer to formal statistical methods employing time series, cross-sectional or longitudinal data, or alternatively to less formal judgmental methods. Usage can differ between areas of application: for example, in hydrology the terms \"forecast\" and \"forecasting\" are sometimes reserved for estimates of values at certain specific future times, while the term \"prediction\" is used for more general estimates, such as the number of times floods will occur over a long period. \n\nInvestors utilize forecasting to determine if events affecting a company, such as sales expectations, will increase or decrease the price of shares in that company. Stock analysts use forecasting to extrapolate how trends, such as GDP or unemployment, will change in the coming quarter or year. \n\n# How to choose right method for forecasting?\n-Right choice of forecasting is very essential because it helps you to derive accurate insights. The reason why we're discussing the right method is because we have different types of forecastung methods and different types have different ways and statistics to do forecasting. So let's discuss different types of forecasting based on todayâ€™s business problem. \n\n1.\tInputs vs. Outputs\n\nInputs: Historical data provided to the model in order to make a single forecast.\nOutputs: Prediction or forecast for a future time step beyond the data provided as input.\n\n2.\tEndogenous vs. Exogenous\n\nEndogenous: Input variables that are influenced by other variables in the system and on which the output variable depends on input variable.\nExogenous: Input variables that are not influenced by other variables in the system and on which the output variable depends.\n\n3.\tUnstructured vs. Structured\n\nUnstructured: No obvious systematic time-dependent pattern in a time series variable.\nStructured: Systematic time-dependent patterns in a time series variable (e.g. trend and/or seasonality).\n\n4.\tRegression vs. Classification\n\nRegression: Forecast a numerical quantity.\n\nClassification: Classify as one of two or more labels.\n\n5.\tUnivariate vs. Multivariate\n\nUnivariate: One variable measured over time.\n\nMultivariate: Multiple variables measured over time.\n\n6.\tSingle-step vs. Multi-step\n\nOne-Step: Forecast the next time step.\n\nMulti-Step: Forecast more than one future time steps.\n\n7.\tStatic vs. Dynamic\n\nStatic. A forecast model is fit once and used to make predictions.\n\nDynamic. A forecast model is fit on newly available data prior to each prediction.\n\n8.\tContiguous vs. Discontiguous\n\nContiguous. Observations are made uniform over time.\n\nDiscontiguous. Observations are not uniform over time.\n\n**What are different types of forecasting methods?**\nThe most common forecasting methods are given below:\n1. ARIMA\n2. SARIMA\n3. Exponential Smoothning\n4. Facebook Prophet Forecasting\n5. RNN\n6. LSTM\n\nPlease comment below if you know any other type of forecasting. I'll be happy to learn ðŸ™‚.\n\nIn this notebook we're going to learn multivarient time series forecasting. Statistically, **Multivarient Analysis** is a statistical procedure for analysis of data involving more than one type of measurement or observation. It may also mean solving problems where more than one dependent variable is analyzed simultaneously with other variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing packages\nTo design any machine learning or deep learning model we will need some libraries like **Pandas**, **Numpy**, **Matplotlib** etc. \nFor this multivarient time series, I'm using LSTM layer along with **Dense** and **Dropout** layers. To use this layers we need to import them from keras. Along with these layers we'll need a modeling API so, we'll import **Sequential Model?** from keras.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout , LSTM , Bidirectional \n\nimport tensorflow.compat.v1 as tf\nprint(tf.test.gpu_device_name())\n# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data using pandas library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-stock-price/Google_Stock_Price_Train.csv\")\ntest = pd.read_csv(\"../input/google-stock-price/Google_Stock_Price_Test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first step towards every time series model is to **set date/ month/ week/ day/time as index**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.set_index(\"Date\")\ntest = test.set_index(\"Date\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this data we have two columns  \"Volume\" and \"Close\" which have numeric data, but with comma, which behaves like a string. So, we'll first replace those comma and then change the datatype as float.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Volume\"] = train[\"Volume\"].replace(\",\", \"\",regex=True)\ntrain[\"Close\"] = train[\"Close\"].replace(\",\", \"\",regex=True)\n\ntrain[\"Volume\"] = train[\"Volume\"].astype(\"float\")\ntrain[\"Close\"] = train[\"Close\"].astype(\"float\")\nprint(\"train dataset shape\", train.shape)\nprint(\"test dataset shape\", test.shape)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will follow the same step as above for testing dataset as well, so that both training and testing data will be in same page.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Volume\"] = test[\"Volume\"].replace(\",\", \"\",regex=True)\ntest[\"Close\"] = test[\"Close\"].replace(\",\", \"\",regex=True)\n\ntest[\"Volume\"] = test[\"Volume\"].astype(\"float\")\ntest[\"Close\"] = test[\"Close\"].astype(\"float\")\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = MinMaxScaler()\n\nnum_col = [\"High\", \"Low\", \"Close\", \"Volume\"]\ntrain1 = scale.fit(train[num_col].to_numpy())\n\ntrain.loc[:, num_col] = train1.transform(train[num_col].to_numpy())\ntest.loc[:,num_col] = train1.transform(test[num_col].to_numpy())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Output variable\nscale1 = MinMaxScaler()\nOpen = scale1.fit(train[[\"Open\"]])\ntrain[\"Open\"] = Open.transform(train[[\"Open\"]].to_numpy())\ntest[\"Open\"] = Open.transform(test[[\"Open\"]].to_numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation before building model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm\ntqdm().pandas()\ndef prepare_data(X,y,time_steps=1):\n    Xs = []\n    Ys = []\n    for i in tqdm(range(len(X) - time_steps)):\n        a = X.iloc[i:(i + time_steps)].to_numpy()\n        Xs.append(a)\n        Ys.append(y.iloc[i+time_steps])\n    return np.array(Xs),np.array(Ys)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In data preparation step, we are trying to assign values to x_train, y_train, x_test and y_test. In our case we're using \"Open\" coulumn as predicted variable and other as predictors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"steps = 10\nX_train , y_train = prepare_data(train,train.Open,time_steps=steps)\nX_test , y_test = prepare_data(test,test.Open,time_steps=steps)\nprint(\"X_train : {}\\nX_test : {}\\ny_train : {}\\ny_test: {}\".format(X_train.shape,X_test.shape,y_train.shape,y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.asarray(X_train).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inputs in LSTM:\n\nâ€¢\tThe input of the LSTM is always is a 3D array. (batch_size, time_steps, seq_len)\n\nâ€¢\tThe output of the LSTM could be a 2D array or 3D array depending upon the return_sequences argument.\n\nâ€¢\tIf return_sequence is False, the output is a 2D array. (batch_size, units)\n\nâ€¢\tIf return_sequence is True, the output is a 3D array. (batch_size, time_steps, units)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128,input_shape=(X_train.shape[1],X_train.shape[2])))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(optimizer=\"adam\",loss=\"mse\")\n\nwith tf.device('/GPU:0'):\n    prepared_model = model.fit(X_train,y_train,batch_size=32,epochs=1000,validation_data=(X_test,y_test))\n\nplt.plot(prepared_model.history[\"loss\"],label=\"loss\")\nplt.plot(prepared_model.history[\"val_loss\"],label=\"val_loss\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"No. Of Epochs\")\nplt.ylabel(\"mse score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(prepared_model.history[\"loss\"],label=\"loss\")\nplt.plot(prepared_model.history[\"val_loss\"],label=\"val_loss\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"No. Of Epochs\")\nplt.ylabel(\"mse score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test)\n\ny_test_inv = scale1.inverse_transform(y_test.reshape(-1,1))\npred_inv = scale1.inverse_transform(pred)\n\nplt.figure(figsize=(16,6))\nplt.plot(y_test_inv.flatten(),marker=\".\",label=\"actual\")\nplt.plot(pred_inv.flatten(),marker=\".\",label=\"prediction\",color=\"r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_actual = scale1.inverse_transform(y_test.reshape(-1,1))\ny_test_pred = scale1.inverse_transform(pred)\n\narr_1 = np.array(y_test_actual)\narr_2 = np.array(y_test_pred)\n\nactual = pd.DataFrame(data=arr_1.flatten(),columns=[\"actual\"])\npredicted = pd.DataFrame(data=arr_2.flatten(),columns = [\"predicted\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.concat([actual,predicted],axis=1)\nfinal.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By loooking at the final data we can say that our model predicted values very near to the actual values. However, if you want you then you can improve the model performance by different model methods such as parameter tunning and GridSearchCV or K fold etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\nrmse = np.sqrt(mean_squared_error(final.actual,final.predicted)) \nr2 = r2_score(final.actual,final.predicted) \nprint(\"rmse is : {}\\nr2 is : {}\".format(rmse,r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.plot(final.actual,label=\"Actual data\")\nplt.plot(final.predicted,label=\"predicted values\")\nplt.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you have a question or feedback, do not hesitate to write and if you like this kernel, please do not forget to UPVOTE ðŸ™‚","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}