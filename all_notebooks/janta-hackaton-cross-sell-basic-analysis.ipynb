{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Welcome to my first kernel!"},{"metadata":{},"cell_type":"markdown","source":"I will try to examine the dataset of JantaHackathon: Cross-sell Prediction. Please give me your feedback, i will be glad to read it! If you like the kernel, please vote up."},{"metadata":{},"cell_type":"markdown","source":" If you are interested in more informations about the dataset, check out the link: https://www.kaggle.com/kbambardekar/av-jantahackathon/tasks?taskId=2063"},{"metadata":{},"cell_type":"markdown","source":"## Content\n\n 1.  Introduction\n     * Info about the dataset  \n    \n    \n 2. Reading the data and knowing a little about it  \n \n    \n 3. Data analysis\n     * Exploratory data analysis\n     * Correlation\n \n \n 4. Models\n     * Setting X and y\n     * Naive Bayes, Decision Tree, Random Forest, Logistic Regression\n     * Stratified Nested Cross Validation\n     * ROC AUC score"},{"metadata":{},"cell_type":"markdown","source":"### 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"#### Context\n\nEach entry of the dataset represents a person who was offered a vehicle insurance.\nThe idea  is to predict whether a customer would be interested vehicle insurance or not based os some caractheristcs. These informations consists of demographics (gender, age, region code type), vehicles (vehicle age, damage), policy (premium, sourcing channel), current insurance (previously insurance, vintage).\n"},{"metadata":{},"cell_type":"markdown","source":"#### Columns definitions\n\n**Id** (numeric) - Unique ID for the customer\n\n**Gender** (text: Male, Female) - Gender of the customer\n\n**Age** (numeric) - Age of the customer\n\n**Driving license** (numeric: 0, 1) - Customer does not have DL / customer has DL\n\n**RegionCode** (numeric) - Unique code for the region of the customer\n\n**PreviouslyInsured** (numeric: 0, 1) - Customer doesn't have vehicle insurance / customer already has vehicle insurance.\n\n**VehicleAge** (numeric) - Age of the Vehicle\n\n**VehicleDamage** (numeric: 0, 1)- Customer didn't get his/her vehicle damaged in the past / customer got his/her vehicle damaged in the past \n\n**AnnualPremium** (numeric) - The amount customer needs to pay as premium in the year\n\n**PolicySalesChannel** (numeric) - Anonymised code for the channel of outreaching to the customer ie. Different agents, over mail, over phone, in person, etc.\n\n**Vintage** (numeric) - Number of days customer has been associated with the company\n\n**Response** (numeric: 0, 1) - Customer is not interested in vehicle insurance / customer is interested in vehicle insurance."},{"metadata":{},"cell_type":"markdown","source":"### 2. Reading the data and knowing a little about it"},{"metadata":{"trusted":true},"cell_type":"code","source":"#math and working with data.\nimport pandas as pd \nimport numpy as np \n\n#plots\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\ntrain = pd.read_csv(\"/kaggle/input/av-jantahackathon/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#What does the data look like?\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Id column is unnecesary.\ntrain = train.drop(columns = [\"id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If there is any duplicated row, lets remove it.\ntrain.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data description\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some statistics from each attribute\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for NA values\ntrain.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x = train[\"Response\"])\nplt.title(\"Target distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x = train[\"Response\"], hue = train[\"Gender\"])\nplt.title(\"Target distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Age "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_both = train[\"Age\"].values.tolist()\ntrain_yes = train.loc[train[\"Response\"] == 1]['Age'].values.tolist()\ntrain_no = train.loc[train[\"Response\"] == 0]['Age'].values.tolist()\n\nplt.figure()\n\nplt.subplot(2, 1, 1)\nsns.kdeplot(x = train_both, fill = True)\nplt.title(\"General age distribution\")\n    \nplt.subplot(2, 2, 3)\nsns.kdeplot(x = train_yes, fill = True, color = \"green\")\nplt.title(\"Took insurance\")\n    \nplt.subplot(2, 2, 4)\nsns.kdeplot(x = train_no, fill = True, color = \"orange\")\nplt.title(\"Did not take insurance\")\n\nplt.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I will  create a category column to work with age."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets work with the age in classes\ninterval = (20, 30, 55, 90)\ncats = ['Young', 'Adult', 'Senior']\ntrain[\"AgeCat\"] = pd.cut(train[\"Age\"], interval, labels=cats)\n\nplt.figure(figsize = (12,6))\nsns.boxplot(x = train[\"AgeCat\"], y = train[\"Age\"], hue = train[\"Response\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Driving License"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x = train[\"Response\"], hue = train[\"Driving_License\"])\nplt.title(\"Target distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like people that don't have a DL don't want to take a vehicle insurance. In fact, at least here in Brazil, people that don't have DL can buy a vehicle insurance, but the insurance company will not provide financial protection against traffic collisions or any other accident that occurs."},{"metadata":{},"cell_type":"markdown","source":"#### Region Code\n\nI will try to find the top 5 regions that people are more and less interested in buying a vehicle insurance."},{"metadata":{"trusted":true},"cell_type":"code","source":"regions = [i for i in range(1, 53)]\n\nregions_yes = []\nregions_no = []\n\nfor region in regions:\n    regions_yes.append(train.loc[(train[\"Region_Code\"] == region) & (train[\"Response\"] == 1)].shape[0])\n    regions_no.append(train.loc[(train[\"Region_Code\"] == region) & (train[\"Response\"] == 0)].shape[0])\n    \nregions_yes, regions_no = np.array(regions_yes), np.array(regions_no)\n\npercent_yes = regions_yes/(regions_yes+regions_no)\npercent_no = regions_no/(regions_yes+regions_no)\n\ntops_yes  = (-percent_yes).argsort()[:5]\ntops_no = (-percent_no).argsort()[:5]\n\ntops_yes += 1\ntops_no += 1\n\narr = train[\"Region_Code\"].values.tolist()\naux_yes = []\naux_no = []\n\nfor region in arr:\n    if region in tops_yes: \n        aux_yes.append(1)\n        aux_no.append(0)\n    elif region in tops_no:\n        aux_no.append(1)\n        aux_yes.append(0)\n    else:\n        aux_yes.append(0)\n        aux_no.append(0)\n\ntrain[\"Top_Yes_Region\"] = aux_yes\ntrain[\"Top_No_Region\"] = aux_no","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_yes_regions = train.loc[train[\"Top_Yes_Region\"] == 1]\ntop_no_regions = train.loc[train[\"Top_No_Region\"] == 1]\n\nplt.figure()\n\nplt.subplot(2, 1, 1)\nsns.countplot(x = top_yes_regions[\"Region_Code\"], hue = train[\"Response\"])\nplt.title(\"Regions most interested in insurance\")\n\nplt.subplot(2, 1, 2)\nsns.countplot(x = top_no_regions[\"Region_Code\"], hue = train[\"Response\"])\nplt.title(\"Regions less interested in insurance\")\n\n\nplt.tight_layout()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Previously Insured"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x = train[\"Previously_Insured\"], hue = train[\"Response\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People that already have an insurance don't want to buy another or change it..."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x = train[\"AgeCat\"], hue = train[\"Previously_Insured\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"Young\" category is the only one that has more people that already have an insurance. \nThat can help explain why the majority of them reject taking insurance."},{"metadata":{},"cell_type":"markdown","source":"#### Vehicle Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x = train[\"Vehicle_Age\"], hue = train[\"Response\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vehicle Damage"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(x = train[\"Vehicle_Damage\"], hue = train[\"Response\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create a new feature \"New_NoDamage\" to know when a car has < 1 Year and no damage."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['New_NoDamage'] = np.where((train[\"Vehicle_Damage\"] == \"No\") & (train[\"Vehicle_Age\"] == \"< 1 Year\"), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Annual Premium"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_both = train[\"Annual_Premium\"].values.tolist()\ntrain_yes = train.loc[train[\"Response\"] == 1]['Annual_Premium'].values.tolist()\ntrain_no = train.loc[train[\"Response\"] == 0]['Annual_Premium'].values.tolist()\n\nplt.figure()\n\nplt.subplot(2, 1, 1)\nsns.kdeplot(x = train_both, fill = True)\nplt.title(\"General annual premium distribution\")\n    \nplt.subplot(2, 2, 3)\nsns.kdeplot(x = train_yes, fill = True, color = \"green\")\nplt.title(\"Took insurance\")\n    \nplt.subplot(2, 2, 4)\nsns.kdeplot(x = train_no, fill = True, color = \"orange\")\nplt.title(\"Did not take insurance\")\n\nplt.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions don't seem to be very different."},{"metadata":{},"cell_type":"markdown","source":"#### Policy Sales Channel"},{"metadata":{},"cell_type":"markdown","source":"Lets look at the top channels in term of percentage of people that do/don't take insurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"channels = [i for i in range(1, 163)]\n\nchannel_yes = []\nchannel_no = []\n\nfor channel in channels:\n    channel_yes.append(train.loc[(train[\"Policy_Sales_Channel\"] == channel) & (train[\"Response\"] == 1)].shape[0])\n    channel_no.append(train.loc[(train[\"Policy_Sales_Channel\"] == channel) & (train[\"Response\"] == 0)].shape[0])\n    \nchannel_yes, channel_no = np.array(channel_yes), np.array(channel_no)\n\nchannel_no = channel_no + 1\n\npercent_yes = channel_yes/(channel_yes+channel_no)\npercent_no = channel_no/(channel_yes+channel_no)\n\ntops_yes  = (-percent_yes).argsort()[:3]\ntops_no = (-percent_no).argsort()[:5]\n\ntops_yes += 1\ntops_no += 1\n\narr = train[\"Policy_Sales_Channel\"].values.tolist()\naux_yes = []\naux_no = []\n\nfor channel in arr:\n    if channel in tops_yes: \n        aux_yes.append(1)\n        aux_no.append(0)\n    elif channel in tops_no:\n        aux_no.append(1)\n        aux_yes.append(0)\n    else:\n        aux_yes.append(0)\n        aux_no.append(0)\n\ntrain[\"Top_Yes_Channel\"] = aux_yes\ntrain[\"Top_No_Channel\"] = aux_no","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Top channels in term of percentage of people that don't take insurance\ntop_yes_channel = train.loc[train[\"Top_Yes_Channel\"] == 1]\ntop_no_channel = train.loc[train[\"Top_No_Channel\"] == 1]\n\nplt.figure()\n\nplt.subplot(1, 2, 1)\nsns.countplot(x = top_yes_channel[\"Policy_Sales_Channel\"], hue = train[\"Response\"])\nplt.title(\"Most effective channel\")\n\nplt.subplot(1, 2, 2)\nsns.countplot(x = top_no_channel[\"Policy_Sales_Channel\"], hue = train[\"Response\"])\nplt.title(\"Less effective channel\")\n\n\nplt.tight_layout()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vintage"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.boxplot(y = train[\"Vintage\"], x  = train[\"Response\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I did some others visualizations and all of them look alike this one. They don't seem to have much information."},{"metadata":{},"cell_type":"markdown","source":"#### What does the dataset look like right now?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will use the one hot encoding to encode the categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train)  #One Hot Encoding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,12))\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True,  linecolor='white', annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Models"},{"metadata":{},"cell_type":"markdown","source":"#### Librarys"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split data\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n\n#Scale data\nfrom sklearn.preprocessing import StandardScaler\n\n#Metrics used in the competition to check model's answers\nfrom sklearn.metrics import roc_auc_score\n\n#Models\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#Reproducibility of results\nimport random\nrandom.seed(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Function to create models and measure their efficency\n\nI'll use the following models:\n    Naive Bayes, Decision tree, Random forest, Logistic regression\n  \nI'll use stratified nested cross validation because our data has a lot more people not interested in the insurance than people interested in it. Finnaly, ROC AUC score will be the score measure because it was the one used in the hackaton."},{"metadata":{"trusted":true},"cell_type":"code","source":"def classification_nested_crossvalidation(X, y, n_folds,k_folds, model, hp_grid):\n    \"\"\"\n    Function that applies stratified nested crossvalidation method in a model and return\n    its mean and std roc auc score\n    \n    Input:\n        X - Independent variables;\n        y - dependent variable (target);\n        n_folds - number of folds;\n        model - model to be applied.\n    \"\"\"\n    \n    #External cross validation\n    cv_outer = StratifiedKFold(n_folds, shuffle=True)\n    results = list()\n    \n    # internal cross validation\n    for train_out, test_out in cv_outer.split(X, y):\n        \n        train_x, train_y = X[train_out], y[train_out]\n        test_x, test_y = X[test_out], y[test_out]\n        \n        # internal CV (training and validation sets)\n        grid_search_cv = GridSearchCV(model, hp_grid, cv=k_folds)\n        \n        # find the best hyperparameters\n        result = grid_search_cv.fit(train_x, train_y)\n        best_model = result.best_estimator_\n        \n        # prediction in the test fold\n        pred_y = best_model.predict(test_x) \n        results.append(roc_auc_score(test_y, pred_y))\n    \n    return np.mean(results), np.std(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.loc[:, train.columns != \"Response\"]\ny = train.loc[:, \"Response\"]\nX, y = X.to_numpy(), y.to_numpy()\n\nmodels = [GaussianNB(),DecisionTreeClassifier(), RandomForestClassifier(), LogisticRegression()]\nnames = [\"Naive Bayes\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\"]\nhiperparams = [{}, \n               {'criterion':['gini', 'entropy']},\n               {'criterion':['entropy', 'gini'],'n_estimators':[10, 20]},\n               {'solver':['newton-cg', 'lbfgs'], 'max_iter':[500]}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, model, params in zip(names, models, hiperparams):\n        \n        n_folds = 5\n        k_folds = 3\n        mean, std = classification_nested_crossvalidation(X, y, n_folds, k_folds, model, params)\n        message = \"Model {}:\\nMean: {}\\tStd:{}\\n\\n\".format(name, mean, std)\n        print(message)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression method did not converge... Any tips of what should I do here, other than increasing the parameter \"max_iter\"?"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}