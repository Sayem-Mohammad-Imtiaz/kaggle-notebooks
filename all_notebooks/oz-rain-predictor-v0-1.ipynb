{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Importing Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport datetime\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report,roc_curve, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:44:01.086245Z","iopub.execute_input":"2021-08-01T10:44:01.086661Z","iopub.status.idle":"2021-08-01T10:44:02.093586Z","shell.execute_reply.started":"2021-08-01T10:44:01.086577Z","shell.execute_reply":"2021-08-01T10:44:02.092597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### Reading and analyzing the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")\n\nprint(f\"The data has {int(df.shape[0])} rows and {int(df.shape[1])} columns\")\nprint(\"\\n\")\n\n# creating a function to print the summary of any dataframe\n\ndef print_summary(df,Datatype):\n    keys = []\n    types = []\n    unique_number = []\n    all_unique_values = []\n    missing_rows_number = []\n    missing_rows_percentage = []\n\n    for i in df:\n        keys.append(i)\n        type = \"categorical variable\" if str(df[i].dtype) == \"object\" else \"continious variable\"\n        types.append(type)\n        missing_rows_number.append(df[i].isna().sum())\n        missing_rows_percentage.append(((df[i].isna().sum()/df.shape[0])*100).round(2))\n\n        if type == \"categorical variable\":\n            unique_values = df[i].nunique()\n\n            if unique_values == 2:\n                all_unique_values.append(df[i].unique())\n            else:\n                all_unique_values.append(\"Not a Boolean Value\")\n\n            unique_number.append(unique_values)\n\n        else:\n            unique_number.append(\"NA\")\n            all_unique_values.append(\"NA\")\n\n    summary_df = pd.DataFrame(data = {\"columns\":keys,\n                                      \"DataType\":types,\n                                      \"missingRows\":missing_rows_number,\n                                      \"PercentageMissing\":missing_rows_percentage,\n                                      \"uniqueValuesNumber\":unique_number,\n                                      \"uniqueValues\":all_unique_values})\n    \n    if Datatype == \"all\":\n        print(summary_df)\n    \n    if Datatype == \"categorical\":\n        print(summary_df[summary_df[\"DataType\"] == \"categorical variable\"])\n        \n    if Datatype == \"continious\":\n        print(summary_df[summary_df[\"DataType\"] == \"continious variable\"])\n\n\n# printing the summary of the original dataframe\nprint_summary(df = df,Datatype = \"all\" )\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:44:02.095023Z","iopub.execute_input":"2021-08-01T10:44:02.095296Z","iopub.status.idle":"2021-08-01T10:44:03.100319Z","shell.execute_reply.started":"2021-08-01T10:44:02.09527Z","shell.execute_reply":"2021-08-01T10:44:03.099566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's explore the target variable RainTomorrow","metadata":{}},{"cell_type":"code","source":"print(\"Number of unique values in the target variable RainTomorrow is %d\" %(df[\"RainTomorrow\"].nunique()))\nprint(\"Number of unique values in the target variable RainTomorrow is %s\" %(df[\"RainTomorrow\"].unique()))\nprint(\"Number of unique values in the target variable RainTomorrow is %d\" %(df[\"RainTomorrow\"].isna().sum()))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:44:03.101942Z","iopub.execute_input":"2021-08-01T10:44:03.102409Z","iopub.status.idle":"2021-08-01T10:44:03.16067Z","shell.execute_reply.started":"2021-08-01T10:44:03.102364Z","shell.execute_reply":"2021-08-01T10:44:03.159742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the target variable has 3267 missing values. \nNow do we need to impute the missing values or delete the missing values ? \nMy approach is to delete them , because to impute the missing values , we have to build another model.\nThe accuracy of that predictor should be 100% which will be quiet unlikely.","metadata":{}},{"cell_type":"code","source":"nonNullRows = df[\"RainTomorrow\"].notna()\ndf = df[nonNullRows]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:44:03.162012Z","iopub.execute_input":"2021-08-01T10:44:03.162302Z","iopub.status.idle":"2021-08-01T10:44:03.201636Z","shell.execute_reply.started":"2021-08-01T10:44:03.162274Z","shell.execute_reply":"2021-08-01T10:44:03.200822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the rows which has null value for the target variable RainTomorrow are removed.\n\nLet's further analyze the target variable after removing the null values","metadata":{}},{"cell_type":"code","source":"print(\"Individual count of unique value of Target variable \\n\",df['RainTomorrow'].value_counts())\nprint(\"\\n\\n\")\nprint(\"Percentage composition of unique value of Target variable \\n\",df['RainTomorrow'].value_counts()/len(df))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:44:03.202945Z","iopub.execute_input":"2021-08-01T10:44:03.203257Z","iopub.status.idle":"2021-08-01T10:44:03.292597Z","shell.execute_reply.started":"2021-08-01T10:44:03.20323Z","shell.execute_reply":"2021-08-01T10:44:03.291862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**77.5% of the total dataset contains No**\n\nSo if we set all the rows in the test data to No, our model will still have an overall accuracy of 77.5% which may look good for layman's eyes but it will not help them in their purpose. \n\nThat's why classification metrics such as **precision**, **recall** comes handy to evaluate our classification model","metadata":{}},{"cell_type":"markdown","source":"### Exploring all the categorical variables","metadata":{}},{"cell_type":"code","source":"print_summary(df = df,Datatype = \"categorical\" )","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:44:03.29375Z","iopub.execute_input":"2021-08-01T10:44:03.294309Z","iopub.status.idle":"2021-08-01T10:44:03.715074Z","shell.execute_reply.started":"2021-08-01T10:44:03.294275Z","shell.execute_reply":"2021-08-01T10:44:03.714092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 7 categorical variable out of them **RainTomorrow** is the target variable. So we have 6 categorical variable of interest\n\n\n****Location****\n\nThis weather data is the data of 49 different location of around Australia. It has no missing values in this dataset.\nTherefore location cannot be used for prediction unless we are going to group the data by location and predict the rain for individual location. \nIf we want to do prediction for the whole dataset , then we need to dummy variables for this location variable.\nNevertheless we can explore the location variable to discover its usefulness if any\n\n****Date****\n\nDate is classified as a categorical variable.\n\nIt also has 3436 unique values which poseses a threat of **high cardiality.** (**cardiality** is the number of unique values of a categorical variable). We need to do some preprocessing to explore some usefulness\n\n\n****RainToday**** is a boolean value of today's rain\n\nRest of them are some variables indicating the direction of the wind. We need to explore more to find its usefulness\n\nWindGustDir,WindDir9am and WindDir3pm have 16 unique values. \n\n\n","metadata":{}},{"cell_type":"code","source":"for col in [\"Location\",\"RainToday\",\"WindGustDir\",\"WindDir9am\",\"WindDir3pm\"]:\n    print(col)\n    print(\"The number of unique values are \" + str(df[col].nunique()))\n    print(\"Number of missing values are \" + str(df[col].isnull().sum()))\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T11:19:43.31708Z","iopub.execute_input":"2021-08-01T11:19:43.317694Z","iopub.status.idle":"2021-08-01T11:19:43.518209Z","shell.execute_reply.started":"2021-08-01T11:19:43.31766Z","shell.execute_reply":"2021-08-01T11:19:43.517314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.describe().T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputing the missing values","metadata":{}},{"cell_type":"markdown","source":"*Filling the categorical value with mode and the continious value with mean*\n\nThere are other ways to fill the missing values by using a regression model ","metadata":{}},{"cell_type":"code","source":"# Filling the categorical value with mode and the continious value with mean\n\nfor i in range(len(summary_df)):\n\n    column_name = summary_df[\"columns\"][i]\n    if summary_df[\"DataType\"][i] == \"categorical variable\":\n        df[column_name]= df[column_name].fillna(df[column_name].mode()[0])\n    \n    if summary_df[\"DataType\"][i] == \"continious variable\":\n        df[column_name]= df[column_name].fillna(df[column_name].mean())\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Investigating categorical variable with too many categories","metadata":{}},{"cell_type":"code","source":"summary_df[summary_df['DataType'] == 'categorical variable']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Breaking data variable, because some of the information in the date could have correlation with the target varaible.\n* Replacing Boolean values Yes,No with 1,0\n* Encoding the categorical variables","metadata":{}},{"cell_type":"code","source":"# Working on Date Variable\ndf['date'] = df.apply(lambda x: datetime.datetime(year = int(x['Date'].split('-')[0]),\n                                               month = int(x['Date'].split('-')[1]),\n                                               day = int(x['Date'].split('-')[2])), axis=1)\n\n# Month of the year can be correlated to the raining , so lets also have that as one of our predictors\ndf['month'] = df.apply(lambda x : int(x['Date'].split('-')[1]),axis=1)\ndf['year'] = df.apply(lambda x : int(x['Date'].split('-')[0]),axis=1)\n\n# Changing to Boolean values for columns RainToday and  RainTomorrow\ndf['RainToday'] = df['RainToday'].replace(to_replace={\"Yes\":1,\"No\":0})\ndf['RainTomorrow'] = df['RainTomorrow'].replace(to_replace={\"Yes\":1,\"No\":0})\n\n# Creating a duplicate variable for location column to plot\ndf[\"Location1\"] = df[\"Location\"]\n\n# Encoding the other variables which has many categorical labels\nle = LabelEncoder()\ndf[\"Location\"] = le.fit_transform(df[\"Location\"])\ndf[\"WindDir9am\"]= le.fit_transform(df[\"WindDir9am\"])\ndf[\"WindDir3pm\"]= le.fit_transform(df[\"WindDir3pm\"])\ndf[\"WindGustDir\"] = le.fit_transform(df[\"WindGustDir\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting some interesting variables","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,20))\n\naxis_0 = fig.add_subplot(6,2,1,title='Rain Today')\nnames = [str(i) for i in list(df[\"RainToday\"].unique())]\nvalues = list(df[\"RainToday\"].value_counts())\naxis_0.bar(names,values,color=['red','green'])\n\n\naxis_1 = fig.add_subplot(6,2,2,title='Rain Tomorrow')\nnames1 = [str(i) for i in list(df[\"RainTomorrow\"].unique())]\nvalues1 = list(df[\"RainTomorrow\"].value_counts())\naxis_1.bar(names1,values1,color=['red','green'])\n\naxis_2 = fig.add_subplot(6,2,(3,4),title='Daily Max Temperature')\ntemp = df[\"MaxTemp\"]\nbins = 5\naxis_2.hist(temp,bins = bins ,color = 'orange')\n\naxis_3 = fig.add_subplot(6,2,(5,6),title='Daily Min Temperature')\ntemp2 = df[\"MinTemp\"]\nbins2 = 5\naxis_3.hist(temp,bins = bins ,color = 'blue')\n\n\ndf_month_mean = df[['month','Rainfall','MaxTemp','MinTemp']].groupby('month').mean().reset_index()\ndf_location_mean = df[['Location1','Rainfall']].groupby('Location1').mean().reset_index()\n\naxis_4 = fig.add_subplot(6,2,(7,8),title='Average Rainfall by Month')\nnames4 = df_month_mean['Rainfall']\nvalues4 = df_month_mean['month']\naxis_4.bar(values4,names4,color='pink')\n\n\naxis_5 = fig.add_subplot(6,2,(9,10),title='Average Max Temp by Month')\nnames5 = df_month_mean['MaxTemp']\nvalues5 = df_month_mean['month']\naxis_5.bar(values5,names5,color='brown')\n\n\naxis_6 = fig.add_subplot(6,2,(11,12),title='Average Rainfall by Location')\nnames6 = df_location_mean['Rainfall']\nvalues6 = df_location_mean['Location1']\naxis_6.bar(values6,names6,color='violet')\naxis_6.tick_params(labelrotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Correlation Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only selecting the variable which has a correlation coefficient value greater than 0.1\ncorr_table = abs(df.corr()['RainTomorrow'])\ncorr_table = corr_table[corr_table > 0.1]\npredictor_variable = list(corr_table.keys())\ndf_ML = df[predictor_variable]\n\nplt.figure(figsize=(20,20))\nsns.heatmap(df_ML.corr(), annot=True)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Findings**\n* It looks like some of the independent variables are correlated among them, this could cause a multi collinearity issue.\n* MaxTemp and Temp3pm has R2 value of 0.97\n* Pressure9am and Pressure3pm has R2 value of 0.96","metadata":{}},{"cell_type":"markdown","source":"### Checking Variance Inflation Factor to investigate multi collinearity","metadata":{}},{"cell_type":"code","source":"def calculate_VIF(df):\n    VIF = pd.DataFrame()\n    all_VIF = []\n    VIF[\"predictors\"] = df.keys()\n    for i in range(df.shape[1]):\n        var_VIF = variance_inflation_factor(df.values,i)\n        all_VIF.append(var_VIF)\n    VIF[\"VIF\"] = all_VIF\n    return VIF\n\ndf_ML_VIF = calculate_VIF(df_ML.iloc[:,:-1])\ndf_ML_VIF[\"Target_correlation\"] = abs(df_ML.corr()[\"RainTomorrow\"]).reset_index(drop=True)\ndf_ML_VIF.sort_values(by=\"Target_correlation\",ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Findings**\n* Humidity is highly correlated with its also correlated with other independent variable \n* MaxTemp and Temp3pm also has high correlation\n* So in order to strike a balance , we need to eliminate those variables with high VIF and low correlation.\n* Then check for VIF again","metadata":{}},{"cell_type":"code","source":"df_ML_VIF[\"survivors\"] = np.where((df_ML_VIF[\"Target_correlation\"] < 0.3) & (df_ML_VIF[\"VIF\"] > 20),0,1)\nfinal_predictor = df_ML_VIF[\"predictors\"][df_ML_VIF[\"survivors\"]==1]\ndf_ML_2 = df_ML[list(final_predictor) + ['RainTomorrow']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df_ML_2.corr(), annot=True)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now looks like Sunshine is highly correlated with other predictors. Lets check for VIF**","metadata":{}},{"cell_type":"code","source":"calculate_VIF(df_ML_2.iloc[:,:-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Findings**\n* Looks like the VIF of sunshine is not too high , usually VIF of more than 15 should be removed.\n* Sunshine is highly correlated with the Target variable\n* Therefore not removing it from the list of predictors","metadata":{}},{"cell_type":"markdown","source":"# 4. Model Training","metadata":{}},{"cell_type":"markdown","source":"### Splitting the training and test dataset","metadata":{}},{"cell_type":"code","source":"x = df_ML_2.iloc[:,:-1]\ny = df_ML_2.iloc[:,-1:]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train a logistic regression model on the training set","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear', random_state=0)\nlogreg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Prediction and Model evaluation","metadata":{}},{"cell_type":"markdown","source":"### Predicting with the trained logistics regression model","metadata":{}},{"cell_type":"code","source":"y_pred_test = logreg.predict(X_test)\npredicted = pd.DataFrame(y_pred_test,columns=[\"prediction\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification Report","metadata":{}},{"cell_type":"code","source":"report = classification_report(y_test, y_pred_test)\nprint(report)\nprint(f\"Accuracy of the Logistics Regresssion Model is: {accuracy_score(y_test,y_pred_test)*100}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoding the Classification report","metadata":{}},{"cell_type":"markdown","source":"**Recall**\n\n> Recall = True Positives / True Positives + False Negative\n\nWhich means the number of positive cases which our model have correctly predicted from the number of actual postive cases\n\nThis means if the audience is only interested in our model's prediction of rain days , then we have to check for model's recall of rain prediction. \n\nEven if we predict all the days as non-rainy days, we will have a decent over all accuracy but that doesn't helps the audience because most of the days will be non-rainy days.\n\n**Recall only looks at the postive cases**\n\nHaving said that, \n\n* Recall for rainy day prediction is low with our model with 45%. Out of 6420 rainy days we only predicted 2859 \n* Recall for non-rainy day prediction is high with our model with 95%. Out of 22672 non-rainy days we only predicted 21443 (But this is not relevent to the audience)","metadata":{}},{"cell_type":"markdown","source":"**Precision**\n\n> Precision = True Positives / True Positives + False Positives\n\nHow often a prediction is precise, is measured by precision.\nIf our model predict its going to be a rainy day , how often its true.\n\nPrecision for Rainy Day predcition = 2859/(2859 + 1229) = 0.70\n\nPrecision for Non-Rainy Day prediction = 21443/(21443 + 3561) = 0.86 (This is not relevant to the audience)\n\nPrecision for rain prediction is low with our model.","metadata":{}},{"cell_type":"markdown","source":"**F-1score**\n\n> F1-score = 2 * {(recall * precision)/(recall + precision)}\n\nF1-score is a metric that gives equal weightage to both precision and recall and computes a value.\nThat value should be higher for a good classification model.\n\nIn our case, the F-1 score for rainy-day prediction is only 0.54, not so great. Eventhough the overall accuracy is 0.84 which is 84%","metadata":{}},{"cell_type":"markdown","source":"### Plotting the confusion matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncm = confusion_matrix(y_test, predicted)\ncm = pd.DataFrame(cm)\ncm.columns = [\"Predicted Non-Rainy Days\",\"Predicted Rainy Days\"]\ncm.index = [\"Actual Non-Rainy Days\", \"Actual Rainy Days\"]\n# cm = cm.pivot(\"Actual\",\"Predicted\")\nsns.heatmap(cm, annot=True,fmt='d',cmap=\"Blues\")\nplt.title(\"Confusion Matrix for Logistics Model\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More to come in the next upload","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}