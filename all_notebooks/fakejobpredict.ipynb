{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"427b02c0-00e9-46d3-b529-2b6ebe8c14df","_cell_guid":"26b00ed6-1ccf-4bc2-b285-4b4dcac614f4","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n#  ##                                             Fake Job Posting Prediction\n\n# ## 1) Importing Library \n# I started yo import all the libraries which I need. In there, I import some models library(MultinominalNB and LinearSVC)and some preprocessing library for the data cleaning.\n\n# In[158]:\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nfrom collections import defaultdict\nimport nltk\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download(\"stopwords\")\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nfrom wordcloud import WordCloud, STOPWORDS\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport scikitplot as skplt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\n# ### 2) Importing the dataset and describing \n\n# In[171]:\n\n\ndata = pd.read_csv(\"../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\")\n\n\n# In[172]:\n\n\ndata.head()\n\n\n# In[173]:\n\n\ndata.info()\n\n\n# In[174]:\n\n\ndata.shape\n\n\n# In[175]:\n\n\ndata.describe()\n\n\n# In[176]:\n\n\nsns.countplot(data.employment_type)\n\n\n# ## 3) Preprocessing step \n# In this step, I start to look at my dataset to fill the NA data to space. After that, I keep going on with merge the all columns into a column which is all_text. So, I can clean the dataset. \n\n# In[177]:\n\n\ndata.isna().sum()\n\n\n# In[178]:\n\n\ndata.fillna(\" \",inplace = True)\n\n\n# In[179]:\n\n\ndata[\"all_text\"] = data[\"title\"] + \" \" + data[\"location\"] + \" \" + data[\"department\"] + \" \" + data[\"company_profile\"] + \" \" + data[\"description\"] + \" \" + data[\"requirements\"] + \" \" + data[\"benefits\"] + \" \" + data[\"employment_type\"] + \" \" + data[\"required_experience\"] + \" \" + data[\"required_education\"] + \" \" + data[\"industry\"] + \" \" + data[\"function\"]\n\n\n# In[180]:\n\n\ndata\n\n\n# I consider the text as a sequence of words can be a reasonable choice. And, I used to find boundaries of words by splitting sentences by space of punctuation. So, I am going to convert text to lowercase. After that, I keep going with Lemmatization. It is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word’s lemma, or dictionary form.\n\n# In[181]:\n\n\ndata['all_text'] = [entry.lower() for entry in data['all_text']]\n\n\n# In[182]:\n\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlmtzr = WordNetLemmatizer()\nprint(data['all_text'])\ndata['last_text'] = data['all_text'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\nprint(data['last_text'])\n\n\n# In the below codes, I consider with removal stop words. These are the most common words in a language like 'some', 'more', 'its'. When I remove all of the stop words the sentence becomes more cleaner.\n\n# In[183]:\n\n\nstop_words = set(stopwords.words('english'))\n\n\n# In[184]:\n\n\nprint(stop_words)\n\n\n# In[185]:\n\n\ndata['last_text'] = data['last_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nprint(data['last_text'])\n\n\n# Now we can see the result of the preparations we made here in the preliminary preparation section, here we can find the most written words in the actual job postings in the fraudulent column on the dataset. I use to plot using the word cloud.\n\n# In[186]:\n\n\n\nplt.figure(figsize = (20,20)) # Text that is not fraudulent(0)\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(data[data.fraudulent == 0].last_text))\nplt.imshow(wc , interpolation = 'bilinear')\n\n\n# In the below image, we are going to see which keyword groups consist of fake job postings.\n\n# In[187]:\n\n\nplt.figure(figsize = (20,20)) # Text that is not fraudulent(0)\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(data[data.fraudulent == 1].last_text))\nplt.imshow(wc , interpolation = 'bilinear')\n\n\n# ## 4) Transforming anf Vectorizing the all_text\n# \n# After the preprocessing part, I need to have test and training data all set up. So, I start by splitting data and use a test size of 0.30. Now, I can create vectorized representations of the all_text. \n\n# In[188]:\n\n\ny = data[\"fraudulent\"]\n\n\n# In[189]:\n\n\nX_train, X_test, y_train, y_test = train_test_split(data[\"last_text\"], y, random_state=0, test_size=.30)\n\n\n# In[190]:\n\n\ncount_vectorizer = CountVectorizer(stop_words=\"english\", min_df=0.05, max_df=0.9)\n\n# Create count train and test variables\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=0.05, max_df=0.9)\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n\n# In[191]:\n\n\nprint(count_train.shape)\nprint(count_test.shape)\nprint(tfidf_train.shape)\nprint(tfidf_test.shape)\n\n\n# ### 5) Modelling with training a multinomial naive bayes model\n# \n# So, I have vectorized data. I can train the multinomial naive Bayes model. And, we are going to see the accuracy results. The accuracy score is %96 which is a very good score for the naive classifier.\n\n# In[192]:\n\n\n\n# Create a MulitnomialNB model\ntfidf_nb = MultinomialNB()\ntfidf_nb.fit(tfidf_train, y_train)\n# ... Train your model here ...\n\n# Run predict on your TF-IDF test data to get your predictions\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n\n# Calculate the accuracy of your predictions\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\n# Create a MulitnomialNB model\ncount_nb = MultinomialNB()\n# ... Train your model here ...\ncount_nb.fit(count_train, y_train)\n# Run predict on your count test data to get your predictions\ncount_nb_pred = count_nb.predict(count_test)\n\n# Calculate the accuracy of your predictions\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\ntf_nb = confusion_matrix(y_test, tfidf_nb_pred)\nc_nb = confusion_matrix(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\n\n# I try out another classifier which is Linear svc. \n\n# In[118]:\n\n\n\n# Create a LinearSVM model\ntfidf_svc = LinearSVC()\n\n# ... Train your model here ...\ntfidf_svc.fit(tfidf_train, y_train)\n# Run predict on your tfidf test data to get your predictions\ntfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n\n# Calculate your accuracy using the metrics module\ntfidf_svc_score = metrics.accuracy_score(y_test, tfidf_svc_pred)\n\nprint(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n\n# Calculate the confusion matrices for the tfidf_svc model\n\nsvc_cm = confusion_matrix(y_test, tfidf_svc_pred)\n\n    \n\nax= plt.subplot()\nsns.heatmap(svc_cm, annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['real', 'fake']); ax.yaxis.set_ticklabels(['real', 'fake']);\n\n\n# Predicting the sentiment for the documents in our test set I can use the predicting method of the RandomForestClassifier class as shown below and I got a very high accuracy score.\n\n# In[193]:\n\n\n\nrf = RandomForestClassifier(bootstrap=True)\n\nrf.fit(tfidf_train, y_train)\n\n# Cross validation of 5 folds\nscore = cross_val_score(rf, tfidf_train, y_train)\n\nprint(f'Prediction score: {np.mean(score) * 100:.2f}%')\n\n\n# ### 6) Evaluating my model using a confusiın matrix\n# The confusion matrix gives additional insight into accuracy by class and intuition for precision and recalls efficiency. And, we are going to see them that is so high efficiency. From the output, it can be seen that our model achieved an accuracy of 97.3%, which is very good given the fact that we randomly chose all the parameters for CountVectorizer as well as for our random forest algorithm.\n\n# In[194]:\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, tfidf_nb_pred, target_names = ['0','1']))\n\n\n# In[195]:\n\n\nprint(classification_report(y_test, tfidf_svc_pred, target_names = ['0','1']))\n\n\n# In[204]:\n\n\nplt.figure(figsize = (4,4))\nsns.heatmap(svc_cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Not Fake'] , yticklabels = ['Fake','Not Fake'])\n\n\n# In below the plots, we are going to benchmark other models using a confusion matrix. \n\n# In[205]:\n\n\nplt.figure(figsize=(20,15))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=20)\n\nplt.subplot(2,3,1)\nplt.title(\"LinearSVC Confusion Matrix\")\nsns.heatmap(svc_cm,cbar=False,annot=True,cmap=\"Greens\",fmt=\"d\")\n\nplt.subplot(2,3,2)\nplt.title(\"MulitnomialNB TF Confusion Matrix\")\nsns.heatmap(tf_nb,cbar=False,annot=True,cmap=\"Greens\",fmt=\"d\")\n\nplt.subplot(2,3,3)\nplt.title(\"MulitnomialNB Count Confusion Matrix\")\nsns.heatmap(c_nb,cbar=False,annot=True,cmap=\"Greens\",fmt=\"d\")\n\n\nplt.show()\n\n\n# IN ADDITION: All_text full data visualization\n\n# In[148]:\n\n\ntext = ''\nfor news in data.all_text.values:\n    text += f\" {news}\"\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text\n\n\n# ### 7) Conclusion\n# \n# At the beginning of the project, I start with importing and cleaning data. After that, I continue with preprocessing by doing text cleaning. So, I prepare my dataset to build a model. I used my model into a fake job dataset. At the end of the project, I compare and evaluate the models using a confusion matrix. The results, the highest accuracy score is LinearSVC model. And, when we looked at the confusion matrix scores, true positive is so high to others in every model, and false negative and false positive is very low. So, we are going to which are real and fake. \n\n# ### 8) Reference\n# [Kaggle notebooks](https://www.kaggle.com/madz2000/text-classification-using-keras-nb-97-accuracy) and\n# [Text classification pages](https://stackabuse.com/text-classification-with-python-and-scikit-learn/)\n\n# In[ ]:","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}