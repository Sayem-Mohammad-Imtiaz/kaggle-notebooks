{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\nApache Spark, once a component of the Hadoop ecosystem, is now becoming the big-data platform of choice for enterprises. It is a powerful open source engine that provides real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and standard interface.\n\nLink :- https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploring The Data**\n\nWe will use the same data set when we built a Logistic Regression in Python, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (Yes/No) to a term deposit. The dataset can be downloaded from Kaggle.\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('ml-bank').getOrCreate()\ndf = spark.read.csv('../input/bank.csv', header = True, inferSchema = True)\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.DataFrame(df.take(5), columns=df.columns).transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"**Analyzing Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('deposit').count().toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\ndf.select(numeric_features).describe().toPandas().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_data = df.select(numeric_features).toPandas()\naxs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\nn = len(numeric_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\ncols = df.columns\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preparing Data for Machine Learning**\n\nThe process includes Category Indexing, One-Hot Encoding and VectorAssembler — a feature transformer that merges multiple columns into a vector column.\n\nCode given below indexes each categorical column using the StringIndexer, then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row. \n\nWe use the StringIndexer again to encode our labels to label indices. \n\nNext, we use the VectorAssembler to combine all the feature columns into a single vector column."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n\ncategoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n\nstages = []\n\nfor categoricalCol in categoricalColumns:\n    \n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    \n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    \n    stages += [stringIndexer, encoder]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the StringIndexer again to encode our labels to label indices. Next, we use the VectorAssembler to combine all the feature columns into a single vector column."},{"metadata":{"trusted":true},"cell_type":"code","source":"label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n\nstages += [label_stringIdx]\n\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n\nstages += [assembler]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pipeline**\n\nWe use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow. A Pipeline’s stages are specified as an ordered array."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\nselectedCols = ['label', 'features'] + cols\ndf = df.select(selectedCols)\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df.take(5), columns=df.columns).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = df.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Logistic Regression Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingSummary = lrModel.summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Precision and Recall**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = lrModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluate Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', evaluator.evaluate(predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Classifier**\n\nDecision trees are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import DecisionTreeClassifier\ndt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\ndtModel = dt.fit(train)\npredictions = dtModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\nrfModel = rf.fit(train)\npredictions = rfModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient-Boosted Tree Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(maxIter=10)\ngbtModel = gbt.fit(train)\npredictions = gbtModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gbt.explainParams())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 60])\n             .addGrid(gbt.maxIter, [10, 20])\n             .build())\ncv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\ncvModel = cv.fit(train)\npredictions = cvModel.transform(test)\nevaluator.evaluate(predictions)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}