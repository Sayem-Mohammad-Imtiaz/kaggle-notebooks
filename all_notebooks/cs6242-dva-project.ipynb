{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport json\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom datetime import datetime\nimport glob\nimport re\nimport io\nfrom scipy.stats import boxcox","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Overview and Basic Analysis**"},{"metadata":{},"cell_type":"markdown","source":"**1.1 Overview the dataset**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/us-accidents/US_Accidents_June20.csv')\nprint(\"The shape of data is:\",(df.shape))\ndisplay(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Details about features(49) in the dataset:\n\n\nTraffic Attributes (12):\n* ID: This is a unique identifier of the accident record.\n* Source: Indicates source of the accident report (i.e. the API which reported the accident.).\n* TMC: A traffic accident may have a Traffic Message Channel (TMC) code which provides more detailed description of the event.\n* Severity: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\n* Start_Time: Shows start time of the accident in local time zone.\n* End_Time: Shows end time of the accident in local time zone.\n* Start_Lat: Shows latitude in GPS coordinate of the start point.\n* Start_Lng: Shows longitude in GPS coordinate of the start point.\n* End_Lat: Shows latitude in GPS coordinate of the end point.\n* End_Lng: Shows longitude in GPS coordinate of the end point.\n* Distance(mi): The length of the road extent affected by the accident.\n* Description: Shows natural language description of the accident.\n\n\nAddress Attributes (9):\n* Number: Shows the street number in address field.\n* Street: Shows the street name in address field.\n* Side: Shows the relative side of the street (Right/Left) in address field.\n* City: Shows the city in address field.\n* County: Shows the county in address field.\n* State: Shows the state in address field.\n* Zipcode: Shows the zipcode in address field.\n* Country: Shows the country in address field.\n* Timezone: Shows timezone based on the location of the accident (eastern, central, etc.).\n\n\nWeather Attributes (11):\n* Airport_Code: Denotes an airport-based weather station which is the closest one to location of the accident.\n* Weather_Timestamp: Shows the time-stamp of weather observation record (in local time).\n* Temperature(F): Shows the temperature (in Fahrenheit).\n* Wind_Chill(F): Shows the wind chill (in Fahrenheit).\n* Humidity(%): Shows the humidity (in percentage).\n* Pressure(in): Shows the air pressure (in inches).\n* Visibility(mi): Shows visibility (in miles).\n* Wind_Direction: Shows wind direction.\n* Wind_Speed(mph): Shows wind speed (in miles per hour).\n* Precipitation(in): Shows precipitation amount in inches, if there is any.\n* Weather_Condition: Shows the weather condition (rain, snow, thunderstorm, fog, etc.).\n\n\nPOI Attributes (13):\n* Amenity: A Point-Of-Interest (POI) annotation which indicates presence of amenity in a nearby location.\n* Bump: A POI annotation which indicates presence of speed bump or hump in a nearby location.\n* Crossing: A POI annotation which indicates presence of crossing in a nearby location.\n* Give_Way: A POI annotation which indicates presence of give_way sign in a nearby location.\n* Junction: A POI annotation which indicates presence of junction in a nearby location.\n* No_Exit: A POI annotation which indicates presence of no_exit sign in a nearby location.\n* Railway: A POI annotation which indicates presence of railway in a nearby location.\n* Roundabout: A POI annotation which indicates presence of roundabout in a nearby location.\n* Station: A POI annotation which indicates presence of station (bus, train, etc.) in a nearby location.\n* Stop: A POI annotation which indicates presence of stop sign in a nearby location.\n* Traffic_Calming: A POI annotation which indicates presence of traffic_calming means in a nearby location.\n* Traffic_Signal: A POI annotation which indicates presence of traffic_signal in a nearby location.\n* Turning_Loop: A POI annotation which indicates presence of turning_loop in a nearby location.\n\n\nPeriod-of-Day (4):\n* Sunrise_Sunset: Shows the period of day (i.e. day or night) based on sunrise/sunset.\n* Civil_Twilight: Shows the period of day (i.e. day or night) based on civil twilight.\n* Nautical_Twilight: Shows the period of day (i.e. day or night) based on nautical twilight.\n* Astronomical_Twilight: Shows the period of day (i.e. day or night) based on astronomical twilight."},{"metadata":{},"cell_type":"markdown","source":"Lets separate the datasets based on data type so that we can make good analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_df = df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"**1.2 Reporting Sources**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_source = df.groupby(['Severity','Source']).size().reset_index().pivot(\\\n    columns='Severity', index='Source', values=0)\ndf_source.plot(kind='bar', stacked=True, title='Severity Count by Sources')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('Severity').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results indicate that there are two main sources, Bing and MapQuest. Meanwhile, we can see that most accidents are moderately severe in degree 2 and 3. "},{"metadata":{},"cell_type":"markdown","source":"**1.3 States**\n\n****"},{"metadata":{"trusted":true},"cell_type":"code","source":"states = df.State.unique()\n\ncount_by_state=[]\nfor i in df.State.unique():\n    count_by_state.append(df[df['State']==i].count()['ID'])\n\nfig,ax = plt.subplots(figsize=(16,10))\nsns.barplot(states,count_by_state)\nplt.title(\"Count of Accidents by States\", size=15, y=1.05)\nplt.xlabel('States',fontsize=15)\nplt.ylabel('Number of Accidents',fontsize=15)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"California, Texas and Florida are the TOP 3 traffic accident prone states."},{"metadata":{},"cell_type":"markdown","source":"**1.4 Weather**\n\nWhat weather conditions are most common when accidents occur?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(figsize=(16,7))\ndf['Weather_Condition'].value_counts().sort_values(ascending=False).head(10).plot.bar(width=0.5,color='Teal',edgecolor='k',align='center',linewidth=2)\nplt.xlabel('Weather Conditions',fontsize=20)\nplt.ylabel('Number of Accidents',fontsize=20)\nax.tick_params(labelsize=20)\nplt.title('Top 10 Weather Condition for accidents',fontsize=25)\nplt.grid()\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.5 Durations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = pd.to_datetime(df.Start_Time, format='%Y-%m-%d %H:%M:%S')\nend = pd.to_datetime(df.End_Time, format='%Y-%m-%d %H:%M:%S')\nlaps=end-start\ntop_15 = laps.astype('timedelta64[m]').value_counts().nlargest(15) \n#Return the first n rows ordered by columns in descending order.\nprint('Top 15 longest accidents correspond to {:.1f}% of the data'.format(top_15.sum()*100/len(laps)))\n(top_15/top_15.sum()).plot.bar(figsize=(10,8), color = 'plum')\nplt.title('Top 15 Accident Durations', fontsize = 24, color='indigo')\nplt.xlabel('Duration in minutes')\nplt.ylabel('% of Total Data')\nplt.grid(linestyle=':', linewidth = '0.2', color ='salmon');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.6 Time**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Start_Time'] = pd.to_datetime(df['Start_Time'])\ndf['End_Time'] = pd.to_datetime(df['End_Time'])\ndf['Month'] = df['Start_Time'].dt.month\ndf['Year'] = df['Start_Time'].dt.year\ndf['Hour'] = df['Start_Time'].dt.hour\ndf['Weekday'] = df['Start_Time'].dt.weekday\ndf['Duration'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds()/60\n\n# clean the data based on the condition that the impact on traffic is between zero-one week,\n# and drop duplicates\noneweek = 60*24*7\ndf_clean = df[(df['Duration']>0) & (df['Duration']< oneweek)].drop_duplicates(subset=['Start_Time','End_Time','City','Street','Number','Description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#time series analysis\ndf1 = df_clean[['Country','Start_Time','End_Time','Year','Month','Weekday','Hour','Duration','Severity']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Year**\n\nAccidents are increasing with time."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.set_context('talk')\nsns.set_palette('GnBu_d')\na = sns.catplot(x='Year',data=df_clean[df_clean['Year'] < 2020],kind='count')\na.fig.suptitle('Yearly Accident Cases(2016-2019)',y=1.03)\na.set(ylabel='yearly cases',xlabel='year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Month**\n\nThere were more accident cases druing August and December compared to other months,excluding the data from 2020. It may be due to more bad weather conditions in the winter.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context('talk')\nm = sns.catplot(x='Month',data=df1[df1['Year'] < 2019],kind='count')\nm.fig.suptitle('Monthly Accident Cases(2016-2019)',y=1.03)\nm.set(ylabel='monthly cases')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Week**\n\nIt looks like that more accidents happened on working day from Mon to Fri than on weekend."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context('talk')\nw = sns.catplot(x='Weekday',data=df1,kind='count')\nw.fig.suptitle('Weekday Accident Cases',y=1.03)\nw.set(ylabel='weekday cases')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hour**\n\nIt shows that the most accidents occur at around 7-8 am in the morning and 4 to 5 pm when most people travel between home and work. The increases of the traffic density could lead to more accidents."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context('paper')\nh = sns.catplot(x='Hour',data=df1,kind='count')\nh.fig.suptitle('Hourly accidents cases',y=1.03)\nh.set(ylabel='hourly cases',xlabel='hour')\nplt.annotate('morning peak',xy=(6,330000))\nplt.annotate('afternoon peak',xy=(15,270000))\nplt.annotate('bottom',xy=(1,25000))\nplt.annotate('go to work',xy=(7.5,0),xytext=(1,125000),arrowprops={'arrowstyle':'fancy'})\nplt.annotate('get off work',xy=(17.5,0),xytext=(19,150000),arrowprops={'arrowstyle':'fancy'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Data Processing and Cleaning**"},{"metadata":{},"cell_type":"markdown","source":"**Plotting correlation matrix**\n\nWe can use this to remove highly correlated features like keeping only one of wind chill and temperature as they both mean almost the same thing."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.gcf()\nfig.set_size_inches(20,20)\nfig=sns.heatmap(df_clean.corr(),annot=True,linewidths=0.2,linecolor='k',square=True,mask=False, vmin=-1, vmax=1,cbar_kws={\"orientation\": \"vertical\"},cbar=True)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.1 Drop meaningless features**\n\nFeatures 'Source' and 'ID' couldn't provide any useful information about accidents themselves. In addition, 'Start_time' 'End_Time' (we have year, month, weekday, hour and duration), 'End_Lat', and 'End_Lng'(we save start location) can be collected only after the accident has already happened and hence cannot be predictors for serious accident prediction. For 'TMC' and 'Description', the POI features have already been extracted from it by dataset creators. Let's get rid of these features first."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean.drop(['ID','TMC','Source','Description', 'End_Time', 'End_Lat', 'End_Lng', 'Distance(mi)','Duration'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop 'Country' and 'Turning_Loop' which have only one class."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_names = ['Side', 'Country', 'Timezone', 'Amenity', 'Bump', 'Crossing', \n             'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', \n             'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', \n             'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\nprint(\"Unique count of categorical features:\")\nfor i in cat_names:\n  print(i,df[i].unique().size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean.drop(['Country','Turning_Loop'], axis=1)\ndf_clean.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.2 Handling missing data**"},{"metadata":{},"cell_type":"markdown","source":"2.2.1 Get the ratio and the columns with missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = df_clean.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['columns_name','missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] /df_clean.shape[0]\nmissing_df.loc[missing_df['missing_ratio']>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.2.2 Missing more than 50%\n\n'Number' and 'Wind_Chill(F)' will be dropped because they are not highly related to severity according to previous research, whereas 'Precipitation(in)' could be a useful predictor and hence can be handled by separating feature.Drop 'Number', 'Wind_Chill(F)'. \nAdd a new feature for missing values in 'Precipitation(in)' and replace missing values with median."},{"metadata":{"trusted":true},"cell_type":"code","source":"missin = missing_df.loc[missing_df['missing_count']>500000]\nremovelist = missin['columns_name'].tolist()\nremovelist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean.drop(['Number','Wind_Chill(F)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['Precipitation_NA'] = 0\ndf_clean.loc[df_clean['Precipitation(in)'].isnull(),'Precipitation_NA'] = 1\ndf_clean['Precipitation(in)'] = df_clean['Precipitation(in)'].fillna(df_clean['Precipitation(in)'].median())\ndf_clean.loc[:5,['Precipitation(in)','Precipitation_NA']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.2.3 Missing less than 1% \n\nDrop NaN by features:\nThere still are some missing values but much less. Just dropna by these features for the sake of simplicity.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean.dropna(subset=['City','Zipcode','Timezone','Airport_Code',\n                       'Sunrise_Sunset','Civil_Twilight',\n                       'Nautical_Twilight','Astronomical_Twilight'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.2.4 Others\n\nThe other absent values will be imputed by employing the most common value\nfor categorical features and the median for continuous ones.\n\nWeather Data with missing values\n\n> - Continuous weather features with missing values:\nTemperature(F),\nHumidity(%),\nPressure(in),\nVisibility(mi),\nWind_Speed(mph),\n\nBefore imputation, weather features will be grouped by location and time first, to which weather is naturally related. 'Airport_Code' is selected as location feature because the sources of weather data are airport-based weather stations. Then the data will be grouped by 'Start_Month' rather than 'Start_Hour' because using the former is computationally cheaper and remains less missing values. Finally, missing values will be replaced by median value of each group."},{"metadata":{"trusted":true},"cell_type":"code","source":"# group data by 'Airport_Code' and 'Start_Month' then fill NAs with median value\nWeather_data=['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\nprint(\"The number of remaining missing values: \")\nfor i in Weather_data:\n  df_clean[i] = df_clean.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(x.median()))\n  print( i + \" : \" + df_clean[i].isnull().sum().astype(str))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean.dropna(subset=Weather_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Categorical Weather Features:\n\nFor categorical weather features, majority rather than median will be used to replace missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# group data by 'Airport_Code' and 'Start_Month' then fill NAs with majority value\nfrom collections import Counter\nweather_cat = ['Wind_Direction','Weather_Condition']\nprint(\"Count of missing values that will be dropped: \")\nfor i in weather_cat:\n  df_clean[i] = df_clean.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\n  print(i + \" : \" + df_clean[i].isnull().sum().astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop na\ndf_clean = df_clean.dropna(subset=weather_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.3 Transformation and Simplification of Categorical features**\n\nFor categorical variables, some attributes have more than 20\nlevels, leading to numerous attributes after dummy coding. Therefore, it would be more practical to simplify\nthem into a few levels. Moreover, relative old features with raw data, outliers and negative features can be removed."},{"metadata":{},"cell_type":"markdown","source":"2.3.1 Time features\n\nTime features has been normalized during EDA overview chapter. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"2.3.2 Weather features\n\n- Wind Direction"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Wind Direction: \", df_clean['Wind_Direction'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Simplify wind direction\ndf = df_clean\ndf.loc[df['Wind_Direction']=='Calm','Wind_Direction'] = 'CALM'\ndf.loc[(df['Wind_Direction']=='West')|(df['Wind_Direction']=='WSW')|(df['Wind_Direction']=='WNW'),'Wind_Direction'] = 'W'\ndf.loc[(df['Wind_Direction']=='South')|(df['Wind_Direction']=='SSW')|(df['Wind_Direction']=='SSE'),'Wind_Direction'] = 'S'\ndf.loc[(df['Wind_Direction']=='North')|(df['Wind_Direction']=='NNW')|(df['Wind_Direction']=='NNE'),'Wind_Direction'] = 'N'\ndf.loc[(df['Wind_Direction']=='East')|(df['Wind_Direction']=='ESE')|(df['Wind_Direction']=='ENE'),'Wind_Direction'] = 'E'\ndf.loc[df['Wind_Direction']=='Variable','Wind_Direction'] = 'VAR'\nprint(\"Wind Direction after simplification: \", df['Wind_Direction'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Weather Condition\n\nWeather-related vehicle accidents kill more people annually than large-scale weather disasters(source: weather.com). According to Road Weather Management Program, wet-pavement during rainfall, winter-condition and fog are the main reasons for weather-related accidents. To extract these three weather conditions, we first look at what we have in 'Weather_Condition' Feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# show distinctive weather conditions \nweather ='!'.join(df['Weather_Condition'].dropna().unique().tolist())\nweather = np.unique(np.array(re.split(\n    \"!|\\s/\\s|\\sand\\s|\\swith\\s|Partly\\s|Mostly\\s|Blowing\\s|Freezing\\s\", weather))).tolist()\nprint(\"Weather Conditions: \", weather)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create features for some common weather conditions.\ndf['Clear'] = np.where(df['Weather_Condition'].str.contains('Clear', case=False, na = False), 1, 0)\ndf['Cloud'] = np.where(df['Weather_Condition'].str.contains('Cloud|Overcast', case=False, na = False), 1, 0)\ndf['Rain'] = np.where(df['Weather_Condition'].str.contains('Rain|storm', case=False, na = False), 1, 0)\ndf['Heavy_Rain'] = np.where(df['Weather_Condition'].str.contains('Heavy Rain|Rain Shower|Heavy T-Storm|Heavy Thunderstorms', case=False, na = False), 1, 0)\ndf['Snow'] = np.where(df['Weather_Condition'].str.contains('Snow|Sleet|Ice', case=False, na = False), 1, 0)\ndf['Heavy_Snow'] = np.where(df['Weather_Condition'].str.contains('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls', case=False, na = False), 1, 0)\ndf['Fog'] = np.where(df['Weather_Condition'].str.contains('Fog', case=False, na = False), 1, 0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop old features with raw data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Start_Time','Weather_Timestamp','Weather_Condition'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Continuous Weather Features\n\nNormalize features with extreamly skewed distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pressure_bc']= boxcox(df['Pressure(in)'].apply(lambda x: x+1),lmbda=6)\ndf['Visibility_bc']= boxcox(df['Visibility(mi)'].apply(lambda x: x+1),lmbda = 0.1)\ndf['Wind_Speed_bc']= boxcox(df['Wind_Speed(mph)'].apply(lambda x: x+1),lmbda=-0.2)\ndf = df.drop(['Pressure(in)','Visibility(mi)','Wind_Speed(mph)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change Name Back\ndf['Pressure(in)'] = df['Pressure_bc']\ndf['Visibility(mi)'] = df['Visibility_bc']\ndf['Wind_Speed(mph)'] = df['Wind_Speed_bc']\ndf = df.drop(['Pressure_bc','Visibility_bc','Wind_Speed_bc'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Year = df.Year.astype(str)\nsns.countplot(x='Year', hue='Severity', data=df ,palette=\"Set2\")\nplt.title('Count of Accidents by Year', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"3.1 Encoding Features\n\n3.1.1 Address features\n\n- Frequency Encoding\n\nSome location features that have too many unique values can be labeled by their frequency. Frequency encoding and log-transform:\n\n'Street'\n\n'City'\n\n'County'\n \n'Zipcode'\n\n'Airport_Code'"},{"metadata":{"trusted":true},"cell_type":"code","source":"fre_list = ['Street', 'City', 'County', 'Zipcode', 'Airport_Code']\nfor i in fre_list:\n  newname = i + '_Freq'\n  df[newname] = df.groupby([i])[i].transform('count')\n  df[newname] = df[newname]/df.shape[0]*df[i].unique().size\n  df[newname] = df[newname].apply(lambda x: np.log(x+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(fre_list, axis  = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.2 POI features"},{"metadata":{"trusted":true},"cell_type":"code","source":"POI_features = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal']\n\nfig, axs = plt.subplots(ncols=3, nrows=4, figsize=(15, 10))\n\nplt.subplots_adjust(hspace=0.5,wspace = 0.5)\nfor i, feature in enumerate(POI_features, 1):    \n    plt.subplot(3, 4, i)\n    sns.countplot(x=feature, hue='Severity', data=df, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['1','2','3','4'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in {}'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents in POI Features',y=1.02, fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accidents near traffic signal and crossing are much less likely to be serious accidents while little more likely to be serious if they are near the junction. Maybe it is because people usually slow down in front of crossing and traffic signal but junction and severity are highly related to speed. Other POI features are so unbalanced that it is hard to tell their relation with severity from plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.drop(['Bump','Give_Way','No_Exit','Roundabout','Traffic_Calming'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.2 Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop features with duplicated meanings and high correlation\ndf = df.drop(['Civil_Twilight', 'Nautical_Twilight', \n              'Astronomical_Twilight'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Severity'] = df['Severity'].astype(int)\nplt.figure(figsize=(20,20))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(df.corr(), annot=True,cmap=cmap, center=0).set_title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop high corrolated features\ndf = df.drop(['County_Freq','Zipcode_Freq',\n              'Airport_Code_Freq','Year'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace([True, False], [1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('/kaggle/working/df_lessF.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if it contains Null value again.\nmissingdf = df.isnull().sum(axis=0).reset_index()\nmissingdf.columns = ['columns_name','missing_count']\nmissingdf['missing_ratio'] = missingdf['missing_count'] /df.shape[0]\nmissingdf.loc[missingdf['missing_ratio']>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.3 Data Resampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Severity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_bl = df.sample(frac=0.05) \n#not as good as resampling. Directly resampling!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl = pd.concat([df[df['Severity']==2].sample(250000, random_state=42),\n                   df[df['Severity']==3].sample(25000, random_state=42),\n                   df[df['Severity']==4].sample(500000, replace = True, random_state=42),\n                   df[df['Severity']==1].sample(7500, replace = True, random_state=42)], axis=0)\n\ndf_bl[\"Severity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl = df_bl.drop(['Street_Freq','City_Freq'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.4 One-hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate dummies for categorical data\ncat = ['Side','State','Timezone','Wind_Direction', 'Weekday', 'Month', 'Hour','Sunrise_Sunset']\ndf_bl[cat] = df_bl[cat].astype('category')\ndf_bl = pd.get_dummies(df_bl, columns=cat, drop_first=True)\ndf_bl.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n#Import Others\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of classification algorithms\nalgo_lst=['Logistic Regression','Decision Trees','Random Forest', \n          'Support Vector Machine', 'Neural network', 'XGBoost' ]\n\n# Initialize an empty list for the accuracy score for each algorithm\naccuracy_lst=[]\n\n# Initialize an empty list for the f1 score for each algorithm\nf1_lst=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_bl.drop('Severity',axis=1)\ny = df_bl['Severity']\n\n\n# split train test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\\\n              X, y, test_size=0.20, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nimport time\ntic2 = time.time()\nlr = LogisticRegression(max_iter=1000,random_state=42)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Get the f1 score\nf1_lr=f1_score(y_test, y_pred, average='weighted')\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\nf1_lst.append(f1_lr)\n\n# Display results\nprint(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(\"[Logistic regression algorithm] f1_score: {:.3f}.\".format(f1_lr))\ntoc2 = time.time()\nprint('Elapsed time for Losigtic regression is %f seconds \\n' % float(toc2 - tic2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------Decision tree algorithm\n# 1. Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(criterion='entropy')\n\n# 1. Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# 1. Use dt_entropy to predict test set labels\ny_pred= dt_entropy.predict(X_test)\n\n# 1. Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n# 1. Get the f1 score\nf1_dt_entropy=f1_score(y_test, y_pred, average='weighted')\n\n# Append to the accuracy list\n#accuracy_lst.append(acc)\n#f1_lst.append(f1_lr)\n\n# 1. Print accuracy_entropy\nprint('[Decision Tree -- entropy] accuracy_score: {:.3f}.'.format(accuracy_entropy))\nprint(\"[Decision Tree -- entropy] f1_score: {:.3f}.\".format(f1_dt_entropy))\n\ntic3 = time.time()\n\n# 2. Instantiate dt_gini, set 'gini' as the information criterion\ndt_gini = DecisionTreeClassifier(criterion='gini')\n\n# 2. Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)\n\n# 2. Use dt_entropy to predict test set labels\ny_pred= dt_gini.predict(X_test)\n\n# 2. Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_test, y_pred)\n\n# 2. Get the f1 score\nf1_dt_gini=f1_score(y_test, y_pred, average='weighted')\n\n# 2. Append to the accuracy list\nacc=accuracy_gini\naccuracy_lst.append(acc)\nf1_lst.append(f1_dt_gini)\n\n# 2. Print accuracy_gini\nprint('[Decision Tree -- gini] accuracy_score: {:.3f}.'.format(accuracy_gini))\nprint(\"[Decision Tree -- gini] f1_score: {:.3f}.\".format(f1_dt_gini))\n\ntoc3 = time.time()\nprint('Elapsed time for Decision tree is %f seconds \\n' % float(toc3 - tic3))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------Random Forest algorithm\ntic4 = time.time()\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=150)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Get the f1 score\nf1_clf=f1_score(y_test, y_pred, average='weighted')\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\nf1_lst.append(f1_clf)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Randon forest algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(\"[Randon forest algorithm] f1_score: {:.3f}.\".format(f1_clf))\ntoc4 = time.time()\nprint('Elapsed time for Randon forest is %f seconds \\n' % float(toc4 - tic4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = '80wRF.sav'\n#pickle.dump(clf, open(filename, 'wb'))\npickle.dump(clf, open('80wRF.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#Algorithm Random Forest\n#Visualize important features\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 350)\npd.set_option('display.max_columns', 350)\nplt.style.use('ggplot')\n\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n\n# Creating a bar plot, displaying only the top k features\nk=20\nsns.barplot(x=feature_imp[:20], y=feature_imp.index[:k])\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List top k important features\nk=20\nfeature_imp.sort_values(ascending=False)[:k]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Algorithm Random Forest\n#Select the top important features, set the threshold\n# Create a selector object that will use the random forest classifier to identify\n# features that have an importance of more than 0.03\nsfm = SelectFromModel(clf, threshold=0.03)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\nfeat_labels=X.columns\n\n# Print the names of the most important features\nfor feature_list_index in sfm.get_support(indices=True):\n    print(feat_labels[feature_list_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the data to create a new dataset containing only the most important features\n# Note: We have to apply the transform to both the training X and test X data.\nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n\n# Create a new random forest classifier for the most important features\nclf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\n# Train the new classifier on the new dataset containing the most important features\nclf_important.fit(X_important_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply The Full Featured Classifier To The Test Data\ny_pred = clf.predict(X_test)\n\n# View The Accuracy Of Our Full Feature Model\nprint('[Randon forest algorithm -- Full feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))\n\n# Apply The Full Featured Classifier To The Test Data\ny_important_pred = clf_important.predict(X_important_test)\n\n# View The Accuracy Of Our Limited Feature Model\nprint('[Randon forest algorithm -- Limited feature] accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_important_pred)))\n\n#View The F1 Score of Our Full Feature Model\nprint('[Randon forest algorithm -- Full feature] f1_score: {:.3f}.'.format(f1_score(y_test, y_pred, average='weighted')))\n\n# View The F1 Score Of Our Limited Feature Model\nprint('[Randon forest algorithm -- Limited feature] f1_score: {:.3f}.'.format(f1_score(y_test, y_important_pred, average='weighted')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'RF_limit.sav'\n#pickle.dump(clf, open(filename, 'wb'))\npickle.dump(clf_important, open('RF_limit.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------- Support vector machine algorithm\nfrom sklearn import svm\ntic5 = time.time()\n\nsvm = svm.SVC(gamma='auto')\nsvm.fit(X_train, y_train)\ny_pred = svm.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Get f1-score\nf1_svm=f1_score(y_test, y_pred, average='weighted')\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\nf1_lst.append(f1_svm)\n\nprint(\"[Support vector machine algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(\"[Support vector machine algorithm] f1_score: {:.3f}.\".format(f1_svm))\ntoc5 = time.time()\nprint('Elapsed time for Support vector machine is %f seconds \\n' % float(toc5 - tic5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------- Neural network algorithm\nfrom sklearn import neural_network\nimport time\n\ntic6 = time.time()\nnn = neural_network.MLPClassifier(hidden_layer_sizes=(5, 2))\nnn.fit(X_train,y_train)\ny_pred=nn.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# f1_score\nf1_nn=f1_score(y_test, y_pred, average='weighted')\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\nf1_lst.append(f1_nn)\n\nprint(\"[Neural network algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(\"[Neural network algorithm] f1_score: {:.3f}.\".format(f1_nn))\ntoc6 = time.time()\nprint('Elapsed time for Neural network is %f seconds \\n' % float(toc6 - tic6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#--------------- XGBoost algorithm\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\ntic7 = time.time()\nxgb = XGBClassifier(objective= 'multi:softmax',num_class=4,n_fold=4,\n                    colsample_bytree = 1,\n                    learning_rate = 0.15,\n                    max_depth = 5,\n                    n_estimators = 600,\n                    subsample = 0.3)\nxgb.fit(X_train, y_train)\n\ny_pred=xgb.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Get f1 score\nf1_xgb=f1_score(y_test, y_pred, average='weighted')\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\nf1_lst.append(f1_xgb)\n\n\nprint(\"[XGBoost algorithm] accuracy_score: {:.3f}.\".format(acc))\nprint(\"[XGBoost algorithm] f1_score: {:.3f}.\".format(f1_xgb))\ntoc7 = time.time()\nprint('Elapsed time for Losigtic regression is %f seconds \\n' % float(toc7 - tic7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a plot of the accuracy scores for different algorithms\n\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)\n\n# Export to a file\ndf_acc.to_csv('./Accuracy_scores_algorithms.csv',index=False)\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title('Which algorithm is better regarding accuracy score?')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a plot of the f1 scores for different algorithms\n\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, f1_lst)), columns=['Algorithm','F1_Score']).sort_values(by=['F1_Score'],ascending = True)\n\n# Export to a file\ndf_acc.to_csv('./f1_scores_algorithms.csv',index=False)\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'F1_Score', align='center',legend=False,color='0.5')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('F1 Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title('Which algorithm is better regarding f1 score?')\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}