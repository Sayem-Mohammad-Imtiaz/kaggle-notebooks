{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"27c81598-6445-1027-6572-26c075ec37ec"},"source":"Author: Heisengarg   \nDate: 9 Januray 2017"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"83278ec1-7fbe-dac8-dbc5-4bff69b0d641","collapsed":true},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"223e8eb6-8cff-2b9f-2f63-f86ed05344be","collapsed":true},"outputs":[],"source":"df=pd.read_csv(\"voice.csv\")\n#Source: https://www.kaggle.com/primaryobjects/voicegender"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0c0fcee-2713-f770-4632-adeca77fa14f"},"outputs":[],"source":"df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ef7ededf-5c10-af74-617a-c424a5a88107"},"source":"### Comparing Random Forest and simple Decision Tree Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4382fb1e-95e8-252d-8e30-a3cb58a6a554"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nX=df.ix[:,df.columns!='label']\nY=df.label\n\nclf_dt = DecisionTreeClassifier()\nclf_dt = clf_dt.fit(X,Y)\n\nclf_rft = RandomForestClassifier()\nclf_rft = clf_rft.fit(X,Y)\n\nclf_et=tree.ExtraTreeClassifier()\nclf_et.fit(X,Y)\n\nscore_dt=[]\nscore_rft=[]\nscore_et=[]\nstart=3\nend=10\n\nfor i in range(start,end+1):\n    \n    score_dt.append(cross_val_score(clf_dt, X, Y,cv=i).mean())\n    score_rft.append(cross_val_score(clf_rft,X,Y,cv=i).mean())\n    score_et.append(cross_val_score(clf_et,X,Y,cv=i).mean())\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59440781-b537-bf0f-1513-14a4f19b8ac6"},"outputs":[],"source":"p1=plt.plot(range(start,end+1),score_dt,'r',label='Decision Tree Classifier')\np2=plt.plot(range(start,end+1),score_rft,'b',label='Random Forest Classifier')\np3=plt.plot(range(start,end+1),score_et,'g',label='Extra Trees Classifier')\nplt.legend(loc=4)\nplt.ylabel('Mean Accuracy (%)')\nplt.xlabel('Number of folds for Cross Validation')\nplt.ylim((0.89,0.97))"},{"cell_type":"markdown","metadata":{"_cell_guid":"eb752f07-d429-4641-e879-541041c4a0f7"},"source":"### Visualizing a Tree (Stuck with bad font)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7cae89a-c088-8db2-6871-735435e420e3"},"outputs":[],"source":"from sklearn import tree\ntree.export_graphviz(clf_dt, out_file='tree.dot') #clf_dt has 10 folds"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91b8a872-e374-504c-b4aa-ef1125164d80"},"outputs":[],"source":"import pydotplus\nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image\n\nimport gi\n#gi.require_version('Gtk', '3.0')\n#from gi.repository import Gtk, Pango\n#import pango\n\ndotfile = StringIO()\ntree.export_graphviz(clf_dt, out_file=dotfile)\ngraph = pydotplus.graph_from_dot_data(dotfile.getvalue())\nImage(graph.create_png())"},{"cell_type":"markdown","metadata":{"_cell_guid":"cc0083af-74cb-925a-109c-09ef4b31d371"},"source":"### Feature Selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1100b2fa-5ff3-4dfa-e791-08a6102360c4"},"outputs":[],"source":"from sklearn.feature_selection import SelectFromModel\n\n#clf=RandomForestClassifier()\n#clf.fit(X,Y)\n\n#Random Forest Classifier\n\nfi_rft=clf_rft.feature_importances_\nfi_dt=clf_dt.feature_importances_\nfi_et=clf_et.feature_importances_\n\nfig=plt.figure(figsize=(18,5))\nax=plt.subplot(111)\n\nwi=0.3\nw=0.3\n\nax.bar(np.arange(1,21),fi_dt,width=wi,align='center',color='r',alpha=0.5)\nax.bar(np.arange(1,21)+w,height=fi_rft,width=wi,align='center',color='b',alpha=0.2)\nax.bar(np.arange(1,21)+2*w,height=fi_rft,width=wi,align='center',color='g',alpha=0.3)\n\nax.legend(loc=4)\n\nax.set_xticks(np.arange(1,21)+wi) #set position of xlabels to match with on the plot\nax.set_xticklabels(np.delete(df.columns,20).values) #now rename those labels\n\nax.set_ylabel('Feature Importance')\nax.set_xlabel('Features')\n\nax.legend(('Decision Tree','Random Forest','Extra Tree'))\nfig.suptitle('Comparison of feature importance based on classifier used',fontsize=20)\n#ax.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4cf111ef-c4a5-78fe-a86d-863aea9bcc52"},"source":"### Using the above to improve classifiers"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d36bb50-7cbf-ba5b-d196-83b44f9d9f6a"},"outputs":[],"source":"# We only choose the IQR,sd, Q25 and meanfun as the features for our classifications\n\n#Decision Tree (IQR and meanfun)\n\nX_dt=df[['IQR','meanfun']]\n\nclf_dt=clf_dt.fit(X_dt,Y)\n#score_dt=cross_val_score(clf_dt,X_dt,Y)\n\n#Random Forest and Extra Tree (meanfun,sd,Q25)\n\nX_new=df[['meanfun','sd','Q25','IQR','sp.ent']]\n\nclf_et=clf_et.fit(X_new,Y)\n#score_et=cross_val_score(clf_et,X_new,Y)\n\nclf_rft=clf_dt.fit(X_new,Y)\n#score_rft=cross_val_score(clf_rft,X_new,Y)\n\nscore_dt=[]\nscore_rft=[]\nscore_et=[]\nstart=3\nend=10\n\n\nfor i in range(3,11):\n    \n\n    score_dt.append(cross_val_score(clf_dt, X_dt, Y,cv=i).mean())\n    score_rft.append(cross_val_score(clf_rft,X_new,Y,cv=i).mean())\n    score_et.append(cross_val_score(clf_et,X_new,Y,cv=i).mean())\n    \np1=plt.plot(range(start,end+1),score_dt,'r',label='Decision Tree Classifier')\np2=plt.plot(range(start,end+1),score_rft,'b',label='Random Forest Classifier')\np3=plt.plot(range(start,end+1),score_et,'g',label='Extra Trees Classifier')\nplt.legend(loc=4)\nplt.ylabel('Mean Accuracy (%)')\nplt.ylim((0.89,0.97))\nplt.xlabel('Number of folds for Cross Validation')"},{"cell_type":"markdown","metadata":{"_cell_guid":"d5d42ff6-b272-76d8-190e-7e4c6bf0ff0b"},"source":"#### Inferences <br>\nImproved only the Extremely Random Forest Classifier performace by feature selection.  <br> This method basically reducing the performace of Random Forests which takes into account smaller subsets of features and then builds a tree at each node.    <br>No significant change in the performance of simple decision tree classifier."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}