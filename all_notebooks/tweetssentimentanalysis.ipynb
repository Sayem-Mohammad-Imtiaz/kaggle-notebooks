{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:11.86239Z","iopub.execute_input":"2021-06-21T20:00:11.862712Z","iopub.status.idle":"2021-06-21T20:00:11.871076Z","shell.execute_reply.started":"2021-06-21T20:00:11.862682Z","shell.execute_reply":"2021-06-21T20:00:11.869953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport torch\nimport torch.nn as nn\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:11.875807Z","iopub.execute_input":"2021-06-21T20:00:11.876077Z","iopub.status.idle":"2021-06-21T20:00:11.883082Z","shell.execute_reply.started":"2021-06-21T20:00:11.876052Z","shell.execute_reply":"2021-06-21T20:00:11.882206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu' )\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:11.887248Z","iopub.execute_input":"2021-06-21T20:00:11.887506Z","iopub.status.idle":"2021-06-21T20:00:11.895009Z","shell.execute_reply.started":"2021-06-21T20:00:11.887476Z","shell.execute_reply":"2021-06-21T20:00:11.894122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets=pd.read_csv('/kaggle/input/tweetsentimentanalysis/Tweets.csv')\ntweets.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:11.897781Z","iopub.execute_input":"2021-06-21T20:00:11.898398Z","iopub.status.idle":"2021-06-21T20:00:11.973062Z","shell.execute_reply.started":"2021-06-21T20:00:11.898358Z","shell.execute_reply":"2021-06-21T20:00:11.972109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:11.974659Z","iopub.execute_input":"2021-06-21T20:00:11.975019Z","iopub.status.idle":"2021-06-21T20:00:11.997828Z","shell.execute_reply.started":"2021-06-21T20:00:11.974983Z","shell.execute_reply":"2021-06-21T20:00:11.996728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(tweets['airline_sentiment'])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:11.999959Z","iopub.execute_input":"2021-06-21T20:00:12.000437Z","iopub.status.idle":"2021-06-21T20:00:12.110727Z","shell.execute_reply.started":"2021-06-21T20:00:12.000403Z","shell.execute_reply":"2021-06-21T20:00:12.109826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=tweets['text'].values\nlabels=tweets['airline_sentiment'].values\nle=LabelEncoder()\nlabel=le.fit_transform(labels)\nx_train,x_test,y_train,y_test=train_test_split(data,label,test_size=0.10,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:12.112237Z","iopub.execute_input":"2021-06-21T20:00:12.112565Z","iopub.status.idle":"2021-06-21T20:00:12.12496Z","shell.execute_reply.started":"2021-06-21T20:00:12.112531Z","shell.execute_reply":"2021-06-21T20:00:12.124219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TEXT PREPROCESSING","metadata":{}},{"cell_type":"code","source":"import re\ndef preprocessing(s):\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n    \n    return s","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:12.126223Z","iopub.execute_input":"2021-06-21T20:00:12.126573Z","iopub.status.idle":"2021-06-21T20:00:12.133369Z","shell.execute_reply.started":"2021-06-21T20:00:12.126536Z","shell.execute_reply":"2021-06-21T20:00:12.13254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\ndef tokenizer(x_train,x_test):\n    word_list=[]\n    stop_word=set(stopwords.words('english'))\n    for sent in x_train:\n        sent=sent.lower().split()[1:]\n        for word in sent:\n            word=preprocessing(word)\n            if word not in stop_word and word != '' : \n                    word_list.append(word)\n                \n                \n    corpus=Counter(word_list)     \n    corpus_=sorted(corpus,key=corpus.get,reverse=True)[:1500]\n    one_hot_dict={w:i+1 for i,w in enumerate(corpus_)}\n    \n    train_list=[]\n    test_list=[] \n    for  sent in  x_train:   \n         train_list.append( [one_hot_dict[preprocessing(word)] for word in sent.lower().split()[1:]\n                           if preprocessing(word) in one_hot_dict.keys()])\n        \n    for sent in x_test :\n        test_list.append( [one_hot_dict[preprocessing(word)]  for word in sent.lower().split()[1:]\n                            if preprocessing(word) in one_hot_dict.keys()])\n    \n    \n    \n    return  train_list,test_list,one_hot_dict\n        \nx_tr,x_ts,vocab=tokenizer(x_train,x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:12.134524Z","iopub.execute_input":"2021-06-21T20:00:12.134916Z","iopub.status.idle":"2021-06-21T20:00:14.765764Z","shell.execute_reply.started":"2021-06-21T20:00:12.13486Z","shell.execute_reply":"2021-06-21T20:00:14.764932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Determine max length from expressions\na=[len(x) for x in x_tr]\nmax_len_index=a.index(max(a))\nlen(x_tr[max_len_index])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:14.766931Z","iopub.execute_input":"2021-06-21T20:00:14.767287Z","iopub.status.idle":"2021-06-21T20:00:14.777308Z","shell.execute_reply.started":"2021-06-21T20:00:14.767252Z","shell.execute_reply":"2021-06-21T20:00:14.774057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#padding used to same size tensors\npad=25\ndef padding(data,padd):\n    for i,sent in enumerate(data):\n        feature=np.zeros((1,padd),dtype=int) \n        if len(np.array(sent)) != 0:\n            feature[:,-len(sent):]=np.array(sent)\n            data[i]=feature\n        else :\n            data[i]=feature\n    return data\n\nX_train=padding(x_tr,pad) # the longest expression length is 18, we set the size to 25\nX_test=padding(x_ts,pad)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:14.780032Z","iopub.execute_input":"2021-06-21T20:00:14.78042Z","iopub.status.idle":"2021-06-21T20:00:14.975383Z","shell.execute_reply.started":"2021-06-21T20:00:14.780384Z","shell.execute_reply":"2021-06-21T20:00:14.974502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class sentimentLSTM(nn.Module):\n    def __init__(self,num_layers,batch_size,hidden_size,vocab_size,embed_size,p,pad):\n        super(sentimentLSTM,self).__init__()\n        \n        self.hidden=hidden_size\n        self.embed=nn.Embedding(vocab_size,embed_size)\n        self.embed_size=embed_size\n        self.num_layers=num_layers\n        self.p=p\n        self.pad=pad\n        self.lstm=nn.LSTM(input_size=self.embed_size,\n                         hidden_size=self.hidden,\n                         num_layers=self.num_layers,\n                         batch_first=True)\n        \n\n        \n        \n        self.linear=nn.Linear(self.hidden,3)\n        self.drop=nn.Dropout(self.p)\n        self.soft=nn.Softmax()\n        self.relu=nn.ReLU()\n    \n\n    \n    def forward(self,x,hidden):\n        \n        batch=x.shape[0]\n        x=x.view(batch,-1)\n        x=self.embed(x)\n        x,hidden=self.lstm(x)\n        x=x.contiguous().view(-1, self.hidden)\n        x=self.drop(x)\n        x=self.relu(x)\n        x=self.linear(x)  \n        \n        out=x.view(batch,self.pad, -1)   \n        return out[:,-1] ,h\n\n    def init_hidden(self,b):\n\n        h0 = torch.zeros((self.num_layers,batch_size,self.hidden)).to(device)\n        c0 = torch.zeros((self.num_layers,batch_size,self.hidden)).to(device)\n        hidden = (h0,c0)\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:14.976934Z","iopub.execute_input":"2021-06-21T20:00:14.977298Z","iopub.status.idle":"2021-06-21T20:00:14.988973Z","shell.execute_reply.started":"2021-06-21T20:00:14.977263Z","shell.execute_reply":"2021-06-21T20:00:14.988257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=20\nlr=0.001\nhidden_size=512\nvocab_size=len(vocab)+1\nembed_size=300\nnum_layers=1\nbatch_size=128\np=0.5\nmodel=sentimentLSTM(num_layers,batch_size,hidden_size,vocab_size,embed_size,p,pad).to(device)\ncriterion=nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n\ntrain_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\ntest_data = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n\ntrain_loader=DataLoader(dataset=train_data,\n                        shuffle=True,\n                        batch_size=batch_size)\n\ntest_loader=DataLoader(dataset=test_data,\n                        shuffle=True,\n                        batch_size=batch_size)\n\nsoft=nn.Softmax(dim=1) #using for normalization 0-1 \n\ntrain_losses=[]\ntest_losses=[]\n\nfor epoch in range(epochs):\n    train_loss=0\n    train_pred=[]\n    h = model.init_hidden(batch_size)\n    labels=[]\n    model.train()\n    for e,(inputs,label) in enumerate(train_loader):\n        \n        inputs=inputs.type(torch.LongTensor).to(device)\n        label=label.type(torch.LongTensor).to(device)\n        \n        h = tuple([each.data for each in h])\n        pred,h=model(inputs,h)\n\n        loss=criterion(pred,label)\n        train_loss+=loss.item()\n        \n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        labels.append(np.array(label.cpu()))\n        train_pred.append(np.array([torch.argmax(i).item() for i in soft(pred)]))\n        \n    train_losses.append(train_loss/e)    \n    acc_train=accuracy_score(np.concatenate(labels),np.concatenate(train_pred))    \n\n    model.eval() \n    test_loss=0\n    test_preds=[]\n    test_labels=[]\n    \n    for t,(input_test,label_test) in enumerate(test_loader):\n        \n        input_test=input_test.type(torch.LongTensor).to(device)\n        label_test=label_test.type(torch.LongTensor).to(device)\n        \n        h = tuple([each.data for each in h])\n        pred_test,h=model(input_test,h)\n\n        loss_test=criterion(pred_test,label_test)\n        test_loss+=loss_test.item()\n        \n        \n        test_labels.append(np.array(label_test.cpu()))\n        test_preds.append(np.array([torch.argmax(i).item() for i in soft(pred_test)]))\n        \n    \n    test_losses.append(test_loss/t)\n    acc_test=accuracy_score(np.concatenate(test_labels),np.concatenate(test_preds))\n    print('epochs {} Train_accurary {:.3f},train error {:.3f} , Test_accuracy {:.3f} Test_error {:.3f}'.format(epoch,acc_train,(train_loss/e),acc_test,(test_loss/t)))\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:48.093156Z","iopub.execute_input":"2021-06-21T20:00:48.093487Z","iopub.status.idle":"2021-06-21T20:01:19.884584Z","shell.execute_reply.started":"2021-06-21T20:00:48.093457Z","shell.execute_reply":"2021-06-21T20:01:19.883677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOT: model has overfit to be fixed :)","metadata":{}},{"cell_type":"code","source":"## Plot the graph here\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.plot(train_losses ,label='Train Loss')\nplt.plot(test_losses,label='Test Loss')\nplt.legend(frameon=True);","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:01:24.711797Z","iopub.execute_input":"2021-06-21T20:01:24.712144Z","iopub.status.idle":"2021-06-21T20:01:24.864181Z","shell.execute_reply.started":"2021-06-21T20:01:24.712112Z","shell.execute_reply":"2021-06-21T20:01:24.863246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#we enter the model with an expression and observe the result\ntext='everything was good'\n\n#text preprocessing \n\nword_seq = np.array([vocab[preprocessing(word)] for word in text.lower().split() \n                 if preprocessing(word)  in vocab.keys()])\nfeature=np.zeros((1,25),dtype=int)\nfeature[:,-len(word_seq):]=np.array(word_seq)\ninputs =  torch.from_numpy(feature).type(torch.LongTensor).to(device)\n\nmodel.eval()\nwith torch.no_grad():\n    h = model.init_hidden(batch_size)\n    h = tuple([each.data for each in h])\n    output, h = model(inputs, h)\n\nprint(le.inverse_transform([torch.argmax(soft(output)).item()])[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:01:27.014225Z","iopub.execute_input":"2021-06-21T20:01:27.014542Z","iopub.status.idle":"2021-06-21T20:01:27.025351Z","shell.execute_reply.started":"2021-06-21T20:01:27.014513Z","shell.execute_reply":"2021-06-21T20:01:27.02433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# because of overfiting we need to parametre tuning\n\nfrom itertools import product\n\np=[ 0.2, 0.3, 0.4, 0.6, 0.7, 0.8]\nwd=[0.01,0.001,0.0001]\nlr=[0.01,0.001,0.0001]\n#hidden_size=[32,64,128,512,1024]\nembed_size=[32,64,128,256]\nbatch_size=[32,64,128,256]\nparameters=pd.DataFrame(\n    list(product(p,wd,lr, embed_size,batch_size)),\n    columns=['p','wd','lr', 'embed','batch']\n)\n\nparameters","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:47.002623Z","iopub.status.idle":"2021-06-21T20:00:47.003174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# code using for  model parameters tuningtrain=[]\n\ntest=[]\nepochs_tr={}\nepochs_ts={}\nfor  index,i in enumerate(np.array(parameters)):\n\n    p=  float(i[0])\n    wd= float(i[1])\n    lr= float(i[2])\n    embed_size=int(i[3])\n    batch_size=int(i[4])\n             \n    train_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n    test_data = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n\n    train_loader=DataLoader(dataset=train_data,\n                        shuffle=True,\n                        batch_size=batch_size)\n    test_loader=DataLoader(dataset=test_data,\n                        shuffle=True,\n                        batch_size=batch_size)\n\n    param_acc={}\n    hidden_size=512\n    vocab_size=len(vocab)+1\n    num_layers=1\n    model=sentimentLSTM(num_layers,batch_size,hidden_size,vocab_size,embed_size,p).to(device)\n    criterion=nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n\n\n    \n    soft=nn.Softmax(dim=1)\n\n    train_l=[]\n    test_l=[]\n\n    for epoch in range(20):\n        train_loss=0\n        train_pred=[]\n        h = model.init_hidden(batch_size)\n        labels=[]\n        model.train()\n        for e,(inputs,label) in enumerate(train_loader):\n\n            inputs=inputs.type(torch.LongTensor).to(device)\n            label=label.type(torch.LongTensor).to(device)\n\n            h = tuple([each.data for each in h])\n            pred,h=model(inputs,h)\n\n            loss=criterion(pred,label)\n            train_loss+=loss.item()\n\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            labels.append(np.array(label.cpu()))\n            train_pred.append(np.array([torch.argmax(i).item() for i in soft(pred)]))\n\n        train_l.append(train_loss/e)\n        \n        acc_train=accuracy_score(np.concatenate(labels),np.concatenate(train_pred))\n\n      #  print('epochs {} accurary {:.3f},tset error {:.5f}'.format(i,acc_train,(train_loss/e)))\n\n\n        model.eval() \n        test_loss=0\n        test_preds=[]\n        test_labels=[]\n\n        for t,(input_test,label_test) in enumerate(test_loader):\n\n            input_test=input_test.type(torch.LongTensor).to(device)\n            label_test=label_test.type(torch.LongTensor).to(device)\n\n            h = tuple([each.data for each in h])\n            pred_test,h=model(input_test,h)\n\n            loss_test=criterion(pred_test,label_test)\n            test_loss+=loss_test.item()\n\n\n            test_labels.append(np.array(label_test.cpu()))\n            test_preds.append(np.array([torch.argmax(i).item() for i in soft(pred_test)]))\n\n\n        test_l.append(test_loss/t)\n        acc_test=accuracy_score(np.concatenate(test_labels),np.concatenate(test_preds))\n     #   print('epochs {} Train_accurary {:.3f},train error {:.5f} , Test_accuracy {:.3f} Test_error {:.5f}'.format(epoch,acc_train,(train_loss/e),acc_test,(test_loss/t)))\n        train.append({index: {'epoch'+str(epoch):acc_train}})\n        test.append({index:  {'epoch'+str(epoch):acc_test}})\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:00:47.004276Z","iopub.status.idle":"2021-06-21T20:00:47.004896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}