{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bag of Words using Random Forest"},{"metadata":{},"cell_type":"markdown","source":"### In this notebook we will learn how we can train a Random Forest using Bag of words approach."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://3.bp.blogspot.com/-4pxORQAgAFI/XMNZhEssXtI/AAAAAAAAGmA/SuQGsp-GyT4jKlUZieg_A5lnTza_GujfwCLcBGAs/s1600/bag_of_words.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThe bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification.\n\n**According to [Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model#:~:text=The%20bag%2Dof%2Dwords%20model,word%20order%20but%20keeping%20multiplicity.):** The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.\n\nIn this tutorial, you will discover the bag-of-words for training the Random Forest model to predict the sentiment of a sentence."},{"metadata":{},"cell_type":"markdown","source":"##  Importing Libraries"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: justify;\"><div style=\"font-size: 16px;\">For understanding the concept of bag-of-word, let us setup the environment and import the necessary libraries such as:</div><br>\n1. <b>pandas: </b> for reading and understanding the data,<br>\n2. <b>numpy: </b> for doing numerical computations on the data,<br>\n3. <b>BeautifulSoup: </b> for pulling data out of HTML and XML files and remove the unnessary tags and helps in navigating, searching, and modifying the parse tree data,<br>\n4. <b>re: </b> is the library for regular expression and we are going to use it to clean out data and based on pattern matching using regular expressions,<br>\n5. <b>nltk: </b> is a natural language toolkit library used to do text processing for classification, tokenization, stemming, tagging, parsing, semantic reasoning, etc,<br>\n6. <b>sklearn: </b> is used for all mahine learning tasks such as here we are using it to training a Random Forest model and predicting it's performance.</div>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd     \nimport numpy as np\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\n# nltk.download()\nfrom nltk.corpus import stopwords # Import the stop word list\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the data (Training & Testing data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv\", \n                              header=0, \n                              delimiter=\"\\t\", \n                              quoting=3)\n\ndf_test = pd.read_csv(\"/kaggle/input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv\",\n                             header=0, \n                             delimiter=\"\\t\", \n                             quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.columns.values)\nprint(df_test.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['review'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PreProcessing data for one item. \n###### beautifying the text of HTML and XML data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbs_data = BeautifulSoup(df_train[\"review\"][0])\nprint(bs_data.get_text())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"letters_only = re.sub(\"[^a-zA-Z]\", \" \", bs_data.get_text() )\nprint(letters_only)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_case = letters_only.lower()  \nwords = lower_case.split()  \nprint(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stopwords.words(\"english\") )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = [w for w in words if not w in stopwords.words(\"english\")]\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  PreProcessing data for all of the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_size = df_train[\"review\"].size\ntesting_data_size = df_test[\"review\"].size\n\nprint(training_data_size)\nprint(testing_data_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text_data(data_point, data_size):\n    review_soup = BeautifulSoup(data_point)\n    review_text = review_soup.get_text()\n    review_letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n    review_lower_case = review_letters_only.lower()  \n    review_words = review_lower_case.split() \n    stop_words = stopwords.words(\"english\")\n    meaningful_words = [x for x in review_words if x not in stop_words]\n    \n    if( (i)%2000 == 0 ):\n        print(\"Cleaned %d of %d data (%d %%).\" % ( i, data_size, ((i)/data_size)*100))\n        \n    return( \" \".join( meaningful_words)) \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean_train_data_list = []\n# clean_test_data_list = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### cleaning training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(training_data_size):\n    df_train[\"review\"][i] = clean_text_data(df_train[\"review\"][i], training_data_size)\nprint(\"Cleaning training completed!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### cleaning testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(testing_data_size):\n    df_test[\"review\"][i] = clean_text_data(df_test[\"review\"][i], testing_data_size)\nprint(\"Cleaning validation completed!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the features ready to be trained "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_cv, Y_train, Y_cv = train_test_split(df_train[\"review\"], df_train[\"sentiment\"], test_size = 0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting the train, validation and test data to vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = vectorizer.fit_transform(X_train)\nX_train = X_train.toarray()\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cv = vectorizer.transform(X_cv)\nX_cv = X_cv.toarray()\nprint(X_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = vectorizer.transform(df_test[\"review\"])\nX_test = X_test.toarray()\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = vectorizer.get_feature_names()\nprint(f\"Printing first 100 vocabulary samples:\\n{vocab[:100]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution = np.sum(X_train, axis=0)\n\nprint(\"Printing first 100 vocab-dist pairs:\")\n\nfor tag, count in zip(vocab[:100], distribution[:100]):\n    print(count, tag)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Random Forest model"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestClassifier() \nforest = forest.fit( X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = forest.predict(X_cv) \nprint(\"Accuracy: \", accuracy_score(Y_cv, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the output submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = forest.predict(X_test) \noutput = pd.DataFrame( data={\"id\":df_test[\"id\"], \"sentiment\":result} )\noutput.to_csv( \"submission.csv\", index=False, quoting=3 )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That marks the end of this notebook, hope it was worth reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}