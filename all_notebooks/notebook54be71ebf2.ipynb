{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reading the data set from csv files\ncolumn_names = pd.read_csv('/kaggle/input/boardgamegeek-reviews/bgg-15m-reviews.csv',header = None, nrows=1)\ndataframe = pd.read_csv('/kaggle/input/boardgamegeek-reviews/bgg-15m-reviews.csv', header = None, skiprows = 1)\ncolumn_list = np.concatenate(column_names.values, axis=0).tolist()\ncolumn_list[0] = 'index'\ndataframe.columns = column_list\ndel column_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('*'*30,'Table Data','*'*30)\nprint(dataframe.head())\nprint('*'*30,'Table Data summary','*'*30)\nprint(dataframe.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the rows with empty rating\ndataframe.dropna(subset=['rating'])\n\n# droping columns that are not required\ndataframe = dataframe[['rating', 'comment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport nltk\n# download wordnet if required\n# nltk.download('wordnet')\n# download stopwords if required\n# nltk.download('stopwords')\n# loading English stop words  \nstop_words = nltk.corpus.stopwords.words('english')\nimport string\nimport re\n\n# Basic cleaning\ndef cleanAndTokenize(review):\n    # removing punctuations\n    non_punc_words = \"\".join([character for character in review if character not in string.punctuation])\n    \n    non_punc_words = non_punc_words.strip()\n    \n    # tokenizing reviews\n    list_of_token = re.split('\\W+',non_punc_words)\n    \n    # removing stop words\n    tokens = [word for word in list_of_token if word not in stop_words]\n    \n    return tokens\n\n# converting words to lower case.\ndataframe['comment'] = dataframe['comment'].apply(lambda review : cleanAndTokenize(str(review).lower()))\n\n# using nltk's wordnet lemmatizer\nword_net_lemma = nltk.WordNetLemmatizer()\n\ndef lemmatize_data(token_list):\n    tokens = [word_net_lemma.lemmatize(word) for word in token_list]\n    return tokens\n\ndataframe['comment'] = dataframe['comment'].apply(lambda review : lemmatize_data(review))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# joining the list of words to form a string for input to count vectorizer\ndataframe['comment'] = dataframe['comment'].apply(lambda review : \" \".join(review))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = dataframe[~dataframe['comment'].isin(['nan'])]\n\n# dividing the data set into test dev and train split\ntrain, development, test = np.split(dataframe.sample(frac=1, random_state=5), [int(.6*len(dataframe)), int(.8*len(dataframe))])\n\nprint('size of train data set: ',train.shape[0])\nprint(train.head())\nprint('*'*100)\nprint('size of development data set: ',development.shape[0])\nprint(development.head())\nprint('*'*100)\nprint('size of test data set: ',test.shape[0])\nprint(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\nalpha_arr = list(range(0,10))\n\nvectorizer_obj = CountVectorizer()#analyzer='word', ngram_range=(2, 2))\ncount_vector_obj = vectorizer_obj.fit_transform(train['comment'].apply(lambda x: np.str_(x)))\n\nclass_data = pd.DataFrame(train['rating'],dtype='int')\n# converting to 1-D array\nclass_data = np.ravel(class_data)\nplot_NB = []\n\nfor alpha_in in alpha_arr:\n    MNB_obj = MultinomialNB(alpha=alpha_in)\n    MNB_obj.fit(count_vector_obj, class_data)\n    actual_rating_arr = []\n    NB_predicted_rating_arr = []\n    for row in development.iterrows():\n        actual_rating = int(row[1]['rating'])\n        test_review = str(row[1]['comment'])\n        actual_rating_arr.append(actual_rating)\n        \n        input_count_obj = vectorizer_obj.transform([test_review])\n        \n        NB_prediction = MNB_obj.predict(input_count_obj)\n        NB_predicted_rating_arr.append(int(NB_prediction[0]))\n    MSE = pd.DataFrame(columns = ['NB_predicted_rating_arr','actual_rating_arr'])\n    MSE['actual_rating_arr'] = actual_rating_arr\n    MSE['NB_predicted_rating_arr'] = NB_predicted_rating_arr\n    mean_squared_error_NB = np.square(np.subtract(MSE['actual_rating_arr'],MSE['NB_predicted_rating_arr'])).mean()\n    plot_NB.append(mean_squared_error_NB)\n\n# Plot the mean v/s max values\nplt.figure(figsize=(15,6))\nplt.plot(alpha_arr,plot_NB,'b-o',label = 'M.S.E of each run')\nplt.legend(loc='upper right')\nplt.xlabel('smooting factor')\nplt.ylabel('M.S.E')\nplt.xticks(alpha_arr)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_data = pd.DataFrame(train['rating'],dtype='int')\n# converting to 1-D array\nclass_data = np.ravel(class_data)\n\nNB_predicted_rating_arr = []\nactual_rating_arr = []\n\nfor row in test.iterrows():\n    actual_rating = int(row[1]['rating'])\n    test_review = str(row[1]['comment'])\n    actual_rating_arr.append(actual_rating)\n\n    input_count_obj = vectorizer_obj.transform([test_review])\n\n    NB_prediction = MNB_obj.predict(input_count_obj)\n    NB_predicted_rating_arr.append(int(NB_prediction[0]))\n\nMSE = pd.DataFrame(columns = ['NB_predicted_rating_arr','actual_rating_arr'])\nMSE['actual_rating_arr'] = actual_rating_arr\nMSE['NB_predicted_rating_arr'] = NB_predicted_rating_arr\nmean_squared_error_NB = np.square(np.subtract(MSE['actual_rating_arr'],MSE['NB_predicted_rating_arr'])).mean()\nprint('Naive Bayes MSE for alpha=1 : ',mean_squared_error_NB)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}