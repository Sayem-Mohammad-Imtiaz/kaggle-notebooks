{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a209dcbe-243a-18e1-364f-2c69a08a6cad"},"source":"# Time Series: Phone Broadcast Data\n\n<br />\n<br />\n\n### Table of Contents\n\n* Introduction\n* Fixing the File\n* Strings to Timestamps\n* Time Deltas\n* Time Series\n* Frequency Histograms\n* Looking at the Full Dataset\n\n<br />\n<br />"},{"cell_type":"markdown","metadata":{"_cell_guid":"1805de12-dbc0-fbec-e905-992c80cf562b"},"source":"## Introduction\n\nDealing with time can be a pain - especially with datasets that have different time formats in each file. This notebook shows how to convert time stamps into useful formats, and use those to manipulate and bin time series to count events and investigate frequencies. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57a2f405-3432-58a8-07b8-ad5a14878cf3"},"outputs":[],"source":"# Also see\n# http://pandas.pydata.org/pandas-docs/stable/timeseries.html"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dba18c77-432f-cd0a-29e7-351f3dfb504a"},"outputs":[],"source":"import os\nos.listdir('../input/')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b61f5f2-14a9-1627-edf0-175b0ba63ef2"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint"},{"cell_type":"markdown","metadata":{"_cell_guid":"574cad3c-c1cb-f2f2-7e9d-b70b4cb57815"},"source":"## Fixing The File\n\nWe'll start with the AllBroadcasts.csv data file. This file is fine for the first 50 lines or so, but it chokes on the whole file because of stray commas."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef35e057-2bc1-43bb-8f2c-e44890210e40"},"outputs":[],"source":"all_broadcasts = pd.read_csv('../input/AllBroadcasts.csv',nrows=50,header=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d280a65-bd31-b478-03ac-3c1995a19f1f"},"outputs":[],"source":"# Loading all of the data chokes on a column with commas\n#all_broadcasts = pd.read_csv('data/AllBroadcasts.csv',nrows=1000,header=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9239ab7c-aa81-e473-3bdd-962e51a94202"},"outputs":[],"source":"with open('../input/AllBroadcasts.csv','r') as f:\n    lines = f.readlines()\nlc = set()\nfor line in lines:\n    lc.add( len( line.split(\",\") ) )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27e2e798-598b-646e-f3de-28c86b33fb9e"},"outputs":[],"source":"print(lc)"},{"cell_type":"markdown","metadata":{"_cell_guid":"adc3ea9f-2cd1-9204-017e-d609f94fc423"},"source":"We are expecting five fields (four commas) based on the file header, but we have some lines with no commas (probably an empty line) and some lines with 1, 2, or more extra commas:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"372206e7-0165-3683-2beb-2b010d3493a4"},"outputs":[],"source":"print(lines[0])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9be19111-03aa-a758-43a2-a41b2daf74d1"},"outputs":[],"source":"Here's an example of one of those lines - it consists of the first two fields, UserId and UUID, which are parsed fine, but an Extras column containing commas. Then the Action and timestamp columns are also parsed okay."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c548d1fe-b875-2049-1ad2-5b2149e56258"},"outputs":[],"source":"sp = lines[73].split(\",\")\nprint(sp)\nprint(len(sp))"},{"cell_type":"markdown","metadata":{"_cell_guid":"28067607-3497-9d13-efb6-a1129d2282e4"},"source":"Our parsing strategy, when there are more than 4 commas, is to split into tokens, then recombine all the middle tokens. We can get a list of token indexes that are the \"middle\" (excluding the first two and last two items) using `range(2, len(sp)-2 )` - except that excludes the last number, so we should actually use `range(2, (len(sp)-2)+1)`."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74a4406c-aa2d-5010-8694-4465ccca9736"},"outputs":[],"source":"#print len(sp)\nnew_sp = []\n\nnew_sp.append(sp[0].strip())\nnew_sp.append(sp[1].strip())\n\n# This one-liner uses a list comprehension to collect each of the middle pieces\n# then concatenates everything together with \"\".join()\nmiddle_token = \"\".join([sp[j].strip() for j in range(2, len(sp)-2 + 1)])\nnew_sp.append(middle_token)\n\nnew_sp.append(sp[-2].strip())\nnew_sp.append(sp[-1].strip())\n\nprint(new_sp)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ab836dc-d047-6d88-ddd9-104b3cc16e43"},"outputs":[],"source":"def strip_non_ascii(string):\n    ''' Returns the string without non ASCII characters'''\n    stripped = (c for c in string if 0 < ord(c) < 127)\n    return ''.join(stripped)\n\n# Note that clean_line returns a list of string tokens, not a string.\ndef clean_line(line):\n    \n    line2 = strip_non_ascii(line)\n    sp = line2.split(\",\")\n    \n    if(len(sp)==5):\n        return sp\n    \n    elif(len(sp) is not 5 and len(sp)>2):\n                \n        new_sp = []\n        \n        new_sp.append(sp[0])\n        new_sp.append(sp[1])\n\n        # This one-liner uses a list comprehension to collect each of the middle pieces\n        # then concatenates everything together with \"\".join()\n        middle_token = \"\".join([sp[j] for j in range(2, len(sp)-2 + 1)])\n        new_sp.append(middle_token)\n\n        new_sp.append(sp[-2])\n        new_sp.append(sp[-1])\n        \n        return new_sp\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01039e01-391f-89d4-d2df-5ac2d5514d10"},"outputs":[],"source":"clean_headers = strip_non_ascii(lines[0]).strip().split(\",\")\n\n# Start at second line (skip header) and end at second-to-last line \n# (arrrrrg, blank lines cause blank lists which cause Pandas problems)\n# (could add an if to the list comprehension too - if line not [])\nclean_tokens = [clean_line(line) for line in lines[1:-1]]\nprint(clean_headers)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7ed2b9a-7547-8fc5-8c9d-5f0511169b24"},"outputs":[],"source":"all_broadcasts_full = pd.DataFrame(clean_tokens, columns = clean_headers)\nprint(all_broadcasts_full.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"222f24d6-ba18-10d4-0174-dc281549a6a1"},"outputs":[],"source":"print(all_broadcasts_full.columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"33923836-558d-66bf-3e7f-d48a15748880"},"source":"Success! We now have all 170,000+ lines of this file loaded into a DataFrame."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97a551ca-899d-e703-b54d-17d0b34ea1da"},"outputs":[],"source":"## Strings to Timestamps\n\nThe AllBroadcasts.csv data file contains timestamps as strings. We can convert these to more useful objects."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8dab1c4f-2c3d-5a98-afbe-1af94eddeed5"},"outputs":[],"source":"#print all_broadcasts\n#print all_broadcasts['timestamp']\nprint(all_broadcasts['timestamp'].loc[0])\nprint(type(all_broadcasts['timestamp'].loc[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fd45630e-4e38-9075-ac08-9fd6a1afe86b"},"source":"This is the optimal situation - we're given a nicely-formatted timestamp as a string that we can definitely turn into a date."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d244aeec-6099-d1f7-b63e-73808c1b21e1"},"outputs":[],"source":"list_of_strings = all_broadcasts['timestamp'].tolist()\nprint(all_broadcasts['timestamp'].describe())"},{"cell_type":"markdown","metadata":{"_cell_guid":"29519604-6780-cd47-2048-0853c3689bd0"},"source":"To convert a single column of date/time strings into datetime objects, use `pd.to_datetime()`:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c698e43b-5c78-57a3-dcd2-d317b46eeb32"},"outputs":[],"source":"print(pd.to_datetime(all_broadcasts['timestamp']).head(10))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66336c85-7e99-5d19-3ea1-13bf5f3e3b55"},"outputs":[],"source":"all_broadcasts.loc[:,'timestamp'] = pd.to_datetime(all_broadcasts['timestamp'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fb30891-f0d0-24d5-a106-67b255138634"},"outputs":[],"source":"print(all_broadcasts['timestamp'].loc[0])\nprint(type(all_broadcasts['timestamp'].loc[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"84b044f1-c147-43a3-0629-3d2dd9681798"},"source":"Now all of the date/time strings have been converted to a Pandas Timestamp type. What can we do with this object? Start by getting the range of dates covered by this data set. We can perform min/max operations on Pandas Timestamp objects and the comparison works as we would expect, so we can get the date range covered by this data set:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"609bde0d-810d-b9be-8613-da4d3fdd63a7"},"outputs":[],"source":"tmin = all_broadcasts['timestamp'].min()\ntmax = all_broadcasts['timestamp'].max()\nprint(tmin)\nprint(tmax)"},{"cell_type":"markdown","metadata":{"_cell_guid":"451df6c6-548f-47a4-6779-b80c0ce63225"},"source":"If we wanted to make our own range of Timestamps, at a specified interval, we could use the `pd.date_range(start,end,freq)` function, which creates a series of timestamp objects that starts at start and ends at end, at a frequency of freq (seconds, minutes, days, etc.)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a78f8f95-8542-0710-40d4-dc9fdbed47a1"},"outputs":[],"source":"dates = pd.date_range(tmin,tmax,freq='S')\nprint(dates)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4cc2c29c-701b-1cca-ac99-dfdcf2bfd89d"},"source":"This is useful if we have data without timestamps (create a series with the data, and specify the date/time index object just created as the index). It is also useful if we want to create a \"master list\" of timestamps covering a certain date/time range with a specified frequency.\n\nSomewhat related, if we need to convert our long list of timestamps (or any other list of timestamps) into a DatetimeIndex object, we can use the DatetimeIndex object constructor:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be586b71-45bc-f724-71f2-da6c1e6b4aac"},"outputs":[],"source":"print(type(list_of_strings))\nprint(type(list_of_strings[0]))\nprint(pd.DatetimeIndex(list_of_strings))"},{"cell_type":"markdown","metadata":{"_cell_guid":"83f7b681-8ec9-e7f8-24c1-5ca37e9aca42"},"source":"## Time Deltas\n\nSuppose we want to know how long this data set spans - how many seconds, minutes, hours, or days? If we subtract two datetime objects, we get the result as a Timedelta object:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3d148b8-db31-16e4-b119-e55c1659473f"},"outputs":[],"source":"diff = tmax-tmin\nprint(diff)\nprint(type(diff))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1e1c4d0-5d0a-f3b3-f114-267517808fa3"},"outputs":[],"source":"#print dir(diff)\nprint(\"%d minutes %d seconds\"%( diff.seconds/60,diff.seconds%60 ))"},{"cell_type":"markdown","metadata":{"_cell_guid":"79b1bb4e-7cbf-fa27-02ac-39d0a94d8fd0"},"source":"If we want to convert the column of absolute timestamps into a column of relative time differences represented with Timedelta objects, we can just subtract a date from the entire row:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17580285-a883-8be7-85c3-f864046be4f9"},"outputs":[],"source":"time_diff = all_broadcasts['timestamp'] - all_broadcasts['timestamp'].loc[0]\nprint(time_diff.head(10))"},{"cell_type":"markdown","metadata":{"_cell_guid":"62842e3b-1436-d069-e4b6-33de5199ada1"},"source":"We can also combine this with Timedelta's built-in methods and fields by defining a function that operates element-wise, then applying that function to the whole column:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d631159-6349-5d74-f1fc-a2d82ccd0a90"},"outputs":[],"source":"def print_me(diff):\n    return \"%d minutes %d seconds\"%( diff.seconds/60,diff.seconds%60 )\n\nprint(time_diff.apply( lambda x : print_me(x) ).head(10))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4db88248-68fc-c60b-9208-b28d4b6d6409"},"source":"## Time Series\n\nLet's talk about time series proper - that is, Pandas Series objects whose index is actually a DatetimeIndex object. This type of index has some more powerful built-in methods that we'll explore. First, how do we turn a column of data in our DataFrame, which has timestamps in another separate column, into a Series with a time index?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d538570a-ee30-98a1-b9d3-e771eb607c57"},"outputs":[],"source":"print(all_broadcasts.columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7da20a29-9a49-3831-93cc-40bf16f9ab23"},"source":"Start by getting the timestamps and the data that we're interested in combining. Let's examine the \"Action\" column. Grab the \"Action\" and \"timestamp\" columns, and here we use the `.values` attribute to reduce these to Numpy arrays to keep things simple, uncluttered, and uncomplicated."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f5b0efd-61b6-236c-a6a7-b91054ed5f62"},"outputs":[],"source":"data_values = all_broadcasts['Action'].values\ndata_index  = all_broadcasts['timestamp'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e275f50-4935-6236-8e7d-fb2af1cec4a7"},"outputs":[],"source":"ts = pd.Series(data_values, index=data_index)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b91b710c-84df-d451-73fe-e2785489ec94"},"outputs":[],"source":"print(ts.head(10))"},{"cell_type":"markdown","metadata":{"_cell_guid":"80da10da-a503-0f40-20f8-9faf9a610eee"},"source":"We already saw that we are looking at about 15 minutes of data:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de321bfc-c52a-ce34-e9e0-529081a3c46a"},"outputs":[],"source":"print(ts.index.max() - ts.index.min())"},{"cell_type":"markdown","metadata":{"_cell_guid":"013bd416-1d2c-8fb9-ae9c-1f5f161b6958"},"source":"Let's look at how to select a range of data. We'll explore two examples:\n* Extract all data between 2016-04-28 00:37:19 and 00:39:19 (that is, 2 minutes of data specified by timestamp)\n* Extract all data between minute 2 and minute 4 of this long (that is, 2 minutes of data specified relatively)\n\nTo perform the first type of filtering, we want to extract all data falling between two timestamps. This turns out to be really easy, assuming we use nicely-formatted timestamp strings. The syntax looks something like array slicing: `ts[start:end]`."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"792e54aa-c183-2ec4-ac40-3adce1c50c2e"},"outputs":[],"source":"# To find all time series data between two timestamp ranges, just specify them as ts[start:end]\nprint(ts['2016-04-28 00:37:19':'2016-04-28 00:39:19'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"9207aa69-c5a0-880c-2819-fd514bcb8d13"},"source":"To perform the second type of filtering, we want to create a new column of time deltas (a \"seconds elapsed\" column), and extract all data falling between two time delta values. This is only slightly more complicated - and made easier by the fact that we can still use the same slicing notation. If we make all timestamps relative to the start of the data set, then we can slice it starting at '00:02:00' (2 minutes) and ending at '00:04:00' (4 minutes)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e258764-b31d-f5cc-3ac4-b151ba1340fa"},"outputs":[],"source":"# To find all time series data based on elapsed time, start by making a new time delta index\ndelta_index = ts.index - ts.index.min()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"181f524a-d1b2-c0ec-a5bf-0b543beece31"},"outputs":[],"source":"# Make a new series object with the new time delta index and the same data\ntsd = pd.Series(data_values, index=delta_index)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04ce65ab-9fe5-a0b2-ccec-97c7ee674aa8"},"outputs":[],"source":"# Now slice this the same way we sliced the other...\nprint(tsd['00:02:00':'00:04:00'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"750a2a5c-994b-3f46-7424-82112431d5cf"},"source":"The documentation for what strings, exactly, these slicing methods will take is not entirely clear. If you opt instead for using Timedelta objects directly, you'll get yourself into some trouble, although it is not obvious why, exactly:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8506496d-19b7-e133-7f13-bd502a8447a6"},"outputs":[],"source":"twomin = pd.Timedelta(2,units='m')\nfourmin = pd.Timedelta(4,units='m')\nprint(tsd[twomin:fourmin])"},{"cell_type":"markdown","metadata":{"_cell_guid":"6d81d2eb-4c0a-9eac-5eb6-7c655089addc"},"source":"## Frequency Histogram\n\nLet's suppose we want to use a long index of timestamps to examine the sampling frequency and see if it is consistent across the data set or whether it occurs at random intervals. In this case, we'll use `ts.iteritems()` to iterate through each item, one timestamp and piece of data at a time. At each step (except the first one), we'll compute the Timedelta between the current timestamp and the previous timestamp. Adding this to a list will give us a collection of data on which to compute statistics and plot histograms."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d3739a8-37ad-b05c-7ad7-882f6f93210c"},"outputs":[],"source":"diffs = []\nfor (i,(t,d)) in enumerate(ts.iteritems()):\n    if i>0:\n        diff = t - prev_value\n        diffs.append(diff)\n    prev_value = t\n\ndiffs = pd.Series(diffs)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a0f1fec9-45eb-b531-3875-2f5bde3615c2"},"source":"One more thing to do, before we can plot a histogram, is to convert the Timedelta object (which plotting libraries will not understand) into a number. We can use the seconds attribute to get the equivalent number of seconds of each Timedelta:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc10c594-9b4b-2d88-85f1-1e98f679ec0d"},"outputs":[],"source":"diffs = diffs.apply(lambda x : x.seconds)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0dde2bb4-031f-edad-910a-3096b419e3f4"},"outputs":[],"source":"%matplotlib inline\nimport matplotlib.pylab as plt\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cebfc702-f21b-9500-dfed-4bc8a697fb39"},"outputs":[],"source":"sns.distplot(diffs, kde=False,bins=10)\nplt.title('Histogram: Sampling Frequencies')\nplt.xlabel('Sampling Interval (s)')\nf = plt.gcf()\nf.set_size_inches(6,3)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6dc58fd5-cead-bb28-62bb-c1287781b6cd"},"source":"Three sampling intervals are dominant. This, together with the data set we are visualizing, indicates that there are probably 2 or 3 background processes constantly running at fixed intervals, with some other less continual processes mixed in. (Note this is also only 15 minutes of data - we'll get to the full dataset in a moment.)\n\nWe could explore this further by grouping the data by \"Activity\" label, and repeating the above procedure on each group to plot a sampling frequency histogram for each different \"Activity\". This would tell us which activities have constant sampling frequencies, and which happen sporadically."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3de750d5-eef4-ae46-f700-b00eac9d8543"},"outputs":[],"source":"all_broadcasts.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12f57906-d265-1fef-da17-715dd9f6a7d0"},"outputs":[],"source":"grp = all_broadcasts[['timestamp','Action']].groupby(['Action'])\n\nprint(grp.groups.keys())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73e790cc-2edd-8f28-9803-a79b8dee93a3"},"outputs":[],"source":"all_keys = grp.groups.keys()\n\nfor key in all_keys:\n\n    print(\"Timestamps matching action '%s':\"%(key))\n    for t in grp.groups[key]:\n        print(all_broadcasts['timestamp'].ix[t])\n        \n    print(\"\")\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"03c349a8-ef08-617d-c5f3-cf59402a84c8"},"source":"This gives us a list of timestamps associated with a particular activity. If we wanted to turn those into Timedeltas from the start of the dataset, we still have the first time stored in `tmin`, so we can subtract that from the matching timestamps:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fc6ad51-cb0c-a314-cda8-02d43088d6e3"},"outputs":[],"source":"all_keys = grp.groups.keys()\n\nfor key in all_keys:\n\n    print(\"Timestamps matching action '%s':\"%(key))\n    for t in grp.groups[key]:\n        diff = all_broadcasts['timestamp'].ix[t] - tmin\n        print(\"%s (%s)\"%( print_me(diff) , diff ))\n        \n    print(\"\")\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"03ad1fa9-51c2-b0c4-cff4-5b73ebe3176c"},"source":"To turn these into histograms, it's probably useful to actually store this information. Let's store it as a list of Series, and use the name attribute of the Series to store the action name:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73475eda-1c06-6b73-e851-69e34d621dcc"},"outputs":[],"source":"all_keys = grp.groups.keys()\n\nlist_of_series = []\n\nfor key in all_keys:\n    \n    data = []\n    list_of_timestamps = grp.groups[key]\n    \n    for c,t in enumerate(list_of_timestamps):\n        if(c>0):\n            diff = (all_broadcasts['timestamp'].ix[t] - prior_value).seconds\n            data.append(diff)\n        prior_value = all_broadcasts['timestamp'].ix[t]\n    \n    label = key\n    \n    s = pd.Series(data,name=label)\n    list_of_series.append(s)\n\nprint(list_of_series)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91a2e606-5c1d-4e77-7e62-c996d260cc76"},"outputs":[],"source":"#sns.distplot(list_of_series[0], norm_hist=True, bins=5, kde=False)\ninterval = 15\nfifteen_second_bins = range(0,3*60+interval,interval)\n[sns.distplot(s, norm_hist=False, kde=False, bins=fifteen_second_bins, label=s.name) for s in list_of_series]\n#plt.xlim([0,0.1])\nf = plt.gcf()\nf.set_size_inches(6,4)\n\nplt.xlabel('Sensor Interval (Seconds)')\nplt.ylabel('Number')\nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ab5a28d-80f2-a4e9-e79a-7df70908a1d6"},"source":"Let's unpack that Seaborn command. Because plotting histograms can be squirrely when you have even slight differences in the distribution of data (especially for small data sets), we specify the bin sizes using the `range()` command. We assume most intervals are 3 minutes or less, and split that into four parts (15 second intervals):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f78d982-8dca-a686-c8e7-963fdcea8b62"},"outputs":[],"source":"interval = 15\nrange(0,3*60+interval,interval)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1119bfd1-13ff-5f7f-c073-8faaf530d8a2"},"source":"This way, we can compare histograms of intervals across categories, without dealing with various other complications."},{"cell_type":"markdown","metadata":{"_cell_guid":"3121fa28-5ad3-5cb9-e508-a2067e303ef2"},"source":"## Looking at the Full Dataset\n\nLet's take a look at the full dataset. For some reason, adding calls to `.strip()` in the methods we defined above still hasn't gotten rid of \\r and \\n characters in the timestamps. We'll have to fix that to properly parse the timestamps:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"882707a6-cfec-3b88-d4df-8f390db95818"},"outputs":[],"source":"pprint(all_broadcasts_full['timestamp'].head(10).tolist())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4f0115e-bdca-79de-c293-83a0ce587aac"},"outputs":[],"source":"all_broadcasts_full.loc[:,'timestamp'] = all_broadcasts_full['timestamp'].apply(lambda x : x.strip())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acc1a542-d53b-197f-4305-63b07dbe6a67"},"outputs":[],"source":"pprint(all_broadcasts_full['timestamp'].head(10).tolist())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80ca3c3a-6d5c-6f30-62e4-17560b9ad991"},"outputs":[],"source":"# This does not work:\n#all_broadcasts_full.loc[:,'timestamp'] = pd.to_datetime(all_broadcasts_full['timestamp'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d5e2cb0-ff37-8b49-fd4e-81abdca82989"},"outputs":[],"source":"nerr = 0\nfor (i,row) in all_broadcasts_full['timestamp'].iteritems():\n    try:\n        pd.to_datetime(row)\n    except:\n        nerr += 1\n        pass\nprint(\"%d errors\"%(nerr))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3548ce21-932b-4821-5af3-adc6decbd7b0"},"outputs":[],"source":"all_broadcasts_full.loc[:,'timestamp'] = pd.to_datetime(all_broadcasts_full['timestamp'],errors='coerce')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a39869b5-c484-ad92-69b1-8d053edd7e91"},"outputs":[],"source":"# ------\n# Step 1: Group\n\ngrp = all_broadcasts_full[['timestamp','Action']].groupby(['Action'])\n\n#print(grp.groups.keys())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"660d1f80-3f5a-221e-841a-a7a8117d7e3c"},"outputs":[],"source":"# -----\n# Step 2: Determine Polling Intervals\n# \n# (this takes a while)\n\nall_keys = grp.groups.keys()\nlist_of_series = []\nfor key in all_keys:\n    \n    data = []\n    list_of_timestamps = grp.groups[key]\n    \n    for c,t in enumerate(list_of_timestamps):\n        \n        skip = False\n        if(c>0):\n            try:\n                diff = (all_broadcasts_full['timestamp'].ix[t] - prior_value).seconds\n                data.append(diff)\n            except:\n                skip = True\n                \n        if(not skip):\n            prior_value = all_broadcasts_full['timestamp'].ix[t]\n\n    label = key\n    \n    s = pd.Series(data,name=label)\n    list_of_series.append(s)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ffed829-6084-124d-7426-d9f7545c8cb3"},"source":"Now we have gone through the list of groups (unique actions and the corresponding list of timestamp indexes) and turned each into a number of seconds since the last timestamp from that service. We then sent that list of intervals to a list. Now we can visualize the list. We'll start by going through the list of keys for the groups - these are the different actions in the `AllBroadcast.csv` file. \n\nUsing list comprehensions, we can filter on different services:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93fc5f85-09e0-9e37-3e58-1c25e3bc1088"},"outputs":[],"source":"pprint([j for j in all_keys if 'bluetooth' in j])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8bc3ed7-bfc5-412e-b551-d1d8a8e28dbb"},"outputs":[],"source":"pprint([j for j in all_keys if 'wifi' in j])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec9b8048-d05b-533d-3c87-5902e7df9f8a"},"outputs":[],"source":"pprint([j for j in all_keys if 'hardware' in j])"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac8ea242-8e1b-4778-5f6b-29603fc1e401"},"source":"We can also use list comprehensions to group our buckets of intervals for each action into groups. Remember, we used the Series name field, which allows us to retrieve Series by name:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82d7cb40-5718-b5d2-7d1f-6b9482eaa5b0"},"outputs":[],"source":"bluetooth_series = [s for s in list_of_series if 'bluetooth' in s.name]\nwifi_series = [s for s in list_of_series if 'wifi' in s.name]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa60a208-6099-6958-37a4-4751c64a2811"},"outputs":[],"source":"# -------\n# Step 3A: Bluetooth Interval Counts\n\n#sns.distplot(list_of_series[0], norm_hist=True, bins=5, kde=False)\nminutes = 5\ninterval = 15\nfifteen_second_bins = range(0,minutes*60+interval,interval)\n\n[sns.distplot(s, norm_hist=False, kde=False, bins=fifteen_second_bins, label=s.name) for s in bluetooth_series]\n\nf = plt.gcf()\nf.set_size_inches(8,6)\n\nplt.xlim([0,minutes*60])\nplt.xlabel('Sensor Interval (Seconds)')\nplt.ylabel('Number')\nplt.legend()\nplt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2967c084-e9c5-466e-74e7-3300ba99b019"},"outputs":[],"source":"# -------\n# Step 3B: Wifi Interval Counts\n\n#sns.distplot(list_of_series[0], norm_hist=True, bins=5, kde=False)\nminutes = 5\ninterval = 15\nfifteen_second_bins = range(0,minutes*60+interval,interval)\n\n[sns.distplot(s, norm_hist=False, kde=False, bins=fifteen_second_bins, label=s.name) for s in wifi_series]\n\nf = plt.gcf()\nf.set_size_inches(8,6)\n\nplt.xlim([0,minutes*60])\nplt.xlabel('Sensor Interval (Seconds)')\nplt.ylabel('Number')\nplt.legend()\nplt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"039a7c40-2516-3313-a4ee-b44fc2479848"},"outputs":[],"source":"# -------\n# Step 3B: Wifi Interval Counts (improved)\n\n#sns.distplot(list_of_series[0], norm_hist=True, bins=5, kde=False)\nminutes = 2\ninterval = 5\nfifteen_second_bins = range(0,minutes*60+interval,interval)\n\n[sns.distplot(s, norm_hist=False, kde=False, bins=fifteen_second_bins, label=s.name) \n         for s in wifi_series\n            if 'RSSI' not in s.name]\n\nf = plt.gcf()\nf.set_size_inches(12,6)\n\nplt.xlim([0,minutes*60])\nplt.xlabel('Sensor Interval (Seconds)')\nplt.ylabel('Number')\nplt.legend()\nplt.show()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"5dc130fc-a5e9-fc99-ba65-455c33eb1232"},"source":"Among the high frequency signals are state changes in the wifi and wifi supplicant, while lower frequency signals are p2p state changes and p2p device changes. Sensible - p2p networks tend to be limited to a smaller area and a smaller number of people and a smaller area."},{"cell_type":"markdown","metadata":{"_cell_guid":"daff16f9-ffa3-1afa-0ca5-c29f9e7ded2a"},"source":"## Conclusions and Next Steps\n\nThis isn't a very in-depth exploration, but got us familiar with time stamps in this file (a format shared by several other files). This sets us up for later data analysis. More concretely, we have an idea of the polling frequencies of various wifi and bluetooth sensors onboard the phone, which can be used as a proxy for changes in environment.\n\nIn later notebooks we'll keep exploring the data in these other data sets, focusing on counts and on broad-level statistics to understand what's in the data. Then we can start to understand how to build machine learning models from the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9092a71c-ee54-e376-8ba2-70a31b3e87d2"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}