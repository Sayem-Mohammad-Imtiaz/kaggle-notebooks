{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"demo=pd.read_csv(\"../input/california-housing-prices/housing.csv\")\ndemo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Values_counts\nfrom collections import Counter\nCounter(demo.ocean_proximity)\n# Nominal data because no order in this data --- have to make dummies encoding\n# ordinal data just normal encoding like high:3, medium:2, low:1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode categorical data\ndata=demo\ndummy = pd.get_dummies(data[\"ocean_proximity\"], prefix='Ocean_').iloc[:,:-1]\ndata = pd.concat([data,dummy], axis=1)\ndata = data.drop(\"ocean_proximity\", axis=1) \ndata.shape\n\n# read about feature hasher for encoding (used when there are a lot of classes in a categorical col)\n# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values\n# using iterative imputer to impute values\n\nfrom sklearn.experimental import enable_iterative_imputer\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\nit = IterativeImputer(estimator=LinearRegression())\nnewdata = pd.DataFrame(it.fit_transform(data))\nnewdata.columns = data.columns\nnewdata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if imputation was correct (we plot distribution)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total_bedroom minimum has gone down in negative so we look for something other than linear regression\n# lets try Random forests\n\nfrom sklearn.experimental import enable_iterative_imputer\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nit = IterativeImputer(estimator=RandomForestRegressor())\nnewdata = pd.DataFrame(it.fit_transform(data))\nnewdata.columns = data.columns\nnewdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see histograms of data and see there are 2 visible clusters,\n# Semi supervised learning (supervised+unsupervised)\n# so seperate the data using clustering and treat clusters differently (not in this notebook)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputation validation\nfrom matplotlib import pyplot  as plt \nimport seaborn as sns\nsns.distplot(data['total_bedrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot  as plt \nimport seaborn as sns\nsns.distplot(newdata['total_bedrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hypothesis testing \n# test of variance\n# t test\nimport scipy.stats as stats\nstats.ttest_ind(data['total_bedrooms'],newdata['total_bedrooms'],nan_policy='omit')\n\n# very high P value \n# H0: mean b4 imputation = mean after imputation\n# H1: not\n# if P<5% reject null\n# P is very high so we fail to reject null hypothesis\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformation - to deal with skewness (Box-cox transform), also deal with Outliers\n#make data more gaussian\nfrom sklearn.preprocessing import PowerTransformer #normalises the data\npt=PowerTransformer()\npowerdata=pd.DataFrame(pt.fit_transform(newdata))\npowerdata.columns=newdata.columns\npowerdata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdata.hist(figsize=(10,10),bins=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"powerdata.hist(figsize=(10,10),bins=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaleddata.hist(figsize=(10,10),bins=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdata.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"powerdata.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test train split before scaling and transforming (if not data leak happens)\n\n#Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nscaleddata = pd.DataFrame(sc.fit_transform(powerdata))\nscaleddata.columns = powerdata.columns\nscaleddata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing everything again with Pipeline\ndata = pd.read_csv(\"/kaggle/input/california-housing-prices/housing.csv\")\ndummy = pd.get_dummies(data[\"ocean_proximity\"], prefix='Ocean_').iloc[:,:-1]\n# one hot encoder if want to do this in pipeline\ndata = pd.concat([data,dummy], axis=1)\ndata = data.drop(\"ocean_proximity\", axis=1)\ndata.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iteration 1 LR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nX=data.drop('median_house_value',axis=1)\ny=data['median_house_value']\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .20,random_state=10)\npipe = Pipeline((\n(\"it\", IterativeImputer(estimator = LinearRegression())),# use random forest for better result\n(\"pt\", PowerTransformer()),\n(\"sc\",StandardScaler()),\n(\"lr\", LinearRegression()),\n))\npipe.fit(Xtrain,ytrain)\nprint(\"Training R2\")\nprint(pipe.score(Xtrain,ytrain))\nprint(\"Testing R2\")\nprint(pipe.score(Xtest,ytest))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract steps from pipeline\npipe.named_steps['lr'].coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iteration 2 LR(poly)\nfrom sklearn.preprocessing import PolynomialFeatures\nX=data.drop('median_house_value',axis=1)\ny=data['median_house_value']\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .20,random_state=10)\npipe = Pipeline((\n(\"it\", IterativeImputer(estimator = LinearRegression())),\n(\"pt\", PowerTransformer()),\n(\"sc\",StandardScaler()),\n(\"poly\",PolynomialFeatures(degree=3)),# X^3 +X^2 highest power of equation is 3 # feature engg technique # making new features like x^2, x^3 ,x^0 etc\n(\"lr\", LinearRegression()),\n))\npipe.fit(Xtrain,ytrain)\nprint(\"Training R2\")\nprint(pipe.score(Xtrain,ytrain))\nprint(\"Testing R2\")\nprint(pipe.score(Xtest,ytest))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iteration 3 #Booster\nfrom xgboost import XGBRegressor\nX=data.drop('median_house_value',axis=1)\ny=data['median_house_value']\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .20,random_state=10)\npipe = Pipeline((\n(\"it\", IterativeImputer(estimator = LinearRegression())),\n(\"pt\", PowerTransformer()),\n(\"sc\",StandardScaler()),\n(\"lr\", XGBRegressor(n_estimators=100)),\n))\npipe.fit(Xtrain,ytrain)\nprint(\"Training R2\")\nprint(pipe.score(Xtrain,ytrain))\nprint(\"Testing R2\")\nprint(pipe.score(Xtest,ytest))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Cross Validation\nfrom sklearn.model_selection import cross_val_score\nscoresxgb = cross_val_score(pipe,Xtrain,ytrain,cv=5,scoring='r2')\nprint(scoresxgb)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nprint(\"Average R2 of model\")\nprint(np.mean(scoresxgb))\nprint(\"SD of model\")\nprint(np.std(scoresxgb))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read about repeted K fold cross validation\n# https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Estimate Confidence Interval of R2\nimport scipy.stats as stats\nn=10 # sample size\nxbar = np.mean(scoresxgb)\ns = np.std(scoresxgb)\nse = s/np.sqrt(n) #se std error\nstats.t.interval(0.95,df=n-1,loc=xbar,scale=se)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}