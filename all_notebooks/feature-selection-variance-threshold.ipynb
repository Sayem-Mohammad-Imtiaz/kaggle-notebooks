{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Feature Selection\n#### Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.","metadata":{}},{"cell_type":"markdown","source":"## Variance Threshold\n________\n#### Variance threshold is a simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold as it is assumed that features with a higher variance may contain more useful information. \n#### Here it is used as a feature selector that removes all low-variance features.","metadata":{}},{"cell_type":"markdown","source":"![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/10/Image-6-1.png)\n\n#### Features having variance lower than or equal to the threshold value will be returned as 'False' (in the array returned by .get_support()) and will be dropped. \n#### After dropping these features  we check the accuracy of the new dataset and compare it with original.\n#### For choosing the threshold values, I experimented with different numbers and chose the suitable ones.","metadata":{}},{"cell_type":"markdown","source":"__________","metadata":{}},{"cell_type":"markdown","source":"### Importing the required libraries ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\ndef split(df,label):\n    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n    return X_tr, X_te, Y_tr, Y_te\n\n\nfrom sklearn.feature_selection import VarianceThreshold\ndef variance_threshold(df,th):\n    var_thres=VarianceThreshold(threshold=th)\n    var_thres.fit(df)\n    new_cols = var_thres.get_support()\n    return df.iloc[:,new_cols]\n    \n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score\n\nclassifiers = ['LinearSVM', 'RadialSVM', \n               'Logistic',  'RandomForest', \n               'AdaBoost',  'DecisionTree', \n               'KNeighbors','GradientBoosting']\n\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          DecisionTreeClassifier(random_state=0),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)]\n\ndef acc_score(df,label):\n    Score = pd.DataFrame({\"Classifier\":classifiers})\n    j = 0\n    acc = []\n    X_train,X_test,Y_train,Y_test = split(df,label)\n    for i in models:\n        model = i\n        model.fit(X_train,Y_train)\n        predictions = model.predict(X_test)\n        acc.append(accuracy_score(Y_test,predictions))\n        j = j+1     \n    Score[\"Accuracy\"] = acc\n    Score.sort_values(by=\"Accuracy\", ascending=False,inplace = True)\n    Score.reset_index(drop=True, inplace=True)\n    return Score\n\ndef acc_score_thr(df,label,thr_list):\n    Score = pd.DataFrame({\"Classifier\":classifiers})\n    for k in range(len(thr_list)):\n        df2 = variance_threshold(df,thr_list[k])\n        X_train,X_test,Y_train,Y_test = split(df2,label)\n        j = 0\n        acc = []\n        for i in models:\n            model = i\n            model.fit(X_train,Y_train)\n            predictions = model.predict(X_test)\n            acc.append(accuracy_score(Y_test,predictions))\n            j = j+1  \n        feat = str(thr_list[k])\n        Score[feat] = acc\n    return Score\n\n        \ndef plot2(df,l1,l2,p1,p2,c = \"b\"):\n    feat = []\n    feat = df.columns.tolist()\n    feat = feat[1:]\n    plt.figure(figsize = (16, 18))\n    for j in range(0,df.shape[0]):\n        value = []\n        k = 0\n        for i in range(1,len(df.columns.tolist())):\n            value.append(df.iloc[j][i])\n        plt.subplot(4, 4,j+1)\n        ax = sns.pointplot(x=feat, y=value,color = c )\n        plt.text(p1,p2,df.iloc[j][0])\n        plt.xticks(rotation=90)\n        ax.set(ylim=(l1,l2))\n        k = k+1\n        \n\ndef highlight_max(data, color='aquamarine'):\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  \n        is_max = data == data.max()\n        return [attr if v else '' for v in is_max]\n    else: \n        is_max = data == data.max().max()\n        return pd.DataFrame(np.where(is_max, attr, ''),\n                            index=data.index, columns=data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:21:59.858134Z","iopub.execute_input":"2021-06-26T11:21:59.858586Z","iopub.status.idle":"2021-06-26T11:21:59.904068Z","shell.execute_reply.started":"2021-06-26T11:21:59.858556Z","shell.execute_reply":"2021-06-26T11:21:59.903337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_______\n### Function Description\n#### 1. split():\nSplits the dataset into training and test set.\n#### 2. variance_threshold():\nReturns the dataframe after dropping features with lower variance than the threshold value.\n#### 3. acc_score():\nReturns accuracy for all the classifiers.\n#### 4. acc_score_thr():\nReturns accuracy for all the classifiers for the respective threshold value.\n#### 5. plot2():\nFor plotting the results.\n___________\n### The following 3 datasets are used:\n1. Breast Cancer\n2. Parkinson's Disease\n3. PCOS\n________\n### Plan of action:\n* Looking at dataset (includes a little preprocessing)\n* Checking Accuracy (comparing accuracies with the new dataset)\n* Visualization (Plotting the graphs)\n_______","metadata":{}},{"cell_type":"markdown","source":"______________\n# Breast Cancer\n_____________","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at dataset","metadata":{}},{"cell_type":"code","source":"data_bc = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\nlabel_bc = data_bc[\"diagnosis\"]\nlabel_bc = np.where(label_bc == 'M',1,0)\ndata_bc.drop([\"id\",\"diagnosis\",\"Unnamed: 32\"],axis = 1,inplace = True)\n\nprint(\"Breast Cancer dataset:\\n\",data_bc.shape[0],\"Records\\n\",data_bc.shape[1],\"Features\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T10:38:42.716154Z","iopub.execute_input":"2021-06-26T10:38:42.716431Z","iopub.status.idle":"2021-06-26T10:38:43.835363Z","shell.execute_reply.started":"2021-06-26T10:38:42.716407Z","shell.execute_reply":"2021-06-26T10:38:43.834672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data_bc.head())\nprint(\"All the features in this dataset have continuous values\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T10:38:43.836602Z","iopub.execute_input":"2021-06-26T10:38:43.837006Z","iopub.status.idle":"2021-06-26T10:38:43.873595Z","shell.execute_reply.started":"2021-06-26T10:38:43.836967Z","shell.execute_reply":"2021-06-26T10:38:43.873007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Checking Accuracy","metadata":{}},{"cell_type":"code","source":"score1 = acc_score(data_bc,label_bc)\nscore1","metadata":{"execution":{"iopub.status.busy":"2021-06-26T10:38:43.874911Z","iopub.execute_input":"2021-06-26T10:38:43.875277Z","iopub.status.idle":"2021-06-26T10:38:47.72209Z","shell.execute_reply.started":"2021-06-26T10:38:43.875249Z","shell.execute_reply":"2021-06-26T10:38:47.720885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold_bc = [0.04,0.02,0.01,0.008,0.004,0.001]\nclassifiers = score1[\"Classifier\"].tolist()\nscore_bc = acc_score_thr(data_bc,label_bc,threshold_bc)\nscore_bc.style.apply(highlight_max, subset = score_bc.columns[1:], axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:22:05.248483Z","iopub.execute_input":"2021-06-26T11:22:05.248948Z","iopub.status.idle":"2021-06-26T11:22:30.367174Z","shell.execute_reply.started":"2021-06-26T11:22:05.248917Z","shell.execute_reply":"2021-06-26T11:22:30.366675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Accuracy with all features : RandomForest Classifier - 0.972\n#### Best Accuracy after applying with VarianceThreshold() : LinearSVM - for threshold = (0.04,0.02,0.01,0.008) - 0.979\n#### Here we can only see a slight improvement.","metadata":{}},{"cell_type":"markdown","source":"### 3. Visualization","metadata":{}},{"cell_type":"code","source":"plot2(score_bc,0.90,1,2.5,0.91,c = \"gold\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:22:30.368087Z","iopub.execute_input":"2021-06-26T11:22:30.368342Z","iopub.status.idle":"2021-06-26T11:22:31.183465Z","shell.execute_reply.started":"2021-06-26T11:22:30.368321Z","shell.execute_reply":"2021-06-26T11:22:31.182991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"______\n# Parkinson's disease\n_______","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at dataset","metadata":{}},{"cell_type":"code","source":"data_pd = pd.read_csv(\"../input/parkinson-disease-detection/Parkinsson disease.csv\")\nlabel_pd = data_pd[\"status\"]\ndata_pd.drop([\"status\",\"name\"],axis = 1,inplace = True)\n\nprint(\"Parkinson's disease dataset:\\n\",data_pd.shape[0],\"Records\\n\",data_pd.shape[1],\"Features\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T10:59:02.696782Z","iopub.execute_input":"2021-06-26T10:59:02.697039Z","iopub.status.idle":"2021-06-26T10:59:02.717247Z","shell.execute_reply.started":"2021-06-26T10:59:02.697015Z","shell.execute_reply":"2021-06-26T10:59:02.71622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data_pd.head())\nprint(\"All the features in this dataset have continuous values\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T10:59:02.719455Z","iopub.execute_input":"2021-06-26T10:59:02.719871Z","iopub.status.idle":"2021-06-26T10:59:02.750719Z","shell.execute_reply.started":"2021-06-26T10:59:02.7198Z","shell.execute_reply":"2021-06-26T10:59:02.749722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Checking Accuracy","metadata":{}},{"cell_type":"code","source":"score3 = acc_score(data_pd,label_pd)\nscore3","metadata":{"execution":{"iopub.status.busy":"2021-06-26T10:59:02.752008Z","iopub.execute_input":"2021-06-26T10:59:02.752278Z","iopub.status.idle":"2021-06-26T10:59:03.760305Z","shell.execute_reply.started":"2021-06-26T10:59:02.752251Z","shell.execute_reply":"2021-06-26T10:59:03.75919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold_pd = [0.05,0.01,0.005,0.001,0.0001,0.0005]\nclassifiers = score3[\"Classifier\"].tolist()\nscore_pd = acc_score_thr(data_pd,label_pd,threshold_pd)\nscore_pd.style.apply(highlight_max, subset = score_pd.columns[1:], axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:11:32.392011Z","iopub.execute_input":"2021-06-26T11:11:32.392447Z","iopub.status.idle":"2021-06-26T11:11:37.024519Z","shell.execute_reply.started":"2021-06-26T11:11:32.392411Z","shell.execute_reply":"2021-06-26T11:11:37.023862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Accuracy with all features : RandomForest Classifier - 0.918\n#### Best Accuracy after applying with VarianceThreshold() : RandomForest Classifier - for threshold = (0.005,0.0001,0.0005) - 0.938 and LiearSVM - for threshold = (0.0005) - 0.938\n#### Here we can see an improvement of 2%.","metadata":{}},{"cell_type":"markdown","source":"### 3. Visualization","metadata":{}},{"cell_type":"code","source":"plot2(score_pd,0.65,1.0,1,0.7,c = \"orange\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:22:31.826791Z","iopub.execute_input":"2021-06-26T11:22:31.827079Z","iopub.status.idle":"2021-06-26T11:22:32.587664Z","shell.execute_reply.started":"2021-06-26T11:22:31.827057Z","shell.execute_reply":"2021-06-26T11:22:32.586873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________\n# PCOS\n________","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at dataset","metadata":{}},{"cell_type":"code","source":"data_pcos = pd.read_csv(\"../input/pcos-dataset/PCOS_data.csv\")\nlabel_pcos = data_pcos[\"PCOS (Y/N)\"]\ndata_pcos.drop([\"Sl. No\",\"Patient File No.\",\"PCOS (Y/N)\",\"Unnamed: 44\",\"II    beta-HCG(mIU/mL)\",\"AMH(ng/mL)\"],axis = 1,inplace = True)\ndata_pcos[\"Marraige Status (Yrs)\"].fillna(data_pcos['Marraige Status (Yrs)'].describe().loc[['50%']][0], inplace = True) \ndata_pcos[\"Fast food (Y/N)\"].fillna(1, inplace = True) \n\nprint(\"PCOS dataset:\\n\",data_pcos.shape[0],\"Records\\n\",data_pcos.shape[1],\"Features\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:00:12.204508Z","iopub.execute_input":"2021-06-26T11:00:12.204869Z","iopub.status.idle":"2021-06-26T11:00:12.255875Z","shell.execute_reply.started":"2021-06-26T11:00:12.204822Z","shell.execute_reply":"2021-06-26T11:00:12.254784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data_pcos.head())\nprint(\"The features in this dataset have both discrete and continuous values\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:00:12.485318Z","iopub.execute_input":"2021-06-26T11:00:12.485618Z","iopub.status.idle":"2021-06-26T11:00:12.509182Z","shell.execute_reply.started":"2021-06-26T11:00:12.485592Z","shell.execute_reply":"2021-06-26T11:00:12.508018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Checking Accuracy","metadata":{}},{"cell_type":"code","source":"score4 = acc_score(data_pcos,label_pcos)\nscore4","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:00:14.543363Z","iopub.execute_input":"2021-06-26T11:00:14.54362Z","iopub.status.idle":"2021-06-26T11:01:10.667055Z","shell.execute_reply.started":"2021-06-26T11:00:14.543597Z","shell.execute_reply":"2021-06-26T11:01:10.665606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold_pcos = [0.17,0.19,0.21,0.23,0.5,0.8]\nclassifiers = score4[\"Classifier\"].tolist()\nscore_pcos = acc_score_thr(data_pcos,label_pcos,threshold_pcos)\nscore_pcos.style.apply(highlight_max, subset = score_pcos.columns[1:], axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:11:37.02578Z","iopub.execute_input":"2021-06-26T11:11:37.026222Z","iopub.status.idle":"2021-06-26T11:17:17.844284Z","shell.execute_reply.started":"2021-06-26T11:11:37.026189Z","shell.execute_reply":"2021-06-26T11:17:17.843767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Accuracy with all features : RandomForest Classifier - 0.889\n#### Best Accuracy after applying with VarianceThreshold() : DecisionTree Classifier - for threshold = (0.19) - 0.897\n#### Here we can see an improvement of ~1%.","metadata":{}},{"cell_type":"markdown","source":"### 3. Visualization","metadata":{}},{"cell_type":"code","source":"plot2(score_pcos,0.3,1.0,1,0.35,c = \"limegreen\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:22:32.588523Z","iopub.execute_input":"2021-06-26T11:22:32.588803Z","iopub.status.idle":"2021-06-26T11:22:33.475752Z","shell.execute_reply.started":"2021-06-26T11:22:32.588781Z","shell.execute_reply":"2021-06-26T11:22:33.475171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________","metadata":{}},{"cell_type":"markdown","source":"#### From looking at these results we can see that there is a possibility of slight improvement in the accuracy after removing certain features with low variance.\n#### Link to other feature selection methods:\n##### [Genetic Algorithm](https://www.kaggle.com/tanmayunhale/genetic-algorithm-for-feature-selection)\n##### [Pearson Correlation](https://www.kaggle.com/tanmayunhale/feature-selection-pearson-correlation)\n##### [F-score](https://www.kaggle.com/tanmayunhale/feature-selection-f-score)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}