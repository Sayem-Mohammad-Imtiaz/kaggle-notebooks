{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\n![image](https://miro.medium.com/max/1200/0*UEtwA2ask7vQYW06.png)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction and Imports\nn this notebook, I will be using only Machine Learning methods to get decent prediction scores. There are much better and sophisticated ways (like RNN, GRU, Fine-tuning BERT, etc) but you have seen them on a lot of notebook already.\n\nThe main aim of this notebook is to just show how quickly and easily you can do Text Classification using Basic Machine Learning Methods, rather than spend waiting 1 hour for a model to train!\n\nIf you like this notebook, please make sure to give an upvote, it helps a lot and motivates me to make much more good-quality content\n\nIf you don't like my work, please leave a comment on what can I do to make it better!"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"color:red\">If you like this notebook, please make sure to give an upvote, it helps a lot and motivates me to make much more good-quality content</p>\n<p style=\"color:blue\">If you don't like my work, please leave a comment on what can I do to make it better!</p>\n<hr>\n<h3 style=\"color:aqua\">Edits:</h3>\n<ul>\n<li style=\"color:green\">All Classifiers now classify for all 3 categories and not just 2. Good Validation Accuracy is maintained.</li>\n</ul>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,LabelBinarizer\nimport lightgbm as lgb\nimport catboost as ct\nimport sklearn\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.model_selection import KFold,RepeatedStratifiedKFold,RandomizedSearchCV,GridSearchCV,cross_val_score\nfrom sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer,ENGLISH_STOP_WORDS\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('../input/60k-stack-overflow-questions-with-quality-rate/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draft_dataset=dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.sort_values('CreationDate',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing and Some EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"lb=LabelEncoder()\nnew_data=lb.fit_transform(dataset.CreationDate)\n#dataset['DateCatCOl']=new_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(['Id','CreationDate'],axis=1,inplace=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.Y.value_counts().to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Y']=dataset.Y.map({'LQ_CLOSE': 0, 'LQ_EDIT': 1, 'HQ': 2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean_tags(T):\n    T=T.lower()\n    text=re.sub(r'<','',T)\n    text=re.sub(r'>',' ',text)\n    return text\n\ndataset['Tags']=dataset['Tags'].map(clean_tags)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.Tags.value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_v=CountVectorizer()\ntags_vecorized=count_v.fit_transform(dataset.Tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop('Tags',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    x=x.lower()\n    x=re.sub(r'<p>',\" \",x)\n    x=re.sub(r'[^(a-zA-Z)\\s]','', x)\n    x=x.strip(os.linesep)\n    x=re.sub(r'[\\n\\r]+', '', x)\n    x=x.strip()"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndef clean_body(x):\n    x=x.lower()\n    x=re.sub(r'[^(a-zA-Z)\\s]','', x)\n    return x\n\ndataset['Body']=dataset.Body.map(clean_body)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's join the title and the body of the text data so that we can use both of them in our classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['CombineTextandBody']=dataset['Title']+' '+dataset['Body']\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(['Title','Body'],axis=1,inplace=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label=dataset.pop('Y')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the Data\nLet's now split the dataset into training and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x,test_x,train_y,test_y=train_test_split(dataset,label,test_size=0.15,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape,test_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing the Data\nLet's vectorize the data so it's in the numerical format"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf=TfidfVectorizer()\ntransform_text_train=tfidf.fit_transform(train_x.CombineTextandBody)\ntransform_text_test=tfidf.transform(test_x.CombineTextandBody)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_text_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## created some folds "},{"metadata":{"trusted":true},"cell_type":"code","source":"rskf=RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling\nLet's start with different non-deep learning approaches for this task."},{"metadata":{},"cell_type":"markdown","source":"# 1. Logistic Regression\n\nLet's first start with our good old, Logistic Regression!"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_classifier = LogisticRegression(C=1.)\nlr_classifier.fit(transform_text_train, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_text_test.shape,test_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Validation Accuracy of Logsitic Regression Classifier is: {(lr_classifier.score(transform_text_test, test_y))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=cross_val_score(lr_classifier,transform_text_train, train_y,cv=3,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cross-validation score of logistic classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. XGBoost\nFinally, let's use the XGBoost Classifier and then we'll compare all the different classifiers so far"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_classifier = XGBClassifier(n_estimators=500,n_jobs=-1,random_state=42)\nxg_classifier.fit(transform_text_train, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print the accuracy score of the XG boost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Validation Accuracy of XGBoost Clf. is: {(xg_classifier.score(transform_text_test, test_y))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Multinomial Naive Bayes\nLet's now switch to the naive the bayes, the NAIVE BAYES!"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_classifier = MultinomialNB()\nnb_classifier.fit(transform_text_train, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print the accuracy score of the naive bayes classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(f\"Validation Accuracy of Naive Bayes Classifier is: {(nb_classifier.score(transform_text_test, test_y))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Light GBM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model=lgb.LGBMClassifier()\nlgb_model.fit(transform_text_train, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print the accuracy score of the lgb_model classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(f\"Validation Accuracy of lgb_model Classifier is: {(lgb_model.score(transform_text_test, test_y))*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper parameter tuning of light GBM "},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n            'learning_rate':[0.1,0.01,0.05,0.001,0.005,0.03,0.003,0.006,0.08]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RS=RandomizedSearchCV(\n    estimator=lgb_model, param_distributions=param_test,\n    cv=3,\n    refit=True,\n    random_state=42,\n    verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RS.fit(transform_text_train, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### best parameters and best score of LIght GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"RS.best_estimator_,RS.best_params_,RS.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: since we got very good result with logistic regression we make it simple beacuase light GBM takes lot of time to train and find optimum result. hence we can compromise with some accuracy and avoid some complexity we can go with logistic Regression.**\n\nTip:- From my experiance most of the time we generally go for complex model but we should always start with some basic model if they dont work then we should go for some complex models."},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter_list={\n    'C':[0.10,0.6,0,3.0,4.0,5.,6.,9.,0.11,0.12,0.15,0.14,0.20],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nlr_classifier_2 = LogisticRegression(n_jobs=-1,random_state=42)\n\nlog_tune=RandomizedSearchCV(\n    estimator=lr_classifier_2, param_distributions=parameter_list,\n    cv=3,\n    refit=True,\n    random_state=42,\n    verbose=True)\n\nlog_tune.fit(transform_text_train, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best Parameter and best score of logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"log_tune.best_estimator_,log_tune.best_params_,log_tune.best_score_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}