{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-15T22:49:08.076197Z","iopub.execute_input":"2021-07-15T22:49:08.07658Z","iopub.status.idle":"2021-07-15T22:49:08.097503Z","shell.execute_reply.started":"2021-07-15T22:49:08.076546Z","shell.execute_reply":"2021-07-15T22:49:08.096331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Leitura dos dados","metadata":{}},{"cell_type":"code","source":"train_tweets = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Tweets/train_tweets.csv',index_col=0)\ntrain_tweets_vectorized_text = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Tweets/train_tweets_vectorized_text.csv',index_col=0)\n\n\ntest_tweets = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Tweets/test_tweets.csv',index_col=0)\ntest_tweets_vectorized_text = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Tweets/test_tweets_vectorized_text.csv',index_col=0)\n\nuser_vectorized_descriptions = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Users/user_vectorized_descriptions.csv',index_col=0)\nuser_vectorized_profile_images = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Users/user_vectorized_profile_images.csv',index_col=0)\nusers = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Users/users.csv',index_col=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:08.945655Z","iopub.execute_input":"2021-07-15T22:49:08.946008Z","iopub.status.idle":"2021-07-15T22:49:18.069033Z","shell.execute_reply.started":"2021-07-15T22:49:08.945977Z","shell.execute_reply":"2021-07-15T22:49:18.068011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pré-processamento dos dados","metadata":{}},{"cell_type":"markdown","source":"## Redução de dimensionalidade: PCA","metadata":{}},{"cell_type":"markdown","source":" Como algumas bases de dados possuem milhares de colunas, e são colunas que não possuem interpretação, é o caso perfeito para aplicação de PCA. Nesse caso foi aplicado PCA em cada base separada para dimunir o numero de features da casa de milhares para casa de dezenas.","metadata":{}},{"cell_type":"code","source":"user_vectorized_descriptions_pc = pd.DataFrame(PCA(n_components=10).fit_transform(normalize(user_vectorized_descriptions)),index=user_vectorized_descriptions.index,columns=user_vectorized_descriptions.columns[0:10])\nuser_vectorized_profile_images_pc = pd.DataFrame(PCA(n_components=5).fit_transform(normalize(user_vectorized_profile_images)),index=user_vectorized_profile_images.index,columns=user_vectorized_profile_images.columns[0:5])\n\npca_tweets = PCA(n_components=25).fit(normalize(train_tweets_vectorized_text))\n\ntrain_tweets_vectorized_text_pc = pd.DataFrame(pca_tweets.transform(normalize(train_tweets_vectorized_text)),index=train_tweets_vectorized_text.index,columns=train_tweets_vectorized_text.columns[0:25])\ntest_tweets_vectorized_text_pc = pd.DataFrame(pca_tweets.transform(normalize(test_tweets_vectorized_text)),index=test_tweets_vectorized_text.index,columns=test_tweets_vectorized_text.columns[0:25])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:18.07113Z","iopub.execute_input":"2021-07-15T22:49:18.071588Z","iopub.status.idle":"2021-07-15T22:49:20.793654Z","shell.execute_reply.started":"2021-07-15T22:49:18.071541Z","shell.execute_reply":"2021-07-15T22:49:20.792444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unindo as bases","metadata":{}},{"cell_type":"markdown","source":"Após a redução de dimensionalidade, é necessário juntar as bases para no final ter uma somente uma base de treino e uma de teste. Primeiramente as bases \"user_vectorized_descriptions_pc\" e \"user_vectorized_profile_images_pc\" foram unidas na base \"users\" por meio de um left join.","metadata":{}},{"cell_type":"code","source":"users = users.merge(user_vectorized_descriptions_pc, on = \"user_id\", how = \"left\")\nusers = users.merge(user_vectorized_profile_images_pc, on = \"user_id\", how = \"left\")","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:20.80109Z","iopub.execute_input":"2021-07-15T22:49:20.804929Z","iopub.status.idle":"2021-07-15T22:49:20.832197Z","shell.execute_reply.started":"2021-07-15T22:49:20.804861Z","shell.execute_reply":"2021-07-15T22:49:20.831069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Após isso a base \"train_tweets_vectorized_text_pc\" foi unida na \"train_tweets\" e a base \"test_tweets_vectorized_text_pc\" foi unida na \"test_tweets\" por meio de um left join.","metadata":{}},{"cell_type":"code","source":"train = train_tweets.merge(train_tweets_vectorized_text_pc, on = \"tweet_id\", how = \"left\")\ntrain = train.merge(users, how='left', left_on='tweet_user_id', right_on='user_id')\n\ntest = test_tweets.merge(test_tweets_vectorized_text_pc, on = \"tweet_id\", how = \"left\")\ntest = test.merge(users, how='left', left_on='tweet_user_id', right_on='user_id')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:20.839207Z","iopub.execute_input":"2021-07-15T22:49:20.843562Z","iopub.status.idle":"2021-07-15T22:49:20.935772Z","shell.execute_reply.started":"2021-07-15T22:49:20.843486Z","shell.execute_reply":"2021-07-15T22:49:20.934978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tratamento","metadata":{}},{"cell_type":"markdown","source":"## Removendo colunas","metadata":{}},{"cell_type":"code","source":"train.tweet_topic_ids.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:20.939658Z","iopub.execute_input":"2021-07-15T22:49:20.940142Z","iopub.status.idle":"2021-07-15T22:49:20.951025Z","shell.execute_reply.started":"2021-07-15T22:49:20.940096Z","shell.execute_reply":"2021-07-15T22:49:20.94982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A coluna \"tweet_topic_ids\" foi removida por apresentar NA e por conter muitas categorias.","metadata":{}},{"cell_type":"code","source":"train.drop('tweet_topic_ids', axis=1, inplace=True)\ntest.drop('tweet_topic_ids', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:20.952168Z","iopub.execute_input":"2021-07-15T22:49:20.952483Z","iopub.status.idle":"2021-07-15T22:49:20.985986Z","shell.execute_reply.started":"2021-07-15T22:49:20.952454Z","shell.execute_reply":"2021-07-15T22:49:20.98506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformando colunas booleanas em inteiras","metadata":{}},{"cell_type":"markdown","source":"Algumas colunas que estavam em formato booleano foram transformadas em numeric para melhor aplicabilidade nos modelos.","metadata":{}},{"cell_type":"code","source":"train.tweet_has_attachment = train.tweet_has_attachment*1\ntest.tweet_has_attachment = test.tweet_has_attachment*1\n\ntrain.user_has_url = train.user_has_url*1\ntest.user_has_url = test.user_has_url*1\n\ntrain.user_has_location = train.user_has_location*1\ntest.user_has_location = test.user_has_location*1","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:20.987265Z","iopub.execute_input":"2021-07-15T22:49:20.987616Z","iopub.status.idle":"2021-07-15T22:49:20.999217Z","shell.execute_reply.started":"2021-07-15T22:49:20.987585Z","shell.execute_reply":"2021-07-15T22:49:20.998126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One hot encoding para variáveis categóricas","metadata":{}},{"cell_type":"markdown","source":"A coluna \"tweet_attachment_class\" foi desmembrada em outras 3 columas por meio de one hot enconding para melhor aplicabilidade nos modelos.","metadata":{}},{"cell_type":"code","source":"dumm_tweet_attachment_class_train = pd.get_dummies(train['tweet_attachment_class'])\ndumm_tweet_attachment_class_test = pd.get_dummies(test['tweet_attachment_class'])\n\n\ntrain = pd.concat([train,dumm_tweet_attachment_class_train], axis=1)\ntrain.drop(columns=['tweet_attachment_class'], inplace=True)\n\ntest = pd.concat([test,dumm_tweet_attachment_class_test], axis=1)\ntest.drop(columns=['tweet_attachment_class'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:21.001696Z","iopub.execute_input":"2021-07-15T22:49:21.002111Z","iopub.status.idle":"2021-07-15T22:49:21.047754Z","shell.execute_reply.started":"2021-07-15T22:49:21.002069Z","shell.execute_reply":"2021-07-15T22:49:21.04655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verificando se há NA","metadata":{}},{"cell_type":"markdown","source":"Após todos os tratamentos foi verificado se as bases de treino e testes continham valores faltantes e foi constatado que não","metadata":{}},{"cell_type":"code","source":"test.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:21.049287Z","iopub.execute_input":"2021-07-15T22:49:21.04962Z","iopub.status.idle":"2021-07-15T22:49:21.059827Z","shell.execute_reply.started":"2021-07-15T22:49:21.049586Z","shell.execute_reply":"2021-07-15T22:49:21.058571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parte da base de treino final.","metadata":{}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:21.061189Z","iopub.execute_input":"2021-07-15T22:49:21.061531Z","iopub.status.idle":"2021-07-15T22:49:21.104435Z","shell.execute_reply.started":"2021-07-15T22:49:21.061501Z","shell.execute_reply":"2021-07-15T22:49:21.1033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelagem","metadata":{}},{"cell_type":"markdown","source":"## Criação de base de teste \"gerada\"","metadata":{}},{"cell_type":"markdown","source":"Como não consegui submeter a tempo para a competição oficial separei uma parte da base de treino para verificação da acurácia somente no final de tudo. No restante da base de treino normal utilizei Cross Validation para encontrar os melhores hiper-parametros.","metadata":{}},{"cell_type":"code","source":"(train_model, test_fk) = train_test_split(train, test_size = 0.15,random_state=57)\n\ny_train = train_model['virality']\nX_train = train_model.drop(['virality','tweet_user_id'],1)\n\ny_test_fk = test_fk['virality']\nX_test_fk = test_fk.drop(['virality','tweet_user_id'],1)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:23.095494Z","iopub.execute_input":"2021-07-15T22:49:23.095838Z","iopub.status.idle":"2021-07-15T22:49:23.126544Z","shell.execute_reply.started":"2021-07-15T22:49:23.095809Z","shell.execute_reply":"2021-07-15T22:49:23.125747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para o processo de modelogem vamos testar varios modelos e diferentes, e após isso vamos testar algumas técnicas de ensemble com esses modelos.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Árvore de decisão","metadata":{}},{"cell_type":"code","source":"#parameters = {'max_depth': [9], 'max_leaf_nodes': [87], 'min_samples_leaf': [0.008889], 'min_samples_split': [0.04737368421052632]}\n\n#parameters = {'max_depth':list(range(2,12)), 'max_leaf_nodes': list(range(2,90))}\n\nparameters = {'max_depth': [11], 'max_leaf_nodes': [60]}\n\n\nbest_params=[]\nbest_score = 0\n\ni=0\n# Treinamento\nparameters_g = shuffle(list(ParameterGrid(parameters)))\nfor param in list(parameters_g):\n    model_dt = tree.DecisionTreeClassifier(max_depth= param['max_depth'],\n                                          max_leaf_nodes = param['max_leaf_nodes'])\n    \n    #model_dt.fit(X_train,y_train)\n    #score = balanced_accuracy_score(y_val, model_dt.predict(X_val))\n\n    # Validacao\n    \n    score_cv = cross_val_score(model_dt, X_train, y_train, cv=10,scoring='accuracy')\n    score = np.mean(score_cv)\n    \n    \n    if score > best_score:\n        best_score = score\n        best_params = param\n        best_model_dt = model_dt\n\n        \n    i+=1\n    if(i%10 == 0):\n        print((i/len(list(ParameterGrid(parameters))))*100,\"Best score:\", best_score,\"Best params:\", best_params)\n\nbest_model_dt.fit(X_train, y_train)\n\nprint(\"Best params:\", best_params)\nprint(\"Best score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:36:22.788187Z","iopub.execute_input":"2021-07-15T22:36:22.788687Z","iopub.status.idle":"2021-07-15T22:36:30.430228Z","shell.execute_reply.started":"2021-07-15T22:36:22.788654Z","shell.execute_reply":"2021-07-15T22:36:30.429079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Florestas Aleatórias","metadata":{}},{"cell_type":"code","source":"#parameters = {'max_depth':np.arange(5, 30, 2), \"n_estimators\":[50],  \"criterion\":[\"gini\", 'entropy']}\n\nparameters = {'criterion': ['entropy'], 'max_depth': [29], 'n_estimators': [200]}\n\nbest_params=[]\nbest_score = 0\n\ni=0\n# Treinamento\nparameters_g = shuffle(list(ParameterGrid(parameters)))\nfor param in list(parameters_g):\n    model_rf = RandomForestClassifier( max_depth= param['max_depth'],\n                                      n_estimators= param['n_estimators'],\n                                      criterion = param['criterion'])\n\n    #model_rf.fit(X_train,y_train)\n    #score = balanced_accuracy_score(y_val, model_rf.predict(X_val))\n\n    # Validacao\n    \n    \n    score_cv = cross_val_score(model_rf, X_train, y_train, cv=10,scoring='accuracy',n_jobs=-1)\n    score = np.mean(score_cv)\n    \n    if score > best_score:\n        best_score = score\n        best_params = param\n        best_model_rf = model_rf\n\n        \n    i+=1\n    print((i/len(list(ParameterGrid(parameters))))*100,\"Best score:\", best_score,\"Best params:\", best_params)\n\nbest_model_rf.fit(X_train, y_train)\n\nprint(\"Best params:\", best_params)\nprint(\"Best score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:49:33.92773Z","iopub.execute_input":"2021-07-15T22:49:33.928307Z","iopub.status.idle":"2021-07-15T22:53:42.928813Z","shell.execute_reply.started":"2021-07-15T22:49:33.92824Z","shell.execute_reply":"2021-07-15T22:53:42.927992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"#model_xgb = XGBClassifier()\n\n#score_cv = cross_val_score(model_xgb, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n#print(\"CV score:\", np.mean(score_cv))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:28:09.418614Z","iopub.execute_input":"2021-07-15T21:28:09.419256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bagging (Árvore de decisão)\n","metadata":{}},{"cell_type":"code","source":"model_bag = BaggingClassifier(n_estimators=50)\nscore_cv = cross_val_score(model_bag, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\nprint(\"CV score:\", np.mean(score_cv))\n\nmodel_bag.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:16:04.539648Z","iopub.execute_input":"2021-07-15T21:16:04.540172Z","iopub.status.idle":"2021-07-15T21:17:40.518864Z","shell.execute_reply.started":"2021-07-15T21:16:04.540133Z","shell.execute_reply":"2021-07-15T21:17:40.517689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Voting Classifier\n","metadata":{}},{"cell_type":"code","source":"model_vot = VotingClassifier(estimators=[('rf', best_model_rf),\n                                         ('bag', model_bag)],\n                             voting='soft', n_jobs=-1)\nscore_cv = cross_val_score(model_vot, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\nprint(\"CV score:\", np.mean(score_cv))\n\nmodel_vot.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:20:12.778968Z","iopub.execute_input":"2021-07-15T21:20:12.779409Z","iopub.status.idle":"2021-07-15T21:22:23.798754Z","shell.execute_reply.started":"2021-07-15T21:20:12.779362Z","shell.execute_reply":"2021-07-15T21:22:23.797614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simulação de dados Teste","metadata":{}},{"cell_type":"code","source":"melhor_modelo_cv = best_model_rf\n\nprint(\"Test score:\", accuracy_score(y_test_fk,melhor_modelo_cv.predict(X_test_fk)))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T22:54:35.888724Z","iopub.execute_input":"2021-07-15T22:54:35.889129Z","iopub.status.idle":"2021-07-15T22:54:36.220402Z","shell.execute_reply.started":"2021-07-15T22:54:35.889092Z","shell.execute_reply":"2021-07-15T22:54:36.219121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gerar csv submissão","metadata":{}},{"cell_type":"code","source":"X_test = test.drop(['tweet_user_id'],1)\n\npredictions = melhor_modelo_cv.predict(X_test)\n\npredictionsDF = pd.DataFrame({\"tweet_id\":X_test.index, \n                                     \"virality\":predictions})\npredictionsDF.to_csv(\"resposta.csv\",index = False, sep = \",\", decimal = \",\", float_format = str)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:27:55.65857Z","iopub.execute_input":"2021-07-15T21:27:55.658959Z","iopub.status.idle":"2021-07-15T21:27:56.475807Z","shell.execute_reply.started":"2021-07-15T21:27:55.658917Z","shell.execute_reply":"2021-07-15T21:27:56.474994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictionsDF","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:27:58.409253Z","iopub.execute_input":"2021-07-15T21:27:58.409949Z","iopub.status.idle":"2021-07-15T21:27:58.423562Z","shell.execute_reply.started":"2021-07-15T21:27:58.409909Z","shell.execute_reply":"2021-07-15T21:27:58.422552Z"},"trusted":true},"execution_count":null,"outputs":[]}]}