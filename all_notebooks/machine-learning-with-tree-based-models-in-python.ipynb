{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification and Regression Trees"},{"metadata":{},"cell_type":"markdown","source":"### Train your first classification tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nwbc = pd.read_csv('../input/ninechapter-breastcancer/breastCancer.csv')\nX = wbc[['radius_mean', 'concave points_mean']]\ny = wbc['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier \n\nSEED = 1 \n\n# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\ndt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\nprint(y_pred[0:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the classification tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import accuracy_score\nfrom sklearn.metrics import accuracy_score\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\n\n# Compute test set accuracy  \nacc = accuracy_score(y_test, y_pred)\nprint(\"Test set accuracy: {:.2f}\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression vs classification tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_decision_regions(X, y, clf,\n                          feature_index=None,\n                          filler_feature_values=None,\n                          filler_feature_ranges=None,\n                          ax=None,\n                          X_highlight=None,\n                          res=0.02, legend=1,\n                          hide_spines=True,\n                          markers='s^oxv<>',\n                          colors='red,blue,limegreen,gray,cyan'):\n    \"\"\"Plot decision regions of a classifier.\n\n    Please note that this functions assumes that class labels are\n    labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class\n    labels with integer labels > 4, you may want to provide additional colors\n    and/or markers as `colors` and `markers` arguments.\n    See http://matplotlib.org/examples/color/named_colors.html for more\n    information.\n\n    Parameters\n    ----------\n    X : array-like, shape = [n_samples, n_features]\n        Feature Matrix.\n    y : array-like, shape = [n_samples]\n        True class labels.\n    clf : Classifier object.\n        Must have a .predict method.\n    feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise)\n        Feature indices to use for plotting. The first index in\n        `feature_index` will be on the x-axis, the second index will be\n        on the y-axis.\n    filler_feature_values : dict (default: None)\n        Only needed for number features > 2. Dictionary of feature\n        index-value pairs for the features not being plotted.\n    filler_feature_ranges : dict (default: None)\n        Only needed for number features > 2. Dictionary of feature\n        index-value pairs for the features not being plotted. Will use the\n        ranges provided to select training samples for plotting.\n    ax : matplotlib.axes.Axes (default: None)\n        An existing matplotlib Axes. Creates\n        one if ax=None.\n    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n        An array with data points that are used to highlight samples in `X`.\n    res : float or array-like, shape = (2,) (default: 0.02)\n        Grid width. If float, same resolution is used for both the x- and\n        y-axis. If array-like, the first item is used on the x-axis, the\n        second is used on the y-axis. Lower values increase the resolution but\n        slow down the plotting.\n    hide_spines : bool (default: True)\n        Hide axis spines if True.\n    legend : int (default: 1)\n        Integer to specify the legend location.\n        No legend if legend is 0.\n    markers : str (default 's^oxv<>')\n        Scatterplot markers.\n    colors : str (default 'red,blue,limegreen,gray,cyan')\n        Comma separated list of colors.\n\n    Returns\n    ---------\n    ax : matplotlib.axes.Axes object\n\n    \"\"\"\n\n    check_Xy(X, y, y_int=True)  # Validate X and y arrays\n    dim = X.shape[1]\n\n    if ax is None:\n        ax = plt.gca()\n\n    if isinstance(res, float):\n        xres, yres = res, res\n    else:\n        try:\n            xres, yres = res\n        except ValueError:\n            raise ValueError('Unable to unpack res. Expecting '\n                             'array-like input of length 2.')\n\n    plot_testdata = True\n    if not isinstance(X_highlight, np.ndarray):\n        if X_highlight is not None:\n            raise ValueError('X_highlight must be a NumPy array or None')\n        else:\n            plot_testdata = False\n    elif len(X_highlight.shape) < 2:\n        raise ValueError('X_highlight must be a 2D array')\n\n    if feature_index is not None:\n        # Unpack and validate the feature_index values\n        if dim == 1:\n            raise ValueError(\n                'feature_index requires more than one training feature')\n        try:\n            x_index, y_index = feature_index\n        except ValueError:\n            raise ValueError(\n                'Unable to unpack feature_index. Make sure feature_index '\n                'only has two dimensions.')\n        try:\n            X[:, x_index], X[:, y_index]\n        except IndexError:\n            raise IndexError(\n                'feature_index values out of range. X.shape is {}, but '\n                'feature_index is {}'.format(X.shape, feature_index))\n    else:\n        feature_index = (0, 1)\n        x_index, y_index = feature_index\n\n    # Extra input validation for higher number of training features\n    if dim > 2:\n        if filler_feature_values is None:\n            raise ValueError('Filler values must be provided when '\n                             'X has more than 2 training features.')\n\n        if filler_feature_ranges is not None:\n            if not set(filler_feature_values) == set(filler_feature_ranges):\n                raise ValueError(\n                    'filler_feature_values and filler_feature_ranges must '\n                    'have the same keys')\n\n        # Check that all columns in X are accounted for\n        column_check = np.zeros(dim, dtype=bool)\n        for idx in filler_feature_values:\n            column_check[idx] = True\n        for idx in feature_index:\n            column_check[idx] = True\n        if not all(column_check):\n            missing_cols = np.argwhere(~column_check).flatten()\n            raise ValueError(\n                'Column(s) {} need to be accounted for in either '\n                'feature_index or filler_feature_values'.format(missing_cols))\n\n    marker_gen = cycle(list(markers))\n\n    n_classes = np.unique(y).shape[0]\n    colors = colors.split(',')\n    colors_gen = cycle(colors)\n    colors = [next(colors_gen) for c in range(n_classes)]\n\n    # Get minimum and maximum\n    x_min, x_max = X[:, x_index].min() - 1, X[:, x_index].max() + 1\n    if dim == 1:\n        y_min, y_max = -1, 1\n    else:\n        y_min, y_max = X[:, y_index].min() - 1, X[:, y_index].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xres),\n                         np.arange(y_min, y_max, yres))\n\n    if dim == 1:\n        X_predict = np.array([xx.ravel()]).T\n    else:\n        X_grid = np.array([xx.ravel(), yy.ravel()]).T\n        X_predict = np.zeros((X_grid.shape[0], dim))\n        X_predict[:, x_index] = X_grid[:, 0]\n        X_predict[:, y_index] = X_grid[:, 1]\n        if dim > 2:\n            for feature_idx in filler_feature_values:\n                X_predict[:, feature_idx] = filler_feature_values[feature_idx]\n    Z = clf.predict(X_predict)\n    Z = Z.reshape(xx.shape)\n    # Plot decisoin region\n    ax.contourf(xx, yy, Z,\n                alpha=0.3,\n                colors=colors,\n                levels=np.arange(Z.max() + 2) - 0.5)\n\n    ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n\n    # Scatter training data samples\n    for idx, c in enumerate(np.unique(y)):\n        if dim == 1:\n            y_data = [0 for i in X[y == c]]\n            x_data = X[y == c]\n        elif dim == 2:\n            y_data = X[y == c, y_index]\n            x_data = X[y == c, x_index]\n        elif dim > 2 and filler_feature_ranges is not None:\n            class_mask = y == c\n            feature_range_mask = get_feature_range_mask(\n                            X, filler_feature_values=filler_feature_values,\n                            filler_feature_ranges=filler_feature_ranges)\n            y_data = X[class_mask & feature_range_mask, y_index]\n            x_data = X[class_mask & feature_range_mask, x_index]\n        else:\n            continue\n\n        ax.scatter(x=x_data,\n                   y=y_data,\n                   alpha=0.8,\n                   c=colors[idx],\n                   marker=next(marker_gen),\n                   edgecolor='black',\n                   label=c)\n\n    if hide_spines:\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n    ax.yaxis.set_ticks_position('left')\n    ax.xaxis.set_ticks_position('bottom')\n    if dim == 1:\n        ax.axes.get_yaxis().set_ticks([])\n\n    if legend:\n        if dim > 2 and filler_feature_ranges is None:\n            pass\n        else:\n            handles, labels = ax.get_legend_handles_labels()\n            ax.legend(handles, labels,\n                      framealpha=0.3, scatterpoints=1, loc=legend)\n\n    if plot_testdata:\n        if dim == 1:\n            x_data = X_highlight\n            y_data = [0 for i in X_highlight]\n        elif dim == 2:\n            x_data = X_highlight[:, x_index]\n            y_data = X_highlight[:, y_index]\n        else:\n            feature_range_mask = get_feature_range_mask(\n                    X_highlight, filler_feature_values=filler_feature_values,\n                    filler_feature_ranges=filler_feature_ranges)\n            y_data = X_highlight[feature_range_mask, y_index]\n            x_data = X_highlight[feature_range_mask, x_index]\n\n        ax.scatter(x_data,\n                   y_data,\n                   c='',\n                   edgecolor='black',\n                   alpha=1.0,\n                   linewidths=1,\n                   marker='o',\n                   s=80)\n\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_labeled_decision_regions(X,y, models):    \n    '''\n    Function producing a scatter plot of the instances contained \n    in the 2D dataset (X,y) along with the decision \n    regions of two trained classification models contained in the\n    list 'models'.\n            \n    Parameters\n    ----------\n    X: pandas DataFrame corresponding to two numerical features \n    y: pandas Series corresponding the class labels\n    models: list containing two trained classifiers \n    \n    '''\n    if len(models) != 2:\n        raise Exception('''\n        Models should be a list containing only two trained classifiers.\n        ''')\n    if not isinstance(X, pd.DataFrame):\n        raise Exception('''\n        X has to be a pandas DataFrame with two numerical features.\n        ''')\n    if not isinstance(y, pd.Series):\n        raise Exception('''\n        y has to be a pandas Series corresponding to the labels.\n        ''')\n    fig, ax = plt.subplots(1, 2, figsize=(6.0,2.7), sharey=True)\n    for i, model in enumerate(models):\n        plot_decision_regions(X.values,y.values, model, legend= 2, ax = ax[i])\n        ax[i].set_title(model.__class__.__name__)\n        ax[i].set_xlabel(X.columns[0])\n        if i == 0:\n            ax[i].set_ylabel(X.columns[1])\n        ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n        ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import LogisticRegression from sklearn.linear_model\nfrom sklearn.linear_model import LogisticRegression\n\n# Instatiate logreg\nlogreg = LogisticRegression(solver = 'liblinear', random_state=1)\n\n# Fit logreg to the training set\nlogreg.fit(X_train, y_train)\n\n# Define a list called clfs containing the two classifiers logreg and dt\nclfs = [logreg, dt]\n\n# Review the decision regions of the two classifiers\n# plot_labeled_decision_regions(X_test, y_test, clfs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using entropy as a criterion"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = wbc[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']]\ny = wbc['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth = 8, criterion='entropy', random_state=1)\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate dt_gini, set 'gini' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth = 8, criterion='gini', random_state=1)\n\n# Fit dt_gini to the training set\ndt_gini.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Entropy vs Gini index"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import accuracy_score from sklearn.metrics\nfrom sklearn.metrics import accuracy_score\n\n# Use dt_entropy to predict test set labels\ny_pred = dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n# Print accuracy_entropy\nprint('Accuracy achieved by using entropy: ', accuracy_entropy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use dt_entropy to predict test set labels\ny_pred = dt_gini.predict(X_test)\n\n# Evaluate accuracy_gini\naccuracy_gini = accuracy_score(y_test, y_pred)\n\n# Print accuracy_gini\nprint('Accuracy achieved by using the gini index: ', accuracy_gini)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train your first regression tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"auto = pd.read_csv('../input/automobile/auto.csv')\nauto.columns\nauto_origin = pd.get_dummies(auto.origin)\nauto = pd.concat([auto, auto_origin], axis = 1).drop('origin', axis = 1)\nauto.columns = ['mpg', 'displ', 'hp', 'weight', 'accel', 'size', 'origin_Asia', 'origin_Europe', 'origin_US']\nauto.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = auto[['displ', 'hp', 'weight', 'accel', 'size', 'origin_Asia',\n       'origin_Europe', 'origin_US']]\ny = auto['mpg']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import DecisionTreeRegressor from sklearn.tree\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Instantiate dt\ndt = DecisionTreeRegressor(max_depth=8,\n             min_samples_leaf=0.13,\n            random_state=3)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the regression tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import mean_squared_error from sklearn.metrics as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute y_pred\ny_pred = dt.predict(X_test)\n\n# Compute mse_dt\nmse_dt = MSE(y_test, y_pred)\n\n# Compute rmse_dt\nrmse_dt = (mse_dt)**0.5\n\n# Print rmse_dt\nprint(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear regression vs regression tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict test set labels \ny_pred_lr = lr.predict(X_test)\n\n# Compute mse_lr\nmse_lr = MSE(y_test, y_pred_lr)\n\n# Compute rmse_lr\nrmse_lr = mse_lr**(1/2)\n\n# Print rmse_lr\nprint('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Bias-Variance Tradeoff"},{"metadata":{},"cell_type":"markdown","source":"### Instantiate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import train_test_split from sklearn.model_selection\nfrom sklearn.model_selection import train_test_split\n\n# Set SEED for reproducibility\nSEED = 1\n\n# Split the data into 70% train and 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\n# Instantiate a DecisionTreeRegressor dt\ndt = DecisionTreeRegressor(max_depth = 4, min_samples_leaf = 0.26, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the 10-fold CV error"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n# Compute the array containing the 10-folds CV MSEs\nMSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n                                  scoring='neg_mean_squared_error', \n                                  n_jobs=-1) \n\n# Compute the 10-folds CV RMSE\nRMSE_CV = (MSE_CV_scores.mean())**(1/2)\n\n# Print RMSE_CV\nprint('CV RMSE: {:.2f}'.format(RMSE_CV))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the training error"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import mean_squared_error from sklearn.metrics as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict the labels of the training set\ny_pred_train = dt.predict(X_train)\n\n# Evaluate the training set RMSE of dt\nRMSE_train = (MSE(y_train, y_pred_train))**(0.5)\n\n# Print RMSE_train\nprint('Train RMSE: {:.2f}'.format(RMSE_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define the ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"liver = pd.read_csv('../input/indian-liver-patient-preprocessed/indian_liver_patient_preprocessed.csv', index_col = 0)\nX = liver.drop('Liver_disease', axis = 1)\ny = liver['Liver_disease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\nliver.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier as KNN\n# Set seed for reproducibility\nSEED=1\n\n# Instantiate lr\nlr = LogisticRegression(random_state=SEED, solver = 'liblinear')\n\n# Instantiate knn\nknn = KNN(n_neighbors=27)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n\n# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate individual classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate over the pre-defined list of classifiers\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)\n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy =accuracy_score(y_test, y_pred)\n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Better performance with a Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers)     \n\n# Fit vc to the training set\nvc.fit(X_train, y_train)\n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\nprint('Voting Classifier: {:.3f}'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging and Random Forests"},{"metadata":{},"cell_type":"markdown","source":"### Define the bagging classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(random_state=1)\n\n# Instantiate bc\nbc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Bagging performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit bc to the training set\nbc.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = bc.predict(X_test)\n\n# Evaluate acc_test\nacc_test = accuracy_score(y_test, y_pred)\nprint('Test set accuracy of bc: {:.2f}'.format(acc_test)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the ground"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n\n# Instantiate bc\nbc = BaggingClassifier(base_estimator=dt, \n            n_estimators=50,\n            oob_score=True,\n            random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### OOB Score vs Test Set Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit bc to the training set \nbc.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = bc.predict(X_test)\n\n# Evaluate test set accuracy\nacc_test = accuracy_score(y_test, y_pred)\n\n# Evaluate OOB accuracy\nacc_oob = bc.oob_score_\n\n# Print acc_test and acc_oob\nprint('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train an RF regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"bike = pd.read_csv('../input/bikesdata/bikes.csv')\nX = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\ny = bike['cnt']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate rf\nrf = RandomForestRegressor(n_estimators=25,\n            random_state=2)\n            \n# Fit rf to the training set    \nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the RF regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Predict the test set labels\ny_pred = rf.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_test = (MSE(y_test, y_pred))**0.5\n\n# Print rmse_test\nprint('Test set RMSE of rf: {:.2f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing features importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind = 'barh', color = 'lightgreen')\nplt.title('Features Importances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting"},{"metadata":{},"cell_type":"markdown","source":"### Define the AdaBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth = 2, random_state=1)\n\n# Instantiate ada\nada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the AdaBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = liver.drop('Liver_disease', axis = 1)\ny = liver['Liver_disease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit ada to the training set\nada.fit(X_train, y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_proba = ada.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the AdaBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import roc_auc_score\nfrom sklearn.metrics import roc_auc_score\n\n# Evaluate test-set roc_auc_score\nada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_roc_auc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define the GB regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor \n\n# Instantiate gb\ngb = GradientBoostingRegressor(max_depth = 4, \n            n_estimators = 200,\n            random_state=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the GB regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\ny = bike['cnt']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit gb to the training set\ngb.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = gb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the GB regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute MSE\nmse_test = MSE(y_test, y_pred)\n\n# Compute RMSE\nrmse_test = mse_test ** 0.5\n\n# Print RMSE\nprint('Test set RMSE of gb: {:.3f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression with SGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Instantiate sgbr\nsgbr = GradientBoostingRegressor(max_depth=4, \n            subsample=0.9,\n            max_features=0.75,\n            n_estimators=200,                                \n            random_state=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the SGB regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit sgbr to the training set\nsgbr.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = sgbr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the SGB regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute test set MSE\nmse_test = MSE(y_test, y_pred)\n\n# Compute test set RMSE\nrmse_test = mse_test ** 0.5\n\n# Print rmse_test\nprint('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tuning"},{"metadata":{},"cell_type":"markdown","source":"### Set the tree's hyperparameter grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define params_dt\nparams_dt = {'max_depth': [2, 3, 4], 'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Search for the optimal tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Instantiate grid_dt\ngrid_dt = GridSearchCV(estimator=dt,\n                       param_grid=params_dt,\n                       scoring='roc_auc',\n                       cv=5,\n                       n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the optimal tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = liver.drop('Liver_disease', axis = 1)\ny = liver['Liver_disease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\ngrid_dt.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Import roc_auc_score from sklearn.metrics \nfrom sklearn.metrics import roc_auc_score\n\n# Extract the best estimator\nbest_model = grid_dt.best_estimator_\n\n# Predict the test set probabilities of the positive class\ny_pred_proba = best_model.predict_proba(X_test)[:,1]\n\n# Compute test_roc_auc\ntest_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print test_roc_auc\nprint('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set the hyperparameter grid of RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the dictionary 'params_rf'\nparams_rf = {'n_estimators': [100, 350, 500], 'max_features': ['log2', 'auto', 'sqrt'], 'min_samples_leaf': [2, 10, 30]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Search for the optimal forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n           oob_score=False, random_state=2, verbose=0, warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import GridSearchCV\nfrom sklearn.model_selection import  GridSearchCV\n\n# Instantiate grid_rf\ngrid_rf = GridSearchCV(estimator=rf,\n                       param_grid=params_rf,\n                       scoring='neg_mean_squared_error',\n                       cv=3,\n                       verbose=1,\n                       n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\ny = bike['cnt']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\ngrid_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the optimal forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import mean_squared_error from sklearn.metrics as MSE \nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Extract the best estimator\nbest_model = grid_rf.best_estimator_\n\n# Predict test set labels\ny_pred = best_model.predict(X_test)\n\n# Compute rmse_test\nrmse_test = MSE(y_test, y_pred)**0.5\n\n# Print rmse_test\nprint('Test RMSE of best model: {:.3f}'.format(rmse_test)) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}