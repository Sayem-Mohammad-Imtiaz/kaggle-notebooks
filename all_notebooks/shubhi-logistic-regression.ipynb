{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/lgdata2020/User_Data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input\nX= dataset.iloc[:, [2, 3]].values\n\n# output\ny = dataset.iloc[:, 4].values\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\n\nX = sc_x.fit_transform(X)\n\nprint (X)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection  import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(xtrain, ytrain)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(xtest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(ytest, y_pred)\n\nprint (\"Confusion Matrix : \\n\", cm)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint (\"Accuracy : \", accuracy_score(ytest, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Logistic Regression Example 2**\n\nThe goal of the project is to predict the binary target, whether the patient has heart disease or not.\n\nhttps://www.justintodata.com/logistic-regression-example-in-python/\nhttps://www.tutorialspoint.com/numpy/numpy_hstack.htm","metadata":{}},{"cell_type":"code","source":"# Step 1\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, roc_auc_score, recall_score, precision_score, average_precision_score, f1_score, classification_report, accuracy_score, plot_roc_curve, plot_precision_recall_curve, plot_confusion_matrix\n\n#Step 2  Reading  and \n\ndf = pd.read_csv('/kaggle/input/heart-attack-prediction/data.csv', na_values='?')\nprint(df.head())\n\n# rename y (column) into target\ndf = df.rename(columns={'num       ': 'target'})\n\ndf['target'].value_counts(dropna=False)\n\ndf.info()\n\n\n# Cleaning \n#To keep the cleaning process simple, we’ll remove:\n\n#the columns with many missing values, which are slope, ca, thal.\n#the rows with missing values.\n\ndf = df.drop(['slope', 'ca', 'thal'], axis=1)\n\ndf = df.dropna().copy()\n\ndf.info()\n\n#Step #3: Transform the Categorical Variables: Creating Dummy Variables\n\ndf['cp'].value_counts(dropna=False)\nprint(df['cp'].value_counts(dropna=False))\nprint(\"*8**********************************\")\ndf['restecg'].value_counts(dropna=False)\nprint(df['restecg'].value_counts(dropna=False))\n\ndf.to_csv(\"pre_dummy.csv\")\n# convert into dummy\n\ndf = pd.get_dummies(df, columns=['cp', 'restecg'], drop_first=True)\ndf.to_csv(\"cleaned.csv\")\n\n\n# seperate numeric and categorical columns\n\nprint(\"# seperate numeric and categorical columns\")\n\nnumeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ncat_cols = list(set(df.columns) - set(numeric_cols) - {'target'})\ncat_cols.sort()\n\nprint(numeric_cols)\nprint(cat_cols)\n\n#Step 4: Split Training and Test Datasets\n\n\nrandom_seed = 888\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=random_seed, stratify=df['target'])\n\nprint(\"Checking the split\")\nprint(df_train.shape)\nprint(df_test.shape)\nprint()\nprint(df_train['target'].value_counts(normalize=True))\nprint()\nprint(df_test['target'].value_counts(normalize=True))\n\n\n#Step 5: Transform the Numerical Variables: Scaling\n\nscaler = StandardScaler()\nscaler.fit(df_train[numeric_cols])\n\ndef get_features_and_target_arrays(df, numeric_cols, cat_cols, scaler):\n    X_numeric_scaled = scaler.transform(df[numeric_cols])\n    X_categorical = df[cat_cols].to_numpy()\n    X = np.hstack((X_categorical, X_numeric_scaled))\n    y = df['target']\n    return X, y\n\nX, y = get_features_and_target_arrays(df_train, numeric_cols, cat_cols, scaler)\n\n\n#Step 6: Fit the Logistic Regression Model\n\nclf = LogisticRegression(penalty='none') # logistic regression with no penalty term in the cost function.\n\nclf.fit(X, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_seed = 888\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=random_seed, stratify=df['target'])\n\nprint(\"Checking the split\")\nprint(df_train.shape)\nprint(df_test.shape)\nprint()\nprint(df_train['target'].value_counts(normalize=True))\nprint()\nprint(df_test['target'].value_counts(normalize=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ten features we’ll be using are:\n\nage: age in years\nsex: sex (1 = male; 0 = female)\ncp: chest pain type\n– 1: typical angina\n– 2: atypical angina\n– 3: non-anginal pain\n– 4: asymptomatic\ntrestbps: resting blood pressure (in mm Hg on admission to the hospital)\nchol: serum cholesterol in mg/dl\nfbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\nrestecg: resting electrocardiographic results\n– 0: normal\n– 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n– 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria\nthalach: maximum heart rate achieved\nexang: exercise-induced angina (1 = yes; 0 = no)\noldpeak: ST depression induced by exercise relative to rest\nWe can also take a quick look at the data itself by printing out the dataset.","metadata":{}},{"cell_type":"markdown","source":"**Step #7: Evaluate the Model**\nhttps://www.justintodata.com/logistic-regression-example-in-python/","metadata":{}},{"cell_type":"code","source":"#Before starting, we need to get the scaled test dataset.\n\nX_test, y_test = get_features_and_target_arrays(df_test, numeric_cols, cat_cols, scaler)\n\nplot_roc_curve(clf, X_test, y_test)\n\nplot_precision_recall_curve(clf, X_test, y_test)\n\ntest_prob = clf.predict_proba(X_test)[:, 1]\ntest_pred = clf.predict(X_test)\n\n\nprint('Log loss = {:.5f}'.format(log_loss(y_test, test_prob)))\nprint('AUC = {:.5f}'.format(roc_auc_score(y_test, test_prob)))\nprint('Average Precision = {:.5f}'.format(average_precision_score(y_test, test_prob)))\nprint('\\nUsing 0.5 as threshold:')\nprint('Accuracy = {:.5f}'.format(accuracy_score(y_test, test_pred)))\nprint('Precision = {:.5f}'.format(precision_score(y_test, test_pred)))\nprint('Recall = {:.5f}'.format(recall_score(y_test, test_pred)))\nprint('F1 score = {:.5f}'.format(f1_score(y_test, test_pred)))\n\nprint('\\nClassification Report')\nprint(classification_report(y_test, test_pred))\n\n\nprint('Confusion Matrix')\nplot_confusion_matrix(clf, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step #8: Interpret the Results**","metadata":{}},{"cell_type":"code","source":"coefficients = np.hstack((clf.intercept_, clf.coef_[0]))\npd.DataFrame(data={'variable': ['intercept'] + cat_cols + numeric_cols, 'coefficient': coefficients})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since the numerical variables are scaled by StandardScaler, we need to think of them in terms of standard deviations. Let’s first print out the list of numeric variable and its sample standard deviation.**","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(data={'variable': numeric_cols, 'unit': np.sqrt(scaler.var_)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For example, holding other variables fixed, there is a 41% increase in the odds of having a heart disease for every standard deviation increase in cholesterol (63.470764) since exp(0.345501) = 1.41.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/\n","metadata":{}}]}