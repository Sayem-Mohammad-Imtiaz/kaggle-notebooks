{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Domain: Object Recognition\n\n- Context: Silhouette of different vehicles from multiple angles are given. Attributes of each silhouette is captured in the data set. Objective is to build a model which will classify the corresponding silhouette and identify type of the vehicle from it. ","metadata":{}},{"cell_type":"code","source":"#1. Data pre-processing – Perform all the necessary preprocessing on the data ready to be fed to an Unsupervised algorithm","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:12.792134Z","iopub.execute_input":"2021-06-17T15:55:12.792783Z","iopub.status.idle":"2021-06-17T15:55:12.797011Z","shell.execute_reply.started":"2021-06-17T15:55:12.792748Z","shell.execute_reply":"2021-06-17T15:55:12.795951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:12.799868Z","iopub.execute_input":"2021-06-17T15:55:12.800468Z","iopub.status.idle":"2021-06-17T15:55:12.813277Z","shell.execute_reply.started":"2021-06-17T15:55:12.800432Z","shell.execute_reply":"2021-06-17T15:55:12.812214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data and name it as vData (i.e. Vehicle Data)\nvData = pd.read_csv(\"../input/vehicle2/vehicle-2.csv\")\nvData.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:12.815144Z","iopub.execute_input":"2021-06-17T15:55:12.815682Z","iopub.status.idle":"2021-06-17T15:55:12.83621Z","shell.execute_reply.started":"2021-06-17T15:55:12.815651Z","shell.execute_reply":"2021-06-17T15:55:12.835177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Given data set have total of 19 properties or columns and 846 rows or vehicle data points","metadata":{}},{"cell_type":"code","source":"# Display the data set info\nvData.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:12.838193Z","iopub.execute_input":"2021-06-17T15:55:12.838577Z","iopub.status.idle":"2021-06-17T15:55:12.870044Z","shell.execute_reply.started":"2021-06-17T15:55:12.838545Z","shell.execute_reply":"2021-06-17T15:55:12.868463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 19 properties are provided to capture the measurement of a vehicle from different angle\n- 4 properties are of integer type data\n- 14 properties are having decimal point values\n- The class properties are of an onject type which denotes the type of a vehicle. As per the problem statement, there are 4 types of vehicle were taken to prepare the data set. Silhouette of each type of vechile are captured through the other peroperties.","metadata":{}},{"cell_type":"code","source":"# Make a copy of the original data set vData for further processing through Unsupervised learning technique \n# in later part of the project\nvDataOrig = vData.copy() #vDataOrig = Original copy of master data set for Vehicle silhouette\n\n# Let's get the sample look of the data\nvDataOrig.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:12.874101Z","iopub.execute_input":"2021-06-17T15:55:12.874688Z","iopub.status.idle":"2021-06-17T15:55:12.910506Z","shell.execute_reply.started":"2021-06-17T15:55:12.874632Z","shell.execute_reply":"2021-06-17T15:55:12.909677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The class columns contains string value to denote the type of the vehicle\n- Other properties are all numerical","metadata":{}},{"cell_type":"code","source":"# The vehichle properties \"class\" is a categorical variable\n# Hence splitting it into 3 separate columns which denotes 1 = True and 0 = False\nvData = pd.get_dummies(vData, columns=['class'])\nvData.head()\n\n#vData i.e. the original Vechicle Data set shall be used for EDA and Supervised learning technique models","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:12.911872Z","iopub.execute_input":"2021-06-17T15:55:12.912345Z","iopub.status.idle":"2021-06-17T15:55:12.954716Z","shell.execute_reply.started":"2021-06-17T15:55:12.912295Z","shell.execute_reply":"2021-06-17T15:55:12.953916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In an another approach, doing the Label Encoding to make the \"class\" properties into numerical value\n# Applying this on the Original data set vDataOrig\n\nfrom sklearn import preprocessing\nencode_column = 'class' # Specify the column which needs to be encoded\nlebelEncoder = preprocessing.LabelEncoder()\nlebelEncoder.fit(vDataOrig[encode_column])\nvDataOrig[encode_column] = lebelEncoder.transform(vDataOrig[encode_column]) \n\nvDataOrig.head() # Have a look into the columns after encoding the class column","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:12.955731Z","iopub.execute_input":"2021-06-17T15:55:12.956157Z","iopub.status.idle":"2021-06-17T15:55:13.19565Z","shell.execute_reply.started":"2021-06-17T15:55:12.956127Z","shell.execute_reply":"2021-06-17T15:55:13.194379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- After performing the encoding, the vehicle type has been lebeled into following groups:\n    - 0 = Bus\n    - 1 = Car\n    - 2 = Van","metadata":{}},{"cell_type":"code","source":"# Check if there is any missing values in the data set\nvData.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.197265Z","iopub.execute_input":"2021-06-17T15:55:13.197687Z","iopub.status.idle":"2021-06-17T15:55:13.207759Z","shell.execute_reply.started":"2021-06-17T15:55:13.197652Z","shell.execute_reply":"2021-06-17T15:55:13.206141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many blank values present in each column\nvData.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.209746Z","iopub.execute_input":"2021-06-17T15:55:13.210501Z","iopub.status.idle":"2021-06-17T15:55:13.224875Z","shell.execute_reply.started":"2021-06-17T15:55:13.210449Z","shell.execute_reply":"2021-06-17T15:55:13.223338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above indicates that there are 41 blank records in the overall dataset","metadata":{}},{"cell_type":"code","source":"# View the median values of each column to replace all the blank values with corresponding median\nvData.median()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.226506Z","iopub.execute_input":"2021-06-17T15:55:13.22691Z","iopub.status.idle":"2021-06-17T15:55:13.245395Z","shell.execute_reply.started":"2021-06-17T15:55:13.226821Z","shell.execute_reply":"2021-06-17T15:55:13.243948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace all the blank values with corresponding median\nfrom sklearn.impute import SimpleImputer #Use the SimpleImputer library to replace all blanks in a generic way\n\n# Get the Imputer initialized for the data set with \"class\" dummy column \nimputer_for_vData = SimpleImputer(missing_values = np.nan , strategy = 'median')\nimputer_for_vData.fit(vData)\n\n# Get the Imputer initialized for the original data set with encoded \"class\" column \nimputer_for_vDataOrig = SimpleImputer(missing_values = np.nan , strategy = 'median')\nimputer_for_vDataOrig.fit(vDataOrig)\n\ncol_names_vData = vData.columns.values # Get the column names for the data set with \"class\" dummy column\ncol_names_vDataOrig = vDataOrig.columns.values # Get the column names for the original data set with encoded \"class\" column\n\n# Get the new data set after replacing the blanks with corresponding column median\nvData = pd.DataFrame(imputer_for_vData.transform(vData), columns=col_names_vData)\nvDataOrig = pd.DataFrame(imputer_for_vDataOrig.transform(vDataOrig), columns=col_names_vDataOrig)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.249751Z","iopub.execute_input":"2021-06-17T15:55:13.250115Z","iopub.status.idle":"2021-06-17T15:55:13.563671Z","shell.execute_reply.started":"2021-06-17T15:55:13.250083Z","shell.execute_reply":"2021-06-17T15:55:13.562556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many blank values present in each column after replacing blank with median\nvData.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.568941Z","iopub.execute_input":"2021-06-17T15:55:13.569292Z","iopub.status.idle":"2021-06-17T15:55:13.580926Z","shell.execute_reply.started":"2021-06-17T15:55:13.569258Z","shell.execute_reply":"2021-06-17T15:55:13.579338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above indicates there is no more blanks in the data set vData (with \"class\" dummy columns)","metadata":{}},{"cell_type":"code","source":"# Check how many blank values present in each column after replacing blank with median\nvDataOrig.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.583345Z","iopub.execute_input":"2021-06-17T15:55:13.583857Z","iopub.status.idle":"2021-06-17T15:55:13.593851Z","shell.execute_reply.started":"2021-06-17T15:55:13.583812Z","shell.execute_reply":"2021-06-17T15:55:13.592812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above indicates there is no more blanks in the data set vDataOrig (with Encoded \"class\" columns)","metadata":{}},{"cell_type":"code","source":"# This is just to compare if the median changed after the replacement of the blank values\nvData.median()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.595325Z","iopub.execute_input":"2021-06-17T15:55:13.595643Z","iopub.status.idle":"2021-06-17T15:55:13.608243Z","shell.execute_reply.started":"2021-06-17T15:55:13.595611Z","shell.execute_reply":"2021-06-17T15:55:13.607324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above indicates the median of each column has not changed even after replacement of the blank fields in the data set","metadata":{}},{"cell_type":"code","source":"# Have a look into the original dataset\nvData.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.609643Z","iopub.execute_input":"2021-06-17T15:55:13.609911Z","iopub.status.idle":"2021-06-17T15:55:13.660817Z","shell.execute_reply.started":"2021-06-17T15:55:13.609886Z","shell.execute_reply":"2021-06-17T15:55:13.659602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Row# 5, column named \"circularity\" had a blank field\n- The median value of the column is 44\n- Above indicates the blank replacement was successful ","metadata":{}},{"cell_type":"code","source":"# Let's have a quick look into the data description to get an idea about the 5 point summary\nvData.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.662258Z","iopub.execute_input":"2021-06-17T15:55:13.662577Z","iopub.status.idle":"2021-06-17T15:55:13.735247Z","shell.execute_reply.started":"2021-06-17T15:55:13.662548Z","shell.execute_reply":"2021-06-17T15:55:13.733302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Except 'skewness_about' and 'skewness_about.1' column all other properties have non-zero values.\n- Minimum value of 'skewness_about' and 'skewness_about.1' columns are ZERO but not replacing these 0 values as the properties itself ranges from very low value up to 11 and 41 respectively.\n- Noting the 75% of the value and max value the above 5 point summary indicates there are possible outliers in following columns:\n    - radius_ratio\n    - pr.axis_aspect_ratio\n    - max.length_aspect_ratio\n    - skewness_about\n    - skewness_about.1","metadata":{}},{"cell_type":"code","source":"# Let's review the skewness of the properties\nvData.skew()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.737411Z","iopub.execute_input":"2021-06-17T15:55:13.737825Z","iopub.status.idle":"2021-06-17T15:55:13.746326Z","shell.execute_reply.started":"2021-06-17T15:55:13.737782Z","shell.execute_reply":"2021-06-17T15:55:13.745365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Following properties are positively skewed (with value >1.5). It indicates that these atributes are not normally distributed. The tail of the distribution is longer on the right side. The mean is greater than the median for these parameters.\n     - pr.axis_aspect_ratio, max.length_aspect_ratio, scaled_radius_of_gyration.1\n     \n- Following properties are negatively skewed. It indicates that these atributes are also not normally distributed. The tail of the distribution is longer on the left side. The mean is lesser than the median for these parameters.\n    - hollows_ratio, class_car","metadata":{}},{"cell_type":"code","source":"#2. Understanding the attributes - Find relationship between different attributes (Independent variables) and \n# choose carefully which all attributes have to be a part of the analysis and why","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.74755Z","iopub.execute_input":"2021-06-17T15:55:13.747829Z","iopub.status.idle":"2021-06-17T15:55:13.760555Z","shell.execute_reply.started":"2021-06-17T15:55:13.747803Z","shell.execute_reply":"2021-06-17T15:55:13.759585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe the correlation of the variables through graphical Heat map\n# +ve and -ve numbers indicate how the variabes are correalted to each other\n\ncolormap = plt.cm.viridis # Color range to be used in heatmap\nplt.figure(figsize=(15,15))\nplt.title('Silhouette properties Correlation of attributes', y=1.05, size=19)\nsns.heatmap(vData.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:13.762158Z","iopub.execute_input":"2021-06-17T15:55:13.762644Z","iopub.status.idle":"2021-06-17T15:55:16.012881Z","shell.execute_reply.started":"2021-06-17T15:55:13.762607Z","shell.execute_reply":"2021-06-17T15:55:16.012062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Dark color - indicates the variables are less associated\n- Brighter color - indicates the variables are comparatively strongly associated\n    - Ex:\n        - compactness is less related to elongatedness, scaled_radius_of_gyration.1 and whether a vehicle is bus or van\n        - elongatedness is less related to many properties as follows\n            - whether a vechile type is car, pr.axis_rectangularity, max.length_rectangularity,\tscaled_variance, scaled_variance.1,\tscaled_radius_of_gyration\n        - pr.axis_rectangularity is strongly related to max.length_rectangularity, scaled_variance, scaled_variance.1, scaled_radius_of_gyration\n        - scaled_variance is strongly related to scaled_variance.1,\tscaled_radius_of_gyration\n        - By referring the elongatedness properties the chance of identifying a vehichle as Van is higher\n        - By referring compactness,\tcircularity, distance_circularity, radius_ratio the chance of identifying a vehichle as Car is higher","metadata":{}},{"cell_type":"code","source":"# Display the pair plot\nsns.pairplot(vData)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:55:16.014024Z","iopub.execute_input":"2021-06-17T15:55:16.014485Z","iopub.status.idle":"2021-06-17T15:56:57.042932Z","shell.execute_reply.started":"2021-06-17T15:55:16.014452Z","shell.execute_reply":"2021-06-17T15:56:57.042104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above pair plots we see the relation between the parameters captured for silhouette of various vehicles from disfferent angle:\n- Ex:\n    - elongatedness and scatter_ratio are negatively related with a greater extent i.e. small increase of  scatter_ratio there is significant decrease of elongatedness\n    - pr.axis_rectangularity and max.length_rectangularity has no impact on skewness and hollows_ratio. From the correlaiton matrix also we observed the same\n    - pr.axis_rectangularity has strong positive relationship with compactness,\tcircularity, distance_circularity, radius_ratio","metadata":{}},{"cell_type":"code","source":"# Measure of possible outliers in the dataset -\n# It can be done by checking the box plot of each properties or by evaluating the z scores of each columns\n\n#As there are 21 properties given in the dataset, let's get the Z score of the entire data set \n#to check outliers statistically\n\nfrom scipy.stats import zscore\n\n# Get the z score\nz_vData = vData.apply(zscore)\n\n# Set the limt to 3 sigma to check outliers in the voice sample provided\nlimit = 3\nt1 = np.where(z_vData > limit) #store the outliers in the touple variable\n\n# print the index value of original data set which contains outliers >3 sigma\nprint(t1[0]) # depicts the row# containing outliers\nprint(t1[1]) # depicts the column# containing outliers","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.044078Z","iopub.execute_input":"2021-06-17T15:56:57.044563Z","iopub.status.idle":"2021-06-17T15:56:57.062035Z","shell.execute_reply.started":"2021-06-17T15:56:57.044527Z","shell.execute_reply":"2021-06-17T15:56:57.060907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list1 = t1[0] #load the row# of the data set containing outliers\nlist2 = t1[1] #load the column# of the data set containing outliers\n\nj = 0 #initiate the iterator\n\n# print the outlier column and it's value from the data set provided\nfor i in list2:   #loop through the columns\n    print(\"Outliers exist in properties: \", vData.columns[i], \" and the value is: \",vData.loc[list1[j]][i])\n    j +=1 #move to the next value of the corresponding row#    ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.063439Z","iopub.execute_input":"2021-06-17T15:56:57.063765Z","iopub.status.idle":"2021-06-17T15:56:57.090814Z","shell.execute_reply.started":"2021-06-17T15:56:57.063734Z","shell.execute_reply":"2021-06-17T15:56:57.089892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Following are few of the properties which have multiple outliers in the dataset, programatically above depicts the same.\n    - pr.axis_aspect_ratio\n    - scaled_variance\n    - skewness_about\n    - skewness_about.1","metadata":{}},{"cell_type":"code","source":"# Let's review the outliers visually through below graph\n# plot pr.axis_aspect_ratio for car type vehicle\nsns.boxplot(x = \"class_car\", y = \"pr.axis_aspect_ratio\", data = vData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.092401Z","iopub.execute_input":"2021-06-17T15:56:57.092705Z","iopub.status.idle":"2021-06-17T15:56:57.317446Z","shell.execute_reply.started":"2021-06-17T15:56:57.092676Z","shell.execute_reply":"2021-06-17T15:56:57.316459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's review the outliers visually through below graph\n# plot scaled_variance for van type vehicle\nsns.boxplot(x = \"class_van\", y = \"scaled_variance\", data = vData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.318695Z","iopub.execute_input":"2021-06-17T15:56:57.319101Z","iopub.status.idle":"2021-06-17T15:56:57.504292Z","shell.execute_reply.started":"2021-06-17T15:56:57.319065Z","shell.execute_reply":"2021-06-17T15:56:57.503296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's review the outliers visually through below graph\n# plot skewness_about to view outliers in it\nsns.boxplot(\"skewness_about\", data = vData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.505527Z","iopub.execute_input":"2021-06-17T15:56:57.505796Z","iopub.status.idle":"2021-06-17T15:56:57.688415Z","shell.execute_reply.started":"2021-06-17T15:56:57.50577Z","shell.execute_reply":"2021-06-17T15:56:57.687566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the target column distribution for models to be used in Supervised Technique\n\n# In the given data set vehicle properties are provided and based on that its given what type of vehicle is it, \n# ex: whether it's a Car, Bus or Van.\n\n# Hence for Supervised learning, we can consider all vehicle properties as independent veriable and \n# type of each car individually as target variable.\n\n# In this case there are 3 target variable i.e. class_bus, class_car and class_van. \n# We are going to build the Supervised learning models for each of these target variables.\n\n# The target column is 'class_bus' which is indicated by 0 and 1\n# 1 - means vehicle is a bus\n# 0 - means vehicle is not a bus\n\n# Let's measure the % of split where a vehicle is a bus or not\n\nvBus_true = len(vData.loc[vData['class_bus'] == 1])\nvBus_false = len(vData.loc[vData['class_bus'] == 0])\n\nprint(\"Number of true cases for Bus: {0} ({1:2.2f}%)\".format(vBus_true, (vBus_true / (vBus_true + vBus_false)) * 100 ))\nprint(\"Number of false cases for Bus: {0} ({1:2.2f}%)\".format(vBus_false, (vBus_false / (vBus_true + vBus_false)) * 100))\nprint(\"---------------------------------------------------------------------\")\n\n# The target column is 'class_car' which is indicated by 0 and 1\n# 1 - means vehicle is a car\n# 0 - means vehicle is not a car\n\n# Let's measure the % of split where a vehicle is a car or not\n\nvCar_true = len(vData.loc[vData['class_car'] == 1])\nvCar_false = len(vData.loc[vData['class_car'] == 0])\n\nprint(\"Number of true cases for Car: {0} ({1:2.2f}%)\".format(vCar_true, (vCar_true / (vCar_true + vCar_false)) * 100 ))\nprint(\"Number of false cases for Car: {0} ({1:2.2f}%)\".format(vCar_false, (vCar_false / (vCar_true + vCar_false)) * 100))\nprint(\"---------------------------------------------------------------------\")\n\n# The target column is 'class_van' which is indicated by 0 and 1\n# 1 - means vehicle is a van\n# 0 - means vehicle is not a van\n\n# Let's measure the % of split where a vehicle is a van or not\n\nvVan_true = len(vData.loc[vData['class_van'] == 1])\nvVan_false = len(vData.loc[vData['class_van'] == 0])\n\nprint(\"Number of true cases for Van: {0} ({1:2.2f}%)\".format(vVan_true, (vVan_true / (vVan_true + vVan_false)) * 100 ))\nprint(\"Number of false cases for Van: {0} ({1:2.2f}%)\".format(vVan_false, (vVan_false / (vVan_true + vVan_false)) * 100))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.68989Z","iopub.execute_input":"2021-06-17T15:56:57.690259Z","iopub.status.idle":"2021-06-17T15:56:57.708638Z","shell.execute_reply.started":"2021-06-17T15:56:57.690227Z","shell.execute_reply":"2021-06-17T15:56:57.706424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target column \"class\" i.e. Vehicle Type wise count\npd.value_counts(vDataOrig['class'])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.710728Z","iopub.execute_input":"2021-06-17T15:56:57.711161Z","iopub.status.idle":"2021-06-17T15:56:57.72497Z","shell.execute_reply.started":"2021-06-17T15:56:57.711116Z","shell.execute_reply":"2021-06-17T15:56:57.723903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As per the encoding performed:\n    - 0 = Bus\n    - 1 = Car\n    - 2 = Van","metadata":{}},{"cell_type":"code","source":"#3. Split the data into train and test (by specifying “random state” as using train_test_split from Sklearn)\n\nfrom sklearn.model_selection import train_test_split\n\nX = vData.drop(['class_bus', 'class_car', 'class_van'],axis=1)     # Predictor feature columns (18 X m)\nX_Orig = vDataOrig.drop('class', axis=1) # Get the independent features from the original data set with encoded \"class\" column\n\nY_bus = vData['class_bus']   # Predicted vehicle type Bus (1=True, 0=False) (1 X m)\nY_car = vData['class_car']   # Predicted vehicle type Car (1=True, 0=False) (1 X m)\nY_van = vData['class_van']   # Predicted vehicle type Van (1=True, 0=False) (1 X m)\nY_Orig = vDataOrig['class']   # Predicted vehicle type \"class\" (0=Bus, 1=Car and 2=Van) (1 X m)\n\n# Split data considering class_bus as target\nx_train_bus, x_test_bus, y_train_bus, y_test_bus = train_test_split(X, Y_bus, test_size=0.3, random_state=5) # 5 is just any random seed number\n\n# Split data considering class_car as target\nx_train_car, x_test_car, y_train_car, y_test_car = train_test_split(X, Y_car, test_size=0.3, random_state=5) # 5 is just any random seed number\n\n# Split data considering class_van as target\nx_train_van, x_test_van, y_train_van, y_test_van = train_test_split(X, Y_van, test_size=0.3, random_state=5) # 5 is just any random seed number\n\n# Split data considering encoded class as target\nx_train_orig, x_test_orig, y_train_orig, y_test_orig = train_test_split(X_Orig, Y_Orig, test_size=0.3, random_state=5) # 5 is just any random seed number\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.726403Z","iopub.execute_input":"2021-06-17T15:56:57.726702Z","iopub.status.idle":"2021-06-17T15:56:57.746116Z","shell.execute_reply.started":"2021-06-17T15:56:57.726673Z","shell.execute_reply":"2021-06-17T15:56:57.744879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look into the train data for vehicle type = Bus\nx_train_bus.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.747597Z","iopub.execute_input":"2021-06-17T15:56:57.747923Z","iopub.status.idle":"2021-06-17T15:56:57.785707Z","shell.execute_reply.started":"2021-06-17T15:56:57.747893Z","shell.execute_reply":"2021-06-17T15:56:57.784822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look into the train data for vehicle type = Car\nx_train_car.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.787187Z","iopub.execute_input":"2021-06-17T15:56:57.787621Z","iopub.status.idle":"2021-06-17T15:56:57.819487Z","shell.execute_reply.started":"2021-06-17T15:56:57.787579Z","shell.execute_reply":"2021-06-17T15:56:57.818633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look into the train data for vehicle type = Van\nx_train_van.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.820475Z","iopub.execute_input":"2021-06-17T15:56:57.820859Z","iopub.status.idle":"2021-06-17T15:56:57.856545Z","shell.execute_reply.started":"2021-06-17T15:56:57.820832Z","shell.execute_reply":"2021-06-17T15:56:57.855611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Train data set with respect to target variable Bus, Car and Van are same\n- However used different specific variable for train-test data split to avoid any confusion during model building\n- Random state = 5 is used in all cases","metadata":{}},{"cell_type":"code","source":"# Have a look into the train data for encoded \"class\"\nx_train_orig.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.858262Z","iopub.execute_input":"2021-06-17T15:56:57.858587Z","iopub.status.idle":"2021-06-17T15:56:57.888133Z","shell.execute_reply.started":"2021-06-17T15:56:57.858559Z","shell.execute_reply":"2021-06-17T15:56:57.887213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validate the size of data set in the train and test data\n\nprint(\"{0:0.2f}% data (Vehicle class = Bus) is in training set\".format((len(x_train_bus)/len(vData.index)) * 100))\nprint(\"{0:0.2f}% data (Vehicle class = Bus) is in test set\".format((len(x_test_bus)/len(vData.index)) * 100))\nprint(\"---------------------------------------------------------\")\nprint(\"{0:0.2f}% data (Vehicle class = Car) is in training set\".format((len(x_train_car)/len(vData.index)) * 100))\nprint(\"{0:0.2f}% data (Vehicle class = Car) is in test set\".format((len(x_test_car)/len(vData.index)) * 100))\nprint(\"---------------------------------------------------------\")\nprint(\"{0:0.2f}% data (Vehicle class = Van) is in training set\".format((len(x_train_van)/len(vData.index)) * 100))\nprint(\"{0:0.2f}% data (Vehicle class = Van) is in test set\".format((len(x_test_van)/len(vData.index)) * 100))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.88945Z","iopub.execute_input":"2021-06-17T15:56:57.889729Z","iopub.status.idle":"2021-06-17T15:56:57.898579Z","shell.execute_reply.started":"2021-06-17T15:56:57.889703Z","shell.execute_reply":"2021-06-17T15:56:57.897793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the data set\n\n# There are multiple ways to scale any data set. As we don't know what units or scales were used for each of the properties,\n# we are going to apply fit/transform method of scaling to compare the outcome.\n\n# Apply the Fit Transform scaling on Train and Test data set - \nfrom sklearn.preprocessing import StandardScaler\n\nscaling = StandardScaler()\n\nx_train_bus_TransScale = scaling.fit_transform(x_train_bus) #Fit the scale on train data - for bus\nx_test_bus_TransScale = scaling.transform(x_test_bus) #transform the scale on test data\n\nx_train_car_TransScale = scaling.fit_transform(x_train_car) #Fit the scale on train data - for car\nx_test_car_TransScale = scaling.transform(x_test_car) #transform the scale on test data\n\nx_train_van_TransScale = scaling.fit_transform(x_train_van) #Fit the scale on train data - for van\nx_test_van_TransScale = scaling.transform(x_test_van) #transform the scale on test data\n\nx_train_orig_TransScale = scaling.fit_transform(x_train_orig) #Fit the scale on train data - for encoded 'class' type\nx_test_orig_TransScale = scaling.transform(x_test_orig) #transform the scale on test data","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.899782Z","iopub.execute_input":"2021-06-17T15:56:57.900246Z","iopub.status.idle":"2021-06-17T15:56:57.938109Z","shell.execute_reply.started":"2021-06-17T15:56:57.900216Z","shell.execute_reply":"2021-06-17T15:56:57.937367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4. Train a Support vector machine using the train set and get the accuracy on the test set\n\nfrom sklearn.svm import SVC #load the support vector classifier\n\n#instanciate SVM Classifier\nvData_bus_svm_model_TransScale = SVC() #For Vehicle Type = Bus \nvData_car_svm_model_TransScale = SVC() #For Vehicle Type = Car\nvData_van_svm_model_TransScale = SVC() #For Vehicle Type = Van\nvDataOrig_svm_model_TransScale = SVC() #For Vehicle Type = Encoded Class\n\n# Fit the SVM model with train data set -  For Vehicle Type = Bus \nvData_bus_svm_model_TransScale.fit(x_train_bus_TransScale, y_train_bus)\n\n# Fit the SVM model with train data set -  For Vehicle Type = Car\nvData_car_svm_model_TransScale.fit(x_train_car_TransScale, y_train_car)\n\n# Fit the SVM model with train data set -  For Vehicle Type = Van \nvData_van_svm_model_TransScale.fit(x_train_van_TransScale, y_train_van)\n\n# Fit the SVM model with train data set -  For Vehicle Type = Encoded Class \nvDataOrig_svm_model_TransScale.fit(x_train_orig_TransScale, y_train_orig)\n\nprint(\"Accuracy of SVM model on training data (Vehicle Type = Bus): {:.2f}\".format(vData_bus_svm_model_TransScale.score(x_train_bus_TransScale, y_train_bus)))\nprint(\"Accuracy of SVM model on test data (Vehicle Type = Bus): {:.2f}\".format(vData_bus_svm_model_TransScale.score(x_test_bus_TransScale, y_test_bus)))\nprint(\"----------------------------------------------------------------------\")\nprint(\"Accuracy of SVM model on training data (Vehicle Type = Car): {:.2f}\".format(vData_car_svm_model_TransScale.score(x_train_car_TransScale, y_train_car)))\nprint(\"Accuracy of SVM model on test data (Vehicle Type = Car): {:.2f}\".format(vData_car_svm_model_TransScale.score(x_test_car_TransScale, y_test_car)))\nprint(\"----------------------------------------------------------------------\")\nprint(\"Accuracy of SVM model on training data (Vehicle Type = Van): {:.2f}\".format(vData_van_svm_model_TransScale.score(x_train_van_TransScale, y_train_van)))\nprint(\"Accuracy of SVM model on test data (Vehicle Type = Van): {:.2f}\".format(vData_van_svm_model_TransScale.score(x_test_van_TransScale, y_test_van)))\nprint(\"----------------------------------------------------------------------\")\nprint(\"Accuracy of SVM model on training data (Vehicle Type = Encoded Class): {:.2f}\".format(vDataOrig_svm_model_TransScale.score(x_train_orig_TransScale, y_train_orig)))\nprint(\"Accuracy of SVM model on test data (Vehicle Type = Encoded Class): {:.2f}\".format(vDataOrig_svm_model_TransScale.score(x_test_orig_TransScale, y_test_orig)))\n\n# predict the model score - For Vehicle Type = Bus\nvData_bus_svm_test_predict_TransScale = vData_bus_svm_model_TransScale.predict(x_test_bus_TransScale)\n\n# predict the model score - For Vehicle Type = Car\nvData_car_svm_test_predict_TransScale = vData_car_svm_model_TransScale.predict(x_test_car_TransScale)\n\n# predict the model score - For Vehicle Type = Van\nvData_van_svm_test_predict_TransScale = vData_van_svm_model_TransScale.predict(x_test_van_TransScale)\n\n# predict the model score - For Vehicle Type = Encoded Class\nvDataOrig_svm_test_predict_TransScale = vDataOrig_svm_model_TransScale.predict(x_test_orig_TransScale)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:57.939412Z","iopub.execute_input":"2021-06-17T15:56:57.939913Z","iopub.status.idle":"2021-06-17T15:56:58.071205Z","shell.execute_reply.started":"2021-06-17T15:56:57.939881Z","shell.execute_reply":"2021-06-17T15:56:58.070261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate confusion matrix for SVM model - For Vehicle Type = Bus\nfrom sklearn import metrics\n\ncm_bus_svm=metrics.confusion_matrix(y_test_bus, vData_bus_svm_test_predict_TransScale, labels=[1, 0])\n\ndf_cm_bus_svm = pd.DataFrame(cm_bus_svm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm_bus_svm, annot=True, cmap=\"PuRd\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:58.072401Z","iopub.execute_input":"2021-06-17T15:56:58.072669Z","iopub.status.idle":"2021-06-17T15:56:58.317844Z","shell.execute_reply.started":"2021-06-17T15:56:58.072644Z","shell.execute_reply":"2021-06-17T15:56:58.316188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The confusion matrix (Based on scaled data) - SVM: For Vehicle Type = Bus\n    - True Positives (TP): we correctly predicted that vehicle type is a Bus - 59\n    - True Negatives (TN): we correctly predicted that vehicle type is not a Bus - 190\n    - False Positives (FP): we incorrectly predicted that vehicle type is a Bus (a \"Type I error\") - 2\n    - False Negatives (FN): we incorrectly predicted that vehicle type is not a Bus (a \"Type II error\") - 6","metadata":{}},{"cell_type":"code","source":"# calculate confusion matrix for SVM model - For Vehicle Type = Car\nfrom sklearn import metrics\n\ncm_car_svm=metrics.confusion_matrix(y_test_car, vData_car_svm_test_predict_TransScale, labels=[1, 0])\n\ndf_cm_car_svm = pd.DataFrame(cm_car_svm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm_car_svm, annot=True, cmap=\"PuRd\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:58.319114Z","iopub.execute_input":"2021-06-17T15:56:58.319436Z","iopub.status.idle":"2021-06-17T15:56:58.611597Z","shell.execute_reply.started":"2021-06-17T15:56:58.319408Z","shell.execute_reply":"2021-06-17T15:56:58.610622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The confusion matrix (Based on scaled data) - SVM: For Vehicle Type = Car\n    - True Positives (TP): we correctly predicted that vehicle type is a Car - 130\n    - True Negatives (TN): we correctly predicted that vehicle type is not a Car - 120\n    - False Positives (FP): we incorrectly predicted that vehicle type is a Car (a \"Type I error\") - 5\n    - False Negatives (FN): we incorrectly predicted that vehicle type is not a Car (a \"Type II error\") - 6","metadata":{}},{"cell_type":"code","source":"# calculate confusion matrix for SVM model - For Vehicle Type = Van\nfrom sklearn import metrics\n\ncm_van_svm=metrics.confusion_matrix(y_test_van, vData_van_svm_test_predict_TransScale, labels=[1, 0])\n\ndf_cm_van_svm = pd.DataFrame(cm_van_svm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm_van_svm, annot=True, cmap=\"PuRd\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:58.613916Z","iopub.execute_input":"2021-06-17T15:56:58.614195Z","iopub.status.idle":"2021-06-17T15:56:58.9177Z","shell.execute_reply.started":"2021-06-17T15:56:58.614169Z","shell.execute_reply":"2021-06-17T15:56:58.916831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The confusion matrix (Based on scaled data) - SVM: For Vehicle Type = Van\n    - True Positives (TP): we correctly predicted that vehicle type is a Van - 46\n    - True Negatives (TN): we correctly predicted that vehicle type is not a Van - 190\n    - False Positives (FP): we incorrectly predicted that vehicle type is a Van (a \"Type I error\") - 9\n    - False Negatives (FN): we incorrectly predicted that vehicle type is not a Van (a \"Type II error\") - 10","metadata":{}},{"cell_type":"code","source":"# calculate confusion matrix for SVM model - For Vehicle Type = Encoded Class\nfrom sklearn import metrics\n\ncm_orig_svm=metrics.confusion_matrix(y_test_orig, vDataOrig_svm_test_predict_TransScale, labels=[1, 0])\n\ndf_cm_orig_svm = pd.DataFrame(cm_orig_svm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm_orig_svm, annot=True, cmap=\"PuRd\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:58.918729Z","iopub.execute_input":"2021-06-17T15:56:58.91899Z","iopub.status.idle":"2021-06-17T15:56:59.20683Z","shell.execute_reply.started":"2021-06-17T15:56:58.918965Z","shell.execute_reply":"2021-06-17T15:56:59.205688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The confusion matrix (Based on scaled data) - SVM: For Vehicle Type = Encoded Class\n    - True Positives (TP): we correctly predicted that vehicle type is a Van - 130\n    - True Negatives (TN): we correctly predicted that vehicle type is not a Van - 62\n    - False Positives (FP): we incorrectly predicted that vehicle type is a Van (a \"Type I error\") - 2\n    - False Negatives (FN): we incorrectly predicted that vehicle type is not a Van (a \"Type II error\") - 0","metadata":{}},{"cell_type":"code","source":"#5. Perform K-fold cross validation and get the cross validation score of the model","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:59.208068Z","iopub.execute_input":"2021-06-17T15:56:59.208376Z","iopub.status.idle":"2021-06-17T15:56:59.211906Z","shell.execute_reply.started":"2021-06-17T15:56:59.20834Z","shell.execute_reply":"2021-06-17T15:56:59.210924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the original data set where X doen't contain any categorical properties\n\nfrom scipy.stats import zscore\nXScaled_Orig = X_Orig.apply(zscore)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:59.21309Z","iopub.execute_input":"2021-06-17T15:56:59.213401Z","iopub.status.idle":"2021-06-17T15:56:59.231418Z","shell.execute_reply.started":"2021-06-17T15:56:59.213368Z","shell.execute_reply":"2021-06-17T15:56:59.230335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score #Import the Library used to capture cross validation score\nfrom sklearn.model_selection import RepeatedStratifiedKFold #This is the library to apply the K-Fold technique\n\nvDataOrig_svm_k_fold_model_TransScale = SVC() #Instanciate the support vector model for Vehicle Type = Encoded Class\n\n# Define the cross validation method where the original data set is splitted into 10 folds\n# During each iteration (k-1) fold shall be used for training validation and kth fold shall be used as Test\n# The process will be repeated for 3 times to better evaluate the score and finally find the best score\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=5) \n\n# Get the Score\nscore = cross_val_score(vDataOrig_svm_k_fold_model_TransScale, \n                         XScaled_Orig, Y_Orig, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n\n# Print the score of the k-fold cross validation applied through Support Vector Machine model\nprint('Model Mean Score: %.3f | Score Standard Deviation: (%.3f)' % (np.mean(score), np.std(score)))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:56:59.23271Z","iopub.execute_input":"2021-06-17T15:56:59.232991Z","iopub.status.idle":"2021-06-17T15:57:01.719359Z","shell.execute_reply.started":"2021-06-17T15:56:59.232966Z","shell.execute_reply":"2021-06-17T15:57:01.718398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#6. Use PCA from Scikit learn, extract Principal Components that capture about 95% of the variance in the data\n\n# Here we shall focus on the Principal Component Analysis (PCA), hence not considering any categorical properties\n# i.e. \"class\" in target calculation.\n\n# X_Orig is the independent features which doesn't include the vehicle type i.e. class\n# Y_Orig is the dependent feature consists of values from vehicle class\n\nXScaled_Orig.head() #Have a look into the sample data set","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:01.721111Z","iopub.execute_input":"2021-06-17T15:57:01.721466Z","iopub.status.idle":"2021-06-17T15:57:01.748619Z","shell.execute_reply.started":"2021-06-17T15:57:01.721431Z","shell.execute_reply":"2021-06-17T15:57:01.747612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above sample data point doesn't include any 'class' column i.e. the categorical variable type of vehicle","metadata":{}},{"cell_type":"code","source":"# Get the covariance matrix for PCA analysis\ncovMatrix = np.cov(XScaled_Orig,rowvar=False)\nprint(covMatrix)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:01.756993Z","iopub.execute_input":"2021-06-17T15:57:01.757399Z","iopub.status.idle":"2021-06-17T15:57:01.775178Z","shell.execute_reply.started":"2021-06-17T15:57:01.757354Z","shell.execute_reply":"2021-06-17T15:57:01.771877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above covariance matrix depicts the relation between all possible pair of diamensions","metadata":{}},{"cell_type":"code","source":"# Performe PCA for the 18 features given\nfrom sklearn.decomposition import PCA # Import PCA library\n\npca_vDataOrig = PCA(n_components=18) #Instanciate the PCA with 18 components\npca_vDataOrig.fit(XScaled_Orig) #Fit the original dataset into the PCA model","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:01.777105Z","iopub.execute_input":"2021-06-17T15:57:01.777615Z","iopub.status.idle":"2021-06-17T15:57:01.815731Z","shell.execute_reply.started":"2021-06-17T15:57:01.777567Z","shell.execute_reply":"2021-06-17T15:57:01.814514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the Eigen values of each compoments\nprint(pca_vDataOrig.explained_variance_)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:01.817543Z","iopub.execute_input":"2021-06-17T15:57:01.81827Z","iopub.status.idle":"2021-06-17T15:57:01.826577Z","shell.execute_reply.started":"2021-06-17T15:57:01.818223Z","shell.execute_reply":"2021-06-17T15:57:01.825374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print corresponding Eigen vectors\nprint(pca_vDataOrig.components_)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:01.828249Z","iopub.execute_input":"2021-06-17T15:57:01.828728Z","iopub.status.idle":"2021-06-17T15:57:01.853493Z","shell.execute_reply.started":"2021-06-17T15:57:01.828684Z","shell.execute_reply":"2021-06-17T15:57:01.852051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the % of the variation for each component\nprint(pca_vDataOrig.explained_variance_ratio_)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:01.855527Z","iopub.execute_input":"2021-06-17T15:57:01.85612Z","iopub.status.idle":"2021-06-17T15:57:01.86822Z","shell.execute_reply.started":"2021-06-17T15:57:01.856072Z","shell.execute_reply":"2021-06-17T15:57:01.867079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the % of variation for each component\n# This is depict the influence of the components in an ordered way\nplt.bar(list(range(1,19)),pca_vDataOrig.explained_variance_ratio_,alpha=0.5, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:01.870014Z","iopub.execute_input":"2021-06-17T15:57:01.870632Z","iopub.status.idle":"2021-06-17T15:57:02.191515Z","shell.execute_reply.started":"2021-06-17T15:57:01.870585Z","shell.execute_reply":"2021-06-17T15:57:02.190387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above plot indicates that first 7 compoments have mejority of the the influence to the overall data set","metadata":{}},{"cell_type":"code","source":"# Plot the cuulative variation with respect to the eigen value of each components\n# This is to get a pictorial view to identify how many components contributes most significantly in the dataset\nplt.step(list(range(1,19)),np.cumsum(pca_vDataOrig.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:02.192605Z","iopub.execute_input":"2021-06-17T15:57:02.192897Z","iopub.status.idle":"2021-06-17T15:57:02.443216Z","shell.execute_reply.started":"2021-06-17T15:57:02.19287Z","shell.execute_reply":"2021-06-17T15:57:02.442219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Based on above plot of cumulative variation vs eigen value it depicts first 7 features can capture around 95% of the variane\n- This way dimensionality can be reduced from 18 to 7 ","metadata":{}},{"cell_type":"code","source":"# Let's review the PCA considering # of components = 7\npca_vDataOrig_7comp = PCA(n_components=7)\npca_vDataOrig_7comp.fit(XScaled_Orig)\n\nprint(pca_vDataOrig_7comp.components_)\nprint(pca_vDataOrig_7comp.explained_variance_ratio_)\nX_pca_vDataOrig_7comp = pca_vDataOrig_7comp.transform(XScaled_Orig)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:02.444547Z","iopub.execute_input":"2021-06-17T15:57:02.444859Z","iopub.status.idle":"2021-06-17T15:57:02.4686Z","shell.execute_reply.started":"2021-06-17T15:57:02.444829Z","shell.execute_reply":"2021-06-17T15:57:02.467097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look into the pair plot - with 7 selected components\n# This is to depict the relations between top 7 influencing components\nsns.pairplot(pd.DataFrame(X_pca_vDataOrig_7comp))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:02.470361Z","iopub.execute_input":"2021-06-17T15:57:02.47113Z","iopub.status.idle":"2021-06-17T15:57:13.838704Z","shell.execute_reply.started":"2021-06-17T15:57:02.47107Z","shell.execute_reply":"2021-06-17T15:57:13.837684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Few observations as follows:\n    - Except feature 0 and 2, all other features are normally distributed\n    - Small change of Feature 2 yeilds a significant change for all other 6 properties\n    - Feature# 0,1,3,4 and 5 have scattered relation with each other","metadata":{}},{"cell_type":"code","source":"# Let's store the eigen values and the eigen vectors into the variables for further calculation\nvData_e_vals, vData_e_vecs = np.linalg.eig(covMatrix)\nprint('Eigenvectors \\n%s' %vData_e_vals)\nprint('\\nEigenvalues \\n%s' %vData_e_vecs)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.840021Z","iopub.execute_input":"2021-06-17T15:57:13.840351Z","iopub.status.idle":"2021-06-17T15:57:13.853547Z","shell.execute_reply.started":"2021-06-17T15:57:13.840302Z","shell.execute_reply":"2021-06-17T15:57:13.85254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the eigen pairs for first 7 components\neigen_pairs = [(np.abs(vData_e_vals[i]), vData_e_vecs[:,i]) for i in range(len(vData_e_vals))]\neigen_pairs.sort(reverse=True)\neigen_pairs[:7]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.854921Z","iopub.execute_input":"2021-06-17T15:57:13.855231Z","iopub.status.idle":"2021-06-17T15:57:13.866995Z","shell.execute_reply.started":"2021-06-17T15:57:13.8552Z","shell.execute_reply":"2021-06-17T15:57:13.865903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build dimensionally reduced datasets (with 7 top components in this case)\nw = np.hstack((eigen_pairs[0][1].reshape(18,1), eigen_pairs[1][1].reshape(18,1),\n               eigen_pairs[2][1].reshape(18,1), eigen_pairs[3][1].reshape(18,1),\n               eigen_pairs[4][1].reshape(18,1), eigen_pairs[5][1].reshape(18,1),\n               eigen_pairs[6][1].reshape(18,1)))\nprint('Matrix W:\\n', w)\nXScaled_PCA = XScaled_Orig.dot(w) #Build the full data set with only 7 compoments","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.870147Z","iopub.execute_input":"2021-06-17T15:57:13.87047Z","iopub.status.idle":"2021-06-17T15:57:13.883845Z","shell.execute_reply.started":"2021-06-17T15:57:13.870441Z","shell.execute_reply":"2021-06-17T15:57:13.882572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XScaled_PCA.shape #View the shape of the Scaled Dataset built after choosing the 7 most contributing components","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.885355Z","iopub.execute_input":"2021-06-17T15:57:13.885755Z","iopub.status.idle":"2021-06-17T15:57:13.906075Z","shell.execute_reply.started":"2021-06-17T15:57:13.885713Z","shell.execute_reply":"2021-06-17T15:57:13.904733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above shows the new dataset built after performing PCA along with 7 features has all 846 records of vehicle silhouette","metadata":{}},{"cell_type":"code","source":"# Independent data set build from original data set without categorical/target column \"class\"\nX_pca_vDataOrig_7comp #This is the Numpy arrary built earlier for PCA with 7 components","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.907848Z","iopub.execute_input":"2021-06-17T15:57:13.908302Z","iopub.status.idle":"2021-06-17T15:57:13.919745Z","shell.execute_reply.started":"2021-06-17T15:57:13.908254Z","shell.execute_reply":"2021-06-17T15:57:13.918774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XScaled_PCA #Data set built with 7 components","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.921612Z","iopub.execute_input":"2021-06-17T15:57:13.921946Z","iopub.status.idle":"2021-06-17T15:57:13.946074Z","shell.execute_reply.started":"2021-06-17T15:57:13.921912Z","shell.execute_reply":"2021-06-17T15:57:13.944982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Data set built with 7 components and it's matching with the Numpy array calculated above\n- This Scaled and data set with 7 component (i.e. XScaled_PCA) will be used further for comparison","metadata":{}},{"cell_type":"code","source":"#7. Repeat steps 3,4 and 5 but this time, use Principal Components instead of the original data. \n# And the accuracy score should be on the same rows of test data that were used earlier. (hint: set the same random state) ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.947531Z","iopub.execute_input":"2021-06-17T15:57:13.947869Z","iopub.status.idle":"2021-06-17T15:57:13.958596Z","shell.execute_reply.started":"2021-06-17T15:57:13.947831Z","shell.execute_reply":"2021-06-17T15:57:13.95754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3.a Split the data into train and test (by specifying “random state” as using train_test_split from Sklearn)\n\n# Split data considering encoded class as target\n# 5 is just any random seed number and it the same as used earlier\nx_train_PCA, x_test_PCA, y_train_PCA, y_test_PCA = train_test_split(XScaled_PCA, Y_Orig, test_size=0.3, random_state=5) ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.960909Z","iopub.execute_input":"2021-06-17T15:57:13.961259Z","iopub.status.idle":"2021-06-17T15:57:13.97454Z","shell.execute_reply.started":"2021-06-17T15:57:13.961214Z","shell.execute_reply":"2021-06-17T15:57:13.973425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4.a Train a Support vector machine using the train set and get the accuracy on the test set\n# For the comparison after Principal COmponent Analysis\nsvm_lassifier = SVC()\nsvm_lassifier.fit(x_train_orig_TransScale, y_train_orig)\nprint ('Before PCA score', svm_lassifier.score(x_test_orig_TransScale, y_test_orig))\n\nsvm_lassifier.fit(x_train_PCA, y_train_PCA)\nprint ('After PCA score', svm_lassifier.score(x_test_PCA, y_test_PCA))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:13.976281Z","iopub.execute_input":"2021-06-17T15:57:13.976785Z","iopub.status.idle":"2021-06-17T15:57:14.033824Z","shell.execute_reply.started":"2021-06-17T15:57:13.976673Z","shell.execute_reply":"2021-06-17T15:57:14.032837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5.a K-fold cross validation model for SVM on dimensionally reduced data set prepared for PCA\n\nfrom sklearn.model_selection import cross_val_score #Import the Library used to capture cross validation score\nfrom sklearn.model_selection import RepeatedStratifiedKFold #This is the library to apply the K-Fold technique\n\nvDataPCA_svm_k_fold_model_TransScale = SVC() #Instanciate the support vector model for Vehicle Type = Encoded Class\n\n# Define the cross validation method where the original data set is splitted into 10 folds\n# During each iteration (k-1) fold shall be used for training validation and kth fold shall be used as Test\n# The process will be repeated for 3 times to better evaluate the score and finally find the best score\ncv_pca = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=5) \n\n# Get the Score\nscore_pca = cross_val_score(vDataPCA_svm_k_fold_model_TransScale, \n                         XScaled_PCA, Y_Orig, scoring='accuracy', cv=cv_pca, n_jobs=-1, error_score='raise')\n\n# Print the score of the k-fold cross validation applied through Support Vector Machine model\nprint('Model Mean Score on PCA data set: %.3f | Score Standard Deviation: (%.3f)' % (np.mean(score_pca), np.std(score_pca)))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:14.035616Z","iopub.execute_input":"2021-06-17T15:57:14.036021Z","iopub.status.idle":"2021-06-17T15:57:14.42613Z","shell.execute_reply.started":"2021-06-17T15:57:14.03597Z","shell.execute_reply":"2021-06-17T15:57:14.424961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#8. Compare the accuracy scores and cross validation scores of Support vector machines – \n# one trained using raw data and the other using Principal Components, and mention your findings","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:14.427385Z","iopub.execute_input":"2021-06-17T15:57:14.427765Z","iopub.status.idle":"2021-06-17T15:57:14.431516Z","shell.execute_reply.started":"2021-06-17T15:57:14.427723Z","shell.execute_reply":"2021-06-17T15:57:14.430684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reasoning on which model is best and their corresponding performance\n\nprint(\"Classification Report - SVM (for Vehicle Type - Bus)\")\nprint(metrics.classification_report(y_test_bus, vData_bus_svm_test_predict_TransScale, labels=[1, 0]))\nprint(\"\")\nprint(\"Classification Report - SVM (for Vehicle Type - Car)\")\nprint(metrics.classification_report(y_test_car, vData_car_svm_test_predict_TransScale, labels=[1, 0]))\nprint(\"\")\nprint(\"Classification Report - SVM (for Vehicle Type - Van)\")\nprint(metrics.classification_report(y_test_van, vData_van_svm_test_predict_TransScale, labels=[1, 0]))\nprint(\"\")\nprint(\"Classification Report - SVM (for Vehicle Type - Encoded Class)\")\nprint(metrics.classification_report(y_test_orig, vDataOrig_svm_test_predict_TransScale, labels=[1, 0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T15:57:14.432538Z","iopub.execute_input":"2021-06-17T15:57:14.433108Z","iopub.status.idle":"2021-06-17T15:57:14.47161Z","shell.execute_reply.started":"2021-06-17T15:57:14.433079Z","shell.execute_reply":"2021-06-17T15:57:14.470673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reasoning on which model is best and their corresponding performance\n\n- From above comparative study we make following observation:\n- Precision:\n    - Ability of a classifier not to label an instance positive that is actually negative i.e. for all instances classified positive, what percent was correct. Best observed in SVM model for encoded class column.\n- Recall:\n    - Ability of a classifier to find all positive instances i.e. for all instances that were actually positive, what percent was classified correctly. With this respect SVM with encoded class model has the highest score of predicting vehicle type.\n- f1 score: \n    - Harmonic mean of Precision and Recall. SVM model (for encoded class) model has the highest f1 value for the prediction of vehicle type denoted by 1\n\n\n- Model Accuracy comparison between SVM computed before and after of PCA:\n    - Before PCA the total# of components were 18 and after PCA the component count is 7\n    - By reducing 11 components, the over all model performance reduced only by 3% (94% to 90%)\n    - From cost analysis perspective it's a significant gain\n    \n\n- Model Accuracy comparison between K-Fold cross validation computed before and after of PCA:\n    - Before PCA the total# of components were 18 and after PCA the component count is 7\n    - By reducing 11 components, the over all model performance reduced only by 5% (97% to 92%)\n    - From cost analysis perspective it's still a significant gain as the model accuracy is 92%","metadata":{}},{"cell_type":"markdown","source":"- Hence from all above analysis we can infer following conclusion:\n\n\n- Before Principal Compnent Analysis (PCA): SVM model with encoded class properties yeilds the best accuracy (94%) in predicting the vehicle type based on the silhouette provided.\n\n\n- After Principal Compnent Analysis (PCA): k-fold cross validaiton model yeilds best accuracy (92%) after dropping the diamensionality from 18 to 7","metadata":{}}]}