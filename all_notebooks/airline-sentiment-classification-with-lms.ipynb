{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analyzing airline sentiment from tweets"},{"metadata":{},"cell_type":"markdown","source":"Even though this dataset is quite rich in metadata, the goal here is to explore how far can we get if we discard all of it, just using the raw tweet text.\n\nWe're going to use a standard fastai language model, pretrained on the English Wikipedia, and fine-tuned on the raw tweets with minimal pre-processing."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom fastai.text import *\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First let's load the data and have a quick look:"},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_path = Path('/kaggle/input/twitter-airline-sentiment')\npath = '/kaggle/output'\ndf = pd.read_csv(csv_path / 'Tweets.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing\nKeeping it to a minimum, we are only going to strip any twitter handles. We'll leave everything else."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nstrip_handles = lambda text: re.sub(r'@[^\\s]+\\s', '', text)\nremove_urls = lambda text: re.sub(r'\\shttps?:[^\\s]+\\s?', '', text)\n\ntweet = df.at[42, 'text']\ntweet, strip_handles(tweet), remove_urls('visit this site http://www.google.com')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will turn it into a fastai preprocessor and prefix it to the default spacy-based tokenizing and numericalizing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StripWeirdThings(PreProcessor):\n    def process_one(self, item: str) -> str:\n        return remove_urls(strip_handles(item))\n    \nprocessor = [StripWeirdThings(), TokenizeProcessor(), NumericalizeProcessor()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're ready to create the databunch for the language model. We'll be reserving 20% of it for cross-validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm = (TextList.from_df(df, path, cols=['text'], \n            processor=processor)\n           .split_by_rand_pct(0.2, seed=42)\n           .label_for_lm()\n           .databunch(bs=42, num_workers=1))\ndata_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use a standard AWD_LSTM with pretrained weights from the English Wikipedia. We'll use mixed precision training to speed up the process."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)\nmin_grad_lr = learn.recorder.min_grad_lr\nmin_grad_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, 1e-02, moms=(0.8, 0.7))\nlearn.recorder.plot_losses()\nlearn.save('lm_head.model')\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(10, 1e-3, moms=(0.9, 0.8))\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('lm_head_2.model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's unfreeze the backbone and do some more training."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.show_results()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that validation loss is starting to go up. Let's stop here, reload the trained head and roll back to 3 epochs of backbone training."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('lm_head_2.model')\nlearn.save_encoder('lm_encoder')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training a multi-label classifier with the LM encoder"},{"metadata":{},"cell_type":"markdown","source":"Now that we've trained a language model to predict the next word of a tweet, we'll use the encoder part of the language model to as an input to our classifier.\n\nWe'll approach the problem as a multi-label classifier problem, concatenating positive and neutral sentiments with specific negative reasons as labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_label(sent_and_reason):\n    sent = sent_and_reason[0]\n    reason = sent_and_reason[1]\n    if sent == 'negative':\n        return reason\n    else:\n        return sent\n\ndf['label'] = df[['airline_sentiment', 'negativereason']].apply(create_label, axis=1)\ndf['label'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total of 12 classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas = (TextList.from_df(df, path, cols='text', vocab=data_lm.vocab, processor=processor)\n             .split_by_rand_pct(0.2, seed=42)\n             .label_from_df(cols='label')\n             .databunch(bs=64))\n\ndata_clas.save('data_clas_export.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3)\nlearn.load_encoder('lm_encoder')\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)\nmin_grad_lr = learn.recorder.min_grad_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(6, 1e-02)\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-2)\nlearn.fit_one_cycle(4, slice(5e-3, 2e-3), moms=(0.8,0.7))\nlearn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(4, slice(2e-3/100, 2e-3), moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got around 60% accuracy in a 12-class classification problem. Not bad!"},{"metadata":{},"cell_type":"markdown","source":"## Predicting and interpreting classification results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(learn, tweet):\n    learn.freeze()\n    learn = learn.to_fp32()\n    interp = TextClassificationInterpretation.from_learner(learn)\n    interp.show_intrinsic_attention(tweet)\n    return learn.predict(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(learn, df.at[8992, 'text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(learn, df.at[1000, 'text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}