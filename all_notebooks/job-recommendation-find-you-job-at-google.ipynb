{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Hi everyone! It's been a while since last time I showed up on Kaggle. \n\n#### This time, I'm also working on the Google Job dataset. While this time, I would like to build a simple recommendation system based on the scenario of looking for a position and finding similar openings for users this time","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![google](http://img.technews.tw/wp-content/uploads/2015/09/Google-logo_1.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Outline\n\n## Recommendation System\n\n- [EDA](#0)   \n    * I'll do simple exploratory on the data structure and values\n- [Modeling](#1)\n    * I'll start to test out vectorize text and find similar positions based on job description\n- [Finalizing](#2)\n    * Will also consider requirements in this part\n    \n## [Text Clustering](#Cluster)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/google-job-skills/job_skills.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [EDA](#0)\n\n**First, I would like to know more about the data**\n\n- Starting from the columns\n- Then, the text pattern in columns\n- Finally, the correlation between different positions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I modify the column name so that I can use df dot column name more easily\ndf = df.rename(columns={'Minimum Qualifications': 'Minimum_Qualifications', 'Preferred Qualifications': 'Preferred_Qualifications'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Company.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Location.value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'] = df['Location'].apply(lambda x : x.split(',')[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Country.value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.isnull(df).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(how='any',axis='rows')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [Modeling](#1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform the necessary imports for similarity\nfrom sklearn.decomposition import NMF\nfrom sklearn.preprocessing import Normalizer, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\n\n\nscaler = MaxAbsScaler()\n\nmodel = NMF(n_components=100)\n\nnormalizer = Normalizer()\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(scaler,model,normalizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nvectors_Responsibilities = vectorizer.fit_transform(df['Responsibilities'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Responsibilities = pipeline.fit_transform(vectors_Responsibilities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Responsibilities = pd.DataFrame(Responsibilities,index=df['Title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Responsibilities.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\nprint(df[df.Title.str.contains('Data Scientist')]['Title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Position = df_Responsibilities.loc['Customer Experience Data Scientist, Google Cloud Support']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarities_1 = df_Responsibilities.dot(Position)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarities_1[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(similarities_1.nlargest())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's see if the role is similar and ideal as an alternative.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[np.isin(df['Title'],similarities_1.nlargest().index.tolist())].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(similarities_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In my opinion, the role is a good alternative choice while the requirement could be a blocker. So let's also consider the part of requirements.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_Requirements = TfidfVectorizer()\nvectors_Requirements = vectorizer_Requirements.fit_transform(df['Minimum_Qualifications'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Requirements = pipeline.fit_transform(vectors_Requirements)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Requirementss = pd.DataFrame(Requirements,index=df['Title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Position = df_Requirementss.loc['Customer Experience Data Scientist, Google Cloud Support']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarities_2 = df_Responsibilities.dot(Position)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(similarities_2.nlargest())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though this looks a bit weird, let's see how we put responsibilities and requirements together first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"similarities_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarities_1.rename(\"similarity\")\nsimilarities_2.rename(\"similarity\")\n\nsimilarities_1.to_frame().join(similarities_2.to_frame(),lsuffix='1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarities_overall = (2 * similarities_1) + similarities_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(similarities_overall.nlargest())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[np.isin(df['Title'],similarities_overall.nlargest(3).index.tolist())].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The result is not bad! Though one of the alternative position looks more emphasize soft skills part while another is similar in terms of the hard skills part, I think they both look like a good choice as well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## [Text Clustering](#Clustering)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The purpose of this part is aiming at finding the relevant words, skills, requirements across different roles using Cluster Analysis instead of Word Cloud in my previous project.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.vq import kmeans, vq\nfrom numpy import random\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport string\n\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom spacy.lang.en.stop_words import STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words_0 = set(stopwords.words('english')) \nstop_words = ['and', 'in', 'of', 'or', 'with','to','on','a']\n\ndef remove_noise(text):\n    tokens = word_tokenize(text)\n    clean_tokens = []\n    lemmatizer=WordNetLemmatizer()\n    for token in tokens:\n        token = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', '', token)\n        token = lemmatizer.lemmatize(token.lower())\n        if len(token) > 1 and token not in stop_words_0 and token not in stop_words:\n            clean_tokens.append(token)\n            \n    return clean_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=100,tokenizer=remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['Minimum_Qualifications'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed = 123","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distortions = []\nnum_clusters = range(2, 25)\n\n# Create a list of distortions from the kmeans function\nfor i in num_clusters:\n    cluster_centers, distortion = kmeans(tfidf_matrix.todense(),i)\n    distortions.append(distortion)\n\n# Create a data frame with two lists - num_clusters, distortions\nelbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n\n# Creat a line plot of num_clusters and distortions\nsns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\nplt.xticks(num_clusters)\nplt.title('Clusters and Distortions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_centers, distortion = kmeans(tfidf_matrix.todense(),13)\n\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(13):\n    # Sort the terms and print top 10 terms\n    center_terms = dict(zip(terms, list(cluster_centers[i])))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the groups of words, I can tell different groups are from different fields of the positions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add in the rest of the parameters\ndef return_weights(vocab, original_vocab, vector, vector_index, top_n):\n    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n    \n    # Let's transform that zipped dict into a series\n    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n    \n    # Let's sort the series to pull out the top n weighted words\n    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n    return [original_vocab[i] for i in zipped_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = {v:k for k,v in tfidf_vectorizer.vocabulary_.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def words_to_filter(vocab, original_vocab, vector, top_n):\n    filter_list = []\n    for i in range(0, vector.shape[0]):\n    \n        # Here we'll call the function from the previous exercise, and extend the list we're creating\n        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n        filter_list.extend(filtered)\n    # Return the list in a set, so we don't get duplicate word indices\n    return set(filter_list)\n\n# Call the function to get the list of word indices\nfiltered_words = words_to_filter(vocab, tfidf_vectorizer.vocabulary_, tfidf_matrix, 5)\n\n# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\nfiltered_text = tfidf_matrix[:, list(filtered_words)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(filtered_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}