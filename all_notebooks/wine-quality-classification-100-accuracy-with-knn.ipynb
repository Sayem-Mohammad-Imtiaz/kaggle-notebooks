{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us do some EDA on the dataset","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the dataset has no null values in any of the features, labels.<br>\nTherefore imputation is not necessary.<br>\nLet us now find out the correlations between the features and labels.<br>","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"since there are a lot of input quantities let us print the corr values","metadata":{}},{"cell_type":"code","source":"df.corr()['quality'].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above correlation helps us understand the parameters that increase the quality(parameters>0) and<br>\nthe parameters that decrease the quality(parameters<0)<br>\nfor residual sugar correlation is almost zero and we can drop it","metadata":{}},{"cell_type":"markdown","source":"Before plotting the distribution plots, let us find out the nature of the label(quality)","metadata":{}},{"cell_type":"code","source":"df.quality.unique() #gives the actual number of labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df.quality) #to see how the output labels are distributed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows that the output lables are not uniformly distributed.<br>\nHence we have to perform some sampling on the input , which we will get to after we see how the data<br>\nis distributed","metadata":{}},{"cell_type":"code","source":"df.hist(figsize=(20,15),bins=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us apply log transformation on features that are more concentrated to the left and make them uniform<br>\ninstead standard scaling can also be used","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndf['chlorides'] = df['chlorides'].apply(lambda x : np.log(x))\ndf['free sulfur dioxide'] = df['free sulfur dioxide'].apply(lambda x : np.log(x))\ndf['total sulfur dioxide'] = df['total sulfur dioxide'].apply(lambda x : np.log(x))\ndf['sulphates'] = df['sulphates'].apply(lambda x : np.log(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df.hist(bins=50,figsize=(20,15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oversample the quality levels 3,4,7,8 along with the majority class to a sufficiently large value(5000)","metadata":{}},{"cell_type":"code","source":"df.quality.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy = {5: 5000, 6: 5000, 7: 5000, 4: 5000, 8: 5000, 3: 5000})\nX_test_os,y_test_os = sm.fit_resample(df.drop(['quality','residual sugar'],axis=1),df['quality'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_test_os,y_test_os, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets do some machine learning with knn<br>\nActual knn parameters found after hyper parameter tuning for best accuracy,scroll below to find the same.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1,leaf_size=10,p=1,metric='manhattan')\nknn.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = knn.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(pred,y_test))\nprint(classification_report(pred,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = [{'weights' : ['uniform', 'distance'],'leaf_size' :[10,20,30],'n_neighbors':[1,10,20,30],\n           'p':[1,2,3]}]\nknn_ = KNeighborsClassifier()\ngrid_search = GridSearchCV(knn_,params,cv=3,n_jobs=100,scoring='f1')\ngrid_search.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn.effective_metric_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}