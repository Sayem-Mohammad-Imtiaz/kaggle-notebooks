{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing requisite libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', delimiter=',', encoding='latin-1')\nprint(df)\n\nY = df['v1']\nX = df['v2']\n\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\nX_train = X_train.tolist()\nX_test = X_test.tolist()\n\nX_train = [text.lower() for text in X_train]\nX_test = [text.lower() for text in X_test]\n\nlabel2idx = {\n    'ham':0,\n    'spam':1\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizing and Padding of text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 150\nvocab_size = 10000\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(X_train)\ntrain_seq = tokenizer.texts_to_sequences(X_train)\ntrain_pad = pad_sequences(train_seq, maxlen=maxlen, truncating='post')\ntest_seq = tokenizer.texts_to_sequences(X_test)\ntest_pad = pad_sequences(test_seq, maxlen=maxlen, truncating='post')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Constructing the RNN architecture \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, 128, input_length=maxlen),\n    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(train_pad, y_train, epochs=10, validation_data=(test_pad, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary of training and accuracy on testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\nmodel.evaluate(test_pad, y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting accuracy and loss vs no. of epochs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel('epochs')\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'loss')\nplot_graphs(history, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  **Performing an alternate training using tfidf vectorization for text.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.5)\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training using logistic regression classifier(due to large no. of features)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(penalty='l2', C=10).fit(X_train_tfidf, y_train)\ny_pred = clf.predict(X_test_tfidf)\nprint(y_pred[:1000])\ny_pred_labels = []\n\n#Converting output of classifier back in terms of input ham/spam labels.\nfor i in y_pred:\n    for label, idx in label2idx.items():\n        if i==idx:\n            y_pred_labels = np.append(y_pred_labels, label)\nprint(y_pred_labels[:1000])\n\n        \n    \n\n#Final accuracy on testing.\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}