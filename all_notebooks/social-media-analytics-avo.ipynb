{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Social Media Analytics\nSocial media analytics is the practice of gathering data from social media websites and analyzing that data using social media analytics tools to make business decisions. The most common use of social media analytics is to mine customer sentiment to support marketing and customer service activities. <br>\n\nThe first step in a social media intelligence initiative is to determine which business goals the data that is gathered and analyzed will benefit. Typical objectives include increasing revenues, reducing customer service costs, getting feedback on products and services, and improving public opinion of a particular product or business division. <br>\n\nOnce the business goals have been identified, businesses should define key performance indicators (KPIs) to objectively evaluate the business analytics data. <br>\n\n[ref](https://searchbusinessanalytics.techtarget.com/definition/social-media-analytics)"},{"metadata":{},"cell_type":"markdown","source":"### About this Dataset\n\nThis data originally came from Crowdflower's Data for Everyone library. As the original source says:\n\n> A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\")."},{"metadata":{},"cell_type":"markdown","source":"## Table Of Content\n### 1. Preprocessing\n### 2. Exploratory data analysis (EDA)\n### 3. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 0. Load Data & Libraries\n\nFirst thing, we need to import some libraries and load the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import mathematical & dataframe module\nimport numpy as np \nimport pandas as pd\n\n#import text module\nimport wordcloud as wc\nimport numpy as np\nimport textblob\nimport re, string, unicodedata\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom textblob import TextBlob\nfrom textblob import Word\n\n\n#import visualization module \nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nsns.set(style='darkgrid')\nfrom wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\ntweet.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 1. Preprocessing\n\nData-gathering methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results.\n\n[ref](https://en.wikipedia.org/wiki/Data_pre-processing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#copy dataframe to tweet2\ntweet2 = tweet.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_drop = tweet2.drop(columns=['tweet_id','tweet_created','tweet_location','user_timezone','tweet_coord'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_drop.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop duplicates from data\ntweet_drop.drop_duplicates(subset =\"text\",keep = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check data variables\ntweet_drop.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data completeness in range 0-100\ntweet_drop.count().sort_values(ascending = False) / len(tweet_drop)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Multiple Step Preprocessing\n\nWe want to make wordcloud, so we must do some preprocessing step to achieve that. <br>\n[preprocessing ref](https://link.springer.com/chapter/10.1007/978-3-319-67008-9_31)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cleansing \ndef preprocessing(text):\n    \n    def removeUnicode(text):\n        \n        text = re.sub(r'(\\\\u[0-9A-Fa-f]+)','', text)       \n        text = re.sub(r'[^\\x00-\\x7f]','',text)\n        return text\n\n    def replaceURL(text):\n        \n        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n        text = re.sub(r'#([^\\s]+)', '', text)\n        return text\n\n    def replaceAtUser(text):\n        \n        text = re.sub('@[^\\s]+','',text)\n        return text\n\n    def removeHashtagInFrontOfWord(text):\n        \n        text = re.sub(r'#([^\\s]+)', r'\\1', text)\n        return text\n\n    def removeNumbers(text):\n        \n        text = ''.join([i for i in text if not i.isdigit()])         \n        return text\n\n    def replaceMultiExclamationMark(text):\n        \n        text = re.sub(r\"(\\!)\\1+\", '', text)\n        return text\n\n    def replaceMultiQuestionMark(text):\n        \n        text = re.sub(r\"(\\?)\\1+\", '', text)\n        return text\n\n    def replaceMultiStopMark(text):\n        \n        text = re.sub(r\"(\\.)\\1+\", '', text)\n        return text\n    \n    def removeEmoticons(text):\n        \n        text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n        return text\n\n    \n    contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n                             (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&amp', ''), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n    def replaceContraction(text):\n        patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n        for (pattern, repl) in patterns:\n            (text, count) = re.subn(pattern, repl, text)\n        return text\n\n    def replaceElongated(word):\n        \n\n        repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n        repl = r'\\1\\2\\3'\n        if wordnet.synsets(word):\n            return word\n        repl_word = repeat_regexp.sub(repl, word)\n        if repl_word != word:      \n            return replaceElongated(repl_word)\n        else:       \n            return repl_word\n\n    def removeEmoticons(text):\n        \n        text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n        return text\n    # Removes unicode strings like \"\\u002c\" and \"x96\"\n    text = removeUnicode(text)\n    # Replaces url address with \"url\"\n    text = replaceURL(text)\n    # Removes hastag in front of a word\n    text = replaceAtUser(text)\n    # Replaces \"@user\"\n    text = removeHashtagInFrontOfWord(text)\n    # Removes integers \n    text = removeNumbers(text)\n    # Replaces repetitions of exlamation marks\n    text = replaceMultiExclamationMark(text)\n    # Replaces repetitions of question marks\n    text = replaceMultiQuestionMark(text)\n    # Replaces repetitions of stop marks\n    text = replaceMultiStopMark(text)\n    # Removes emoticons from text\n    text = removeEmoticons(text)\n    # Replaces contractions from a string to their equivalents\n    text = replaceContraction(text)\n    # Replaces an elongated word with its basic form, unless the word exists in the lexicon\n    text = replaceElongated(text)\n    # Removes emoticons from text\n    text = removeEmoticons(text)\n    \n    return text.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_drop['text'] = tweet_drop['text'].apply(preprocessing)\ntweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Punctuation removal\n\nmethod to remove punctuation on tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_drop['text'] = tweet_drop['text'].str.replace('[^\\w\\s]','')\ntweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3 Stopword Removal\n\nmethod to remove stop word on tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = stopwords.words('english')\ntweet_drop['text'] = tweet_drop['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Remove Frequent Word\n\nmethod to erase frequent word on tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(tweet_drop['text']).split()).value_counts()[:3]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = list(freq.index)\ntweet_drop['text'] = tweet_drop['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.5 Rare Word Removal\n\nmethod to erase rare word on tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"rare_word = pd.Series(' '.join(tweet_drop['text']).split()).value_counts()[-5000:]\nrare_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rare_word = list(rare_word.index)\ntweet_drop['text'] = tweet_drop['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.6 Correction Word\n\nmethod to correction some word. but, if we use this method to our dataset would be a problem for running time. If you want, you can change '10' to ' ', it will correct every word in dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_drop['text'][:10].apply(lambda x: str(TextBlob(x).correct()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.7 Stemming & Lemmatization"},{"metadata":{},"cell_type":"markdown","source":"I choose to not using Stemming and Lemmatization because it will hard to recognize/read when we make the wordcloud. If you want to use that, just erase the hashtag. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stemming\n\n# st = PorterStemmer()\n# tweet_drop['text'] = tweet_drop['text'][:].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n# tweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lemmatization\n\n# tweet_drop['text'] = tweet_drop['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n# tweet_drop['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory data analysis (EDA)\n\nIn this part, we will explore what happend/do some data analysis with this dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = tweet_drop.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet['tweet_created'] = tweet.tweet_created.str[:10]\ndate = tweet['tweet_created'].unique().tolist()\ndate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is collected on 16 February 2015 - 24 February 2015."},{"metadata":{},"cell_type":"markdown","source":"We want too see our sentiment on tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nplt.title('Count Plot',fontsize = 20)\nax=sns.countplot(data=df, x='airline_sentiment',order = df['airline_sentiment'].value_counts().index)\n\nplt.figure(figsize=(8,6))\nsns.countplot(x=df[\"airline\"])\nplt.title(\"Airlines Distribution\")\n\nax.set_xlabel('airline_sentiment', fontsize = 15)\nax.tick_params(labelsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The count plot shows that the majority of data we are working contains so much tweets that have negative sentiments. What about the airlines? the tweet are dominant from United company and the least is Virgin America. <br> <br>\nThe question is, are the company have so many negative sentiment tweets too?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sub_sentiment(Airline):\n    data=df[df['airline']==Airline]\n    count=data['airline_sentiment'].value_counts().index\n    ax=sns.countplot(data=data, x='airline_sentiment',order = count)\n    plt.title('Count Plot '+Airline,fontsize = 15)\n    plt.ylabel('Sentiment Count')\n    plt.xlabel('Mood')\n    \nplt.figure(1,figsize=(15, 15))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\nplt.subplot(231)\nplot_sub_sentiment('US Airways')\nplt.subplot(232)\nplot_sub_sentiment('United')\nplt.subplot(233)\nplot_sub_sentiment('American')\nplt.subplot(234)\nplot_sub_sentiment('Southwest')\nplt.subplot(235)\nplot_sub_sentiment('Delta')\nplt.subplot(236)\nplot_sub_sentiment('Virgin America')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can get insight in 16 February until 24 February, US Airways, American Airways and United Airways got negative sentiment from public. Although other companies also get negative sentiment too, but not as many as these 3 companies."},{"metadata":{},"cell_type":"markdown","source":" Lets see whats make them got negative sentiment/problem from that 3 airlines (US Airways, American Airways and United Airways)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"three_airlines = df.copy()\nthree_airlines = three_airlines[(three_airlines['airline'] == 'US Airways') | (three_airlines['airline'] == 'United') | (three_airlines['airline'] == 'American')]\nthree_airlines = three_airlines[three_airlines['airline_sentiment'] == 'negative']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"three_airlines.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22, 5))\nax = sns.countplot(x=\"negativereason\", hue=\"airline\", data=three_airlines)\nplt.xticks(rotation=15)\nplt.ylabel('Sentiment Count')\nplt.xlabel('Negative Reason')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In 16 February until 25 February, we can see:\n> for United airlines and US Airways the problem is Customer Service and Late Flight. <br>\n\n> for American airlines the problem is Customer Service and Cancelled Flight. <br>"},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define happy and not happy \nsentiment_positive = df.loc[df['airline_sentiment'] == \"positive\"]\nsentiment_neutral  = df.loc[df['airline_sentiment'] == \"neutral\"]\nsentiment_negative = df.loc[df['airline_sentiment'] == \"negative\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_positive.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge all the happy comments into one paragraph\nall_description_happy = \"\".join(sentiment_positive['text'].values)\nall_description_neutral = \"\".join(sentiment_neutral['text'].values)\nall_description_not_happy = \"\".join(sentiment_negative['text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_word_cloud(string):\n    plt.figure(1,figsize=(10, 10))\n    cloud = WordCloud(background_color = \"white\",width=1000,\n                      height=500, max_words = 150, stopwords = set(STOPWORDS)).generate(string)\n    plt.imshow(cloud, interpolation='bilinear')\n    \n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### happy wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncreate_word_cloud(all_description_happy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, 'thank' word is the biggest (thats mean so many word on positive sentiment) word on the wordcloud. 'great' and 'time' are the top 2 after thank. Thats because time means the flight are on time and have a great quality or experience."},{"metadata":{},"cell_type":"markdown","source":"### neutral wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"#neutral\ncreate_word_cloud(all_description_neutral)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On neutral wordcloud, the 'flight' word are the biggest. We can see the small part are dominant, in my opinion thats because frequency of word are so much. The other intersting part is 'time' and 'help' are top 6 from the neutral sentiment. "},{"metadata":{},"cell_type":"markdown","source":"### not happy wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"#not happy \ncreate_word_cloud(all_description_not_happy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'One' and 'hour' word explain the late of the flight and followed by 'time', so its must be the time problem. 'stil' can be interpreted as 'still at the airport'."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n> 1. There are so much negative sentiment to the six airlines on 16 february to 24 february.\n\n> 2. Top 3 negative sentiment (US Airways, American Airways and United Airways), are having same problem: Customer Sercive Issue.\n\n> 3. The positive sentiment can be represented by word 'thank', the negative with 'hour' and the neutral with 'flights'."},{"metadata":{},"cell_type":"markdown","source":"***.avo***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}