{"cells":[{"metadata":{"id":"i81ObA5QpJal","outputId":"143f723e-73f2-4cf7-f29f-ec351597866e","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom PIL import Image\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom skimage import io\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport os\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport torchvision\nimport torch.optim as optim\nimport copy\nfrom sklearn.externals import joblib\nfrom skimage.transform import rotate, AffineTransform, warp\nfrom skimage.util import random_noise\nfrom skimage.filters import gaussian\nfrom skimage.transform import resize\nfrom sklearn.metrics import f1_score as f\nfrom PIL import Image ","execution_count":null,"outputs":[]},{"metadata":{"id":"LJfx8SHibhHV","outputId":"607c1a93-564c-4f78-db3a-7fbdc5af7914","trusted":false},"cell_type":"code","source":"#enable GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"hvzpzRlLrAS3","trusted":false},"cell_type":"code","source":"train=pd.read_csv('../input/identify-the-dance-form/test.csv')#specify input location\ntest=pd.read_csv('../input/identify-the-dance-form/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"ZmFfgNcP_ePx","trusted":false},"cell_type":"code","source":"Class_map={'manipuri':0, 'bharatanatyam':1, 'odissi':2 ,'kathakali':3, 'kathak':4, 'sattriya':5,\n 'kuchipudi':6, 'mohiniyattam':7}\ninverse_map={0:'manipuri', 1:'bharatanatyam', 2:'odissi' ,3:'kathakali',4: 'kathak', 5:'sattriya',\n 6:'kuchipudi', 7:'mohiniyattam'}\ntrain['target']=train['target'].map(Class_map)#repalcing names with repective labels","execution_count":null,"outputs":[]},{"metadata":{"id":"VqWfmP7Rr9z9","outputId":"28b3114c-01c0-4219-af77-e6a5d79ad429","trusted":false},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"id":"tcI3CkYiO5E2","outputId":"604d2876-530e-4669-d6e4-a909790c0017","trusted":false},"cell_type":"code","source":"test_imgdir=[]#collecting all the locaions of test images\nfor img in list(test['Image']):\n  test_imgdir.append('../input/identify-the-dance-form/test/'+str(img))\nprint(test_imgdir)","execution_count":null,"outputs":[]},{"metadata":{"id":"qqbDQp3ArJgx","outputId":"8e3126e6-a5c9-4dec-89c8-c163eabc20d5","trusted":false},"cell_type":"code","source":"train_imgdir=[]#collecting all the locations of train images\nfor img in list(train['Image']):\n  train_imgdir.append('../input/identify-the-dance-form/train/'+str(img))\nprint(train_imgdir)","execution_count":null,"outputs":[]},{"metadata":{"id":"pEpz6B9JrPLK","outputId":"2f928f4b-1339-43c2-e3bf-c22a5b572d2f","trusted":false},"cell_type":"code","source":"train_label=[]#collecting labels of train images\nfor i in list(train['target']):\n  train_label.append(i)\nprint(train_label)","execution_count":null,"outputs":[]},{"metadata":{"id":"HhBNZubT8NEJ","outputId":"028bd0c0-9785-4d83-81ec-c75c0cb82e92","trusted":false},"cell_type":"code","source":"#data agumentation by using various techniques\nextend_train_imgdir=[]\nextend_train_label=[]\nfor img,label in zip(train_imgdir,train_label):\n  image=io.imread(img)\n  name=img.split('/')[-1].split('.')[0]\n  noice_img=random_noise(image,var=0.2**2)\n  blurred_img = gaussian(image,sigma=1,multichannel=True)\n  rotated_img = rotate(image, angle=45, mode = 'wrap')\n  flipped_img=np.fliplr(image)\n  #rotated_img2 = rotate(image, angle=315, mode = 'wrap')\n  #updown_img=np.flipud(image)\n  io.imsave('./dataset/train/'+name+'noice_img.jpg',noice_img)\n  io.imsave('./dataset/train/'+name+'blurred_img.jpg',blurred_img)\n  io.imsave('./dataset/train/'+name+'rotated_img.jpg',rotated_img)\n  io.imsave('./dataset/train/'+name+'flipped_img.jpg',flipped_img)\n  #io.imsave('./dataset/train/'+name+'inverserotated_img.jpg',rotated_img2)\n  extend_train_imgdir.append('./dataset/train/'+name+'noice_img.jpg')\n  extend_train_imgdir.append('./dataset/train/'+name+'blurred_img.jpg')\n  extend_train_imgdir.append('./dataset/train/'+name+'rotated_img.jpg')\n  extend_train_imgdir.append('./dataset/train/'+name+'flipped_img.jpg')\n  #extend_train_imgdir.append('./dataset/train/'+name+'inverserotated_img.jpg')\n  extend_train_label.append(label)\n  extend_train_label.append(label)\n  extend_train_label.append(label)\n  extend_train_label.append(label)\n  #extend_train_label.append(label)\nprint(extend_train_imgdir)\nprint(extend_train_label)","execution_count":null,"outputs":[]},{"metadata":{"id":"HOUSQtv6RbnS","outputId":"68a6a0c5-3897-4b30-dd99-480acc1b31c5","trusted":false},"cell_type":"code","source":"print(len(extend_train_imgdir))\nprint(len(extend_train_label))","execution_count":null,"outputs":[]},{"metadata":{"id":"f0aBizH0B1Nk","outputId":"b152d2e3-bc69-4d53-dd43-02a0f8aeb859","trusted":false},"cell_type":"code","source":"train_imgdir.extend(extend_train_imgdir)\ntrain_label.extend(extend_train_label)\nprint(train_imgdir)\nprint(train_label)\nprint(len(train_imgdir))\nprint(len(train_label))","execution_count":null,"outputs":[]},{"metadata":{"id":"wCZq8gqYum7L","trusted":false},"cell_type":"code","source":"x_train,x_val,y_train,y_val=train_test_split(train_imgdir,train_label,test_size=0.15,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"1Ach3qyfV-CO","outputId":"01978648-e0b5-4e82-fb3e-54d0147e30da","trusted":false},"cell_type":"code","source":"print(len(x_train))\nprint(len(x_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"Ge5YrqLxMK5u","trusted":false},"cell_type":"code","source":"def returnpaths(imgdir,labeldir):\n  paths=[]\n  for dir,label in zip(imgdir,labeldir):\n    paths.append((dir,label))\n  return paths","execution_count":null,"outputs":[]},{"metadata":{"id":"WjnoqNzuMpy_","trusted":false},"cell_type":"code","source":"train_paths=returnpaths(x_train,y_train)\nval_paths=returnpaths(x_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"079_3xLcM-Tc","outputId":"4aeadb88-bee7-40d1-e475-78192973f39b","trusted":false},"cell_type":"code","source":"print(train_paths)","execution_count":null,"outputs":[]},{"metadata":{"id":"45_wxo_3Pc09","trusted":false},"cell_type":"code","source":"class Load_testdata(Dataset):\n    def __init__(self,paths,transform=None):\n        self.paths = paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self,index):\n        img=self.paths[index]\n        name=img.split('/')[-1]\n        img_path = os.path.join(img)\n        image = io.imread(img_path)\n        #image=cv2.resize(image,(224,224))\n        #image=image/255\n        image=resize(image, output_shape=(400,400,3), mode='constant', anti_aliasing=True)\n        #y_label = torch.tensor(int(img[1]))\n\n        if self.transform:\n            transform_train = transforms.Compose([ \n                            transforms.ToTensor(),\n                            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n                            ])\n            image = transform_train(image)\n\n        return image,name","execution_count":null,"outputs":[]},{"metadata":{"id":"3VOAc-ZUP1U_","trusted":false},"cell_type":"code","source":"testdataset = Load_testdata(\n    paths=test_imgdir,\n    transform=transforms.ToTensor()\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"FHRA8I1yQLOA","trusted":false},"cell_type":"code","source":"test_loader = DataLoader(dataset=testdataset)#specify batchsize if you need and modify other functions accordingly","execution_count":null,"outputs":[]},{"metadata":{"id":"EFTZJi1-QRWs","outputId":"f6d9a37b-ba2b-43ff-b3d2-ccd551074aff","trusted":false},"cell_type":"code","source":"its=iter(test_loader)\nimg,name=next(its)\nprint(img.shape)\nprint(name[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"-2YdjZGeHmtf","trusted":false},"cell_type":"code","source":"class Load_data(Dataset):\n    def __init__(self,paths,transform=None):\n        self.paths = paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self,index):\n        img=self.paths[index]\n        img_path = os.path.join(img[0])\n        image = io.imread(img_path)\n        #image=cv2.resize(image,(224,224)) \n        #image=image/255\n        image=resize(image, output_shape=(400,400,3), mode='constant', anti_aliasing=True)       \n        y_label = torch.tensor(int(img[1]))\n        #print(y_label)\n        if self.transform :\n          transform_train = transforms.Compose([ \n                            transforms.ToTensor(),\n                            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n                            ])\n          image = transform_train(image)\n\n        return (image, y_label)","execution_count":null,"outputs":[]},{"metadata":{"id":"WtABTRbsJdQc","trusted":false},"cell_type":"code","source":"traindataset = Load_data(\n    paths=train_paths,\n    transform=transforms.ToTensor()\n)\nvaldataset = Load_data(\n    paths=val_paths,\n    transform=transforms.ToTensor()\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"pLfUt5z3Jy-R","trusted":false},"cell_type":"code","source":"train_loader = DataLoader(dataset=traindataset)\nval_loader=DataLoader(dataset=valdataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"zN6_KJBqJ3zP","trusted":false},"cell_type":"code","source":"it=iter(val_loader)","execution_count":null,"outputs":[]},{"metadata":{"id":"Osx4cOWTJ8KT","outputId":"789cff60-0541-4867-b854-d101bb670f56","trusted":false},"cell_type":"code","source":"image,label=next(it)\nprint(type(image))\nprint(type(label))\nprint(image)\nprint(label)\nprint(image.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"SPQGG5O2TClQ","trusted":false},"cell_type":"code","source":"def imshow(img, title):\n    npimg = img.numpy() / 2 + 0.5\n    plt.figure(figsize=(8,4))\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"uCUx91SXTFRC","trusted":false},"cell_type":"code","source":"def show_batch_images(dataloader):\n    images, labels = dataloader\n    img = torchvision.utils.make_grid(images)\n    # images=images.to(device)\n    # images=images.float()\n    # val=inception(images)\n    # print(val)\n    label=labels\n    imshow(img, title=(int(label),inverse_map[int(label)]))","execution_count":null,"outputs":[]},{"metadata":{"id":"l8zVlSYHTHsX","outputId":"ffc74402-a8d1-4dba-c67a-ed0e58403b1a","trusted":false},"cell_type":"code","source":"var=iter(train_loader)\nfor i in range(4):\n    show=next(var)\n    show_batch_images(show)#printing images with labels","execution_count":null,"outputs":[]},{"metadata":{"id":"CTJz6SlN4rpV","trusted":false},"cell_type":"code","source":"model=models.resnet34(pretrained=True)\n# for param in model.parameters():\n#     param.requires_grad = False\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features,8)","execution_count":null,"outputs":[]},{"metadata":{"id":"WFTV4tE3-wfA","outputId":"2ceb38a8-5add-45ad-e452-397bd2caa6cf","trusted":false},"cell_type":"code","source":"print(model)","execution_count":null,"outputs":[]},{"metadata":{"id":"HkvmssHra8fw","trusted":false},"cell_type":"code","source":"#evaluation function for calculating accuracy\ndef evaluation(dataloader, model):\n    total, correct = 0, 0\n    y_true=[]\n    y_pred=[]\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        inputs=inputs.float()\n        output = model(inputs)\n        pred = output.argmax(1)\n        #print(pred)\n        label=labels\n        y_pred.append(pred)\n        y_true.append(label)\n        total += labels.size(0)\n        correct += (pred == label).sum().item()\n    #print(f(y_true,y_pred,average='weighted'))\n    return 100 * correct / total","execution_count":null,"outputs":[]},{"metadata":{"id":"1H8cdd9jCczI","outputId":"28336c6c-1a5b-4783-deab-279d33563a6d","trusted":false},"cell_type":"code","source":"evaluation(val_loader,model)#checking wether function is working well ","execution_count":null,"outputs":[]},{"metadata":{"id":"Q_Y67XtsbsTG","trusted":false},"cell_type":"code","source":"model = model.to(device)#moving model to GPU\nloss_fn = nn.CrossEntropyLoss()\n#opt = optim.Adam(model.parameters(), lr=0.000155)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By finetuning below part (learining rate)you can achieve accuracy upto 90%","execution_count":null},{"metadata":{"id":"_BtI8cJ4XZK8","outputId":"ce50d0c4-fb49-44ed-8b4d-bffadd5215e9","trusted":false},"cell_type":"code","source":"loss_epoch_arr = []\nmax_epochs =2\nopt = optim.Adam(model.parameters(),lr=0.00000125)\nmin_loss = 100\n\nn_iters =len(x_train)\n\nfor epoch in range(max_epochs):\n    print('Epoch: %d/%d' % (epoch+1, max_epochs))\n    for i, data in enumerate(train_loader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        inputs=inputs.float()\n        opt.zero_grad()\n\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        # outputs, aux_outputs = model(inputs)\n        # loss = loss_fn(outputs, labels) + 0.3 * loss_fn(aux_outputs, labels)\n        # loss.backward()\n        # opt.step()\n        \n        if min_loss > loss.item():\n            min_loss = loss.item()\n            best_model = copy.deepcopy(model.state_dict())\n            print('Min loss %0.9f' % min_loss)\n        \n        if i % 100 == 0:\n            print('Iteration: %d/%d, Loss: %0.7f' % (i, n_iters, loss.item()))\n            \n        del inputs, labels, outputs\n        torch.cuda.empty_cache()\n        \n    loss_epoch_arr.append(loss.item())\n        \n    \n    \n    \nplt.plot(loss_epoch_arr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"fNfJq-YVX2Gk","outputId":"25f2179f-c495-4a4c-aa8e-9f9e3b9daba6","trusted":false},"cell_type":"code","source":"#model.load_state_dict(best_model)\n#joblib.dump(model, 'dance_25.pkl')\n#print(evaluation(train_loader,model), evaluation(val_loader,model))","execution_count":null,"outputs":[]},{"metadata":{"id":"fRmAGiJKM4lM","outputId":"4462b845-ee5a-40dd-d5ce-7a6f59bcb029","trusted":false},"cell_type":"code","source":"# model2=models.resnet34(pretrained=True)\n# # for param in model.parameters():\n# #     param.requires_grad = False\n# in_features = model2.fc.in_features\n# model2.fc = nn.Linear(in_features,8)\nmodel2.load_state_dict(best_model)#loading best model if you need to,but this best model will not assure to give heighest accuracy","execution_count":null,"outputs":[]},{"metadata":{"id":"B_oBhSysNtTR","trusted":false},"cell_type":"code","source":"#model2=model2.to(device)#uncomment this if you want to use model2","execution_count":null,"outputs":[]},{"metadata":{"id":"F4fd1p30MZcb","trusted":false},"cell_type":"code","source":"#model=joblib.load('dance_25.pkl')","execution_count":null,"outputs":[]},{"metadata":{"id":"AcHEYHWkJHUr","outputId":"c53ecc67-0e6b-411c-ffc7-ab1e4debf117","trusted":false},"cell_type":"code","source":"#predicting test values\nnames=[]\npreds=[]\nfor data in test_loader:\n  img,name=data\n  img=img.to(device)\n  img=img.float()\n  out=model(img)\n  pred=int(out.argmax(1))\n  preds.append(str(inverse_map[pred]))\n  names.append(name[0])\nprint(names)\nprint(preds)\nprint(len(names))\nprint(len(preds))","execution_count":null,"outputs":[]},{"metadata":{"id":"5s_6r9vRKTfm","trusted":false},"cell_type":"code","source":"final_result5 = np.array(list(map(list, zip(names,preds))))","execution_count":null,"outputs":[]},{"metadata":{"id":"5ezqeGhIK0T_","trusted":false},"cell_type":"code","source":"df_final= pd.DataFrame(data=final_result5, columns=[\"Image\", \"target\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"-x_1ukT0LCuA","trusted":false},"cell_type":"code","source":"df_final.to_csv('submission_dance_31.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"WGmJZh6hLFf5","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"hYgCsdBcgLhA","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}