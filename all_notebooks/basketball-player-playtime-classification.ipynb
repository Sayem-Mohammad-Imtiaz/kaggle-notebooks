{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# The imports...\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Data handling and analysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Models\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import and Look at Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/performance-prediction/summary.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code in this section from https://www.kaggle.com/sachinsharma1123/kernel439a1a3a5b\ndf['3PointPercent']=df['3PointPercent'].fillna(df['3PointPercent'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We do not need names, so we will drop the column.    \ndf=df.drop(['Name'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Developing the Model and Finding the Best Solution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into targets and features \ny = df['Target']\nX = df.drop(['Target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use ANOVA to select best features\n# Since the data is small enough, we will check many models and features to be comprehensive.\n\nbestAcc = 0\nnumFeatures = 0\nKNN_size = 0\nlogistic = False\nSVM = False\ntree = False\nKNN = False\nnaive = False\n\n# Check different models and their accuracies \n# Loop 1 through 10, for 1-10 amount of ANOVA features\nfor i in range(1,13):\n        # Selection best i features        \n        fvalue_selector = SelectKBest(f_classif, k=i)\n        newX = fvalue_selector.fit_transform(X, y)\n        \n        X_train, X_test, y_train, y_test = train_test_split(newX, y, random_state=0, test_size=0.3)\n    \n        \n        # Check Logistic Regression model\n        logistic = LogisticRegression(max_iter = 10000)\n        logistic.fit(X_train, y_train)\n        prediction = logistic.predict(X_test)\n        score = accuracy_score(y_test, prediction)\n        if score > bestAcc:\n            bestAcc = score\n            logistic = True\n            SVM = False\n            tree = False\n            KNN = False\n            naive = False\n            numFeatures = i\n        \n        # Check KNN model\n        for j in range(1,10):\n            knn = KNeighborsClassifier(n_neighbors = j)\n            knn.fit(X_train, y_train)\n            prediction = knn.predict(X_test)\n            score = accuracy_score(y_test, prediction)\n            if score > bestAcc:\n                bestAcc = score\n                numFeatures = i\n                KNN_size = j\n                logistic = False\n                SVM = False\n                tree = False\n                KNN = True\n                naive = False\n                \n\n        # Check Naive Bayes\n        nb = GaussianNB()\n        prediction = nb.fit(X_train, y_train).predict(X_test)\n        score = accuracy_score(y_test, prediction)\n        if score > bestAcc:\n            bestAcc = score\n            logistic = False\n            SVM = False\n            tree = False\n            KNN = False\n            naive = True\n            numFeatures = i\n        \n        # Check SVM\n        sv = svm.SVC()\n        sv = sv.fit(X_train, y_train)\n        prediction = sv.predict(X_test)\n        score = accuracy_score(y_test, prediction)\n        if score > bestAcc:\n            bestAcc = score\n            logistic = False\n            SVM = True\n            tree = False\n            KNN = False\n            naive = False\n            numFeatures = i \n                \n                \nprint(\"The best accuracy was\", round(bestAcc, 5), \", using this many features:\", numFeatures)\n\n\nif logistic:\n    print(\"Logistic was the best model.\")\nelif SVM:\n    print(\"SVM was the best model.\")\nelif naive:\n    print(\"Naive Bayes was the best model.\")\nelse:\n    print(\"KNN was the best model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See which features were important\n# Code modified from https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n\nselector = SelectKBest(f_classif, k=numFeatures)\nselector.fit(X, y)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\ntopFeatures = X.iloc[:,cols]\n\nprint(\"The top 8 most important features were as follows\")\nlist(topFeatures.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nFirst off, a huge **THANK YOU** for taking the time to read my notebook.\n\nAfter checking various models with various features selected, I found that the ideal amount of features was 8 to predict if a player has been player for more than or less than 5 years. My model was able to predict with 0.7288 accuracy. \n\nThe most important features were\n* Games Played\n* Minutes Played\n* Field Goals Made\n* Free Throws Made\n* Free Throw Attemps\n* Offensive Rebounds\n* Rebounds\n\nThe first 2 make the most sense, as they would logically be the strongest correlation. I am not a basketball pro, so I do not know what field goals are, but free throws being correlated makes sense as many players would throw them, regardless of position, similar to the rebounds. I suspect if we had a feature of player positions, we could develop a model with higher accuracy as the stats likely vary heavily by position of the player. Ignoring positions groups the stats into one, making it more general.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}