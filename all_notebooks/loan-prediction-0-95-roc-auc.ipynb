{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loan Prediction\n# XGBoost / Hyperparameter tunning","metadata":{}},{"cell_type":"markdown","source":"` This notebook is work in progress, I will add comments later`","metadata":{}},{"cell_type":"markdown","source":"## Plan\n1. Data Analysis \n2. Data Pre processing\n3. Models testing\n4. Hyperparameter tunning","metadata":{}},{"cell_type":"markdown","source":"Let us first start by importing the necessary packages:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T22:16:57.220202Z","iopub.execute_input":"2021-08-30T22:16:57.220927Z","iopub.status.idle":"2021-08-30T22:16:58.202163Z","shell.execute_reply.started":"2021-08-30T22:16:57.220832Z","shell.execute_reply":"2021-08-30T22:16:58.200928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I would be using ggplot theme for my plots","metadata":{}},{"cell_type":"code","source":"plt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:34.74509Z","iopub.execute_input":"2021-08-26T12:57:34.745507Z","iopub.status.idle":"2021-08-26T12:57:34.749704Z","shell.execute_reply.started":"2021-08-26T12:57:34.745477Z","shell.execute_reply":"2021-08-26T12:57:34.74867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the data","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/loan-prediction-based-on-customer-behavior/Training Data.csv'\npd.set_option('display.max_columns', None)\ndata = pd.read_csv(path)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:34.751323Z","iopub.execute_input":"2021-08-26T12:57:34.75182Z","iopub.status.idle":"2021-08-26T12:57:35.414774Z","shell.execute_reply.started":"2021-08-26T12:57:34.751788Z","shell.execute_reply":"2021-08-26T12:57:35.414145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data analysis","metadata":{}},{"cell_type":"markdown","source":"Before diving into model testing, we have to get familiar with the data and understand what it represents.","metadata":{}},{"cell_type":"code","source":"data.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:35.41598Z","iopub.execute_input":"2021-08-26T12:57:35.416263Z","iopub.status.idle":"2021-08-26T12:57:35.722221Z","shell.execute_reply.started":"2021-08-26T12:57:35.416236Z","shell.execute_reply":"2021-08-26T12:57:35.721327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:35.723425Z","iopub.execute_input":"2021-08-26T12:57:35.723694Z","iopub.status.idle":"2021-08-26T12:57:35.800273Z","shell.execute_reply.started":"2021-08-26T12:57:35.723666Z","shell.execute_reply":"2021-08-26T12:57:35.799299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have multiple categorical data, since we will test various models and not only tree based models, we should econde these variables. <br>\nHowever we can see we do not have any missing values in our data.","metadata":{}},{"cell_type":"markdown","source":"## Age","metadata":{}},{"cell_type":"code","source":"data.groupby('Risk_Flag').describe()['Age']","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:35.801474Z","iopub.execute_input":"2021-08-26T12:57:35.801754Z","iopub.status.idle":"2021-08-26T12:57:35.959295Z","shell.execute_reply.started":"2021-08-26T12:57:35.801724Z","shell.execute_reply":"2021-08-26T12:57:35.958516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the minimum age, maximum age, and even average age does not affect the risk flag variable. <br>\nAlso we can see that both (0 and 1) have the same difference between the age groups and the mean: standard deviation of 17.","metadata":{}},{"cell_type":"code","source":"data[data['Risk_Flag'] == 1]['Age'].value_counts(sort=True)[:20]","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:35.960205Z","iopub.execute_input":"2021-08-26T12:57:35.960574Z","iopub.status.idle":"2021-08-26T12:57:35.975945Z","shell.execute_reply.started":"2021-08-26T12:57:35.960546Z","shell.execute_reply":"2021-08-26T12:57:35.974877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not a big difference between the top 5 of the Risk Flag list, however starting from the sixth position the difference grow, but the distribution of age groups is normal, since we don't have a certain age group repeating. As we can see the first place is for people aged 22, second place for people aged 66.","metadata":{}},{"cell_type":"markdown","source":"## Marital status","metadata":{}},{"cell_type":"code","source":"data.groupby('Risk_Flag')['Married/Single'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:35.978445Z","iopub.execute_input":"2021-08-26T12:57:35.978833Z","iopub.status.idle":"2021-08-26T12:57:36.018918Z","shell.execute_reply.started":"2021-08-26T12:57:35.97879Z","shell.execute_reply":"2021-08-26T12:57:36.018021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that single people are more risky, 91% of risky flag people are single. But actually this can't tell much since most of people that apply are single.","metadata":{}},{"cell_type":"code","source":"data['Married/Single'].hist()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:36.020343Z","iopub.execute_input":"2021-08-26T12:57:36.020605Z","iopub.status.idle":"2021-08-26T12:57:36.253749Z","shell.execute_reply.started":"2021-08-26T12:57:36.020578Z","shell.execute_reply":"2021-08-26T12:57:36.252682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Income","metadata":{}},{"cell_type":"code","source":"sns.distplot(data['Income'], bins=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:36.25528Z","iopub.execute_input":"2021-08-26T12:57:36.255673Z","iopub.status.idle":"2021-08-26T12:57:37.900848Z","shell.execute_reply.started":"2021-08-26T12:57:36.25563Z","shell.execute_reply":"2021-08-26T12:57:37.900101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Status'] = np.where(data['Income']>=data['Income'].mean(), 'Above Average', 'Under Average')\ndata['Status'].hist()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:37.901787Z","iopub.execute_input":"2021-08-26T12:57:37.902194Z","iopub.status.idle":"2021-08-26T12:57:38.17438Z","shell.execute_reply.started":"2021-08-26T12:57:37.902122Z","shell.execute_reply":"2021-08-26T12:57:38.173646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is distributed equally between people with under average and above average income.","metadata":{}},{"cell_type":"code","source":"data.groupby('Risk_Flag')['Status'].hist()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:38.175411Z","iopub.execute_input":"2021-08-26T12:57:38.175838Z","iopub.status.idle":"2021-08-26T12:57:38.426895Z","shell.execute_reply.started":"2021-08-26T12:57:38.175786Z","shell.execute_reply":"2021-08-26T12:57:38.425924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The income does not affect the risk flag variable of a person.","metadata":{}},{"cell_type":"code","source":"data.groupby(\"Risk_Flag\")['Income'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:38.428056Z","iopub.execute_input":"2021-08-26T12:57:38.428354Z","iopub.status.idle":"2021-08-26T12:57:38.467794Z","shell.execute_reply.started":"2021-08-26T12:57:38.428316Z","shell.execute_reply":"2021-08-26T12:57:38.466903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average salary of Risk Flag group 1 is 0.3 e+06 smaller than the non risky group. Meanwhile other metrics for both groups are similar.","metadata":{}},{"cell_type":"markdown","source":"## State","metadata":{}},{"cell_type":"code","source":"data.groupby(\"Risk_Flag\")['STATE'].value_counts(sort=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:38.468921Z","iopub.execute_input":"2021-08-26T12:57:38.469201Z","iopub.status.idle":"2021-08-26T12:57:38.52463Z","shell.execute_reply.started":"2021-08-26T12:57:38.469172Z","shell.execute_reply":"2021-08-26T12:57:38.523695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uttar Pradesh ranks first in both groups which is normal since Uttar has the most number of applicants, ","metadata":{}},{"cell_type":"code","source":"sns.countplot(y='STATE', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:57:38.526172Z","iopub.execute_input":"2021-08-26T12:57:38.52656Z","iopub.status.idle":"2021-08-26T12:57:38.947305Z","shell.execute_reply.started":"2021-08-26T12:57:38.526517Z","shell.execute_reply":"2021-08-26T12:57:38.946358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"West Bengal ranks second with the most risky flags, even though it is the fourth city in the number of applicants.","metadata":{}},{"cell_type":"code","source":"data[data['STATE'] == 'West_Bengal'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T12:08:20.798576Z","iopub.execute_input":"2021-08-26T12:08:20.799177Z","iopub.status.idle":"2021-08-26T12:08:20.940265Z","shell.execute_reply.started":"2021-08-26T12:08:20.79913Z","shell.execute_reply":"2021-08-26T12:08:20.939184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:05:00.404055Z","iopub.execute_input":"2021-08-26T13:05:00.404409Z","iopub.status.idle":"2021-08-26T13:05:00.539125Z","shell.execute_reply.started":"2021-08-26T13:05:00.404377Z","shell.execute_reply":"2021-08-26T13:05:00.538424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Income and age of applicants from West Bangali does not differ much from other states.","metadata":{}},{"cell_type":"markdown","source":"## Current Job","metadata":{}},{"cell_type":"code","source":"data.groupby(\"Risk_Flag\")['CURRENT_JOB_YRS'].value_counts(sort=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:01:37.507994Z","iopub.execute_input":"2021-08-26T13:01:37.508582Z","iopub.status.idle":"2021-08-26T13:01:37.544105Z","shell.execute_reply.started":"2021-08-26T13:01:37.508547Z","shell.execute_reply":"2021-08-26T13:01:37.543407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='CURRENT_JOB_YRS', hue='Risk_Flag', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:03:19.789931Z","iopub.execute_input":"2021-08-26T13:03:19.790315Z","iopub.status.idle":"2021-08-26T13:03:20.08098Z","shell.execute_reply.started":"2021-08-26T13:03:19.790285Z","shell.execute_reply":"2021-08-26T13:03:20.079959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot doesn't tell much.","metadata":{}},{"cell_type":"markdown","source":"## Profession","metadata":{}},{"cell_type":"code","source":"data['Profession'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:06:28.813201Z","iopub.execute_input":"2021-08-26T13:06:28.813541Z","iopub.status.idle":"2021-08-26T13:06:28.846193Z","shell.execute_reply.started":"2021-08-26T13:06:28.813511Z","shell.execute_reply":"2021-08-26T13:06:28.845193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most applicants have well paid jobs.","metadata":{}},{"cell_type":"code","source":"data.groupby('Profession')['Income'].mean().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:14:04.075113Z","iopub.execute_input":"2021-08-26T13:14:04.07546Z","iopub.status.idle":"2021-08-26T13:14:04.109931Z","shell.execute_reply.started":"2021-08-26T13:14:04.075431Z","shell.execute_reply":"2021-08-26T13:14:04.109049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Encoding:","metadata":{}},{"cell_type":"markdown","source":"The data does not need much cleaning, we only need to encode the categorical data so it could be used by our algorithms. For this step I will use LabelEncode of the sklearn pre processing library.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"Married/Single\"] = label_encoder.fit_transform(data[\"Married/Single\"])\ndata[\"House_Ownership\"] = label_encoder.fit_transform(data[\"House_Ownership\"])\ndata[\"Car_Ownership\"] = label_encoder.fit_transform(data[\"Car_Ownership\"])\ndata[\"Profession\"] = label_encoder.fit_transform(data[\"Profession\"])\ndata[\"CITY\"] = label_encoder.fit_transform(data[\"CITY\"])\ndata[\"STATE\"] = label_encoder.fit_transform(data[\"STATE\"])\ndata[\"Status\"] = label_encoder.fit_transform(data[\"Status\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:15:32.659107Z","iopub.execute_input":"2021-08-26T13:15:32.659518Z","iopub.status.idle":"2021-08-26T13:15:33.14933Z","shell.execute_reply.started":"2021-08-26T13:15:32.659485Z","shell.execute_reply":"2021-08-26T13:15:33.148411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['STATE'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:15:47.04959Z","iopub.execute_input":"2021-08-26T13:15:47.049926Z","iopub.status.idle":"2021-08-26T13:15:47.05934Z","shell.execute_reply.started":"2021-08-26T13:15:47.049895Z","shell.execute_reply":"2021-08-26T13:15:47.058403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tada!","metadata":{}},{"cell_type":"markdown","source":"# Testing Classifiers","metadata":{}},{"cell_type":"markdown","source":"Moving to the next step, we will start by importing the classifiers that we will test:\n* Logistic Regression\n* XGBoost\n* Random Forest \n* Gradient Boosting\n\nWe would test other classifiers but it takes so much time to run a cross validation test on many classifiers.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:18:05.861647Z","iopub.execute_input":"2021-08-26T13:18:05.862007Z","iopub.status.idle":"2021-08-26T13:18:06.222652Z","shell.execute_reply.started":"2021-08-26T13:18:05.861976Z","shell.execute_reply":"2021-08-26T13:18:06.221405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And of course evaluation metrics are important, alongside the splitting function.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:18:44.383634Z","iopub.execute_input":"2021-08-26T13:18:44.384026Z","iopub.status.idle":"2021-08-26T13:18:44.38774Z","shell.execute_reply.started":"2021-08-26T13:18:44.383995Z","shell.execute_reply":"2021-08-26T13:18:44.38699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we have to drop the target column from the data.","metadata":{}},{"cell_type":"code","source":"pred = data['Risk_Flag']\ndata.drop(columns=['Risk_Flag'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T13:20:34.93063Z","iopub.execute_input":"2021-08-26T13:20:34.930968Z","iopub.status.idle":"2021-08-26T13:20:34.970754Z","shell.execute_reply.started":"2021-08-26T13:20:34.93094Z","shell.execute_reply":"2021-08-26T13:20:34.969606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To avoid wasting too much time running one cell of code, first we will test logistic regression and gradient boosting, and then move to XGBoost and Random Forest.","metadata":{}},{"cell_type":"code","source":"KFold_Score = pd.DataFrame()\nclassifiers = ['LogisticRegression', 'GradientBoostingClassifier']\nmodels = [\n          LogisticRegression(max_iter = 1000),\n          GradientBoostingClassifier(random_state=0)\n         ]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T13:47:07.857053Z","iopub.execute_input":"2021-08-25T13:47:07.857689Z","iopub.status.idle":"2021-08-25T13:47:07.863464Z","shell.execute_reply.started":"2021-08-25T13:47:07.857639Z","shell.execute_reply":"2021-08-25T13:47:07.862401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, data, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1","metadata":{"execution":{"iopub.status.busy":"2021-08-25T13:47:10.463666Z","iopub.execute_input":"2021-08-25T13:47:10.465163Z","iopub.status.idle":"2021-08-25T13:50:43.693506Z","shell.execute_reply.started":"2021-08-25T13:47:10.465101Z","shell.execute_reply":"2021-08-25T13:50:43.692198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we can clearly see we have got nearly the same mean cross validation score. Moving on to the next test.","metadata":{}},{"cell_type":"code","source":"mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T13:50:43.695518Z","iopub.execute_input":"2021-08-25T13:50:43.695832Z","iopub.status.idle":"2021-08-25T13:50:43.716881Z","shell.execute_reply.started":"2021-08-25T13:50:43.695786Z","shell.execute_reply":"2021-08-25T13:50:43.715433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KFold_Score2 = pd.DataFrame()\nclassifiers = ['RandomForestClassifier', 'XGBoostClassifier']\nmodels = [\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          xgb.XGBClassifier(n_estimators=100),\n         ]\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score2[classifiers[j]] = (cross_val_score(model, data, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1","metadata":{"execution":{"iopub.status.busy":"2021-08-25T13:38:00.566561Z","iopub.execute_input":"2021-08-25T13:38:00.567009Z","iopub.status.idle":"2021-08-25T13:45:36.486702Z","shell.execute_reply.started":"2021-08-25T13:38:00.56697Z","shell.execute_reply":"2021-08-25T13:45:36.485357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = pd.DataFrame(KFold_Score2.mean(), index= classifiers)\nKFold_Score2 = pd.concat([KFold_Score2,mean.T])\nKFold_Score2.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score2.T.sort_values(by=['Mean'], ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T13:46:58.39746Z","iopub.execute_input":"2021-08-25T13:46:58.39807Z","iopub.status.idle":"2021-08-25T13:46:58.420299Z","shell.execute_reply.started":"2021-08-25T13:46:58.398034Z","shell.execute_reply":"2021-08-25T13:46:58.419094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After testig four classifiers, we can see that XGBoostClassifier returns the best mean cross validation score.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter tunning:","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data, pred, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:50:44.640037Z","iopub.execute_input":"2021-08-25T16:50:44.640693Z","iopub.status.idle":"2021-08-25T16:50:44.70424Z","shell.execute_reply.started":"2021-08-25T16:50:44.640641Z","shell.execute_reply":"2021-08-25T16:50:44.70344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of numpy arrays or pandas dataFrame, XGBoost uses DMatrices.","metadata":{}},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:50:44.705498Z","iopub.execute_input":"2021-08-25T16:50:44.705971Z","iopub.status.idle":"2021-08-25T16:50:44.766095Z","shell.execute_reply.started":"2021-08-25T16:50:44.705926Z","shell.execute_reply":"2021-08-25T16:50:44.76509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will start by defining the initial parameters of the model:","metadata":{}},{"cell_type":"code","source":"params = {\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:linear',\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:50:44.768197Z","iopub.execute_input":"2021-08-25T16:50:44.769026Z","iopub.status.idle":"2021-08-25T16:50:44.77573Z","shell.execute_reply.started":"2021-08-25T16:50:44.768974Z","shell.execute_reply":"2021-08-25T16:50:44.77464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since this is a classification problem, we will set the evaluation metric to log loss","metadata":{}},{"cell_type":"code","source":"params['eval_metric'] = 'logloss'","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:50:44.778312Z","iopub.execute_input":"2021-08-25T16:50:44.779203Z","iopub.status.idle":"2021-08-25T16:50:44.790518Z","shell.execute_reply.started":"2021-08-25T16:50:44.779118Z","shell.execute_reply":"2021-08-25T16:50:44.789262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the model with the parameters selected, and specify early stopping at ten rounds","metadata":{}},{"cell_type":"code","source":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=999,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:50:44.792843Z","iopub.execute_input":"2021-08-25T16:50:44.793904Z","iopub.status.idle":"2021-08-25T16:51:03.945346Z","shell.execute_reply.started":"2021-08-25T16:50:44.793816Z","shell.execute_reply":"2021-08-25T16:51:03.944282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us check the initial log loss","metadata":{}},{"cell_type":"code","source":"print(\"Best Log Loss: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:51:03.94928Z","iopub.execute_input":"2021-08-25T16:51:03.949747Z","iopub.status.idle":"2021-08-25T16:51:03.956255Z","shell.execute_reply.started":"2021-08-25T16:51:03.949707Z","shell.execute_reply":"2021-08-25T16:51:03.954892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=999,\n    seed=42,\n    nfold=5,\n    metrics={'logloss'},\n    early_stopping_rounds=10\n)\ncv_results","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:51:03.957736Z","iopub.execute_input":"2021-08-25T16:51:03.9581Z","iopub.status.idle":"2021-08-25T16:52:43.839082Z","shell.execute_reply.started":"2021-08-25T16:51:03.958068Z","shell.execute_reply":"2021-08-25T16:52:43.837697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_results['test-logloss-mean'].min()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:52:43.841551Z","iopub.execute_input":"2021-08-25T16:52:43.841895Z","iopub.status.idle":"2021-08-25T16:52:43.850791Z","shell.execute_reply.started":"2021-08-25T16:52:43.841847Z","shell.execute_reply":"2021-08-25T16:52:43.84925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will start by searching two parameters: max_depth and min_child_weight","metadata":{}},{"cell_type":"code","source":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:00:36.316623Z","iopub.execute_input":"2021-08-25T17:00:36.317279Z","iopub.status.idle":"2021-08-25T17:00:36.323118Z","shell.execute_reply.started":"2021-08-25T17:00:36.317229Z","shell.execute_reply":"2021-08-25T17:00:36.321964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_log = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=999,\n        seed=42,\n        nfold=5,\n        metrics={'logloss'},\n        early_stopping_rounds=10\n    \n    mean_log = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\logloss {} for {} rounds\".format(mean_log, boost_rounds))\n    if mean_log < min_log:\n        min_log = mean_log\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, logloss: {}\".format(best_params[0], best_params[1], min_log))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:05:24.507562Z","iopub.execute_input":"2021-08-25T17:05:24.50838Z","iopub.status.idle":"2021-08-25T17:13:53.379913Z","shell.execute_reply.started":"2021-08-25T17:05:24.508335Z","shell.execute_reply":"2021-08-25T17:13:53.37901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save these two parameters to the params dictionary","metadata":{}},{"cell_type":"code","source":"params['max_depth'] = 9\nparams['min_child_weight'] = 7","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:18:16.175237Z","iopub.execute_input":"2021-08-25T17:18:16.175717Z","iopub.status.idle":"2021-08-25T17:18:16.181915Z","shell.execute_reply.started":"2021-08-25T17:18:16.175681Z","shell.execute_reply":"2021-08-25T17:18:16.180432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Moving to the subsample and colsample paramters","metadata":{}},{"cell_type":"code","source":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i/10. for i in range(7,11)]\n    for colsample in [i/10. for i in range(7,11)]\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:18:18.108135Z","iopub.execute_input":"2021-08-25T17:18:18.108683Z","iopub.status.idle":"2021-08-25T17:18:18.113544Z","shell.execute_reply.started":"2021-08-25T17:18:18.108648Z","shell.execute_reply":"2021-08-25T17:18:18.1128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_log = float(\"Inf\")\nbest_params = None\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=500,\n        seed=42,\n        nfold=5,\n        metrics={'logloss'},\n        early_stopping_rounds=10\n    )\n    mean_log = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\log {} for {} rounds\".format(mean_log, boost_rounds))\n    if mean_log < min_log:\n        min_log = mean_log\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, log: {}\".format(best_params[0], best_params[1], min_log))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:18:36.04723Z","iopub.execute_input":"2021-08-25T17:18:36.047688Z","iopub.status.idle":"2021-08-25T17:33:30.380163Z","shell.execute_reply.started":"2021-08-25T17:18:36.04765Z","shell.execute_reply":"2021-08-25T17:33:30.379188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The optimal results are 1 and 0.7","metadata":{}},{"cell_type":"code","source":"params['subsample'] = 1.0\nparams['colsample_bytree'] = .7","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:34:12.641701Z","iopub.execute_input":"2021-08-25T17:34:12.64214Z","iopub.status.idle":"2021-08-25T17:34:12.646926Z","shell.execute_reply.started":"2021-08-25T17:34:12.642105Z","shell.execute_reply":"2021-08-25T17:34:12.645801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tuning the learning rate might take some time, if you clone the code prepare to wait up to 30mins for this cell to be executed","metadata":{}},{"cell_type":"code","source":"min_log = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    params['eta'] = eta\n    cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=500,\n    seed=42,\n    nfold=5,\n    metrics=['logloss'],\n    early_stopping_rounds=10)\n    mean_log = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\Log Loss {} for {} rounds\\n\".format(mean_log, boost_rounds))\n    if mean_log < min_log:\n        min_log = mean_log\n        best_params = eta\nprint(\"Best params: {}, Log Loss: {}\".format(best_params, min_log))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:34:46.56594Z","iopub.execute_input":"2021-08-25T17:34:46.566376Z","iopub.status.idle":"2021-08-25T18:02:40.831972Z","shell.execute_reply.started":"2021-08-25T17:34:46.566343Z","shell.execute_reply":"2021-08-25T18:02:40.83074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best learning rate is 0.1","metadata":{}},{"cell_type":"code","source":"params['eta'] = .1","metadata":{"execution":{"iopub.status.busy":"2021-08-25T18:04:51.01364Z","iopub.execute_input":"2021-08-25T18:04:51.014071Z","iopub.status.idle":"2021-08-25T18:04:51.020556Z","shell.execute_reply.started":"2021-08-25T18:04:51.01404Z","shell.execute_reply":"2021-08-25T18:04:51.019095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us have a look at the parameters we have got so far","metadata":{}},{"cell_type":"code","source":"params","metadata":{"execution":{"iopub.status.busy":"2021-08-25T18:05:01.294078Z","iopub.execute_input":"2021-08-25T18:05:01.294451Z","iopub.status.idle":"2021-08-25T18:05:01.301732Z","shell.execute_reply.started":"2021-08-25T18:05:01.294422Z","shell.execute_reply":"2021-08-25T18:05:01.300461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the model","metadata":{}},{"cell_type":"code","source":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=900,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)\n\nprint(\"Best Log: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T18:05:33.929266Z","iopub.execute_input":"2021-08-25T18:05:33.929634Z","iopub.status.idle":"2021-08-25T18:06:27.151396Z","shell.execute_reply.started":"2021-08-25T18:05:33.929604Z","shell.execute_reply":"2021-08-25T18:06:27.149925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_boost_round = model.best_iteration + 1\nbest_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T18:07:29.722548Z","iopub.execute_input":"2021-08-25T18:07:29.723287Z","iopub.status.idle":"2021-08-25T18:08:19.543433Z","shell.execute_reply.started":"2021-08-25T18:07:29.723248Z","shell.execute_reply":"2021-08-25T18:08:19.542295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = best_model.predict(dtest)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T18:14:14.572323Z","iopub.execute_input":"2021-08-25T18:14:14.572809Z","iopub.status.idle":"2021-08-25T18:14:14.788146Z","shell.execute_reply.started":"2021-08-25T18:14:14.57277Z","shell.execute_reply":"2021-08-25T18:14:14.78712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC_AUC score on the test set","metadata":{"execution":{"iopub.status.busy":"2021-08-25T18:18:54.32071Z","iopub.execute_input":"2021-08-25T18:18:54.321224Z","iopub.status.idle":"2021-08-25T18:18:54.325475Z","shell.execute_reply.started":"2021-08-25T18:18:54.321183Z","shell.execute_reply":"2021-08-25T18:18:54.324349Z"}}},{"cell_type":"markdown","source":"As required, we will calculate the roc auc score of the model on the test set","metadata":{}},{"cell_type":"code","source":"metrics.roc_auc_score(y_test, predictions)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T18:14:19.678188Z","iopub.execute_input":"2021-08-25T18:14:19.67884Z","iopub.status.idle":"2021-08-25T18:14:19.716326Z","shell.execute_reply.started":"2021-08-25T18:14:19.678789Z","shell.execute_reply":"2021-08-25T18:14:19.715125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have got quite interesting result, 0.94. Of course it could be improved but we will it here for the moment.\nSave the model:","metadata":{}},{"cell_type":"code","source":"best_model.save_model(\"my_model.model\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# REFERENCES","metadata":{}},{"cell_type":"markdown","source":"For more details of the hyperparameter tuning techniques I have used in this notebook, reger to the following blog, it explains hyperparameter tuning in xgboost in detail:","metadata":{}},{"cell_type":"markdown","source":"XGBoost Hyperparameter tunning: https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f","metadata":{}}]}