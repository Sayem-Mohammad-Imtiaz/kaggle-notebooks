{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # **Introduction to the Dataset**"},{"metadata":{},"cell_type":"markdown","source":"The UrbanSound8k dataset is a collection of 8732 sound files classified in 10 different classes:\n\n> 1. air_conditioner;\n> 2. car_horn; \n> 3. children_playing;\n> 4. dog_bark;\n> 5. drilling;\n> 6. enginge_idling;\n> 7. gun_shot;\n> 8. jackhammer;\n> 9. siren;\n> 10. street_music. \n\nThese sounds are representative of real urban sounds. The dataset also contains a .csv file with metadata about each audiofile."},{"metadata":{},"cell_type":"markdown","source":"# Objective"},{"metadata":{},"cell_type":"markdown","source":"The goal with this notebook is to use the UrbanSound8k dataset to create a classification model that can correctly identify the sound of real life audio files. Then, this classification model should be able to classify diffent segments in a single audio file."},{"metadata":{},"cell_type":"markdown","source":"# Methodology"},{"metadata":{},"cell_type":"markdown","source":"The methodology used in this notebook is the following:\n\n1. Understand the data\n2. Extract features using librosa library\n3. Separate our dataset in train, test and validation\n4. Create and run a Keras Model\n5. Collect accuracy results data for multiple combination of features\n6. Analyse the results and select the best combination of features\n7. Run the model with real audio data\n\nIn this model I'll not be using image analysis for spectrogram images. All the features will be using data extracted directly form the audio."},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sound(filename):\n    plt.figure(figsize=(30,4))\n    data,sample_rate = librosa.load(filename)\n    librosa.display.waveplot(data,sr=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# siren sound\nfilename = '../input/urbansound8k/fold4/24347-8-0-12.wav' \nplot_sound(filename)\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# drilling sound\nfilename = '../input/urbansound8k/fold7/104625-4-0-52.wav'\nplot_sound(filename)\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading metadata file\nmetadata = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\nmetadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class distribution\ndist = metadata['class'].value_counts()\nprint(dist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the data isn't evenly distibuted between classes.\nSince there's significantly less data for gun_shots and car_horn, there is a chance that we have a different accuracy for this classes.\nThis may not be a problem if this sample is distinct enough from the other ones."},{"metadata":{},"cell_type":"markdown","source":"# Extracting Features"},{"metadata":{},"cell_type":"markdown","source":"There is a lot of ways to extract features from a sound file. The library librosa has a lot of options for this kind of feature extracting.\n\n\nFirst, we will test extracting mfccs (Mel Frequency Cepstral Co-efficients) from the soundfiles. Then we will be testing extracting Spectral Contrast information from the soundfiles. Lastly we will be testing extracting Chroma Stft information.\n\nAll the functions used for these testing are listed below"},{"metadata":{"trusted":true},"cell_type":"code","source":"### MFCC based feature extraction:\n\ndef extract_features_mfcc_mean(file_name):\n    '''Extract mfcc features and calculate the mean for each segment. Returns a 60 size 1D array'''\n    audio, sample_rate = librosa.load(file_name,res_type='kaiser_fast') \n    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc = 60)\n    mfccs_mean = np.mean(mfccs.T,axis=0)\n    return mfccs_mean\n\ndef extract_features_mfcc_mean_var_std(file_name):\n    '''Extract mfcc features and calculate the mean, var, max, min and std for each segment. Returns a 300 size 1D array'''\n    audio, sample_rate = librosa.load(file_name,res_type='kaiser_fast') \n    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc = 60)\n    mfccs_mean = np.mean(mfccs.T,axis = 0)\n    mfccs_variance = np.var(mfccs.T,axis = 0)\n    mfccs_max = np.amax(mfccs.T,axis = 0)\n    mfccs_min = np.amin(mfccs.T,axis = 0)\n    mfccs_std = np.std(mfccs.T, axis = 0)\n    mfccs_features = np.hstack([mfccs_mean, mfccs_variance, mfccs_max, mfccs_min, mfccs_std])\n    return mfccs_features\n\ndef extract_features_mfcc_mean_std(file_name):\n    '''Extract mfcc features and calculate the mean and std for each segment. Returns a 120 size 1D array'''\n    audio, sample_rate = librosa.load(file_name,res_type='kaiser_fast') \n    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=60)\n    mfccs_mean = np.mean(mfccs.T,axis=0)\n    mfccs_std = np.std(mfccs.T, axis=0)\n    mfccs_features = np.hstack([mfccs_mean, mfccs_std])\n    return mfccs_features\n\n\n### Spectral Contrast based feature extraction:\ndef extract_features_spectral_contrast(file_name):\n    '''Extract Spectral Contrast features and calculate the mean and std for each segment. Returns a 8 size 1D array'''\n    audio, sample_rate = librosa.load(file_name,res_type='kaiser_fast') \n    spec_con = librosa.feature.spectral_contrast(y=audio, sr=sample_rate, n_bands=3)\n    spec_con_mean = np.mean(spec_con.T,axis=0)\n    spec_con_std = np.std(spec_con.T, axis=0)\n    spec_con_feature = np.hstack([spec_con_mean, spec_con_std])\n    return spec_con_feature\n\n\n### Chroma Stft based feature extraction:\ndef extract_features_chroma_stft(file_name):\n    '''Extract Chroma Stft features and calculate the mean and std for each segment. Returns a 24 size 1D array'''\n    audio, sample_rate = librosa.load(file_name,res_type='kaiser_fast') \n    stft = np.abs(librosa.stft(audio))\n    chroma_stft = librosa.feature.chroma_stft(S=stft, sr=sample_rate)\n    chroma_mean = np.mean(chroma_stft.T,axis=0)\n    chroma_std = np.std(chroma_stft.T, axis=0)\n    chroma_feature = np.hstack([chroma_mean, chroma_std])\n    return chroma_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll be using MFCCs Mean and STD. You can find the accuracy analysis later in this notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_extraction_function = extract_features_mfcc_mean_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldatasetpath = '../input/urbansound8k/'\nfeatures = []\n\n\n# extracting features for each audio file:\nfor index, row in tqdm(metadata.iterrows()):\n    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n    class_label = row[\"class\"]\n    data = feature_extraction_function(file_name)\n    features.append([data, class_label])\n\n# converting to dataframe\nfeaturesdf = pd.DataFrame(features, columns=['feature','class_label'])\nprint('Finished feature extraction from ', len(featuresdf), ' files')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing features and labels in arrays\nX = np.array(featuresdf.feature.tolist())\ny = np.array(featuresdf.class_label.tolist())\n\nprint('Features Shape:', X.shape)\nprint('Class Shape:', y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we want the output of our prediction model to be a probability for each class, we will use One Hot Encoder. The final result for the prediction will be the one with the highest probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nyy = to_categorical(le.fit_transform(y))\nprint('Label Encoder')\nprint(y)\nprint(yy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll also be separating our dataset in 3.\nWith this separation, we can validate our model without using any audio files that were involved in our train and test process."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\nx_train, x_val, y_train, y_val  = train_test_split(x_train, y_train, test_size=0.25, random_state = 42)\n\nprint('Train Shapes')\nprint(x_train.shape)\nprint(y_train.shape)\nprint()\n\nprint('Test Shapes')\nprint(x_test.shape)\nprint(y_test.shape)\nprint()\n\nprint('Validation Shapes')\nprint(x_val.shape)\nprint(y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the Classification Model"},{"metadata":{},"cell_type":"markdown","source":"We'll be using keras for model training."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import Adam\nfrom sklearn import metrics\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nnum_labels = yy.shape[1] # dynamically selects the number of labels for the model \nn_features = X.shape[1] # dynamically selects the number of features for the model\n\n# Creating our model\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(n_features,)))\nmodel.add(Activation('relu')) # rectified linear unit activation\nmodel.add(Dropout(0.2)) # dropout to minimize overfitting\n\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(num_labels)) # last layer needs to have the same number of labels\nmodel.add(Activation('softmax')) # since we're using one hot enconder, let's use sotfmax on the last layer\n\n\n# Model Compilation\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll use 3 callbacks for our model fit:\n\n> 1. ModelCheckpoint - Save our model in a .h5 file.\n> 2. ReduceLROnPlateau - Reduce learning rate when the learning estagnates.\n> 3. EarlyStopping - Stop traing when the model stops improving 'val_loss' metric after 10 epochs.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training parameters\nnum_epochs = 100\nnum_batch_size = 32\n\n# callback definition\ncheckpoint = 'audio_classification_best_model.h5'\nmc = ModelCheckpoint(checkpoint, monitor = 'val_loss', mode = 'min', verbose = 1, save_best_only = True)\nrp = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10, verbose = 1, mode ='min', min_lr = 0.00000001)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience=10)\n\n# Model Training\nmodel.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_val, y_val), verbose=1, callbacks=[es, rp, mc])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing model with testing data\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])\n\n# testing model with validation data\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Validation Accuracy: \", score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using extract_features_mfcc_mean_std function, the testing accuracy rate is usually around 0,90~0,93"},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing model\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Validation Accuracy: \", score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using extract_features_mfcc_mean_std function, the validation accuracy rate is also usually around 0,90~0,93"},{"metadata":{},"cell_type":"markdown","source":"# Accuracy Analysis with Other Features and Model Configuration (Training History):\n\nFirst I tested different n_mfccs:\n\n        MFCCs numbers using only mean for each segment\n\n        --- n_mfccs_mean = 100, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.6542644500732422;\n        Tentativa 2 Accuracy: 0.7012020349502563;\n        Tentativa 3 Accuracy: 0.6891814470291138\n\n        --- n_mfccs_mean = 80, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.6742988228797913;\n        Tentativa 2 Accuracy: 0.6645678281784058;\n        Tentativa 3 Accuracy: 0.6674298644065857\n\n        --- n_mfccs_mean = 60, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.6176302433013916;\n        Tentativa 2 Accuracy: 0.6325128674507141;\n        Tentativa 3 Accuracy: 0.6330853104591370\n\n        --- n_mfccs_mean = 40, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.5523754954338074;\n        Tentativa 2 Accuracy: 0.635374903678894;\n        Tentativa 3 Accuracy: 0.6136233806610107\n\n        --- n_mfccs_mean = 20, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.48654836416244507;\n        Tentativa 2 Accuracy: 0.6159129738807678;\n        Tentativa 3 Accuracy: 0.5317687392234802\n\n        --- n_mfccs_mean = 10, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.47853463888168335;\n        Tentativa 2 Accuracy: 0.45907270908355713;\n        Tentativa 3 Accuracy: 0.45621064305305480\n\n\nWith these results, I had a benchmark for the testing accuracy rate.\nI also noticied that the gains for n_mfccs >60 were very small.\n\n\nAfter these initial testings I started looking for more features for the mfccs segments:\n\n        MFCCs numbers using only mean and std for each segment\n\n        --- n_mfccs_mean_std = 100, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.8248425722122192\n        Tentativa 2 Accuracy: 0.8236977458000183\n        Tentativa 3 Accuracy: 0.805380642414093\n\n        --- n_mfccs_mean_std = 80, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.8168288469314575\n        Tentativa 2 Accuracy: 0.8093875050544739\n        Tentativa 3 Accuracy: 0.8185460567474365\n\n        --- n_mfccs_mean_std = 60, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.7767601609230042\n        Tentativa 2 Accuracy: 0.782484233379364\n        Tentativa 3 Accuracy: 0.7985117435455322\n\n        --- n_mfccs_mean_std = 40, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.7784773707389832\n        Tentativa 2 Accuracy: 0.7641671299934387\n        Tentativa 3 Accuracy: 0.7710360884666443\n\n        --- n_mfccs_mean_std = 20, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.7063537240028381\n        Tentativa 2 Accuracy: 0.7000572681427002\n        Tentativa 3 Accuracy: 0.6748712062835693\n\n        --- n_mfccs_mean_std = 10, Epochs = 10 ---\n        Tentativa 1 Accuracy: 0.5792787671089172;\n        Tentativa 2 Accuracy: 0.5970234870910645\n        Tentativa 3 Accuracy: 0.6130509376525879\n\n\nWith mean and std I noticed an increment in testing accuracy.\n\nAlso, surprisingly the gunshot classifications started to become more precise.\nSince the gunshot usually is a fast and loud sound, std might be a better solution than just mean for the classification.\n\nI started testing some more calculations over the mfccs segments, like  variance, min, max,\nbut no other combination had the same results than mean and std.\n\n\n\nAfter that I started testing with callbacks for the model\nFist I used early stopping:\n\n        MFCCs numbers using only mean and std for each segment and keras early stopping\n\n        --- n_mfccs_mean_std = 100 ---\n        Tentativa 1 Accuracy: 0.8992558717727661, Epochs = 37/50\n        Tentativa 2 Accuracy: 0.8878076672554016, Epochs = 44/50\n        Tentativa 3 Accuracy: 0.9055523872375488, Epochs = 50/50\n\n        --- n_mfccs_mean_std = 80 ---\n        Tentativa 1 Accuracy: 0.8969662189483643, Epochs = 35/50\n        Tentativa 2 Accuracy: 0.8946765661239624, Epochs = 35/50\n        Tentativa 3 Accuracy: 0.8815111517906189, Epochs = 27/50\n\n        --- n_mfccs_mean_std = 60 ---\n        Tentativa 1 Accuracy: 0.9084144234657288, Epochs = 48/50\n        Tentativa 2 Accuracy: 0.8872352838516235, Epochs = 39/50\n        Tentativa 3 Accuracy: 0.8889524936676025, Epochs = 44/50\n\n        --- n_mfccs_mean_std = 40 ---\n        Tentativa 1 Accuracy: 0.8609043955802917, Epochs = 31/50\n        Tentativa 2 Accuracy: 0.8717802166938782, Epochs = 37/50\n        Tentativa 3 Accuracy: 0.8660560846328735, Epochs = 32/50\n\n        --- n_mfccs_mean_std = 20 ---\n        Tentativa 1 Accuracy: 0.8156840205192566, Epochs = 30/50\n        Tentativa 2 Accuracy: 0.8156840205192566, Epochs = 37/50\n        Tentativa 3 Accuracy: 0.8420149087905884, Epochs = 59/50\n\nWith early stopping I noticied a new increment in testing accuracy.\nI could also see that the number of epochs used in the previous testing was limiting the testing accuracy\n\nI noticied that n_mfcc = 60 is the best number for mfccs segments.\nConsidering this, I only used this configuration for my mfccs models.\n\n\n\n\nThen I started experimenting with some other features that the librosa library has:\n\n\nFirst, let's try Spectral_Contrast:\n\n        Spectral_Contrast using mean, std and early stopping\n\n        Tentativa 1 Accuracy: 0.587292492389679, Epochs = 74/100\n        Tentativa 2 Accuracy: 0.578133940696716, Epochs = 70/100\n        Tentativa 3 Accuracy: 0.6113337278366089, Epochs = 98/100\n\nThe results were significantly lower than mfccs.\n\n\nNow , let's try Chroma_Stft:\n\n        Chroma_Stft using mean, std and early stopping\n\n        Tentativa 1 Accuracy: 0.659416139125824, Epochs = 78/100\n        Tentativa 2 Accuracy: 0.6765884160995483, Epochs = 100/100\n        Tentativa 3 Accuracy: 0.6685746908187866, Epochs = 97/100\n\nThe results were also significantly lower than mfccs.\n\n\n\nWith these trainings I was quite happy with the 60 mfccs, mean, std and early stopping results.\nLastly I added the Reduce Learning On Plateau callback to cut off the learning when the results were satisfatory.\n\n\n#### Model Chosen:\n\n        features:\n            n_MFCC = 60 with mean and std for each segment\n        \n        model:\n            ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10, verbose = 1, mode ='min', min_lr = 0.00000001)\n            ModelCheckpoint(checkpoint, monitor = 'val_loss', mode = 'min', verbose = 1, save_best_only = True)\n            EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience=10)\n        \n        results bechmark:\n            Accuracy: 0.9273039698600769\n\n\nAll the functions used for this training is on this notebook."},{"metadata":{},"cell_type":"markdown","source":"# Applying the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_prediction_mfcc_mean_std(filename):\n    '''Classify audio files applying a model that uses mfcc features'''\n    prediction_feature = np.array([extract_features_mfcc_mean_std(filename)])\n    predicted_vector = model.predict_classes(prediction_feature)\n    predicted_class = le.inverse_transform(predicted_vector) \n    print(\"\\n\", \"Class: \", predicted_class[0], '\\n') \n\n    predicted_proba_vector = model.predict_proba(prediction_feature) \n    predicted_proba = predicted_proba_vector[0]\n    for i in range(len(predicted_proba)):\n        category = le.inverse_transform(np.array([i]))\n        print(category[0], \": \", format(predicted_proba[i], '.2f'), sep='')\n    print('\\n', '=-' *20, sep='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Testing File 1\nprint('Expected drilling')\nfilename = '../input/urbansound8k/fold7/104625-4-0-52.wav'\nprint_prediction_mfcc_mean_std(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Testing File 2\nprint('Expected siren')\nfilename = '../input/urbansound8k/fold4/24347-8-0-12.wav'\nprint_prediction_mfcc_mean_std(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Testing File 3\nprint('Expected dog_bark')\nfilename = '../input/urbansound8k/fold6/101281-3-0-5.wav'\nprint_prediction_mfcc_mean_std(filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model seems to be classifying the audios correctly!"},{"metadata":{},"cell_type":"markdown","source":"# Creating splicing function to classify audios in the same file"},{"metadata":{},"cell_type":"markdown","source":"Now that our model is ready, let's create a function to identify segments in an audio file to classify them separately"},{"metadata":{"trusted":true},"cell_type":"code","source":"def splice_audio(filename):\n   \n    '''Splicing audio files, returns a list with the audios segments'''\n    \n    data,sample_rate = librosa.load(filename)\n    \n    # Splicing\n    splits_indexes = librosa.effects.split(data,50)\n    audios = []\n    for n in range(len(splits_indexes)):\n        start = splits_indexes[n][0]\n        end = splits_indexes[n][1]\n        data_spliced = data[start:end]\n        audios.append(data_spliced)\n        \n    return audios\n\n\ndef predict_spliced_audio(audios):\n    \n    '''Classify each audio segment in a list'''\n    \n    for n in range(len(audios)):\n        mfccs = librosa.feature.mfcc(y=audios[n], sr=sample_rate, n_mfcc=60)\n        mfccs_mean = np.mean(mfccs.T,axis=0)\n        mfccs_std = np.std(mfccs.T, axis=0)\n        mfccs_features = np.hstack([mfccs_mean, mfccs_std])\n        prediction_feature = np.array([mfccs_features])\n\n        predicted_vector = model.predict_classes(prediction_feature)\n        predicted_class = le.inverse_transform(predicted_vector) \n        print(\"\\n\", \"Class: \", predicted_class[0], '\\n') \n\n        predicted_proba_vector = model.predict_proba(prediction_feature) \n        predicted_proba = predicted_proba_vector[0]\n\n        for i in range(len(predicted_proba)): \n            category = le.inverse_transform(np.array([i]))\n            print(category[0], \": \", format(predicted_proba[i], '.2f'), sep='')\n        print('\\n', '=-' *20)\n        \n        \n\ndef splice_and_classify(filename):\n    \n    '''In the same function, splice and classify all the identified audio segments in a file and print the results'''\n    \n    data,sample_rate = librosa.load(filename)\n    \n    # Splicing\n    splits_indexes = librosa.effects.split(data,50)\n    audios = []\n    for n in range(len(splits_indexes)):\n        start = splits_indexes[n][0]\n        end = splits_indexes[n][1]\n        data_spliced = data[start:end]\n        audios.append(data_spliced)\n    \n    # Using spliced audio segments\n    for n in range(len(audios)):\n        \n        # extracting features\n        mfccs = librosa.feature.mfcc(y=audios[n], sr=sample_rate, n_mfcc=60)\n        mfccs_mean = np.mean(mfccs.T,axis=0)\n        mfccs_std = np.std(mfccs.T, axis=0)\n        features = np.hstack([mfccs_mean, mfccs_std])\n        prediction_feature = np.array([features])\n        \n        # predicts using the trained model\n        predicted_vector = model.predict_classes(prediction_feature)\n        predicted_class = le.inverse_transform(predicted_vector) \n        print(\"\\n\", \"Class: \", predicted_class[0], '\\n') \n        \n        # calculating the probability for each class\n        predicted_proba_vector = model.predict_proba(prediction_feature) \n        predicted_proba = predicted_proba_vector[0]\n        \n        # printing results\n        for i in range(len(predicted_proba)): \n            category = le.inverse_transform(np.array([i]))\n            print(category[0], \": \", format(predicted_proba[i], '.2f'), sep='')\n        print('\\n', '=-' *20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using some youtube videos I created an audio file to check, if every classification is correct.\nThese are the videos I used to make this file:\n\n> 1.  siren (0:24-0:30)\n    https://www.youtube.com/watch?v=TXpm0gKG17k\n> 2.  dog_bark (0:30-0:37)\n    https://www.youtube.com/watch?v=iuy-oOJCOoM\n> 3.  gun_shot (0:00-0:05)\n    https://www.youtube.com/watch?v=-lbUHipN0F0\n> 4.  children_playing (0:05-0:15)\n    https://www.youtube.com/watch?v=7iDxLF2PWFw\n "},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"filename = '../input/custom-test-audio/custom_test_audio.wav'\nplot_sound(filename)\nsplice_and_classify(filename)\nipd.Audio(filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every classification is correct!"},{"metadata":{},"cell_type":"markdown","source":"#######"},{"metadata":{},"cell_type":"markdown","source":"# Bonus Section"},{"metadata":{},"cell_type":"markdown","source":"You can use the .h5 model that is already created to classify other audio files.With only the cells below you can make the classification, there's no need to run any cell above this section."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def splice_and_classify_model_input(filename, model):\n    \n    '''Alternate version for the splice_and_classify function. Please use .wav audio files.'''\n    \n\n    plt.figure(figsize=(30,4))\n    data,sample_rate = librosa.load(filename)\n    librosa.display.waveplot(data,sr=sample_rate)\n    \n    \n\n    splits_indexes = librosa.effects.split(data,50)\n    audios = []\n    for n in range(len(splits_indexes)):\n        start = splits_indexes[n][0]\n        end = splits_indexes[n][1]\n        data_spliced = data[start:end]\n        audios.append(data_spliced)\n\n\n    for n in range(len(audios)):\n\n        mfccs = librosa.feature.mfcc(audios[n], sr=sample_rate, n_mfcc=60)\n        mfccs_mean = np.mean(mfccs.T,axis=0)\n        mfccs_std = np.std(mfccs.T, axis=0)\n        mfccs_features = np.hstack([mfccs_mean, mfccs_std])\n\n \n        prediction_feature = np.array([mfccs_features])\n        predicted_vector = model.predict_classes(prediction_feature)\n        le = LabelEncoder()\n        y = np.array(['dog_bark', 'children_playing', 'car_horn', 'air_conditioner', 'street_music', 'gun_shot', 'siren', 'engine_idling', 'jackhammer', 'drilling'])\n        yy = to_categorical(le.fit_transform(y))\n        predicted_class = le.inverse_transform(predicted_vector) \n        print(\"\\n\", \"Class: \", predicted_class[0], '\\n') \n\n\n        predicted_proba_vector = model.predict_proba(prediction_feature) \n        predicted_proba = predicted_proba_vector[0]\n\n\n        for i in range(len(predicted_proba)):\n            category = le.inverse_transform(np.array([i]))\n            print(category[0], \": \", format(predicted_proba[i], '.2f'), sep='')\n        print('\\n', '=-' *20, sep='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model('../input/modelo/audio_classification_best_model (2).h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = '../input/custom-test-audio/custom_test_audio.wav'\nsplice_and_classify_model_input(filename, model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}