{"cells":[{"metadata":{},"cell_type":"markdown","source":"The necessary packages are imported.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the necessary packages\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is read into ‘df’ dataframe.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading the file\ndf = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us understand the dataframe ‘df’.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape # Shape of ‘df’","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns # Prints columns of ‘df’\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns are [‘Pregnancies’, ‘Glucose’, ‘BloodPressure’, ‘SkinThickness’, ‘Insulin’, ‘BMI’, ‘DiabetesPedigreeFunction’, ‘Age’, ‘Outcome’]\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() # Displays properties of each column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the columns have count = 768 which suggests there are no missing values. The mean of ‘Outcome’ is 0.35 which suggests there are more ‘Outcome’ = 0 than ‘Outcome’ = 1 cases in the given dataset.\nThe dataframe ‘df’ is converted into numpy array ‘dataset’","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = df.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ‘dataset’ is split into input X and output y\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset[:,0:8]\ny = dataset[:,8].astype(\"int\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardization\nIt can be observed that mean value of columns are very different. Hence the dataset is to be standardized so that no inappropriate weightage is given to any feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization\na = StandardScaler()\na.fit(X)\nX_standardized = a.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us look aat mean and standard deviation of ‘X_standardized’.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(X_standardized).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean of all columns is around 0 and Standard deviation of all columns is around 1. The data has been standardized.\n\n## Tuning of Hyperparameters :- Batch Size and Epochs\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the necessary packages\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The neural architecture and optimization algorithm are defined. The neural network consists of 1 input layer, 2 hidden layers with rectified linear unit activation function and 1 output layer with sigmoid activation function. Adams optimization is chosen as the optimization algorithm for the neural network model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Dense(8, input_dim=8, kernel_initializer='normal', activation=\"relu\"))\n    model.add(Dense(4, input_dim=8, kernel_initializer='normal', activation=\"relu\"))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    \n    adam = Adam(lr=0.01)\n    model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We run the grid search for 2 hyperparameters :- ‘batch_size’ and ‘epochs’. The cross validation technique used is K-Fold with the default value k = 3. The accuracy score is calculated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the model\nmodel = KerasClassifier(build_fn = create_model,verbose = 0)\n# Define the grid search parameters\nbatch_size = [10,20,40]\nepochs = [10,50,100]\n# Make a dictionary of the grid search parameters\nparam_grid = dict(batch_size = batch_size,epochs = epochs)\n# Build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are summarized. The best accuracy score and the best values of hyperparameters are printed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summarize the results\nprint(\"Best : {}, using {}\".format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\nfor mean, stdev, param in zip(means, stds, params):\n  print(\"{},{} with: {}\".format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best accuracy score is 0.7604 for ‘batch_size’ = 40 and ‘epochs’ = 10. So we choose ‘batch_size’ = 40 and ‘epochs’ = 10 while tuning other hyperparameters.\n\n## Tuning of Hyperparameters:- Learning rate and Drop out rate\nThe learning rate plays an important role in optimization algorithm. If the learning rate is too large, the algorithm may diverge and thus can’t find the local optima. If the learning rate is too small, the algorithm may take many iterations to converge which results in high computational power and time. Thus we need an optimum value of learning rate which is small enough for the algorithm to converge and large enough to fasten the converging process. The learning rate helps with ‘Early Stopping’ which is a regularization method where the training set is trained as long as test set accuracy is increasing.\n\nDrop out is a regularization method that reduces the complexity of the model and thus prevents overfitting the training data. By dropping an activation unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections. Dropout rate can take values between 0 and 1. 0 implies no activation units are knocked out and 1 implies all the activation units are knocked out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout\n\n# Defining the model\n\ndef create_model(learning_rate,dropout_rate):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = learning_rate)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nlearning_rate = [0.001,0.01,0.1]\ndropout_rate = [0.0,0.1,0.2]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best accuracy score is 0.7695 for ‘dropout_rate’ = 0.1 and ‘learning_rate’ = 0.001. So we choose ‘dropout_rate’ = 0.1 and ‘learning_rate’ = 0.001 while tuning other hyperparameters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Tuning of Hyperparameters:- Activation Function and Kernel Initializer\nActivation functions introduce non-linear properties to the neural network such that non-linear complex functional mappings between input and output can be established. If we do not apply the activation function, then the output would be a simple linear function of the input.\n\nThe neural network needs to start with some weights and then iteratively update them to better values. Kernel initializer decides the statistical distribution or function to be used for initializing the weights.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model\n\ndef create_model(activation_function,init):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nactivation_function = ['softmax','relu','tanh','linear']\ninit = ['uniform','normal','zero']\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(activation_function = activation_function,init = init)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best accuracy score is 0.7591 for ‘activation_function’ = tanh and ‘kernel_initializer’ = uniform. So we choose ‘activation_function’ = tanh and ‘kernel_initializer’ = uniform while tuning other hyperparameters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Tuning of Hyperparameter :-Number of Neurons in activation layer\nThe complexity of the data has to be matched with the complexity of the model. The number of neurons in activation layer decides the complexity of the model. Higher the number of neurons in activation layer, higher is the degree of non-linear complex functional mappings between input and output.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model\n\ndef create_model(neuron1,neuron2):\n    model = Sequential()\n    model.add(Dense(neuron1,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nneuron1 = [4,8,16]\nneuron2 = [2,4,8]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best accuracy score is 0.7591 for number of neurons in first layer = 16 and number of neurons in second layer = 4.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Training model with optimum values of Hyperparameters\nThe model is trained using optimum values of hyperparameters found in previous section.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\n# Defining the model\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(16,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Fitting the model\n\nmodel.fit(X_standardized,y)\n\n# Predicting using trained model\n\ny_predict = model.predict(X_standardized)\n\n# Printing the metrics\n\nprint(accuracy_score(y,y_predict))\nprint(classification_report(y,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get an accuracy of 77.6% and F1 scores of 0.84 and 0.65.\nThe hyperparameter optimization was carried out by taking 2 hyperparameters at once. We may have missed the best values. The performance can be further improved by finding the optimum values of hyperparameters all at once given by the code snippet below. Note:- This process is computationally expensive.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n    model = Sequential()\n    model.add(Dense(neuron1,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = learning_rate)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0)\n\n# Define the grid search parameters\n\nbatch_size = [10,20,40]\nepochs = [10,50,100]\nlearning_rate = [0.001,0.01,0.1]\ndropout_rate = [0.0,0.1,0.2]\nactivation_function = ['softmax','relu','tanh','linear']\ninit = ['uniform','normal','zero']\nneuron1 = [4,8,16]\nneuron2 = [2,4,8]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}