{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel I compare Keras Classifier algorithms for different \"Numbers of ANN\", \"CV\" and \"epochs\" values\".\nI try to guess \"sex\" of people who is die becuase of assassination from only 7 numeric columns. In fact there are a lot of columns and I should have benefit from them by using \"one hot encoder\" and \"label encoder\" techniques but I don't know very well to these techniques. I hope I will use these techniques in other project.\n\nIn fact hoping to gueess to gender of victims as true is not realistic but my results are not bad. I believe they can be better.\n\n**CONTENT**\n1. Data Cleaning\n2. Normalization\n3. Train Test Split\n4. Build to 2 Layer Neural Network\n5. Buil ANN from Keras for different hyperparameters\n6. Conclusion \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/suicide-rates-overview-1985-to-2016/master.csv\",sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Data Cleaning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping null and non numeretic columns\ndata.dropna(inplace=True)\ndata.drop([\"age\",\"country\",\"country-year\",\"generation\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert male and femala 1 and 0\ndata.sex = [1 if each == \"male\" else 0 for each in data.sex]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deleting spaces\ndata.rename(columns={' gdp_for_year ($) ':'gdp_year'}, inplace=True)\ndata.rename(columns={'HDI for year':'HDI_year'}, inplace=True)\ndata.rename(columns={'suicides/100k pop':'suicides/100k_pop'}, inplace=True)\ndata.rename(columns={'gdp_per_capita ($)':'gdp_per_capita_dollar'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deleting comas\ndata.gdp_year = data.gdp_year.str.replace(',','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining x and y\ny = data.sex\nx_data = data.drop([\"sex\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I should convert type of sixth column to float from string.\nx_data.gdp_year = data.gdp_year.apply(lambda x: float(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Normalization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Train Test Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.15, random_state = 42)\n\nx_train = x_train.values.T\nx_test = x_test.values.T\ny_test = y_test.values.reshape(1,y_test.shape[0])\ny_train = y_train.values.reshape(1,y_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sigmoid function\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# intialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forward propagation\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute cost\ndef compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)/Y.shape[1]\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Backward Propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# update parameters\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n      \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))    \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library \nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1])) # we use dimension of x_train as input\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu')) # we use 4 nodes in first layer\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # if we use sigmoid function it means we add output layer\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) # we will use accuracy as metrics\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100) # epochs means that is number of iteration \naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_of_values = {'ANN_Num': [], 'CV': [],'epochs': [] , 'accuracy': [] }\ndata_temproray = pd.DataFrame.from_dict(dict_of_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_temproray.ANN_Num = [2]\ndata_temproray.CV = [3]\ndata_temproray.epochs = [100]\ndata_temproray.accuracy = [0.6558003964712986]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_temproray.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the ANN V2\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library \nfrom keras.layers import Dense # build our layers library\ndef build_classifier2():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1])) # we use dimension of x_train as input\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu')) # we use 4 nodes in second layer\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # if we use sigmoid function it means we add output layer\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) # we will use accuracy as metrics\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier2, epochs = 150) # epochs means that is number of iteration \naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 4)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_temproray.loc[-1] = [2, 4, 150,  0.6640782611279521]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [3, 3, 100, 0.492614638565769]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [3, 4, 150, 0.49022363005586056]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [2, 4, 150, 0.7318890200427076]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [2, 5, 150, 0.7341438597594337]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [2, 4, 200, 0.7310445842269522]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have changed ANN_Num, CV and epochs but I don't run them again because it takes a long time. I will write results of these algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_temproray","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I multiply with 100 to accuracy column if I don't do it, plotly will have be problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_temproray.accuracy = data_temproray.accuracy.apply(lambda x: x*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_temproray","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotly\n#import plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\ninternational_color = [float(each) for each in data_temproray.epochs]\ndata2 = [\n    {\n        'y':data_temproray.CV,\n        'x': data_temproray.ANN_Num,\n        'mode': 'markers',\n        'marker': {\n            'color': international_color,\n            'size': data_temproray.accuracy,\n            'showscale': True\n        },\n        \"text\" :  data_temproray.accuracy    \n    }\n]\niplot(data2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Coclusion**\n\nAltough it looks that trying to guess to gender of victims as true is non realistic I have guessed 73.41% as true but I do it to improve myself.\nThere are no doubt that this model can be improved by using other variables whose type is string.\nThe best result of this model is 73.41% thanks to \"Layer of Network\" know as ANN_num = 2, CV=5, epochs=150 "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}