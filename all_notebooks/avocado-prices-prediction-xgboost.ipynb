{"cells":[{"metadata":{},"cell_type":"markdown","source":">Hello! My name is [Mauricio Ruanova](https://mruanova.com)."},{"metadata":{},"cell_type":"markdown","source":"Table of Contents\n1. [Step 1 - Identify The Problem](#step1)\n1. [Step 2 - Exploratory Data Analysis](#step2)\n1. [Step 3 - Distribution](#step3)\n1. [Step 4 - Feature Importance](#step4)\n1. [Step 5 - Outliers](#step5)\n1. [Step 6 - Missing data](#step6)\n1. [Step 7 - Select the model](#step6)\n1. [Step 8 - Evaluate the model](#step7)\n1. [Step 9 - Conclusion](#step8)"},{"metadata":{},"cell_type":"markdown","source":"![avocado](https://mruanova.com/avocado.gif)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step1\"></a>\n# Step 1 Identify The Problem\nGiven the data set with the [avocado prices](https://www.kaggle.com/neuromusic/avocado-prices) from 2015 to 2018, we will predict the prices using xgboost."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step2\"></a>\n# Step 2 Exploratory Data Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\nimport numpy as np # linear algebra\nprint(\"NumPy version: {}\". format(np.__version__))\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nprint(\"pandas version: {}\". format(pd.__version__))\nimport matplotlib # collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings # ignore warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/avocado-prices/avocado.csv',index_col=0) # df.rename( columns={'Unnamed: 0':'id'}, inplace=True )\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will rename the first column to 'id'."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step3\"></a>\n# Step 3 Distribution\nFirst let's take a look at the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(df.AveragePrice, ax=ax[0])\nsns.boxplot(df.AveragePrice, ax=ax[1])\nfrom scipy import stats\nstats.probplot(df['AveragePrice'], plot=ax[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: Bimodal distribution but why? Maybe because conventional versus organic."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['type'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see that there are two types: conventional and organic."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.title(\"Avocado Average Price by Type\")\nsns.barplot(x=\"type\",y=\"AveragePrice\",data= df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conventional = len(df[df['type'] == 'conventional'])\nconventional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"organic = len(df[df['type']== 'organic'])\norganic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ny = ('conventional', 'organic')\ny_pos = np.arange(len(y))\nx = (conventional, organic)\nlabels = 'conventional', 'organic'\nsizes = [conventional, organic]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=90) \nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Percentage', size=16)\nplt.show() # Pie chart, where the slices will be ordered and plotted counter-clockwise:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: data is 50% conventional and 50% organic."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: %f\" % df['AveragePrice'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Acceptable values of skewness fall between − 3 and + 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Kurtosis: %f\" % df['AveragePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kurtosis is appropriate from a range of − 10 to + 10."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_conventional = df[df['type'] == 'conventional']\n# df_conventional.shape\ndf_organic = df[df['type'] == 'organic']\n# df_organic.shape\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 4))\nsns.distplot(df_conventional['AveragePrice']) # histogram\nsns.distplot(df_organic['AveragePrice']) # histogram\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: The organic avocados are more expensive."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_conventional = df[df['type'] == 'conventional']\n# df_conventional.shape\ndf_organic = df[df['type'] == 'organic']\n# df_organic.shape\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 4))\nsns.boxplot(df_conventional['AveragePrice']) # histogram\nsns.boxplot(df_organic['AveragePrice'],palette = 'pink') # histogram\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But now we need to know what other features are driving up the price? Maybe the region?"},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = df['type']=='organic'\ng = sns.factorplot('AveragePrice','region',data=df[mask],\n    hue='year',size=13,aspect=0.8,palette='Spectral',join=False,)\n# https://seaborn.pydata.org/tutorial/color_palettes.html","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: The price not only depends on the type, but also on the region. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step4\"></a>\n# Step 4 Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = df.corr()\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 10))\nax.set_title(\"Correlation Matrix\", fontsize=16)\nfilter = df.columns != 'id'\nsns.heatmap(df[df.columns[filter]].corr(), vmin=-1, vmax=1, cmap='coolwarm', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: Total Volume (98) and Total Bags (99) also have a strong correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total number of duplicate values : ',sum(df.duplicated()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step5\"></a>\n# Step 5 Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() # outliers?","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: I usually take a look at the min and max values to identify outliers but I didn't find any this time."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step6\"></a>\n# Step 6 Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Missing data: {df.isna().sum(axis=0).any()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['type']= df['type'].map({'conventional':0,'organic':1})\n\n# Extracting month from date column.\ndf.Date = df.Date.apply(pd.to_datetime)\ndf['Month'] = df['Date'].apply(lambda x:x.month)\ndf.drop('Date',axis=1,inplace=True)\ndf.Month = df.Month.map({1:'JAN',2:'FEB',3:'MARCH',4:'APRIL',5:'MAY',6:'JUNE',7:'JULY',8:'AUG',9:'SEPT',10:'OCT',11:'NOV',12:'DEC'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step7\"></a>\n# Step 7 Select the model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummy variables\ndummies = pd.get_dummies(df[['year','region','Month']],drop_first=True)\ndf_dummies = pd.concat([df[['Total Volume', '4046', '4225', '4770', 'Total Bags',\n       'Small Bags', 'Large Bags', 'XLarge Bags', 'type']],dummies],axis=1)\ntarget = df['AveragePrice']\n\n# Splitting data into training and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_dummies,target,test_size=0.30)\n\n# Standardizing the data\ncols_to_std = ['Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags','Large Bags', 'XLarge Bags']\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X_train[cols_to_std])\nX_train[cols_to_std] = scaler.transform(X_train[cols_to_std])\nX_test[cols_to_std] = scaler.transform(X_test[cols_to_std])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step8\"></a>\n# Step 8 Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nmodel = XGBRegressor()\nmodel.fit(X_train, y_train)\nY_pred = model.predict(X_test)\nscore = model.score(X_train, y_train)\nprint('Training Score:', score)\nscore = model.score(X_test, y_test)\nprint('Testing Score:', score)\noutput = pd.DataFrame({'Predicted':Y_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(output.head())\npeople = output.loc[output.Predicted == 1][\"Predicted\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"step9\"></a>\n# Step 9 Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"mae = np.round(mean_absolute_error(y_test,Y_pred),3)\nprint('Mean Absolute Error:', mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse = np.round(mean_squared_error(y_test,Y_pred),3)\nprint('Mean Squared Error:', mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = np.round(r2_score(y_test,Y_pred),3)\nprint('R2 Score:', score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost has a score of 89% which is pretty good but could be better."},{"metadata":{},"cell_type":"markdown","source":"thanks to [ayushikaushik](https://www.kaggle.com/ayushikaushik/comparison-of-all-regression-models) for your examples."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}