{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This is my first Kaggle project. I will do some EDA on the dataset. I will then do some feature selection, apply different models, tune and select the best one and predict cancellation. I am new to this field and Kaggle so I appreciate if you can leave comments and suggestions. Thanks "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Ignore warnings\nimport warnings  \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/hotel-booking-demand/hotel_bookings.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's look at missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_missing_data = pd.DataFrame([data.isnull().sum(),data.isnull().sum()*100.0/data.shape[0]]).T\nperc_missing_data.columns = ['No. of Missing Data', '% Missing Data']\nperc_missing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 0.003% of rows have missing information for children. Let's look at distribution of children to fill missing information"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['children'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most bookings had no children and hence we will fill the missing rows for children with value 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['children'].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 0.4% of rows have missing information for country. Let's look at distribution of country to fill missing information"},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_country_data = pd.DataFrame([data['country'].value_counts(),data['country'].value_counts()*100/data.shape[0]]).T\nperc_country_data.columns = ['Count', '% Distribution']\nperc_country_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 40.7% of bookings are from Portugal. Only 0.4% is missing information. We will fill the missing rows of country as Portugal since the distribution will not change by much and we still get to preserve data and discard the row."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['country'].fillna('PRT',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 13% of agent ID and 94% of company ID is missing. It is possible to dive deep into the details of the dataset to find a correlation of missing information in agent ID and company ID vs other other features like market segment, distribution channel etc; For example most direct bookings may not have an agent ID or company ID and the information is probably null. It is therefore possible to fill these missing values based on other features however for simplicity, we will drop both the columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['agent','company'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's confirm all missing data have been handled"},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_missing_data = pd.DataFrame([data.isnull().sum(),data.isnull().sum()*100.0/data.shape[0]]).T\nperc_missing_data.columns = ['No. of Missing Data', '% Missing Data']\nperc_missing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA VISUALIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at distribution of hotel bookings and separate them by their cancellation status"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.countplot(x='hotel',data=data,hue='is_canceled',palette='pastel')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### About 25% of resort hotel bookings have been cancelled and about 40% of city hotel bookings have been cancelled. These numbers are high and have potential implications in revenue for the hotels"},{"metadata":{},"cell_type":"markdown","source":"### Let's look at deposit type vs cancellation status"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.countplot(x='deposit_type',data=data,hue='is_canceled',palette='pastel')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### About 30k bookings of deposit type 'No Deposit' were cancelled. These numbers are huge if the hotels were not able to replace the cancelled bookings in time. It's a significant loss for the hotel. But in the next section, we will look at date of cancellation vs date of arrival to understand the impact of cancellation and how much time the hotel had to prepare for cancellations.\n\n#### Also it is interesting to note that non-refundable deposits had more cancellation than refundable deposits. Logically one would have assumed that refundable deposits have more cancellation as hotel rates are usually higher for refundable deposit type rooms and customers pay more in anticipation of cancellation"},{"metadata":{},"cell_type":"markdown","source":"### Date of Cancellation vs Date of Arrival"},{"metadata":{},"cell_type":"markdown","source":"#### We first need to create a new column called arrival_date that combines arrival date year, month and date. We then compare arrival_date to cancellation date to find out how the cancellation happens. Cancellation date can be identified from reservation_status_date for reservation_status = Cancelled"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['arrival_date'] = data['arrival_date_year'].astype(str) + '-' + data['arrival_date_month'] + '-' + data['arrival_date_day_of_month'].astype(str)\ndata['arrival_date'] = data['arrival_date'].apply(pd.to_datetime)\ndata['reservation_status_date'] = data['reservation_status_date'].apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a new dataframe for cancelled bookings called cancelled_data. Add a new column called canc_to_arrival_days that is the difference between cancellation date and arrival date"},{"metadata":{"trusted":true},"cell_type":"code","source":"cancelled_data = data[data['reservation_status'] == 'Canceled']\ncancelled_data['canc_to_arrival_days'] = cancelled_data['arrival_date'] - cancelled_data['reservation_status_date']\ncancelled_data['canc_to_arrival_days'] = cancelled_data['canc_to_arrival_days'].dt.days","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's visualize distribution of days from cancellation to arrival"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.distplot(cancelled_data['canc_to_arrival_days'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Assuming the hotel can sufficiently replace the cancelled reservation in a week, we are only interested in cancellations that happen less than a week to arrival date which bear a financial cost to the hotels"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of cancellations that are within a week of arrival: ', \n      (cancelled_data[cancelled_data['canc_to_arrival_days']<=7]['canc_to_arrival_days'].count()*100/cancelled_data['canc_to_arrival_days'].count()).round(2), '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 12% of cancellations happen less than a week. There is huge benefits to predicting if a customer will cancel a booking so the hotel can adequately prepare for it. "},{"metadata":{},"cell_type":"markdown","source":"### Let's visualize other features to have an idea about the dataset "},{"metadata":{},"cell_type":"markdown","source":"#### Let's see at what times of the year do we have the highest bookings"},{"metadata":{"trusted":true},"cell_type":"code","source":"month_sorted = ['January','February','March','April','May','June','July','August','September','October','November','December']\nplt.figure(figsize=(14,6))\nsns.countplot(data['arrival_date_month'], palette='pastel', order = month_sorted)\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It looks like the summer months May-August have the highest demand. The winter months November-February have the lowest demand. Let's now see which months have the highest cancellations as our target is cancellations."},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_monthly_canc = pd.DataFrame(data[data['is_canceled'] == 1]['arrival_date_month'].value_counts() * 100 / data['arrival_date_month'].value_counts())\nperc_monthly_canc.reset_index()\nplt.figure(figsize=(14,6))\nsns.barplot(x=perc_monthly_canc.index,y='arrival_date_month',data=perc_monthly_canc, order=month_sorted, palette='pastel')\nplt.xticks(rotation = 90)\nplt.ylabel('% cancellation per month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is not a significant difference in the percentage cancellations between months however the lowest demand months have the lowest % cancellations and the highest demand months have the highest % cancellations. The hotels will accept this trend as filling in cancelled rooms during peak season becomes easier. "},{"metadata":{},"cell_type":"markdown","source":"### Let's now look at market segment vs cancellation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nexplode = [0.005] * len(cancelled_data['market_segment'].unique())\ncolors = ['royalblue','orange','y','darkgreen','gray','purple','red','lightblue']\nplt.pie(cancelled_data['market_segment'].value_counts(),\n       autopct = '%.1f%%',\n       explode = explode,\n       colors = colors)\nplt.legend(cancelled_data['market_segment'].unique(), bbox_to_anchor=(-0.1, 1.),\n           fontsize=14)\nplt.title('Market Segment vs Cancelled Bookings')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### About 65% of the cancelled bookings are by travel agents or tour operators"},{"metadata":{},"cell_type":"markdown","source":"#### So far we've looked at some features and plotted their general distribution and how they behave against cancellation. Now let's look at the entire feature set and see how they correlate with cancellation status. This step is going to help us select features for our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ndata.corr()['is_canceled'].sort_values()[:-1].plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The correlation happens only with numerical values. Let's look at distribution of some of categorical variables that we've not covered already in the previous sections to decide which ones out of those we want to carry over for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.subplot(221)\nsns.countplot(data['meal'], hue=data['is_canceled'])\nplt.xlabel('Meal Type')\nplt.subplot(222)\nsns.countplot(data['customer_type'], hue=data['is_canceled'])\nplt.xlabel('Customer Type')\nplt.subplot(223)\nsns.countplot(data['reserved_room_type'], hue=data['is_canceled'])\nplt.xlabel('Reserved Room Type')\nplt.subplot(224)\nsns.countplot(data['reservation_status'], hue=data['is_canceled'])\nplt.xlabel('Reservation Status')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It's clear that meal type and reserved room type don't have bookings evenly distributed. In both the features, bookings heavily favour one category and hence we will drop both columns. We will drop deposit type(visualized previously) for the same reasons\n\n#### We will keep customer type feature and convert to dummy variables.\n\n#### The reservation status feature is basically the target variable. To avoid data leakage we will drop this column as well.\n\n#### There are too many countries and will add a lot of dimension when converted to dummy variables. We will drop this column as well. (NOTE - we could have dropped this at the filling missing data stage itself)\n\n#### We will also drop all the date features.\n\n#### We will convert the other categorical features which we've visualized in the previous sections based on intuition."},{"metadata":{},"cell_type":"markdown","source":"## CONVERTING CATEGORICAL COLUMNS TO DUMMY VARIABLES AND DROPPING UNNECESSARY COLUMNS"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['meal','country','reserved_room_type','assigned_room_type','deposit_type','reservation_status','reservation_status_date','arrival_date'], axis=1)\ndata = pd.concat([data, \n                 pd.get_dummies(data['hotel'], drop_first=True), \n                 pd.get_dummies(data['arrival_date_month'], drop_first=True), \n                 pd.get_dummies(data['market_segment'], drop_first=True),\n                 pd.get_dummies(data['distribution_channel'], drop_first=True),\n                 pd.get_dummies(data['customer_type'], drop_first=True)\n                 ], axis=1)\ndata = data.drop(['hotel','arrival_date_month','market_segment','distribution_channel','customer_type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Verifying no categorical variables exist in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let us again look at the correlation of the target variable with rest of the selected features after dummy variable conversion"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\ndata.corr()['is_canceled'].sort_values()[:-1].plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODELING"},{"metadata":{},"cell_type":"markdown","source":"We will use all features left to build our model. The following are the steps we will take to build our model.\n\n1. Split into training and test sets\n2. Apply feature scaling\n3. Baseline model\n4. Train and predict multiple models - LogisticRegression, KNearestNeighbors, SVM, RandomForest\n5. Compare against baseline model and choose the best model using accuracy\n6. Use grid search to tune hyperparameters\n7. Retrain model using chosen hyperparameters\n8. Predict "},{"metadata":{},"cell_type":"markdown","source":"#### Split into training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:, 1:].values\ny = data.iloc[:, 0].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Functions and Variable Assignments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Empty dictionary of model accuracy results\nmodel_accuracy_results = {}\n\n# Function for calculating accuracy from confusion matrix\nfrom sklearn.metrics import confusion_matrix\ndef model_accuracy(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    accuracy = ((cm[0,0] + cm [1,1]) * 100 / len(y_test)).round(2)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Baseline Model - We will take the class that has most observations in the training set and applying it as the predicted result and compute accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline model\n(unique, counts) = np.unique(y_train, return_counts=True)\nif counts[0]  > counts[1]:\n    idx = 0\nelse:\n    idx = 1\n\n# Applying baseline results to y_pred\nif idx == 0:\n    y_pred = np.zeros(y_test.shape)\nelse:\n    y_pred = np.ones(y_test.shape)\n\n# Computing accuracy\nmodel_accuracy_results['Baseline'] = model_accuracy(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and train\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, max_iter=250)\nclassifier.fit(X_train, y_train)\n\n# Predict\ny_pred = classifier.predict(X_test)\n\n# Computing accuracy\nmodel_accuracy_results['LogisticRegression'] = model_accuracy(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and train\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 10)\nclassifier.fit(X_train,y_train)\n\n# Predict\ny_pred = classifier.predict(X_test)\n\n# Computing accuracy\nmodel_accuracy_results['KNearestNeighbors'] = model_accuracy(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and train\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state=0)\nclassifier.fit(X_train,y_train)\n\n# Predict\ny_pred = classifier.predict(X_test)\n\n# Computing accuracy\nmodel_accuracy_results['SVM'] = model_accuracy(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and train\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)\n\n# Predict\ny_pred = classifier.predict(X_test)\n\n# Computing accuracy\nmodel_accuracy_results['RandomForest'] = model_accuracy(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's visualize the accuracy results"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model_accuracies = pd.DataFrame(list(model_accuracy_results.values()), index=model_accuracy_results.keys(), columns=['Accuracy'])\ndf_model_accuracies\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will choose the RandomForest Model as it gives the best accuracy. Now we will tune the hyper parameters on the random forest model using grid search and retrain the model to see if performance improved"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators': [10,25,50,100,500] , 'criterion': ['entropy', 'gini']}]\nrandomforestclassifier = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = randomforestclassifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           n_jobs = -1)\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Results of grid search"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Score: ', grid_search.best_score_.round(2))\nprint('Best Parameters: ', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will choose the best parameters and re-train the model with the new parameters and compare against the original RandomForest classifier accuracy result"},{"metadata":{},"cell_type":"markdown","source":"#### RandomForest with new parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit and train\noptimized_classifier = RandomForestClassifier(n_estimators=500, criterion='entropy', random_state=0)\noptimized_classifier.fit(X_train,y_train)\n\n# Predict\ny_pred = optimized_classifier.predict(X_test)\n\n# Computing accuracy\nmodel_accuracy_results['OptimizedRandomForest'] = model_accuracy(y_test, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's visualize the model accuracy results again with OptimizedRandomForest included"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model_accuracies = pd.DataFrame(list(model_accuracy_results.values()), index=model_accuracy_results.keys(), columns=['Accuracy'])\ndf_model_accuracies","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's visualize the confusion matrix of the OptimizedRandomForest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"orf_cm = confusion_matrix(y_test, optimized_classifier.predict(X_test))\n\nnames = ['True Neg','False Pos','False Neg','True Pos'] # list of descriptions for each group\nvalues = [value for value in orf_cm.flatten()] # list of values for each group\npercentages = [str(perc.round(2))+'%' for perc in orf_cm.flatten()*100/np.sum(orf_cm)] # list of percentages for each group\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(names,values,percentages)] # zip them into list of strings as labels\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(orf_cm, annot=labels, fmt='', cmap='binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We've completed predicting cancellations with an accuracy of 86%. Please upvote and leave comments. Thanks for your time."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}