{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# *Pump-it-up project*\n\n### Can you predict which water pumps are faulty?\n\n## Goal\nUsing data from Taarifa and the Tanzanian Ministry of Water, predict which pumps are functional, which need some repairs, and which don't work at all based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. \n\nA smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania."},{"metadata":{},"cell_type":"markdown","source":"# II. Data Cleaning & Preprocessing\n## 1. Libraries and input data"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n%matplotlib inline\n\nprint(\"Setup Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load the data after EDA\ntrain_df = pd.read_csv(\"../input/pumpitup-challenge-dataset/train_df_after_EDA.csv\")\nX_test = pd.read_csv(\"../input/pumpitup-challenge-dataset/X_test_after_EDA.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Dropping similar features"},{"metadata":{},"cell_type":"markdown","source":"The following groups of features\n\n- (extraction_type, extraction_type_group, extraction_type_class),\n- (payment, payment_type),\n- (water_quality, quality_group),\n- (source, source_class),\n- (subvillage, region, region_code, district_code, lga, ward),\n- (waterpoint_type, waterpoint_type_group)\n- (scheme_name, scheme_management)\n\ncontain very similar information, so the correlation between them is high. This way we are risking overfitting the training data by including all the features in our analysis.\n\nBesides:\n\nnum_private is ~99% zeros and has no description, so we cannot interpret it\nin the wpt_name feature 45k unique values out of 75k observations, not very informative -> drop for now\n\n**TO DO**: The correlation between \"construction_year\" and \"gps_height\" is high, but these 2 variables don't have any obvious connection, so explore this correlation further to take a decision.\n\nAs we saw earlier, there exists quite a strong correlation between district_code and region_code, so we will drop one of these variables. The negative correlation to the target variable of the \"region_code\" is higher than that of the \"district_code\". Keep the variable with higher correlation to the target."},{"metadata":{"trusted":false},"cell_type":"code","source":"# drop columns\ntrain_df = train_df.drop(['installer','management_group','status_group','id_x','id_y', 'num_private', 'wpt_name', \n          'recorded_by', 'subvillage', 'scheme_name', 'region', \n          'quantity', 'water_quality', 'lga','ward', 'source_type', 'payment', \n          'waterpoint_type_group','extraction_type_group','extraction_type_class'],axis=1)\nX_test = X_test.drop(['installer','management_group','id', 'num_private', 'wpt_name', \n          'recorded_by', 'subvillage', 'scheme_name', 'region', \n          'quantity', 'water_quality', 'lga','ward', 'source_type', 'payment', \n          'waterpoint_type_group','extraction_type_group','extraction_type_class'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Dealing with missing values"},{"metadata":{"trusted":false},"cell_type":"code","source":"## Null scheme_management, funder, installer, public_meeting and permit values replace with \"unknown\" text\ntrain_df[\"scheme_management\"].fillna(\"unknown\", inplace = True)\ntrain_df[\"public_meeting\"].fillna(\"unknown\", inplace = True)\ntrain_df[\"permit\"].fillna(\"unknown\", inplace = True)\ntrain_df[\"funder\"].fillna(\"unknown\", inplace = True)\n# train_df[\"installer\"].fillna(\"unknown\", inplace = True)\n\nX_test[\"scheme_management\"].fillna(\"unknown\", inplace = True)\nX_test[\"public_meeting\"].fillna(\"unknown\", inplace = True)\nX_test[\"permit\"].fillna(\"unknown\", inplace = True)\nX_test[\"funder\"].fillna(\"unknown\", inplace = True)\n# X_test[\"installer\"].fillna(\"unknown\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.isna().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ** Transforming data to reduce skew\nUni-modal, skewed distributions could potentially be log transformed: \n* Longtitude\n* GPS_hight\n* Region_code\n\n** For later"},{"metadata":{},"cell_type":"markdown","source":"## 5. Reducing cardinality\n### 5.1 Select features"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get number of unique entries in each column with categorical data\ncat_vars = train_df.select_dtypes(include='object').columns\nobject_nunique = list(map(lambda col: train_df[col].nunique(), cat_vars))\nd = dict(zip(cat_vars, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the above analysis:\n\nthe \"recorded_by\" feature can be dropped as there is only 1 unique value, it doesn't help in predicting.\n\nThe columns in which values can be ordered we can perform an Ordinal encoding: \n* quality_group\n* quantity_group\n* payment_type\n\nThe cardinality of the following 2 features should be reduced to 10 and then one-hot encode them or try Binary encoding:\n* scheme_managenemt\n* extraction_type\n\nWhat to do with the following 3? The cardinality is too high... : --> will drop at the first model run, later could try Frequency encoding,Binary encoding if reduce cardinality to at least 100.\n* funder\n* installer\n* subvillage\n\nThe rest can be one-hot encoded as the cardinality is lower than 10:\n* public_meeting # later -> Binary?\n* permit # later-> Binary?\n* source_class\n* management_group\n* waterpoint_type_group\n* source_type\n* basin\n\n### 5.2 Scheme_management"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.scheme_management.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## scheme_management\ndef scheme_wrangler(row):\n    if row['scheme_management']=='VWC':\n        return 'vwc'\n    elif row['scheme_management']=='WUG':\n        return 'wug'\n    elif row['scheme_management']=='Water authority':\n        return 'wtr_auth'\n    elif row['scheme_management']=='WUA':\n        return 'wua'\n    elif row['scheme_management']=='Water Board':\n        return 'wtr_brd'\n    elif row['scheme_management']=='Parastatal':\n        return 'parastatal'\n    elif row['scheme_management']=='Private operator':\n        return 'pri_optr'\n    elif row['scheme_management']=='SWC':\n        return 'swc'\n    elif row['scheme_management']=='Company':\n        return 'company'\n    else:\n        return 'other'\ntrain_df['scheme_management'] = train_df.apply(lambda row: scheme_wrangler(row), axis=1)\nX_test['scheme_management'] = X_test.apply(lambda row: scheme_wrangler(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Extraction_type"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.extraction_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## extraction_type\ndef extraction_wrangler(row):\n    if row['extraction_type']=='gravity':\n        return 'gravity'\n    elif row['extraction_type']=='nira/tanira':\n        return 'nira/tanira'\n    elif row['extraction_type']=='submersible':\n        return 'submersible'\n    elif row['extraction_type']=='swn 80':\n        return 'swn_80'\n    elif row['extraction_type']=='mono':\n        return 'mono'\n    elif row['extraction_type']=='india mark ii':\n        return 'india_mark_ii'\n    elif row['extraction_type']=='afridev':\n        return 'afridev'\n    elif row['extraction_type']=='ksb':\n        return 'ksb'\n    elif row['extraction_type']=='windmill':\n        return 'windmill'\n    else:\n        return 'other'\ntrain_df['extraction_type'] = train_df.apply(lambda row: extraction_wrangler(row), axis=1)\nX_test['extraction_type'] = X_test.apply(lambda row: extraction_wrangler(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Funder"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.funder.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## funder\ndef funder_wrangler(row):  \n    '''Keep top 8 values and set the rest to 'other'''\n\n    if row['funder']=='Government Of Tanzania':\n        return 'gov'\n    elif row['funder']=='Danida':\n        return 'danida'\n    elif row['funder']=='Hesawa':\n        return 'hesawa'\n    elif row['funder']=='Rwssp':\n        return 'rwssp'\n    elif row['funder']=='World Bank':\n        return 'world_bank'   \n    elif row['funder']=='Kkkt':\n        return 'kkkt'   \n    elif row['funder']=='World Vision':\n        return 'world_vision'  \n    elif row['funder']=='Unicef':\n        return 'unicef'\n    else:\n        return 'other'\n    \ntrain_df['funder'] = train_df.apply(lambda row: funder_wrangler(row), axis=1)\nX_test['funder'] = X_test.apply(lambda row: funder_wrangler(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Ordinal encoding of categorical data\nSeveral categorical features can be encoded in a specific order that follows from the range of its values. By using ordinal encoding instead of one-hot encoding we will avoid creating numerious additional columns and provide some logic to the model on how to evaluate these features. For example for the quality_group variable, the higher the label, the better the water quality, the more likely a pump is functional.\n\n### 6.1 Quality_group"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.quality_group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"order_dict_quality = {\"good\":3,\"salty\":2,\"milky\":2,\"colored\":2,\"fluoride\":2,\"unknown\":1}\ntrain_df[\"quality_group_code\"] = [order_dict_quality[item] for item in train_df.quality_group]\ndel train_df[\"quality_group\"]\n\nX_test[\"quality_group_code\"] = [order_dict_quality[item] for item in X_test.quality_group]\ndel X_test[\"quality_group\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Quantity_group"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.quantity_group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"order_dict_quantity = {\"enough\":3,\"insufficient\":2,\"dry\":2,\"seasonal\":2,\"unknown\":1}\ntrain_df[\"quantity_group_code\"] = [order_dict_quantity[item] for item in train_df.quantity_group] \ndel train_df[\"quantity_group\"]\n\nX_test[\"quantity_group_code\"] = [order_dict_quantity[item] for item in X_test.quantity_group] \ndel X_test[\"quantity_group\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.3 Payment_type"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.payment_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"order_dict_payment = {\"monthly\":4,\"annually\":4,\"on failure\":3,\"per bucket\":3,\"never pay\":2,\"unknown\":1,\"other\":1}\ntrain_df[\"payment_code\"] = [order_dict_payment[item] for item in train_df.payment_type] \ndel train_df[\"payment_type\"]\n\nX_test[\"payment_code\"] = [order_dict_payment[item] for item in X_test.payment_type] \ndel X_test[\"payment_type\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.4 Public_meeting"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.public_meeting.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"order_dict_pub_meet = {True:2,False:1,\"unknown\":0}\ntrain_df[\"public_meeting_code\"] = [order_dict_pub_meet[item] for item in train_df.public_meeting] \ndel train_df[\"public_meeting\"]\n\nX_test[\"public_meeting_code\"] = [order_dict_pub_meet[item] for item in X_test.public_meeting] \ndel X_test[\"public_meeting\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.5 Permit"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.permit.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"order_dict_permit = {True:2,False:1,\"unknown\":0}\ntrain_df[\"permit_code\"] = [order_dict_pub_meet[item] for item in train_df.permit] \ndel train_df[\"permit\"]\n\nX_test[\"permit_code\"] = [order_dict_pub_meet[item] for item in X_test.permit] \ndel X_test[\"permit\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Feature Engineering\n\nFeature engineering is a powerful way to improve the model. We will create new variables based on the features in the dataset that will better describe the target.\n\n### 7.1 Amount_tsh\nBased on the EDA we have previously defined a threshold to separate functional and non-functional pumps. Let's create a new binary variable that reflects this information."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.loc[train_df['amount_tsh'] < 200000, 'amount_tsh'] = 0\ntrain_df.loc[train_df['amount_tsh'] >= 200000, 'amount_tsh'] = 1\n\nX_test.loc[train_df['amount_tsh'] < 200000, 'amount_tsh'] = 0\nX_test.loc[train_df['amount_tsh'] >= 200000, 'amount_tsh'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.2 Combine longitude and latitude in one feature\nIn this section we tried to create a new variable that would combine both longitude and latitude using a zip method. "},{"metadata":{},"cell_type":"raw","source":"from shapely.geometry import Point # Shapely for converting latitude/longtitude to geometry\nimport geopandas as gpd # To create GeodataFrame\nimport geoplot as gp"},{"metadata":{},"cell_type":"raw","source":"# creating a geometry column \ngeopoint = [Point(xy) for xy in zip(train_df['longitude'], train_df['latitude'])]\n# Coordinate reference system : WGS84\ncrs = {'init': 'epsg:4326'}\n# Creating a Geographic data frame \ngdf = gpd.GeoDataFrame(train_df, crs=crs, geometry=geopoint)"},{"metadata":{},"cell_type":"raw","source":"gdf.plot(marker='*', markersize=0.2, column=\"label\",legend=True)"},{"metadata":{},"cell_type":"markdown","source":"Yellow spots - functional pumps, green - needs repair, purple - non-functional.\n\nAs we see, south-est and almost all central and south west pumps are mostly non-functional. Area around the capital (central-south part) is mostly functional as well as north-est pumps. The rest seems unclear. \n\nHow to use this info for the model? Try to create clusters (for ex around big cities)?"},{"metadata":{},"cell_type":"raw","source":"all_df[\"geopoint\"] = [xy for xy in zip(all_df['longitude'], all_df['latitude'])]"},{"metadata":{},"cell_type":"markdown","source":"I have tested this featureand realized that models are unable to process a tuple. We will use the linear discriminant analysis for these variables."},{"metadata":{},"cell_type":"markdown","source":"### 7.3 Linear Discriminant Analysis (LDA)\n\nLDA helps reducing dimentiality by maximizing the difference between categories. In our highly dimentional model it is a great tool. We will use LDA for the following columns as all of them contain geographical information:\n \n- latitude\n- longitude\n- gps_height\n\nReference: https://zlatankr.github.io/posts/2017/01/23/pump-it-up"},{"metadata":{"trusted":false},"cell_type":"code","source":"LDA_cols = [\"latitude\",\"longitude\",\"gps_height\"]\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\ntrain_df_sc = sc.fit_transform(train_df[LDA_cols])\nX_test_sc = sc.transform(X_test[LDA_cols])\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components=None)\n\ntrain_df_lda = lda.fit_transform(train_df_sc, train_df.label.values.ravel())\nX_test_lda = lda.transform(X_test_sc)\n\ntrain_df = pd.concat((pd.DataFrame(train_df_lda), train_df), axis=1)\nX_test = pd.concat((pd.DataFrame(X_test_lda), X_test), axis=1)\n    \n    \nfor i in LDA_cols:\n    del train_df[i]\n    del X_test[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.rename(columns={0: \"LDA_0\",1:\"LDA_1\"},inplace=True)\nX_test.rename(columns={0: \"LDA_0\",1:\"LDA_1\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.4 Construction year\nWe will turn construction_year into a categorical column with bins containing the following values: '60s', '70s', '80s', '90s, '00s', '10s', 'unknown'."},{"metadata":{"trusted":false},"cell_type":"code","source":"def construction_wrangler(row):\n    if row['construction_year'] >= 1960 and row['construction_year'] < 1970:\n        return '60s'\n    elif row['construction_year'] >= 1970 and row['construction_year'] < 1980:\n        return '70s'\n    elif row['construction_year'] >= 1980 and row['construction_year'] < 1990:\n        return '80s'\n    elif row['construction_year'] >= 1990 and row['construction_year'] < 2000:\n        return '90s'\n    elif row['construction_year'] >= 2000 and row['construction_year'] < 2010:\n        return '00s'\n    elif row['construction_year'] >= 2010:\n        return '10s'\n    else:\n        return 'unknown'\n    \ntrain_df['construction_year'] = train_df.apply(lambda row: construction_wrangler(row), axis=1)\nX_test['construction_year'] = X_test.apply(lambda row: construction_wrangler(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.5 Date_recorded\nWe will calculate the number of days past since the date_recorded of a particular pump till the most recent date of the dataset. The idea being that more recently recorded pumps might be more likely to be functional than non-functional. \n\nLet's first convert the column to type datetime. "},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.date_recorded = pd.to_datetime(train_df.date_recorded)\nX_test.date_recorded = pd.to_datetime(X_test.date_recorded)\n\ntrain_df.date_recorded.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# The most recent data is 2013-12-03. Subtract each date from this point to obtain a \n# 'days_since_recorded' column.\n\ntrain_df['days_since_recorded'] = pd.datetime(2013, 12, 3) - pd.to_datetime(train_df.date_recorded)\ntrain_df['days_since_recorded'] = train_df['days_since_recorded'].astype('timedelta64[D]').astype(int)\n\nX_test['days_since_recorded'] = pd.datetime(2013, 12, 3) - pd.to_datetime(X_test.date_recorded)\nX_test['days_since_recorded'] = X_test['days_since_recorded'].astype('timedelta64[D]').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df['days_since_recorded']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = train_df.drop(\"date_recorded\",axis=1)\nX_test = X_test.drop(\"date_recorded\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. One-hot encoding of categorical features"},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_vars = train_df.select_dtypes(include='object').columns\nprint(cat_vars)\nlen(cat_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_df[cat_vars])).astype(np.int64)\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[cat_vars])).astype(np.int64)\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_df.index\nOH_cols_test.index = X_test.index\n\nOH_cols_train.columns = OH_encoder.get_feature_names(cat_vars)\nOH_cols_test.columns = OH_encoder.get_feature_names(cat_vars)\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train_df.drop(cat_vars, axis=1)\nnum_X_valid = X_test.drop(cat_vars, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_train_df = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_valid, OH_cols_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"OH_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Feature selection\n### 9.1 L1 regularization with logistic regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\nX, y = OH_train_df[OH_train_df.columns.drop(\"label\")], OH_train_df['label']\n\n# Set the regularization parameter C=1\nlogistic = LogisticRegression(solver=\"saga\",C=1, penalty=\"l1\", random_state=7).fit(X, y)\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(X)\nX_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X.index,\n                                 columns=X.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nselected_columns = selected_features.columns[selected_features.var() != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(selected_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"selected_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df_selected_features = OH_train_df[selected_columns].join(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_selected_features = OH_X_test[selected_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.2 Feature importances with Random Forest"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(criterion='gini',min_samples_split=8, n_estimators=1000,\n                           random_state = 7)\nrf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# helper function for creating a feature importance dataframe\ndef imp_df(column_names, importances):\n    df = pd.DataFrame({'feature': column_names,\n                       'feature_importance': importances}) \\\n           .sort_values('feature_importance', ascending = False) \\\n           .reset_index(drop = True)\n    return df\n\n# plotting a feature importance dataframe (horizontal barchart)\ndef var_imp_plot(imp_df, title):\n    imp_df.columns = ['feature', 'feature_importance']\n    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n       .set_title(title, fontsize = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"base_imp = imp_df(X.columns, rf.feature_importances_)\ntop_30_imp = base_imp[0:30]\ntop_30_features = top_30_imp.feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pylab.rcParams[\"figure.figsize\"] = (10,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"var_imp_plot(base_imp, 'Default feature importance (scikit-learn)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df_final_top_imp = OH_train_df[top_30_features].join(y)\nX_test_final_top_imp = OH_X_test[top_30_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df_final_top_imp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df_selected_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Export the final dfs\n\nIn the part *III. Model selection* I have tested different models with these 2 final sets of variables (80 vars and 30 vars). The result seems to be stable across all the models - the set of 80 variables we received after doing L1 regularization with logistic regression scores higher every time. I am thus using the selected_features dfs as the final input."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df_selected_features.to_csv(\"train_df_final.csv\", index=False)\nX_test_selected_features.to_csv(\"X_test_final.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}