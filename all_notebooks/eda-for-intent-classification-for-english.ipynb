{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n <a id=\"1\"></a>\n# <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;\">Importing Libraries</p>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textblob import TextBlob\nimport string\n# for Tokenization\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n# for removing accented and special chracters\nimport unicodedata\n# for stopwords Removal\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\n# for calculating Polarity and Subjectivity\nfrom textblob import TextBlob\nimport plotly.express as px\n#nlp = spacy.load(\"en_core_web_sm\")\n# function for making ngrams\nfrom nltk.util import ngrams \nimport collections\n# for Wordscloud\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import zero_one_loss","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:53.335611Z","iopub.execute_input":"2021-09-13T20:14:53.336017Z","iopub.status.idle":"2021-09-13T20:14:56.41269Z","shell.execute_reply.started":"2021-09-13T20:14:53.33591Z","shell.execute_reply":"2021-09-13T20:14:56.411843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset='../input/intent-classification-enfr/intent_classification.csv'\ndf=pd.read_csv(dataset)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.414263Z","iopub.execute_input":"2021-09-13T20:14:56.41464Z","iopub.status.idle":"2021-09-13T20:14:56.501661Z","shell.execute_reply.started":"2021-09-13T20:14:56.414602Z","shell.execute_reply":"2021-09-13T20:14:56.500626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lets check the head of the dataset","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.503787Z","iopub.execute_input":"2021-09-13T20:14:56.50421Z","iopub.status.idle":"2021-09-13T20:14:56.530762Z","shell.execute_reply.started":"2021-09-13T20:14:56.504172Z","shell.execute_reply":"2021-09-13T20:14:56.529806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.532456Z","iopub.execute_input":"2021-09-13T20:14:56.532817Z","iopub.status.idle":"2021-09-13T20:14:56.539016Z","shell.execute_reply.started":"2021-09-13T20:14:56.532782Z","shell.execute_reply":"2021-09-13T20:14:56.53799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### lets check if the dataset has any Missing Values","metadata":{}},{"cell_type":"code","source":"df.isnull()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.540601Z","iopub.execute_input":"2021-09-13T20:14:56.541142Z","iopub.status.idle":"2021-09-13T20:14:56.562644Z","shell.execute_reply.started":"2021-09-13T20:14:56.541031Z","shell.execute_reply":"2021-09-13T20:14:56.561804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" #### lets check the Descriptive Summary of the Dataset","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.563683Z","iopub.execute_input":"2021-09-13T20:14:56.563936Z","iopub.status.idle":"2021-09-13T20:14:56.636994Z","shell.execute_reply.started":"2021-09-13T20:14:56.563901Z","shell.execute_reply":"2021-09-13T20:14:56.636286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### lets check the summary of Date\n\n","metadata":{}},{"cell_type":"code","source":"df.describe(include = 'object')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.637962Z","iopub.execute_input":"2021-09-13T20:14:56.638345Z","iopub.status.idle":"2021-09-13T20:14:56.692691Z","shell.execute_reply.started":"2021-09-13T20:14:56.638307Z","shell.execute_reply":"2021-09-13T20:14:56.691784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### lets check the Value Counts for Variation ","metadata":{}},{"cell_type":"code","source":"df['text-en'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.694911Z","iopub.execute_input":"2021-09-13T20:14:56.695201Z","iopub.status.idle":"2021-09-13T20:14:56.715223Z","shell.execute_reply.started":"2021-09-13T20:14:56.695176Z","shell.execute_reply":"2021-09-13T20:14:56.714456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets calculate the length of text-en","metadata":{}},{"cell_type":"code","source":"# Lets calculate the length of the Reviews\ndf['length'] = df['text-en'].apply(len)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.716872Z","iopub.execute_input":"2021-09-13T20:14:56.717139Z","iopub.status.idle":"2021-09-13T20:14:56.728517Z","shell.execute_reply.started":"2021-09-13T20:14:56.717113Z","shell.execute_reply":"2021-09-13T20:14:56.727707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Polarity\n\nIt is the expression that determines the sentimental aspect of an opinion. In textual data, the result of sentiment analysis can be determined for each entity in the sentence, document or sentence. The sentiment polarity can be determined as positive, negative and neutral.\n","metadata":{}},{"cell_type":"code","source":"# Lets calculate the Polarity of the Reviews\ndef get_polarity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    pol = textblob.sentiment.polarity\n    return pol\n\n# lets apply the function\ndf['polarity'] = df['text-en'].apply(get_polarity)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:56.729595Z","iopub.execute_input":"2021-09-13T20:14:56.729851Z","iopub.status.idle":"2021-09-13T20:14:59.734716Z","shell.execute_reply.started":"2021-09-13T20:14:56.729826Z","shell.execute_reply":"2021-09-13T20:14:59.733983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Subjectivity\n\nIn natural language, subjectivity refers to expression of opinions, evaluations, feelings, and speculations and thus incorporates sentiment. Subjective text is further classified with sentiment or polarity.","metadata":{}},{"cell_type":"code","source":"def get_subjectivity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    subj = textblob.sentiment.subjectivity\n    return subj\n\n# lets apply the Function\ndf['subjectivity'] = df['text-en'].apply(get_subjectivity)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:14:59.735809Z","iopub.execute_input":"2021-09-13T20:14:59.736177Z","iopub.status.idle":"2021-09-13T20:15:02.694812Z","shell.execute_reply.started":"2021-09-13T20:14:59.736146Z","shell.execute_reply":"2021-09-13T20:15:02.693795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets summarize the Newly Created Features\ndf[['length','polarity','text-en']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:02.696127Z","iopub.execute_input":"2021-09-13T20:15:02.696532Z","iopub.status.idle":"2021-09-13T20:15:02.723237Z","shell.execute_reply.started":"2021-09-13T20:15:02.696492Z","shell.execute_reply":"2021-09-13T20:15:02.722346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets summarize the Newly Created Features\ndf[['length','polarity','subjectivity']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:02.72448Z","iopub.execute_input":"2021-09-13T20:15:02.724821Z","iopub.status.idle":"2021-09-13T20:15:02.749078Z","shell.execute_reply.started":"2021-09-13T20:15:02.72479Z","shell.execute_reply":"2021-09-13T20:15:02.748327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets create a Part of speech Dictionary\npos_dic = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pron' : ['PRP','PRP$','WP','WP$'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n    'adj' :  ['JJ','JJR','JJS'],\n    'adv' : ['RB','RBR','RBS','WRB']\n}\n\n# function to check and get the part of speech tag count of a words in a given sentence\ndef pos_check(x, flag):\n    cnt = 0\n    try:\n        wiki = TextBlob(x)\n        for tup in wiki.tags:\n            ppo = list(tup)[1]\n            if ppo in pos_dic[flag]:\n                cnt += 1\n    except:\n        pass\n    return cnt","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:02.750367Z","iopub.execute_input":"2021-09-13T20:15:02.750964Z","iopub.status.idle":"2021-09-13T20:15:02.75933Z","shell.execute_reply.started":"2021-09-13T20:15:02.750894Z","shell.execute_reply":"2021-09-13T20:15:02.758512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Nouns\n\nA noun is a word that functions as the name of a specific object or set of objects, such as living creatures, places, actions, qualities, states of existence, or ideas. However, noun is not a semantic category, so that it cannot be characterized in terms of its meaning.","metadata":{}},{"cell_type":"code","source":"# lets calculate the count of Nouns in the Text\ndf['noun_count'] = df['text-en'].apply(lambda x: pos_check(x, 'noun'))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:02.760555Z","iopub.execute_input":"2021-09-13T20:15:02.761227Z","iopub.status.idle":"2021-09-13T20:15:15.617238Z","shell.execute_reply.started":"2021-09-13T20:15:02.761184Z","shell.execute_reply":"2021-09-13T20:15:15.616364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Verbs\n\nA verb, from the Latin verbum meaning word, is a word that in syntax conveys an action, an occurrence, or a state of being. In the usual description of English, the basic form, with or without the particle to, is the infinitive. In many languages, verbs are inflected to encode tense, aspect, mood, and voice.","metadata":{}},{"cell_type":"code","source":"df['verb_count'] = df['text-en'].apply(lambda x: pos_check(x, 'verb'))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:15.618535Z","iopub.execute_input":"2021-09-13T20:15:15.618962Z","iopub.status.idle":"2021-09-13T20:15:28.328762Z","shell.execute_reply.started":"2021-09-13T20:15:15.618904Z","shell.execute_reply":"2021-09-13T20:15:28.327979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['noun_count','verb_count']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:28.32998Z","iopub.execute_input":"2021-09-13T20:15:28.33026Z","iopub.status.idle":"2021-09-13T20:15:28.351156Z","shell.execute_reply.started":"2021-09-13T20:15:28.330234Z","shell.execute_reply":"2021-09-13T20:15:28.350217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Here calculate the char_count & word_count & word_density & punctuation_count**","metadata":{}},{"cell_type":"code","source":"# calculating the Character Count in the Reviews\ndf['char_count'] = df['text-en'].apply(len)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:28.352322Z","iopub.execute_input":"2021-09-13T20:15:28.352619Z","iopub.status.idle":"2021-09-13T20:15:28.364912Z","shell.execute_reply.started":"2021-09-13T20:15:28.352593Z","shell.execute_reply":"2021-09-13T20:15:28.363858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating the Word Count\ndf['word_count'] = df['text-en'].apply(lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:28.366168Z","iopub.execute_input":"2021-09-13T20:15:28.36646Z","iopub.status.idle":"2021-09-13T20:15:28.394142Z","shell.execute_reply.started":"2021-09-13T20:15:28.366434Z","shell.execute_reply":"2021-09-13T20:15:28.393334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the Word Density\ndf['word_density'] = df['char_count'] / (df['word_count']+1)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:28.395476Z","iopub.execute_input":"2021-09-13T20:15:28.395954Z","iopub.status.idle":"2021-09-13T20:15:28.40817Z","shell.execute_reply.started":"2021-09-13T20:15:28.395895Z","shell.execute_reply":"2021-09-13T20:15:28.407206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuation = string.punctuation\n\n# Calculating the Punctuation Count\ndf['punctuation_count'] = df['text-en'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) ","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:28.409516Z","iopub.execute_input":"2021-09-13T20:15:28.410012Z","iopub.status.idle":"2021-09-13T20:15:28.468321Z","shell.execute_reply.started":"2021-09-13T20:15:28.409978Z","shell.execute_reply":"2021-09-13T20:15:28.467374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets summarize the Newly Created Features\ndf[['char_count','word_count','word_density','punctuation_count']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:28.469566Z","iopub.execute_input":"2021-09-13T20:15:28.469858Z","iopub.status.idle":"2021-09-13T20:15:28.500067Z","shell.execute_reply.started":"2021-09-13T20:15:28.46983Z","shell.execute_reply":"2021-09-13T20:15:28.499163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adjectives\n\nIn linguistics, an adjective is a word that modifies a noun or noun phrase or describes its referent. Its semantic role is to change information given by the noun. Adjectives are one of the main parts of speech of the English language, although historically they were classed together with nouns.","metadata":{}},{"cell_type":"code","source":"df['adj_count'] = df['text-en'].apply(lambda x: pos_check(x, 'adj'))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:28.505636Z","iopub.execute_input":"2021-09-13T20:15:28.506028Z","iopub.status.idle":"2021-09-13T20:15:41.221188Z","shell.execute_reply.started":"2021-09-13T20:15:28.505995Z","shell.execute_reply":"2021-09-13T20:15:41.220171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adverbs\n\nAn adverb is a word or an expression that modifies a verb, adjective, another adverb, determiner, clause, preposition, or sentence. Adverbs typically express manner, place, time, frequency, degree, level of certainty, etc., answering questions such as how?, in what way?, when?, where?, and to what extent?","metadata":{}},{"cell_type":"code","source":"df['adv_count'] = df['text-en'].apply(lambda x: pos_check(x, 'adv'))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:41.223135Z","iopub.execute_input":"2021-09-13T20:15:41.223401Z","iopub.status.idle":"2021-09-13T20:15:53.980068Z","shell.execute_reply.started":"2021-09-13T20:15:41.223376Z","shell.execute_reply":"2021-09-13T20:15:53.979056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pronouns\n\nA pronoun (I, me, he, she, herself, you, it, that, they, each, few, many, who, whoever, whose, someone, everybody, etc.) is a word that takes the place of a noun. In the sentence Joe saw Jill, and he waved at her, the pronouns he and her take the place of Joe and Jill, respectively.","metadata":{}},{"cell_type":"code","source":"# lets calculate the count of Pronouns in the Text\ndf['pron_count'] = df['text-en'].apply(lambda x: pos_check(x, 'pron'))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:15:53.983058Z","iopub.execute_input":"2021-09-13T20:15:53.983518Z","iopub.status.idle":"2021-09-13T20:16:06.855976Z","shell.execute_reply.started":"2021-09-13T20:15:53.983474Z","shell.execute_reply":"2021-09-13T20:16:06.854893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets summarize the Newly Created features\ndf[['adj_count','adv_count','pron_count']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:06.857305Z","iopub.execute_input":"2021-09-13T20:16:06.857628Z","iopub.status.idle":"2021-09-13T20:16:06.881269Z","shell.execute_reply.started":"2021-09-13T20:16:06.857596Z","shell.execute_reply":"2021-09-13T20:16:06.880544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we can Clear the data","metadata":{}},{"cell_type":"code","source":"# First lets remove Punctuations from the Reviews\ndef punctuation_removal(messy_str):\n    clean_list = [char for char in messy_str if char not in string.punctuation]\n    clean_str = ''.join(clean_list)\n    return clean_str\n\ndf['text-en'] = df['text-en'].apply(punctuation_removal)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:06.882394Z","iopub.execute_input":"2021-09-13T20:16:06.882847Z","iopub.status.idle":"2021-09-13T20:16:06.990003Z","shell.execute_reply.started":"2021-09-13T20:16:06.882814Z","shell.execute_reply":"2021-09-13T20:16:06.98915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def drop_numbers(list_text):\n    list_text_new = []\n    for i in list_text:\n        if not re.search('\\d', i):\n            list_text_new.append(i)\n    return ''.join(list_text_new)\n\ndf['text-en'] = df['text-en'].apply(drop_numbers)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:06.991251Z","iopub.execute_input":"2021-09-13T20:16:06.991768Z","iopub.status.idle":"2021-09-13T20:16:07.665188Z","shell.execute_reply.started":"2021-09-13T20:16:06.991738Z","shell.execute_reply":"2021-09-13T20:16:07.66419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text-en'].head(10).sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:07.667841Z","iopub.execute_input":"2021-09-13T20:16:07.668535Z","iopub.status.idle":"2021-09-13T20:16:07.675261Z","shell.execute_reply.started":"2021-09-13T20:16:07.668492Z","shell.execute_reply":"2021-09-13T20:16:07.674329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_data='''can you walk me through setting up direct deposits\nto my bank of internet savings accounti want to switch to \ndirect depositset up direct deposit\nfor mehow do i go about setting up direct depositi need \nto get my paycheck direct deposited to my chase accountwhat are the steps to \nset up direct deposit to my chase accountif i would like to set up direct \ndeposit how do i do ithow do i direct deposit my checkwhat do i need to set u\np direct depositid like to have my paycheck direct \ndeposited to my chase account'''\nnltk_tokens = nltk.sent_tokenize(sentence_data)\nprint (nltk_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:07.676555Z","iopub.execute_input":"2021-09-13T20:16:07.676882Z","iopub.status.idle":"2021-09-13T20:16:07.685997Z","shell.execute_reply.started":"2021-09-13T20:16:07.676854Z","shell.execute_reply":"2021-09-13T20:16:07.684982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization\n\nIn Python tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language. The various tokenization functions in-built into the nltk module","metadata":{}},{"cell_type":"code","source":"# Words Tokenization\nnltk_tokens = nltk.word_tokenize(sentence_data)\nprint (nltk_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:07.687457Z","iopub.execute_input":"2021-09-13T20:16:07.687766Z","iopub.status.idle":"2021-09-13T20:16:07.697835Z","shell.execute_reply.started":"2021-09-13T20:16:07.68773Z","shell.execute_reply":"2021-09-13T20:16:07.69698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to remove special characters\ndef remove_special_characters(text):\n    pat = r'[^a-zA-z0-9]' \n    return re.sub(pat, ' ', text)\n \n# lets apply this function\ndf['text-en'] = df.apply(lambda x: remove_special_characters(x['text-en']), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:07.699022Z","iopub.execute_input":"2021-09-13T20:16:07.699674Z","iopub.status.idle":"2021-09-13T20:16:07.995389Z","shell.execute_reply.started":"2021-09-13T20:16:07.699633Z","shell.execute_reply":"2021-09-13T20:16:07.994611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text-en'].head(10).sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:07.99637Z","iopub.execute_input":"2021-09-13T20:16:07.996759Z","iopub.status.idle":"2021-09-13T20:16:08.003313Z","shell.execute_reply.started":"2021-09-13T20:16:07.996731Z","shell.execute_reply":"2021-09-13T20:16:08.002246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Accented Characters\n\nAccented characters are important elements which are used to signify emphasis on a particular word during pronunciation or understanding. In some instances, the accent mark also clarifies the meaning of a word, which might be different without the accent. While their use in English is largely limited but there are very good chances that you will come across accented characters/letters in a free text corpus. \n","metadata":{}},{"cell_type":"markdown","source":"### Stopwords\n\nStop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead.","metadata":{}},{"cell_type":"code","source":"# Now lets Remove the Stopwords\n\n# targeting only English Stopwords\nstop = stopwords.words('english')\nstop_words = []\nfrom nltk.tokenize import word_tokenize\n\ntext = '''can you walk me through setting up direct deposits to my bank of\ninternet savings accounti want to switch to direct depositset up direct deposit\nfor mehow do i go about setting up direct depositi need to get my paycheck direct \ndeposited to my chase accountwhat are the steps to set up direct deposit to my chase \naccountif i would like to set up direct deposit how do i do ithow do i direct deposit\nmy checkwhat do i need to set up direct depositid like to have \nmy paycheck direct deposited to my chase account'''\ntext_tokens = word_tokenize(text)\n\ntokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n\nprint(tokens_without_sw)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:08.005121Z","iopub.execute_input":"2021-09-13T20:16:08.005991Z","iopub.status.idle":"2021-09-13T20:16:08.404339Z","shell.execute_reply.started":"2021-09-13T20:16:08.00595Z","shell.execute_reply":"2021-09-13T20:16:08.403496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lemmatization\n\nThough stemming and lemmatization both generate the root form of inflected/desired words, but lemmatization is an advanced form of stemming. Stemming might not result in actual word, whereas lemmatization does conversion properly with the use of vocabulary, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\nBefore using lemmatization, we should be aware that it is considerably slower than stemming, so performance should be kept in mind before choosing stemming or lemmatization.","metadata":{}},{"cell_type":"code","source":"# for lemmatization\nimport spacy\nnlp = spacy.load('en_core_web_sm',parse=True,tag=True, entity=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:08.405427Z","iopub.execute_input":"2021-09-13T20:16:08.405688Z","iopub.status.idle":"2021-09-13T20:16:10.286323Z","shell.execute_reply.started":"2021-09-13T20:16:08.405664Z","shell.execute_reply":"2021-09-13T20:16:10.285324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for stemming\ndef get_stem(text):\n    stemmer = nltk.porter.PorterStemmer()\n    text = ' '.join([stemmer.stem(word) for word in text.split()])\n    return text\n\n# call function\nget_stem(\"can you walk me through setting up direct deposits to my bank of internet savings accounti want to switch to direct depositset up direct deposit for mehow do i go about setting up direct depositi need to get my paycheck direct deposited to my chase accountwhat are the steps to set up direct deposit to my chase accountif i would like to set up direct deposit how do i do ithow do i direct deposit my checkwhat do i need to set up direct depositid like to have my paycheck direct deposited to my chase account \")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:10.287459Z","iopub.execute_input":"2021-09-13T20:16:10.287737Z","iopub.status.idle":"2021-09-13T20:16:10.299915Z","shell.execute_reply.started":"2021-09-13T20:16:10.287709Z","shell.execute_reply":"2021-09-13T20:16:10.298698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to remove special characters\ndef get_lem(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n\n# call function\nget_lem(\"can you walk me through set up direct deposit to my bank of internet save accounti want to switch to direct depositset up direct deposit for mehow do i go about set up direct depos need to get my paycheck direct deposit to my chase accountwhat are the step to set up direct deposit to my chase accountif i would like to set up direct deposit how do i do ithow do i direct deposit my checkwhat do i need to set up direct depositid like to have my paycheck direct deposit to my chase account \")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:10.301431Z","iopub.execute_input":"2021-09-13T20:16:10.301905Z","iopub.status.idle":"2021-09-13T20:16:10.346797Z","shell.execute_reply.started":"2021-09-13T20:16:10.301859Z","shell.execute_reply":"2021-09-13T20:16:10.345947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bag of Words\n\nThe bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.","metadata":{}},{"cell_type":"code","source":"## Cleaning the Data\n\ncorpus = []\n\nfor i in range(0, 3150):\n    review = re.sub('[^a-zA-Z]', ' ', df['text-en'][i])  ## Removing all Unecessary items\n    review = review.lower()                                         ## Converting into Lower Case\n    review = review.split()\n    ps = PorterStemmer()                                            ## Stemming\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]  ## Removing Stopwords\n    review = ' '.join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:10.347885Z","iopub.execute_input":"2021-09-13T20:16:10.348179Z","iopub.status.idle":"2021-09-13T20:16:14.56977Z","shell.execute_reply.started":"2021-09-13T20:16:10.348153Z","shell.execute_reply":"2021-09-13T20:16:14.56883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Polarity\n\nIt is the expression that determines the sentimental aspect of an opinion. In textual data, the result of sentiment analysis can be determined for each entity in the sentence, document or sentence. The sentiment polarity can be determined as positive, negative and neutral.","metadata":{}},{"cell_type":"code","source":"# Lets calculate the Polarity of the Reviews\ndef get_polarity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    pol = textblob.sentiment.polarity\n    return pol\n\n# lets apply the function\ndf['polarity'] = df['text-en'].apply(get_polarity)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:14.571051Z","iopub.execute_input":"2021-09-13T20:16:14.571344Z","iopub.status.idle":"2021-09-13T20:16:17.459629Z","shell.execute_reply.started":"2021-09-13T20:16:14.571309Z","shell.execute_reply":"2021-09-13T20:16:17.458558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Subjectivity\n\nIn natural language, subjectivity refers to expression of opinions, evaluations, feelings, and speculations and thus incorporates sentiment. Subjective text is further classified with sentiment or polarity.","metadata":{}},{"cell_type":"code","source":" #Lets calculate the Subjectvity of the Reviews\ndef get_subjectivity(text):\n    textblob = TextBlob(str(text.encode('utf-8')))\n    subj = textblob.sentiment.subjectivity\n    return subj\n\n# lets apply the Function\ndf['subjectivity'] = df['text-en'].apply(get_subjectivity)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:17.46096Z","iopub.execute_input":"2021-09-13T20:16:17.46127Z","iopub.status.idle":"2021-09-13T20:16:20.332314Z","shell.execute_reply.started":"2021-09-13T20:16:17.46124Z","shell.execute_reply":"2021-09-13T20:16:20.331297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Visualizing Polarity and Subjectivity\n\nplt.rcParams['figure.figsize'] = (10, 4)\n\nplt.subplot(1, 2, 1)\nsns.distplot(df['polarity'])\n\nplt.subplot(1, 2, 2)\nsns.distplot(df['subjectivity'])\n\nplt.suptitle('Distribution of Polarity and Subjectivity')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:20.33391Z","iopub.execute_input":"2021-09-13T20:16:20.334322Z","iopub.status.idle":"2021-09-13T20:16:21.373377Z","shell.execute_reply.started":"2021-09-13T20:16:20.334281Z","shell.execute_reply":"2021-09-13T20:16:21.372464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check relation between Polarity and Subjectivity\n\nsns.scatterplot(df['polarity'], df['subjectivity'])\nplt.title('Polarity vs Subjectivity')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:21.374696Z","iopub.execute_input":"2021-09-13T20:16:21.374968Z","iopub.status.idle":"2021-09-13T20:16:21.619283Z","shell.execute_reply.started":"2021-09-13T20:16:21.374942Z","shell.execute_reply":"2021-09-13T20:16:21.618517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(data=df.drop(['subjectivity'], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:21.620433Z","iopub.execute_input":"2021-09-13T20:16:21.620714Z","iopub.status.idle":"2021-09-13T20:16:31.830606Z","shell.execute_reply.started":"2021-09-13T20:16:21.620689Z","shell.execute_reply":"2021-09-13T20:16:31.829558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(data=df.drop(['polarity'], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:31.831999Z","iopub.execute_input":"2021-09-13T20:16:31.832538Z","iopub.status.idle":"2021-09-13T20:16:42.274658Z","shell.execute_reply.started":"2021-09-13T20:16:31.832494Z","shell.execute_reply":"2021-09-13T20:16:42.273746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df[\"polarity\"], bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:42.275847Z","iopub.execute_input":"2021-09-13T20:16:42.276167Z","iopub.status.idle":"2021-09-13T20:16:42.514897Z","shell.execute_reply.started":"2021-09-13T20:16:42.276139Z","shell.execute_reply":"2021-09-13T20:16:42.513994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df[\"subjectivity\"], bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:42.516129Z","iopub.execute_input":"2021-09-13T20:16:42.516419Z","iopub.status.idle":"2021-09-13T20:16:42.739368Z","shell.execute_reply.started":"2021-09-13T20:16:42.516383Z","shell.execute_reply":"2021-09-13T20:16:42.737851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(df['subjectivity'], color = 'b')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:42.740383Z","iopub.execute_input":"2021-09-13T20:16:42.740638Z","iopub.status.idle":"2021-09-13T20:16:42.916672Z","shell.execute_reply.started":"2021-09-13T20:16:42.740614Z","shell.execute_reply":"2021-09-13T20:16:42.915844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the Most Frequent Words","metadata":{}},{"cell_type":"markdown","source":"## lets plot the Wordscloud","metadata":{}},{"cell_type":"code","source":"\ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(df['text-en'])\nsum_words = words.sum(axis=0)\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n\nwordcloud = WordCloud(background_color = 'lightcyan', width = 2000, height = 2000).generate_from_frequencies(dict(words_freq))\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.title(\"Vocabulary from Reviews\", fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:42.917813Z","iopub.execute_input":"2021-09-13T20:16:42.918097Z","iopub.status.idle":"2021-09-13T20:16:51.070545Z","shell.execute_reply.started":"2021-09-13T20:16:42.91807Z","shell.execute_reply":"2021-09-13T20:16:51.069643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make feature_extraction","metadata":{}},{"cell_type":"code","source":"# creating bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\ncv = CountVectorizer(max_features=2)\ndf=cv.fit_transform(corpus)\n#X = cv.fit_transform(corpus).toarray()\n#X=df['']\n#y = df.iloc[:, -1].values\nX = df[:,:-1].toarray()\ny = df[:,:-1].toarray()\n\nprint(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:24:12.930235Z","iopub.execute_input":"2021-09-13T20:24:12.930848Z","iopub.status.idle":"2021-09-13T20:24:12.970811Z","shell.execute_reply.started":"2021-09-13T20:24:12.930814Z","shell.execute_reply":"2021-09-13T20:24:12.970026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.280898Z","iopub.status.idle":"2021-09-13T20:16:51.281309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.282641Z","iopub.status.idle":"2021-09-13T20:16:51.283295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <a id=\"1\"></a>\n <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;\">we can split the data and using LogisticRegrission</p>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.284578Z","iopub.status.idle":"2021-09-13T20:16:51.28524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.286634Z","iopub.status.idle":"2021-09-13T20:16:51.287329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LogisticRegressionModel = LogisticRegression(penalty='l2',solver='sag',C=1.0,random_state=33)\nLogisticRegressionModel.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.288649Z","iopub.status.idle":"2021-09-13T20:16:51.289296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculating Details\nprint('LogisticRegressionModel Train Score is : ' , LogisticRegressionModel.score(X_train, y_train))\nprint('LogisticRegressionModel Test Score is : ' , LogisticRegressionModel.score(X_test, y_test))\nprint('LogisticRegressionModel Classes are : ' , LogisticRegressionModel.classes_)\nprint('LogisticRegressionModel No. of iteratios is : ' , LogisticRegressionModel.n_iter_)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.290735Z","iopub.status.idle":"2021-09-13T20:16:51.29141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculating Prediction\ny_pred = LogisticRegressionModel.predict(X_test)\ny_pred_prob = LogisticRegressionModel.predict_proba(X_test)\nprint('Predicted Value for LogisticRegressionModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for LogisticRegressionModel is : ' , y_pred_prob[:10])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.292658Z","iopub.status.idle":"2021-09-13T20:16:51.293357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculating Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.294692Z","iopub.status.idle":"2021-09-13T20:16:51.295339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drawing confusion matrix\nsns.heatmap(CM)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.296481Z","iopub.status.idle":"2021-09-13T20:16:51.297164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculating Accuracy Score  : ((TP + TN) / float(TP + TN + FP + FN))\nAccScore = accuracy_score(y_test, y_pred, normalize=True)\nprint('Accuracy Score is : ', AccScore)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T20:16:51.298258Z","iopub.status.idle":"2021-09-13T20:16:51.298652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n <a id=\"1\"></a>\n# <p style=\"background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;\">In the end, we see that we got the best score</p>","metadata":{}}]}