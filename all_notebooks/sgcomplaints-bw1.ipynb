{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm  as lgb\n\nimport os\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a565262cb63ea145fab9d622e3a1ab18c407555"},"cell_type":"code","source":"train['set'] = 'train'\ntest['set'] = 'test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c66b018e522ef93071ae6680a5ff028d2bd6610"},"cell_type":"code","source":"traintest = pd.concat([train,test])\ntraintest['texts'] = traintest['Complaint-reason'] + ' ' + traintest['Consumer-complaint-summary']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"717b97a1d2155ecca8e5e05d15a05e9a9e13ba43"},"cell_type":"code","source":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nstop_words = set(stopwords.words(['english','french','spanish']))\ndef convert_lower(x):\n    return x.lower().strip()\ndef remove_numbers(x):\n    x = re.sub(r'\\d+[$]','##d',x)\n    return re.sub(r'\\d+|[x]+|[/]+','',x )\ndef remove_punctuation(x):\n    translator = str.maketrans('', '', string.punctuation)\n    return x.translate(translator)\ndef stop_word_removal(x):\n    tokens = word_tokenize(x)\n    result = [i for i in tokens if not i in stop_words]\n    return ' '.join(result)\n\ndef stem_words(x):\n    ans = []\n    stemmer= PorterStemmer()\n    input_str=word_tokenize(x)\n    for word in input_str:\n        ans.append(stemmer.stem(word))\n    return ' '.join(ans)\ndef preprocess_text(x):\n    x = convert_lower(x)\n    x = remove_numbers(x)\n    x = remove_punctuation(x)\n    x = stop_word_removal(x)\n    x = stem_words(x)\n    return x\n# preprocess_text(traintest['texts'].iloc[2243])\n# from nltk.tokenize import word_tokenize\n# tokens = word_tokenize(input_str)\n# result = [i for i in tokens if not i in stop_words]\ntraintest['texts'] = traintest['texts'].apply(lambda x: preprocess_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da3d6ec17df5d2e9d26d388e54c0575afbc4fe71","scrolled":false},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nSTOPWORDS = stop_words = set(stopwords.words(['english','french','spanish']))\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=400, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(traintest[\"texts\"], title=\"Word Cloud of Complaint-Summary\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"babad1c644a0adff50d84d87b0a79bf72633e3fb"},"cell_type":"code","source":"train['Complaint-Status'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2132714075fd2903d49c41434f4337caf4bca09f"},"cell_type":"code","source":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nSTOPWORDS  = set(stopwords.words(['english','french','spanish']))\ntrain0_df = traintest[train['Complaint-Status']=='Closed with explanation']\ntrain1_df = traintest[train['Complaint-Status']=='Closed with non-monetary relief']\ntrain2_df = traintest[train['Complaint-Status']=='Closed']\ntrain3_df = traintest[train['Complaint-Status']=='Closed with monetary relief']\ntrain4_df = traintest[train['Complaint-Status']=='Untimely response']\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from target ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"texts\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"texts\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"texts\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n#class 3\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"texts\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n#class 4\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"texts\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of closed with explanation\", \n                                          \"Frequent words of closed with non-monatory relief\",\n                                          \"Frequent words of closed\",\"Frequent Words of closed with monetory relief\",\n                                          \"Untimely response\", \"\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\nfig.append_trace(trace4, 3, 1)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b90499c5d139637216f11ac6e58943c8f8db079"},"cell_type":"code","source":"## Get the bar chart from target ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"texts\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"texts\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"texts\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n#class 3\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"texts\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n#class 4\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"texts\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent bigram words of closed with explanation\", \n                                          \"Frequent bigram words of closed with non-monatory relief\",\n                                          \"Frequentbigram words of closed\",\"Frequent bigrams Words of closed with monetory relief\",\n                                          \"bigrams words of Untimely response\", \"\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\nfig.append_trace(trace4, 3, 1)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"73be3e587198f6cc5c6a13325d47f78cd0f9873e"},"cell_type":"code","source":"## Get the bar chart from target ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"texts\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"texts\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"texts\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n#class 3\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"texts\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n#class 4\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"texts\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=5, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent bigram words of closed with explanation\", \n                                          \"Frequent bigram words of closed with non-monatory relief\",\n                                          \"Frequentbigram words of closed\",\"Frequent bigrams Words of closed with monetory relief\",\n                                          \"bigrams words of Untimely response\", \"\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig.append_trace(trace3, 4, 1)\nfig.append_trace(trace4, 5, 1)\nfig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc94310bbdbf281fe37db567564d886b8232874e"},"cell_type":"code","source":"traintest.to_csv('bowTFIDF.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb79c23e43e148bc7b17668e26a3ede77e8adfa6"},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"038e3a4730b865a23a2db881a856661ef5cc0ac0"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"054564419cd24364a37561fad856eef899e5ff81"},"cell_type":"code","source":"TfidfVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe449404cb83bc1b5cff9cb52ee065477ba772dc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"821a9a738489267056f0f22ca2378c12fffbcc30"},"cell_type":"code","source":"\n# import catboost as cb\n# from catboost import Pool\n\n# cb_clf=cb.CatBoostClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ebc0731e7e0bdd73b02e0191e266978cafaf351"},"cell_type":"code","source":"# X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba51d47dfe4771b07e0a53f4af2f53652051a98d","scrolled":true},"cell_type":"code","source":"# word_vectorizer = TfidfVectorizer(\n#     sublinear_tf=True,\n#     strip_accents='unicode',\n#     analyzer='word',\n#     token_pattern=r'\\w{1,}',\n#     stop_words='english',\n#     ngram_range=(1, 1),\n#     max_features=10000)\n# word_vectorizer.fit(traintest['texts'])\n# traintest_word_features = word_vectorizer.transform(traintest['texts'])\n# # test_word_features = word_vectorizer.transform(test_text)\n\n# char_vectorizer = TfidfVectorizer(\n#     sublinear_tf=True,\n#     strip_accents='unicode',\n#     analyzer='char',\n#     stop_words='english',\n#     ngram_range=(2, 6),\n#     max_features=20000)\n\n# char_vectorizer.fit(traintest['texts'])\n# traintest_char_features = char_vectorizer.transform(traintest['texts'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83caa40d2c6599899daf358763b591f955c2f668"},"cell_type":"code","source":"\n# vectorizer = TfidfVectorizer(ngram_range=(2,3),min_df = 5,max_df= 10000)\nvectorizer = TfidfVectorizer(ngram_range=(1,3),\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\nX = vectorizer.fit_transform(traintest['texts'])\ntraintestc = vectorizer.transform(traintest['texts'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05fb23bcb62ddbf2132fc1031dc00e38d0c43df9"},"cell_type":"code","source":"# from scipy import sparse\n# traintestc = sparse.hstack([traintest_char_features, traintest_word_features]).tocsr()\n# train_tfidf = traintestc[:43266]\n# test_tfidf = traintestc[43266:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c51867d0f30a4abc6dd64cd822d7b3c6fb43f74d"},"cell_type":"code","source":"from sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3af9e09f8f4774e128319ec67037963e26bfe0fe"},"cell_type":"code","source":"traintestc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e11509a4e6b90633953e0923876687d57dbabed"},"cell_type":"code","source":"# features_chi2 = chi2(train_tfidf,train['Complaint-Status'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02ecdfd9fda47bbad3e32c43ee1ce54b3c08a539"},"cell_type":"code","source":"# features_ind = np.argsort(features_chi2[0])\n# feature_names = np.array(vectorizer.get_feature_names())[features_ind]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1429565b2e68ac950f6618c50971760a2edc8f3d"},"cell_type":"code","source":"# selected = feature_names[:10000]\n# traintestc=X[:,features_ind[:390]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84c550cbda911266b43bb3d2cf76b147815503fe"},"cell_type":"code","source":"# N = 3\n# for Product, category_id in sorted(category_to_id.items()):\n#   features_chi2 = chi2(features, labels == category_id)\n#   indices = np.argsort(features_chi2[0])\n#   feature_names = np.array(tfidf.get_feature_names())[indices]\n#   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n#   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n#   print(\"# '{}':\".format(Product))\n#   print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n#   print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ae11b94bdcd17a1e86a9350b052fbc4b65f9788"},"cell_type":"code","source":"gp = train[['Complaint-ID','Complaint-Status']].groupby('Complaint-Status')\ngp['Complaint-Status'].apply(lambda x: x.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7da1a68d7ef2bd01dd71c832bcedd8a4872c7673"},"cell_type":"code","source":"for ttype in  train['Transaction-Type'].unique():\n    tempdf = train[train['Transaction-Type']==ttype][['Transaction-Type','Complaint-Status']]\n    tempdf = tempdf.groupby('Complaint-Status')['Complaint-Status'].apply(lambda x : x.count())\n    plt.figure(figsize=(15,5))\n    sns.barplot(tempdf.index, tempdf.values, alpha=0.8)\n    plt.title(ttype+' transaction with target')\n    plt.show()\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62a3873a39d00e79f0aa7f2e7a82dc68aea49d34"},"cell_type":"code","source":"train['Date-received']=pd.to_datetime(train['Date-received'])\ntrain['Date-sent-to-company'] = pd.to_datetime(train['Date-sent-to-company'])\ntest['Date-received']=pd.to_datetime(test['Date-received'])\ntest['Date-sent-to-company'] = pd.to_datetime(test['Date-sent-to-company'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0414fdcc6ae6f981daa05bc8e1e52e9ab12f31d8"},"cell_type":"code","source":"train['Diff_days'] = train['Date-sent-to-company'] - train['Date-received']\ntest['Diff_days'] = test['Date-sent-to-company'] - test['Date-received']\ntrain['Diff_days'] = train.Diff_days.apply(lambda x: x.days)\ntest['Diff_days'] = test.Diff_days.apply(lambda x: x.days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b116df494332b1dcce540feee4ca7eea6a86f158"},"cell_type":"code","source":"train.groupby('Complaint-Status')['Diff_days'].apply(lambda x: x.sum()/x.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dfda7261c84c05b4d1203778b32f935677b7001"},"cell_type":"code","source":"train['month'] = train['Date-received'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1677a2626a4e7985c22bed359eaa85b644351d2d"},"cell_type":"code","source":"for mnth in train.month.unique():\n    tdf = train[train.month==mnth][['Complaint-Status','month']]\n    tdf = tdf.groupby('Complaint-Status')['Complaint-Status'].apply(lambda x : x.count())\n    print(tdf.head())\n    plt.figure(figsize=(12,5))\n    sns.barplot(tdf.index,tdf.values)\n    plt.title('month '+str(mnth))\n    plt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13d69f91f8623413462378a90573665dcad41e5d","scrolled":true},"cell_type":"code","source":"train.groupby('month')['Complaint-ID'].apply(lambda x: x.count())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51bf6909af27eabc267e606edc16d56fed28fb2c"},"cell_type":"markdown","source":"looks like month is not a useful feature"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"0868ffbe69083b62d074500d984f9c356caae0a7"},"cell_type":"code","source":"for cstatus in train['Complaint-Status'].unique():\n    tdf = train[train['Complaint-Status']==cstatus][['Complaint-Status','Consumer-disputes']]\n    tdf = tdf.groupby(['Consumer-disputes'])['Consumer-disputes'].apply(lambda x: x.count())\n    print(tdf.head())\n    \n    sns.barplot(tdf.index,tdf.values)\n    plt.title('target '+ cstatus)\n    plt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8a41338fe5aec22cf937b6ab7965fe8e71e9427"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder as le\nfrom sklearn.naive_bayes import MultinomialNB\n# clf = MultinomialNB().fit(train_tfidf, twenty_train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8364134189c92d6b78fd75df759495170b00a941"},"cell_type":"code","source":"le1 = le()\nle2 = le()\nle3 = le()\nle4 = le()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"08f01da13bafd34d13ba6eb8abea0c727f0cf627"},"cell_type":"code","source":"print('Null counts')\nfor col in train.columns:\n    t = pd.isnull(train[col]).sum()\n    print(col,' : ',t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2e878a3aa40a68c69f7de0bd347723584f4dc6b"},"cell_type":"code","source":"train['Company-response'].fillna(value='New CAT',inplace =True)\ntrain['Consumer-disputes'].fillna(value = \"Not Known\",inplace =True)\nX = train.copy(deep=True)\ntest['Company-response'].fillna(value='New CAT',inplace =True)\ntest['Consumer-disputes'].fillna(value = \"Not Known\",inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"262b9c011dea8983c0419675ef2f432c8226b143"},"cell_type":"code","source":"train['Transaction-Type'] = pd.Series(le1.fit_transform(train['Transaction-Type']))\ntrain['Complaint-reason'] = pd.Series(le2.fit_transform(train['Complaint-reason']))\ntrain['Company-response'] = pd.Series(le3.fit_transform(train['Company-response']))\ntrain['Consumer-disputes'] = pd.Series(le3.fit_transform(train['Consumer-disputes']))\ntrain['Complaint-Status'] = pd.Series(le4.fit_transform(train['Complaint-Status']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49308448dbb536f3adfb623b3847d7cf4234e713"},"cell_type":"code","source":"test['Transaction-Type'] = pd.Series(le1.fit_transform(test['Transaction-Type']))\ntest['Complaint-reason'] = pd.Series(le2.fit_transform(test['Complaint-reason']))\ntest['Company-response'] = pd.Series(le3.fit_transform(test['Company-response']))\ntest['Consumer-disputes'] = pd.Series(le3.fit_transform(test['Consumer-disputes']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ae0226d530fb7212c923dd2c915990ca3235f66"},"cell_type":"code","source":"use_col = ['Transaction-Type','Company-response','Consumer-disputes', 'Diff_days']\ntarget = 'Complaint-Status'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e3710a16ccfc2257e35d09c03cc7588e37aff3f"},"cell_type":"code","source":"import scipy\n# trainAll = scipy.sparse.hstack([train_tfidf, train[use_col]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edcd1be96336eef203b0089d3b93eaa35d831952"},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_selection import chi2,SelectFpr, SelectFromModel,SelectKBest,f_classif\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2af031ce54c1f502d19512f245b1dddb9603e7d"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nss = MinMaxScaler()\nss.fit(train.Diff_days.values.reshape((-1,1)))\ntrain['Diff_days'] = pd.DataFrame(ss.transform(train.Diff_days.values.reshape((-1,1))))\ntest['Diff_days'] = pd.DataFrame(ss.transform(test.Diff_days.values.reshape((-1,1))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43f203d0a17dcba2088790a3e8750e0ad565cdfc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dd3aa073c73edd606bf8046d4742c9c6242b003"},"cell_type":"code","source":"traincpy = train[use_col].copy()\ntraincpy = pd.concat([traincpy,pd.get_dummies(traincpy['Transaction-Type'],prefix = 'TT')],axis=1)\ntraincpy = pd.concat([traincpy,pd.get_dummies(traincpy['Consumer-disputes'],prefix= 'Consumer_Dispute')],axis = 1)\ntraincpy = pd.concat([traincpy,pd.get_dummies(traincpy['Company-response'],prefix= 'company_response')],axis = 1)\ntraincpy.drop(columns=['Transaction-Type','Consumer-disputes','Company-response'],axis = 1,inplace=True)\ntraincpy.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac200e69f7242ee5e8c37af4a2334d7d457e4895"},"cell_type":"code","source":"testcpy = test[use_col].copy()\ntestcpy = pd.concat([testcpy,pd.get_dummies(testcpy['Transaction-Type'],prefix = 'TT')],axis=1)\ntestcpy = pd.concat([testcpy,pd.get_dummies(testcpy['Consumer-disputes'],prefix= 'Consumer_Dispute')],axis = 1)\ntestcpy = pd.concat([testcpy,pd.get_dummies(testcpy['Company-response'],prefix= 'company_response')],axis = 1)\ntestcpy.drop(columns=['Transaction-Type','Consumer-disputes','Company-response'],axis = 1,inplace=True)\ntestcpy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fd813928ea3d292c77485d6bf4a7111389af2da"},"cell_type":"code","source":"from scipy import sparse\ntrain_tfidf = traintestc[:43266]\ntest_tfidf = traintestc[43266:]\n\ntrain_tfidf = sparse.hstack([train_tfidf,sparse.csr_matrix(traincpy)]).tocsr()\ntest_tfidf = sparse.hstack([test_tfidf,sparse.csr_matrix(testcpy)]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c86ba25055e5279a2c6a994ca940ce57efd54e6f"},"cell_type":"code","source":"print(train_tfidf.shape,test_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86a2cb841170e7430a97173b21b815b4e7a51528"},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn import model_selection\nimport xgboost as xgb\n\n# >>> iris = datasets.load_iris()\n# >>> X, y = iris.data, iris.target\nfrom sklearn.metrics import f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.feature_extraction.text import TfidfTransformer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"829864200b2f9a2763abe118da23a4b46a7ef9e1"},"cell_type":"code","source":"\ndef f1_eval(y_pred, dtrain):\n\n    y_true = dtrain.get_label()\n#     print('y_true',y_pred)\n    err = 1-f1_score(y_true,y_pred,average='weighted')\n    print( 'F1 score : {}'.format(f1_score(y_true,y_pred,average='weighted')))\n    return 'f1_err', err\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1311d0e76e4e4bc3a1d58e20097f030f7aa2a506"},"cell_type":"code","source":"# model = xgb.XGBClassifier(objective='multi:softmax',num_class= 5,eval_metric = f1_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"685762116a71f5b97223595c761a5e563d63e858"},"cell_type":"code","source":"# model = xgb.XGBClassifie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31f35d15474b0f8f644d1d14593dcfa77a86df86"},"cell_type":"code","source":"model = OneVsRestClassifier(LinearSVC(verbose=1,class_weight='balanced',C = 5.0 ,random_state=0,max_iter=4000))\n# model = LogisticRegression(C=0.1, solver='sag',multi_class='multinomial')\n# # model2 = KNeighborsClassifier(n_neighbors=20)\n# # model2 = RandomForestClassifier()\n\nfs = SelectFromModel(model)\n# # fs = SelectKBest(f_classif,k = 200000)\n# # fs = SelectFpr()\ntrain_tfidf = fs.fit_transform(train_tfidf,train['Complaint-Status'])\ntest_tfidf = fs.transform(test_tfidf)\n\n                            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7daca5f6846ed7fbe61a856b19f11b95abe3c868"},"cell_type":"code","source":"# # from sklearn.feature_selection import RFE\n# model = OneVsrestClassifier(LinearSVC(C = 5.0 , random_state=0))\n# model2 = MultinomialNB()\n# # selector = RFE(model, 390, step=20000)\n# # selector = selector.fit(train_tfidf, train['Complaint-Status'].values)\n# from sklearn.pipeline import Pipeline\n# text_clf = Pipeline([('tfidf', vectorizer),('clf', model)],)\n# xgb.train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81749f6527c1229782e98e614d0525f7b8e2bffa","scrolled":true},"cell_type":"code","source":"# train_y = train['Complaint-Status'].values\n# params = {'max_depth': 2,\"min_child_weight\" : 30,'objective' :'multi:softmax','num_class' :5,'silent': 1 ,'gamma' : 10,'eval_metric':'auc','early_stopping_rounds':100}\n# model = xgb.train(params,train_tfidf,1000)\n# # #     model = KNeighborsClassifier(n_neighbors=20) \n# # #     model = RandomForestClassifier()\n# print('training_started')\n# model.fit(train_X, train_y)\n# #     print(model.predict(test_X))\n# pred_test_y = model.predict(train_tfidf)\n# pred_test_y2 = model.predict(test_tfidf)\ntrain_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d853af978992152d7343d85af3e9b24739c3473b"},"cell_type":"code","source":"train_y = train['Complaint-Status'].values\n# train_tfidf = train_tfidf.toarray()\n# test_tfidf = test_tfidf.toarray()\n# test_tfidf = traintestc[:43266,features_ind[:232000]]\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n#     model = linear_model.LogisticRegression(C=5., solver='sag')\n#     model = OneVsOneClassifier(LinearSVC(C = 1.0 , random_state=0))\n#     model = LogisticRegression(C=0.1, solver='sag',multi_class='multinomial')\n    model = OneVsRestClassifier(LinearSVC(verbose=1,class_weight='balanced',C = 5.0 ,random_state=0))\n#     params = {'eta': 0.03, 'max_depth': 6,\"min_child_weight\" : 30,'objective' :'multi:softmax','num_class' :5,'silent': 1 ,'gamma' : 10}\n#     train_X = xgb.DMatrix(train_X,label= train_y)\n#     test_X = xgb.DMatrix(test_X,label=test_y)\n#     model = KNeighborsClassifier(n_neighbors=20) \n#     model = RandomForestClassifier()\n#     test_X2 = xgb.DMatrix(test_X2)\n    print('training_started')\n#     evallist  = [(test_X, 'eval')]\n#     model = xgb.train(params,train_X,2000,evallist,feval=f1_eval)\n    model.fit(train_X,train_y)\n#     print(model.predict(test_X))\n    pred_test_y = model.predict(test_X)\n    pred_test_y2 = model.predict(test_X2)\n    return pred_test_y, pred_test_y2, model\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test = 0\npred_train = pd.Series(np.zeros([train.shape[0]]))\npred_full_test  = pd.Series(np.zeros([test.shape[0]]))\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pd.concat([pred_full_test,pd.Series(pred_test_y)],axis=1)\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(f1_score(val_y, pred_val_y,average='weighted'))\n    print(cv_scores[-1])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59ab0dd455af512c64857a6224e30db195a33442"},"cell_type":"code","source":"test['Complaint-Status'] = pd.Series(le4.inverse_transform(pred_full_test[1].apply(lambda x: int(x))))\ntest[['Complaint-ID','Complaint-Status']].to_csv('tfidf2metakbestc01.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c4b7af562124c2751bf8443f072f30ed5890bee7"},"cell_type":"code","source":"train_tfidf = traintestc[:43266]\ntest_tfidf = traintestc[43266:]\n\ntrain_tfidf = sparse.hstack([train_tfidf,sparse.csr_matrix(traincpy)]).tocsr()\ntest_tfidf = sparse.hstack([test_tfidf,sparse.csr_matrix(testcpy)]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93cd8d025978ae52b50f2aeee44162b3fea4ec04"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63244c0df66f45c5bafa01e4c4d4895437a5c5c9"},"cell_type":"code","source":"# # model = LogisticRegression(C=5.0, solver='sag',multi_class='multinomial')\n# fs = SelectKBest(f_classif,k = 20000)\n\n# train_tfidf = fs.fit_transform(train_tfidf,train['Complaint-Status'])\n# test_tfidf = fs.transform(test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c632d6cfba0c6061425e523837ce55bef4e00026"},"cell_type":"code","source":"def runModel(train_X, train_y, test_X, test_y, test_X2):\n#     model = linear_model.LogisticRegression(C=5., solver='sag')\n#     model = OneVsOneClassifier(LinearSVC(C = 1.0 , random_state=0))\n    model = LogisticRegression(C=1.0, solver='sag',multi_class='multinomial')\n#     model = OneVsRestClassifier(LinearSVC(verbose=1,class_weight='balanced',C = 5.0 ,random_state=0))\n#     params = {'eta': 0.03, 'max_depth': 6,\"min_child_weight\" : 30,'objective' :'multi:softmax','num_class' :5,'silent': 1 ,'gamma' : 10}\n#     train_X = xgb.DMatrix(train_X,label= train_y)\n#     test_X = xgb.DMatrix(test_X,label=test_y)\n#     model = KNeighborsClassifier(n_neighbors=20) \n#     model = RandomForestClassifier()\n#     test_X2 = xgb.DMatrix(test_X2)\n    print('training_started')\n#     evallist  = [(test_X, 'eval')]\n#     model = xgb.train(params,train_X,2000,evallist,feval=f1_eval)\n    model.fit(train_X,train_y)\n#     print(model.predict(test_X))\n    pred_test_y = model.predict(test_X)\n    pred_test_y2 = model.predict(test_X2)\n    return pred_test_y, pred_test_y2, model\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test2 = 0\npred_train = pd.Series(np.zeros([train.shape[0]]))\npred_full_test2  = pd.Series(np.zeros([test.shape[0]]))\npred_full_test3  = pd.Series(np.zeros([test.shape[0]]))\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test2 = pd.concat([pred_full_test2,pd.Series(pred_test_y)],axis=1)\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(f1_score(val_y, pred_val_y,average='weighted'))\n    print(cv_scores[-1])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95c1de5870f2165cd58c305bcc40fbd5e93fbc6f"},"cell_type":"code","source":"ans = pd.concat([pred_full_test[1],pred_full_test2[1]],axis=1)\nans.columns = ['SVM','LR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e80b7b4bcde3b00d8c1ca976ae792911c0b60da"},"cell_type":"code","source":"ans['probLR'] = pd.Series(np.max(model.predict_proba(test_tfidf),axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64488bc2a897fc60c9b5a7a13aa8c6ceff8d9c1a"},"cell_type":"code","source":"# ans[(ans.SVM !=ans.LR )& (ans.probLR > 0.75)]\np = ans.apply( lambda x : x.LR if  (x.SVM != x.LR) and x.probLR >=0.75 else x.SVM,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e7412ab4ed57f7c05376382e14a7b0da6fc9747"},"cell_type":"code","source":"test['Complaint-Status'] = pd.Series(le4.inverse_transform(p.apply(lambda x: int(x))))\ntest[['Complaint-ID','Complaint-Status']].to_csv('ensembleLRSVM.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"675622c23d34a53e0e34311c70e64de06995ef2f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d5339589fadb48229e3102275d2e6f21691514"},"cell_type":"code","source":"model.pre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a53e2a91f3e71ef3efb4a43e77557f1ea328db3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4461246161ca173a604d3f6caf08e67e6f0548d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}