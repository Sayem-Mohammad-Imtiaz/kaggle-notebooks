{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ntf.set_random_seed(777)","execution_count":7,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ecf64e826f8679b8bf5faa498244461632121f7e"},"cell_type":"code","source":"tf.set_random_seed(777)\n\nclass DataFeeder():\n\n    def __init__(self,path):\n\n        data = pd.read_csv(path)\n        data_x = data[data.columns.values[:-1]].values##get 20 features\n        data_y = data[data.columns.values[-1]].values##get 1 labels\n         \n        ##Standard Scaler\n        ##more efficient than min-max normalization in this data\n        scaler = StandardScaler()\n        scaler.fit(data_x)\n        data_x = scaler.transform(data_x)\n\n        data_y_label = set(data_y)\n        self.y_dict = {data_y_label.pop():0}\n\n        for step in range(len(data_y_label)):\n            self.y_dict.update({data_y_label.pop():len(self.y_dict)})\n\n        data_onehot = np.zeros((len(data_y),len(self.y_dict)),np.int32)\n        \n        for ix,value in enumerate(data_y):\n            idx = self.y_dict[value]\n            data_onehot[ix][idx] = 1\n       \n        data_y = data_onehot\n\n        data_len = len(data_y)\n        idx_rand = np.random.choice(data_len, data_len,replace=False)\n        idx_train, idx_test, _ = np.split(idx_rand,\n                                        [int(data_len*0.8),\n                                        data_len])    \n\n        self.x_train = data_x[idx_train]\n        self.y_train = data_y[idx_train]\n        self.x_test = data_x[idx_test]\n        self.y_test = data_y[idx_test]\n\n    def get_nextbatch(self,batch_size=100):\n\n        idx_len = len(self.y_train)\n        idx = np.random.choice(idx_len,idx_len, replace=False)\n\n        for step in range(len(idx)//batch_size):\n            ix = idx[step*batch_size:(step+1)*batch_size]\n            yield self.x_train[ix],self.y_train[ix]\n\n\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5314a0678db27daa490d363e3308532da5d9406a"},"cell_type":"code","source":"data = DataFeeder('../input/voice.csv') \nbank_size=[20]\nnum_filters=30\nx_features=20\ny_labels=2\ndense_batch_dims=32\ndense_fc1_dims=12\ndense_fc2_dims=8\ntrain_epoch=500\nprint_every_acc=25\nlearning_rate=0.005\nval_acc=[]\n\ntf.reset_default_graph() \nwith tf.Session() as sess:\n\n    input_x = tf.placeholder(tf.float32, shape=[None,x_features],name='x')\n    input_y = tf.placeholder(tf.int32, shape=[None,y_labels],name='y')\n    phase = tf.placeholder(tf.bool,name=\"phase\")\n\n    result=[] \n\n    ex_x = tf.expand_dims(input_x,1)\n    ex_x = tf.expand_dims(ex_x,3)\n\n    ##convolution bank. I user only one size(20)\n    ##I make more sence in num_filters(30) than conv bank size(20)\n    for i, filter_size in enumerate(bank_size):\n        with tf.variable_scope('conv_%s'%i):\n\n            filter_shape = [1,filter_size,1,num_filters]\n            W = tf.Variable(tf.truncated_normal(filter_shape,stddev=0.1),name='w_%s'%i)\n            b = tf.Variable(tf.constant(0.0,shape=[num_filters]),name='b_%s'%i)\n            conv =tf.nn.conv2d(ex_x,W,strides=[1,1,1,1],padding='VALID')\n\n            h = tf.nn.sigmoid(tf.nn.bias_add(conv,b))\n            pooled = tf.nn.max_pool(\n                h,\n                ksize=[1,1,20-filter_size+1,1],\n                strides=[1,1,1,1],\n                padding='VALID')\n            \n        result.append(pooled)\n        \n    result = tf.concat(result,2)\n    cnn_output = tf.squeeze(result,1)\n    cnn_output = tf.reshape(cnn_output, [-1,num_filters*len(bank_size)])\n\n    def dense_batch_relu(x,phase,scope): \n        with tf.variable_scope(scope):\n            h1 = tf.contrib.layers.fully_connected(x,dense_batch_dims)\n            h2 = tf.contrib.layers.batch_norm(h1,center=True,scale=True,\n                                              is_training=phase)\n            return tf.nn.relu(h2)\n\n    def dense(x,size,scope):\n        return tf.contrib.layers.fully_connected(x,size,\n                                                 scope=scope)\n    \n    ##do batch normalization one time\n    l1 = dense_batch_relu(cnn_output,phase,'dense_batch')\n\n    l2 = tf.nn.relu(dense(l1,dense_fc1_dims,'dense_fc_1'),'d2')\n\n    l3 = tf.nn.relu(dense(l2,dense_fc2_dims,'dense_fc_2'),'de')\n\n    logits = dense(l3,y_labels,'logits')\n\n    \n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=input_y))\n    \n    ##add loss for batch norma\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n        \n    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(input_y,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float32\"))\n\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    for step in range(train_epoch):\n\n        for train_x, train_y in data.get_nextbatch():\n            feed_dict = {input_x:train_x,input_y:train_y,phase:True}\n            t1 = sess.run([train_step], feed_dict=feed_dict)\n\n        if step%print_every_acc==0:        \n            feed_dict = {input_x:data.x_train,input_y:data.y_train,phase:True}\n            loss1,acc1 = sess.run([loss,accuracy],feed_dict=feed_dict)\n\n            feed_dict = {input_x:data.x_test,input_y:data.y_test,phase:False}\n            loss2,acc2 = sess.run([loss,accuracy],feed_dict=feed_dict)\n            \n            print('[%s]loss = %f , train acc = %f , test acc = %f'%(step,loss1,acc1,acc2))\n            val_acc.append(acc2)\n            \n    print('[Finished]max test accuracy : %f'%max(val_acc))\n         \n\n\n\n\n\n\n","execution_count":13,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5bab15b3798eabb221de665ab73025c19f954b08"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}