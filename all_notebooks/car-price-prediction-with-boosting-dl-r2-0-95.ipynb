{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Used Car Price Prediction"},{"metadata":{},"cell_type":"markdown","source":"- Predict used car price by various regression models\n\n- Regression Models:\n  - Linear Regression\n  - Multivariate Adaptive Regression Splines\n  - Decision Tree Regressor\n  - XGBoost Regressor\n  - Deep Neural Network\n\n- Performace Metrics:\n  - R-Squared\n  - Mean Absolute Error"},{"metadata":{},"cell_type":"markdown","source":"# 1. Import Libraries and Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes=True)\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom pyearth import Earth\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\ndf = pd.read_csv(\"/kaggle/input/vehicle-dataset-from-cardekho/car data.csv\")\nprint(\"Shape of Dataset:\", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Exploration and Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Information\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descriptive statistics\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Remove Outliers in Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the distribution of car price \nsns.distplot(df['Selling_Price'],color=\"blue\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution is right-skewed. Let's remove outliers using the IQR as the criteria."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find IQR\nQ1 = df['Selling_Price'].quantile(0.25)\nQ3 = df['Selling_Price'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers with a criteria: 1.5 x IOR\ndf = df[~((df['Selling_Price'] < (Q1 - 1.5 * IQR)) |(df['Selling_Price'] > (Q3 + 1.5 * IQR)))]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the distribution of price: outliers removed\nsns.distplot(df['Selling_Price'], color=\"blue\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Exploration of Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the list of car models\nprint(df['Car_Name'].unique().tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the frequency of each car model\nfor index, value in df['Car_Name'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since great many car models are contained, let's ignore car models in prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the value counts of transmission\nfor index, value in df['Fuel_Type'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the value counts of transmission\nfor index, value in df['Seller_Type'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the value counts of transmission\nfor index, value in df['Transmission'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the value counts of transmission\nfor index, value in df['Owner'].value_counts().iteritems():\n    print(index, ': ', value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Representing categorical data using swarm plots\nfig = plt.figure(figsize=(10,7))\nplt.subplot(2,2,1)\nsns.swarmplot(x = 'Fuel_Type', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.subplot(2,2,2)\nsns.swarmplot(x = 'Seller_Type', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.subplot(2,2,3)\nsns.swarmplot(x = 'Transmission', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.subplot(2,2,4)\nsns.swarmplot(x = 'Owner', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3. Exploration of Continuous Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list of continuous variables\ncont = [\"Selling_Price\", \"Present_Price\", \"Year\", \"Kms_Driven\"]\n\n# Create a dataframe of continuous variables\ndf_cont = df[cont]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize correlation between continuous variables\n\n# Compute the correlation matrix\ncorr = df_cont.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(4, 3))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize regression between each feature and output variable\n\n# The number of variables\nk = 4\nfig = plt.figure(figsize=(16,6))\n# Correlations between each variable\ncorrmat = df_cont.corr()\n# Take k elements in descending order of coefficient \ncols = corrmat.nlargest(k, \"Selling_Price\")[\"Selling_Price\"].index\n# Calculate correlation\nfor i in np.arange(1,k):\n    regline = df_cont[cols[i]]\n    ax = fig.add_subplot(1,3,i)\n    sns.regplot(x=regline, y=df['Selling_Price'], scatter_kws={\"color\": \"royalblue\", \"s\": 3},\n                line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4. Data Preparation for Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split X and y\nX = df.drop(['Car_Name', 'Selling_Price'], axis=1)\ny = df['Selling_Price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dummies for categorical variables\n\n# subset all categorical variables\ncars_categorical = X.select_dtypes(include=['object'])\n# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first=True)\n# drop categorical variables \nX = X.drop(list(cars_categorical.columns), axis=1)\n# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the features\n\n# Store column names since the column names will be lost after scaling\ncols = X.columns\n\n# Scale the features and convert it back to a dataframe\nX = pd.DataFrame(scale(X))\n\n# Write in the column names again\nX.columns = cols\nX.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Regression"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make prediction\ny_pred = lm.predict(X_test)\n\n# Performance metrics\nlr_r2= r2_score(y_test, y_pred)\nlr_mae = mean_absolute_error(y_test, y_pred)\n\n# Show the metrics\nprint(\"Linear Regression R2: \", lr_r2)\nprint(\"Linear Regression MAE: \", lr_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model based on the assumption of linear regression:\n\n# Assumption 1. The error terms are normally distributed with mean approximately 0.\n\nfig = plt.figure()\nsns.distplot((y_test-y_pred),bins=50, color=\"blue\")\nfig.suptitle('Error Terms', fontsize=14)                  \nplt.xlabel('y_test-y_pred', fontsize=12)                  \nplt.ylabel('Index', fontsize=12)                          \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first assumption seems to be met."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assumption 2: Homoscedasticity, i.e. the variance of the error term (y_true-y_pred) is constant.\n\nc = [i for i in range(len(y_pred))]\nfig = plt.figure()\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\", alpha=0.4)\nfig.suptitle('Error Terms', fontsize=14)               \nplt.xlabel('Index', fontsize=12)                      \nplt.ylabel('ytest-ypred', fontsize=12)                \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second assumption seems to be met."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assumption 3: There is little correlation between the predictors. i.e., Multicollinearity:\n\npredictors = ['Year', 'Present_Price', 'Kms_Driven', 'Owner', 'Fuel_Type_Diesel','Fuel_Type_Petrol', \n              'Seller_Type_Individual', 'Transmission_Manual']\n\n# Compute the correlation matrix\ncors = X.loc[:, list(predictors)].corr()\n\n# Generate a mask for the upper triangle\nmask_2 = np.triu(np.ones_like(cors, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(9, 6))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cors, mask=mask_2, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features are highly correlated. So let's check the multicolliearity by VIF."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"fuel_Petrol\" shows the highest VIF, so let's delete it."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop('Fuel_Type_Diesel', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make prediction\ny_pred = lm.predict(X_test)\n\n# Performance metrics\nlr_r2= r2_score(y_test, y_pred)\nlr_mae = mean_absolute_error(y_test, y_pred)\n\n# Show the metrics\nprint(\"Linear Regression R2: \", lr_r2)\nprint(\"Linear Regression MAE: \", lr_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Multivariate Adaptive Regression Splines (MARS)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate the model\nmars_model = Earth()\n\n# By default, we do not need to set any of the algorithm hyperparameters.\n# The algorithm automatically discovers the number and type of basis functions to use.\n\n# Fit the model\nmars_model.fit(X_train, y_train)\n\n# Making predictions\nmars_y_pred = mars_model.predict(X_test)\n\n# Performance Metrics\nmars_r2 = r2_score(y_test, mars_y_pred)\nmars_mae = mean_absolute_error(y_test, mars_y_pred)\n\n# Show the model performance\nprint(\"MARS R2: \", mars_r2)\nprint(\"MARS MAE: \", mars_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Decision Tree Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate the model\ndt_model = DecisionTreeRegressor()\n\n# Grid search\ndt_gs = GridSearchCV(dt_model,\n                     param_grid = {'max_depth': range(1, 11),\n                                   'min_samples_split': range(1, 10, 1)},\n                     cv=5,\n                     n_jobs=1,\n                     scoring='neg_mean_squared_error')\n\ndt_gs.fit(X_train, y_train)\n\nprint(dt_gs.best_params_)\nprint(-dt_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate the best model\ndt_model_best = DecisionTreeRegressor(max_depth=8, min_samples_split=2)\n\n# Fit the best model\ndt_model_best.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\ndt_y_pred = dt_model_best.predict(X_test)\n\n# Performance metrics\ndt_r2 = r2_score(y_test, dt_y_pred)\ndt_mae = mean_absolute_error(y_test, dt_y_pred)\n\n# Show the model performance\nprint(\"DT R2: \", dt_r2)\nprint(\"DT MAE: \", dt_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4. XGB Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate the model\nxgb_model = xgb.XGBRegressor()\n\n# Grid search\nxgb_gs = GridSearchCV(xgb_model,\n                      param_grid = {'max_depth': range(8, 15),\n                                   'min_samples_split': range(2, 11, 3)},\n                      cv=5,\n                      n_jobs=1,\n                      scoring='neg_mean_squared_error')\n                      \nxgb_gs.fit(X_train, y_train)\n\nprint(xgb_gs.best_params_)\nprint(-xgb_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate the best model\nxgb_model_best = xgb.XGBRegressor(max_depth=9, min_samples_split=2)\n\n# Fit the best model\nxgb_bst = xgb_model_best.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\nxgb_y_pred = xgb_bst.predict(X_test)\n\n# Performance metrics\nxgb_r2 = r2_score(y_test, xgb_y_pred)\nxgb_mae = mean_absolute_error(y_test, xgb_y_pred)\n\n# Show the model performance\nprint(\"XGB R2: \", xgb_r2)\nprint(\"XGB MAE: \", xgb_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5. Deep Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a DNN\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate DNN\ndnn = KerasRegressor(build_fn=create_model, epochs=10000, batch_size=20, verbose=1)\n\n# Fit DNN\ndnn_history = dnn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the DNN learning\nloss_train = dnn_history.history['loss']\nepochs = range(1,10001)\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss_train, 'royalblue', label='Training loss', linewidth=3)\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\ndnn_y_pred = dnn.predict(X_test)\n\n# Performance metrics\ndnn_r2 = r2_score(y_test, dnn_y_pred)\ndnn_mae = mean_absolute_error(y_test, dnn_y_pred)\n\n# Show the model performance\nprint(\"DNN R2: \", dnn_r2)\nprint(\"DNN MAE: \", dnn_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Summary of Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_table = pd.DataFrame([[np.mean(lr_r2), np.mean(lr_mae)],\n                             [np.mean(mars_r2), np.mean(mars_mae)],\n                             [np.mean(dt_r2), np.mean(dt_mae)],\n                             [np.mean(xgb_r2), np.mean(xgb_mae)],\n                             [np.mean(dnn_r2), np.mean(dnn_mae)]],\n                            columns=['R2', 'MAE'],\n                            index=[\"Linear Regression\",\"MARS\",\"Decision Tree\",\"XGBoost\",\"DNN\"])\npd.options.display.precision = 3\nresults_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_table = pd.DataFrame({\"Linear Regression: Predicted Price\": y_pred,\n                           \"MARS: Predicted Price\": mars_y_pred,\n                           \"Decision Tree: Predicted Price\": dt_y_pred,\n                           \"XGBoost: Predicted Price\": xgb_y_pred,\n                           \"DNN: Predicted Price\": dnn_y_pred,\n                          \"Actual Price\": y_test})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the predicted price and actual price\nfig = plt.figure(figsize=(10,10))\nplt.subplot(3,2,1)\nsns.regplot(x = 'Linear Regression: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,2)\nsns.regplot(x = 'MARS: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,3)\nsns.regplot(x = 'Decision Tree: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,4)\nsns.regplot(x = 'XGBoost: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,5)\nsns.regplot(x = 'DNN: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"Since DNN achieved the highest performance, show the feature importance of DNN."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import libaries\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Show permutation importance\nperm = PermutationImportance(dnn, random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}