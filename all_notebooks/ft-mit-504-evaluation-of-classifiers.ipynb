{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Diabetic Retinopathy\n\n**Overview**\n\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. This dataset was only extracted from a larger database according to author. \n\nOur target variable is the outcome based on 1 or 0, whether or not the patient has diabetes or not, respectively. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, Skin thickness, Blood pressure and so on.\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings('always')  \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n\nprint(f'Data Frame Shape (rows, columns): {df.shape}') \ndf.head()\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis and Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue=\"Outcome\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info(verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize outcome of classes\nsns.countplot(data=df, x=\"Outcome\").set_title(\"Diabetic Retinopathy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this part we check on how many instances of the class on our dataset. As we can see in our dataset there is a little difference in the outcome so what we do is to do balancing to avoid biases or possibility of overfitting.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  DATA PREPARATION, BALANCING AND CLEANUP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the library to import to be able to use random under sampler balancing technique.\nfrom imblearn.under_sampling import RandomUnderSampler\nwarnings.filterwarnings('ignore')\n \n# If you want to know when to balance a data set, just read here:\n# https://stats.stackexchange.com/questions/227088/when-should-i-balance-classes-in-a-training-data-set\n \n# There are various way to balance a dataset, one basic way is to under sample. \n# Setup our Under Sampler\nrUnderSampler = RandomUnderSampler(random_state=10) # This is just a random seed.\n\n# Perform random under sampling.\ndfBalancedFeatures, dfBalanceTarget = rUnderSampler.fit_resample(df.drop(columns=\"Outcome\", axis=0), df[\"Outcome\"])\n\nprint(f'New Shape of balanced features: {dfBalancedFeatures.shape}')\nprint(f'New Shape of balanced target: {dfBalanceTarget.shape}')\n# Visualize new classes distributions\nsns.countplot(dfBalanceTarget ).set_title('Balanced Data Set')\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So in this part we try to balance our dataset to avoid biases in our output. After the Reduction Sampling, there are 536 samples left in our dataset. As we can see our dataset is now equal, therefore we have now a balance dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CLASSIFIER SETUPS AND MODELS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now this is my custom function on doing matrix caculation to the Confusion Matrix to get the TP, FP, TN and FN respectively. This function returns an ordered list of the performance measures.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"def sensitivity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TPR = (TP/(TP+FN)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TPR = (TP/(TP+FN))[1] \n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TPR\n\ndef specificity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TNR = (TN/(TN+FP)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TNR = (TN/(TN+FP))[1]\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TNR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perf_measure(actual, prediction):\n    cm = confusion_matrix (actual, prediction)\n    FP = cm.sum(axis=0) - np.diag(cm)  \n    FN = cm.sum(axis=1) - np.diag(cm)\n    TP = np.diag(cm)\n    TN = cm.sum() - (FP + FN + TP)\n\n    return(TP, FP, TN, FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SETTING UP OUR SCORERS\n\nscoring = {\n            'accuracy':make_scorer(accuracy_score), \n            'precision':make_scorer(precision_score, average='weighted',zero_division='warn'),\n            'f1_score':make_scorer(f1_score, average='weighted'),\n            'recall':make_scorer(recall_score, average='weighted'), \n            'sensitivity':make_scorer(sensitivity_score, mode=\"multiclass\"), \n            'specificity':make_scorer(specificity_score, mode=\"multiclass\"), \n           }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.svm import LinearSVC # Support Vector Machine\n\n\n# Instantiate the machine learning classifiers\nDTClassifier_model = DecisionTreeClassifier()\ngaussianNB_model = GaussianNB()\nLR_model = LogisticRegression(max_iter=10000)\nlinearSVC_model = LinearSVC(dual=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def models_evaluation(features, target, folds):    \n    # Perform cross-validation to each machine learning classifier\n    DTClassifier_result = cross_validate(DTClassifier_model, features, target, cv=folds, scoring=scoring)\n    gaussianNB_result = cross_validate(gaussianNB_model, features, target, cv=folds, scoring=scoring)\n    LR_result = cross_validate(LR_model, features, target, cv=folds, scoring=scoring)\n    linearSVC_result = cross_validate(linearSVC_model, features, target, cv=folds, scoring=scoring)\n    \n    \n    models_scores_table = pd.DataFrame({\n      'Decision Tree':[\n                        DTClassifier_result['test_accuracy'].mean(),\n                        DTClassifier_result['test_precision'].mean(),\n                        DTClassifier_result['test_recall'].mean(),\n                        DTClassifier_result['test_sensitivity'].mean(),\n                        DTClassifier_result['test_specificity'].mean(),\n                        DTClassifier_result['test_f1_score'].mean()\n                       ],\n      'Gaussian Naive Bayes':[\n                        gaussianNB_result['test_accuracy'].mean(),\n                        gaussianNB_result['test_precision'].mean(),\n                        gaussianNB_result['test_recall'].mean(),\n                        gaussianNB_result['test_sensitivity'].mean(),\n                        gaussianNB_result['test_specificity'].mean(),\n                        gaussianNB_result['test_f1_score'].mean()\n                              ],\n      'Logistic Regression':[\n                        LR_result['test_accuracy'].mean(),\n                        LR_result['test_precision'].mean(),\n                        LR_result['test_recall'].mean(),\n                        LR_result['test_sensitivity'].mean(),\n                        LR_result['test_specificity'].mean(),\n                        LR_result['test_f1_score'].mean()\n                            ],\n      'Support Vector Classifier':[\n                       linearSVC_result['test_accuracy'].mean(),\n                       linearSVC_result['test_precision'].mean(),\n                       linearSVC_result['test_recall'].mean(),\n                       linearSVC_result['test_sensitivity'].mean(),\n                       linearSVC_result['test_specificity'].mean(),\n                       linearSVC_result['test_f1_score'].mean()\n                                   ],\n         },\n\n      index=['Accuracy', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', ])\n        \n        \n    return(models_scores_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running our Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this part here we run our evaluation using our balanced target and balanced features and using a 5-fold cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\nevaluationResult = models_evaluation(dfBalancedFeatures, dfBalanceTarget, 5)\nview = evaluationResult\nview = view.rename_axis('Performance Measure').reset_index() #Add the index names to the column. This will be used for our presentation\n\n# https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n# Re-Organizing our dataframe to fit our view need\nview = view.melt(var_name='Classifier', value_name='Value', id_vars='Performance Measure')\n# result\nsns.catplot(data=view, x=\"Performance Measure\", y=\"Value\", hue=\"Classifier\", kind='bar', palette=\"bright\", alpha=0.8, legend=True, height=5, margin_titles=True, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the graph, Support Vector Machine has the highest score in terms of accuracy, precision, recall, Sensitivity, Specificity and F-1 scoring.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluationResult['Best Score'] = evaluationResult.idxmax(axis=1)\nevaluationResult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**\n\nSo we therefore conclude that for this particular data set and classifiers used, Support Vector Classifier is the most convenient and performing classifier to be used in terms of accuracy, precision, recall, sensitivity, specificity, f-1 score\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}