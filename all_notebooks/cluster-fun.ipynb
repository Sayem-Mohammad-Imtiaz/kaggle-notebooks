{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**General Information Of Data**\n* The customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal.\n\n **Features**\n* Fresh: annual spending (m.u.) on fresh products (Continuous);\n* Milk: annual spending (m.u.) on milk products (Continuous);\n* Grocery: annual spending (m.u.) on grocery products (Continuous);\n* Frozen: annual spending (m.u.) on frozen products (Continuous);\n* Detergents_Paper: annual spending (m.u.) on detergents and paper products (Continuous);\n* Delicatessen: annual spending (m.u.) on and delicatessen products (Continuous);\n* Channel: {Hotel/Restaurant/Cafe - 1, Retail - 2} (Nominal)\n* Region: {Lisbon - 1, Oporto - 2, or Other - 3} (Nominal)","metadata":{}},{"cell_type":"code","source":"#Important libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n# Pretty display for notebooks\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:36:18.548559Z","iopub.execute_input":"2021-05-31T13:36:18.549171Z","iopub.status.idle":"2021-05-31T13:36:18.56781Z","shell.execute_reply.started":"2021-05-31T13:36:18.549129Z","shell.execute_reply":"2021-05-31T13:36:18.566882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read_Data\ndata = pd.read_csv('../input/customer-segmentaion/customers.csv' , sep = ',' , encoding ='utf8')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:36:19.474495Z","iopub.execute_input":"2021-05-31T13:36:19.47502Z","iopub.status.idle":"2021-05-31T13:36:19.49014Z","shell.execute_reply.started":"2021-05-31T13:36:19.474984Z","shell.execute_reply":"2021-05-31T13:36:19.488724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing (data):\n    data['fresh']  = np.log(data['Fresh'])\n    data['milk']   = np.log(data['Milk'])\n    data['grocery']= np.log(data['Grocery'])\n    data[\"frozen\"] = np.log(data[\"Frozen\"])\n    data[\"detergents_Paper\"] = np.log(data[\"Detergents_Paper\"])\n    data[\"delicatessen\"]     = np.log(data[\"Delicatessen\"])\n    \n    #Drop Columns after log transform\n    data.drop(['Fresh' , 'Milk','Grocery' , 'Frozen' , 'Detergents_Paper' , 'Delicatessen'],axis=1 ,inplace = True)\n    #convert to object to be more readable\n    #Channel:{Hotel/Restaurant/Cafe - 1, Retail - 2}\n    data['Channel'].replace({1:\"h-r-c\" , 2:\"Retail\"} , inplace=True)\n    #Region:{Lisbon - 1, Oporto - 2, or Other - 3} \n    data['Region'].replace({1:\"Lisbon\" , 2:\"Oporto\" , 3:\"Other Zone\"}, inplace=True)  \n    #We have transformed categorical columns to dummy.\n    data= pd.concat([data, pd.get_dummies(data[\"Channel\"], drop_first=True),pd.get_dummies(data[\"Region\"])], axis=1)\n    data.drop(columns=[\"Channel\", \"Region\"], axis=1, inplace=True)\n    #drop for Retail columns as the same column of h-r-c\n\n    outliers_list = []\n# For each feature find the data points with extreme high or low values\n    for feature in data.keys():\n        # Calculate Q1 (25th percentile of the data) for the given feature\n        Q1 = np.percentile(data[feature], 25)\n        # Calculate Q3 (75th percentile of the data) for the given feature\n        Q3 = np.percentile(data[feature], 75)\n        # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n        step = (Q3 - Q1) * 1.5\n        outliers = list(data[~((data[feature] >= Q1 - step) & (data[feature] <= Q3 + step))].index.values)\n        outliers_list.extend(outliers)\n    duplicate_outliers_list = list(set([x for x in outliers_list if outliers_list.count(x) >= 2]))\n    # Remove the outliers\n    outliers  = duplicate_outliers_list\n    new_data = data.drop(data.index[outliers]).reset_index(drop = True)\n#Before clustering, we transform features from original version to standardize version\n#as after dummy for two columns has zero and ones \n#and another columns has data by milliones \n\n    scaler= StandardScaler()\n    std_data= scaler.fit_transform(new_data)\n    mean_vec = np.mean(std_data, axis=0)\n    cov_mat = (std_data - mean_vec).T.dot((std_data - mean_vec)) / (std_data.shape[0]-1)    \n    cov_mat = np.cov(std_data.T)\n    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n    tot = sum(eig_vals)\n    var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n    cum_var_exp = np.cumsum(var_exp)\n    # Apply PCA by fitting the new data with the same number of dimensions as features\n\n#svd_solver auto , full , arpack\n#n_component is number of new features\n#n_component is 4 as important component\n    pca = PCA(n_components=4, copy=True , svd_solver='full' , random_state=0 , iterated_power='auto' ,whiten = False)\n    reduced_data = pca.fit_transform(std_data)\n    # Create a DataFrame for the reduced data\n    reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2','Dimension 3', 'Dimension 4'])\n    \n    return reduced_data\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:38:20.815154Z","iopub.execute_input":"2021-05-31T13:38:20.815803Z","iopub.status.idle":"2021-05-31T13:38:20.832751Z","shell.execute_reply.started":"2021-05-31T13:38:20.815751Z","shell.execute_reply":"2021-05-31T13:38:20.831815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = preprocessing(data)\nnew_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:38:43.27635Z","iopub.execute_input":"2021-05-31T13:38:43.276737Z","iopub.status.idle":"2021-05-31T13:38:43.361986Z","shell.execute_reply.started":"2021-05-31T13:38:43.276706Z","shell.execute_reply":"2021-05-31T13:38:43.360433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering Model","metadata":{}},{"cell_type":"code","source":"KMeanModel = KMeans(n_clusters= 3, init='k-means++' , random_state=33 , algorithm='auto')\n#algorithm is auto , full or elkan\ndef train(data_pro):\n    #Fitting Model\n    KMeanModel.fit(data_pro)\n    return 'train is Done successful:)'\n\n#calling function\ntrain()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:17:58.4148Z","iopub.execute_input":"2021-05-30T22:17:58.415097Z","iopub.status.idle":"2021-05-30T22:17:58.420737Z","shell.execute_reply.started":"2021-05-30T22:17:58.415073Z","shell.execute_reply":"2021-05-30T22:17:58.420034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(data_pro):\n    y_predict=KMeanModel.predict(data_pro)\n    return y_predict\n#call function\npredict()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:25:47.447884Z","iopub.execute_input":"2021-05-30T22:25:47.448245Z","iopub.status.idle":"2021-05-30T22:25:47.452327Z","shell.execute_reply.started":"2021-05-30T22:25:47.448214Z","shell.execute_reply":"2021-05-30T22:25:47.451281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation (data_pro):\n    labels  = KMeanModel.labels_\n    silhouette_Score = silhouette_score(data_pro, labels)\n    return silhouette_Score\n#call function\nevaluation()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:27:47.889887Z","iopub.execute_input":"2021-05-30T22:27:47.89028Z","iopub.status.idle":"2021-05-30T22:27:47.908671Z","shell.execute_reply.started":"2021-05-30T22:27:47.890242Z","shell.execute_reply":"2021-05-30T22:27:47.907442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}