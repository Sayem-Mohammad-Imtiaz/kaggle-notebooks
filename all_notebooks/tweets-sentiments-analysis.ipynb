{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport os \nimport pandas as pd \nimport re\n\n## Data Path \nneg_path = \"../input/sentimental-analysis-nlp/neg_tweets.txt\"\npos_path = \"../input/sentimental-analysis-nlp/pos_tweets.txt\"\n\n\npos_tweets = []\nneg_tweets = []\n\n## Read Negative Tweets Data\nwith open(neg_path, \"r\", encoding = \"utf-8\") as f :\n    neg_lines = f.read().split(\"\\n\")\ni = 0\nfor j, line in enumerate(neg_lines) :\n    sent = re.sub(r\"[^a-zA-Z1234567890?.,!@]\", \" \", line)\n    sent = sent.split()\n    tweet = None\n    k = 0\n    if len(sent) > 1 and i != 14 :\n        tweet = \" \".join(word for word in sent if word.find(\"@\") != 0)\n    neg_tweets.append(tweet)\n    i += 1\nprint(len(neg_tweets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Read Positive Tweets Data\nwith open(pos_path, \"r\", encoding = \"utf-8\") as f :\n    pos_lines = f.read().split(\"\\n\")\ni = 0\nfor j, line in enumerate(pos_lines[0::]) :\n    sent = re.sub(r\"[^a-zA-Z1234567890?.,!@]\", \" \", line)\n    sent = sent.split()\n    tweet = None\n    k = 0\n    if len(sent) > 1 and i != 14 :\n        tweet = \" \".join(word for word in sent if word.find(\"@\") != 0)\n    pos_tweets.append(tweet)\n    i += 1\n\nprint(len(pos_tweets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importing Stopwords and Lemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n## Define Stopwords and Lemmatizer Variable\nstop_words = stopwords.words(\"english\")\nlemm = WordNetLemmatizer()\n\n##  Find Number of Negatif Tweets That Give Error\n## When Do Word Lemmatizer\n\nerror_num = [14, 104, 421, 1109]\n\n\n## Do Stopwords and Word Lemmatizer for Negative Tweets\nnegative_tweets = []\nfor p, sent in enumerate(neg_tweets) :\n    if p not in error_num :\n        sent = re.sub(r\"[^a-zA-Z1234567890?.,!]\", \" \", sent)\n        sent = sent.lower()\n        sent = sent.split()\n        new_sent = \" \".join(lemm.lemmatize(word) for word in sent if word not in set(stop_words))\n        negative_tweets.append(new_sent)\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##  Find Number of Positive Tweets That Give Error\n## When Do Word Lemmatizer\n\nerror_line = [14, 261, 313, 348, 551]\n## Do Stopwords and Word Lemmatizer for Positive Tweets\npositive_tweets = []\nfor l, sentence in enumerate(pos_tweets) :\n    if l not in error_line :\n        sent = re.sub(r\"[^a-zA-Z1234567890?.,!]\", \" \", sentence)\n        sent = sent.lower()\n        sent = sent.split()\n        new_sent = \" \".join(lemm.lemmatize(word) for word in sent if word not in set(stop_words))\n        positive_tweets.append(new_sent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importing Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n## Import Imblearn to Do Undersampling\nfrom imblearn.under_sampling import NearMiss\n\n## Concat Negative and Positive Tweets \n## Create Label\nall_tweets = positive_tweets + negative_tweets\nY = np.hstack((np.ones(len(positive_tweets)), np.zeros(len(negative_tweets))))\n\n##  Define TFIDF and Make All Tweets Become an Array\nvectorizer = TfidfVectorizer()\nvectorizer.fit(all_tweets)\nX = vectorizer.transform(all_tweets)\n\n## Split Data\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 42, \n                                                    test_size = 0.2)\n\n## Do  Undersampling for Train Data\nnms = NearMiss(n_neighbors = 3)\nX_train, Y_train = nms.fit_resample(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importing Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n## Define Classifier\n## Fitting the Model\nclassifier = RandomForestClassifier(n_estimators = 110)\nclassifier.fit(X_train, Y_train)\n\n## Predict Test Data\nY_predict = classifier.predict(X_test)\nlabel = {1 : \"Positive\", 0 : \"Negative\"}\n\n## Get Accuracy Score and Confusion Matrix\naccuracy = accuracy_score(Y_test, Y_predict)\ncf_matrix = confusion_matrix(Y_test, Y_predict)\n\nprint(\"Accuracy Score : {}\".format(accuracy))\nprint(\"Confusion Matrix : {}\".format(cf_matrix))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importing Pickle\nimport pickle\n\n## Save Clasifier Model and TFIDF\ntfidf_file = \"vectorizer.pickle\"\npickle.dump(vectorizer, open(tfidf_file, \"wb\"))\n\nmodel_file = \"randomforest.pickle\"\npickle.dump(classifier, open(model_file, \"wb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nsn.heatmap(cf_matrix, annot = True)\nplt.xlabel(\"Predict\")\nplt.ylabel(\"True\")\nplt.savefig(\"confusion.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}