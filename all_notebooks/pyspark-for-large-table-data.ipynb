{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-19T13:59:28.204396Z","iopub.execute_input":"2021-06-19T13:59:28.205387Z","iopub.status.idle":"2021-06-19T13:59:28.252373Z","shell.execute_reply.started":"2021-06-19T13:59:28.205326Z","shell.execute_reply":"2021-06-19T13:59:28.251379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Motivation: data is large\n\nOur data is around 2 GB. It would be cumbersome to analyse it using traditional instruments for flat table data such as pandas. Therefore let's try using Apache Spark (or it's Python wrapper: PySpark) to get around this problem.","metadata":{}},{"cell_type":"markdown","source":"### First, let's install PySpark with pip and import all the necessary functions:","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:05:40.122634Z","iopub.execute_input":"2021-06-19T14:05:40.123279Z","iopub.status.idle":"2021-06-19T14:05:47.33801Z","shell.execute_reply.started":"2021-06-19T14:05:40.123234Z","shell.execute_reply":"2021-06-19T14:05:47.336811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as f\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\n\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('anime-data').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:05:47.340155Z","iopub.execute_input":"2021-06-19T14:05:47.340542Z","iopub.status.idle":"2021-06-19T14:05:47.348975Z","shell.execute_reply.started":"2021-06-19T14:05:47.340498Z","shell.execute_reply":"2021-06-19T14:05:47.347843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It is preferable to expicitly set the structure of data which we'll be reading with PySpark. It can be done by defining schemas. Basically, schema is a list of columns to be found in our table data as well as types of each column.","metadata":{}},{"cell_type":"code","source":"anime_schema = StructType(fields=[StructField('MAL_ID', IntegerType()),\n                           StructField('name', StringType()),\n                            StructField('score', DoubleType()),\n                            StructField('genres', StringType()),\n                            StructField('synopsis', StringType())\n                           ])\n\n\nanimelist_schema = StructType(fields=[StructField('user_id', IntegerType()),\n                           StructField('anime_id', IntegerType()),\n                            StructField('rating', IntegerType()),\n                            StructField('watching_status', IntegerType()),\n                            StructField('watched_episodes', IntegerType())\n                           ])","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:00:48.041928Z","iopub.execute_input":"2021-06-19T14:00:48.042323Z","iopub.status.idle":"2021-06-19T14:00:48.049055Z","shell.execute_reply.started":"2021-06-19T14:00:48.042291Z","shell.execute_reply":"2021-06-19T14:00:48.047926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next, let's read csv-files proving our pre-defined schemas.","metadata":{}},{"cell_type":"code","source":"anime = spark.read.csv('/kaggle/input/anime-recommendation-database-2020/anime_with_synopsis.csv', \n              schema=anime_schema,\n              sep=',',\n              header=True)\n\nanimelist = spark.read.csv('/kaggle/input/anime-recommendation-database-2020/animelist.csv', \n              schema=animelist_schema,\n              sep=',',\n              header=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:11:00.51597Z","iopub.execute_input":"2021-06-19T14:11:00.516517Z","iopub.status.idle":"2021-06-19T14:11:02.804002Z","shell.execute_reply.started":"2021-06-19T14:11:00.516483Z","shell.execute_reply":"2021-06-19T14:11:02.802684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmmm... it might seem odd that 2 GB data were read so quickly. Actually, so far Spark hasn't read anything. Spark implements so-called 'lazy evaluation' which basically means that it won't handle any actual data untill we ask it to do so. In another words, we can define multiple 'transformations' which we want to make on our data, but they will be actually implemented only when we call an 'action' function.\n\n\nFor example, showing N rows of the resulting DataFrame is an 'action'. Calling .show() method will trigger execution of the chain of transformations which we defined previously, so that actual 'computation' takes place.","metadata":{}},{"cell_type":"code","source":"anime.show(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:18:31.898732Z","iopub.execute_input":"2021-06-19T14:18:31.900006Z","iopub.status.idle":"2021-06-19T14:18:34.595218Z","shell.execute_reply.started":"2021-06-19T14:18:31.899947Z","shell.execute_reply":"2021-06-19T14:18:34.593951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Subsetting data, filtering, grouping, aggregating the results of grouping, ordering, renaming columns etc. - all these typical operations are done by Spark in a 'lazy' manner. The actual computation of transformation will occur only when calling an action.","metadata":{}},{"cell_type":"markdown","source":"### For example, this way we can get the top-10 scored anime in our data:","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_anime = anime\\\n            .filter('score is not null')\\\n            .withColumnRenamed('MAL_ID', 'anime_id')\\\n            .orderBy('score', ascending=False)\\\n            .cache()\n\nbest_anime.show(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:27:15.082792Z","iopub.execute_input":"2021-06-19T14:27:15.083242Z","iopub.status.idle":"2021-06-19T14:27:17.670416Z","shell.execute_reply.started":"2021-06-19T14:27:15.083204Z","shell.execute_reply":"2021-06-19T14:27:17.669258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conveniently, Spark DataFrames can be exported into our favorite pandas DataFrame at anytime:","metadata":{}},{"cell_type":"code","source":"best_anime_pandas = best_anime.toPandas()\nbest_anime_pandas.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:28:05.937117Z","iopub.execute_input":"2021-06-19T14:28:05.937558Z","iopub.status.idle":"2021-06-19T14:28:05.96808Z","shell.execute_reply.started":"2021-06-19T14:28:05.937515Z","shell.execute_reply":"2021-06-19T14:28:05.966694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now, let's find out what are the most popular anime genres in our data. Also let's calculate the average score of each genre and it's share among all the data.","metadata":{}},{"cell_type":"code","source":"popular_genres = anime.withColumn('genres_list', f.split('genres', ','))\\\n                     .withColumn('genre', f.explode('genres_list'))\\\n                     .groupBy('genre')\\\n                     .agg(f.count(f.col('name')).alias('genre_count'), f.avg(f.col('score')).alias('avg_score'))\\\n                     .orderBy('genre_count', ascending=False)\\\n                    .withColumn('pct_of_total', f.col('genre_count') / f.sum('genre_count').over(Window.partitionBy()))\\\n                    .withColumn('pct_of_total', f.col('pct_of_total') * 100)\\\n                    .cache()\n\npopular_genres.show(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:30:57.839153Z","iopub.execute_input":"2021-06-19T14:30:57.839569Z","iopub.status.idle":"2021-06-19T14:30:58.215189Z","shell.execute_reply.started":"2021-06-19T14:30:57.839534Z","shell.execute_reply":"2021-06-19T14:30:58.213979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pct_of_total adds up to 100% which is a sign that we've calculated this column correctly:","metadata":{}},{"cell_type":"code","source":"popular_genres.select(f.sum(popular_genres['pct_of_total']).alias('pct_sum')).show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:33:51.565624Z","iopub.execute_input":"2021-06-19T14:33:51.566117Z","iopub.status.idle":"2021-06-19T14:33:51.775174Z","shell.execute_reply.started":"2021-06-19T14:33:51.566073Z","shell.execute_reply":"2021-06-19T14:33:51.774014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's do something more complicated: \n\n**1) Find the top 10% of anime by score;**\n\n**2) Get the distribution of genres among top 10%;**\n\n**3) Compare this distribution with the one of all anime. List anime titles with the greatest absolute difference in distributions**","metadata":{}},{"cell_type":"code","source":"anime_10th_ptile = anime.filter('score is not null')\\\n            .select('name', 'genres', 'score', f.percent_rank().over(Window.partitionBy().orderBy(anime['score'])).alias('pct_rank'))\\\n            .filter('pct_rank >= 0.9')\\\n\npopular_genres_10th_ptile = anime_10th_ptile.withColumn('genres_list', f.split('genres', ','))\\\n             .withColumn('genre', f.explode('genres_list'))\\\n             .groupBy('genre')\\\n             .agg(f.count(f.col('name')).alias('genre_count'), f.avg(f.col('score')).alias('avg_score'))\\\n             .orderBy('genre_count', ascending=False)\\\n            .withColumn('pct_of_total', f.col('genre_count') / f.sum('genre_count').over(Window.partitionBy()))\\\n            .withColumn('pct_of_total', f.col('pct_of_total') * 100)\\\n            .cache()\n\npopular_compare = popular_genres[['genre', 'pct_of_total']].withColumnRenamed('pct_of_total', 'pct_of_total_left')\\\n    .join(popular_genres_10th_ptile[['genre', 'pct_of_total']].withColumnRenamed('pct_of_total', 'pct_of_total_right'), on='genre', how='left')\\\n    .withColumn('abs_difference', f.abs(f.col('pct_of_total_right') - f.col('pct_of_total_left')))\\\n    .orderBy('abs_difference', ascending=False)\n\npopular_compare.show(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:40:40.994796Z","iopub.execute_input":"2021-06-19T14:40:40.995385Z","iopub.status.idle":"2021-06-19T14:40:41.492289Z","shell.execute_reply.started":"2021-06-19T14:40:40.995347Z","shell.execute_reply":"2021-06-19T14:40:41.491006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Genres, which are not present among the top 10% anime (by score):","metadata":{}},{"cell_type":"code","source":"popular_compare.select('genre').filter(f.col('pct_of_total_right').isNull()).show(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:41:09.827251Z","iopub.execute_input":"2021-06-19T14:41:09.827636Z","iopub.status.idle":"2021-06-19T14:41:10.146202Z","shell.execute_reply.started":"2021-06-19T14:41:09.827602Z","shell.execute_reply":"2021-06-19T14:41:10.14496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's analyse really big table data, which was in *anime-recommendation-database-2020/animelist.csv* . This dataframe contains evaluations of anime titles by each user.","metadata":{}},{"cell_type":"code","source":"animelist.show(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:44:50.1583Z","iopub.execute_input":"2021-06-19T14:44:50.158659Z","iopub.status.idle":"2021-06-19T14:44:50.417613Z","shell.execute_reply.started":"2021-06-19T14:44:50.158626Z","shell.execute_reply":"2021-06-19T14:44:50.416228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's group by anime_id and calculate some useful stats:","metadata":{}},{"cell_type":"code","source":"most_watched_anime = animelist.groupBy('anime_id')\\\n    .agg(f.count(f.col('user_id')).alias('user_cnt'),\n         f.mean(f.col('rating')).alias('mean_rating'),\n         f.stddev(f.col('rating')).alias('std_rating'),\n         f.percentile_approx(f.col('rating'), 0.5).alias('median_rating'),\n         f.mean(f.col('watched_episodes')).alias('mean_num_episodes'))\\\n    .orderBy('user_cnt', ascending=False)\\\n    .cache()\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:46:50.576744Z","iopub.execute_input":"2021-06-19T14:46:50.577337Z","iopub.status.idle":"2021-06-19T14:46:50.692651Z","shell.execute_reply.started":"2021-06-19T14:46:50.577285Z","shell.execute_reply":"2021-06-19T14:46:50.691913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmost_watched_anime.show(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:46:52.516437Z","iopub.execute_input":"2021-06-19T14:46:52.517108Z","iopub.status.idle":"2021-06-19T14:50:52.912217Z","shell.execute_reply.started":"2021-06-19T14:46:52.517057Z","shell.execute_reply":"2021-06-19T14:50:52.911045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It really takes long time to process...","metadata":{}},{"cell_type":"markdown","source":"### Let's join the resulting DataFrame with another one and sort the result by user count","metadata":{}},{"cell_type":"code","source":"anime_joined = best_anime\\\n    .join(most_watched_anime, on='anime_id', how='inner')\\\n    .orderBy('user_cnt', ascending=False)\\\n    .cache()\n\nanime_joined.show(3, truncate=False, vertical=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:53:57.524043Z","iopub.execute_input":"2021-06-19T14:53:57.524432Z","iopub.status.idle":"2021-06-19T14:53:57.647798Z","shell.execute_reply.started":"2021-06-19T14:53:57.524397Z","shell.execute_reply":"2021-06-19T14:53:57.646819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spark allows not only for pandas-like data wrangling, but also inference with standard ML-models.\n\nFor example, let's try to fit a simple linear regression model.","metadata":{}},{"cell_type":"markdown","source":"First, we encode each genre an anime has into 1/0:","metadata":{}},{"cell_type":"code","source":"genres_list = [g['genre'].strip() for g in popular_genres.select('genre').collect()]","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:57:56.358123Z","iopub.execute_input":"2021-06-19T14:57:56.358563Z","iopub.status.idle":"2021-06-19T14:57:56.482111Z","shell.execute_reply.started":"2021-06-19T14:57:56.358523Z","shell.execute_reply":"2021-06-19T14:57:56.480624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Unique genres: %s' % len(genres_list))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T14:59:37.194381Z","iopub.execute_input":"2021-06-19T14:59:37.194997Z","iopub.status.idle":"2021-06-19T14:59:37.201188Z","shell.execute_reply.started":"2021-06-19T14:59:37.194943Z","shell.execute_reply":"2021-06-19T14:59:37.200133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use sklearn MultiLabelBinarizer. We fit 83 genres we have into it.","metadata":{}},{"cell_type":"code","source":"ml_bin = MultiLabelBinarizer()\nml_bin.fit([genres_list])\n\nprint(ml_bin.classes_[:10])\nprint(len(ml_bin.classes_))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T15:01:27.032236Z","iopub.execute_input":"2021-06-19T15:01:27.032844Z","iopub.status.idle":"2021-06-19T15:01:27.039992Z","shell.execute_reply.started":"2021-06-19T15:01:27.032793Z","shell.execute_reply":"2021-06-19T15:01:27.038913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we define a custom UDF (User-Defined Function) that we'll feed into Spark in a groupby statement.","metadata":{}},{"cell_type":"code","source":"def binarize_genres(entry, binarizer):\n    entry_list = [el.strip() for el in entry.split(', ')]\n    entry_list = [el for el in entry_list if el != '']\n    entry_tpl = tuple(entry_list)\n    vector = binarizer.transform([entry_tpl])\n    return [int(i) for i in vector[0]]","metadata":{"execution":{"iopub.status.busy":"2021-06-19T15:03:17.462083Z","iopub.execute_input":"2021-06-19T15:03:17.462688Z","iopub.status.idle":"2021-06-19T15:03:17.468532Z","shell.execute_reply.started":"2021-06-19T15:03:17.462652Z","shell.execute_reply":"2021-06-19T15:03:17.467672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binarize_genres_udf = f.udf(lambda x: binarize_genres(x, ml_bin), returnType=ArrayType(IntegerType()))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:41.963296Z","iopub.execute_input":"2021-06-19T16:00:41.963666Z","iopub.status.idle":"2021-06-19T16:00:41.976665Z","shell.execute_reply.started":"2021-06-19T16:00:41.963632Z","shell.execute_reply":"2021-06-19T16:00:41.975498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = anime_joined\\\n    .withColumn('genres_binarized', binarize_genres_udf(f.col('genres')))\\\n    .select(['anime_id', 'genres_binarized', 'user_cnt', 'mean_num_episodes', 'score'])\\\n    .cache()\n\nfor idx, genre_name in enumerate(ml_bin.classes_):\n    data = data.withColumn(f'genre_{idx}', f.col('genres_binarized').getItem(idx))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:42.755473Z","iopub.execute_input":"2021-06-19T16:00:42.756064Z","iopub.status.idle":"2021-06-19T16:00:43.434213Z","shell.execute_reply.started":"2021-06-19T16:00:42.756031Z","shell.execute_reply":"2021-06-19T16:00:43.433098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:44.283495Z","iopub.execute_input":"2021-06-19T16:00:44.283934Z","iopub.status.idle":"2021-06-19T16:00:44.291235Z","shell.execute_reply.started":"2021-06-19T16:00:44.283895Z","shell.execute_reply":"2021-06-19T16:00:44.29026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop('genres_binarized')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:47.909178Z","iopub.execute_input":"2021-06-19T16:00:47.909677Z","iopub.status.idle":"2021-06-19T16:00:47.924783Z","shell.execute_reply.started":"2021-06-19T16:00:47.909643Z","shell.execute_reply":"2021-06-19T16:00:47.923825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When using SparkML (built-in machine-learning models' classes in Spark), you have to input your data in form of a one big Vector-column.","metadata":{}},{"cell_type":"code","source":"assembler = VectorAssembler(inputCols=data.drop('score').columns,\n                            outputCol='features')\n\nlinreg = LinearRegression(featuresCol='features', labelCol='score')\n\npipeline = Pipeline(stages=[assembler,\n                           linreg])","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:49.862353Z","iopub.execute_input":"2021-06-19T16:00:49.862704Z","iopub.status.idle":"2021-06-19T16:00:49.888543Z","shell.execute_reply.started":"2021-06-19T16:00:49.862674Z","shell.execute_reply":"2021-06-19T16:00:49.887336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_model = pipeline.fit(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:50.713283Z","iopub.execute_input":"2021-06-19T16:00:50.714016Z","iopub.status.idle":"2021-06-19T16:00:59.700618Z","shell.execute_reply.started":"2021-06-19T16:00:50.713963Z","shell.execute_reply":"2021-06-19T16:00:59.699504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pipeline_model.transform(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:59.712917Z","iopub.execute_input":"2021-06-19T16:00:59.713349Z","iopub.status.idle":"2021-06-19T16:00:59.832132Z","shell.execute_reply.started":"2021-06-19T16:00:59.713304Z","shell.execute_reply":"2021-06-19T16:00:59.831001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results seem to be mediocre - but the purpose of this exerise was to familiarize ourselves with PySpark functionality and not to make a flawless ML-model.","metadata":{}},{"cell_type":"code","source":"result.select('prediction', 'score').sample(0.1).show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:00:59.850285Z","iopub.execute_input":"2021-06-19T16:00:59.850587Z","iopub.status.idle":"2021-06-19T16:01:00.158741Z","shell.execute_reply.started":"2021-06-19T16:00:59.850559Z","shell.execute_reply":"2021-06-19T16:01:00.157517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Not forget to stop SparkSession in the end!","metadata":{}},{"cell_type":"code","source":"spark.stop()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T16:07:28.493973Z","iopub.execute_input":"2021-06-19T16:07:28.494561Z","iopub.status.idle":"2021-06-19T16:07:29.603122Z","shell.execute_reply.started":"2021-06-19T16:07:28.494509Z","shell.execute_reply":"2021-06-19T16:07:29.601966Z"},"trusted":true},"execution_count":null,"outputs":[]}]}