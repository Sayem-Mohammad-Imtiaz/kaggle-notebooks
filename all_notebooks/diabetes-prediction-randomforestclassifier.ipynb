{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='begin'></a>\n# <h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:350%; text-align:center; border-radius: 15px 50px;\">Pima Indian</h1>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n<center><img\nsrc=\"https://www.legendsofamerica.com/wp-content/uploads/2018/12/PimaIndiansCarloGentile1870.jpg\" style=\"width:50%;height:50%;\">\n</center>"},{"metadata":{},"cell_type":"markdown","source":"### Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n<br>\n\n* **Pregnancies: Number of times pregnant**\n* **Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test**\n* **BloodPressure: Diastolic blood pressure (mm Hg)**\n* **SkinThickness: Triceps skin fold thickness (mm)**\n* **Insulin: 2-Hour serum insulin (mu U/ml)**\n* **BMI: Body mass index (weight in kg/(height in m)^2)**\n* **DiabetesPedigreeFunction: Diabetes pedigree function**\n* **Age: Age (years**)\n* **Outcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0**"},{"metadata":{},"cell_type":"markdown","source":"<a id='begin'></a>\n# <h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:350%; text-align:center; border-radius: 15px 50px;\">Load Data üìö</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport missingno as msno\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option(\"display.float_format\",lambda x: \"%.5f\" % x)\npd.set_option(\"display.max_rows\",None)\npd.set_option(\"display.max_columns\",None)\n\ndf = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='begin'></a>\n# <h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:350%; text-align:center; border-radius: 15px 50px;\">Check Data üîé</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(3))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(3))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is not possible for BMI and some variables to be zero. Values equal to zero in the data set are missing values. NA should be written instead of these values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We convert values with zero in variables to NaN values.\ncols = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\nfor col in cols:\n    df[col].replace(0, np.NaN, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.heatmap(df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='begin'></a>\n# <h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:350%; text-align:center; border-radius: 15px 50px;\">Data Preprocessing üõ†Ô∏è</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can fill in the NaN values with a median relative to the target.\nfor col in df.columns:\n    df.loc[(df[\"Outcome\"] == 0) & (df[col].isnull()), col] = df[df[\"Outcome\"] == 0][col].median()\n    df.loc[(df[\"Outcome\"] == 1) & (df[col].isnull()), col] = df[df[\"Outcome\"] == 1][col].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outliers visualization\nfor col in df.columns:\n    if col != \"Outcome\":\n        sns.catplot(\"Outcome\", col, data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize = (15,7));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outliers\ndef outlier_thresholds(dataframe, col_name, th1=0.05, th3=0.95):\n    quartile1 = dataframe[col_name].quantile(th1)\n    quartile3 = dataframe[col_name].quantile(th3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ndef replace_with_thresholds(dataframe, col_name, th1=0.05, th3=0.95):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, th1, th3)\n    if low_limit > 0:\n        dataframe.loc[(dataframe[col_name] < low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical columns\nnum_cols = [col for col in df.columns if df[col].dtypes in [int, float]\n            and df[col].nunique() > 10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Outliers\nfor col in df.columns:\n    print(check_outlier(df, col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace Outliers\nfor col in df.columns:\n    replace_with_thresholds(df, col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Outliers\nfor col in df.columns:\n    print(check_outlier(df, col))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='begin'></a>\n# <h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:350%; text-align:center; border-radius: 15px 50px;\"> Feature Engineering ‚öôÔ∏è</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoder(dataframe, binary_col):\n    labelencoder = preprocessing.LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\n\ndef rare_analyser(dataframe, target, rare_perc):\n    rare_columns = [col for col in dataframe.columns if dataframe[col].dtypes == 'O'\n                    and (dataframe[col].value_counts() / len(dataframe) < rare_perc).any(axis=None)]\n\n    for col in rare_columns:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() / len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() / len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n\n    return temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New categorical BMI\ndf['NEW_BMI_CAT'] = pd.cut(x=df['BMI'], bins=[0, 18.4, 25.0, 30.0, 70.0],\n                           labels=['weakness', 'normal', 'slightly_fat', 'obese']).astype('O')\n\n# New categorical Glucose\ndf['NEW_GLUCOSE_CAT'] = pd.cut(x=df['Glucose'], bins=[0, 139, 200],\n                               labels=['Normal', 'Prediabetes']).astype('O')\n\n#  New categorical BloodPressure\ndf['NEW_BLOOD_CAT'] = pd.cut(x=df['BloodPressure'], bins=[0, 79, 90, 123],\n                             labels=['Normal', 'Hypertension_S1', 'Hypertension_S2']).astype('O')\n\n# New categorical SkinThickness\ndf['NEW_SKINTHICKNESS_CAT'] = df['SkinThickness'].apply(lambda x: 1 if x <= 18.0 else 0)\n\n# New categorical Insulin\ndf['NEW_INSULIN_CAT'] = df['Insulin'].apply(lambda x: 'Normal' if 16.0 <= x <=166   else 'Abnormal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nlabel_cols = [col for col in df.columns if df[col].dtypes == 'O' and df[col].nunique() <= 2]\nfor col in label_cols:\n    label_encoder(df, col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One_hot Encoding\nohe_cols = [col for col in df.columns if 10 >= len(df[col].unique()) > 2]\ndf = one_hot_encoder(df, ohe_cols, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = [col.upper() for col in df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='begin'></a>\n# <h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:350%; text-align:center; border-radius: 15px 50px;\"> Modeling üß©</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[['OUTCOME']]\nX = df.drop('OUTCOME', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\nrf = RandomForestClassifier().fit(X_train, y_train)\ny_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_random_forest = round(rf.score(X_test, y_pred) * 100, 2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><img\nsrc=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTO00xMsiob_1AgrAfctXJ50--hHtXxBLg3uWJ1Guc4NGm9Y-61QnmuOagYXA2h0XaFkC0&usqp=CAU\" style=\"width:50%;height:50%;\">\n</center>"},{"metadata":{},"cell_type":"markdown","source":"# **Cross validation processes to prevent excessive learning.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_params_ = {'max_depth': [3, 6, 10, None],\n              'max_features': [3, 5, 15],\n              'n_estimators': [100, 500, 700],\n              'min_samples_split': [2, 5, 8],\n              'min_samples_leaf': [1, 3, 5]}\n\nrf_model = RandomForestClassifier(random_state=42)\n\nrf_cv_model = RandomizedSearchCV(rf_model, rf_params_, cv=5, n_jobs=-1, verbose=1).fit(X_train, y_train)\nrf_cv_model = RandomForestClassifier(**rf_cv_model.best_params_).fit(X_train, y_train)\ny_pred = rf_cv_model.predict(X_test)\nacc_random_forest = round(rf.score(X_test, y_pred) * 100, 2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}