{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, let's do anomaly detection on volume first.\n\nThen check future price movement by aftermath plot!"},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Analysis"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import sparse, stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 1 csv file in the current version of the dataset:\n"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check 1st file: /kaggle/input/all_japanese_stocks.csv"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/all_japanese_stocks.csv', delimiter=',')\ndf1.dataframeName = 'all_japanese_stocks.csv'\ncolumns = df1.columns.to_list()\ncolumns[0] = \"code\"\ndf1.columns = columns\ndf1[\"Date\"] = pd.to_datetime(df1[\"Date\"])\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df1.set_index([\"code\", \"Date\"])\ndf = df[(df[\"Volume\"] > 0) & (df[\"Close\"] < df[\"Close\"].rolling(10).mean()*2)]\ndf[\"TurnOver\"] = df[\"Close\"]*df[\"Volume\"]\nprint(df.shape)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"turnover = df[[\"TurnOver\"]].unstack().T\nturnover.index = turnover.index.get_level_values(1)\nturnover = turnover.sort_index()\nturnover = turnover.dropna(thresh=2000, axis=1).ffill().dropna()\nturnover.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"volume = df[[\"Volume\"]].unstack().T\nvolume.index = volume.index.get_level_values(1)\nvolume = volume.sort_index()\nvolume = volume.dropna(thresh=2000, axis=1).ffill().dropna()\nvolume.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_stocks = turnover.mean().nlargest(1000, keep='last')\ntarget_stocks.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"close_price = df[[\"Close\"]].unstack().T\nclose_price.index = close_price.index.get_level_values(1)\nclose_price = close_price.sort_index()\nclose_price = close_price.dropna(thresh=2000, axis=1).ffill().dropna()\nfirst_date = close_price.index[0]\nclose_price = close_price[target_stocks.index]\nclose_price.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"open_price = df[[\"Open\"]].unstack().T\nopen_price.index = open_price.index.get_level_values(1)\nopen_price = open_price.sort_index()\nopen_price = open_price.dropna(thresh=2000, axis=1).ffill().dropna()\nfirst_date = open_price.index[0]\nopen_price = open_price[target_stocks.index]\nopen_price.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Anomaly Detection\n\nIn this notebook, let's try to use anomaly detection by refering https://www.kaggle.com/liubenyuan/time-series-and-anomaly-detection\n\nAnomaly Detection (ad) Using hp filter and mad test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hodrick Prescott filter\ndef hp_filter(x, lamb=5000):\n    w = len(x)\n    b = [[1]*w, [-2]*w, [1]*w]\n    D = sparse.spdiags(b, [0, 1, 2], w-2, w)\n    I = sparse.eye(w)\n    B = (I + lamb*(D.transpose()*D))\n    return sparse.linalg.dsolve.spsolve(B, x)\n\n\ndef mad(data, axis=None):\n    return np.mean(np.abs(data - np.mean(data, axis)), axis)\n\n\ndef AnomalyDetection(x, alpha=0.2, lamb=5000):\n    \"\"\"\n    x         : pd.Series\n    alpha     : The level of statistical significance with which to\n                accept or reject anomalies. (expon distribution)\n    lamb      : penalize parameter for hp filter\n    return r  : Data frame containing the index of anomaly\n    \"\"\"\n    # calculate residual\n    xhat = hp_filter(x, lamb=lamb)\n    resid = x - xhat\n\n    # drop NA values\n    ds = pd.Series(resid)\n    ds = ds.dropna()\n\n    # Remove the seasonal and trend component,\n    # and the median of the data to create the univariate remainder\n    md = np.median(x)\n    data = ds - md\n\n    # process data, using median filter\n    ares = (data - data.median()).abs()\n    data_sigma = data.mad() + 1e-12\n    ares = ares/data_sigma\n\n    # compute significance\n    p = 1. - alpha\n    R = stats.expon.interval(p, loc=ares.mean(), scale=ares.std())\n    threshold = R[1]\n\n    # extract index, np.argwhere(ares > md).ravel()\n    r_id = ares.index[ares > threshold]\n\n    return r_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sample usage of this method"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\n\n# sample signals\nN = 1024  # number of sample points\nt = np.linspace(0, 2*np.pi, N)\ny = np.sin(t) + 0.02*np.random.randn(N)\n\n# outliers are assumed to be step/jump events at sampling points\nM = 3  # number of outliers\nfor ii, vv in zip(np.random.rand(M)*N, np.random.randn(M)):\n    y[int(ii):] += vv\n\n# detect anomaly\nr_idx = AnomalyDetection(y, alpha=0.1)\n\n# plot the result\nplt.figure()\nplt.plot(y, 'b-')\nplt.plot(r_idx, y[r_idx], 'ro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So let's use this anomaly detection method to volume data.\n\nIn this notebook, we use some techniques such as Walkforward to calculate anomaly step by step."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\n\ny = volume[target_stocks.index[0]]\n\nN = len(y)\nnum_data = 100\nstep = 1#N//10\n\nr_idx = []\nindex = y.index\n\nfor n in tqdm(range(num_data, N-num_data)):\n    _r_idx = AnomalyDetection(y[max(n-100, 0):n], alpha=0.1)\n    if len(_r_idx) > 0:\n        if _r_idx[-1] == index[n-1]:\n            r_idx.append(_r_idx[-1])\n\nfig, ax = plt.subplots(figsize=(15, 5))\n\nax.plot(y, color='b',linestyle='-', label=\"volume --code : {} --step : {}days\".format(target_stocks.index[0], step))\nax.scatter(r_idx, y[r_idx], color='r', marker='o')\nax.grid(True)\nax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aftermath = []\n\nprice = open_price[target_stocks.index[0]]\nfor index in r_idx:\n    aftermath.append(price[index:].iloc[1:31].values)\n\naftermath = pd.DataFrame(aftermath).T\naftermath /= aftermath.iloc[0]\nprint(aftermath.shape)\naftermath.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aftermath.plot(figsize=(15, 5), grid=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aftermath.mean(axis=1).plot(figsize=(15, 5), grid=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"up_aftermath = []\n\n_open_price = open_price[target_stocks.index[0]]\n_close_price = close_price[target_stocks.index[0]]\n\nfor index in r_idx:\n    if _close_price[index] > _open_price[index]:\n        up_aftermath.append(price[index:].iloc[1:31].values)\n\nup_aftermath = pd.DataFrame(up_aftermath).T\nup_aftermath /= up_aftermath.iloc[0]\nup_aftermath.plot(figsize=(15, 2), grid=True, alpha=0.5, legend=False)\nplt.show()\nup_aftermath.mean(axis=1).plot(figsize=(15, 2), grid=True, label=\"average\", color=\"black\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"down_aftermath = []\n\n_open_price = open_price[target_stocks.index[0]]\n_close_price = close_price[target_stocks.index[0]]\n\nfor index in r_idx:\n    if _close_price[index] < _open_price[index]:\n        down_aftermath.append(price[index:].iloc[1:31].values)\n\ndown_aftermath = pd.DataFrame(down_aftermath).T\ndown_aftermath /= down_aftermath.iloc[0]\ndown_aftermath.plot(figsize=(15, 2), grid=True, alpha=0.5, legend=False)\nplt.show()\ndown_aftermath.mean(axis=1).plot(figsize=(15, 2), grid=True, label=\"average\", color=\"black\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nAnomaly detection might be helpful to extract alpha.\n\nAs we see in the last graph, close price tends to go up for 30 days after there is an anomaly in volume.\n\nGood Luck!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}