{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport os\nfrom sklearn import preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of rows: {}, Number of columns = {}'.format(train.shape[0], train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.MonthlyCharges.value_counts()[train.MonthlyCharges.value_counts().index < 19]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train.TotalCharges[0])\ntrain.TotalCharges = train.TotalCharges.replace(' ', None)\ntrain.TotalCharges = train.TotalCharges.apply(lambda x: float(eval(x)))\ntrain.SeniorCitizen.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tenure.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers Detection\n\nThere are many methods that can be used to detect outliers in a dataset. In this workshop we will discuss the following:\n* Box Plot method\n* Standarization (Z-sore) method\n\n##### Box Plot :: Consists of five main components:\n* Q1, first quartile (Midean of the first half of the data)\n* Q2, Midean of the data\n* Q3, midean of the second half of the data\n* Max value\n* Min value\n\n##### Main equations in box plots:\n$$ IQR = Q3 - Q1 $$\n$$ Outliers = Q3 + 1.5 * IQR$$\n$$ Q1 - 1.5 * IQR $$\n\n##### Z-score method\nZ-score represents the number of standard deviations removed from the mean for each data point. In a simpler way, it is the distance for a point from the mean in standard deviations.\n$$ z-score = {x - mean \\over std} $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize = (20,10))\nax = fig.gca()\nsns.boxplot(data= train['TotalCharges'], orient=\"h\", palette=\"Set1\", ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20,10))\nax = fig.gca()\nsns.boxplot(data= train[['tenure', 'MonthlyCharges']], orient=\"h\", palette=\"Set1\", ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nrows = np.any(stats.zscore(train[['tenure', 'MonthlyCharges', 'TotalCharges']].values) > 2.5, axis=1)\noutliers = train.loc[rows]\noutliers.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tenure.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.TotalCharges.hist(figsize = (20,20), bins = 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variables\nThe column consists of two categories only, Y and N. Let's explore further if we can order those or just one-hot encode them.\n\n#### Note:\nOne hot encoding a feature adds new features for each unique category, so if you have only two catogries \"Y\" and \"N\" in Churn, you will have two new columns Y and N where Y feature will have 1s in the places diagnosis = \"Y\" and N feature will have 1's in the places diagnosis = \"N\"\n\n#### Example\n\nOne-hot encoding:\n\ndiagnosis &nbsp;&nbsp;&nbsp; Y | N <br>\nY &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nN &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         0 | 1 <br>\nN &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         0 | 1 <br>\nY &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nY &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\n\nLabel encoding: if Y is ranked lower than N: <br>\ndiagnosis  &nbsp;    diagnosis_new <br>\nY  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\nN  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            2 <br>\nN  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            2 <br>\nY  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\nY  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\n\n\nBut how can we really know if the data is ranked or not? This can be done using dommain knowledge of the data, for example most of the features are described in the main dataset page, You can also determine this in real life problems using your own knowledge of the problem and the data collected.\nHere in this data it is obvious that Y means the customer left and N means still a customer, also note that Churn is the target variable so I will go for label encoding the variable to get one output for each row."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nlabel_enc = preprocessing.LabelEncoder()\ntrain.Churn = label_enc.fit_transform(train.Churn)\nlabels = train.Churn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_enc.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['Female', 'Male']] = pd.get_dummies(train.gender)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['part_n', 'part_y']] = pd.get_dummies(train.Partner)\ntrain[['dep_n', 'dep_y']] = pd.get_dummies(train.Dependents)\ntrain[['phone_n', 'phone_y']] = pd.get_dummies(train.PhoneService)\ntrain[['senior', 'not-senior']] = pd.get_dummies(train.SeniorCitizen)\ntrain[['one_line', 'no_line', 'multi-line']] = pd.get_dummies(train.MultipleLines)\ntrain[['bt' ,'cc', 'ec', 'mc']] = pd.get_dummies(train.PaymentMethod)\ntrain[['mm' ,'oy', 'ty']] = pd.get_dummies(train.Contract)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_COLS = list(train.dtypes[train.dtypes != 'object'].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_COLS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new = train[NUM_COLS].drop(['Churn'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection\n\nLets now see if the new features we added have any segnificance for the extra tee model or not and how important are our features. We can check that through the Extra trees algorithm which can predict the useful features internally usign \"feature_importances\""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split( train_new.values, labels.values, test_size = 0.2, random_state=42 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\n\nimport matplotlib.pyplot as plt\n\n\ncorr = train.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=1, vmin = -1, center=0,\n            square=True, linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.ensemble import ExtraTreesClassifier\n# load the iris datasets\ndataset = datasets.load_iris()\n# fit an Extra Trees model to the data\nclf = ExtraTreesClassifier()\nclf.fit(x_train,y_train)\n# display the relative importance of each attribute\nz = clf.feature_importances_\n#make a dataframe to display every value and its column name\ndf = pd.DataFrame()\nprint(len(z))\nprint(len(list(train_new.columns.values)))\n\ndf[\"values\"] = z\ndf['column'] = list(train_new.columns.values)\n# Sort then descendingly to get the worst features at the end\ndf.sort_values(by='values', ascending=False, inplace = True)\ndf.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['Churn', 'gender']].groupby('gender').sum().plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SelectKBest selects features according to the k highest scores of a given scoring function \nfrom sklearn.feature_selection import SelectKBest # This models a statistical test known as ANOVA \nfrom sklearn.feature_selection import f_classif\n\nk_best = SelectKBest(f_classif, k = 10)\nk_best.fit_transform( x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_best.pvalues_ \np_values = pd.DataFrame({'column': train_new.columns, 'p_value': k_best.pvalues_})\np_values.sort_values('p_value')\np_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the hypothesis testing, the correlations and the model based feature selection we can conclude that the gender feature doesn't have a segnificant effect of the Churn feature. And it is also obvious that the Teneor and MonthlyCharges are the most important features from our analysis based on the above analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.classifier import StackingCVClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import  GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn import ensemble,model_selection,svm\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nc1 = ExtraTreesClassifier(n_estimators=700,bootstrap=True) \nmeta2 = ExtraTreesClassifier(n_estimators=200,bootstrap=True) \n\nc2 = RandomForestClassifier(n_estimators=500,bootstrap=True)\nc3 = XGBClassifier()\nc4 = svm.LinearSVC()\nc5 = GradientBoostingClassifier()\nc6 = AdaBoostClassifier()\nmeta = LogisticRegression()\n\netc = StackingCVClassifier(classifiers=[c1, c2, c3, meta, c5],use_probas=True,meta_classifier=meta2)\n\netc.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of classifier on training set: {:.2f}'.format(etc.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(etc.score(x_test, y_test) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\ndef objective(trial,data=train_new.values,target=labels.values):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    \n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 4000, 100),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n    }\n    \n    model = XGBClassifier(**param)  \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(test_x)\n    acc = accuracy_score(test_y, preds)\n    return acc\n\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'lambda': 0.019097303955226335, 'alpha': 6.255501364107075, 'colsample_bytree': 0.5, 'subsample': 0.7, 'learning_rate': 0.018, 'n_estimators': 3000, 'max_depth': 5, 'random_state': 24}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier(**param)\n\nclf.fit(x_train, y_train)\n\nprint('Accuracy of classifier on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}