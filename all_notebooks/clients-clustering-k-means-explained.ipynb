{"cells":[{"metadata":{},"cell_type":"markdown","source":"___\n<h1><center>  Clustering</center></h1>\n\n___\n\n## Customer Segmentation of a Wholesale Distributor\n\n### Description :\n\nThe objective of the problem is to separate the customers of a wholesale distributor into groups that are as homogeneous as possible but differ as much as possible in order to carry out different targeted actions for each of the groups.\n\nWe will use the * Wholesale customers * dataset. This dataset can be downloaded from the following path from the University of California Irvine (** Url: ** https://archive.ics.uci.edu/ml/datasets/Wholesale+customers)\n\n### Dataset Description:\n\nThe dataset has ** 8 descriptive variables X **.\n\nThe total number of samples is 440 clients.\n\n** Independent variables X: **\n\n\n1. FRESH: annual expense (CU) on fresh products (Continuous);\n1. MILK: annual expense (CU) on dairy products (Ongoing);\n1. GROCERY: annual expense (CU) on grocery products (Ongoing);\n1. FROZEN: annual expenditure (CU) on frozen products (Continuous)\n1. DETERGENTS_PAPER: Annual expenditure (CU) on detergents and paper products (Ongoing)\n1. DELICATESSEN: annual expense (CU) on delicatessen products (Continuous);\n1. CHANNEL: Customer channel - Horeca (Hotel / Restaurant / Café) or Retail channel (Nominal)\n1. REGION: Client region - Lisnon, Porto or Others (Nominal)\n\n**More details:**\n\nThere are two categorical or nominal variables, \"REGION\" and \"CHANNEL\".\n\nREGION Frequency\n* Lisbon 77\n* Oporto 47\n* Other Region 316\n\nCHANNEL Frequency\n* Horeca 298\n* Retail 142"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import boxcox, probplot, norm, shapiro\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler\nfrom sklearn.cluster import KMeans\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Main function for plot. Usefull for other case of clustering.\n\ndef comprueba_normalidad(df, return_type='axes', title='Comprobación de normalidad'):\n    '''\n    '''\n    fig_tot = (len(df.columns))\n    fig_por_fila = 3.\n    tamanio_fig = 4.\n    num_filas = int( np.ceil(fig_tot/fig_por_fila) )    \n    plt.figure( figsize=( fig_por_fila*tamanio_fig+5, num_filas*tamanio_fig+2 ) )\n    c = 0 \n    shapiro_test = {}\n    lambdas = {}\n    for i, col in enumerate(df.columns):\n        ax = plt.subplot(num_filas, fig_por_fila, i+1)\n        probplot(x = df[df.columns[i]], dist=norm, plot=ax)\n        plt.title(df.columns[i])\n        shapiro_test[df.columns[i]] = shapiro(df[df.columns[i]])\n    plt.suptitle(title)\n    plt.show()\n    shapiro_test = pd.DataFrame(shapiro_test, index=['Test Statistic', 'p-value']).transpose()\n    return shapiro_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center> First Step </center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = pd.read_csv('../input/uci-wholesale-customers-data/Wholesale customers data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapeo los datos\nXY['Channel'] = XY['Channel'].map({1:'Horeca', 2:'Retail'})\nXY['Region'] = XY['Region'].map({3:'Other Region', 2:'Oporto', 1: 'Lisboa'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center> GRAPHICS </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"I save in a variable ** X_cuants ** only the numeric variables, since I am going to represent them and apply some transformation on them."},{"metadata":{"trusted":true},"cell_type":"code","source":"XY_cuants = XY[['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY_normalizado = (XY_cuants-XY_cuants.mean())/XY.std()\n# This function, let as see a more ordered graph. \n# try not to use it yourself and see how the graph changes ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nax = sns.boxplot(data=XY_normalizado)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representación de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nax = sns.boxplot(data=XY)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representación de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')\n#For this case there are not so much difference. But always its a good idea tried it. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Representation of the distributions of the variables using histograms.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,20))\nn = 0\nfor i, column in enumerate(XY_cuants.columns):\n    n+=1\n    plt.subplot(5, 5, n)\n    sns.distplot(XY_cuants[column], bins=30)\n    plt.title('Distribución var {}'.format(column))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center> Representation of the correlation Matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"matriz_correlaciones = XY.corr(method='pearson')\nn_ticks = len(XY.columns)\nplt.figure( figsize=(9, 9) )\nplt.xticks(range(n_ticks), XY.columns, rotation='vertical')\nplt.yticks(range(n_ticks), XY.columns)\nplt.colorbar(plt.imshow(matriz_correlaciones, interpolation='nearest', \n                            vmin=-1., vmax=1., \n                            cmap=plt.get_cmap('Blues')))\n_ = plt.title('Matriz de correlaciones de Pearson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center> Data Transformation to find the hypothesis."},{"metadata":{},"cell_type":"markdown","source":"As we are going to apply a K-means algorithm later, the data must meet a series of hypotheses.\n\n* The K-means assumes that the data have a ** normal distribution **.\n* Also, it is very prone to ** outliers **.\n\nTherefore, we must transform the variables so that they follow a normal distribution and treat the outliers."},{"metadata":{},"cell_type":"markdown","source":"# Data normalization:"},{"metadata":{},"cell_type":"markdown","source":"Variable normalization is the process in which a variable is transformed to follow a normal or Gaussian distribution.\n\nIn general, we will only want to normalize the data if we are going to use a machine learning algorithm or a statistical technique that assumes that the data is distributed in a Gaussian or normal way. For example, student's t tests, ANOVAs, linear regressions, logistic regressions, linear discriminant analysis (LDA), k-means, etc.\n\nAmong the ways to transform a variable to normal are methods such as the Box-Cox transformation or the Yeo-Johnson method."},{"metadata":{},"cell_type":"markdown","source":"The following graphs represent the <a href='https://es.wikipedia.org/wiki/Gr%C3%A1fico_Q-Q'>Q-Q Plot</a>, which is a graph that compares between two distributions. In this case, each of the variables with a normal distribution. If they follow the same distribution, the points fall close to the red line."},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro_test = comprueba_normalidad(XY_cuants, title='Normalidad variables originales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All variables are statistically significantly not distributed as a normal.\n\n** Shapiro-Wilk test: ** If the p-value is less than a significance level $ \\ alpha $, it is concluded that the distribution does not come from a normal one."},{"metadata":{},"cell_type":"markdown","source":" Now I transform the variables with a Box-Cox transform."},{"metadata":{"trusted":true},"cell_type":"code","source":"bc = PowerTransformer(method='box-cox')\nX_cuants_boxcox = bc.fit_transform(XY_cuants)\nX_cuants_boxcox = pd.DataFrame(X_cuants_boxcox, columns=XY_cuants.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro_test = comprueba_normalidad(X_cuants_boxcox, title='Normalidad variables transformadas')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looks perfect ¡","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The normality statistic is very high in all variables now, so we continue with this transform."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,20))\nn = 0\nfor i, column in enumerate(X_cuants_boxcox.columns):\n    n+=1\n    plt.subplot(4, 4, n)\n    sns.distplot(X_cuants_boxcox[column], bins=30)\n    plt.title('Distribución var {}'.format(column))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nNow, the distributions look Gaussian."},{"metadata":{},"cell_type":"markdown","source":"<h1><center> Outliers:"},{"metadata":{},"cell_type":"markdown","source":"\nAnother treatment that we must do is to treat outliers or atypical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=X_cuants_boxcox)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representación de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in list(X_cuants_boxcox.columns):\n    IQR = np.percentile(X_cuants_boxcox[k],75) - np.percentile(X_cuants_boxcox[k],25)\n    \n    limite_superior = np.percentile(X_cuants_boxcox[k],75) + 1.5*IQR\n    limite_inferior = np.percentile(X_cuants_boxcox[k],25) - 1.5*IQR\n    \n    X_cuants_boxcox[k] = np.where(X_cuants_boxcox[k] > limite_superior,limite_superior,X_cuants_boxcox[k])\n    X_cuants_boxcox[k] = np.where(X_cuants_boxcox[k] < limite_inferior,limite_inferior,X_cuants_boxcox[k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=X_cuants_boxcox)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representación de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No Outliers now. Ok, next step. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I create dummies of categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#In df one the two initial categorical variables and the transformed numeric variables\ndf  =  pd.concat([XY[['Channel','Region']],X_cuants_boxcox],axis=1)\ndf[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df,columns=['Channel','Region'],drop_first=True)\ndf[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Pre-scaling the data:"},{"metadata":{},"cell_type":"markdown","source":"We must scale the data when we use methods based on distance measurements, such as SVMs, K-NNs, or K-means. In these algorithms, a \"1\" unit change in a numeric variable is given equal importance regardless of the variable.\n\nFor example, we can look at prices in different currencies. A dollar is much more than a Yen, so if there are two products in different currencies, the algorithm will give the same importance to an increase of one Yen as that of a dollar.\n\nIn this case, since everything is spent on products in the same currency, it would not be strictly necessary. However, when scaling we are comparing ranges of variables. That is, customer C is one of those who spends more or spends less on a product."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0, 1))\nX_escalado = scaler.fit_transform(df)\nX_escalado = pd.DataFrame(X_escalado,columns=df.columns)\nX_escalado.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center> Segmentation using K-means clustering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, with the next code, we are looking for the best number of cluster for our dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_range = range(1,20)\ncluster_wss=[] \nfor cluster in cluster_range:\n    model = KMeans(cluster)\n    model.fit(X_escalado)\n    cluster_wss.append(model.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10,6])\nplt.title('Curva WSS para encontrar el valor óptimo de clústers o grupos')\nplt.xlabel('# grupos')\nplt.ylabel('WSS')\nplt.plot(list(cluster_range),cluster_wss,marker='o')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe graph assumes the optimal point when the curve creates a bend. In this case it would be about 4-6 groups."},{"metadata":{},"cell_type":"markdown","source":"We will choose the number of groups at 6, but what is usually done is to try several and see if the final results make sense from a business point of view, as I will comment later."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KMeans(n_clusters=6,random_state=0)\nmodel.fit(X_escalado)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### I predict and get customers with your prediction"},{"metadata":{},"cell_type":"markdown","source":"I create a dataframe with all the variables and a new one that is the prediction of the assigned cluster:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Original Dataset with the predictions\ndf_total = XY.copy()\ndf_total['cluster']=model.predict(X_escalado)\ndf_total[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.cluster.value_counts().plot(kind='bar', figsize=(10,4))\nplt.title('Conteo de clientes por grupo')\nplt.xlabel('Grupo')\n_ = plt.ylabel('Conteo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here, we coud see our clients inside of a cluster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now, we have to obtain the characteristic of each group to find the hide informtion inside our DF and give value to our analysis."},{"metadata":{},"cell_type":"markdown","source":"I also get a dataframe with the means of the variables in each group. This would represent each of the groups.\n\nThis is very necessary since the actions that the objective of this problem would be to do actions to each of the groups separately. For this, it is very important to know what each group is like, in order to act differently."},{"metadata":{"trusted":true},"cell_type":"code","source":"descriptivos_grupos = df_total.groupby(['cluster'],as_index=False).mean()\ndescriptivos_grupos","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I explain the groups using the means of each variable per group: ¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.groupby('cluster').mean().plot(kind='bar', figsize=(15,7))\nplt.title('Gasto medio por producto en cada clúster')\nplt.xlabel(u'Número de clúster')\n_ = plt.ylabel('Valor medio de gasto')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, for each of the groups, I obtain their average expenditure on each product.\n\nAs an annotation ... behavior could also be analyzed by dividing the groups into their two categorical variables and analyzing the average expenses, in this way segmentations would be made by channel, geography and customer characteristics."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total[:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thats was all. Any questions or suggestion will be welcome. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}