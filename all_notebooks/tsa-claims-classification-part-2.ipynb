{"cells":[{"metadata":{"_cell_guid":"892d9b7f-856a-4bec-b91b-8343dc2ec0d6","_uuid":"a4275b70a2223e538c31f27460f6cb0e3986a88d"},"cell_type":"markdown","source":"# TSA Claims Classification (Approve / Settle / Deny)  \n## (Part 2/2)\n\n## Introduction\nIn this kernel, we continue to work toward building a model to predict whether a TSA claim is approved, settled, or denied. This is mostly a practice exercise, but it could have some very neat real-world uses!   \n  \nThis is a continuation from part one [>here<](https://www.kaggle.com/perrychu/tsa-claims-classification-part-1)\n\nIn part one, we covered data cleaning, resulting in tsa_claims_clean.csv. Now we'll pull in the clean dataset as an input and us it for feature engineering and modeling."},{"metadata":{"_cell_guid":"5d11821f-687f-4870-8d55-1b7381ef3be6","_uuid":"82c60ece03d32c424baf284bf3fe6dd7e4f3af08"},"cell_type":"markdown","source":"## Setup\nLet's start by importing the necessary packages:"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T16:00:04.504375Z","end_time":"2018-02-21T16:00:05.270768Z"},"collapsed":true,"_cell_guid":"519ede40-d4fe-4bd7-9bd8-c6b6fb9a903c","_uuid":"feb7a32015ed8265e61678c583d4a81bd92871c3","trusted":true},"cell_type":"code","source":"#Data / vizualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Feature engineering\nfrom datetime import datetime\nfrom collections import defaultdict\n\n#Modeling\n##Utilities\nfrom sklearn.model_selection import KFold, train_test_split\nimport sklearn.metrics as skm\nimport pickle\n##Models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.svm import SVC\nimport xgboost as xgb","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"b1886246-2b42-431f-b641-970acf9b9a88","_uuid":"12bd58e9ff55ea207e3cb1dd6fc976ab4799ff37"},"cell_type":"markdown","source":"And load our clean dataset:"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T16:00:05.27832Z","end_time":"2018-02-21T16:00:05.722445Z"},"_cell_guid":"c5b2229f-e8cf-42d1-8b21-ce0fd1252d58","_uuid":"be6ce263ac4abc85a6ddb59dbbbd0a563a743eb2","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/tsa-claims-classification-part-1/tsa_claims_clean.csv\", low_memory=False)\n\nprint(len(df))\ndf.head(3)","execution_count":2,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-02-16T04:18:09.747707Z","end_time":"2018-02-16T04:18:09.767772Z"},"_cell_guid":"0ebca269-cffe-4ef7-aeca-fb738d2f599f","_uuid":"957dcc29e21a5306c2d35d6e23946368e5491e26"},"cell_type":"markdown","source":"# Part 2: Feature Engineering"},{"metadata":{"_cell_guid":"7712319e-785e-4e9a-a08c-32a96d0d5255","_uuid":"66c43d07c5fc14ff46e94fd8548aaaced08c8f87"},"cell_type":"markdown","source":"## Make date variables\nFirst let's expand our date strings into DateTime objects, and then convert dates to useful numeric features.\n\nWe have two dates - when an incident happened, and when the claim for that incident was received. Using the full date is likely too specific - it doesn't generalize well for future claims. \n\nSpecifically, we are going to leave out the year field because it is capturing forward progress in time rather than recurring patterns. For example, knowing the weather for each of 12 months in 2008 may not be very helpful for predicting weather in 2020... however knowing weather for 12 Januaries from 2008-2019 is much more likely to be useful for predicting weather in January 2020.\n\nExcluding year for each date, we can look at the month [0-12], day of the month [0-31], and day of the year [0-366]. These features should capture any seasonal or annually recurring trends in the data.\n\nNext, we can also look at the difference between the dates - in other words, how long someone waited before they reported their claim.\n\nThere are probably other ways to think about feature engineering, but this feels like a decent set for now."},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T16:00:05.725786Z","end_time":"2018-02-21T16:00:07.077315Z"},"collapsed":true,"_cell_guid":"05ee6257-9b43-4da5-a2b7-c9030d9fd74d","_uuid":"2c496a20ec61a2fd1d185b6a49e0ef4f49a989bd","scrolled":false,"trusted":true},"cell_type":"code","source":"df[\"Date_Received\"] = pd.to_datetime(df.Date_Received,format=\"%Y-%m-%d\")\ndf[\"Month_Received\"] = df.Date_Received.dt.month\ndf[\"DayMonth_Received\"] = df.Date_Received.dt.day\ndf[\"DayYear_Received\"] = df.Date_Received.dt.dayofyear\n\ndf[\"Incident_Date\"] = pd.to_datetime(df.Incident_Date,format=\"%Y-%m-%d\")\ndf[\"Incident_Month\"] = df.Incident_Date.dt.month\ndf[\"Incident_DayMonth\"] = df.Incident_Date.dt.day\ndf[\"Incident_DayYear\"] = df.Incident_Date.dt.dayofyear\n\ndf[\"Report_Delay\"] = (df.Date_Received - df.Incident_Date).dt.days\n\ndate_var = [\"Report_Delay\",\n        \"Month_Received\",\"DayYear_Received\",\"DayMonth_Received\",\n        \"Incident_Month\",\"Incident_DayYear\",\"Incident_DayMonth\"]","execution_count":3,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-02-17T01:49:54.782472Z","end_time":"2018-02-17T01:49:54.797612Z"},"_cell_guid":"db842092-4293-4c56-bcc9-ca43e76cdb84","_uuid":"7bd3a818fb6d5f03e7aaec36af59deb58a6c0cd7"},"cell_type":"markdown","source":"## Convert text categories to numeric\nNext, we need to modify our categorical text data - Airports, Airlines, Claim Type, and Claim Site. Our models require numbers as input - we would like to represent items in the same category (e.g. United Airlines, or LAX) with a similar number. \n\nOne way to do this is to create dummy variables (aka one-hot encoding). In this method, we would create one column for each unique label in a category (United Airlines, American Airlines, Delta Airlines, etc.) and put 1 in the column correspoding to the category of that row. For our data, a few columns have a ton of possible values... which creates a ton of columns. We can try this approach anyway, but there is probably a better way.\n\nAnother option is to give each category a numeric rank. For example, if there are 200 possible airlines, we could number them from 1 to 200. However, if our models will learn using this ordering, we want the ordering to make sense! So we can do the ordering in a determined way, like ranking them by how often they appear in the claims data. For example, if United appears in the most claims, we give assign it the number 1, then 2 for second most, 3 for third most, etc.\n\nAlong the same lines, we could simply use the frequency count without flattening into rank. For example if United appears in 10k claims, then we assign it the number 10k, then if the next most is 9k claims we assign it 9k, etc. Under this plan, the number assigned to categories may not be unique. For example, two airlines may both appear in 10 claims. However, this is ok for our application - instead of differentiating by distinct airline this feature is now differentiating by airline popularity. \n\nOk, so lets actually go through appllying each of these methods.\n"},{"metadata":{"_cell_guid":"8199bf96-b3d2-4c45-ac45-da3ac7c85289","_uuid":"ee00a2d8a161fd70cab23a5dd9e279c37d15f067"},"cell_type":"markdown","source":"### Helper functions\nLets break out the calculation details into helper functions so our code is more easily understood.\n\nThe first two functions calculate mappings from text category to frequency rank and frequency count.\n\nThe third function applies a calculated mapping to a column.\n\nThe fourth function does a few things:\n* runs the mapping calculation\n* applies the mapping\n* repeats for multiple specified columns\n* inserts the new mapped columns in the data frame with a new name to avoid overwriting the original text data\n\nIt also nicely handles taking in a training set vs. testing set."},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T16:00:07.087307Z","end_time":"2018-02-21T16:00:07.187962Z"},"collapsed":true,"_cell_guid":"3b11dfbb-d200-4f5f-9201-975d55f8ff4b","_uuid":"333c0e575e7750021a2f2d141a073cee099c0b99","scrolled":false,"trusted":true},"cell_type":"code","source":"#Frequency rank\n#Default is max rank + 1\ndef get_count_rank(var_column):\n    val_count = var_column.value_counts(dropna=False)\n    conversion_dict = defaultdict(lambda: len(val_count)+1, zip(val_count.index, range(len(val_count.values))))\n        \n    return conversion_dict\n    \n#Frequency\n#Default is zero\ndef get_count(var_column):\n    val_count = var_column.value_counts(dropna=False)\n    conversion_dict = defaultdict(lambda: 0, zip(val_count.index, val_count.values))\n    \n    return conversion_dict\n\n#Apply conversion from text to numeric\ndef apply_conversion(var_column, conversion_dict):\n\n    return var_column.map(lambda x: conversion_dict[x])\n\ndef create_numeric(train_df, test_df, conversion_func, columns, postfix=\"_\"):\n    '''\n    Applies conversion function over the training set to train and test sets.\n    Inputs:\n        train_df: training data (input to conversion func, then applied)\n        test_df: test data (conversion applied)\n        conversion_func: returns a map assigning numeric value to each text category\n        columns: columns to apply\n        postfix: postfix to add to created columns (may overwrite, e.g. on empty string)\n        \n    '''\n    maps = {}\n    new_columns = []\n    \n    for name in columns:\n        new_name = name+postfix\n        new_columns.append(new_name)\n        \n        maps[name] = conversion_func(train_df[name])\n        \n        train_df[new_name] = apply_conversion(train_df[name], maps[name])\n        test_df[new_name] = apply_conversion(test_df[name],maps[name])\n\n    text_count_var = [x + \"_Count\" for x in string_categories]\n    \n    return train_df, test_df, new_columns\n","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"6dc4dd53-7db9-4cee-b9ca-1173c120eff3","_uuid":"7fbd108d38d943c9728fadc810223fefa8a8e638"},"cell_type":"markdown","source":" ### Conversion \nNow we  apply each of the conversions!\n\nFirst we'll split the data into a training/validation set and a holdout test set. The conversions will be calculated on the training set and then applied to both training and holdout sets. "},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T16:00:07.200155Z","end_time":"2018-02-21T16:00:08.00744Z"},"_cell_guid":"274f5202-d76f-4d8b-8f2d-4da6768c65a3","_uuid":"ee4df9ab177aa9fdaa26b4e2ab1ddf3de11da2ab","trusted":true},"cell_type":"code","source":"df, df_holdout = train_test_split(df, test_size = .2, shuffle = True, random_state = 1)\n\nstring_categories = [\"Claim_Type\",\"Claim_Site\",\"Airport_Code_Group\",\"Airline_Name\"]\n\n#Category complaint counts\ndf, df_holdout, text_count_var = create_numeric(df, df_holdout, get_count, string_categories, \"_Count\")\n\n#Category complaint rank\ndf, df_holdout, text_rank_var = create_numeric(df, df_holdout, get_count_rank, string_categories, \"_Rank\")\n\n\nprint(\"Training / Validation:\", len(df), \"\\nTest:\", len(df_holdout))\ndf_holdout.head(5)","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"b4f68b7b-3860-47c2-b374-d2b21b45e296","_uuid":"4fe01ccc1844e5076e9c019380d27abe814b3d4a"},"cell_type":"markdown","source":"# Part 3: Modeling\n\nNow that we've got our features, we can finally do some modeling. There are two things we want to optimize - the set of features to use, and the model to use on those features. Ideally we would do a giant grid search iterating over every model and every set of features... however, that would take too long to run, and might not be very interpretable.\n\nInstead, we'll first do two distinct steps:\n* Feature selection - look at how a baseline model performs with different feature sets \n* Model selection - look at how different models perform with our best feature set \n\nNote: After feature selection, we may consider going back to feature engineering (depending on our results and how much time we have!). This is an interative process, not a one-way street!\n\nFor each run, we'll track multiple performance metrics: Accuracy (global), Precision / Recall / F1 (for each class). I'm going to assume we're familiar with these classification metrics - but its worth reading about if you're not! "},{"metadata":{"_cell_guid":"c090b89b-ab9d-453a-9a65-da5e1e88bb8c","_uuid":"39a9a466d5b25addf0eddd441fa2599cc0ea5ee9"},"cell_type":"markdown","source":"## Helper fuctions\nAgain, we'll use some helper functions to clean up our code.\n\nThe first function neatly prints out the scoring metrics.\n\nThe second function executes a cross-validation loop for a given model and dataset, then calls the first function to print the resulting average scores."},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T01:27:54.338962Z","end_time":"2018-02-21T01:27:54.46263Z"},"collapsed":true,"_cell_guid":"45d8db5c-c600-402d-8698-7ba2a10cfd87","_uuid":"2e101fe6cae88f8778ce7d92b136a36deebb19cf","trusted":true},"cell_type":"code","source":"def print_scores(scores_array, ylabel_strings):\n    ''' Prints a table with headings for output of sklearn.metrics.precision_recall_fscore_support\n        Inputs: scores_array -> np.array of scores from skm.\n                ylabel_strings -> the target labels\n    '''\n    #Each row is a score in the output, transpose to get features across rows\n    array = np.transpose(scores_array) \n    macro_avg = np.average(array,axis=0)\n    labels = sorted(ylabel_strings)\n    \n    max_len = str(np.max([len(s) for s in ylabel_strings]))\n        \n    print((\"\\n{:>\"+max_len+\"} {:>10s} {:>10s} {:>10s} {:>10s}\").format(\"\",\"Precision\",\"Recall\",\"F1\",\"Support\"))\n    \n    for i in range(len(labels)):\n        print((\"{:>\"+max_len+\"} {:>10.5f} {:>10.5f} {:>10.5f} {:>10.0f}\")\n              .format(labels[i],array[i][0],array[i][1],array[i][2],array[i][3]))\n    \n    print((\"{:>\"+max_len+\"} {:>10.5f} {:>10.5f} {:>10.5f} {:>10.0f}\")\n          .format(\"Avg/Tot\",macro_avg[0],macro_avg[1],macro_avg[2],macro_avg[3]))\n    \ndef validation_loop(model,X,Y,k=5,rand_state=1):\n    ''' Runs k-fold validation loop for input model, X, Y. Prints classification accuracy \n             and the following per-label metrics: precision, recall, f1, support.\n        Inputs: \n                ylabel_strings -> the target labels\n    '''\n    test_accs, test_scores = [], []\n    train_accs, train_scores = [], []\n    \n    i=1\n\n    for train_ind, test_ind in KFold(k,shuffle=True,random_state=rand_state).split(X,Y):\n        #print(\"Starting {} of {} folds\".format(i,k))\n\n        model.fit(X[train_ind],Y[train_ind])\n        \n        #Test metrics\n        pred = model.predict(X[test_ind])\n        acc = skm.accuracy_score(Y[test_ind],pred)\n        test_accs.append(acc)\n        score = skm.precision_recall_fscore_support(Y[test_ind],pred)\n        test_scores.append(score)\n        \n        #Train metrics\n        pred = model.predict(X[train_ind])\n        acc = skm.accuracy_score(Y[train_ind],pred)\n        train_accs.append(acc)\n        score = skm.precision_recall_fscore_support(Y[train_ind],pred)\n        train_scores.append(score)\n        \n        i+=1\n    \n    print(\"\\nAvg. Train Metrics\")\n    print (\"Accuracy: {:.5f}\".format(np.average(train_accs)))\n    print_scores(np.average(train_scores,axis=0),np.unique(Y))\n    \n    print(\"\\nAvg. Validation Metrics\")\n    print (\"Accuracy: {:.5f}\".format(np.average(test_accs)))\n    print_scores(np.average(test_scores,axis=0),np.unique(Y))\n    \n    ","execution_count":25,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-02-16T04:18:27.088723Z","end_time":"2018-02-16T04:18:27.099785Z"},"_cell_guid":"9824f257-98e6-4525-a553-585d60f153da","_uuid":"4261677726fbf05ffd29e7fca35f52b7cedfc03d"},"cell_type":"markdown","source":"## Feature Selection\n\nWe'll use random forest (with 250 trees) as our baseline model. I picked random forest because it tends decent results with default parameters on most datasets, and I suspect our data set has a lot of non-linear relationships which are more difficult for logistic regression to handle. The downside is RF takes longer to run.\n\n### Baseline\nWe'll baseline with only one feature (claim value).\n\nRandom Forest ends up performing slightly better (~51.7% accuracy) than naively guessing the most common class \"Denied\" (46.7%). It manages to pick up signal on the minority classes (Approved / Settled) so this is much more useful than the naive guess model.\n\nNote: The cross-validation function prints both training and validation set metrics. The validation set metrics are what matters for picking between models. Training set metrics are used as a rough reference for where we are on the bias-variance curve (and should always be higher). Bias-variance is worth learning about if you're not familiar!"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T22:36:02.943651Z","end_time":"2018-02-20T22:36:58.419386Z"},"_cell_guid":"e7ca0354-020c-4ecf-a7f9-e4e4e3b8a8b5","_uuid":"d6280b8460d5cf8eaf08a67bbbb9175cc62bfc54","scrolled":false,"trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators=250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":7,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-02-16T23:25:22.554956Z","end_time":"2018-02-16T23:25:22.566726Z"},"_cell_guid":"9b5e649f-69bf-4d98-864d-c63ca1a8638d","_uuid":"353bf2f71ea99d149c2393f0ab13c27edf14c40f"},"cell_type":"markdown","source":"### Base + Date Features\nLet's try adding Date features. Our results improve a little, to ~52.7% accuracy."},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T07:01:58.891279Z","end_time":"2018-02-20T07:10:25.846154Z"},"_cell_guid":"ee39f68b-fdcf-4db1-ad4f-019ebc56f25c","_uuid":"99692202914346f92a514ff795573301c3506c49","scrolled":true,"trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators = 250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"54087df9-adc1-4c57-8a1d-4e0c739955ab","_uuid":"35a0a7b586186f68f79c9af992ceb624138a6dc7"},"cell_type":"markdown","source":"### Base + Categorical Features\nLet's try adding our Categorical features. Here, we had a few different representations. Let's try each style separately (and a combination), then use the best one.\n\nResults (accuracy):\n* Dummy Vars: ~53.69%\n* Frequency Count: ~54.22%\n* Frequency Rank: ~54.19%\n* Count & Rank: ~54.16%\n\nFrequency Count is best, so we'll use that going forward."},{"metadata":{"_cell_guid":"dfceb488-225f-422d-bf85-9075070a5b87","_uuid":"339d8b7fff4a5c714b19c6b1db418e6669af64fd"},"cell_type":"markdown","source":"#### Dummy Vars (aka One-hot encoding)"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T07:30:51.062339Z","end_time":"2018-02-20T08:02:39.251454Z"},"_cell_guid":"012e5792-24e9-4116-82e0-c49ce649f571","_uuid":"024e5e4301639d38b9c91a11aff05eb37df5b909","trusted":true},"cell_type":"code","source":"dummies_df = pd.get_dummies(df[[\"Claim_Type\",\"Claim_Site\",\"Airport_Code_Group\",\"Airline_Name\"]],prefix=[\"Type\",\"Site\",\"Airport\",\"Airline\"])\n\nfeatures = [\"Claim_Value\"] + list(dummies_df.columns)\ntarget = \"Status\"\n\nmodel_df=df[[\"Status\",\"Claim_Value\"]].join(dummies_df).dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators = 250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":19,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T05:21:37.92602Z","end_time":"2018-02-20T05:21:37.931205Z"},"_cell_guid":"4f5fa949-4dbd-4716-94a2-a7b203286dcd","_uuid":"4e84bbd400d7fedf2a3c84666022ad6a1e7c1475"},"cell_type":"markdown","source":"#### Frequency Count"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T08:02:39.554296Z","end_time":"2018-02-20T08:10:32.695461Z"},"_cell_guid":"d1e185fb-c7d1-4717-a339-25e30148f6aa","_uuid":"b67df222f796f23bc59d1529a749d4814c3a547e","trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"] + text_count_var \ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators = 250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"a118d533-1100-4d98-9267-abc02c542306","_uuid":"87ff2c0bbfb955cbe46f8275cdf936a0ffff0903"},"cell_type":"markdown","source":"#### Frequency Rank"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T08:10:33.130655Z","end_time":"2018-02-20T08:18:08.108197Z"},"_cell_guid":"6d42aea9-1939-43cf-a509-76ed718333bc","_uuid":"9af98b2862578da7e60ab0d9c58fedccaf0fc907","trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"] + text_rank_var\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators = 250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"60c9fcbc-3ea6-40f1-85c4-da1ff81044ac","_uuid":"48c86ae87af9f2879e8a4a60ce829b8d8710b59c"},"cell_type":"markdown","source":"#### Frequency Count + Rank"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T08:18:08.542287Z","end_time":"2018-02-20T08:26:15.414187Z"},"_cell_guid":"5e6abfe6-8088-4443-89d2-cc6a9ae19bd8","_uuid":"b52e60d4133d75a1c3568641ce3c85122c8ae582","scrolled":true,"trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"] + text_count_var + text_rank_var\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators = 250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"9e91088b-0097-426e-b2d8-cacc19366925","_uuid":"ab78d7e72af271fb938c8482bcb33215fdf3dfa8"},"cell_type":"markdown","source":"### Base + Date + Categorical Features\nLet's try all of the features (using our best categorical representation).\n\nAccuracy is 56.5% - we'll use these features for model selection."},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T22:43:30.53392Z","end_time":"2018-02-20T22:44:53.451289Z"},"_cell_guid":"10a6ecbe-52d5-4649-b32a-8d47e9943303","_uuid":"125a3fa3bb9d301716bb9517fd94fa8315f2eed3","trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var+text_count_var\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators = 250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"371dc0e8-92c4-47e8-b489-6eda5456fc5d","_uuid":"43e4f7514a358d28bf2c76d12b92e4720a8ccc9c"},"cell_type":"markdown","source":"### Feature Selection Results\n* Baseline: 51.4%\n* Base + Date: 52.7%\n* Base + Category: 54.2%\n* Base + Category + Date: 56.5%\n\nThe best result was using all of the features. If desired, we could go back and create more features, then do more validation to figure out whether those features improve the model.\n\nFor now, lets stick with what we have. We can record this data set as a .csv for future use."},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T06:28:06.629919Z","end_time":"2018-02-21T06:28:07.9667Z"},"collapsed":true,"_cell_guid":"206b8aa4-dc9b-4128-8c51-5e1c6ccf2ee9","_uuid":"c049cf6f01b9ab3dc73e42c0ebf26aa4c7f9e342","trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var+text_count_var\ntarget = \"Status\"\ncsv_df=df[[target]+features].dropna()\n\ncsv_df.to_csv(\"tsa_model_features.csv\",index=False)","execution_count":14,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T08:39:43.826987Z","end_time":"2018-02-20T08:39:43.831983Z"},"_cell_guid":"83ed12a8-ff10-43c3-ada8-147baca203a6","_uuid":"a3f54b7703428bec57a6e69656f997d6a6805c90"},"cell_type":"markdown","source":"## Model Selection\nNow that we have a solid feature set, lets test some other models.\n\nOur dataset seems non-linear, which tree-based models should handle best. Aside from Random Forest, we can try gradient boosted trees. We'll use the XGBoost implementation.\n\nWe can also try two linear models - Logistic Regression and Naive Bayes - just to see how they do."},{"metadata":{"_cell_guid":"fcf579fc-197e-41f7-b8a0-5eb4e6840bb4","_uuid":"3ec6a91ba16d9b2abb8fb7995f22003e0786e4a1"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T23:02:03.507798Z","end_time":"2018-02-20T23:04:48.572933Z"},"_cell_guid":"7453cbc6-ed91-424b-b050-0c06a6dfe620","_uuid":"192470247577244b4039299a950399a28f94cfcd","trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var+text_count_var\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = RandomForestClassifier(n_estimators = 250, min_samples_split=10, n_jobs=-1)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"9e89b410-f84a-4a54-8233-c297b118a461","_uuid":"44710d0193896cec962ceac827036abd35f19c25"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T01:34:19.836413Z","end_time":"2018-02-21T01:36:30.933471Z"},"_cell_guid":"168095b3-9c3e-4190-a3af-f1b56b3c5189","_uuid":"a9af2ebe0dccd27d7a45c99d4c867c32b32fadca","scrolled":false,"trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var+text_count_var\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = model_df[features].reset_index(drop=True)\nY = model_df[target].reset_index(drop=True)\n\nmodel = xgb.XGBClassifier(n_estimators = 30000,\n                          learning_rate = .2,\n                          max_depth = 4,\n                          objective = \"multi:softmax\",\n                          subsample=1,\n                          min_child_weight=1,\n                          colsample_bytree=.8,\n                          random_state = 1,\n                          n_jobs = -1\n                         )\n\ntest_accs, test_scores = [], []\ntrain_accs, train_scores = [], []\n\nlogloss = []\nntrees = []\n\ni=1\nk=5\nrand_state = 1\n\nfor train_ind, test_ind in KFold(k,shuffle=True,random_state=rand_state).split(X,Y):\n    #print(\"Starting {} of {} folds\".format(i,k))\n\n    eval_set=[(X.iloc[train_ind],Y.iloc[train_ind]),(X.iloc[test_ind],Y.iloc[test_ind])] \n    fit_model = model.fit( \n                    X.iloc[train_ind], Y.iloc[train_ind], \n                    eval_set=eval_set,\n                    eval_metric='mlogloss',\n                    early_stopping_rounds=50,\n                    verbose=False\n                   )\n    \n    logloss.append(model.best_score)\n    ntrees.append(model.best_ntree_limit)\n    \n    #Test metrics\n    pred = model.predict(X.iloc[test_ind],ntree_limit=model.best_ntree_limit)\n    acc = skm.accuracy_score(Y.iloc[test_ind],pred)\n    test_accs.append(acc)\n    score = skm.precision_recall_fscore_support(Y.iloc[test_ind],pred)\n    test_scores.append(score)\n    #print(acc)\n    #print(skm.classification_report(Y[test_ind],pred))\n\n    #Train metrics\n    pred = model.predict(X.iloc[train_ind],ntree_limit=model.best_ntree_limit)\n    acc = skm.accuracy_score(Y.iloc[train_ind],pred)\n    train_accs.append(acc)\n    score = skm.precision_recall_fscore_support(Y.iloc[train_ind],pred)\n    train_scores.append(score)\n\n    i+=1\n\nprint(\"\\nAvg. Train Metrics\")\nprint (\"Accuracy: {:.5f}\".format(np.average(train_accs)))\nprint_scores(np.average(train_scores,axis=0),np.unique(Y))\n\nprint(\"\\nAvg. Validation Metrics\")\nprint (\"Accuracy: {:.5f}\".format(np.average(test_accs)))\nprint_scores(np.average(test_scores,axis=0),np.unique(Y))\n\nprint(\"\\nLogloss:\", np.average(logloss), \"Std Dev:\", np.std(logloss))\nprint(\"Best number of trees\", ntrees)","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"1b287a73-f24a-421a-848b-acc77eae06e5","_uuid":"1340394ff0ad7a5db5ab5b736837875d58e671d1","heading_collapsed":true},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T22:46:23.41051Z","end_time":"2018-02-20T22:46:35.768895Z"},"_cell_guid":"c6513300-2895-4870-a462-a0e36cec6f46","_uuid":"b3e585b67bd7620d7234f3c1aa0e6a0864439ce2","hidden":true,"trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var+text_count_var \ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = LogisticRegression(C=100)\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":32,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T08:54:23.457752Z","end_time":"2018-02-20T08:54:23.462347Z"},"_cell_guid":"d202f7c9-a6aa-4673-9222-114bac463482","_uuid":"bc7264a991df0fc6ad6481d4244dda747eee6e59","heading_collapsed":true},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"ExecuteTime":{"start_time":"2018-02-20T22:46:35.889084Z","end_time":"2018-02-20T22:46:39.2674Z"},"_cell_guid":"b31b0fc0-ad42-41e5-b07f-da9c843131a1","_uuid":"eb80a114ce7a57fa1f77efd3ed38c5636585007a","hidden":true,"trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var+text_count_var \ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = np.array(model_df[features])\nY = np.array(model_df[target])\n\nmodel = GaussianNB()\n\nvalidation_loop(model,X,Y,rand_state=1)","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"2379ada51ddffd88a63d4da0356757f2f206d340"},"cell_type":"markdown","source":"### Model Selection Results\n\nResults (accuracy):\n* Random Forest: 56.4%\n* XG Boost: 57.5%\n* Logistic Regression: 47.5%\n* Gaussian NB: 31.3%\n\nFrom our testing, we see the linear models did significantly worse than the trees. Gradient boosted trees outperformed random forest, so we'll choose this model for our final test."},{"metadata":{"_cell_guid":"18198bfc-9bd3-44e6-9f55-2297b1b3db69","_uuid":"e5e7d5acb5b632d1165a3fd83ab2f4a8cb262973"},"cell_type":"markdown","source":"## Final Model Test\nFinally, we'll test the XGBoost model on the holdout test to estimate how it does on new data. \n\nThe holdout set accuracy is 57.6% - pretty much in line with our validation accuracy."},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T07:06:22.514913Z","end_time":"2018-02-21T07:06:34.270405Z"},"_cell_guid":"74c11d7b-4107-4636-abec-48a5a365e27e","_uuid":"699bbdb17cb75246e973ef7f2d1f450419c43d97","trusted":true},"cell_type":"code","source":"features = [\"Claim_Value\"]+date_var+text_count_var\ntarget = \"Status\"\n\nmodel_df=df[[target]+features].dropna()\n\nX = model_df[features].reset_index(drop=True)\nY = model_df[target].reset_index(drop=True)\n\nmodel_df_holdout = df_holdout[[target]+features].dropna()\nX_holdout = model_df_holdout[features]\nY_holdout = model_df_holdout[target]\n\nmodel =  xgb.XGBClassifier(max_depth = 6,\n                           subsamples = 1,\n                           min_child_weight=6,\n                           colsample_bytree=0.6,\n                           n_estimators=107,\n                           learning_rate=.1,\n                           objective = \"multi:softmax\",\n                           random_state = 1,\n                           n_jobs = -1)\n\nfit_model = model.fit(X, Y)\n\nprint(\"Training\")\npred = model.predict(X)\nprint(skm.accuracy_score(Y,pred))\nprint(skm.classification_report(Y, pred))\n\nprint(\"\\nHoldout\")\npred = model.predict(X_holdout)\nprint(skm.accuracy_score(Y_holdout,pred))\nprint(skm.classification_report(Y_holdout, pred))\n\n","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"a035dbe9c2fe23823973a93816f05a847a6d9f98"},"cell_type":"markdown","source":"We can also try to make some inferences from the model. Due to being a tree-based model, we don't have directional relationships between variables and our prediction. However, we can look at the features that the tree found most important.\n\nClaim Value, Claim Site, and Claim Type give the most information gain. We can dig into these variables a bit more. "},{"metadata":{"ExecuteTime":{"start_time":"2018-02-21T06:51:11.511013Z","end_time":"2018-02-21T06:51:12.201973Z"},"_cell_guid":"ef3e02fa-b7ef-489d-8fc4-e0e51b83308d","_uuid":"0adb1b642684ad5d8406b23f47367decf895de80","trusted":true},"cell_type":"code","source":"xgb.plot_importance(fit_model, importance_type=\"gain\");","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eff00c8913c89c03163495bb23fc7f44d307e3de"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"cols":{"lenType":16,"lenVar":40,"lenName":16},"kernels_config":{"python":{"delete_cmd_postfix":"","varRefreshCmd":"print(var_dic_list())","library":"var_list.py","delete_cmd_prefix":"del "},"r":{"delete_cmd_postfix":") ","varRefreshCmd":"cat(var_dic_list()) ","library":"var_list.r","delete_cmd_prefix":"rm("}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"toc":{"number_sections":false,"toc_cell":false,"sideBar":true,"toc_window_display":true,"toc_section_display":"block","toc_position":{},"nav_menu":{},"skip_h1_title":true}},"nbformat":4,"nbformat_minor":1}