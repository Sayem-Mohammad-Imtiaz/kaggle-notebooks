{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"Final (Optimization).ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1_GP6hhj2Lauzt5OD4G1d08lL2XaBlY8A\n\"\"\"\n\n!pip install scikit-fuzzy\n\n##mfDerivs.py\nimport numpy as np\n\n\ndef partial_dMF(x, mf_definition, partial_parameter):\n    \"\"\"Calculates the partial derivative of a membership function at a point x.\n\n\n\n    Parameters\n    ------\n\n\n    Returns\n    ------\n\n    \"\"\"\n    mf_name = mf_definition[0]\n\n    if mf_name == 'gaussmf':\n\n        sigma = mf_definition[1]['sigma']\n        mean = mf_definition[1]['mean']\n\n        if partial_parameter == 'sigma':\n            result = (2./sigma**3) * np.exp(-(((x-mean)**2)/(sigma)**2))*(x-mean)**2\n        elif partial_parameter == 'mean':\n            result = (2./sigma**2) * np.exp(-(((x-mean)**2)/(sigma)**2))*(x-mean)\n\n    elif mf_name == 'gbellmf':\n\n        a = mf_definition[1]['a']\n        b = mf_definition[1]['b']\n        c = mf_definition[1]['c']\n\n        if partial_parameter == 'a':\n            result = (2. * b * np.power((c-x),2) * np.power(np.absolute((c-x)/a), ((2 * b) - 2))) / \\\n                (np.power(a, 3) * np.power((np.power(np.absolute((c-x)/a),(2*b)) + 1), 2))\n        elif partial_parameter == 'b':\n            result = -1 * (2 * np.power(np.absolute((c-x)/a), (2 * b)) * np.log(np.absolute((c-x)/a))) / \\\n                (np.power((np.power(np.absolute((c-x)/a), (2 * b)) + 1), 2))\n        elif partial_parameter == 'c':\n            result = (2. * b * (c-x) * np.power(np.absolute((c-x)/a), ((2 * b) - 2))) / \\\n                (np.power(a, 2) * np.power((np.power(np.absolute((c-x)/a),(2*b)) + 1), 2))\n\n    elif mf_name == 'sigmf':\n\n        b = mf_definition[1]['b']\n        c = mf_definition[1]['c']\n\n        if partial_parameter == 'b':\n            result = -1 * (c * np.exp(c * (b + x))) / \\\n                np.power((np.exp(b*c) + np.exp(c*x)), 2)\n        elif partial_parameter == 'c':\n            result = ((x - b) * np.exp(c * (x - b))) / \\\n                np.power((np.exp(c * (x - c))) + 1, 2)\n\n\n    return result\n\n##membershipfunction.py\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Mar 31 15:41:58 2014\n\n@author: tim.meggs\n\"\"\"\n\nfrom skfuzzy import gaussmf, gbellmf, sigmf\n\nclass MemFuncs:\n    'Common base class for all employees'\n    funcDict = {'gaussmf': gaussmf, 'gbellmf': gbellmf, 'sigmf': sigmf}\n\n\n    def __init__(self, MFList):\n        self.MFList = MFList\n\n    def evaluateMF(self, rowInput):\n        if len(rowInput) != len(self.MFList):\n            print(\"Number of variables does not match number of rule sets\")\n\n        return [[self.funcDict[self.MFList[i][k][0]](rowInput[i],**self.MFList[i][k][1]) for k in range(len(self.MFList[i]))] for i in range(len(rowInput))]\n\n##afnis.py\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Apr 03 07:30:34 2014\n\n@author: tim.meggs\n\"\"\"\nimport itertools\nimport numpy as np\n\nimport sys\n# insert at 1, 0 is the script path (or '' in REPL)\nsys.path.insert(1, './membership')\n\nimport copy\n\nclass ANFIS:\n    \"\"\"Class to implement an Adaptive Network Fuzzy Inference System: ANFIS\"\n\n    Attributes:\n        X\n        Y\n        XLen\n        memClass\n        memFuncs\n        memFuncsByVariable\n        rules\n        consequents\n        errors\n        memFuncsHomo\n        trainingType\n\n\n    \"\"\"\n\n    def __init__(self, X, Y, memFunction):\n        self.X = np.array(copy.copy(X))\n        self.Y = np.array(copy.copy(Y))\n        self.XLen = len(self.X)\n        self.memClass = copy.deepcopy(memFunction)\n        self.memFuncs = self.memClass.MFList\n        self.memFuncsByVariable = [[x for x in range(len(self.memFuncs[z]))] for z in range(len(self.memFuncs))]\n        self.rules = np.array(list(itertools.product(*self.memFuncsByVariable)))\n        self.consequents = np.empty(self.Y.ndim * len(self.rules) * (self.X.shape[1] + 1))\n        self.consequents.fill(0)\n        self.errors = np.empty(0)\n        self.memFuncsHomo = all(len(i)==len(self.memFuncsByVariable[0]) for i in self.memFuncsByVariable)\n        self.trainingType = 'Not trained yet'\n\n    def LSE(self, A, B, initialGamma = 1000.):\n        coeffMat = A\n        rhsMat = B\n        S = np.eye(coeffMat.shape[1])*initialGamma\n        x = np.zeros((coeffMat.shape[1],1)) # need to correct for multi-dim B\n        for i in range(len(coeffMat[:,0])):\n            a = coeffMat[i,:]\n            b = np.array(rhsMat[i])\n            S = S - (np.array(np.dot(np.dot(np.dot(S,np.matrix(a).transpose()),np.matrix(a)),S)))/(1+(np.dot(np.dot(S,a),a)))\n            x = x + (np.dot(S,np.dot(np.matrix(a).transpose(),(np.matrix(b)-np.dot(np.matrix(a),x)))))\n        return x\n\n    def trainHybridJangOffLine(self, epochs=5, tolerance=1e-5, initialGamma=1000, k=0.01):\n\n        self.trainingType = 'trainHybridJangOffLine'\n        convergence = False\n        epoch = 1\n\n        while (epoch < epochs) and (convergence is not True):\n\n            #layer four: forward pass\n            [layerFour, wSum, w] = forwardHalfPass(self, self.X)\n\n            #layer five: least squares estimate\n            layerFive = np.array(self.LSE(layerFour,self.Y,initialGamma))\n            self.consequents = layerFive\n            layerFive = np.dot(layerFour,layerFive)\n\n            #error\n            error = np.sum((self.Y-layerFive.T)**2)\n            print('current error: '+ str(error))\n            average_error = np.average(np.absolute(self.Y-layerFive.T))\n            self.errors = np.append(self.errors,error)\n\n            if len(self.errors) != 0:\n                if self.errors[len(self.errors)-1] < tolerance:\n                    convergence = True\n\n            # back propagation\n            if convergence is not True:\n                cols = range(len(self.X[0,:]))\n                dE_dAlpha = list(backprop(self, colX, cols, wSum, w, layerFive) for colX in range(self.X.shape[1]))\n\n\n            if len(self.errors) >= 4:\n                if (self.errors[-4] > self.errors[-3] > self.errors[-2] > self.errors[-1]):\n                    k = k * 1.1\n\n            if len(self.errors) >= 5:\n                if (self.errors[-1] < self.errors[-2]) and (self.errors[-3] < self.errors[-2]) and (self.errors[-3] < self.errors[-4]) and (self.errors[-5] > self.errors[-4]):\n                    k = k * 0.9\n\n            ## handling of variables with a different number of MFs\n            t = []\n            for x in range(len(dE_dAlpha)):\n                for y in range(len(dE_dAlpha[x])):\n                    for z in range(len(dE_dAlpha[x][y])):\n                        t.append(dE_dAlpha[x][y][z])\n\n            eta = k / np.abs(np.sum(t))\n\n            if(np.isinf(eta)):\n                eta = k\n\n            ## handling of variables with a different number of MFs\n            dAlpha = copy.deepcopy(dE_dAlpha)\n            if not(self.memFuncsHomo):\n                for x in range(len(dE_dAlpha)):\n                    for y in range(len(dE_dAlpha[x])):\n                        for z in range(len(dE_dAlpha[x][y])):\n                            dAlpha[x][y][z] = -eta * dE_dAlpha[x][y][z]\n            else:\n                dAlpha = -eta * np.array(dE_dAlpha)\n\n\n            for varsWithMemFuncs in range(len(self.memFuncs)):\n                for MFs in range(len(self.memFuncsByVariable[varsWithMemFuncs])):\n                    paramList = sorted(self.memFuncs[varsWithMemFuncs][MFs][1])\n                    for param in range(len(paramList)):\n                        self.memFuncs[varsWithMemFuncs][MFs][1][paramList[param]] = self.memFuncs[varsWithMemFuncs][MFs][1][paramList[param]] + dAlpha[varsWithMemFuncs][MFs][param]\n            epoch = epoch + 1\n\n\n        self.fittedValues = predict(self,self.X)\n        self.residuals = self.Y - self.fittedValues[:,0]\n\n        return self.fittedValues\n\n\n    def plotErrors(self):\n        if self.trainingType == 'Not trained yet':\n            print(self.trainingType)\n        else:\n            import matplotlib.pyplot as plt\n            plt.plot(range(len(self.errors)),self.errors,'ro', label='errors')\n            plt.ylabel('error')\n            plt.xlabel('epoch')\n            plt.show()\n\n    def plotMF(self, x, inputVar):\n        import matplotlib.pyplot as plt\n        from skfuzzy import gaussmf, gbellmf, sigmf\n\n        for mf in range(len(self.memFuncs[inputVar])):\n            if self.memFuncs[inputVar][mf][0] == 'gaussmf':\n                y = gaussmf(x,**self.memClass.MFList[inputVar][mf][1])\n            elif self.memFuncs[inputVar][mf][0] == 'gbellmf':\n                y = gbellmf(x,**self.memClass.MFList[inputVar][mf][1])\n            elif self.memFuncs[inputVar][mf][0] == 'sigmf':\n                y = sigmf(x,**self.memClass.MFList[inputVar][mf][1])\n\n            plt.plot(x,y,'r')\n\n        plt.show()\n\n    def plotResults(self):\n        if self.trainingType == 'Not trained yet':\n            print(self.trainingType)\n        else:\n            import matplotlib.pyplot as plt\n            plt.plot(range(len(self.fittedValues)),self.fittedValues,'r', label='trained')\n            plt.plot(range(len(self.Y)),self.Y,'b', label='original')\n            plt.legend(loc='upper left')\n            plt.show()\n\n\n\ndef forwardHalfPass(ANFISObj, Xs):\n    layerFour = np.empty(0,)\n    wSum = []\n\n    for pattern in range(len(Xs[:,0])):\n        #layer one\n        layerOne = ANFISObj.memClass.evaluateMF(Xs[pattern,:])\n\n        #layer two\n        miAlloc = [[layerOne[x][ANFISObj.rules[row][x]] for x in range(len(ANFISObj.rules[0]))] for row in range(len(ANFISObj.rules))]\n        layerTwo = np.array([np.product(x) for x in miAlloc]).T\n        if pattern == 0:\n            w = layerTwo\n        else:\n            w = np.vstack((w,layerTwo))\n\n        #layer three\n        wSum.append(np.sum(layerTwo))\n        if pattern == 0:\n            wNormalized = layerTwo/wSum[pattern]\n        else:\n            wNormalized = np.vstack((wNormalized,layerTwo/wSum[pattern]))\n\n        #prep for layer four (bit of a hack)\n        layerThree = layerTwo/wSum[pattern]\n        rowHolder = np.concatenate([x*np.append(Xs[pattern,:],1) for x in layerThree])\n        layerFour = np.append(layerFour,rowHolder)\n\n    w = w.T\n    wNormalized = wNormalized.T\n\n    layerFour = np.array(np.array_split(layerFour,pattern + 1))\n\n    return layerFour, wSum, w\n\n\ndef backprop(ANFISObj, columnX, columns, theWSum, theW, theLayerFive):\n\n    paramGrp = [0]* len(ANFISObj.memFuncs[columnX])\n    for MF in range(len(ANFISObj.memFuncs[columnX])):\n\n        parameters = np.empty(len(ANFISObj.memFuncs[columnX][MF][1]))\n        timesThru = 0\n        for alpha in sorted(ANFISObj.memFuncs[columnX][MF][1].keys()):\n\n            bucket3 = np.empty(len(ANFISObj.X))\n            for rowX in range(len(ANFISObj.X)):\n                varToTest = ANFISObj.X[rowX,columnX]\n                tmpRow = np.empty(len(ANFISObj.memFuncs))\n                tmpRow.fill(varToTest)\n\n                bucket2 = np.empty(ANFISObj.Y.ndim)\n                for colY in range(ANFISObj.Y.ndim):\n\n                    rulesWithAlpha = np.array(np.where(ANFISObj.rules[:,columnX]==MF))[0]\n                    adjCols = np.delete(columns,columnX)\n\n                    senSit = partial_dMF(ANFISObj.X[rowX,columnX],ANFISObj.memFuncs[columnX][MF],alpha)\n                    # produces d_ruleOutput/d_parameterWithinMF\n                    dW_dAplha = senSit * np.array([np.prod([ANFISObj.memClass.evaluateMF(tmpRow)[c][ANFISObj.rules[r][c]] for c in adjCols]) for r in rulesWithAlpha])\n\n                    bucket1 = np.empty(len(ANFISObj.rules[:,0]))\n                    for consequent in range(len(ANFISObj.rules[:,0])):\n                        fConsequent = np.dot(np.append(ANFISObj.X[rowX,:],1.),ANFISObj.consequents[((ANFISObj.X.shape[1] + 1) * consequent):(((ANFISObj.X.shape[1] + 1) * consequent) + (ANFISObj.X.shape[1] + 1)),colY])\n                        acum = 0\n                        if consequent in rulesWithAlpha:\n                            acum = dW_dAplha[np.where(rulesWithAlpha==consequent)] * theWSum[rowX]\n\n                        acum = acum - theW[consequent,rowX] * np.sum(dW_dAplha)\n                        acum = acum / theWSum[rowX]**2\n                        bucket1[consequent] = fConsequent * acum\n\n                    sum1 = np.sum(bucket1)\n\n                    if ANFISObj.Y.ndim == 1:\n                        bucket2[colY] = sum1 * (ANFISObj.Y[rowX]-theLayerFive[rowX,colY])*(-2)\n                    else:\n                        bucket2[colY] = sum1 * (ANFISObj.Y[rowX,colY]-theLayerFive[rowX,colY])*(-2)\n\n                sum2 = np.sum(bucket2)\n                bucket3[rowX] = sum2\n\n            sum3 = np.sum(bucket3)\n            parameters[timesThru] = sum3\n            timesThru = timesThru + 1\n\n        paramGrp[MF] = parameters\n\n    return paramGrp\n\n\ndef predict(ANFISObj, varsToTest):\n\n    [layerFour, wSum, w] = forwardHalfPass(ANFISObj, varsToTest)\n\n    #layer five\n    layerFive = np.dot(layerFour,ANFISObj.consequents)\n\n    return layerFive\n\n\nif __name__ == \"__main__\":\n    print(\"I am main!\")\n\n#from google.colab import files\n#uploaded = files.upload()\n\n##svr_anfis.py\n##IMPORTING REQUIRED PACKAGES\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport sys\nfrom sklearn.metrics import mean_absolute_error as mae\nimport math\nfrom sklearn.model_selection import GridSearchCV\n\n\n##HYPERPARAMETERS\nepochs_anfis = 50\n\n##LOAD ORIGINIAL DATA AND VISUALIZE THEM\ndataset = pd.read_csv('../input/well-water-data/abkhan_truncated4.csv') \ny = dataset.values[:, -1].ravel()\nprint(y)\nx1 = np.array(range(y.shape[0]))\nx1 = x1.reshape(len(x1), 1)\nx = dataset.values[0:178 , 25:26]\nx1_train = dataset.values[0:140 , 0]\nx_train = dataset.values[0:140 , 25:26]\ny_train = dataset.values[0:140 , -1]\ny_train = y_train.reshape(y_train.shape[0], 1)\nx1_test= dataset.values[140:178 , 0]\nx_test = dataset.values[140:178 , 25:26]\ny_test = dataset.values[140:178 , -1]\ny_test = y_test.reshape(y_test.shape[0], 1)\n#plt.plot(x)                                           # dispaly original data\n#plt.xlabel('Time (month)')\n#plt.ylabel('Ground Water Level (m)')\n#plt.title('Original Data')\n#plt.show()\n\n#GridSearch\nparam = {'kernel' : ('poly', 'rbf', 'sigmoid'),'C' : [1,5,10],'degree' : [3,8],'coef0' : [0.01,10,0.5],'gamma' : ('auto','scale')},\n\nregressior = SVR(),\n\nregressior = GridSearchCV(regressior,param,cv=5)\n\n#grids.fit(Xtrain,ytrain)\n\n##IMPLEMENTING SVR (%80 of Data fo Training)\n#regressior = SVR()\nfor k in range(1,1000,10):\n    regressior = SVR(C=k)\n    regressior1=SVR()\n    regressior.fit(x_train, y_train)\n    regressior1.fit(x_train, y_train)\n    confidence = regressior.score(x_test, y_test)\n    print(k,confidence)\ny_pred_svr = regressior.predict(x)\ny_pred_svr1 = regressior1.predict(x)\ny_pred1_svr = regressior.predict(x_train)\ny_pred1_svr1 = regressior1.predict(x_train)\ny_pred2_svr = regressior.predict(x_test)\ny_pred2_svr1 = regressior1.predict(x_test)\n\nplt.title('SVR Test')\nplt.scatter(x1_test, y_test, color='red', label='Original Data')\nplt.plot(x1_test,y_pred2_svr, color='blue', label='SVR', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('SVR Total Results')\nplt.scatter(x1, y, color='red', label='Original Data')\nplt.plot(x1,y_pred_svr1, color='Blue', label='SVR', linewidth=3)\nplt.plot(x1,y_pred_svr, color='Orange', label='Optimal SVR', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\n##PRINTING THE RESULTS OF SVR\nprint('==========================\\nSVR Scores:\\n') \nprint('{:30} {}'.format('R2 for Total Data:', r2_score(y, y_pred_svr)))\nprint('{:30} {}'.format('R2 for Training Data:', r2_score(y_train, y_pred1_svr)))\nprint('{:30} {}'.format('R2 for Test Data:', r2_score(y_test, y_pred2_svr)))\nprint('{:30} {}'.format('MSE for Total Data:', mean_squared_error(y, y_pred_svr)))\nprint('{:30} {}'.format('MSE for Training Data:', mean_squared_error(y_train, y_pred1_svr)))\nprint('{:30} {}'.format('MSE for Test Data:', mean_squared_error(y_test, y_pred2_svr)))\nprint('{:30} {}'.format('MAE for Total Data:', mae(y, y_pred_svr)))\nprint('{:30} {}'.format('MAE for Training Data:', mae(y_train, y_pred1_svr)))\nprint('{:30} {}'.format('MAE for Test Data:', mae(y_test, y_pred2_svr)))\nprint('{:30} {}'.format('RMSE for Total Data:', math.sqrt(mean_squared_error(y, y_pred_svr))))\nprint('{:30} {}'.format('RMSE for Training Data:', math.sqrt(mean_squared_error(y_train, y_pred1_svr))))\nprint('{:30} {}'.format('RMSE for Test Data:', math.sqrt(mean_squared_error(y_test, y_pred2_svr))))\nprint('==========================')\n\n##IMPLEMENTING ANFIS\nsys.path.insert(1, './membership')\n\n#Define membership functions\nmf = [[['gaussmf',{'mean':1667.,'sigma':10.}],['gaussmf',{'mean':1672.,'sigma':20.}],['gaussmf',{'mean':1677.,'sigma':40.}]]]\n\nmfc = MemFuncs(mf)\nanf = ANFIS(x_train, y_train.ravel(), mfc)\nanf.trainHybridJangOffLine(epochs=epochs_anfis)\nprint(round(anf.consequents[-1][0],6))\nprint(round(anf.consequents[-2][0],6))\nprint(round(anf.fittedValues[9][0],6))\nif round(anf.consequents[-1][0],6) == -5.275538 and round(anf.consequents[-2][0],6) == -1.990703 and round(anf.fittedValues[9][0],6) == 0.002249:\n\tprint('test is good')\n#anf.plotErrors()\n##anf.plotResults()\n\ny_pred_anfis = predict(anf, x)\ny_pred1_anfis = predict(anf, x_train)\ny_pred2_anfis = predict(anf, x_test)\n\n#plt.plot(y_pred_anfis)\n#plt.show()\n\nplt.title('ANFIS Train')\nplt.scatter(x1_train, y_train, color='red', label='Original Data')\nplt.plot(x1_train, y_pred1_anfis, color='blue', label='ANFIS', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('ANFIS Test')\nplt.scatter(x1_test, y_test, color='red', label='Original Data')\nplt.plot(x1_test,y_pred2_anfis, color='magenta', label='ANFIS', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('ANFIS Total Results')\nplt.scatter(x1, y, color='red', label='Original Data')\nplt.plot(x1,y_pred_anfis, color='magenta', label='ANFIS', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n##Optimal ANFIS\n#Define membership functions\nmfOP = [[['gaussmf',{'mean':1667.,'sigma':7}],['gaussmf',{'mean':1672.,'sigma':7}],['gaussmf',{'mean':1677.,'sigma':7}]]]\n\nmfcOP = MemFuncs(mfOP)\nanfOP = ANFIS(x_train, y_train.ravel(), mfcOP)\nanfOP.trainHybridJangOffLine(epochs=epochs_anfis)\nprint(round(anfOP.consequents[-1][0],6))\nprint(round(anfOP.consequents[-2][0],6))\nprint(round(anfOP.fittedValues[9][0],6))\nif round(anfOP.consequents[-1][0],6) == -5.275538 and round(anfOP.consequents[-2][0],6) == -1.990703 and round(anfOP.fittedValues[9][0],6) == 0.002249:\n\tprint('test is good')\n#anf.plotErrors()\n##anf.plotResults()\n\ny_predOP_anfis = predict(anfOP, x)\ny_pred1OP_anfis = predict(anfOP, x_train)\ny_pred2OP_anfis = predict(anfOP, x_test)\n\n#plt.plot(y_pred_anfis)\n#plt.show()\n\nplt.title('ANFIS OP Train')\nplt.scatter(x1_train, y_train, color='red', label='Original Data')\nplt.plot(x1_train, y_pred1OP_anfis, color='blue', label='ANFIS', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('ANFIS OP Test')\nplt.scatter(x1_test, y_test, color='red', label='Original Data')\nplt.plot(x1_test,y_pred2OP_anfis, color='magenta', label='ANFIS', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('ANFIS OP Total Results')\nplt.scatter(x1, y, color='red', label='Original Data')\nplt.plot(x1,y_predOP_anfis, color='magenta', label='ANFIS', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\n##PRINTING THE RESULTS OF ANFIS\nprint('==========================\\nANFIS Scores:\\n') \nprint('{:30} {}'.format('R2 for Total Data OP:', r2_score(y, y_predOP_anfis)))\nprint('{:30} {}'.format('R2 for Training Data:', r2_score(y_train, y_pred1OP_anfis)))\nprint('{:30} {}'.format('R2 for Test Data:', r2_score(y_test, y_pred2OP_anfis)))\nprint('{:30} {}'.format('MSE for Total Data:', mean_squared_error(y, y_predOP_anfis)))\nprint('{:30} {}'.format('MSE for Training Data:', mean_squared_error(y_train, y_pred1OP_anfis)))\nprint('{:30} {}'.format('MSE for Test Data:', mean_squared_error(y_test, y_pred2OP_anfis)))\nprint('{:30} {}'.format('MAE for Total Data:', mae(y, y_predOP_anfis)))\nprint('{:30} {}'.format('MAE for Training Data:', mae(y_train, y_pred1OP_anfis)))\nprint('{:30} {}'.format('MAE for Test Data:', mae(y_test, y_pred2OP_anfis)))\nprint('{:30} {}'.format('RMSE for Total Data:', math.sqrt(mean_squared_error(y, y_predOP_anfis))))\nprint('{:30} {}'.format('RMSE for Training Data:', math.sqrt(mean_squared_error(y_train, y_pred1OP_anfis))))\nprint('{:30} {}'.format('RMSE for Test Data:', math.sqrt(mean_squared_error(y_test, y_pred2OP_anfis))))\n\n###########################################################33\nplt.title('ANFIS and SVR comparision')\nplt.scatter(x1, y, color='red', label='Original Data')\nplt.plot(x1, y_pred_anfis, color='magenta', label='ANFIS', linewidth=3)\nplt.plot(x1, y_pred_svr, color='blue', label='SVR', linewidth=3)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\n##PRINTING THE RESULTS OF ANFIS\nprint('==========================\\nANFIS Scores:\\n') \nprint('{:30} {}'.format('R2 for Total Data:', r2_score(y, y_pred_anfis)))\nprint('{:30} {}'.format('R2 for Training Data:', r2_score(y_train, y_pred1_anfis)))\nprint('{:30} {}'.format('R2 for Test Data:', r2_score(y_test, y_pred2_anfis)))\nprint('{:30} {}'.format('MSE for Total Data:', mean_squared_error(y, y_pred_anfis)))\nprint('{:30} {}'.format('MSE for Training Data:', mean_squared_error(y_train, y_pred1_anfis)))\nprint('{:30} {}'.format('MSE for Test Data:', mean_squared_error(y_test, y_pred2_anfis)))\nprint('{:30} {}'.format('MAE for Total Data:', mae(y, y_pred_anfis)))\nprint('{:30} {}'.format('MAE for Training Data:', mae(y_train, y_pred1_anfis)))\nprint('{:30} {}'.format('MAE for Test Data:', mae(y_test, y_pred2_anfis)))\nprint('{:30} {}'.format('RMSE for Total Data:', math.sqrt(mean_squared_error(y, y_pred_anfis))))\nprint('{:30} {}'.format('RMSE for Training Data:', math.sqrt(mean_squared_error(y_train, y_pred1_anfis))))\nprint('{:30} {}'.format('RMSE for Test Data:', math.sqrt(mean_squared_error(y_test, y_pred2_anfis))))\n\nimport pandas as pd\nimport numpy as np\nimport keras\nimport tensorflow as tf\nfrom keras import backend\nnp.random.seed(1234)\ntf.random.set_seed(1234)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nimport pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import mean_absolute_error as mae\nimport math\n\n#from google.colab import files\n#uploaded = files.upload()\n!pip install xlrd\n#!pip install openpyxl\nimport xlrd\ndata = pd.read_excel('../input/d/hamedetezadi/well-water-level-meter/data2.xlsx')\nlabeled_data = data[data['Y'].notna()]\nlabeled_data\n\nlabeled_data\n\n\"\"\"# Data pre-peration\n\n```\n# This is formatted as code\n```\n\n\n\"\"\"\n\nXn = labeled_data[[x for x in data.columns if x != 'Y']]\nXn = Xn.fillna(labeled_data.mean())\nXn  = Xn[[x for x in Xn.columns][2:26]]\nYn = labeled_data['Y']\n#X_train1, X_test1, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=40)\nx_train_mlp, x_test_mlp, y_train_mlp, y_test_mlp = train_test_split(Xn, Yn, test_size=0.2, random_state=40)\n\nimport matplotlib.pyplot as plt\nplt.plot(Yn)\nplt.ylabel('Y value')\nplt.show()\nset([int(x) for x in Yn])\n\n\"\"\"# The Model\"\"\"\n\ndef baseline_model(inp_size, neurons, x, y):\n  # create model\n  model = Sequential()\n  model.add(Dense(neurons, input_dim=inp_size, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))\n  # Compile model\n  model.compile(loss='mean_squared_error', optimizer='adam')\n  model.fit(x, y, epochs=100)\n  return model\n\n#for i in range(512,1024,512):\nmodel = baseline_model(23, 512, x_train_mlp, y_train_mlp)\n\npred_train= model.predict(x_train_mlp)\nprint('Training RMSE:', np.sqrt(mean_squared_error(y_train_mlp,pred_train)))\nprint('Training MSE:', mean_squared_error(y_train_mlp,pred_train))\nprint('Training R2:', r2_score(y_train_mlp,pred_train))\nprint('Training MAE:', mae(y_train_mlp,pred_train))\npred_test= model.predict(x_test_mlp)\nprint('Test RMSE:', np.sqrt(mean_squared_error(y_test_mlp,pred_test)))\nprint('Test MSE:', mean_squared_error(y_test_mlp,pred_test))\nprint('Test R2:', r2_score(y_test_mlp,pred_test))\nprint('Test MAE:', mae(y_test_mlp,pred_test))\npred= model.predict(Xn)\nprint('Total RMSE:', np.sqrt(mean_squared_error(Yn,pred))) \nprint('Total MSE:', mean_squared_error(Yn,pred)) \nprint('Total R2:', r2_score(Yn,pred))\nprint('Total MAE:', mae(Yn,pred))\n\nplt.title('MLP')\nYn=Yn.ravel()\nxn = np.array(range(Yn.shape[0]))\nxn = xn.reshape(len(xn), 1)\nyn = Yn.reshape(Yn.shape[0], 1)\nplt.plot(pred, color='green', label='MLP', linewidth=3)\nplt.scatter(xn, yn, color='red', label='Original Data')\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('MLP and ANFIS comparision')\nYn=Yn.ravel()\nxn = np.array(range(Yn.shape[0]))\nxn = xn.reshape(len(xn), 1)\nyn = Yn.reshape(Yn.shape[0], 1)\nplt.plot(pred, color='green', label='MLP', linewidth=3)\nplt.plot(x1,y_pred_anfis, color='magenta', label='ANFIS', linewidth=3)\nplt.scatter(xn, yn, color='red', label='Original Data')\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\n\"\"\"# Forcasting\"\"\"\n\nplt.title('ANFIS and SVR and MLP comparision')\n#plt.scatter(xn, yn, color='red', label='Original Data')\nplt.plot(x1, y_pred_svr, color='blue', label='SVR', linewidth=2)\nplt.plot(x1, y_pred_anfis, color='magenta', label='ANFIS', linewidth=2)\nplt.plot(pred, color='green', label='MLP', linewidth=2)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('MLP and ANFIS and Optimal SVR comparision')\nYn=Yn.ravel()\nxn = np.array(range(Yn.shape[0]))\nxn = xn.reshape(len(xn), 1)\nyn = Yn.reshape(Yn.shape[0], 1)\nplt.plot(pred, color='green', label='MLP', linewidth=2)\nplt.plot(x1,y_pred_anfis, color='magenta', label='ANFIS', linewidth=2)\nplt.plot(x1,y_pred_svr, color='orange', label='Optimal SVR', linewidth=2)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('SVR and Optimal SVR and ANFIS comparision')\nplt.plot(x1,y_pred_svr1, color='Blue', label='SVR', linewidth=2)\nplt.plot(x1,y_pred_svr, color='orange', label='Optimal SVR', linewidth=2)\nplt.plot(x1,y_pred_anfis, color='magenta', label='ANFIS', linewidth=2)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('Original Data and ANFIS and Optimal ANFIS')\nplt.scatter(x1, y, color='red', label='Original Data')\nplt.plot(x1,y_pred_anfis, color='magenta', label='ANFIS', linewidth=2)\nplt.plot(x1,y_predOP_anfis, color='yellow', label='Optimal ANFIS', linewidth=2)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('Original Data and Optimal SVR and Optimal ANFIS')\nplt.scatter(x1, y, color='red', label='Original Data')\nplt.plot(x1,y_pred_svr, color='orange', label='Optimal SVR', linewidth=2)\nplt.plot(x1,y_predOP_anfis, color='yellow', label='Optimal ANFIS', linewidth=2)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()\n\nplt.title('MLP and Optimal SVR and Optimal ANFIS')\nplt.plot(pred, color='green', label='MLP', linewidth=2)\nplt.plot(x1,y_pred_svr, color='orange', label='Optimal SVR', linewidth=2)\nplt.plot(x1,y_predOP_anfis, color='yellow', label='Optimal ANFIS', linewidth=2)\nplt.legend()\nplt.xlabel('Time (month)')\nplt.ylabel('Ground Water Level (m)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T08:36:21.320105Z","iopub.execute_input":"2021-06-24T08:36:21.320552Z","iopub.status.idle":"2021-06-24T08:37:01.846711Z","shell.execute_reply.started":"2021-06-24T08:36:21.320517Z","shell.execute_reply":"2021-06-24T08:37:01.84572Z"},"trusted":true},"execution_count":null,"outputs":[]}]}