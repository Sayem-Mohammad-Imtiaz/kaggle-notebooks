{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first kernel.Any suggestions and comments are most welcomed. This is just a beginners classification model using decision tree and random forest."},{"metadata":{},"cell_type":"markdown","source":"**Importing libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#reading the data \ndata = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# exploring the data to get a basic idea of the data\ndata.describe()\nprint(data.shape)\nprint(data.dtypes)\nprint(data.dtypes.value_counts())\n# dropping the columns ['Over18','StandardHours','EmployeeCount','EmployeeNumber'] as they all have the same value.\ndata=data.drop(['Over18','StandardHours','EmployeeCount','EmployeeNumber'],axis=1)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy values for categorical variable.\nchar_cols = data.dtypes.pipe(lambda x: x[x == 'object']).index\nlabel_mapping = {}\n\nfor c in char_cols:\n    data[c], label_mapping[c] = pd.factorize(data[c])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now changing data type of categorical variables into category\ndata[['BusinessTravel','Department','EducationField','Gender',\n      'JobRole','MaritalStatus','OverTime']]=data[['BusinessTravel','Department','EducationField','Gender',\n      'JobRole','MaritalStatus','OverTime']].astype('category')\nprint(data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtaining the descriptive statistics of the data.\ndata.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating boxplot to check for outliers\ndata.boxplot(column=['Age', 'DailyRate','DistanceFromHome', 'Education','EnvironmentSatisfaction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column=['HourlyRate', 'JobInvolvement','JobLevel', 'JobSatisfaction','MonthlyIncome'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column=['TotalWorkingYears', 'TrainingTimesLastYear','WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column=['NumCompaniesWorked','PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n                     'StockOptionLevel'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from the above boxplot we can see that Monthly income has more outliers ouliers compared to other varbales\n#below is a function that filters out outliers row from our data.\ndef outlier(df,col_name):\n    Q1 = df[col_name].quantile(0.25)\n    Q3 = df[col_name].quantile(0.75)\n    IQR = Q3 - Q1\n    print(\"IQR: \",IQR)\n    print(\"Q1: \",Q1)\n    print(\"Q3: \",Q3)\n    lw=Q1-(1.5*IQR)\n    up=Q3+(1.5*IQR)\n    print(\"the limits of outlier is: \",(lw,up))\n    print(\"% of observation above upperlimit is: \",(df[col_name]>up).value_counts(normalize=True)*100)\n    print(\"% of observation below lowerlimit is: \",(df[col_name]<lw).value_counts(normalize=True)*100)\n    df=df.loc[(df[col_name]>lw)&(df[col_name]<up)]\n    return(df)\ndata=outlier(data,'MonthlyIncome')\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the data into independent var and target variables\ny=data['Attrition']\nx=data.drop('Attrition',axis=1)\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are more variables, I have reduced the dimension of the data by selecting important features using chi square method for checking the association with the target variable(attrition)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can apply feature selection method to select the features to be included in our model.\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n#apply SelectKBest class to extract top 20 best features using chi-square \nbestfeatures = SelectKBest(score_func=chi2, k=20)\nfit = bestfeatures.fit(x,y)\ndf_scores=pd.DataFrame(fit.scores_)\ndf_columns=pd.DataFrame(x.columns)\nfeature_scores=pd.concat([df_columns,df_scores],axis=1)\nfeature_scores.columns=['variable','score']\nfinal_variables=feature_scores.nlargest(20,'score')\nfinal_variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filtering out the important columns alone for building our model\nfiltered_data=data[['MonthlyIncome','MonthlyRate','DailyRate','TotalWorkingYears','YearsAtCompany','YearsInCurrentRole',\n                   'YearsWithCurrManager','Age','DistanceFromHome','StockOptionLevel','OverTime','JobLevel','MaritalStatus',\n                   'EducationField','YearsSinceLastPromotion','JobSatisfaction','EnvironmentSatisfaction','NumCompaniesWorked',\n                   'JobInvolvement','TrainingTimesLastYear','Attrition']]\nfiltered_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\ny=filtered_data['Attrition']\nx=filtered_data.drop('Attrition',axis=1)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# buildng a decision tree model using 'entropy' criterion\nfrom sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier(criterion='entropy',max_depth=5,random_state=100)\nmodel.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=model.predict(x_test)\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_test,predictions)*100\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To avoid overfitting , I am using cross validation to check my mean accuracy score of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model evaluation using cross validation score to check our accuracy.\nfrom sklearn.model_selection import cross_val_score\ndt_score=cross_val_score(model,x_train,y_train,scoring=\"accuracy\",cv=10)\nprint(dt_score)\nprint(\"mean_accuracy:\",dt_score.mean())\nprint(\"std_deviation of accuracy:\",dt_score.std())\n#the mean accuracy of our model after running it 10 times is 80.3%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score,recall_score\nprint(confusion_matrix(y_test, predictions))\nprint(\"precision_score: \",precision_score(y_test,predictions))\nprint(\"recall_score: \",recall_score(y_test,predictions))\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building a random forest classifier for the data\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(random_state=500)\nrf.fit(x_train,y_train)\npredictions_rf=rf.predict(x_test)\nprint(accuracy_score(y_test,predictions_rf))\n# model evaluation\nfrom sklearn.model_selection import cross_val_score\nrf_score=cross_val_score(rf,x_train,y_train,scoring=\"accuracy\",cv=10)\nprint(rf_score)\nprint(\"mean_accuracy:\",rf_score.mean())\nprint(\"std_deviation of accuracy:\",rf_score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest performs on the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix\nprint(confusion_matrix(y_test, predictions))\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions_rf))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}