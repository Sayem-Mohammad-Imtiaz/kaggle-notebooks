{"cells":[{"metadata":{"_uuid":"e16f236ab552c36944575a5dce001b5d0e0c1605"},"cell_type":"markdown","source":"This kernel describes few ways of visualizing the outputs of your trained Convolutional neural network. I learned about these methods from a book **Deep Learning with Python** written by **Francois Chollet**, author of Keras. I am going to demonstrate below mentioned three techniques of visualizing the intermediate and final outputs generated by CNNs, which can give us insights about their working and structure.\n\n1. Visualizing intermediate convolutional layer activations of trained model\n2. Visualizing intermedate convoutional layer filters of trained model\n3. Viualizing Class Activation Maps of trained model on images.\n\nFor this demonstration, I will be using pre-trained InceptionV3 model from Keras and then train it on the **Flowers Recognition** dataset. I will be explaining the code and significance behind all these techniques."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input, decode_predictions\n\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a793404a8a07102047af114e780c698f284548a1"},"cell_type":"markdown","source":"I will be starting by creating a image generator using Keras inbuilt **ImageDataGenerator** module which will also help us to augment the training images like rotation, horizontal and vertical shift and horizontal flip. From this generator I instantiated a *Training image generator* and *Validation image generator* using its *flow_from_directory* module. "},{"metadata":{"trusted":true,"_uuid":"3f6beac4b5d3049dcd92b6e47547b5967e602058"},"cell_type":"code","source":"image_dir = \"../input/flowers/flowers\"\nimage_datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True, rescale=1./255, validation_split=0.15)  \n\ntrain_gen = image_datagen.flow_from_directory(image_dir, target_size=(299, 299), batch_size=32, class_mode=\"categorical\", subset=\"training\")\nvalid_gen = image_datagen.flow_from_directory(image_dir, target_size=(299, 299), batch_size=32, class_mode=\"categorical\", subset=\"validation\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332b550931776226e9e5bfa68a42817d19b1ff85"},"cell_type":"markdown","source":"Moving on, here I am creating a new model using Keras Functional api and the InceptionV3 model (pre-trained on ImageNet dataset). Since I want my model to be trained on a different dataset than ImageNet, I do not want to attach the final prediction layer of the pre-trained model so I have put *include_top = False* and then compile the model. Make sure the *Internet* setting for your kernel is *Connected* in the Settings tab on the right-side pane, since the model will download the pre-trained weights for InceptionV3 model. You can also activate *GPU* from same place."},{"metadata":{"trusted":true,"_uuid":"4d62a3ee799d1464fbbb92f583694a201207fdab"},"cell_type":"code","source":"inp = Input((299, 299, 3))\ninception = InceptionV3(include_top=False, weights='imagenet', input_tensor=inp, input_shape=(299, 299, 3), pooling='avg')\nx = inception.output\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.1)(x)\nout = Dense(5, activation='softmax')(x)\n\ncomplete_model = Model(inp, out)\n\ncomplete_model.compile(optimizer='adam', loss='categorical_crossentropy')\n#complete_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41f98cb0bd7a0d0ec486b7ef422c562e8f9f5dbc","scrolled":true},"cell_type":"code","source":"# Running the model using the fit_generater method for 10 epochs\nhistory = complete_model.fit_generator(train_gen, steps_per_epoch=115, epochs=10, validation_data=valid_gen, validation_steps=20, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"415719eccf963d8256eb661be60d7d6a98c49e2a"},"cell_type":"markdown","source":"**Visualizing Activation Maps**\n\nNow since we have trained our model on Flowers dataset, we can begin with the first technique i.e. visualizing the outputs for intermediate convolution layers. When we are training a deep network, continuously feeding it with training images of objects. these images are passed through various convolution layers, which apply the Convolution operator on these images producing a new image as output. So, in below mentioned few cells I try to show you how these convolved images look.\n\nBut for those of you who are not familiar with the Convolution operator, you should take a look at Chris Olah's post on [Understanding Convolutions](http://colah.github.io/posts/2014-07-Understanding-Convolutions/). The code that I am using is very much inspired from Fracois Chollet's book that I previously mentioned, and I have also commented working of code portions, but otherwise its a pretty straight forward use of Keras and Matplotlib."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f83ae9f805bd1dafacf69ee7ae1e38f933b98055"},"cell_type":"code","source":"# Taking the outputs of first 100 layers from trained model, leaving the first Input layer, in a list\nlayer_outputs = [layer.output for layer in complete_model.layers[1:100]]\n\n# This is image of a Rose flower from our dataset. All of the visualizations in this cell are of this image.\ntest_image = image_dir+\"/rose/2258973326_03c0145f15_n.jpg\"\n\n# Loading the image and converting it to a numpy array for feeding it to the model. Its important to use expand_dims since our original model takes batches of images\n# as input, and here we are feeding a single image to it, so the number of dimensions should match for model input.\nimg = image.load_img(test_image, target_size=(299, 299))\nimg_arr = image.img_to_array(img)\nimg_arr = np.expand_dims(img_arr, axis=0)\nimg_arr /= 255.\n\n# Defining a new model using original model's input and all the 100 layers outputs and then predicting the values for all those 100 layers for our test image.\nactivation_model = Model(inputs=complete_model.input, outputs=layer_outputs)\nactivations = activation_model.predict(img_arr)\n\n# These are names of layers, the outputs of which we are going to visualize.\nlayer_names = ['conv2d_1', 'activation_1', 'conv2d_4', 'activation_4', 'conv2d_9', 'activation_9']\nactiv_list = [activations[0], activations[2], activations[10], activations[12], activations[17], activations[19]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcb664204aa110383166d58578d3354a236262a9"},"cell_type":"markdown","source":"Now we have stored the values of activation maps from first 100 layers of our model in *activations* list, all we need to do now is to visualize the maps of the layers in which we are interested, to see what these layers actually produce. We are going to use the *subplot* functionality of *Matplotlib* to visualize how different activation maps look in different layers. And in doing so, we can get a rough idea of what each layer is trying to learn."},{"metadata":{"trusted":true,"_uuid":"1fa0e8402fcb610bbd76331b23772381cdc7997a"},"cell_type":"code","source":"# Visualization of the activation maps from first convolution layer. Different filters activate different parts of the image, like some are detecting edges, some are\n# detecting background, while others are detecting just the outer boundary of the flower and so on.\nfig = plt.figure(figsize=(22, 3))\nfor img in range(30):\n    ax = fig.add_subplot(2, 15, img+1)\n    ax = plt.imshow(activations[0][0, :, :, img], cmap='plasma')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"885cfe027f4cb311405adba5c4109b46aae1aad6"},"cell_type":"code","source":"# This is the visualization of activation maps from third convolution layer. In this layer the abstraction has increased. Filters are now able to regognise the edges\n# of the flower more closely. Some filters are activating the surface texture of the image as well\nfig = plt.figure(figsize=(22, 6))\nfor img in range(60):\n    ax = fig.add_subplot(4, 15, img+1)\n    ax = plt.imshow(activations[6][0, :, :, img], cmap='plasma')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"858f4ad9cb2cd8896ede68da25a111179a18d298"},"cell_type":"code","source":"# These are activation maps from fourth convolution layer. The images have become a little blurry, because of the MaxPooling operation done just before this layer. As\n# more Pooling layers are introduced the knowledge reaching the convolution layer becomes more and more abstract, which helps the complete network to finally classify\n# the image properly, but visually they don't provide us with much information.\nfig = plt.figure(figsize=(22, 6))\nfor img in range(60):\n    ax = fig.add_subplot(4, 15, img+1)\n    ax = plt.imshow(activations[10][0, :, :, img], cmap='plasma')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ad875c84ae5d09790569d5248109647c862b1ad"},"cell_type":"code","source":"# These are the activation maps from next convolution layer after next MaPooling layer. The images have become more blurry\nfig = plt.figure(figsize=(22, 6))\nfor img in range(60):\n    ax = fig.add_subplot(4, 15, img+1)\n    ax = plt.imshow(activations[17][0, :, :, img], cmap='plasma')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"177c6cb81052fb46dddc93eb5e0123f6cb8d71a6"},"cell_type":"code","source":"# Activation maps from first Concatenate layer Mixed0, which concatenates the ReLU activated outputs from four convolution layers.\nfig = plt.figure(figsize=(22, 6))\nfor img in range(60):\n    ax = fig.add_subplot(4, 15, img+1)\n    ax = plt.imshow(activations[39][0, :, :, img], cmap='plasma')\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5483fd8425a30659d7e506fd645337551f34c642"},"cell_type":"markdown","source":"Looking at the activations from these convolution layers we can understand that the layers nearer to input learn very basic features like edges and textures in the image. But as we move deeper, the network is able to learn more abstract features which helps it to classify the image. This property of deep networks is used in **Transfer Learning**, where we use the pre-trained weights from some other dataset and freeze the weights of earlier layers and only allow the deeper layers to learn, because anyways earlier layers will learn to recognise basic features in image, which should be similar to already learnt features.\n\n**Visualizing Filter Patterns of Convolution layers**\n\nMoving on to the second visualization technique, I will now show how you can visualize the layer filters for different convolutional layers. The idea here is to visualize the pattern to which the corresponding kernel of the layer will look for in the image and in the presence of which it will be activated. The basic idea of the code is to apply *Gradient Ascent* in input space i.e., applying *Gradient Descent* to the value of the input image of a convnet so as to maximize the response of a specific filter, starting with a blank input image. The resulting input image will be the pattern to which the chosen filter is maximally responsive to. I do this by calling the function *generate_pattern* and a helper function *deprocess_image*, working of each is explained in their respective cells."},{"metadata":{"trusted":true,"_uuid":"d5b77508e130ed0021c9eda39929afa1a67e8806"},"cell_type":"code","source":"# The purpose of this function is to just convert a numpy array to a standard image format, so that it can be displayed and viewed comfortably\ndef deprocess_image(x):\n    \n    x -= x.mean()\n    x /= (x.std() + 1e-5)\n    x *= 0.1\n    x += 0.5\n    x = np.clip(x, 0, 1)\n    x *= 255\n    x = np.clip(x, 0, 255).astype('uint8')\n\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29c2cfef80afa53d87b653442cd875a35d8fe331"},"cell_type":"code","source":"# This function is used to create a loss function that maximizes the value of a given filter in a convolution layer, and then we use SGD to adjust the values of the\n# input image so as to maximize this activation value. We pass the layer name and the filter index to the function as arguments. 'loss' is the mean for that particular\n# filter, 'grads' is the gradient calculated for this loss with respect to input image. Finally, SGD is run for 80 iterations which continuously maximizes the response\n# to input image by adding the gradient. Finally, it uses 'deprocess_image' to convert this array to a representable image format.\n\ndef generate_pattern(layer_name, filter_index, size=150):\n    \n    layer_output = complete_model.get_layer(layer_name).output\n    loss = K.mean(layer_output[:, :, :, filter_index])\n    grads = K.gradients(loss, complete_model.input)[0]\n    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n    iterate = K.function([complete_model.input], [loss, grads])\n    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n    step = 1.\n    for i in range(80):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n        \n    img = input_img_data[0]\n    return deprocess_image(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1177c5d51237cad0e2ccfe0ba18f2b601e797515"},"cell_type":"code","source":"# Below are the patterns to which the filters from first convolution layer get activated. As we can see these are very basic cross-sectional patterns formed by\n# horizontal and vertical lines, which is what the these filters look in the input image and get activated if they find one.\nfig = plt.figure(figsize=(15, 12))\nfor img in range(30):\n    ax = fig.add_subplot(5, 6, img+1)\n    ax = plt.imshow(generate_pattern('conv2d_1', img))\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cf6e71a0cb71c345d7c64aacc66fd6d8905222b"},"cell_type":"code","source":"# Here are patterns to which filters from third convolution layer respond to. These patterns are liitle more abstract than the simple cross-sectional patterns we saw\n# for first layer. This tells us that this layer is looking for more deeper and complex patterns than the earlier convolutional layer.\nfig = plt.figure(figsize=(25, 13))\nfor img in range(50):\n    ax = fig.add_subplot(5, 10, img+1)\n    ax = plt.imshow(generate_pattern('conv2d_3', img))\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c532b85fc1aaeabb656472aa4cb50d35dbb188c"},"cell_type":"code","source":"fig = plt.figure(figsize=(15, 13))\nfor img in range(60):\n    ax = fig.add_subplot(6, 10, img+1)\n    ax = plt.imshow(generate_pattern('conv2d_4', img))\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b25bdf1016943c0ce0186baf2ba07241d46a64ce"},"cell_type":"code","source":"fig = plt.figure(figsize=(15, 13))\nfor img in range(60):\n    ax = fig.add_subplot(6, 10, img+1)\n    ax = plt.imshow(generate_pattern('conv2d_9', img))\n    plt.xticks([])\n    plt.yticks([])\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"644a619530fd3d034bac548efb35115aed430604"},"cell_type":"markdown","source":"**Visualizing HeatMaps for Class Activations**\n\nThe third visualization technique that I am going to explain is how you can visualize what part of the image is getting activated by the network for a particular class label. This technique can be helpful when you try to debug the wrong predictions produced by the network.\n\nThe general category of these class of visualizations is called **Class Activation Maps (CAM) Visualization**. The specific category that I am going to discuss here is producing heatmaps of class activations over input images. A Class Activation heatmap is a 2D grid of scores associated with a particular output class, computed for every location of input image, indicating how important is each location with respect to that class.\n\nFor doing this, we are going to set up Grad-CAM process. It was proposed in this [paper](https://arxiv.org/abs/1610.02391). Conceptually what this technique does is it uses the class-specific gradient information flowing into the final convolution layer of network to produce a coarse localization map of the important parts of the image. The major advantage of this process is that it does not require any re-training of the network. So using Grad-CAM we are going to create a heatmap of the important areas in image and then superimpose that heatmap with the original image., so that it is clear whether right area in image is getting activated or not."},{"metadata":{"trusted":true,"_uuid":"3d7bf2934818df5a4769b0ab644d425eda2c47df"},"cell_type":"code","source":"img_path = image_dir+'/daisy/9158041313_7a6a102f7a_n.jpg'\n\nimg = image.load_img(img_path, target_size=(299, 299))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = complete_model.predict(x)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fddfe1dff6318baab869fe64f4610838540bd1b8"},"cell_type":"code","source":"# 0 is the class index for class 'Daisy' in Flowers dataset\nflower_output = complete_model.output[:, 0]\nlast_conv_layer = complete_model.get_layer('mixed10')\n\ngrads = K.gradients(flower_output, last_conv_layer.output)[0]                               # Gradient of output with respect to 'mixed10' layer\npooled_grads = K.mean(grads, axis=(0, 1, 2))                                                # Vector of size (2048,), where each entry is mean intensity of\n                                                                                            # gradient over a specific feature-map channel\niterate = K.function([complete_model.input], [pooled_grads, last_conv_layer.output[0]])\npooled_grads_value, conv_layer_output_value = iterate([x])\n\n#2048 is the number of filters/channels in 'mixed10' layer\nfor i in range(2048):                                                                       # Multiplies each channel in feature-map array by \"how important this\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]                           # channel is\" with regard to the class\n        \nheatmap = np.mean(conv_layer_output_value, axis=-1)\nheatmap = np.maximum(heatmap, 0)                                                            # Following two lines just normalize heatmap between 0 and 1\nheatmap /= np.max(heatmap)\n\nplt.imshow(heatmap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96340130645e7109c31826efc5c2d5e4f6da693a"},"cell_type":"code","source":"img = plt.imread(img_path)\nextent = 0, 300, 0, 300\nfig = plt.Figure(frameon=False)\n\nimg1 = plt.imshow(img, extent=extent)\nimg2 = plt.imshow(heatmap, cmap='viridis', alpha=0.4, extent=extent)\n\nplt.xticks([])\nplt.yticks([])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"868e725caf9fd650f5795971614f6a76386e96a4"},"cell_type":"code","source":"img_path = image_dir+'/dandelion/458011386_ec89115a19.jpg'\n\nimg = image.load_img(img_path, target_size=(299, 299))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = complete_model.predict(x)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fed4ceb95c82c4df29769e418359bbab5f5986b2"},"cell_type":"code","source":"#1 is the class index for class 'Dandelion' in Flowers dataset\nflower_output = complete_model.output[:, 1]\nlast_conv_layer = complete_model.get_layer('mixed10')\n\ngrads = K.gradients(flower_output, last_conv_layer.output)[0]\npooled_grads = K.mean(grads, axis=(0, 1, 2))\niterate = K.function([complete_model.input], [pooled_grads, last_conv_layer.output[0]])\npooled_grads_value, conv_layer_output_value = iterate([x])\n\n#2048 is the number of filters/channels in 'mixed10' layer\nfor i in range(2048):\n    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\nheatmap = np.mean(conv_layer_output_value, axis=-1)\nheatmap = np.maximum(heatmap, 0)\nheatmap /= np.max(heatmap)\n\nplt.imshow(heatmap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d16c5ebde253189f9f08dda2ba57cc96a436ab8"},"cell_type":"code","source":"img = plt.imread(img_path)\n\nplt.subplot(121)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(122)\nplt.imshow(heatmap)\nplt.xticks([])\nplt.yticks([])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ebe1e493d9e1c92cc01729bf8cfffd53f7b23c7"},"cell_type":"code","source":"img = plt.imread(img_path)\nextent = 0, 300, 0, 300\nfig = plt.Figure(frameon=False)\n\nimg1 = plt.imshow(img, extent=extent)\nimg2 = plt.imshow(heatmap, cmap='viridis', alpha=0.4, extent=extent)\n\nplt.xticks([])\nplt.yticks([])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7287d4a87098ab9bad0c8f4aa5c4e92a84328f37"},"cell_type":"markdown","source":"So, the above few cells show us how we can use Grad-CAM process to see whether our network is able to recognise the right objects in the given image or not. As I mentioned in the beginning techniques like these can help you to understand what your CNN is learning, which can further help you to debug them if they are not learning what they need to.\n\nI have tried to explain as clear as I could, but in case there are any descrepancies or any queries you can put them in comments. If you liked like kernel you can upvote it and also fork it if you want to generate such visualizations on some other Image dataset, maybe celebrity faces, and using some other model like Xception."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}