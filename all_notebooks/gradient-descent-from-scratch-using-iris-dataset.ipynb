{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom matplotlib import pyplot as plt\nimport random\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sources:https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n#","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#read the dataset\niris=pd.read_csv(\"../input/Iris.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the first 5 elements\niris.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get length of the iris dataset\nprint(len(iris.SepalLengthCm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m,n=iris.shape\n#get the first 100 values as training set\nx_train=iris.SepalLengthCm.iloc[0:100].values.reshape(100,1)\ny_train=iris.SepalWidthCm.iloc[0:100].values.reshape(100,1)\n#get last 50 elemennt as test set\nx_test=iris.SepalLengthCm.iloc[101:149].values.reshape(48,1)\ny_test=iris.SepalWidthCm.iloc[101:149].values.reshape(48,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualitzation of data\nplt.scatter(x_train,y_train)\nplt.xlabel(\"Sepal length\")\nplt.ylabel(\"sepal width\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assign random number for weight\nweight=np.random.randn(1)\nbias=np.random.randn(1)\n#length of datapoints\nN=len(x)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iterations=2000 #number of iteration\nlearning_rate=0.001\n\ndef gradient_descent(weight_,bias_,N_,learning_rate_,iteration,y_,x_):\n    past_costs=[]\n    past_weight=[]\n    \n    for i in range(iteration):\n        #y_pred=mx+b where m is weight and b is bias\n        y_pred=(x_*weight_)+bias_\n        \n        #transpose the x_ matrix . matrix is now [row=1:column=100]\n        x_trans=x_.T\n        \n        #print((np.dot(x_trans,(y_pred-y_))))\n        #gradient descent formula\n        #(1/number of samples) (summation of (x*(error)) -->done using dot matrix)\n        #where error is y_pred-y_train \n        #dot is [1:100]dot[100:1] resulting matrix is [1:1]\n        weight_=weight_-(((1/N_)*learning_rate_)*((np.dot(x_trans,(y_pred-y_)))))\n        #print((((1/N_)*learning_rate_)*(((np.dot(x_trans,(y_pred-y_)))))))\n     \n        #cost function\n        #sum of square error\n        loss_calc=(np.sum((y_pred-y_)**2)/(2*N_))\n        \n        #get the past costs and past weights for plotting \n        #past_costs.append(loss_calc)\n        #past_weight.append(weight_)\n        \n    \n    #weight_calc=weight_\n    #print(weight_calc)\n    #loss_calc=loss_calc\n    return weight_,loss_calc\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_ans,loss_ans=gradient_descent(weight,bias,N,learning_rate,iterations,y_train,x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the value of weight bbased on our training\nprint(weight_ans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the ypre_test using x_test data, trained weight and bias\nypred_test= (x_test*weight_ans)+bias\nprint(ypred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the accuracy using Mean Absolute Percent Error\n#MAPE= summation of(|actual- predicted|/actual) *100/Num_samples or\naccuracy=np.mean(np.abs((y_test - ypred_test)) / y_test) * 100\nprint(accuracy)\n\n#model still not accurate with large error","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}