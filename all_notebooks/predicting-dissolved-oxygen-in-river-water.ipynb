{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading of libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib as mp\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.read_csv(\"../input/dissolved-oxygen-prediction-in-river-water/train.csv\")\nd.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we have a statistical look at the data\nd.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"d.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### There are high rates of null values from stations 3 - 7 within the entire data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Our total rows and columns\nd.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### For predicting O2 levels at the target station, we will use O2 reading from station 1 and 2 which has significantly less missing data compared to over 50% in the other columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating our sample data set.\n\nd1 = d[['target',\"O2_1\", \"O2_2\"]]\nd1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the two missing values\nd2 = d1.dropna()\nd2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First a scatter plot is created to view the distribution of the data.\n\nscatter_matrix(d2, figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1 = sns.distplot(d2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2.plot(kind='box')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### A few sigle widely dispursed outlaires can be observed from the scatter plots and further observed in the histogram.\n###### In order to get a better picture, a line graph wil now be created."},{"metadata":{"trusted":true},"cell_type":"code","source":"d2.plot(kind=\"line\", figsize=(8,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### There seems to be an abnormal spike in O2 levels at two of the station. Lets have a closer look"},{"metadata":{"trusted":true},"cell_type":"code","source":"d2[['O2_1', 'O2_2']].plot(kind=\"line\", figsize=(8, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Concentrations of approximately 46.95 and 40.9 are observed at this stations which is clearly abnormal. From further [investigations](https://www.fondriest.com/environmental-measurements/parameters/water-quality/dissolved-oxygen/#:~:text=As%20oxygen%20in%20the%20atmosphere,100%25%20air%20saturation%20at%20equilibrium.), such high levels of O2 in river water, especially in this data set, might be an error.\n###### These vaues will be removed and replaced with the mean of the respective column."},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing the maximum values which seem to be outlairs with a mean value\nd3 = d2.replace(d2[\"O2_1\"].max(), value=d2[\"O2_1\"].mean())\nd4 = d3.replace(d2[\"O2_2\"].max(), value=d2[\"O2_2\"].mean())\n\nd4[['target','O2_1' ,'O2_2']].plot(kind=\"line\", figsize=(7,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = sns.distplot(d4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### With the outlaired removed, we can move onto the next step."},{"metadata":{},"cell_type":"markdown","source":"### Data Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the target and feature sets\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nX = d2.drop('target', axis=1)\ny = d2[['target']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creation of the train test split sets as well as standardisation of the data.\n\nx_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=35)\nX_train = preprocessing.StandardScaler().fit(x_train).transform(x_train)\nX_test = preprocessing.StandardScaler().fit(x_test).transform(x_test)\n\n\n## change to 1d array\ny_train = np.array(y_train)\ny_train = y_train.ravel()\n\ny_test = np.array(y_test)\ny_test = y_test.ravel()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing various regression models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing a simple linear regression\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression(fit_intercept=True)\nreg.fit(X_train, y_train)\n\n\nprint('R2 model score:  ', reg.score(X_test, y_test))\nprint('RMSE    :  ', np.sqrt(mean_squared_error(reg.predict(X_test), y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function to test several regression models\n\ndef model_result(m_odel):\n    m = m_odel\n    m.fit(X_train, y_train)\n    print('R2 model score:  ', m.score(X_test, y_test))\n    print('RMSE    :  ', np.sqrt(mean_squared_error(m.predict(X_test), y_test)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing ridge regression\nfrom sklearn.linear_model import Ridge\nmodel_result(m_odel=Ridge(alpha = 1, random_state = 42))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing Lasso regression\nfrom sklearn.linear_model import Lasso\nmodel_result(m_odel=Lasso(alpha = 1, random_state = 42))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing huber regression\nfrom sklearn.linear_model import HuberRegressor\nmodel_result(m_odel=HuberRegressor())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nmodel_result(m_odel=ElasticNet(alpha = 1, random_state = 42))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From our observation so far, the Ridge regression model had the highest accuracy at **67.38%**"},{"metadata":{},"cell_type":"markdown","source":"## Testing a support vector model"},{"metadata":{"trusted":true},"cell_type":"code","source":"##support vector regressor\n\nfrom sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR(epsilon=2.5543, random_state=42)\nsvm_reg.fit(X_train, y_train)\nsvm_reg.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The accuracy is not as high as with the linear model"},{"metadata":{},"cell_type":"markdown","source":"## Testing a Random forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#random forest\n\nfrom sklearn.ensemble import RandomForestRegressor\nftree = RandomForestRegressor(max_depth=2,random_state=42)\nftree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre = ftree.predict(X_test)\nf = mean_squared_error(y_test, pre)\nFs = np.sqrt(f)\nprint(\"RMSE\",Fs)\nprint(\"Accuracy\", ftree.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We also note that the random forest performs even more poorly."},{"metadata":{},"cell_type":"markdown","source":"### In this step we try using polynomial features see the effect on the accuracy. This involves:"},{"metadata":{},"cell_type":"markdown","source":"##### - importing polynomial features\n##### - transforming features\n##### - creating new train test splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Testing polynomial regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\n\nX_poly = poly_features.fit_transform(X)\n\nXp_train, Xp_test, yp_train, yp_test = train_test_split(X_poly, y, test_size=0.2, random_state=35)\n\nyp_train = np.array(yp_train)\nyp_train = yp_train.ravel()\n\nyp_test = np.array(yp_test)\nyp_test = yp_test.ravel()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define mode to test accuracy\n\ndef model_resultp(m_odelp):\n    mp = m_odelp\n    mp.fit(Xp_train, yp_train)\n    print('R2 model score =',mp.score(Xp_test, yp_test))\n    print('RMSE =',np.sqrt(mean_squared_error(mp.predict(Xp_test), yp_test)))\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we test the accuracy of the linear model with polynomial features\n\nmodel_resultp(m_odelp=linear_model.LinearRegression(fit_intercept=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we test the accuracy of the Ridge regression model with polynomial features\n\nmodel_resultp(m_odelp=Ridge(alpha = 1, random_state = 42))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we test the accuracy of the Lasso regression model with polynomial features\n\nmodel_resultp(m_odelp=Lasso(alpha = 1, random_state = 42))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we test the accuracy of the Huber regression model with polynomial features\n\nmodel_resultp(m_odelp=HuberRegressor())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we test the accuracy of the Elastic net model with polynomial features\n\nmodel_resultp(m_odelp=ElasticNet(alpha = 1, random_state = 42))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### There is a huge improvement in model accuracy when using polynomial features. Results all range between **71** and **72%!**"},{"metadata":{},"cell_type":"markdown","source":"## Model Fine Tuning."},{"metadata":{},"cell_type":"markdown","source":"##### Now that we have isolated the best performing models, we will now twick the parameters in order to get the highest accuracy"},{"metadata":{},"cell_type":"markdown","source":"##### First we will tune the hyper parameters for the **ridge regression model with linear features**."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tunning for ridge regression\n\nparam_grid2 = [\n \n {'alpha' : np.logspace(-1,0.00001,1000), 'max_iter' : [1000], \n  \"fit_intercept\": [True, False], \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}]\nmodel2 = Ridge(random_state=42 )\ngrid_search2 = GridSearchCV(model2, param_grid2, cv=5,\n scoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search2.fit(X_train, y_train)\n\nRRg = grid_search2.best_estimator_\nFL1 = RRg.predict(X_test)\n\nrmse3 = np.sqrt(mean_squared_error(FL1, y_test))\n\n\n\nprint(\"RSE\",rmse3)\nridge_score = RRg.score(X_test, y_test)\nprint(\"accuracy score\", ridge_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Aftertuning, the ridge regression modelimproved slightly from to **67.38%** to **67.40%**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Now cross validation will be used to fine the best model with **polynomial features**."},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross validation on polynomial features\n\nmodelsT = [Ridge, Lasso, ElasticNet, HuberRegressor]\nmodel_names = ['ridge', 'lasso', 'elasticnet', 'huber']\n\nfor x in range(len(modelsT)):\n    print(model_names[x])\n    \n    param_grid = {'alpha' : np.logspace(-1,0.009,2500),\n                  'max_iter' : [1000]}\n    lin_model  = modelsT[x]() \n    model_cv   = GridSearchCV(estimator  = lin_model, \n                        param_grid = [param_grid],\n                        cv = 5,\n                        scoring='neg_mean_squared_error', \n                        n_jobs = -1,\n                        verbose = 1)\n    model_cv.fit(Xp_train, yp_train)\n\n    best_model              = model_cv.best_estimator_\n    print(best_model)\n    bestmodelFitTime        = model_cv.cv_results_['mean_fit_time'][model_cv.best_index_]\n    bestmodelScoreTime      = model_cv.cv_results_['mean_score_time'][model_cv.best_index_]\n    best_model.fit(Xp_train, yp_train)\n    print('R2 score: ', best_model.score(Xp_test, yp_test))\n\n    \n    y_pred = best_model.predict(Xp_test)\n    rmse   = np.sqrt(mean_squared_error(y_pred, yp_test))\n    print('Test RMSE : ', rmse)\n    print(\"**********************************\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The best performing model turned out to be the Lasso regression with alpha = 0.254. An accuracy of **72.69%** was achieved!"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = Lasso(alpha=0.25455422642263903, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n\nmy_model.fit(Xp_train, yp_train)\nmy_model.score(Xp_test, yp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the target 02 concentrations in the remainded of the test set.\nmy_model.predict(Xp_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting the target station from test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Loading test data\nTest = pd.read_csv(\"../input/dissolved-oxygen-prediction-in-river-water/test.csv\")\nTest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We notice there is an outlier in O2_2 measurements so we replace it with the mean\nTest = Test.replace(Test[\"O2_2\"].max(), value=Test[\"O2_2\"].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Creating our data set with O2 measures"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating o2 data set\nTest_X = Test[[\"O2_1\", \"O2_2\"]]\nTest_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating and fitting ploynomial features\n\npoly_features_X = PolynomialFeatures(degree=2, include_bias=False)\n\nTes_X_poly = poly_features_X.fit_transform(Test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Test set O2 predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we predict the target station O2 concentration from test set\n\nmy_model.predict(Tes_X_poly)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"##### There were high amounts of null values in the data set and as a result the model had to be built on 2 feature sets - station 1 and 2. Further more there were outlier values not only present in the O2 data but within the entire data set. The final, best accuracy score which I was able to get, given these constraints was 72%. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}