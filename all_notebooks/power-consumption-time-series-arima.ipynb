{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Electricity Consumption Using Time Series Analysis**\n\nTime series analysis is a statistical method to analyse the past data within a given duration of time to forecast the future. It comprises of ordered sequence of data at equally spaced interval.To understand the time series data & the analysis let us consider an example. Consider an example of Airline Passenger data. It has the count of passenger over a period of time."},{"metadata":{},"cell_type":"markdown","source":"![](https://image.freepik.com/free-photo/distribution-electric-substation-with-power-lines-transformers_156373-17.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Here the **Objective** is- Build a model to forecast the electricity power consumtion(value. The data is classified in date/time and the value of consumption. The goal is to predict electricity consumption for the next 6 years i.e. till 2024.\n\n**Time Series:**<br>\nTime Series is a series of observations taken at particular time intervals (usually equal intervals). Analysis of the series helps us to predict future values based on previous observed values. In Time series, we have only 2 variables, time & the variable we want to forecast.\n\n**Why & where Time Series is used?**<br>\nTime series data can be analysed in order to extract meaningful statistics and other charecteristsics. It's used in atleast the 4 scenarios:\n\n1. Business Forecasting\n2. Understanding past behavior\n3. Plan the future\n4. Evaluate current accomplishment\n\n**Importance of Time Series Analysis:**<br>\nAmple of time series data is being generated from a variety of fields. And hence the study time series analysis holds a lot of applications. Let us try to understand the importance of time series analysis in different areas.\n\n1. Economics\n2. Finance\n3. Healthcare\n4. Environmental Science\n5. Sales Forecasting\n6. Weather forecasting\n7. Earthquake prediction\n8. Astronomy\n9. Signal processing"},{"metadata":{},"cell_type":"markdown","source":"**Loading the basic libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading Electric Production data set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom = pd.read_csv('../input/electric-production/Electric_Production.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check first 5 and last 5 records of data set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are 397 records in datasets and 2 columns. There are no null records present. But, look at the DATE column. We need to convert them in to datetime datatype.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nelecom['DATE']=pd.to_datetime(elecom['DATE'],infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, we will need to index DATE column.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"elecomind = elecom.set_index('DATE',inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecomind.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's plot the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.xlabel('Date')\nplt.ylabel('Electric Power Consumption')\nplt.plot(elecomind)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above plot, we can see that there is a Trend compoenent in the series. Hence, we now check for stationarity of the data.**"},{"metadata":{},"cell_type":"markdown","source":"**Let's make one function consisting of stationary data checking and ADCF test working. Because we will need to repeat the steps many times, therefore, making function will become very handy.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(timeseries):\n    \n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #Plot rolling statistics\n    plt.figure(figsize=(10,5))\n    plt.plot(timeseries, color='blue', label='Original')\n    plt.plot(movingAverage, color='red', label='Rolling Mean')\n    plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickeyâ€“Fuller test:\n    print('Results of Dickey Fuller Test:')\n    elecom_test = adfuller(timeseries['Value'], autolag='AIC')\n    dfoutput = pd.Series(elecom_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in elecom_test[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's determine & plot rolling statistics.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(elecomind)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From above plot, we can see that Rolling Mean itself has a trend component even though Rolling Standard Deviation is fairly constant with time.**\n\n**For time series to be stationary, we need to ensure that both Rolling Mean and Rolling Standard Deviation remain fairly constant WRT time.**\n\n**Both the curves needs to be parallel to X-Axis, in our case it is not so.**\n\n**We've also conducted the ADCF ie Augmented Dickey Fuller Test. Having the Null Hypothesis to be Time Series is Non Stationary.**"},{"metadata":{},"cell_type":"markdown","source":"For a Time series to be stationary, the ADCF test should have:\n\n1. p-value should be low (according to the null hypothesis)\n2. The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\nFrom the above ADCF test result, we can see that p-value(near to 0.18) is very large. Also critical values lower than Test Statistics. Hence, we can safely say that our Time Series at the moment is **NOT STATIONARY**"},{"metadata":{},"cell_type":"markdown","source":"### **Data Transformation To Achieve Stationarity**\n\nNow, we will have to perform some data transformation to achieve Stationarity. We can perform any of the transformations like taking log scale, square, square root, cube, cube root, time shift, exponential decay, etc.\n\nLet's perform Log Transformation.\n\nBasically we need to remove the trend component."},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_log = np.log(elecomind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.xlabel('Date')\nplt.ylabel('Electric Power Consumption')\nplt.plot(elecom_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Working on Rolling stats seperately (not using function) because we would need Rolling stats separately for computing.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rollmean_log = elecom_log.rolling(window=12).mean()\nrollstd_log = elecom_log.rolling(window=12).std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(elecom_log, color='blue', label='Original')\nplt.plot(rollmean_log, color='red', label='Rolling Mean')\nplt.plot(rollstd_log, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation (Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph we can say that, we slightly bettered our previous results. Now, we are heading into the right direction.\n\nFrom the above graph, Time series with log scale as well as Rolling Mean(moving avg) both have the trend component. Thus subtracting one from the other should remove the trend component.\n\n**R (result) = Time Series Log Scale - Rolling Mean Log Scale -> this can be our final non trend curve.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_new = elecom_log - rollmean_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_new.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's determine & plot rolling statistics.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(elecom_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above plot, we came to know that \"indeed subtracting two related series having similar trend components actually removed trend and made the dataset stationary\"**"},{"metadata":{},"cell_type":"markdown","source":"Also, after concluding the results from ADFC test, we can now say that given series is now **STATIONARY**"},{"metadata":{},"cell_type":"markdown","source":"### **Time Shift Transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_log_diff = elecom_log - elecom_log.shift()\nplt.figure(figsize=(10,5))\nplt.plot(elecom_log_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_log_diff.dropna(inplace=True)\nplt.figure(figsize=(10,5))\nplt.plot(elecom_log_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's determine & plot rolling statistics.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(elecom_log_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot, we can see that, visually this is the very best result as our series along with rolling stats values of moving avg(mean) & moving standard deviation is very much flat & stationary."},{"metadata":{},"cell_type":"markdown","source":"**Let us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = seasonal_decompose(elecom_log)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(10,5))\nplt.subplot(411)\nplt.plot(elecom_log, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There can be cases where an observation simply consist of trend & seasonality. In that case, there won't be any residual component & that would be a null or NaN. Hence, we also remove such cases.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_decompose = residual\nelecom_decompose.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rollmean_decompose = elecom_decompose.rolling(window=12).mean()\nrollstd_decompose = elecom_decompose.rolling(window=12).std()\n\nplt.figure(figsize=(10,5))\nplt.plot(elecom_decompose, color='blue', label='Original')\nplt.plot(rollmean_decompose, color='red', label='Rolling Mean')\nplt.plot(rollstd_decompose, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Plotting ACF & PACF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_acf = acf(elecom_log_diff, nlags=20)\nlag_pacf = pacf(elecom_log_diff, nlags=20, method='ols')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(elecom_log_diff)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(elecom_log_diff)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(elecom_log_diff)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(elecom_log_diff)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the ACF graph, we can see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 3 From the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 3\n\n(from the above graphs the p and q values are very close to 3 where the graph cuts off the origin)\n\n**ARIMA is AR + I + MA.** Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower the RSS values indicates a better model."},{"metadata":{},"cell_type":"markdown","source":"### **AR Model**\nMaking order = (3,1,0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = ARIMA(elecom_log, order=(3,1,0))\nresults_AR = model1.fit(disp=-1)\nplt.figure(figsize=(10,5))\nplt.plot(elecom_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - elecom_log_diff['Value'])**2))\nprint('Plotting AR model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **MA Model**\nMaking order = (0,1,3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = ARIMA(elecom_log, order=(0,1,3))\nplt.figure(figsize=(10,5))\nresults_MA = model2.fit(disp=-1)\nplt.plot(elecom_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - elecom_log_diff['Value'])**2))\nprint('Plotting MA model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **AR+I+MA = ARIMA Model**\nMaking order = (3,1,3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(elecom_log, order=(3,1,3))\nplt.figure(figsize=(10,5))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(elecom_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - elecom_log_diff['Value'])**2))\nprint('Plotting ARIMA model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RSS value for:** AR Model - 0.8695, MA Model - 1.2793\n\nARIMA Model - 0.5227\n\nBy combining AR & MA into ARIMA, we see that RSS value has decreased from either case to 0.5227, indicating ARIMA to be better than its individual component models.\n\nWith the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data."},{"metadata":{},"cell_type":"markdown","source":"### **Prediction & Reverse Transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\npredictions_ARIMA_diff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_diff_cumsum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_log = pd.Series(elecom_log['Value'].iloc[0], index=elecom_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Inverse of log is exp**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.figure(figsize=(10,5))\nplt.plot(elecomind)\nplt.plot(predictions_ARIMA)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From above plot, we can see that our predicted forecasts are very close to the real time series values. It also indicates a fairly accurate model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_log.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elecom_log.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have 396 (existing data of 33 yrs in months) data points. Now, we can to forecast for additional 6 yrs (6x12 months=72 data points).**\n\n**396+72 = 468 records/data points**"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_ARIMA.plot_predict(1,468)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My other time series notebook(Air Passenger): https://www.kaggle.com/sunaysawant/air-passengers-time-series-arima"},{"metadata":{},"cell_type":"markdown","source":"# **THANK YOU ;)**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}