{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Please note some codes are commented out because of high computation requirements.\n\n## Data Wrangling\n\n\n#### Standard Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Necessary Imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.options.display.float_format = '{:.2f}'.format #Turning off scientific notations\n# import sweetviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text Analysis related imports\nimport nltk \nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model building Imports\nimport statsmodels.api as sm\nfrom sklearn import preprocessing\nfrom sklearn import tree, metrics\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n# from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \n# import pydotplus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Ingestion"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Data Ingestion\n\ndf=pd.read_csv(\"../input/zomato-restaurants-in-india/zomato_restaurants_in_India.csv\")\nprint(df.shape)\ndf.head(1).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking EDA Report through Sweetviz Package on the whole data"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# report = sweetviz.analyze(df)\n# report.show_html('full_data.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check for duplicates"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.duplicated().sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['res_id'].duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop_duplicates(subset='res_id') \n#Dropped duplicates on the basis of res_id as res_id is unique for every restaurant and for each branch, after this zipcode will be removed\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Missing value treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.isnull().sum() / len(df)).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Zip Code, has to be dropped as 80% values are missing and it is not much of a contributor to the analysis.\n# Locality can very well be used inplace of Zip Code.\n\n# Also, res_id now is just an identifier, and isn't of much use, so dropping this as well.\n\n# country_id is redundant as all restaurants pertain to India only, so dropping it.\n\n# url isn't much help here either, customers will order from Zomato, and Zomato already has all info, so dropping it.\n\n# Address and locality have extra info which isn't required as such because locality_verbose variable is here.\n\n# Dropping city_id, city name is available here\n\n# Currency field has to go, currency is INR only!\n\n# opentable_support has all 0 values, so this should be dropped too","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(['res_id','url','country_id','currency', 'address', 'locality','city_id', 'zipcode', 'opentable_support'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Prep : Fixing Incorrect Data Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delivery, takeaway and price_range columns are categorical and are stored as int, so this needs to be fixed.\ndf['delivery'] = df['delivery'].astype(object)\ndf['takeaway'] = df['takeaway'].astype(object)\ndf['price_range'] = df['price_range'].astype(object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['delivery'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['takeaway'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assuming -1 is a data entry error, so -1 is to be encoded as 1 for both delivery and takeaway\n\ndf['delivery'] = df['delivery'].replace(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['takeaway'] = df['takeaway'].replace(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(df['delivery'].value_counts(normalize=True)) #99% offer delivery, 1% don't\nprint('*****************************************')\nprint(df['takeaway'].value_counts(normalize=True)) #100% offer takeaway","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.heatmap(df.corr(), cmap='coolwarm', annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Photo_count and votes are highly correlated\n# Rest all seem to be in acceptable range","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualising the Dependent Variable (Aggregate Rating)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['aggregate_rating'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['aggregate_rating'] = df['aggregate_rating'].replace(0, np.nan)\ndf['average_cost_for_two'] = df['average_cost_for_two'].replace(0, np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['aggregate_rating'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting Not Null values only\ndf_nn = df[pd.notnull(df['aggregate_rating'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_nn['aggregate_rating'])\nplt.title('Histogram of aggregate_rating (not null)')\nplt.xlabel('aggregate_rating')\nplt.ylabel('Counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Zomato has a rating scale of 1-5 only, so restaurants rated 0 seem incorrect.\n# 0 is missing value here (could be unrated as rating_text for these values says not rated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing missing values for average_cost_for_two\ndf_nn['average_cost_for_two'].fillna(df_nn['average_cost_for_two'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.relplot(y = 'average_cost_for_two', x = 'aggregate_rating', size='average_cost_for_two', hue = 'aggregate_rating',\n            sizes= (15,200), data = df_nn)\nplt.title(\"Average Cost for Two vs. AggregateRating\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # Checking Sweetviz report again on this trimmed and cleaned version of the data\n\n# report_trim = sweetviz.analyze(df)\n# report_trim.show_html('data_trim.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploring the text columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Establishment\ndf_nn.establishment[0].replace('[','').replace(']','').replace(\"'\",'')\n\ndf_nn.establishment=df_nn.establishment.apply(lambda x:x.replace('[','').replace(']','').replace(\"'\",''))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"est_wc = ' '.join(df_nn['establishment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud \nwordcloud = WordCloud(width = 3000, height = 3000, \n                background_color ='black', \n                min_font_size = 10, random_state=100).generate(est_wc) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\")\nplt.xlabel('Word Cloud')\nplt.tight_layout(pad = 0) \n\nprint(\"Word Cloud of Establishment!!\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nn['establishment'].value_counts(dropna=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick Bites, Casual Dining, Cafe are the dominant establishment types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Highlights\ndf_nn.highlights[0].replace('[','').replace(']','').replace(\"'\",'')\n\ndf_nn.highlights=df_nn.highlights.apply(lambda x:x.replace('[','').replace(']','').replace(\"'\",''))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_nn['highlights'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since I want to do some analysis on the highlights vs ratings,\n# it is better tosplit the values of each record to extract different words, \n# Else, the whole data frame will become really cluttered\nsubset = df_nn[['highlights', 'aggregate_rating']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"high_split = subset['highlights'].str.get_dummies(sep = \",\")\n\nhigh_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset = pd.concat([subset, high_split], axis=1).drop('highlights', axis = 1)\nsubset","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"subset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df\nX = subset.drop(['aggregate_rating'], axis=1)\n\n# Declare a response variable, called y, and assign it the AdultWeekend column of the df \ny = subset['aggregate_rating'] \n\n# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \nscaler = preprocessing.StandardScaler().fit(X)\n\n# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \nX_scaled=scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gini_model = tree.DecisionTreeRegressor(criterion = 'mse', random_state=5)\n\ngini_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp=pd.Series(gini_model.feature_importances_,index=X.columns)\na = feature_imp.sort_values(ascending=False).head(20)\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Table booking recommended, Credit Card, Digital Payments Accepted, Outdoor seating, etc are the top highlights\n# affecting Ratings (out of all highlights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking if cuisine has an effect on rating "},{"metadata":{"trusted":true},"cell_type":"code","source":"subset2 = df_nn[['cuisines', 'aggregate_rating']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuisines = subset2['cuisines'].str.get_dummies(sep = ',')\ncuisines","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset2 = pd.concat([subset2, cuisines], axis=1).drop('cuisines', axis = 1)\nsubset2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Declare an explanatory variable, called X,and assign it the result of dropping 'aggregate_rating' from the df\nX2 = subset2.drop(['aggregate_rating'], axis=1)\n\n# Declare a response variable, called y, and assign it the aggregate_rating column of the df \ny2 = subset2['aggregate_rating'] \n\n# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \nscaler2 = preprocessing.StandardScaler().fit(X2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \nX_scaled2=scaler2.transform(X2)\n\ny2 = y2.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt2 = tree.DecisionTreeRegressor(criterion = 'mse', random_state=5)\n\ndt2.fit(X2, y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp=pd.Series(dt2.feature_importances_,index=X2.columns)\nb = feature_imp.sort_values(ascending=False).head(20)\nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Restaurants having Italian, Asian, Chinese cuisines are better rated than those that don't!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Developing train test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nn.columns # Using the subset without NULL Values for Aggregate_Rating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_nn.drop(['aggregate_rating', 'name', 'establishment', 'locality_verbose', 'cuisines', 'timings', 'highlights',\n               'rating_text'], axis=1)\ny = df_nn['aggregate_rating']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Regressor\ndt_model = tree.DecisionTreeRegressor(criterion = 'mse', random_state=5, max_features='sqrt')\n\ndt_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dot_data = StringIO()\n\n\n# tree.export_graphviz(dt_model, out_file=dot_data,  \n#                 filled=True, rounded=True,\n#                 special_characters=True, feature_names=X_train.columns) \n\n\n# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n# Image(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_dt = pd.Series(dt_model.predict(X_test))\n\nfrom sklearn.model_selection import cross_val_score\n\nnp.mean(cross_val_score(dt_model, X_test, y_test, cv=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE - DT Model\nfrom sklearn.metrics import mean_squared_error\nmse_dt = mean_squared_error(y_test, y_pred_dt)\nrmse_dt = np.sqrt(mse_dt)\nrmse_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForestRegressor\n\nreg_model = RandomForestRegressor(criterion = 'mse', random_state=5, max_features='sqrt')\n\nreg_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_reg = pd.Series(reg_model.predict(X_test))\n\nnp.mean(cross_val_score(reg_model, X_test, y_test, cv=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE - RF Model\nmse_rf = mean_squared_error(y_test, y_pred_reg)\nrmse_rf = np.sqrt(mse_rf)\nrmse_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# from lazypredict.Supervised import LazyRegressor\n\n# reg = LazyRegressor(verbose=0, predictions=False, custom_metric='None')\n# models,predictions = reg.fit(X_train, X_test, y_train, y_test)\n\n# models_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GB Regressor\n\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(criterion = 'mse', random_state=5, max_features='sqrt')\n\ngbr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gbr = pd.Series(gbr.predict(X_test))\n\nnp.mean(cross_val_score(gbr, X_test, y_test, cv=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE - GBR Model\nmse_gbr = mean_squared_error(y_test, y_pred_gbr)\nrmse_gbr = np.sqrt(mse_gbr)\nrmse_gbr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoost Regressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nadr = AdaBoostRegressor(random_state=5)\nadr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_adr = pd.Series(adr.predict(X_test))\n\nnp.mean(cross_val_score(adr, X_test, y_test, cv=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_adr = mean_squared_error(y_test, y_pred_adr)\nrmse_adr = np.sqrt(mse_adr)\nrmse_adr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import uniform, truncnorm, randint\n\nmodel_params = {\n    # randomly sample numbers from 4 to 110 estimators\n    'n_estimators': randint(1,110),\n#     # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n#     'max_features': truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n#     # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n#     'min_samples_split': uniform(0.01, 0.199)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nmod=RandomizedSearchCV(reg_model, model_params, n_iter=100, cv=5, random_state=5, n_jobs=-1) \n\nmod.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\npprint(mod.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building a tuned model with Best Parameters\nrf_t = RandomForestRegressor(criterion = 'mse', random_state=5, \n                             max_features='sqrt', \n                             min_samples_split=2,\n                            n_estimators=109, verbose=0,\n                            min_samples_leaf=1)\n\nrf_t.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rft = pd.Series(rf_t.predict(X_test))\n\nnp.mean(cross_val_score(rf_t, X_test, y_test, cv=5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_rft = mean_squared_error(y_test, y_pred_rft)\nrmse_rft = np.sqrt(mse_rft)\nrmse_rft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Metrics and Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MODEL_METRICS_RMSE\")\nprint(\"RMSE for Decision Tree Regressor : \" + str(rmse_dt))\nprint(\"RMSE for Random Forest Regressor : \" + str(rmse_rf))\nprint(\"RMSE for Gradient Boosting Regressor : \" + str(rmse_gbr))\nprint(\"RMSE for AdaBoost Regressor : \" + str(rmse_adr))\nprint(\"RMSE for RandomForest_pruned Model : \" +str(rmse_rft))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MODEL_METRICS_R2\")\nprint(\"R2 for Decision Tree Regressor : \" +str(np.mean(cross_val_score(dt_model, X_test, y_test, cv=5))))\nprint(\"R2 for RandomForest Regressor : \" +str(np.mean(cross_val_score(reg_model, X_test, y_test, cv=5))))\nprint(\"R2 for Gradient Boosting Regressor : \" + str(np.mean(cross_val_score(gbr, X_test, y_test, cv=5))))\nprint(\"R2 for Adaboost Regressor : \" +str(np.mean(cross_val_score(adr, X_test, y_test, cv=5))))\nprint(\"R2 for RandomForest Pruned Model : \" + str(np.mean(cross_val_score(rf_t, X_test, y_test, cv=5))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE is the least for RandomForest (Pruned) Model\n# R2 is maximum for RandomForest (Pruned) Model\n\n#So, finalising this model!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Feature Importances\n\nfi_rft=pd.Series(rf_t.feature_importances_,index=X_train.columns)\nd = fi_rft.sort_values(ascending=False).head(20)\nd\n\n# Votes and photo_count contribute max to aggregate_rating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}