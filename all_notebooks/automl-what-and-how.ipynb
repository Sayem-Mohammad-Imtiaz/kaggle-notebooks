{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://media1.tenor.com/images/1c519d623788d3c049c47b0873dc5bc7/tenor.gif?itemid=15055034)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### In this Notebook, I have tried to compare various AUTOML frameworks that are commonly used and what AUTOML does is exactly what the guy says above, you telling the framework to do the heavy-lifting.\n\n\n### This notebook also has some background on each framework along with the code snippet. Do let know your  thoughts about AutoML and any other framework that you know of. \n\n\n### Contents\n\n1. [What is AUTOML](#at)\n\n2. [Auto-sklearn](#ask)\n\n3. [MLBOX](#mlbox)\n\n4. [H2o](#h2o)\n\n5. [TPOT](#tpo)\n\n6. [Autokeras](#ak)\n\n7. [Other frameworks](#oth)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"at\"></a>\n\n### What Is AutoML?\n\nAutoML (automated machine learning) refers to the automated end-to-end process of applying machine learning in real and practical scenarios.\n\nA typical machine learning model includes the four following steps:\n\n![](https://yqintl.alicdn.com/7e193d8335256ae97a8bbb94afd225435e7c60f2.png)\n\n\nRight from ingesting data to pre-processing, optimization, and then predicting outcomes, every step is controlled and performed by humans. AutoML essentially focuses on two major aspects — data acquisition/collection and prediction. All the other steps that take place in between can be easily automated while delivering a model that’s optimized well and ready to make predictions.\n\nThe success of machine learning in a wide range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts¹. AutoML tends to automate the maximum number of steps in an ML pipeline—with a minimum amount of human effort and without compromising the model’s performance.\n\n\n### Advantages\n\n#### The advantages of AutoML can be summed up in three major points:\n\n    - Increases productivity by automating repetitive tasks. This enables a data scientist to focus more on the problem rather than the models.\n\n    - Automating the ML pipeline also helps to avoid errors that might creep in manually.\n\n    - Ultimately, AutoML is a step towards democratizing machine learning by making the power of ML accessible to everybody.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let us begin to look at the various frameworks, along with code.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.target.values\nx_data = df.drop(['target'], axis = 1)\n\n# Normalize\n#x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\n#x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ask\"></a>\n\n### AUTO-SKLEARN\n\n\n![](https://miro.medium.com/max/512/1*s2myX8bJIp9mQ2V_htcEpw.png)\n\n\n### We begin with auto-sklearn which is basically the auto-ml component of the most used ML framework SCIKIT-LEARN.\n\n### Auto-sklearn frees a machine learning user from algorithm selection and hyperparameter tuning. It leverages recent advantages in Bayesian optimization, meta-learning and ensemble construction. [Here](http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf) is the paper for this framework, in case you are interested. \n\n### It Creates a pipeline and optimizes it using Bayesian search. Two components are added to Bayesian hyperparameter optimization of an ML framework: meta-learning for initializing the Bayesian optimizer and automated ensemble construction from configurations evaluated during optimization. It performs well on small and medium-sized datasets, but it cannot be applied to modern deep learning systems that yield state-of-the-art performance on large datasets.\n\n\n### Lets build a model now.\n\n\n### Here are the main features of the API. You can:\n\n    - Set time and memory limits\n    - Restrict the searchspace by selecting or excluding some preprocessing methods or some estimators\n    - Specify some resampling strategies (e.g. 5-fold cv)\n    - Perform some parallel computation with the SMAC algorithm (sequential model-based algorithm configuration) that stores some data on a shared file system\n    - Save your model as you would do with scikit-learn (pickle)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install -U scikit-learn\n\nimport sklearn\nprint(sklearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl -OL https://github.com/AxeldeRomblay/mlbox/tarball/3.0-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get -y remove swig\n\n!apt-get -y install swig3.0 build-essential -y\n\n!ln -s /usr/bin/swig3.0 /usr/bin/swig\n!apt-get -y install build-essential\n\n#!pip install --upgrade setuptools\n#!pip install auto-sklearn\n#!pip install --no-cache-dir -v pyrfr\n\n\n#try:\n #   import autosklearn.classification\n#except:\n #   pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/automl/auto-sklearn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Is ensemble default in auto-sklearn?\n\n    - Yes, refer Introduction section in the paper mentioned above\n    \n    \nBelow is a simple auto-sklearn pipeline    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip uninstall -y scikit-learn\n\n#import sklearn\n#print(sklearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install scikit-learn\n\n#import sklearn\n#print(sklearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** IGNORE THE ERRORS FROM MLBOX AND AUTOSKLEARN** , THERE IS A PACKAGE ISSUE IN KAGGLE AND IT SOMETIMES DOESNT WORK AND SOMETIMES WORKS!! THATS WHY HAVE COMMENTED OUT THE CODE, BUT IT WORKS!! \n\n\nA RELATED THREAD -- https://www.kaggle.com/general/64808","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn import model_selection, metrics\n#import sklearn\n#import autosklearn\n#import autosklearn.classification\n\n#%timeit\n\n#X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(x_data, y, random_state=1)\n\n#automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=3600,\n#per_run_time_limit=300,resampling_strategy='cv', resampling_strategy_arguments={'folds': 5},\n#include_preprocessors=[\"no_preprocessing\"],ensemble_size=2)\n\n# Do not construct ensembles in parallel to avoid using more than one\n# core at a time. The ensemble will be constructed after auto-sklearn\n# finished fitting all machine learning models.\n\n#automl.fit(X_train, y_train)\n\n# This call to fit_ensemble uses all models trained in the previous call\n# to fit to build an ensemble which can be used with automl.predict()\n\n#automl.fit_ensemble(y_train, ensemble_size=50)\n\n#print(automl.show_models())\n\n#predictions = automl.predict(X_test)\n\n#print(automl.sprint_statistics())\n\n#print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pros:\n\n    - Easy to get started if you know sklearn\n    \n    - Has parameters/hyper parameters similar to SKLEARN API, so a quick baseline shouldn't take time.\n    \n    - Support Model Persistence and Parallel Computation\n    \n    \n### Cons:\n\n    - Installation of the package is not straight forward and has lots of dependencies, be it in kaggle or outside.\n    \n    - Not as explanatory in terms of the results and doesn't have plenty of options that other tools have. \n    \n    - Takes a lot of time to return the results. For a simple baseline with the above dataset, it took nearly an hour.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"mlbox\"></a>\n\n### ML-BOX\n\n\nMLBox is a powerful Automated Machine Learning python library.\n\nAccording to the official document, it provides the following features:\n\n    - Fast reading and distributed data preprocessing/cleaning/formatting\n    \n    - Highly robust feature selection and leak detection as well as accurate hyper-parameter optimization\n    \n    - State-of-the art predictive models for classification and regression (Deep Learning, Stacking, LightGBM,...)\n    \n    - Prediction with model interpretation\n\n\nMLBox focuses on the below three points in particular in comparison to the other libraries:\n\n    - Drift Identification – A method to make the distribution of train data similar to the test data.\n    - Entity Embedding – A categorical features encoding technique inspired from word2vec.\n    - Hyperparameter Optimization\n\n\n### MLBox architecture\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/07/03230616/Screenshot-from-2017-07-03-23-05-23.png)\n\n\nMLBox main package contains 3 sub-packages:\n\n    - Pre-processing: reading and pre-processing data\n\n    - Optimization: testing or optimizing a wide range of learners\n    \n    - Prediction: predicting the target on a test dataset\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install mlbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import warnings\n#warnings.filterwarnings(\"ignore\")\n\n#from mlbox.preprocessing.reader import Reader\n#from mlbox.preprocessing.drift_thresholder import Drift_thresholder\n#from mlbox.optimisation.optimiser import Optimiser \n#from mlbox.prediction.predictor import Predictor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** IGNORE THE ERRORS FROM MLBOX AND AUTOSKLEARN** , THERE IS A PACKAGE ISSUE IN KAGGLE AND IT SOMETIMES DOESNT WORK AND SOMETIMES WORKS!! \n\n\nA RELATED THREAD -- https://www.kaggle.com/general/64808","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Inputs to MLBox\n\n### If you're having a train and a test set like in any Kaggle competition, you can feed these two paths directly to MLBox as well as the target name.\n\n### Otherwise, if fed a train set only, MLBox creates a test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"paths = [\"../input/titanic/train.csv\", \"../input/titanic/test.csv\"] \n\ntarget_name = \"Survived\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading and preprocessing\n\nThe Reader class of MLBox is in charge of preparing the data.\n\nIt basically provides methods and utilities to:\n\n    - Read in the data with the correct separator (csv, xls, json, and h5) and load it\n    - Clean the data by ;\n        - deleting Unnamed columns\n        - inferring column types (float, int, list)\n        - processing dates and extracting relevant information from it: year, month, day, dayofweek, hour, etc. removing duplicates\n        - Prepare train and test splits","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#rd = Reader(sep=\",\")\n#df = rd.train_test_split(paths, target_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### When this function is done running, it creates a folder named save where it dumps the target encoder for later use.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df[\"train\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### DRIFT REMOVAL\n\n    This is an innovative feature I haven't encountered in other packages.\n\n    The main idea is to automatically detect and remove variables that have a distribution that is substantially different between the train and the test set.\n\n    This happens quite a lot and we generally talk about biased data. \n\n    You could have for example a situation when the train set has a population of young people whereas the test has elderly only. This indicates that the age feature is not robust and may lead to a poor performance of the model when testing. So it has to be discarded.\n\n#### How does MLBox compute drifts for individual variables? \n\n\n    MLBox builds a classifier that separates train from test data. It then uses the ROC score related to this classifier as a measure of the drift.\n\n    This makes sense:\n\n    If the drift score is high (i.e. the ROC score is high) the ability the discern train data from test data is easy, which means that the two distributions are very different.\n    Otherwise, if the drift score is low (i.e. the ROC score is low) the classifier is not able to separate the two disctributions correctly.\n    MLBox provides a class called Drift_thresholder that takes as input the train and test sets as well as the target and computes a drift score of each one of the variables.\n\n    Drift_thresholder then deletes the variables that have a drift score higher that a threshold (default to 0.6).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#dft = Drift_thresholder()\n#df = dft.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The heavy lifting : optimizing\n\n#### All the functionalities inside this sub-package can be used via the command-\n\n    from mlbox.optimisation import *\n\nThis hyper-parameter optimisation method in this library uses the hyperopt library which is very fast and you can almost optimise anything in this library from choosing the right missing value imputation method to the depth of an XGBOOST model. This library creates a high-dimensional space of the parameters to be optimised and chooses the best combination of the parameters that lowers the validation score.\n\n#### This section performs the optimisation of the pipeline and tries different configurations of the parameters:\n\n    - NA encoder (missing values encoder)\n    - CA encoder (categorical features encoder)\n    - Feature selector (OPTIONAL)\n    - Stacking estimator - feature engineer (OPTIONAL)\n    - Estimator (classifier or regressor)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#opt = Optimiser()\n\n# Then we can run it using the default model configuration set as default (LightGBM) without any autoML or complex grid search.\n\n# This should be the first baseline\n\n#warnings.filterwarnings('ignore', category=DeprecationWarning)\n#score = opt.evaluate(None, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we get to define a space of multiple configurations:\n\n    ne_numericalstrategy: how to handle missing data in numerical features\n    ce__strategy: how to handle categorical variables encoding\n    fs: feature selection\n    stck: meta-features stacker\n    est: final estimator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"space = {\n        'ne__numerical_strategy':{\"search\":\"choice\",\"space\":[0, \"mean\"]},\n        'ce__strategy':{\"search\":\"choice\", \"space\":[\"label_encoding\", \"random_projection\", \"entity_embedding\"]}, \n        'fs__threshold':{\"search\":\"uniform\", \"space\":[0.001, 0.2]}, \n        'est__strategy':{\"search\":\"choice\", \"space\":[\"RandomForest\", \"ExtraTrees\", \"LightGBM\"]},\n        'est__max_depth':{\"search\":\"choice\", \"space\":[8, 9, 10, 11, 12, 13]}\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step1: \n\nCreate an object of class Optimiser which has the parameters as ‘scoring’ and ‘n_folds’. Scoring is the metric against which we want to optimise our hyper-parameter space and n_folds is the number of folds of cross-validation\nScoring values for Classification- \"accuracy\", \"roc_auc\", \"f1\", \"log_loss\", \"precision\", \"recall\"\nScoring values for Regression- \"mean_absolute_error\", \"mean_squarred_error\", \"median_absolute_error\", \"r2\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#opt = Optimiser(scoring=\"accuracy\",n_folds=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step2:\n\nUse the optimise function of the object created above which takes the hyper-parameter space, dictionary created by the train_test_split and number of iterations as the parameters. This function returns the best hyper-paramters from the hyper-parameter space.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#opt.evaluate(params, df)\n\n#best=opt.optimise(space, df, 40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There's clearly very good potential of more improvement if we define a better space of search or stacking operations and maybe other feature selection techniques. You can also see the best hyper parameters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Running predictions\n\n#### we fit the optimal pipeline and predict on our test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n#prd = Predictor()\n#prd.fit_predict(best, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above method saves the feature importance, drift variables coefficients and the final predictions into a separate folder named ‘save’.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"h2o\"></a>\n\n### H2o\n\n\nH2O is an open source, in memory, distributed, fast and scalable machine learning and predictive analytics that allow building machine learning models to be an ease. They have majorly 2 products, H2o3 which is open source and DRIVERLESS AI which is a paid product and the rest are related to their Big data offerings as shown below.\n\n\nH2O includes an automatic machine learning module that uses its own algorithms to build a pipeline. It performs an exhaustive search over its feature engineering methods and model hyperparameters to optimize its pipelines\nH2O automates some of the most difficult data science and machine learning workflows, such as feature engineering, model validation, model tuning, model selection and model deployment. In addition to this, it also offers automatic visualizations and machine learning interpretability (MLI).\n\n\n![](https://miro.medium.com/max/512/1*vQe69lEIajJFl86sWjDrnQ.png)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import h2o\nfrom h2o.automl import H2OAutoML","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.init()\n#df = h2o.import_file(\"../input/heart-disease-uci/heart.csv\")\ndf = h2o.import_file(\"../input/titanic/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = df.split_frame([0.7], seed=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We already have our train and test sets, so we just need to choose our response variable, as well as the predictors. We will do the same thing that we did for the first tutorial.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = \"Survived\"\n\nignore = [\"Survived\", \"PassengerId\", \"Name\"] \n\nx = list(set(train.names) - set(ignore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = df.split_frame(ratios=[0.7], seed=1)\n\ntrain = splits[0]\n\ntest = splits[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = \"Survived\" \n\nx = df.columns \n\nx.remove(y) \n\nx.remove(\"PassengerId\")\n\nx.remove(\"Name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nNow we are ready to run AutoML. Below you can see some of the default parameters that we could change for AutoML.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#H2OAutoML(nfolds=5, max_runtime_secs=3600, max_models=None, stopping_metric='AUTO', stopping_tolerance=None, stopping_rounds=3, seed=None, project_name=None)\n\naml = H2OAutoML(max_models=25, max_runtime_secs_per_model=30, seed=42)\n\n%time aml.train(x=x, y=y, training_frame=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aml = H2OAutoML(max_runtime_secs=120, seed=1)\n\naml.train(x=x,y=y, training_frame=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only required parameters for H2O's AutoML are, ytraining_frame, and max_runtime_secs, which let us train AutoML for ‘x' amount of seconds and/or max_models, which would train a maximum number of models. Please note that max_runtime_secs has a default value, while max_models does not. For this task, we will set a number of models constraint. The seed is the usual parameter that we set for reproducibility purposes. We also need a project name because we will do both classification and regression with AutoML.\n\nThe second line of code has the parameters that we need in order to train our model. For now, we will just pass x, y, and the training frame. Please note that the parameter x is optional because if you were using all the columns in your dataset, you would not need to declare this parameter. The leaderboard frame can be used to score and rank models on the leaderboard, but we will use the validation scores to do so because we will check the performance of our models with the test set.\n\nOnce AutoML is finished, print the leaderboard, and check out the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lb = aml.leaderboard\n\n# lb.head(rows=lb.nrows)\n\nlb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also print a leaderboard with the training time, in milliseconds, of each model and the time it takes each model to predict each row, in milliseconds:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from h2o.automl import get_leaderboard\n\nlb2 = get_leaderboard(aml, extra_columns='ALL')\n\nlb2.head(rows=lb2.nrows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the leaderboard, we can see that the best model at the top. The Ensembles will usually have a GLM, a Distributed Random Forest, Extremely-Randomized Forest, a GBM, and XGBoost, and Deep Learning model if you give it enough time to train all those models. Let's explore the coefficients of the metalearner to see the models in the Stacked Ensemble with their relative importance.\n\nFirst, let's retrieve the metalearner, and we can do it as follow:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get model ids for all models in the AutoML Leaderboard\n\nmodel_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n\n# Get the \"All Models\" Stacked Ensemble model\n\nse = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_BestOfFamily\" in mid][0])\n\n# Get the Stacked Ensemble metalearner model\n\nmetalearner = h2o.get_model(se.metalearner()['name'])\nmetalearner.coef()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the list above, we can see that the most important model used in our Stacked Ensemble is GLM.\n\n#### We can also plot the standardized coefficients with the following code (assuming you retrieved the metalearner from the step above):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metalearner.std_coef_plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now lets check the performance in our test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aml.leader.model_performance(test_data=test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%matplotlib inline\n#aml.leader.model_performance(test_data=test).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lastly, let's make some predictions on our test set.\n\n\npred = aml.predict(test)\n\npred.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the Leader Model\n\nYou can also save and download your model and use it for deploying it to production.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.save_model(aml.leader, path=\"./output\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pros:\n\n    - very intuitive and lots of custom options to build models\n    \n    - easy to get started and build a baseline.\n    \n    - usage of H20 Flow in Web UI enables quick development and sharing of the analytical model\n    \n    - Readily available algorithms, easy to use in your analytical projects\n\n    - Faster than Python scikit learn (in machine learning supervised learning area)\n    \n    - Well documented and suitable for fast training or self studying\n\n \n### Cons:\n\n    - Mostly the best models are Stacked ensembles although they end up as the best model, but not a lot of options to do DL, especially the latest methods. Although compared to other tools, they are ahead. So DL model options can be added to make it better. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"tpo\"></a>\n\n\n![](https://miro.medium.com/max/450/0*dCD9QwVjhVnKKz6U.jpg)\n\n\n### TPOT(Tree-Based Pipeline Optimization Tool)\n\n\nTPOT is a tree-based pipeline optimization tool that uses genetic algorithms to optimize machine learning pipelines. TPOT is built on top of scikit-learn and uses its own regressor and classifier methods. TPOT explore thousands of possible pipelines and finds the one that best fit the data.\n\nTPOT cannot automatically process natural language inputs. Additionally, it’s also not able to processes categorical strings, which must be integer-encoded before being passed in as data.\n\n\n\n![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537396029/output_2_0_d7uh0v.png)\n\n\n#### SOURCE - DATACAMP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/titanic/train.csv\") \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna(-999)\n\ndf_class = df['Survived'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntraining_indices, validation_indices = training_indices, testing_indices = train_test_split(df.index,\n                                                                                            stratify = df_class,\n                                                                                            train_size=0.75, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_indices.size, validation_indices.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For TPOT, everything needs to be in float or int, therefore deleting variables that are not those for example purpose.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.info()\ndf.drop(['Name', 'PassengerId', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TPOTClassifier has a wide variety of parameters, and you can read all about them here. But the most notable ones you must know are:\n\n    generations: Number of iterations to the run pipeline optimization process. The default is 100.\n\n    population_size: Number of individuals to retain in the genetic programming population every generation. The default is 100.\n\n    offspring_size: Number of offspring to produce in each genetic programming generation. The default is 100.\n\n    mutation_rate: Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation. Default is 0.9\n\n    crossover_rate: Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to \"breed\" every generation.\n\n    scoring: Function used to evaluate the quality of a given pipeline for the classification problem like accuracy, average_precision, roc_auc, recall, etc. The default is accuracy.\n\n    cv: Cross-validation strategy used when evaluating pipelines. The default is 5.\n\n    random_state: The seed of the pseudo-random number generator used in TPOT. Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from tpot import TPOTClassifier\n#from tpot import TPOTRegressor\n\n#tpot = TPOTClassifier(generations=5, verbosity=2)\n#tpot.fit(df.drop('Survived', axis=1).loc[training_indices].values, df.loc[training_indices,'Survived'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above, 5 generations were computed, each giving the training efficiency of the fitting model on the training set. \n\nAs evident, the best pipeline is the one that has the CV accuracy score of 74%. The model that produces this result is the pipeline, Consisting of ET classifier. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#tpot.score(df.drop('Survived',axis=1).loc[validation_indices].values,  df.loc[validation_indices, 'Survived'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### You can also tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the export function and I personally think this is an amazing feature:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#tpot.export('pipeline.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### PROS:\n\n    - Very easy to get started, install and build a quick model\n    \n    - Easy preprocessing\n\n\n### CONS:\n\n    TPOT can take a long time to finish its search. Running TPOT isn’t as simple as fitting one model on the dataset. It is considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with numerous preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyper-parameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline. That’s why it usually takes a long time to execute and isn’t feasible for large datasets.\n\n    TPOT can recommend different solutions for the same dataset. If you're working with a reasonably complex dataset or run TPOT for a short amount of time, different TPOT runs may result in different pipeline recommendations. When two TPOT runs recommend different pipelines, this means that the TPOT runs didn't converge due to lack of time or that multiple pipelines perform more-or-less the same on your dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ak\"></a>\n\n\n### AUTO-KERAS\n\n\n#### One of the most widely used AUTOML framework, as the name says it all it is an AutoML system based on Keras. keras is a popular and widely used DL framework, so the support is huge.\n\n#### The API’s design follows the classic design of the Scikit-Learn API; hence, it’s extremely simple to use. The current version provides functionalities to automatically search for hyperparameters during the deep learning process. Auto-Keras tends to simplify the ML process through the use of automated Neural Architecture Search (NAS) algorithms. Neural Architecture Search essentially replaces the deep learning engineer/practitioner with a set of algorithms that automatically tunes the model.\n\n\n#### For deep learning, for now you have the ImageClassifier, the BayesianSearcher, a Graph module, a PreProcessor, a LayerTransformer, a NetTransformer, a ClassifierGenerator and some utilities. This is an evolving package.\n\n\n#### For tabular dataset, keras has a method called StructuredDataClassifier which is what we will use. Keras also has Automodel that combines multiple inputs. So in simple words, StructuredDataClassifier, TextClassifiers etc. are all TaskAPIs. \n\nWhen doing a classical task such as image classification/regression, text classification/regression, ..., you can use the simplest APIs provided by autokeras called Task API: ImageClassifier, ImageRegressor, TextClassifier, TextRegressor, ... In this case you have one input (image or text or tabular data, ...) and one output (classification, regression). \n\nAutomodel, however when you are in a situation where you have for example a task that requires multi inputs/outputs architecture, then you cannot use directly Task API, and this is where Automodel comes into play with the I/O API. \n\nGraphAutomodel works like keras functional API. It assembles different blocks (Convolutions, LSTM, GRU, ...) and create a model using this block, then it will look for the best hyperparameters given this architecture you provided. \n    \n\nKeras AUTOMODEL is mainly used for Multi-Modal and Multi-Task. \n\nMulti-model data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data.\n\nMulti-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1.\n\n    \n#### Now, lets do the modeling using StructuredDataClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install autokeras\n\n!pip install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport autokeras as ak\n\nx_data = pd.read_csv(\"../input/titanic/train.csv\")\n\nprint(type(x_data))\n\ny = x_data.pop('Survived')\n\nprint(type(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = pd.DataFrame(y)\n\nprint(type(y_train)) \n\n# You can also use numpy.ndarray for x_train and y_train.\n\nx_train = x_data.to_numpy().astype(np.unicode)\n\ny_train = y.to_numpy()\n\nprint(type(x_train)) \n\nprint(type(y_train)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing testing data.\n\nx_test = pd.read_csv(\"../input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\n\nfrom sklearn import model_selection, metrics\n%timeit\n\nx_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x_data, y, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The arguments used in structured data classification class:\n\n#### Arguments\n\ncolumn_names: A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame.\n\ncolumn_types: Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data.\n\nnum_classes: Int. Defaults to None. If None, it will be inferred from the data.\n\nmulti_label: Boolean. Defaults to False.\n\nloss: A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes.\n\nmetrics: A list of Keras metrics. Defaults to use 'accuracy'.\n\nproject_name: String. The name of the AutoModel. Defaults to 'structured_data_classifier'.\n\nmax_trials: Int. The maximum number of different Keras Models to try. The search may finish before reaching the \nmax_trials. Defaults to 100.\n\ndirectory: String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory.\n\nobjective: String. Name of model metric to minimize or maximize. Defaults to 'val_accuracy'.\ntuner Union[str, Type[autokeras.engine.tuner.AutoTuner]]: String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models.\n\noverwrite: Boolean. Defaults to False. If False, reloads an existing project of the same name if one is found. Otherwise, \noverwrites the project.\n\nseed: Int. Random seed.\n\n**kwargs: Any arguments supported by AutoModel.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = ak.StructuredDataClassifier(overwrite=True , max_trials=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feed the structured data classifier with training data.\n\nclf.fit(x_train, y_train, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict with the best model.\n\npredicted_y = clf.predict(x_test)\n\n# Evaluate the best model with testing data.\n\nprint(clf.evaluate(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Evaluate Returns:\n\nScalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.\n\n\nThe model can also be saved/exported as done in the keras models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.export_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Customized Search Space\n\nFor advanced users, you may customize your search space by using AutoModel instead of StructuredDataClassifier. \n\nYou can configure the StructuredDataBlock for some high-level configurations, e.g., categorical_encoding for whether to use the CategoricalToNumerical. You can also do not specify these arguments, which would leave the different choices to be tuned automatically and that is the major difference between using a StructuredDataClassifier and AutoModel.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pros:\n\n    - Quick set up of framework and can build a baseline quickly.\n    \n    - Imitates Keras API and therefore easy to use.\n    \n    \n### Cons:\n\n    - Compared with other frameworks, a bit of a a blackbox when it comes to knowing the best model that the automl chose.\n    \n    - As the name says it, its dependent on keras and is only DL models whereas frameworks like h2o also has other ML models being used that may sometimes be useful in a production setting where explainability is key. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"oth\"></a>\n\n### Other Frameworks like Cloud AutoML by google, TransmogrifAI by Salesforce are also in the market but both cant be tried here as the former is a paid Version and the latter needs Spark to be installed to be tried. If you are interested you can read about them [here](https://cloud.google.com/automl/) and [here](https://transmogrif.ai/)\n\n\n### Cloud AutoML is a suite of machine learning products from Google that enables developers with limited machine learning expertise to train high-quality models specific to their business needs by leveraging Google’s state-of-the-art transfer learning and Neural Architecture Search technology. Cloud AutoML provides a simple graphical user interface (GUI) to train, evaluate, improve, and deploy models based on your own data. \n\n\n\n### TransmogrifAI:\n\n### is an open source automated machine learning library from Salesforce. The company’s flagship ML platform called Einstein is also powered by TransmogrifAI. It is an end-to-end AutoML library for structured data written in Scala that runs on top of Apache Spark. \n\n### TransmogrifAI is especially useful when you need to :\n    \n    Rapidly train good quality machine learning models with minimal hand tuning.\n     \n    Build modular, reusable, strongly-typed machine learning workflows\n    \n\n#### You may read the examples [here](https://docs.transmogrif.ai/en/stable/)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The Reason for this notebook was to check the various Automl frameworks that are widely used and also try them Hands-on. Do let know your thoughts and upvote. :)\n\n\n### There are debates whether AUTOML would replace data scientists in the future and my personal take is that we are still a long way to go for that and that DS is more than just .fit() and .predict() and tuning hyperparameters and data preparation play a huge role in that and thats not something, automl can do. It is what you feed to it, is returned and hence the process before the modeling is something that cant be replaced. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}