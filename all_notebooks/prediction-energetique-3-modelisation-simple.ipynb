{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width:100%;text-align: center;\">\n    <img src=\"https://user.oc-static.com/upload/2019/02/24/15510245026714_Seattle_logo_landscape_blue-black.png\" />\n</div>\n\n# Introduction\n\nVous travaillez pour la ville de Seattle. Pour atteindre son objectif de ville neutre en émissions de carbone en 2050, votre équipe s’intéresse de près aux émissions des bâtiments non destinés à l’habitation.\n\nDes relevés minutieux ont été effectués par vos agents en 2015 et en 2016. Cependant, ces relevés sont coûteux à obtenir, et à partir de ceux déjà réalisés, vous voulez tenter de prédire les émissions de CO2 et la consommation totale d’énergie de bâtiments pour lesquels elles n’ont pas encore été mesurées.\n\nVotre prédiction se basera sur les données déclaratives du permis d'exploitation commerciale (taille et usage des bâtiments, mention de travaux récents, date de construction..)\n\nVous cherchez également à évaluer l’intérêt de l’\"ENERGY STAR Score\" pour la prédiction d’émissions, qui est fastidieux à calculer avec l’approche utilisée actuellement par votre équipe.\n","metadata":{}},{"cell_type":"markdown","source":"# Sommaire\n\n1. [Prétraitement](#preprcessing)  \n    1.1. [Chargement et contrôle des données](#data-load-and-check)  \n    1.2. [Préparation des jeux de données](#data-prepare)   \n    1.2. [Cas des outliers](#outliers)\n2. [Régression linéaire basique](#simple-linear-regression)  \n3. [Comparaison des modèles](#model-compare)  \n4. [Comparaison des modèles sans le EnergyStarScore](#model-compare-without-ESS)  \n5. [Conclusion](#conclude)","metadata":{}},{"cell_type":"code","source":"# import des bibliothèques utilisées\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import *\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import *\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:28.42544Z","iopub.execute_input":"2021-08-21T19:48:28.425837Z","iopub.status.idle":"2021-08-21T19:48:29.940106Z","shell.execute_reply.started":"2021-08-21T19:48:28.425753Z","shell.execute_reply":"2021-08-21T19:48:29.939125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"preprocessing\">Prétraitement<a/> \n\n##  <a id=\"data-load-and-check\">Chargement et contrôle des données<a/>   ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/refined-data/data_hard_refined.csv\")\nprint(data.columns)\nprint(data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:29.941523Z","iopub.execute_input":"2021-08-21T19:48:29.941845Z","iopub.status.idle":"2021-08-21T19:48:30.034654Z","shell.execute_reply.started":"2021-08-21T19:48:29.941815Z","shell.execute_reply":"2021-08-21T19:48:30.033676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.036281Z","iopub.execute_input":"2021-08-21T19:48:30.036579Z","iopub.status.idle":"2021-08-21T19:48:30.108589Z","shell.execute_reply.started":"2021-08-21T19:48:30.036552Z","shell.execute_reply":"2021-08-21T19:48:30.107452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.ComplianceStatus.unique())\nprint(data.DefaultData.unique())","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.110279Z","iopub.execute_input":"2021-08-21T19:48:30.110646Z","iopub.status.idle":"2021-08-21T19:48:30.122072Z","shell.execute_reply.started":"2021-08-21T19:48:30.110613Z","shell.execute_reply":"2021-08-21T19:48:30.120979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <a id=\"data-prepare\">Préparation des jeux de données<a/>   ","metadata":{}},{"cell_type":"markdown","source":"Pour répondre au consigne de l'énoncé nous devons supprimer les données liées à la consommation énergétique car les relevés sont coûteux à obtenir. Nous supprimons donc la colonne 'SteamUse(kBtu)'","metadata":{}},{"cell_type":"code","source":"data = data.drop(['SteamUse(kBtu)'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.123786Z","iopub.execute_input":"2021-08-21T19:48:30.12411Z","iopub.status.idle":"2021-08-21T19:48:30.130782Z","shell.execute_reply.started":"2021-08-21T19:48:30.12408Z","shell.execute_reply":"2021-08-21T19:48:30.12957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.13226Z","iopub.execute_input":"2021-08-21T19:48:30.132578Z","iopub.status.idle":"2021-08-21T19:48:30.163487Z","shell.execute_reply.started":"2021-08-21T19:48:30.132549Z","shell.execute_reply":"2021-08-21T19:48:30.1623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Certaines colonnes ne sont pas numérique, il va falloir les modifier. Analysons plus en détail leur contenu:","metadata":{}},{"cell_type":"code","source":"objectColumns = list(data.dtypes[data.dtypes == np.object].index)\nnumericColumns = list(data.dtypes[data.dtypes != np.object].index)\nprint(objectColumns)\nprint(numericColumns)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.164637Z","iopub.execute_input":"2021-08-21T19:48:30.164919Z","iopub.status.idle":"2021-08-21T19:48:30.172875Z","shell.execute_reply.started":"2021-08-21T19:48:30.164892Z","shell.execute_reply":"2021-08-21T19:48:30.171851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in objectColumns:\n    print('{}: {} uniques values'.format(column,len(data[column].unique())))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.176027Z","iopub.execute_input":"2021-08-21T19:48:30.176464Z","iopub.status.idle":"2021-08-21T19:48:30.190678Z","shell.execute_reply.started":"2021-08-21T19:48:30.176402Z","shell.execute_reply":"2021-08-21T19:48:30.188643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Certaines colonnes sont liées à l'identification du batiment. C'est le cas de 'PropertyName', 'Address'. Nous allons donc les enlever car peu exploitable. De plus si on voulait utiliser une technique liée à la proximité entre les batiments, la longitude et la latitude serait plus facilement utilisable que l'adresse. Les autres sont des variables catégorielles. ","metadata":{}},{"cell_type":"code","source":"data = data.drop(['PropertyName', 'Address'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.193267Z","iopub.execute_input":"2021-08-21T19:48:30.193724Z","iopub.status.idle":"2021-08-21T19:48:30.201019Z","shell.execute_reply.started":"2021-08-21T19:48:30.193678Z","shell.execute_reply":"2021-08-21T19:48:30.199689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Idem pour 'OSEBuildingID', 'TaxParcelIdentificationNumber' dans les variables numériques.","metadata":{}},{"cell_type":"code","source":"data = data.drop(['OSEBuildingID', 'TaxParcelIdentificationNumber'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.202688Z","iopub.execute_input":"2021-08-21T19:48:30.203114Z","iopub.status.idle":"2021-08-21T19:48:30.214885Z","shell.execute_reply.started":"2021-08-21T19:48:30.203069Z","shell.execute_reply":"2021-08-21T19:48:30.213767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# suite à la suppression des outliers et à nos nouvelles colonnes par surface on peut supprimer  les anciennes colonnes\n# data.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.216373Z","iopub.execute_input":"2021-08-21T19:48:30.216792Z","iopub.status.idle":"2021-08-21T19:48:30.228224Z","shell.execute_reply.started":"2021-08-21T19:48:30.216749Z","shell.execute_reply":"2021-08-21T19:48:30.227481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on remet à jour la liste des colonnes catégorielles\nobjectColumns = list(data.dtypes[data.dtypes == np.object].index)\nnumericColumns = list(data.dtypes[data.dtypes != np.object].index)\nprint(objectColumns)\nprint(numericColumns)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.229304Z","iopub.execute_input":"2021-08-21T19:48:30.229727Z","iopub.status.idle":"2021-08-21T19:48:30.243715Z","shell.execute_reply.started":"2021-08-21T19:48:30.229678Z","shell.execute_reply":"2021-08-21T19:48:30.242545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"N'ayant pas fait d'analyse poussée sur les outliers je vais utiliser le RobustScaler (les statistiques de centrage et de mise à l'échelle de RobustScaler sont basées sur des centiles et ne sont donc pas influencées par un petit nombre de valeurs aberrantes marginales très importantes) pour les valeurs numériques et le OneHotEncoderpour les catégories","metadata":{}},{"cell_type":"code","source":"#columns_to_drop=['SiteEnergyUse(kBtu)','Energy/Surface', 'TotalGHGEmissions', 'GHG/Surface']\n#y_columns=['Energy/Surface', 'GHG/Surface']\ny_columns = ['TotalGHGEmissions', 'SiteEnergyUse(kBtu)']\nX = data.drop(y_columns, axis=1)\n\nprint(X.shape)\ny = data[y_columns]\nprint(y.shape)\nprint(len(numericColumns))\nfor i in y_columns:\n    numericColumns.remove(i)\nprint(len(numericColumns))\n\n# X = data.drop(['TotalGHGEmissions', 'SiteEnergyUse(kBtu)'], axis=1)\n# y = data(['TotalGHGEmissions'])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.24534Z","iopub.execute_input":"2021-08-21T19:48:30.245698Z","iopub.status.idle":"2021-08-21T19:48:30.258138Z","shell.execute_reply.started":"2021-08-21T19:48:30.245668Z","shell.execute_reply":"2021-08-21T19:48:30.257372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardiser les données\npreprocessor = make_column_transformer((RobustScaler(),numericColumns),(OneHotEncoder(handle_unknown = 'ignore'),objectColumns))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.259034Z","iopub.execute_input":"2021-08-21T19:48:30.259337Z","iopub.status.idle":"2021-08-21T19:48:30.281106Z","shell.execute_reply.started":"2021-08-21T19:48:30.259307Z","shell.execute_reply":"2021-08-21T19:48:30.279837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <a id=\"outliers\">Cas des outliers<a/>   \n    \n    J'ai fait des tests avec le jeu de données épurés des outliers mais les résultat étaient moins bon, j'ai donc décidé de continuer à travailler avec le jeu de données initiales.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"simple-linear-regression\">Régression linéaire basique<a/>","metadata":{}},{"cell_type":"code","source":"print(X.columns)\nprint(y.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.283045Z","iopub.execute_input":"2021-08-21T19:48:30.283509Z","iopub.status.idle":"2021-08-21T19:48:30.294026Z","shell.execute_reply.started":"2021-08-21T19:48:30.283462Z","shell.execute_reply":"2021-08-21T19:48:30.292855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_pipeline(preprocessor,LinearRegression())\nmodel.fit(X_train,y_train)\nprint(\"score d'entrainement = \",model.score(X_train,y_train))\ny_pred = model.predict(X_test)\nprint(\"score de la prédiction:\")#, accuracy_score(y_test, y_pred)), \nprint(\"MAE = \",mean_absolute_error(y_test,y_pred))\nprint(\"RMSE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"median abs err = \",median_absolute_error(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.295505Z","iopub.execute_input":"2021-08-21T19:48:30.295808Z","iopub.status.idle":"2021-08-21T19:48:30.480982Z","shell.execute_reply.started":"2021-08-21T19:48:30.295779Z","shell.execute_reply":"2021-08-21T19:48:30.480094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le score de prédiction est assez correct, peut-il être meilleur en utilisant un modèle par variable à prédire:","metadata":{}},{"cell_type":"code","source":"for column in y_columns:\n    X_train, X_test, y_train, y_test = train_test_split(X, y[column], test_size=0.2)\n    model = make_pipeline(preprocessor,LinearRegression())\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    print('Méthode: LinearRegression OneHotEncoder RobustScaler')\n    print('+ utilisation de pipelines')\n    print('Prédiction de ',column)\n    print('score d\\'entrainement = ',model.score(X_train,y_train))\n    print(\"score de la prédiction: \",  model.score(X_test, y_test)), \n    print(\"MAE = \",mean_absolute_error(y_test,y_pred))\n    print(\"RMSE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\n    print(\"median abs err = \",median_absolute_error(y_test,y_pred))\n    print('')\n ","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.482193Z","iopub.execute_input":"2021-08-21T19:48:30.482527Z","iopub.status.idle":"2021-08-21T19:48:30.725507Z","shell.execute_reply.started":"2021-08-21T19:48:30.482495Z","shell.execute_reply":"2021-08-21T19:48:30.724486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On a fait certaines approximations avec ce modèle: certraines catégories  (celles du jeu de test) n'ont malheureusement pas été encodées. Cela générait des erreurs qu'on a décidé d'ignorer avec le paramètre handle_unknown = 'ignore' du OneHotEncoder. Il faudrait donc pour une telle  méthode se passer de l'utilisation des pîpeline ou il faudrait appliquer la modification avant au rique de créer une fuite des données?","metadata":{}},{"cell_type":"code","source":"print(X.columns)\nprint(y.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.726627Z","iopub.execute_input":"2021-08-21T19:48:30.72691Z","iopub.status.idle":"2021-08-21T19:48:30.732531Z","shell.execute_reply.started":"2021-08-21T19:48:30.726883Z","shell.execute_reply":"2021-08-21T19:48:30.73153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX = data.drop(y_columns, axis=1)\ny = data[y_columns]\n\n# essais peu concluant avec les transformation appliquées avant.\nencoder=LabelEncoder()\nfor column in objectColumns:\n    X[column] = encoder.fit_transform(X[column])\n    \nencoder=StandardScaler()\nX_std = encoder.fit_transform(X)\n\nfor column in y_columns:\n    X_train, X_test, y_train, y_test = train_test_split(X_std, y[column], test_size=0.2)\n    model = LinearRegression()\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    print('Méthode: LinearRegression LabelEncoder StandardScaler')\n    print('Prédiction de ',column)\n    print('score d\\'entrainement = ',model.score(X_train,y_train))\n    print(\"score de la prédiction:\"), \n    print(\"MAE = \",mean_absolute_error(y_test,y_pred))\n    print(\"RMSE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\n    print(\"median abs err = \",median_absolute_error(y_test,y_pred))\n    print('')","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.734122Z","iopub.execute_input":"2021-08-21T19:48:30.734569Z","iopub.status.idle":"2021-08-21T19:48:30.988433Z","shell.execute_reply.started":"2021-08-21T19:48:30.734527Z","shell.execute_reply":"2021-08-21T19:48:30.987228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# essais peu concluant avec les transformation appliquées avant.\nencoder= LabelBinarizer()\nfor column in objectColumns:\n    X[column] = encoder.fit_transform(X[column])\n    \nencoder=StandardScaler()\nX_std = encoder.fit_transform(X)\n\nfor column in y_columns:\n    X_train, X_test, y_train, y_test = train_test_split(X_std, y[column], test_size=0.2)\n    model = LinearRegression()\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    print('Méthode: LinearRegression LabelBinarizer StandardScaler')\n    print('Prédiction de ',column)\n    print('score d\\'entrainement = ',model.score(X_train,y_train))\n    print(\"score de la prédiction:\"), \n    print(\"MAE = \",mean_absolute_error(y_test,y_pred))\n    print(\"RMSE = \",np.sqrt(mean_squared_error(y_test,y_pred)))\n    print(\"median abs err = \",median_absolute_error(y_test,y_pred))\n    print('')","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:30.99049Z","iopub.execute_input":"2021-08-21T19:48:30.991338Z","iopub.status.idle":"2021-08-21T19:48:31.163321Z","shell.execute_reply.started":"2021-08-21T19:48:30.991282Z","shell.execute_reply":"2021-08-21T19:48:31.162518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\nLe choix des transformers utilisés pour standardiser nos données est très impactant sur nos résultat. Un LabelEncoder ou un  LabelBinarizer associés avec des StandardScaler donne des résultats très médiocres. Un pipeline utilisant RobustScaler et OneHotEncoder donne des résultats bien meilleur et totalement admissible mais obligent par contre à ignorer certaines lignes dont les catégories se retrouvent dans le jeu d'entrainement mais pas dans le jeu de test et ne sont par conséquent pas connues du modèle. On essaiera de gommer ces imperfections dans la suite avec une validation croisée.\n\nessai raté avec le StratifiedShuffleSplit...","metadata":{}},{"cell_type":"code","source":"n_bins = 20\nfig = plt.figure(figsize=(18,9))\nplt.hist(data['TotalGHGEmissions'],n_bins)\nplt.title('TotalGHGEmissions')\nplt.show()\nfig = plt.figure(figsize=(18,9))\nplt.hist(np.log(data['TotalGHGEmissions']),n_bins)\nplt.title('log TotalGHGEmissions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:31.164742Z","iopub.execute_input":"2021-08-21T19:48:31.165321Z","iopub.status.idle":"2021-08-21T19:48:31.710425Z","shell.execute_reply.started":"2021-08-21T19:48:31.165285Z","shell.execute_reply":"2021-08-21T19:48:31.709626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On pourrait justifier un passage au log pour améliorer la précision des algorithme mais le RobustScaler nous permet de nous affranchir de cette transormation car il prend déjà en charge les changements d'échelles et nivelle les différences importantes et dans la pratique nous a donné de meilleurs résultats.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"model-compare\">Comparaison des modèles<a/> ","metadata":{}},{"cell_type":"code","source":"results = []\nalgos = {\n    'LinearRegression' : LinearRegression(),\n    'Ridge' : Ridge(),\n    'Lasso' : Lasso(tol=0.5),\n    'ElasticNet' : ElasticNet(),\n    'SGDRegressor': SGDRegressor(),\n    'SVR': SVR(),\n    'RandomForestRegressor' : RandomForestRegressor(),\n    'XGBRegressor' : XGBRegressor()\n}\nX_train, X_test, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:31.711608Z","iopub.execute_input":"2021-08-21T19:48:31.71208Z","iopub.status.idle":"2021-08-21T19:48:31.720033Z","shell.execute_reply.started":"2021-08-21T19:48:31.712049Z","shell.execute_reply":"2021-08-21T19:48:31.719259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for algo_name, algo in algos.items():\n    print('Algorithme: ',algo_name)\n    for column in y_columns:\n        y_test = y_test_all[column]\n        y_train = y_train_all[column]\n        model = make_pipeline(preprocessor,algo)\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        print('Prédiction de ',column)\n        print('score d\\'entrainement = ',model.score(X_train,y_train))\n        print(\"score de la prédiction: \",  model.score(X_test, y_test))\n        mae = mean_absolute_error(y_test,y_pred)\n        rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n        med_abs_err = median_absolute_error(y_test,y_pred)\n        print(\"MAE = \", mae)        \n        print(\"RMSE = \",rmse)\n        print(\"median abs err = \", med_abs_err)\n        print('')\n        results.append([algo_name, column, model.score(X_test, y_test), mae, rmse, med_abs_err])\n    print('-'*100)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:31.722737Z","iopub.execute_input":"2021-08-21T19:48:31.723236Z","iopub.status.idle":"2021-08-21T19:48:37.809298Z","shell.execute_reply.started":"2021-08-21T19:48:31.723202Z","shell.execute_reply":"2021-08-21T19:48:37.80815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"model-compare-without-ESS\">Comparaison des modèles sans le EnergyStarScore<a/> ","metadata":{}},{"cell_type":"code","source":"X_train = X_train.drop(['ENERGYSTARScore'], axis=1)\nX_test = X_test.drop(['ENERGYSTARScore'], axis=1)\nresults_without_energyStarScore = []\nnumericColumns.remove('ENERGYSTARScore')\nprint(numericColumns)\npreprocessor = make_column_transformer((RobustScaler(),numericColumns),(OneHotEncoder(handle_unknown = 'ignore'),objectColumns))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:37.811211Z","iopub.execute_input":"2021-08-21T19:48:37.811669Z","iopub.status.idle":"2021-08-21T19:48:37.822776Z","shell.execute_reply.started":"2021-08-21T19:48:37.811624Z","shell.execute_reply":"2021-08-21T19:48:37.821827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for algo_name, algo in algos.items():\n    print('Algorithme: ',algo_name)\n    for column in y_columns:\n        y_test = y_test_all[column]\n        y_train = y_train_all[column]\n        model = make_pipeline(preprocessor,algo)\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        print('Prédiction de ',column)\n        print('score d\\'entrainement = ',model.score(X_train,y_train))\n        print(\"score de la prédiction: \",  model.score(X_test, y_test))\n        mae = mean_absolute_error(y_test,y_pred)\n        rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n        med_abs_err = median_absolute_error(y_test,y_pred)\n        print(\"MAE = \", mae)        \n        print(\"RMSE = \",rmse)\n        print(\"median abs err = \", med_abs_err)\n        print('')\n        results_without_energyStarScore.append([algo_name, column, model.score(X_test, y_test), mae, rmse, med_abs_err])\n    print('-'*100)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-21T19:48:37.824719Z","iopub.execute_input":"2021-08-21T19:48:37.825655Z","iopub.status.idle":"2021-08-21T19:48:43.542739Z","shell.execute_reply.started":"2021-08-21T19:48:37.825606Z","shell.execute_reply":"2021-08-21T19:48:43.541775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"conclude\">Conclusion<a/>","metadata":{}},{"cell_type":"code","source":"df_results = pd.DataFrame(results,columns=['algorithm', 'column','predict score', 'MAE', 'RMSE', 'median abs err'])\ndisplay(df_results.sort_values(by=['column','predict score'],ascending=False))\nprint(\"Sans le EnergyStarScore:\")\ndf_results_without_energyStarScore = pd.DataFrame(results_without_energyStarScore,columns=['algorithm', 'column','predict score', 'MAE', 'RMSE', 'median abs err'])\ndisplay(df_results_without_energyStarScore.sort_values(by=['column','predict score'],ascending=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T19:48:43.547038Z","iopub.execute_input":"2021-08-21T19:48:43.54935Z","iopub.status.idle":"2021-08-21T19:48:43.604138Z","shell.execute_reply.started":"2021-08-21T19:48:43.549298Z","shell.execute_reply":"2021-08-21T19:48:43.603021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Le XGBRegresson et le RandomForestRegressor sont nos deux algorithmes les plus performants. Ils obtiennent des résultats très satisfaisants. Cependant une optimisation des paramètres des différents algorithmes pourrait créer des différences. On va donc chercher à optimiser les paramètres de ces différents algorithme par le biais d'une validation croisée. Nous supprimerons néanmoins le SGDRegressor qui est totalement contre-performant et le SVR qui a des résultats pas assez bon.","metadata":{"execution":{"iopub.execute_input":"2021-08-16T13:11:54.422061Z","iopub.status.busy":"2021-08-16T13:11:54.421612Z","iopub.status.idle":"2021-08-16T13:11:54.437029Z","shell.execute_reply":"2021-08-16T13:11:54.435585Z","shell.execute_reply.started":"2021-08-16T13:11:54.421945Z"}}}]}