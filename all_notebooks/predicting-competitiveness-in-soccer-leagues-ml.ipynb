{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Selected Dataset \n\n**Dataset** https://www.kaggle.com/taranguyen/form-in-bundesliga-la-liga-mls-premier-league\n\n**Context**\n\nThese datasets were created as part of a class project on Soccer Competitiveness, which aimed to compare the competitiveness of four different soccer leagues: the Bundesliga (in Germany), the La Liga (in Spain), the Premier League (in the U.K.), and the Major League Soccer (in the U.S.).\n\n**Content**\n\nEach of the four datasets contains form tables for five seasons, from Season 2015/16 to Season 2019/20. Match results are denoted as \"W\" for a win, \"D\" for a draw, or \"L\" for a loss.\n\n**Acknowledgements**\n\nAll data came from https://www.transfermarkt.us/.\n\n**Inspiration**\n\nWhich soccer league is the most competitive?\nHow competitive is the Major League Soccer compared to the other leagues, all of which are well-known for their competitiveness (as well as prestige)?\n\n### Predicting Competitiveness in Soccer Leagues \n\n**Abstract**: As part of the Data Science Course Project, one of Kaggle dataset is selected to be predicted by implementing classification models. The datasets which were created as part of the Soccer Competitiveness project, aimed to compare the competitiveness of four different soccer leagues: the Bundesliga (in Germany), the La Liga (in Spain), the Premier League (in the U.K.), and the Major League Soccer (in the U.S.). Using these datasets, we are motivated to find “how early the Teams’ 1st, 2nd and 3rd positions at the end of the Season can be predicted?”. Since each League has different dynamics, the datasets are processed independently from each other. After evaluation step is performed with several binary classification model trials, La Liga league has found the most competitive among the others with the lowest balanced accuracy value during the season, the other leagues are found as Bundesliga, Premier League, Major League Soccer respectively. However, when we examined the final scores of the teams and the distribution of the differences between them, we observed that the competitive environment of the Major League Soccer was higher than the others, similar to the La Liga.\n\n**Keywords**: soccer competitiveness, binary classification, predictive analysis\n\n**Programming Language**: Python 3.7\n\n**Introduction**: Predicting the competitiveness of football leagues is a vital problem in the football industry, which can affect the interests and emotions of football fans, the ambitions of the players and the odds.All teams start with zero points and earn points according to the result of the game by 3 points for Win, 1 point for Draw, 0 point for Loss. For each season all teams play games with each other and once all rounds are finalized the winner is the one which earns the highest point and in the 1st position. There might be variety of features cause the change in competitiveness levels considering several leagues dynamics, but in these given datasets, we are focusing on the provided features as described below:\n\nEach of the four datasets contains form tables for five seasons, from Season 2015/16 to Season 2019/20. “Match” results are denoted as \"W\" for a win, \"D\" for a draw, or \"L\" for a loss. All data came from https://www.transfermarkt.us/. The “final positions”, “Total goals scored” and “cancelled” during the season are also provided by the datasets.  \n\nAll match results which are given as categorical features are converted to their respective score values as M={‘W’:3,’D’:1,’L’:0} then created as cumulative sum of final points for each week. For example, if we are in week 24, we have 24 matches in our table, including all W, D, L values, and this week's Final Point column consists of the cumulative sum of these results. Additionally, for each week cumulative sum of the number of total wins, draws and losses are calculated. \n\nPoint differences might be an indicator factor to predict competitiveness in the league such that if the final points for the given week are close to each other then the predictiveness become harder because of the uncertainty level is increased. If the differences are more diverse, in other words are more separated and the gaps are increased then we can conclude with the predictiveness can be easier for this league. In order to observe the effect of point differences in the given weeks, after the cumulative totals were calculated, the Point Differences attributes were created by ranking the final scores of each week and calculating the differences between consecutive teams.\n\nTarget column is defined as binary outcome which is 1 for the teams scores in the 1st, 2nd and 3rd positions, and 0 for the rest. So, our predictions will be based on the first 3 positions in the given league and season. \n\nSince each of the data sets has 5 seasons, the train and test sets are separated, with the training set for the first four seasons and the test set for the last season. The training sets are used for grid search analysis and base models, after which the final estimation is performed on the test sets.\n\n**Project Objective**: The minimum requirement of this project includes “Model Selection”, “Application of at least four machine learning algorithms”, “Feature selection”, “Overfit/Underfit analysis”, “Hyperparameter optimization”. Using these data science steps, we are motivated to predict competitiveness levels of given leagues by predictability scores with classification models.\n","metadata":{}},{"cell_type":"code","source":"import os\nos.getcwd()\nprint(os.listdir(\"../input\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:10:05.160739Z","iopub.execute_input":"2021-06-30T12:10:05.161069Z","iopub.status.idle":"2021-06-30T12:10:05.16645Z","shell.execute_reply.started":"2021-06-30T12:10:05.161042Z","shell.execute_reply":"2021-06-30T12:10:05.165537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Solution and Methods \n","metadata":{}},{"cell_type":"code","source":"! pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:11:49.441863Z","iopub.execute_input":"2021-06-30T12:11:49.442198Z","iopub.status.idle":"2021-06-30T12:11:57.133938Z","shell.execute_reply.started":"2021-06-30T12:11:49.442168Z","shell.execute_reply":"2021-06-30T12:11:57.133192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing required libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nfrom scipy.stats import norm\n\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support, balanced_accuracy_score\nfrom sklearn.compose import make_column_selector,make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score , cross_validate, GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.compose import make_column_selector,make_column_transformer, ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.feature_selection import SelectFromModel , mutual_info_classif,RFECV\n\nfrom collections import Counter\n\nfrom xgboost import XGBClassifier\n\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, RandomOverSampler\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.pipeline import Pipeline as imbpipeline\nfrom imblearn.pipeline import make_pipeline as imbmake_pipeline\nfrom imblearn.base import BaseSampler\nfrom imblearn.under_sampling import (ClusterCentroids, RandomUnderSampler,\n                                     NearMiss,\n                                     InstanceHardnessThreshold,\n                                     CondensedNearestNeighbour,\n                                     EditedNearestNeighbours,\n                                     RepeatedEditedNearestNeighbours,\n                                     AllKNN,\n                                     NeighbourhoodCleaningRule,\n                                     OneSidedSelection)\n\nimport tqdm\nimport time\ndef hms_string(sec_elapsed):\n        h = int(sec_elapsed / (60 * 60))\n        m = int((sec_elapsed % (60 * 60)) / 60)\n        s = sec_elapsed % 60\n        return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_colwidth', 1000)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:11:09.064811Z","iopub.execute_input":"2021-06-30T12:11:09.065123Z","iopub.status.idle":"2021-06-30T12:11:11.033516Z","shell.execute_reply.started":"2021-06-30T12:11:09.065096Z","shell.execute_reply":"2021-06-30T12:11:11.032656Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read & Merge all datasets","metadata":{}},{"cell_type":"code","source":"#Read Datasets. Here we are dropping Matches, because we will merge with all leagues' dataframes \ndf = pd.read_excel('../input/form-in-bundesliga-la-liga-mls-premier-league/all-leaguetables.xlsx')\ndf['Position'] = df['Position'].astype(int)\ndf.sort_values(by=['Season','Position','Team'], inplace=True)\n\n\n#Find all Final Points differences between winner and the teams & differences between all teams \nnews = []\n\nfor i in list(df.League.unique()):\n    temp = df[df['League']==i]\n    for j in list(temp.Season.unique()):\n        temp_ = temp[temp['Season']==j]\n        temp_['winner'] = temp_[temp_['Position']==1]['FinalPoints'].values[0]\n        temp_['FinalPointsDiff_Winner'] = temp_['winner'] - temp_['FinalPoints']\n        temp_.drop(['winner'], axis=1, inplace=True)\n        temp_['FinalPoints_Diff'] = temp_.FinalPoints.diff(-1).shift().fillna(0)\n        news.append(temp_)\n        \ndf = pd.concat(news)\n\n# Read all separated dataframes \nbundesliga = pd.read_csv('../input/form-in-bundesliga-la-liga-mls-premier-league/form-bundesliga.csv').drop('Matches', axis=1)\nepl = pd.read_csv('../input/form-in-bundesliga-la-liga-mls-premier-league/form-epl.csv').drop('Matches', axis=1)\nlaliga = pd.read_csv('../input/form-in-bundesliga-la-liga-mls-premier-league/form-laliga.csv').drop('Matches', axis=1)\nmls = pd.read_csv('../input/form-in-bundesliga-la-liga-mls-premier-league/form-mls.csv').drop('Matches', axis=1)\n\n#Merge separated League datasets with the all league table. \nbundesliga_ = df[df['League']=='Bundesliga'].merge(bundesliga, on=['Season','Team'], how='left')\nlaliga_ = df[df['League']=='La Liga'].merge(laliga, on=['Season','Team'], how='left')\nepl_ = df[df['League']=='Premier League'].merge(epl, on=['Season','Team'], how='left')\nmls_ = df[df['League']=='Major League Soccer'].merge(mls, on=['Season','Team'], how='left')\n\n#Position column indicates the final ranking of the team, to be able to sort , this column converted to integer.\n# All categorical and integer typed columns are extracted in a list format to be handled during pipelines. \nbundesliga_['Position'] = bundesliga_['Position'].astype(int)\nlaliga_['Position'] = laliga_['Position'].astype(int)\nepl_['Position'] = epl_['Position'].astype(int)\nmls_['Position'] = mls_['Position'].astype(int)\n\n#Sorting all dataframe values by 'Season','Position','Team' \nbundesliga_.sort_values(by=['Season','Position','Team'], inplace=True)\nlaliga_.sort_values(by=['Season','Position','Team'], inplace=True)\nepl_.sort_values(by=['Season','Position','Team'], inplace=True)\nmls_.sort_values(by=['Season','Position','Team'], inplace=True)\n\n#MLS dataframe season column is different than other dataframes. So, here we map all the values to commonly used values. \nmapper = dict(zip(list(mls_.Season.unique()),list(bundesliga_.Season.unique())))\nmls_['Season'] = mls_['Season'].map(mapper)\n\n#Concat all prepared dataframes\ndf_ = pd.concat([bundesliga_,epl_,laliga_,mls_], axis=0)\n\n\ndisplay(df.head(5))        \n\n#Select all numerical and categorical columns\ncat_selector = make_column_selector(dtype_exclude=np.number)\nnum_selector = make_column_selector(dtype_include=np.number)\n\ncats = cat_selector(df)\nnums = num_selector(df)\n\nprint(f'categorical columns:{cats}')\nprint(f'numerical columns{nums}')\n\ndisplay(df_.head(5))  ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:12:47.462831Z","iopub.execute_input":"2021-06-30T12:12:47.463193Z","iopub.status.idle":"2021-06-30T12:12:47.814059Z","shell.execute_reply.started":"2021-06-30T12:12:47.463156Z","shell.execute_reply":"2021-06-30T12:12:47.813225Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values","metadata":{}},{"cell_type":"code","source":"all_dfs = [df,bundesliga_,epl_,laliga_,mls_]\nlabels = ['all_dataframes','bundesliga','epl','laliga','mls']\n\nplt.figure(figsize=(20,10))\n#plt.subplots_adjust(wspace=0.05, hspace=0.5)\na = 3  # number of rows\nb = 2  # number of columns\nc = 1  # initialize plot counter\n\nfor t,z in enumerate(zip(labels,all_dfs)):\n    plt.subplot(a, b, c+t)\n    sns.heatmap(z[1].isnull())\n    plt.title(z[0])\n    plt.axis('off')\n\nplt.show()    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:12:54.605238Z","iopub.execute_input":"2021-06-30T12:12:54.605553Z","iopub.status.idle":"2021-06-30T12:12:55.914147Z","shell.execute_reply.started":"2021-06-30T12:12:54.605526Z","shell.execute_reply":"2021-06-30T12:12:55.913277Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are lots of missing values in especially La Liga dataframe. When we check the names of the Teams, it seems they are represented differently in two dataframes which we use to merge them. So, we will singularize the values of the Teams then merge two dataframe","metadata":{}},{"cell_type":"code","source":"to_bundesliga_ = list(bundesliga_[(bundesliga_['Match1'].isna())].Team.unique())\nfrom_bundesliga = list(bundesliga['Team'].unique())\n\nto_laliga_ = list(laliga_[(laliga_['Match1'].isna())].Team.unique())\nfrom_laliga = list(laliga['Team'].unique())\n\nfor i in from_bundesliga:\n    for j in to_bundesliga_:\n        if i in j:\n            bundesliga.replace(i,j, inplace=True)\n            \nfor i in from_laliga:\n    for j in to_laliga_:\n        if i in j:\n            laliga.replace(i,j, inplace=True)    ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:05.138347Z","iopub.execute_input":"2021-06-30T12:13:05.138702Z","iopub.status.idle":"2021-06-30T12:13:05.253033Z","shell.execute_reply.started":"2021-06-30T12:13:05.138666Z","shell.execute_reply":"2021-06-30T12:13:05.252164Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bundesliga_ = df[df['League']=='Bundesliga'].merge(bundesliga, on=['Season','Team'], how='left')\nlaliga_ = df[df['League']=='La Liga'].merge(laliga, on=['Season','Team'], how='left')\n\nall_dfs = [df,bundesliga_,epl_,laliga_,mls_]\nlabels = ['all_dataframes','bundesliga','epl','laliga','mls']\n\nplt.figure(figsize=(20,10))\n#plt.subplots_adjust(wspace=0.05, hspace=0.5)\na = 3  # number of rows\nb = 2  # number of columns\nc = 1  # initialize plot counter\n\nfor t,z in enumerate(zip(labels,all_dfs)):\n    plt.subplot(a, b, c+t)\n    sns.heatmap(z[1].isnull())\n    plt.title(z[0])\n    plt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:11.124677Z","iopub.execute_input":"2021-06-30T12:13:11.125172Z","iopub.status.idle":"2021-06-30T12:13:12.532004Z","shell.execute_reply.started":"2021-06-30T12:13:11.125115Z","shell.execute_reply":"2021-06-30T12:13:12.531091Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t,z in enumerate(zip(labels,all_dfs)):\n    for i in z[1].columns:\n        null_rate = z[1][i].isna().sum() / len(z[1]) * 100 \n        if null_rate > 0 :\n            print(\"{} dataframe,{}'s null rate :{}%\".format(z[0],i,round(null_rate,2)))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:16.366664Z","iopub.execute_input":"2021-06-30T12:13:16.367201Z","iopub.status.idle":"2021-06-30T12:13:16.423478Z","shell.execute_reply.started":"2021-06-30T12:13:16.367149Z","shell.execute_reply":"2021-06-30T12:13:16.422502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this we checked there are no missing values left!","metadata":{}},{"cell_type":"code","source":"#Concat all prepared dataframes\ndf_ = pd.concat([bundesliga_,epl_,laliga_,mls_], axis=0);df_.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:18.991878Z","iopub.execute_input":"2021-06-30T12:13:18.992441Z","iopub.status.idle":"2021-06-30T12:13:19.042439Z","shell.execute_reply.started":"2021-06-30T12:13:18.992395Z","shell.execute_reply":"2021-06-30T12:13:19.041383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Matrix among all Numeric Features","metadata":{}},{"cell_type":"code","source":"all_dfs = [bundesliga_,epl_,laliga_,mls_]\nlabels = ['bundesliga','epl','laliga','mls']\n\nplt.figure(figsize=(15,15))\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\na = 2  # number of rows\nb = 2  # number of columns\nc = 1  # initialize plot counter\n\nfor t,z in enumerate(zip(labels,all_dfs)):\n    plt.subplot(a, b, c+t)\n    mask = np.triu(np.ones_like(z[1].corr(), dtype=bool))\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    sns.heatmap(z[1].corr(),annot=True, mask=mask, cmap=cmap, vmax=1.0, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.title(z[0],fontweight=\"bold\")\n    #plt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:24.53887Z","iopub.execute_input":"2021-06-30T12:13:24.539226Z","iopub.status.idle":"2021-06-30T12:13:27.147411Z","shell.execute_reply.started":"2021-06-30T12:13:24.53919Z","shell.execute_reply":"2021-06-30T12:13:27.146574Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graphs above, we observe that there are strong correlations between some of the features. This is a predictably meaningful result, as many features come from the teams' final standings at the end of the season. \n\nFor example;\n\nThe wins column includes all wins for the season. This column is highly correlated with the Position, which is the last position of the team, so if there are many wins then the position will be 1 for the given season. It also explains the relationship between Final Points and Position.\n","metadata":{}},{"cell_type":"markdown","source":"#### Distribution between the winner and the rest of the teams \n\nDistribution of the final points is one of the indicators of uncertainty levels. To observe this, we prepared distribution considering 4 factors as shown in below \"Distribution of League Final Points\" graph. All charts have been prepared for each team and the colors in the chart correspond to the seasons’ representation. The first charts, titled \"Overall\", show the distribution of teams among Final Points for each season. We observe that the MLS League has the most skewed distribution compared to the others, which shows us that MLS is the most competitive league given the final scores. The competitiveness between the top 3 leading teams shown in the graphs in the second row of the same Figure is a strong factor, as is the competitiveness between the other teams in the league. We can observe that the features of the graphics change between leagues and seasons. From the results, it can be seen that La Liga has more competitive power for the 2015/16 season than other seasons, similar to MLS. Therefore, we can conclude that the competition within the seasons can change and is not fixed. Another factor would be Final Points differences with the winner team which shows how the leader team moves away from the rest. The results are slightly changes when compared to the first Overall graph. Finally, the final point differences between the teams, which is the last factor where we observe the differences between the distributions, are shown in the final graphs. This confirms that the MLS league is more skewed than the others.","metadata":{}},{"cell_type":"code","source":"g = sns.displot(data=df_, x=\"FinalPoints\", hue=\"Season\", row=\"League\",kind=\"kde\",  height=3, aspect=4)\ng.set_axis_labels(\"Density\", \"Final Points\") \ng.set_titles(\"{row_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:45.98911Z","iopub.execute_input":"2021-06-30T12:13:45.989621Z","iopub.status.idle":"2021-06-30T12:13:47.243466Z","shell.execute_reply.started":"2021-06-30T12:13:45.989587Z","shell.execute_reply":"2021-06-30T12:13:47.242603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,15))\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\ngs = fig.add_gridspec(4,4)\n\nax00 = fig.add_subplot(gs[0,0])#, sharey = ax9)\nax01 = fig.add_subplot(gs[0,1], sharex = ax00)#, sharey = ax7, sharex = ax6)\nax02 = fig.add_subplot(gs[0,2], sharex = ax00)#, sharey = ax0, sharex = ax6)\nax03 = fig.add_subplot(gs[0,3], sharex = ax00)\nax10 = fig.add_subplot(gs[1,0], sharex = ax00)#, sharey = ax9)\nax11 = fig.add_subplot(gs[1,1], sharex = ax00)#, sharey = ax7, sharex = ax6)\nax12 = fig.add_subplot(gs[1,2], sharex = ax00)#, sharex = ax6, sharey = ax2)\nax13 = fig.add_subplot(gs[1,3], sharex = ax00)\nax20 = fig.add_subplot(gs[2,0], sharex = ax00)#, sharey = ax9)\nax21 = fig.add_subplot(gs[2,1], sharex = ax00)#, sharex = ax6)\nax22 = fig.add_subplot(gs[2,2], sharex = ax00)#, sharex = ax6, sharey = ax2)\nax23 = fig.add_subplot(gs[2,3], sharex = ax00)\nax30 = fig.add_subplot(gs[3,0], sharex = ax00)#, sharex = ax6)\nax31 = fig.add_subplot(gs[3,1], sharex = ax00)#, sharey = ax7, sharex = ax6)\nax32 = fig.add_subplot(gs[3,2], sharex = ax00)#, sharex = ax6, sharey = ax2)\nax33 = fig.add_subplot(gs[3,3], sharex = ax00)\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=epl_,fill=True, alpha=.5,shade=True, ax=ax00, palette='gist_gray_r').set(title='Premier League | Overall')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=epl_[epl_['Position'].isin(list(np.arange(1,4)))],fill=True, alpha=.5,shade=True, ax=ax01, palette='gist_gray_r').set(title='Premier League | Final Points - First 3 Teams')\nsns.despine()\n\nsns.kdeplot(x='FinalPointsDiff_Winner',hue='Season', data=epl_,fill=True, alpha=.5,shade=True, ax=ax02, palette='gist_gray_r').set(title='Premier League | Final Points Diff Winner')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints_Diff',hue='Season', data=epl_,fill=True, alpha=.5,shade=True, ax=ax03, palette='gist_gray_r').set(title='Premier League | Final Points Differences')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=bundesliga_,fill=True, alpha=.5,shade=True, ax=ax10, palette='gist_gray_r').set(title='Bundesliga | Overall')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=bundesliga_[bundesliga_['Position'].isin(list(np.arange(1,4)))],fill=True, alpha=.5,shade=True, ax=ax11, palette='gist_gray_r').set(title='Bundesliga | Final Points - First 3 Teams')\nsns.despine()\n\nsns.kdeplot(x='FinalPointsDiff_Winner',hue='Season', data=bundesliga_,fill=True, alpha=.5,shade=True, ax=ax12, palette='gist_gray_r').set(title='Bundesliga | Final Points Diff Winner')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints_Diff',hue='Season', data=bundesliga_,fill=True, alpha=.5,shade=True, ax=ax13, palette='gist_gray_r').set(title='Bundesliga | Final Points Differences')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=laliga_,fill=True, alpha=.5,shade=True, ax=ax20, palette='gist_gray_r').set(title='La Liga | Overall')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=laliga_[laliga_['Position'].isin(list(np.arange(1,4)))],fill=True, alpha=.5,shade=True, ax=ax21, palette='gist_gray_r').set(title='La Liga | Final Points - First 3 Teams')\nsns.despine()\n\nsns.kdeplot(x='FinalPointsDiff_Winner',hue='Season', data=laliga_,fill=True, alpha=.5,shade=True, ax=ax22, palette='gist_gray_r').set(title='La Liga | Final Points Diff Winner')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints_Diff',hue='Season', data=laliga_,fill=True, alpha=.5,shade=True, ax=ax23, palette='gist_gray_r').set(title='La Liga | Final Points Differences')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=mls_,fill=True, alpha=.5,shade=True, ax=ax30, palette='gist_gray_r').set(title='Major League Soccer | Overall')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints',hue='Season', data=mls_[mls_['Position'].isin(list(np.arange(1,4)))],fill=True, alpha=.5,shade=True, ax=ax31, palette='gist_gray_r').set(title='MLS | Final Points - First 3 Teams')\nsns.despine()\n\nsns.kdeplot(x='FinalPointsDiff_Winner',hue='Season', data=mls_,fill=True, alpha=.5,shade=True, ax=ax32, palette='gist_gray_r').set(title='MLS | Final Points Diff Winner')\nsns.despine()\n\nsns.kdeplot(x='FinalPoints_Diff',hue='Season', data=mls_,fill=True, alpha=.5,shade=True, ax=ax33, palette='gist_gray_r').set(title='MLS | Final Points Differences')\nsns.despine()\n\n# Title & Subtitle    \nfig.text(0.4, 0.93, 'Distribution of League Final Points', fontsize=20, fontweight='bold', fontfamily='serif', ha='left') \nplt.show();\nfig.savefig('Distributionof.LeagueFinalPoints.jpeg', dpi=350, bbox_inches='tight')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:49.240673Z","iopub.execute_input":"2021-06-30T12:13:49.241007Z","iopub.status.idle":"2021-06-30T12:13:55.451887Z","shell.execute_reply.started":"2021-06-30T12:13:49.240978Z","shell.execute_reply":"2021-06-30T12:13:55.451007Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the charts above, we can observe that La Liga has a high competition between the top three teams in 2015/16 Season, in contrast to the general distributions of the same year.\n\nIn the 2016/17 Season, MLS has high competition among the top three teams.\n\nFor other Leagues, the situation is more dispersed, considering the top three teams and the entire league.\n\nThe MLS league has the most skewed plot values compared to the others. Based on this result, the first observation we can make is that MLS may be the most competitive league compared to the others.\n\nFinal Points gaps are tighter in MLS than in the others. This indicates that all consecutive teams have close differences. So, we can conclude that this league is competitive in this sense.","metadata":{}},{"cell_type":"markdown","source":"### Feature Preperation & Encoding\n\nHere we encode the categorical features corresponding to the match result as follows:\nWhen the team wins it will earn 3 points, Draws gives 1 point, and Loss does not give any point so results  with 0. \n\n- W:3 \n- D:1\n- L:0\n\nNumerical features are scaled between [-1,1]\n\n- Although the **GoalsScored** and **GoalsConceded** columns are capable of carrying information about the success of the team in a particular season, we cannot add it to the forecasting model, since in our approach we will add the information of the matches played so far to the model to predict the final positions, and these columns only carry the end-of-season information. Adding the end-of-season information to the model may distort the model predictions since it will be highly correlated.Since **GoalDiff** is related to these two columns, we can drop it before the model phase.\n\n\n- Total **Wins**, **Draws** and **Losses** for the end of the season are also Match1, Match2, etc. It does not need to be added to the model because it is given in sequential order in the columns. The information of these columns will be reproduced for the prediction model.\n\n\n- **Matches** column is constant for the given League so we drop it since it will not contribute the final prediction. \n\n\n- **Position** gives the final state of the Team, so this column will be our target. But, we set up our problem as being in the first 3 Position or not, thus we  encode this column to be in the form of our target \n\n\n- **League**: We will predict Positions with given League information, so each league will have different models, so this column will not be encoded.","metadata":{}},{"cell_type":"code","source":"#Create Score columns \nfor i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):\n    df_['Point_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n    df_['Point_'+str(i+1)] = df_['Point_'+str(i+1)].map({'W':3,'D':1,'L':0})\n\n#Cumsum all Point scores\ndf_.loc[:,'Point_1':'Point_38']= df_.loc[:,'Point_1':'Point_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n\nfor i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):    \n    df_['Win_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n    df_['Win_'+str(i+1)] = df_['Win_'+str(i+1)].map({'W':1,'D':0,'L':0})\n\n#Cumsum all Win scores\ndf_.loc[:,'Win_1':'Win_38']= df_.loc[:,'Win_1':'Win_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n\nfor i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):\n    df_['Loss_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n    df_['Loss_'+str(i+1)] = df_['Loss_'+str(i+1)].map({'W':0,'D':0,'L':1})\n\n#Cumsum all Loss scores\ndf_.loc[:,'Loss_1':'Loss_38']= df_.loc[:,'Loss_1':'Loss_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n    \nfor i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):\n    df_['Draw_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n    df_['Draw_'+str(i+1)] = df_['Draw_'+str(i+1)].map({'W':0,'D':1,'L':0})\n\n#Cumsum all Draw scores     \ndf_.loc[:,'Draw_1':'Draw_38']= df_.loc[:,'Draw_1':'Draw_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n\n#Create difference columns by sorting each score column to have positions, then taking the differences between consecutive scores. \ndists = []\nfor leg in list(df_.League.unique()):\n    temp = df_[df_['League']==leg]\n    for ses in list(temp.Season.unique()):\n        temp_ = temp[temp['Season']==ses]\n        for i,j in enumerate(list(temp_.loc[:,'Point_1':'Point_38'].columns)):\n            temp_['PointDiff_'+str(i+1)] = temp_.sort_values(by=[j], ascending=False)[j].diff(-1).shift()\n            dists.append(temp_)\n            \ndf_ = pd.concat(dists).drop_duplicates().sort_values(by=['League','Season','Position','Team'])\ndf_ = df_.reset_index().drop('index', axis=1)\n\n#Preperae target column \ndf_['target'] = np.where(df_['Position'].isin([1,2,3]), 1, 0) \n\ndisplay(df_.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:13:57.966066Z","iopub.execute_input":"2021-06-30T12:13:57.966433Z","iopub.status.idle":"2021-06-30T12:14:18.530197Z","shell.execute_reply.started":"2021-06-30T12:13:57.9664Z","shell.execute_reply":"2021-06-30T12:14:18.52928Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create all separated dataframes corresponding to the Leagues\nbundesliga_ = df_[df_['League']=='Bundesliga']\nepl_ = df_[df_['League']=='Premier League']\nlaliga_ = df_[df_['League']=='La Liga']\nmls_ = df_[df_['League']=='Major League Soccer']\n\n#Create below lists to be able to iterate over \nall_dfs = [bundesliga_,epl_,laliga_,mls_]\n\n### Select the columns which will be in the model and drop all missing values .\nfor i in all_dfs:\n    #i.drop(removed_cols, axis=1, inplace=True)\n    i.dropna(axis=1, how='all',inplace=True)\n    i.fillna(0, inplace=True)\n\n\nseasons = ['2015/16', '2016/17', '2017/18', '2018/19', '2019/20']\n\n#removed_cols are selected to be removed since they are highly correlated with the final position and final points. Also since we will predict the positions of each team before the season will be finalized, we cannot include them in the model. \nremoved_cols = ['League','Position','Matches','Wins','Draws','Losses','GoalsScored','GoalsConceded','GoalDiff','FinalPoints','FinalPointsDiff_Winner','FinalPoints_Diff']","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:18.532463Z","iopub.execute_input":"2021-06-30T12:14:18.532852Z","iopub.status.idle":"2021-06-30T12:14:18.765684Z","shell.execute_reply.started":"2021-06-30T12:14:18.532811Z","shell.execute_reply":"2021-06-30T12:14:18.764505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building model with {percentage}% of the first matches ","metadata":{}},{"cell_type":"code","source":"# Decide on percentage and prep related columns \npercentage = 0.7\ntrain_number = round(bundesliga_.Matches.unique()[0]*percentage)\n\n#Remove columns not to be included in the model \nbundesliga_.drop(removed_cols, axis=1, inplace=True)\n\nbundesliga_.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:18.767001Z","iopub.execute_input":"2021-06-30T12:14:18.767437Z","iopub.status.idle":"2021-06-30T12:14:18.928161Z","shell.execute_reply.started":"2021-06-30T12:14:18.767389Z","shell.execute_reply":"2021-06-30T12:14:18.927172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we will prepare dataframe columns which we will be using during model phase \n# Create columns index name for the given dataset\n\ncols = dict()\nfor i,j in enumerate(list(bundesliga_.columns)):\n    cols[j] = i\n\n# Prepare a list of related indexes extracted from the above dictionary    \n\na=['Season','Team','Win_'+str(train_number),'Loss_'+str(train_number),'Draw_'+str(train_number) , 'Point_'+str(train_number),'PointDiff_1','PointDiff_'+str(train_number+1),'target']\n\nsearch = []\n\nfor k,j in cols.items():\n    for s in a:\n        if k==s:\n            search.append(j)   \n\n\n# Define the new dataframe with the selected order     \nbundesliga_ = bundesliga_.iloc[:, np.r_[search[0],search[1],search[2],search[3],search[4],search[5], search[6]:search[7],search[8]]]\n\n\n\nbundesliga_.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:18.929571Z","iopub.execute_input":"2021-06-30T12:14:18.929961Z","iopub.status.idle":"2021-06-30T12:14:18.971445Z","shell.execute_reply.started":"2021-06-30T12:14:18.92992Z","shell.execute_reply":"2021-06-30T12:14:18.970297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Test Split\n\nFor each dataset we will use last season as validation of the model. ","metadata":{}},{"cell_type":"code","source":"#Train Test Split\nbundesliga_train = bundesliga_[bundesliga_['Season']!=seasons[-1]].drop('Season', axis=1)\nbundesliga_test = bundesliga_[bundesliga_['Season']==seasons[-1]].drop('Season', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:19.265839Z","iopub.execute_input":"2021-06-30T12:14:19.266191Z","iopub.status.idle":"2021-06-30T12:14:19.280564Z","shell.execute_reply.started":"2021-06-30T12:14:19.266154Z","shell.execute_reply":"2021-06-30T12:14:19.27944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create prep function to apply all datasets\n\nHere we define a function to iterate over all datasets then have a train and test sets. ","metadata":{}},{"cell_type":"code","source":"\npath_df = '../input/form-in-bundesliga-la-liga-mls-premier-league/all-leaguetables.xlsx'\npath_bundesliga = '../input/form-in-bundesliga-la-liga-mls-premier-league/form-bundesliga.csv'\npath_epl = '../input/form-in-bundesliga-la-liga-mls-premier-league/form-epl.csv'\npath_laliga = '../input/form-in-bundesliga-la-liga-mls-premier-league/form-laliga.csv'\npath_mls = '../input/form-in-bundesliga-la-liga-mls-premier-league/form-mls.csv'\n\ndef prep_step_1(path_df,path_bundesliga,path_epl,path_laliga,path_mls):\n    #Read Datasets. Here we are dropping Matches, because we will merge with the all leagues dataframes \n    df = pd.read_excel(path_df)\n    df['Position'] = df['Position'].astype(int)\n    df.sort_values(by=['Season','Position','Team'], inplace=True)\n\n\n    #Find all Final Points differences between winner and the teams & differences between all teams \n    news = []\n\n    for i in list(df.League.unique()):\n        temp = df[df['League']==i]\n        for j in list(temp.Season.unique()):\n            temp_ = temp[temp['Season']==j]\n            temp_['winner'] = temp_[temp_['Position']==1]['FinalPoints'].values[0]\n            temp_['FinalPointsDiff_Winner'] = temp_['winner'] - temp_['FinalPoints']\n            temp_.drop(['winner'], axis=1, inplace=True)\n            temp_['FinalPoints_Diff'] = temp_.FinalPoints.diff(-1).shift().fillna(0)\n            news.append(temp_)\n\n    df = pd.concat(news)\n\n    # Read all separated dataframes \n    bundesliga = pd.read_csv(path_bundesliga).drop('Matches', axis=1)\n    epl = pd.read_csv(path_epl).drop('Matches', axis=1)\n    laliga = pd.read_csv(path_laliga).drop('Matches', axis=1)\n    mls = pd.read_csv(path_mls).drop('Matches', axis=1)\n\n    #Merge separated League datasets with the all league table. \n    bundesliga_ = df[df['League']=='Bundesliga'].merge(bundesliga, on=['Season','Team'], how='left')\n    laliga_ = df[df['League']=='La Liga'].merge(laliga, on=['Season','Team'], how='left')\n    epl_ = df[df['League']=='Premier League'].merge(epl, on=['Season','Team'], how='left')\n    mls_ = df[df['League']=='Major League Soccer'].merge(mls, on=['Season','Team'], how='left')\n\n    #Position column indicates the final ranking of the team, to be able to sort , this column converted to integer.\n    # All categorical and integer typed columns are extracted in a list format to be handled during pipelines. \n    bundesliga_['Position'] = bundesliga_['Position'].astype(int)\n    laliga_['Position'] = laliga_['Position'].astype(int)\n    epl_['Position'] = epl_['Position'].astype(int)\n    mls_['Position'] = mls_['Position'].astype(int)\n\n    #Sorting all dataframe values by 'Season','Position','Team' \n    bundesliga_.sort_values(by=['Season','Position','Team'], inplace=True)\n    laliga_.sort_values(by=['Season','Position','Team'], inplace=True)\n    epl_.sort_values(by=['Season','Position','Team'], inplace=True)\n    mls_.sort_values(by=['Season','Position','Team'], inplace=True)\n\n    #MLS dataframe season column is different than other dataframes. So, here we map all the values to commonly used values. \n    mapper = dict(zip(list(mls_.Season.unique()),list(bundesliga_.Season.unique())))\n    mls_['Season'] = mls_['Season'].map(mapper)\n\n    to_bundesliga_ = list(bundesliga_[(bundesliga_['Match1'].isna())].Team.unique())\n    from_bundesliga = list(bundesliga['Team'].unique())\n\n    to_laliga_ = list(laliga_[(laliga_['Match1'].isna())].Team.unique())\n    from_laliga = list(laliga['Team'].unique())\n\n    for i in from_bundesliga:\n        for j in to_bundesliga_:\n            if i in j:\n                bundesliga.replace(i,j, inplace=True)\n\n    for i in from_laliga:\n        for j in to_laliga_:\n            if i in j:\n                laliga.replace(i,j, inplace=True)    \n\n    bundesliga_ = df[df['League']=='Bundesliga'].merge(bundesliga, on=['Season','Team'], how='left')\n    laliga_ = df[df['League']=='La Liga'].merge(laliga, on=['Season','Team'], how='left')\n\n    all_dfs = [bundesliga_,epl_,laliga_,mls_]\n    labels = ['bundesliga','epl','laliga','mls']\n\n    #Concat all prepared dataframes\n    df_ = pd.concat([bundesliga_,epl_,laliga_,mls_], axis=0);df_.head()\n\n    #Create Score columns \n    for i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):\n        df_['Point_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n        df_['Point_'+str(i+1)] = df_['Point_'+str(i+1)].map({'W':3,'D':1,'L':0})\n\n    #Cumsum all Point scores\n    df_.loc[:,'Point_1':'Point_38']= df_.loc[:,'Point_1':'Point_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n\n    for i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):    \n        df_['Win_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n        df_['Win_'+str(i+1)] = df_['Win_'+str(i+1)].map({'W':1,'D':0,'L':0})\n\n    #Cumsum all Win scores\n    df_.loc[:,'Win_1':'Win_38']= df_.loc[:,'Win_1':'Win_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n\n    for i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):\n        df_['Loss_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n        df_['Loss_'+str(i+1)] = df_['Loss_'+str(i+1)].map({'W':0,'D':0,'L':1})\n\n    #Cumsum all Loss scores\n    df_.loc[:,'Loss_1':'Loss_38']= df_.loc[:,'Loss_1':'Loss_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n\n    for i,j in enumerate(list(df_.loc[:,'Match1':'Match38'].columns)):\n        df_['Draw_'+str(i+1)] = df_['Match'+str(i+1)].copy()\n        df_['Draw_'+str(i+1)] = df_['Draw_'+str(i+1)].map({'W':0,'D':1,'L':0})\n\n    #Cumsum all Draw scores     \n    df_.loc[:,'Draw_1':'Draw_38']= df_.loc[:,'Draw_1':'Draw_38'].cumsum(axis = 1, skipna = True).astype(pd.Int64Dtype())\n\n    #Create difference columns by sorting each score column to have positions, then taking the differences between consecutive scores. \n    dists = []\n    for leg in list(df_.League.unique()):\n        temp = df_[df_['League']==leg]\n        for ses in list(temp.Season.unique()):\n            temp_ = temp[temp['Season']==ses]\n            for i,j in enumerate(list(temp_.loc[:,'Point_1':'Point_38'].columns)):\n                temp_['PointDiff_'+str(i+1)] = temp_.sort_values(by=[j], ascending=False)[j].diff(-1).shift()\n                dists.append(temp_)\n\n    df_ = pd.concat(dists).drop_duplicates().sort_values(by=['League','Season','Position','Team'])\n    df_ = df_.reset_index().drop('index', axis=1)\n\n    #Preperae target column \n    df_['target'] = np.where(df_['Position'].isin([1,2,3]), 1, 0) \n\n    #Create all separated dataframes corresponding to the Leagues\n    bundesliga_ = df_[df_['League']=='Bundesliga']\n    epl_ = df_[df_['League']=='Premier League']\n    laliga_ = df_[df_['League']=='La Liga']\n    mls_ = df_[df_['League']=='Major League Soccer']\n\n    #Create below lists to be able to iterate over \n    all_dfs = [bundesliga_,epl_,laliga_,mls_]\n    \n    return all_dfs","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:33.902582Z","iopub.execute_input":"2021-06-30T12:14:33.903068Z","iopub.status.idle":"2021-06-30T12:14:33.931243Z","shell.execute_reply.started":"2021-06-30T12:14:33.903037Z","shell.execute_reply":"2021-06-30T12:14:33.930417Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_step_2 (df, percentage=0.7):\n    df.dropna(axis=1, how='all',inplace=True)\n    df.fillna(0, inplace=True)\n    \n    seasons = ['2015/16', '2016/17', '2017/18', '2018/19', '2019/20']\n    #removed_cols are selected to be removed since they are highly correlated with the final position and final points. Also since we will predict the positions of each team before the season will be finalized, we cannot include them in the model. \n    removed_cols = ['League','Position','Matches','Wins','Draws','Losses','GoalsScored','GoalsConceded','GoalDiff','FinalPoints','FinalPointsDiff_Winner','FinalPoints_Diff']\n    \n    # Decide on percentage and prep related columns \n    percentage = percentage\n    train_number = round(df.Matches.unique()[0]*percentage)\n    \n    #Remove columns not to be included in the model \n    df.drop(removed_cols, axis=1, inplace=True)\n    \n    # Here we will prepare dataframe columns which we will be using during model phase \n    # Create columns index name for the given dataset\n    cols = dict()\n    for i,j in enumerate(list(df.columns)):\n        cols[j] = i    \n        \n    # Prepare a list of related indexes extracted from the above dictionary    \n    a=['Season','Team','Win_'+str(train_number),'Loss_'+str(train_number),'Draw_'+str(train_number) , 'Point_'+str(train_number),'PointDiff_1','PointDiff_'+str(train_number+1),'target']\n\n    search = []\n    for k,j in cols.items():\n        for s in a:\n            if k==s:\n                search.append(j)   \n                \n    # Define the given     \n    df = df.iloc[:, np.r_[search[0],search[1],search[2],search[3],search[4],search[5], search[6]:search[7],search[8]]]\n\n    #Train Test Split\n    df_train = df[df['Season']!=seasons[-1]].drop('Season', axis=1)\n    df_test = df[df['Season']==seasons[-1]].drop('Season', axis=1)\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:35.107757Z","iopub.execute_input":"2021-06-30T12:14:35.10811Z","iopub.status.idle":"2021-06-30T12:14:35.118709Z","shell.execute_reply.started":"2021-06-30T12:14:35.10807Z","shell.execute_reply":"2021-06-30T12:14:35.117509Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dealing with imbalanced dataset\n\n- Paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3648438/\n- As stated in the paper, feature selection preferred to be handled before SMOTE, since Oversampling the minority class with SMOTE violates the independence assumption.\n\nTarget columns only contains 3 True values so all datasets are imbalanced. To deal with this we try different SMOTE techniques \n\nHere are the ones that gave better results as a result of all our trials below:\n\n- BorderlineSMOTE\n- SVMSMOTE\n- OverSampling (SMOTE) + UnderSampling (InstanceHardnessThreshold) ","metadata":{}},{"cell_type":"code","source":"# Make an identity sampler\nclass FakeSampler(BaseSampler):\n\n    _sampling_type = 'bypass'\n\n    def _fit_resample(self, X, y):\n        return X, y\n    \ndef plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    # summarize the new class distribution\n    counter = Counter(y_res)\n    print(f'Target distribution: {sampling.__class__.__name__}, {counter}')  \n\n    # scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = np.where(y_res == label)[0]\n        ax.scatter(X_res[row_ix, 0], X_res[row_ix, 1], label=str(label))\n    ax.legend()\n    #ax.title('Target distribution after Resampling')\n    #ax.show()\n    return Counter(y_res)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:37.193223Z","iopub.execute_input":"2021-06-30T12:14:37.193838Z","iopub.status.idle":"2021-06-30T12:14:37.199502Z","shell.execute_reply.started":"2021-06-30T12:14:37.1938Z","shell.execute_reply":"2021-06-30T12:14:37.198772Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Oversampling for imbalanced datasets\ncat_selector = make_column_selector(dtype_exclude=np.number)\nnum_selector = make_column_selector(dtype_include=np.number)\ncats = cat_selector(bundesliga_train)\nnums = num_selector(bundesliga_train)\nnums.remove('target')\ncat_linear_processor = OneHotEncoder(handle_unknown=\"ignore\")\nnum_linear_processor = make_pipeline(StandardScaler())\n\nlinear_preprocessor = make_column_transformer(\n    (num_linear_processor, nums), (cat_linear_processor, cats))\n\n# Features and Target are defined \nX = bundesliga_train.drop('target', axis=1)\ny = bundesliga_train['target'].values\n\nsteps_ = [(('linear_preprocessor', linear_preprocessor))]\npipeline_sp_ = Pipeline(steps=steps_)\n\nX = pipeline_sp_.fit_transform(X)  \n\n\nfig, ((ax1,ax2,ax3, ax4), (ax5, ax6,ax7,ax8), (ax9, ax10, ax11, ax12), (ax13, ax14, ax15, ax16), (ax17, ax18, ax19,ax20)) = plt.subplots(5, 4, figsize=(20, 20))\nsampler = FakeSampler()\nclf = imbmake_pipeline(sampler, LinearSVC())\nplot_resampling(X, y, sampler, ax1)\nax1.set_title('Original data - y={}'.format(Counter(y)))\n\n# Over + Under Sampling with RandomUnderSampler\nover = SMOTE(k_neighbors=5,n_jobs=-1) #sampling_strategy=0.1,\nunder = RandomUnderSampler() #sampling_strategy=0.5\nsteps_sp = [('o', over), ('u', under)]\npipeline_sp = imbpipeline(steps=steps_sp)\nX_sp, y_sp = pipeline_sp.fit_resample(X, y)\ncounter = Counter(y_sp)\nprint(f'target distribution with Over + Under Sampling: {counter}')\nfor label, _ in counter.items():\n    row_ix = np.where(y_sp == label)[0]\n    ax2.scatter(X_sp[row_ix, 0], X_sp[row_ix, 1], label=str(label))\nax2.legend()\nax2.set_title('Over (SMOTE) + Under(RandomUnderSampler) Sampling')\n\n\nax_arr = (ax3, ax4,ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18, ax19)\n\nfor ax, sampler in zip(ax_arr,   \n                               (\n                                BorderlineSMOTE(k_neighbors=5,n_jobs=-1),\n                                SVMSMOTE(n_jobs=-1),\n                                RandomOverSampler(random_state=0),\n                                SMOTE(random_state=0),\n                                ADASYN(random_state=0),\n                                SMOTEENN(),\n                                \n                                ClusterCentroids(),\n                                NearMiss(version=1),\n                                NearMiss(version=2),\n                                NearMiss(version=3),\n                                EditedNearestNeighbours(),\n                                RepeatedEditedNearestNeighbours(),\n                                AllKNN(allow_minority=True),\n                                CondensedNearestNeighbour(random_state=0),\n                                OneSidedSelection(random_state=0),\n                                NeighbourhoodCleaningRule(),\n                                InstanceHardnessThreshold(\n    random_state=0, estimator=LogisticRegression(solver='lbfgs',\n                                                 multi_class='auto'))\n                               )\n                      ):\n    clf = imbmake_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title('Resampling using {}'.format(sampler.__class__.__name__))\n    \n# Over + Under Sampling with InstanceHardnessThreshold\nover = SMOTE(k_neighbors=5,n_jobs=-1) #sampling_strategy=0.1,\nunderr = InstanceHardnessThreshold(\n    random_state=0, estimator=LogisticRegression(solver='lbfgs',\n                                                 multi_class='auto'))\nsteps_sp = [('o', over), ('u', underr)]\npipeline_sp = imbpipeline(steps=steps_sp)\nclf = imbmake_pipeline(over,underr, LinearSVC())\nclf.fit(X, y)\nX_res, y_res = pipeline_sp.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y_res)\nprint(f'Target distribution: Over (SMOTE) + Under(InstanceHardnessThreshold) Sampling, {counter}')  \n\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y_res == label)[0]\n    ax20.scatter(X_res[row_ix, 0], X_res[row_ix, 1], label=str(label))\nax20.legend()\nax20.set_title('Over (SMOTE) + Under(InstanceHardnessThreshold) Sampling')\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:14:39.043028Z","iopub.execute_input":"2021-06-30T12:14:39.043514Z","iopub.status.idle":"2021-06-30T12:14:43.588857Z","shell.execute_reply.started":"2021-06-30T12:14:39.043481Z","shell.execute_reply":"2021-06-30T12:14:43.58787Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features and Target are defined \nX = bundesliga_train.drop('target', axis=1)\ny = bundesliga_train['target'].values\n\nsteps_ = [(('linear_preprocessor', linear_preprocessor))]\npipeline_sp_ = Pipeline(steps=steps_)\n\nX = pipeline_sp_.fit_transform(X)  \nfig, ((ax1,ax2),(ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\n\nsampler = FakeSampler()\nclf = imbmake_pipeline(sampler, LinearSVC())\nplot_resampling(X, y, sampler, ax1)\nax1.set_title('Original data - y={}'.format(Counter(y)))\n\n# Over + Under Sampling with InstanceHardnessThreshold\nover = SMOTE(k_neighbors=5,n_jobs=-1) #sampling_strategy=0.1,\nunderr = InstanceHardnessThreshold(\n    random_state=0, estimator=LogisticRegression(solver='lbfgs',\n                                                 multi_class='auto'))\nsteps_sp = [('o', over), ('u', underr)]\npipeline_sp = imbpipeline(steps=steps_sp)\nclf = imbmake_pipeline(over,underr, LinearSVC(class_weight=\"balanced\"))\nclf.fit(X, y)\nX_res, y_res = pipeline_sp.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y_res)\nprint(f'Target distribution: Over (SMOTE) + Under(InstanceHardnessThreshold) Sampling, {counter}')  \n\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y_res == label)[0]\n    ax2.scatter(X_res[row_ix, 0], X_res[row_ix, 1], label=str(label))\nax2.legend()\nax2.set_title('Over (SMOTE) + Under(InstanceHardnessThreshold) Sampling')\n\nax_arr = (ax3, ax4)\n\nfor ax, sampler in zip(ax_arr,   \n                               (\n                                BorderlineSMOTE(k_neighbors=5,n_jobs=-1),\n                                SVMSMOTE(n_jobs=-1)\n                                \n                               )\n                      ):\n    clf = imbmake_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title('Resampling using {}'.format(sampler.__class__.__name__))\n    \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:15:02.554864Z","iopub.execute_input":"2021-06-30T12:15:02.555218Z","iopub.status.idle":"2021-06-30T12:15:04.027808Z","shell.execute_reply.started":"2021-06-30T12:15:02.555187Z","shell.execute_reply":"2021-06-30T12:15:04.026765Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Below plots show us to select  borderline-1\nX = bundesliga_train.drop('target', axis=1)\ny = bundesliga_train['target'].values\n\nsteps_ = [(('linear_preprocessor', linear_preprocessor))]\npipeline_sp_ = Pipeline(steps=steps_)\n\nX = pipeline_sp_.fit_transform(X)  \nfig, ((ax1,ax2, ax3)) = plt.subplots(1, 3, figsize=(12, 5))\n\nsampler = FakeSampler()\nclf = imbmake_pipeline(sampler, LinearSVC())\nplot_resampling(X, y, sampler, ax1)\nax1.set_title('Original data - y={}'.format(Counter(y)))\n\nax_arr = (ax2,ax3)\n\nfor ax, sampler in zip(ax_arr,   \n                               (\n                                BorderlineSMOTE(k_neighbors=5,n_jobs=-1,random_state=0, kind='borderline-1'),\n                                BorderlineSMOTE(k_neighbors=5,n_jobs=-1,random_state=0, kind='borderline-2')\n                                \n                               )\n                      ):\n    clf = imbmake_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title('Resampling using {}'.format(sampler.__class__.__name__))\n    \nfig.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-30T12:15:15.399233Z","iopub.execute_input":"2021-06-30T12:15:15.399602Z","iopub.status.idle":"2021-06-30T12:15:16.332221Z","shell.execute_reply.started":"2021-06-30T12:15:15.399567Z","shell.execute_reply":"2021-06-30T12:15:16.331104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection \n\nHere we will try different feature selection models to remove the high importance ones and reduce the size of the dataset.\n\nThe results from the defined feature selection methods will be recorded in a data frame. Voting from the outputs will be done by summing up all the selected features and defining a threshold.\n\nWith this method we will be able to process different feature selection strategies and then select the most important ones.","metadata":{}},{"cell_type":"code","source":"def feature_selection(X,y,feature_names, RF, PIRF,XGB,PIXGB,SFMRF,SFMXGB, MI,RFECVLinearSVC):\n    #Define a Voting dataframe\n    feature_voting = pd.DataFrame(columns=['Feature_names','RF', 'PI+RF','XGB','PI+XGB', 'SFM+RF', 'SFM+XGB', 'MI', 'RFECV+LinearSVC', 'Voting'])\n    feature_voting['Feature_names'] = feature_names\n\n    #Model Based Selectors \n    #Feature Selection with  RandomForestClassifier\n    clf = RandomForestClassifier()\n    clf.fit(X, y)\n    if RF:\n        print(\"\\nRF accuracy: %0.3f\" % clf.score(X, y))\n        print(f'\\nTotal # of features found with RandomForestClassifier: {len([i for i in list(clf.feature_importances_) if i>0])}')\n\n        y_ticks = np.arange(0, len(feature_names))\n        fig, ax = plt.subplots(figsize=(5,5))\n        sorted_idx = clf.feature_importances_.argsort()\n        ax.barh(y_ticks, clf.feature_importances_[sorted_idx])\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(feature_names[sorted_idx])\n        ax.set_title(\"Random Forest Feature Importances\")\n        fig.tight_layout()\n        plt.show()\n    selection = list(feature_names[(np.asarray([True if i >0 else False for i in list(clf.feature_importances_)]))])\n    feature_voting['RF'] = np.where(feature_voting['Feature_names'].isin(selection), 1, 0)\n\n\n    #Feature Selection with  permutation_importance\n    result = permutation_importance(clf, X, y, n_repeats=10,\n                                    random_state=42, n_jobs=2)\n    if PIRF:\n        print(f'\\nTotal # of features found with RandomForestClassifier + Permutation_importance: {len([i for i in list(result.importances_mean) if i>0])}')\n        y_ticks = np.arange(0, len(feature_names))\n        fig, ax = plt.subplots(figsize=(5,5))\n        sorted_idx = result.importances_mean.argsort()\n        ax.barh(y_ticks, result.importances_mean[sorted_idx])\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(feature_names[sorted_idx])\n        ax.set_title(\"RandomForestClassifier + Permutation Importances Feature Importances\")\n        fig.tight_layout()\n        plt.show()\n    selection = list(feature_names[(np.asarray([True if i >0 else False for i in list(result.importances_mean)]))])\n    feature_voting['PI+RF'] = np.where(feature_voting['Feature_names'].isin(selection), 1,0)\n\n\n    #Feature Selection with  XGBClassifier\n\n    xgb = XGBClassifier()\n    xgb.fit(X, y)\n    if XGB:\n        print(f'\\nTotal # of features found with XGBClassifier: {len([i for i in list(xgb.feature_importances_) if i>0])}')\n        y_ticks = np.arange(0, len(feature_names))\n        fig, ax = plt.subplots(figsize=(5,5))\n        sorted_idx = xgb.feature_importances_.argsort()\n        ax.barh(y_ticks, xgb.feature_importances_[sorted_idx])\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(feature_names[sorted_idx])\n        ax.set_title(\"XGB Feature Importances\")\n        fig.tight_layout()\n        plt.show()\n    selection = list(feature_names[(np.asarray([True if i >0 else False for i in list(xgb.feature_importances_)]))])\n    feature_voting['XGB'] = np.where(feature_voting['Feature_names'].isin(selection), 1,0)\n\n    #Feature Selection with  permutation_importance\n    result = permutation_importance(xgb, X, y, n_repeats=10,\n                                    random_state=42, n_jobs=2)\n    if PIXGB:\n        print(f'\\nTotal # of features found with XGBClassifier + Permutation_importance: {len([i for i in list(result.importances_mean) if i>0])}')\n        y_ticks = np.arange(0, len(feature_names))\n        fig, ax = plt.subplots(figsize=(5,5))\n        sorted_idx = result.importances_mean.argsort()\n        ax.barh(y_ticks, result.importances_mean[sorted_idx])\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(feature_names[sorted_idx])\n        ax.set_title(\"XGBClassifier + Permutation Importances Feature Importances\")\n        fig.tight_layout()\n        plt.show()\n    selection = list(feature_names[(np.asarray([True if i >0 else False for i in list(result.importances_mean)]))])\n    feature_voting['PI+XGB'] = np.where(feature_voting['Feature_names'].isin(selection), 1,0)\n\n    #Feature Selection with SelectFromModel RandomForestClassifier\n\n    rf = RandomForestClassifier(n_estimators=100,class_weight =\"balanced\", random_state=42)\n    select = SelectFromModel(rf)  #max_features=20\n    selection = select.fit(X, y)\n    selected_feat=feature_names[(selection.get_support())]\n    if SFMRF:\n        print(f'\\nOptimal # features with SelectFromModel RandomForestClassifier:{len(selected_feat)}')\n        print(f'\\nSelected features from SelectFromModel with RandomForestClassifier{selected_feat}')\n    feature_voting['SFM+RF'] = np.where(feature_voting['Feature_names'].isin(selected_feat), 1,0)\n\n    #Feature Selection with SelectFromModel XGBClassifier\n\n    rf = XGBClassifier(n_jobs=-1)\n    select = SelectFromModel(rf)  #max_features=20\n    selection = select.fit(X, y)\n    selected_feat=feature_names[(selection.get_support())]\n    if SFMXGB:\n        print(f'\\nOptimal # features SelectFromModel XGBClassifier:{len(selected_feat)}')\n        print(f'\\nSelected features from SelectFromModel with XGBClassifier {selected_feat}')\n    feature_voting['SFM+XGB'] = np.where(feature_voting['Feature_names'].isin(selected_feat), 1,0)\n   \n\n    #Filter Methods\n    #Feature Selection with  mutual_info_classif\n    mutual_information = mutual_info_classif(X, y)\n    if MI:\n        print(f'\\nTotal # of features found with mutual_info_classif: {len([i for i in list(mutual_information) if i>0])}')\n        plt.subplots(1, figsize=(26, 1))\n        sns.heatmap(mutual_information[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\n        plt.yticks([], [])\n        plt.gca().set_xticklabels(feature_names, rotation=45, ha='right', fontsize=12)\n        plt.suptitle(\"Variable Importance (mutual_info_classif)\", fontsize=18, y=1.2)\n        plt.gcf().subplots_adjust(wspace=0.2)\n        plt.show()\n        pass\n    selection = list(feature_names[(np.asarray([True if i >0 else False for i in list(mutual_information)]))])\n    feature_voting['MI'] = np.where(feature_voting['Feature_names'].isin(selection), 1,0)\n\n    #Wrapper \n    #Feature Selection with  RFECV + LinearSVC\n    min_features_to_select = 1  # Minimum number of features to consider\n    rfecv = RFECV(estimator=LinearSVC(class_weight='balanced'), step=1, cv=StratifiedKFold(2),\n                  scoring='accuracy',\n                  min_features_to_select=min_features_to_select)\n    rfecv.fit(X, y)\n    if RFECVLinearSVC:\n        print(\"\\nOptimal number of features with RFECV + LinearSVC: %d\" % rfecv.n_features_)\n        print(f'\\nSelected Features with RFECV + LinearSVC : {list(feature_names[np.asarray([i for i in list(rfecv.ranking_) if i !=1])])}')\n        # Plot number of features VS. cross-validation scores\n        plt.figure()\n        plt.xlabel(\"Number of features selected\")\n        plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n        plt.plot(range(min_features_to_select,\n                       len(rfecv.grid_scores_) + min_features_to_select),\n                 rfecv.grid_scores_)\n        plt.show()\n    \n    selection = list(feature_names[(np.asarray([True if i!=1 else False for i in list(rfecv.ranking_)]))])\n    feature_voting['RFECV+LinearSVC'] = np.where(feature_voting['Feature_names'].isin(selection), 1,0)\n\n    feature_voting['Voting'] = feature_voting.iloc[:,1:].sum(axis=1).astype(int)\n    feature_voting.sort_values(by='Voting', ascending=False, inplace=True)\n    return feature_voting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-30T12:15:23.079437Z","iopub.execute_input":"2021-06-30T12:15:23.079766Z","iopub.status.idle":"2021-06-30T12:15:23.107854Z","shell.execute_reply.started":"2021-06-30T12:15:23.079738Z","shell.execute_reply":"2021-06-30T12:15:23.10679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare X and y \ncat_selector = make_column_selector(dtype_exclude=np.number)\nnum_selector = make_column_selector(dtype_include=np.number)\ncats = cat_selector(bundesliga_train)\nnums = num_selector(bundesliga_train)\nnums.remove('target')\n\ncat_linear_processor = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\nnum_linear_processor = Pipeline(steps=[('scaler', StandardScaler())])\n\nlinear_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_linear_processor, nums),\n        ('cat', cat_linear_processor, cats)])\n\n# Features and Target are defined \nX = bundesliga_train.drop('target', axis=1)\nX = X[cats + nums]\ny = bundesliga_train['target'].values\n\nsteps_ = [(('linear_preprocessor', linear_preprocessor))]\npipeline_sp_ = Pipeline(steps=steps_)\n\nX = pipeline_sp_.fit_transform(X)\n\n#Define feature names in the correct order\nfeature_names =  np.r_[pipeline_sp_.named_steps['linear_preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names(cats), nums]\n\ndf_ = pd.DataFrame(X, columns=feature_names)\ndf_['target'] = y\n\nfeature_voting = feature_selection(X,y,feature_names,True, True,True,True,True,True, True,True)","metadata":{"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-30T12:15:29.349272Z","iopub.execute_input":"2021-06-30T12:15:29.349612Z","iopub.status.idle":"2021-06-30T12:15:49.249035Z","shell.execute_reply.started":"2021-06-30T12:15:29.34958Z","shell.execute_reply":"2021-06-30T12:15:49.247795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_voting","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:15:49.251384Z","iopub.execute_input":"2021-06-30T12:15:49.251814Z","iopub.status.idle":"2021-06-30T12:15:49.277266Z","shell.execute_reply.started":"2021-06-30T12:15:49.251759Z","shell.execute_reply":"2021-06-30T12:15:49.276259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Baseline Models:\n\nBefore implementing grid search and hypermeter optimization, all models are tested on the training sets by defining a pipeline which includes Categorical Feature Encoding by One Hot Encoder and Numerical Feature Standard Scaling. Feature Selection is performed before oversampling as indicated in the SMOTE paper.\nModels which are compared and kept in a data frame are: 'Decision Tree', 'Neural Network', 'Logistic Regression', 'Linear SVC', 'K-Nearest Neighbors', 'Gradient Boosting Classifier', 'Extra Tree Classifier', 'Adaptive Boosting Classifier', 'Random Forest', 'Extreme Gradient Boosting' and lastly 'Stacking Classifier' as an ensemble method. For the test of these models, we used shuffled K Fold Cross Validation with scoring ROC-AUC. Each fold’s results are saved as in a data frame. Then by taking the mean and standard deviations of each model results, we prepared a final table which then ranked considering test accuracy and standard deviations. The results can also be seen below.     \n","metadata":{}},{"cell_type":"code","source":"def baseline_models(df):\n\n    print(f'Shape of the dataset:{df.shape}')\n    #Here we are defining categoric and numerical variables, Target column is removed from numerical list\n    cat_selector = make_column_selector(dtype_exclude=np.number)\n    num_selector = make_column_selector(dtype_include=np.number)\n    cats = cat_selector(df)\n    nums = num_selector(df)\n    nums.remove('target')\n\n    #print('Categorical Variables:',cats)\n    #print('Numerical Variables:',nums)\n\n    # Features and Target are defined \n    X = df.drop('target', axis=1)\n    y = df['target'].values\n    # summarize the new class distribution\n    counter = Counter(y)\n    print(f'Target distribution: {counter}')\n\n    # To prevent data leakage during cross validation we prepare a pipeline to handle each splits. Categorical columns are transformed with OneHotEncoding, and numerical columns are standard scaled. \n\n    cat_linear_processor = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    num_linear_processor = Pipeline(steps=[('scaler', StandardScaler())])\n        \n    linear_preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', num_linear_processor, nums),\n            ('cat', cat_linear_processor, cats)])  \n\n    #Create to observe resulted features, targets before cv\n    steps_ = [(('linear_preprocessor', linear_preprocessor))]\n    pipeline_sp_ = Pipeline(steps=steps_)\n    X = pipeline_sp_.fit_transform(X)\n    \n    #Define feature names in the correct order\n    feature_names =  np.r_[pipeline_sp_.named_steps['linear_preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names(cats), nums]\n\n    df_ = pd.DataFrame(X, columns=feature_names)\n    df_['target'] = y\n    \n    #Feature Selection \n    feature_voting = feature_selection(X,y,feature_names,False, False,False,False,False,False, False,False)\n    selected_feature = list(feature_voting[feature_voting['Voting']>=3].Feature_names.unique()) + ['target']\n    \n    # Prepare a dataframe consist of selected features \n    df_ = df_[selected_feature]\n    cats = cat_selector(df_)\n    nums = num_selector(df_)\n    nums.remove('target')\n    \n    linear_preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', num_linear_processor, nums),\n            ('cat', cat_linear_processor, cats)])  \n    \n    # Prep X-Features and y-Target\n    X = df_.drop('target', axis=1)\n    #X = X[cats + nums]\n    y = df_['target'].values\n    #########################################\n    # To observe target distribution prep X_ \n    steps_ = [(('linear_preprocessor', linear_preprocessor))]\n    pipeline_sp_ = Pipeline(steps=steps_)\n\n    X_ = pipeline_sp_.fit_transform(X)\n    \n    # SMOTE\n    sampler = BorderlineSMOTE(k_neighbors=5,n_jobs=-1,random_state=123, kind='borderline-1')\n    \n    # Extract Target distribution from selected SMOTE result  \n    clf_ = imbmake_pipeline(sampler, LinearSVC())\n    clf_.fit(X_, y) \n    stepss = [(('sampler', sampler))]\n    pipeline_sp = imbpipeline(steps=stepss)\n    X_res, y_res = pipeline_sp.fit_resample(X_, y)\n    counter = Counter(y_res)\n    print(f'Target distribution -> BorderlineSMOTE Sampling, {counter}')\n    print(f'Dataset Shape after -> Feature Selection and BorderlineSMOTE Sampling, {X_res.shape}')\n    #########################################\n    def get_stacking():\n        # define the base models\n        level0 = list()\n        level0.append(('lr', LogisticRegression()))\n        level0.append(('knn', KNeighborsClassifier()))\n        level0.append(('cart', DecisionTreeClassifier()))\n        level0.append(('svm', LinearSVC()))\n        level0.append(('bayes', GaussianNB()))\n        level0.append(('rf', RandomForestClassifier()))\n        level0.append(('gbc', GradientBoostingClassifier()))\n        level0.append(('ec', ExtraTreesClassifier()))\n        level0.append(('abc', AdaBoostClassifier(DecisionTreeClassifier())))\n        level0.append(('xgb', XGBClassifier()))\n        # define meta learner model\n        level1 = XGBClassifier()\n        # define the stacking ensemble\n        model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n        return model\n    \n\n    #For each classification model we prepare a pipeline, then combine them in a list. \n    dt_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', DecisionTreeClassifier(random_state=42))])\n\n    nn_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', MLPClassifier(random_state=42))])\n\n    lg_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', LogisticRegression(random_state=42))])\n\n    lsvc_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', LinearSVC(class_weight=\"balanced\",random_state=42))])\n\n    knn_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', KNeighborsClassifier())])\n    \n    gbc_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', GradientBoostingClassifier())])\n    ec_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', ExtraTreesClassifier())])\n    abc_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', AdaBoostClassifier(DecisionTreeClassifier()))])\n    \n    xgb_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', XGBClassifier())])\n    \n    rf_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', RandomForestClassifier())])\n    \n    Stacked = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf',  get_stacking())])\n\n\n    pp = [dt_pipeline,nn_pipeline,lg_pipeline,lsvc_pipeline,knn_pipeline,gbc_pipeline,ec_pipeline,abc_pipeline,xgb_pipeline,rf_pipeline,Stacked]\n    pp_dict = {0: 'Decision Tree', 1: 'Neural Network',2: 'Logistic Regression',3:'LinearSVC',4:'KNN', 5:'Gradient BC', 6:'ExtraTree C', 7:'AdaBoost C',8:'XGB',9:'Random Forest', 10:'Stacked'}\n\n    # 5 folds are defined with shuffling. We select stratified to have the same distribution in target \n    kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    clfs = []\n    train_all = []\n    trains = []\n    trains_std = []\n    tests = []\n    test_all = []\n    tests_std = []\n\n    for idx, gs in enumerate(pp):\n        results = cross_validate(gs, X, y,cv=kf, return_train_score=True, scoring='roc_auc',n_jobs=-1)\n        clfs.append(pp_dict[idx])\n        trains.append(results['train_score'].mean())\n        train_all.append(results['train_score'])\n        trains_std.append(results['train_score'].std())\n        tests.append(results['test_score'].mean())\n        test_all.append(results['test_score'])\n        tests_std.append(results['test_score'].std())\n\n    folds = pd.DataFrame(columns=clfs)\n    for i,j in zip(clfs,test_all):\n        folds[i] = j  \n        \n    display(folds)    \n        \n    tables = pd.DataFrame()\n    tables['model'] = clfs\n    tables['train'] = trains\n    tables['train_std'] = trains_std\n    tables['test'] = tests\n    tables['test_std'] = tests_std\n    tables['test_rank'] = (tables['test'].rank(ascending=False) + tables['test_std'].rank(ascending=True)).rank(ascending=True).astype(int)\n    tables['type'] = 'baseline'\n    tables.sort_values(by='test_rank', inplace=True)\n    display(tables)\n    # plot model performance for comparison\n    plt.boxplot(test_all, labels=clfs, showmeans=True)\n    plt.title('Test Results')\n    plt.xticks(rotation=45)\n    plt.show()\n\n    return tables\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-30T12:15:49.27863Z","iopub.execute_input":"2021-06-30T12:15:49.278899Z","iopub.status.idle":"2021-06-30T12:15:49.306469Z","shell.execute_reply.started":"2021-06-30T12:15:49.278872Z","shell.execute_reply":"2021-06-30T12:15:49.305561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tables = baseline_models(bundesliga_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:15:54.079979Z","iopub.execute_input":"2021-06-30T12:15:54.080343Z","iopub.status.idle":"2021-06-30T12:17:46.509827Z","shell.execute_reply.started":"2021-06-30T12:15:54.080311Z","shell.execute_reply":"2021-06-30T12:17:46.509006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hypermeter Optimization, Overfit/Underfit Analysis**: For each models hypermeter optimization is performed with given parameter grids in a Grid Search Cross Validation method. Selected model parameters are kept in a dictionary as like below, which then used to predict the validation sets that is defined in the beginning of the split as the last season. Model accuracies are combined including balanced accuracy and classification reports of precision, recall and f1 scores for true and false classes. An example from one of the reports is shown below. This final result table is used to analyze overfitting and create a leaderboard for the given leagues. ","metadata":{}},{"cell_type":"code","source":"def hyp_optimization_model_selection(df, df_test):\n\n    print(f'Shape of the dataset:{df.shape}')\n    #Here we are defining categoric and numerical variables, Target column is removed from numerical list\n    cat_selector = make_column_selector(dtype_exclude=np.number)\n    num_selector = make_column_selector(dtype_include=np.number)\n    cats = cat_selector(df)\n    nums = num_selector(df)\n    nums.remove('target')\n\n    #print('Categorical Variables:',cats)\n    #print('Numerical Variables:',nums)\n\n    # Features and Target are defined \n    X = df.drop('target', axis=1)\n    y = df['target'].values\n    # summarize the new class distribution\n    counter = Counter(y)\n    print(f'Target distribution: {counter}')\n\n    # To prevent data leakage during cross validation we prepare a pipeline to handle each splits. Categorical columns are transformed with OneHotEncoding, and numerical columns are standard scaled. \n\n    cat_linear_processor = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    num_linear_processor = Pipeline(steps=[('scaler', StandardScaler())])\n        \n    linear_preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', num_linear_processor, nums),\n            ('cat', cat_linear_processor, cats)])  \n\n    #Create to observe resulted features, targets before cv\n    steps_ = [(('linear_preprocessor', linear_preprocessor))]\n    pipeline_sp_ = Pipeline(steps=steps_)\n    X = pipeline_sp_.fit_transform(X)\n    \n    #Define feature names in the correct order\n    feature_names =  np.r_[pipeline_sp_.named_steps['linear_preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names(cats), nums]\n\n    df_ = pd.DataFrame(X, columns=feature_names)\n    df_['target'] = y\n    \n    #Feature Selection \n    feature_voting = feature_selection(X,y,feature_names,False, False,False,False,False,False, False,False)\n    selected_feature = list(feature_voting[feature_voting['Voting']>=3].Feature_names.unique()) + ['target']\n    \n    # Prepare a dataframe consist of selected features \n    df_ = df_[selected_feature]\n    cats = cat_selector(df_)\n    nums = num_selector(df_)\n    nums.remove('target')\n    \n    linear_preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', num_linear_processor, nums),\n            ('cat', cat_linear_processor, cats)])  \n    \n    # Prep X-Features and y-Target\n    X = df_.drop('target', axis=1)\n    #X = X[cats + nums]\n    y = df_['target'].values\n #########################################\n    # To observe target distribution prep X_ \n    steps_ = [(('linear_preprocessor', linear_preprocessor))]\n    pipeline_sp_ = Pipeline(steps=steps_)\n\n    X_ = pipeline_sp_.fit_transform(X)\n    \n    # SMOTE\n    sampler = BorderlineSMOTE(k_neighbors=5,n_jobs=-1,random_state=123, kind='borderline-1')\n    \n    # Extract Target distribution from selected SMOTE result  \n    clf_ = imbmake_pipeline(sampler, LinearSVC())\n    clf_.fit(X_, y)  \n    stepss = [(('sampler', sampler))]\n    pipeline_sp = imbpipeline(steps=stepss)  \n    X_res, y_res = pipeline_sp.fit_resample(X_, y)\n    counter = Counter(y_res)\n    print(f'Target distribution -> BorderlineSMOTE Sampling, {counter}')\n    print(f'Dataset Shape after -> Feature Selection and BorderlineSMOTE Sampling, {X_res.shape}')\n #########################################\n\n    \n    def get_stacking():\n        # define the base models\n        level0 = list()\n        level0.append(('lr', LogisticRegression()))\n        level0.append(('knn', KNeighborsClassifier()))\n        level0.append(('cart', DecisionTreeClassifier()))\n        level0.append(('svm', LinearSVC()))\n        level0.append(('bayes', GaussianNB()))\n        level0.append(('rf', RandomForestClassifier()))\n        level0.append(('gbc', GradientBoostingClassifier()))\n        level0.append(('ec', ExtraTreesClassifier()))\n        level0.append(('xgb', XGBClassifier()))\n        # define meta learner model\n        level1 = XGBClassifier()\n        # define the stacking ensemble\n        model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n        return model\n\n\n    \n    grid_params_dt = [ {\"clf__criterion\" : [\"gini\", \"entropy\"],\n                  \"clf__splitter\" :   [\"best\", \"random\"],\n                  \"clf__max_depth\" :[None,2,4,6,8,10,12]} ]    \n    \n    \n    grid_params_nn = [{'clf__hidden_layer_sizes': [(10,10,10), (10,10,10,10), (10,10,10,10,10), (10,10,10,10,10,10)], 'clf__alpha': list(10.0 ** -np.arange(1, 6))}]\n    \n    grid_params_lr = [{\"clf__C\": [1.0],  'clf__class_weight':[None,'balanced']}] \n\n    \n    grid_params_svc= [{'clf__penalty': ['l2','l1'], \n                       'clf__class_weight': [None,'balanced'],\n                       'clf__dual': [True, False],\n                       'clf__C': [0.01,0.1,1.0,10],\n                       'clf__tol':[0.001,0.0008,0.0009,0.0011]}]\n    \n    \n    grid_params_knn= [{'clf__n_neighbors': [4,5,6,7], \n                  'clf__leaf_size': [10,20,30,40]}]\n    \n    \n    grid_params_gbc = [{'clf__loss' : [\"deviance\"],\n                 'clf__n_estimators' : [450,460,500],\n                 'clf__learning_rate': [0.1,0.11],\n                 'clf__max_depth': [7,8],\n                 'clf__min_samples_leaf': [30,40],\n                 'clf__max_features': [0.1,0.4,0.6]}]    \n\n    \n    grid_params_ec = [{\"clf__max_depth\": [3, 4, 5],\n                 \"clf__max_features\": [3, 10, 15],\n                 \"clf__min_samples_split\": [2, 3, 4],\n                 \"clf__min_samples_leaf\": [1, 2],\n                 \"clf__bootstrap\": [False,True],\n                 \"clf__n_estimators\" :[100,200,300],\n                 \"clf__criterion\": [\"gini\",\"entropy\"]} ] \n\n    \n    grid_params_xgb = [{'clf__learning_rate': [0.1,0.04,0.01], \n                  'clf__max_depth': [3,4,5,6,7],\n                  'clf__n_estimators': [100,350,400,450,2000], \n                  'clf__gamma': [0,1,5,8],\n                  'clf__subsample': [0.8,0.95,1.0]}]\n    \n    grid_params_rf = [{'clf__n_estimators': [10, 50, 100,250,500,1000], 'clf__max_depth': [50, 150, 250], 'clf__min_samples_split':[2,3,4], 'clf__min_samples_leaf':[1,2,3,4], \"clf__bootstrap\": [True],\n           \"clf__n_estimators\" :[50,80],\n           \"clf__criterion\": [\"gini\",\"entropy\"],\n           \"clf__max_leaf_nodes\":[26,28],\n           \"clf__min_impurity_decrease\":[0.0],\n           \"clf__min_weight_fraction_leaf\":[0.0]}]\n\n    grid_params_s = [{'clf__passthrough':[True,False]}] \n\n    \n    #For each classification model we prepare a pipeline, then combine them in a list. \n    dt_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', DecisionTreeClassifier(random_state=42))])\n\n    nn_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', MLPClassifier(random_state=42))])\n\n    lg_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', LogisticRegression(random_state=42))])\n\n    lsvc_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', LinearSVC(class_weight=\"balanced\",random_state=42))])\n\n    knn_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', KNeighborsClassifier())])\n    \n    gbc_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', GradientBoostingClassifier())])\n    ec_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', ExtraTreesClassifier())])\n    \n    xgb_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', XGBClassifier())])\n\n    rf_pipeline = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf', RandomForestClassifier())])\n    \n    Stacked = imbpipeline(\n        steps=[('linear_preprocessor', linear_preprocessor),\n                                    ('sampler', sampler),\n                                    ('clf',  get_stacking())])\n    \n    pp = [dt_pipeline, nn_pipeline, lg_pipeline, lsvc_pipeline, knn_pipeline, gbc_pipeline,  xgb_pipeline, rf_pipeline, Stacked] #ec_pipeline,\n\n    # 5 folds are defined with shuffling. We select stratified to have the same distribution in target \n    kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n\n\n    scoring='roc_auc'\n    gs_dt = GridSearchCV(estimator=dt_pipeline, param_grid=grid_params_dt, scoring=scoring, cv=kf)\n    gs_nn = GridSearchCV(estimator=nn_pipeline, param_grid=grid_params_nn, scoring=scoring, cv=kf)\n    gs_lr = GridSearchCV(estimator=lg_pipeline, param_grid=grid_params_lr, scoring=scoring, cv=kf)\n    gs_svc = GridSearchCV(estimator=lsvc_pipeline, param_grid=grid_params_svc, scoring=scoring, cv=kf)\n    gs_knn = GridSearchCV(estimator=knn_pipeline, param_grid=grid_params_knn, scoring=scoring, cv=kf)\n    gs_gbc = GridSearchCV(estimator=gbc_pipeline, param_grid=grid_params_gbc, scoring=scoring, cv=kf)\n    gs_ec = GridSearchCV(estimator=ec_pipeline, param_grid=grid_params_ec, scoring=scoring, cv=kf) \n    gs_xgb = GridSearchCV(estimator=xgb_pipeline, param_grid=grid_params_xgb, scoring=scoring, cv=kf)\n    gs_rf = GridSearchCV(estimator=rf_pipeline, param_grid=grid_params_rf, scoring=scoring, cv=kf)\n    gs_s = GridSearchCV(estimator=Stacked, param_grid=grid_params_s, scoring=scoring, cv=kf)\n\n    # List of pipelines for ease of iteration\n    grids = [gs_dt, gs_nn, gs_lr, gs_svc, gs_knn, gs_gbc,gs_xgb, gs_rf, gs_s] #\n\n    # Dictionary of pipelines and classifier types for ease of reference\n    grid_dict = {0: 'Decision Tree', 1: 'Neural Network',2: 'Logistic Regression',3:'LinearSVC',4:'KNN', 5:'Gradient BC', 6:'XGB',7:'Random Forest', 8:'Stacked'}  #6:'ExtraTree C', 7:'XGB',8:'Random Forest', 9:'Stacked'\n\n    clfs = []\n    train_all = []\n    trains = []\n    trains_std = []\n    tests = []\n    test_all = []\n    tests_std = []\n    best_params_results = dict()    \n    \n    start_time=time.time()\n    for idx, gs in enumerate(grids):\n        start_time_=time.time()\n        print('\\nTask: %s is processing!' % grid_dict[idx])    \n\n        # Non_nested parameter search and scoring\n        gs.fit(X, y)\n        best_regressor = gs.best_estimator_\n        best_params = gs.best_params_\n        print('Best parameters for {} = {}'.format(grid_dict[idx],best_params))\n        print(f'Best score with {grid_dict[idx]} is {gs.best_score_}')\n        best_params_results[grid_dict[idx]] = best_regressor\n        results  = cross_validate(best_regressor, X, y, cv=kf, return_train_score=True, scoring=scoring, n_jobs=-1)\n        clfs.append(grid_dict[idx])\n        trains.append(results['train_score'].mean())\n        train_all.append(results['train_score'])\n        trains_std.append(results['train_score'].std())\n        tests.append(results['test_score'].mean())\n        test_all.append(results['test_score'])\n        tests_std.append(results['test_score'].std())\n        print('\\n')\n        time_took = time.time() - start_time_\n        print(f\"Total runtime for {grid_dict[idx]} Grid Search: {hms_string(time_took)}\")\n    \n    time_took = time.time() - start_time\n    print(f\"Total runtime for Grid Search: {hms_string(time_took)}\")\n\n\n    folds = pd.DataFrame(columns=clfs)\n    for i,j in zip(clfs,test_all):\n        folds[i] = j  \n        \n    display(folds)    \n        \n    tables = pd.DataFrame()\n    tables['model'] = clfs\n    tables['train'] = trains\n    tables['train_std'] = trains_std\n    tables['test'] = tests\n    tables['test_std'] = tests_std\n    tables['test_rank'] = (tables['test'].rank(ascending=False) + tables['test_std'].rank(ascending=True)).rank(ascending=True).astype(int)\n    tables['type'] = 'gridsearch'\n    tables.sort_values(by='test_rank', inplace=True)\n    display(tables)\n    # plot model performance for comparison\n    plt.boxplot(test_all, labels=clfs, showmeans=True)\n    plt.title('Test Results')\n    plt.xticks(rotation=45)\n    plt.show()\n    \n    #Get ranked 1 model and predict testvalue \n    winner_model = tables['model'].values[0] #tables[tables['test_rank']==1]['model'].values[0]\n    model = best_params_results[winner_model]\n    \n    # Prep X-Features and y-Target\n    test_results = df_test.copy()\n    test_results = test_results[['Team','target']]\n\n    print(f'Shape of the test dataset:{df_test.shape}')\n\n    cats = cat_selector(df_test)\n    nums = num_selector(df_test)\n    nums.remove('target')\n\n    # Features and Target are defined \n    X = df_test.drop('target', axis=1)\n    #X = [cats+nums]\n    y = df_test['target'].values\n    # summarize the new class distribution\n    counter = Counter(y)\n    print(f'Target distribution of Test Set: {counter}')\n\n    cat_linear_processor = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    num_linear_processor = Pipeline(steps=[('scaler', StandardScaler())])\n\n\n    linear_preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', num_linear_processor, nums),\n            ('cat', cat_linear_processor, cats)]) \n\n    #Create to observe resulted features, targets before cv\n    steps_ = [(('linear_preprocessor', linear_preprocessor))]\n    pipeline_sp_ = Pipeline(steps=steps_)\n    X = pipeline_sp_.fit_transform(X)\n\n    #Define feature names in the correct order\n    feature_names =  np.r_[pipeline_sp_.named_steps['linear_preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names(cats), nums]\n\n    df_test = pd.DataFrame(X, columns=feature_names)\n    df_test['target'] = y\n\n    for i in list(df_test.columns):\n        if i not in selected_feature:\n            df_test.drop(i, axis=1, inplace=True)\n\n    for i in selected_feature:\n        if i not in list(df_test.columns):\n            df_test[i] = 0\n\n    df_test=df_test[selected_feature]\n\n    # Prep X-Features and y-Target\n    X = df_test.drop('target', axis=1)\n    #X = X[cats + nums]\n    y = df_test['target'].values\n    \n    preds = model.predict(X)\n    test_results[\"Predicted\"] = list(preds)\n    print('Pedicted Test Set Results:')\n    display(test_results)\n\n    print('Classification report from test results:\\n',classification_report(y, preds))\n\n    balanced_accuracy = balanced_accuracy_score(y, preds)\n    precision_fc = precision_recall_fscore_support(y, preds)[0][0]\n    precision_tc = precision_recall_fscore_support(y, preds)[0][1]\n    recall_fc = precision_recall_fscore_support(y, preds)[1][0]\n    recall_tc = precision_recall_fscore_support(y, preds)[1][1]\n    fscore_fc = precision_recall_fscore_support(y, preds)[2][0]\n    fscore_tc = precision_recall_fscore_support(y, preds)[2][1]\n\n\n\n    all_results = {}\n    all_results['winner_model_name'] = [winner_model]\n    all_results['time_for_model_selection'] = [hms_string(time_took)]\n    all_results['balanced_accuracy'] = balanced_accuracy\n    all_results['precision_tc'] = precision_tc\n    all_results['recall_tc'] = recall_tc\n    all_results['fscore_tc'] = fscore_tc\n    all_results['precision_fc'] = precision_fc\n    all_results['recall_fc'] = recall_fc\n    all_results['fscore_fc'] = fscore_fc\n    \n    \n    all_results = pd.DataFrame.from_dict(all_results)\n    \n    return tables, test_results, all_results, winner_model\n\n\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-30T12:17:46.511473Z","iopub.execute_input":"2021-06-30T12:17:46.511874Z","iopub.status.idle":"2021-06-30T12:17:46.560052Z","shell.execute_reply.started":"2021-06-30T12:17:46.511834Z","shell.execute_reply":"2021-06-30T12:17:46.559269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tables, test_results, all_results =  hyp_optimization_model_selection(bundesliga_train,bundesliga_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Selection: \n\nAs described in the motivation of the project, to find out competitiveness levels, we prepared a percentage list from 0.3 to 0.9 increasing by 0.1 steps to cut the datasets from these levels and predict the target for each league. For example, 0.3 means we use the first 30% of the total matches during the season to predict the target. For all the leagues it took 13h39m to prepare final results, then saved as in csv format. Model prediction results based on Baseline model, Grid Search and Balanced Accuracy Score are represented. \n\n### Model pipelines for all datasets \n\n\n- We will research the prediction results of all leagues separately from each other to find out how competitive each league is.\n- Being able to predict the score ranking of any league at the end of the season at the beginning of the season will show us that the competition level of the league may be low from this point of view.\n- The prediction results of each model will be recorded in a table in order to be compared with the values obtained from the grid search and baseline model results.\n- Similarly, the accuracy rates of the successful models on the validation sets of the last season, which were separated in the previous steps, will also be recorded in this table.\n\n","metadata":{}},{"cell_type":"code","source":"#Create below lists to be able to iterate over \n\ncolumns = ['League','Percentage','winner_model_name','time_for_model_selection','time_for_percentage_evaluation','Baseline_model_accuracy','Gridsearch_model_accuracy','balanced_accuracy','precision_tc', 'recall_tc','fscore_tc','precision_fc','recall_fc','fscore_fc']\n\nleaderboard =[]\npercentages = list(np.round(np.arange(0.3,1.0,0.1),2))\n\nstart_time_=time.time()\nfor i in percentages:\n    all_dfs = prep_step_1(path_df,path_bundesliga,path_epl,path_laliga,path_mls)\n    labels = ['bundesliga','epl','laliga','mls']\n    for j, k  in zip(labels,all_dfs):\n        start_time=time.time()\n        print(f'{j} with {round(i*100)}% is processing!')\n        df_train, df_test = prep_step_2(k, percentage=i)\n        print(f'{j} baseline models started!')\n        tables_baseline = baseline_models(df_train)\n        print(f'{j} model selection with grid search started!')\n        tables, test_results, all_results, winner_model =  hyp_optimization_model_selection(df_train,df_test)\n        time_took = time.time() - start_time\n        print(f\"Total runtime for processing {j}: {hms_string(time_took)}\")\n        all_results['time_for_percentage_evaluation'] = hms_string(time_took)\n        all_results['League'] = j\n        all_results['Percentage'] = round(i*100)\n        all_results['Baseline_model_accuracy'] = tables_baseline[tables_baseline['model']==winner_model]['test'].values[0]\n        all_results['Gridsearch_model_accuracy'] =tables[tables['model']==winner_model]['test'].values[0]\n        leaderboard.append(all_results)\n        \ntime_took = time.time() - start_time_\nprint(f\"Total runtime for all iteration: {hms_string(time_took)}\")\n\nleaderboard = pd.concat(leaderboard)\nleaderboard = leaderboard[columns]\nleaderboard.to_csv('leaderboard.csv', index=0)\n","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leaderboard = pd.read_csv('../input/model-selection-outputs/leaderboard.csv');leaderboard","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:12.04384Z","iopub.execute_input":"2021-06-30T12:23:12.044305Z","iopub.status.idle":"2021-06-30T12:23:12.086829Z","shell.execute_reply.started":"2021-06-30T12:23:12.044265Z","shell.execute_reply":"2021-06-30T12:23:12.085841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Result evaluations\n\nAccording to the result table obtained, the following examinations were made:\n\n- **Winner Model Distributions**: Evaluation of the distribution of the selected models among the leagues, considering the percentages from which the model predictions were taken, since it gave the best results among all the algorithms tested. As shown in below graph Linear SVC chosen by gridsearch which is used as prediction models for all leagues except La Liga which mostly chose Logistic Regression and Neural Networks.\n\n\n- **Model Accuracy Results**: Evaluation of success rates of all model results, specific to leagues, according to certain percentages of the matches played.\n\n","metadata":{}},{"cell_type":"code","source":"all_dfs = [leaderboard[leaderboard['League']=='bundesliga'],\n          leaderboard[leaderboard['League']=='epl'],\n          leaderboard[leaderboard['League']=='laliga'],\n          leaderboard[leaderboard['League']=='mls'],]\nlabels = ['bundesliga','epl','laliga','mls']\n\nfig = plt.figure(figsize=(20,10))\nplt.suptitle('Winner Model Distributions', fontsize= 20,fontweight= 'bold')\nplt.subplots_adjust(wspace=0.1, hspace=0.3)\na = 2 \nb = 2 \nc = 1 \n\nfor t,z in enumerate(zip(labels,all_dfs)):\n    plt.subplot(a, b, c+t)\n    sns.histplot(binwidth=0.5, x=\"winner_model_name\", hue=\"Percentage\", data=z[1], stat=\"count\", multiple=\"stack\")\n    plt.title(z[0], fontdict={'fontsize': 15,\n        'fontweight': 'bold'})\n    #plt.axis('off')\n\nplt.show() \nfig.savefig('Winner.Model.Distributions.jpeg', dpi=350, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:33.966317Z","iopub.execute_input":"2021-06-30T12:23:33.966935Z","iopub.status.idle":"2021-06-30T12:23:36.547589Z","shell.execute_reply.started":"2021-06-30T12:23:33.966899Z","shell.execute_reply":"2021-06-30T12:23:36.546468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the Balanced Score chart in the first line of below graph, after being stable for weeks, we see that Major League Soccer (MLS) is more predictable as we approach the end of the season. Based on this observation, we can say that it maintains its competitiveness up to about 70% of the season. Of all the leagues, only the Bundesliga experienced a sharp decline in the middle of the league, while at the beginning of the league it was at a predictable level of competitiveness. On the contrary, the La Liga league’s predictability continues to increase up to a 60% of the season and then drops sharply, so we can conclude with its competitiveness level increases rapidly towards to the end of season. Premier League starts with a high predictability accuracy and continues with small ups and downs throughout the season, closing the season with an upward trend. Therefore, we can say its competitiveness level is low when compared to the others.","metadata":{}},{"cell_type":"code","source":"#Define a dataframe for Viz\n\nall_ = []\n\nfor j in list(leaderboard.League.unique()):\n    b_ = leaderboard[leaderboard['League']==j]\n    b_ = pd.DataFrame(b_.loc[:,'Baseline_model_accuracy':].T).reset_index().rename(columns={'index':'Accuracy_Type'})\n    b_.columns=['Accuracy_Type'] + list(leaderboard.Percentage.unique())\n\n    finals = []\n    for i in (b_.iloc[:,1:].columns):\n        temp = b_.copy()\n        temp = temp[['Accuracy_Type',i]]\n        temp['Percentage'] = i\n        temp.rename(columns={i:'Accuracy'}, inplace=True)\n        finals.append(temp)\n    finals = pd.concat(finals)  \n    finals['League'] = j\n    all_.append(finals)\n    \nall_ = pd.concat(all_)\nall_.replace('_model_accuracy','', inplace=True, regex=True)\nall_.replace('balanced_accuracy','Balanced', inplace=True, regex=True)\nall_.replace('_tc',' true class', inplace=True,regex=True)\nall_.replace('_fc',' false class', inplace=True, regex=True)\nall_.sort_values(by='Accuracy_Type', inplace=True)\nall_.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:36.629216Z","iopub.execute_input":"2021-06-30T12:23:36.629574Z","iopub.status.idle":"2021-06-30T12:23:36.722175Z","shell.execute_reply.started":"2021-06-30T12:23:36.629542Z","shell.execute_reply":"2021-06-30T12:23:36.721329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(all_[all_['Accuracy_Type'].isin(['Balanced','Baseline','Gridsearch'])], col=\"League\", row=\"Accuracy_Type\", margin_titles=True, despine=False)\ng.map_dataframe(sns.lineplot, x=\"Percentage\", y=\"Accuracy\")\ng.set_axis_labels(\"Percentage of Season\", \"Accuracy\")\ng.set_titles(row_template = '{row_name}', col_template = '{col_name}',fontsize=12,fontweight='bold',)\ng.fig.subplots_adjust(wspace=0, hspace=0)\ng.fig.suptitle( x=0.4, y=1.02, t=\"Model Accuracy Results\",\n                  fontsize=14,fontweight='bold',  ha='left')\ng.savefig('Model.Accuracy.Results.jpeg', dpi=350, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:37.641944Z","iopub.execute_input":"2021-06-30T12:23:37.642307Z","iopub.status.idle":"2021-06-30T12:23:40.595402Z","shell.execute_reply.started":"2021-06-30T12:23:37.642273Z","shell.execute_reply":"2021-06-30T12:23:40.594695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparison of classification accuracies are shown in below. In these graphs, we can observe to what extent the True Class and False Class performances have changed and are accurate in the predictions of each league. For example, the high predictability of the Premier League during the season, when both Precision and Recall are taken into account, confirms that this league has a low competitive value. On the contrary, the La Liga shows a value of 50 percent or less when these values are taken into account, showing that the teams that completed the season in the top 3 positions were unsuccessful in their predictions and therefore the competition is high.","metadata":{}},{"cell_type":"code","source":"g = sns.FacetGrid(all_[all_['Accuracy_Type'].isin([ 'fscore false class',\n 'fscore true class',\n 'precision false class',\n 'precision true class',\n 'recall false class',\n 'recall true class'])], col=\"League\", row=\"Accuracy_Type\", margin_titles=True, despine=False)\ng.map_dataframe(sns.lineplot, x=\"Percentage\", y=\"Accuracy\")\ng.set_axis_labels(\"Percentage of Season\", \"Accuracy\")\ng.set_titles(row_template = '{row_name}', col_template = '{col_name}',fontsize=12,fontweight='bold',)\ng.fig.subplots_adjust(wspace=0, hspace=0)\ng.fig.suptitle( x=0.4, y=1.02, t=\"Model Classification Results\",\n                  fontsize=14,fontweight='bold',  ha='left')\ng.savefig('Model.Classification.Results.jpeg', dpi=350, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:40.596893Z","iopub.execute_input":"2021-06-30T12:23:40.597152Z","iopub.status.idle":"2021-06-30T12:23:46.883594Z","shell.execute_reply.started":"2021-06-30T12:23:40.597114Z","shell.execute_reply":"2021-06-30T12:23:46.882723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Competitiveness Ranking\n\nConsidering the Balanced Accuracy Results, we score within each percentile, ranking from the lowest accuracy to the higher, with #1 corresponding to the lowest accuracy. \n\nFrom this point of view, we state that the competition rate of the league with low prrediction accuracy is higher than the league with high accuracy, and it ranks 1st.\n\n**Comments**:By ranking the accuracy results descending order we could observe how competitiveness distributed over the season. Although the prediction accuracies of La Liga were in an upward trend until the last quarter of the season, we can observe that when the results are compared with other leagues, it is the most difficult league to predict during the whole season. Therefore, we conclude that it is the league with the highest competition. The Premier League and Bundesliga are progressing by passing and lagging behind each other until the week when 60 percent of the games of the season are played. In this respect, we can say that both leagues share the second and third places. However, in the last quarter of the season, we observe that the competition rate of the Bundesliga league has increased sharply. Since Bundesliga has more downwards trend considering accuracy value, we can put it in the second order of the competitiveness. We observe that the MLS league, which has the least competition compared to the others until the 60 percent match of the season is played, although it increases its competition fiercely at some point, decreases rapidly after the 70 percent matches played. From this point of view, we can say that it has the lowest level of competition among others.","metadata":{}},{"cell_type":"code","source":"ranks = []\nfor i in list(leaderboard.Percentage.unique()):\n    temp = leaderboard.copy()\n    temp_ = temp[temp['Percentage']==i]\n    temp_['competitiveness_rank'] = temp_['Baseline_model_accuracy'].rank(ascending=False).astype(int)\n    ranks.append(temp_)\n    \nranks = pd.concat(ranks)\nranks = ranks.pivot(index='League',columns='Percentage', values='competitiveness_rank').reset_index()\nranks","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:46.884668Z","iopub.execute_input":"2021-06-30T12:23:46.885078Z","iopub.status.idle":"2021-06-30T12:23:46.91726Z","shell.execute_reply.started":"2021-06-30T12:23:46.885048Z","shell.execute_reply":"2021-06-30T12:23:46.916424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas.plotting import parallel_coordinates\nfig = plt.figure(figsize=(12,5))\nparallel_coordinates(ranks, 'League',colormap=plt.get_cmap(\"tab20b_r\") ) #color=sns.color_palette()  \nplt.title(\"Competitiveness levels of the Leagues\", fontsize=12, fontweight='bold')\nplt.xlabel(\"Played Percentage of the Season\")\nplt.show()\nfig.savefig('Competitiveness.levels.Leagues.jpeg', dpi=350, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:46.918965Z","iopub.execute_input":"2021-06-30T12:23:46.919223Z","iopub.status.idle":"2021-06-30T12:23:47.571741Z","shell.execute_reply.started":"2021-06-30T12:23:46.919199Z","shell.execute_reply":"2021-06-30T12:23:47.570618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Competitiveness Ranking in the Training Set\n\nIn order to evaluate the success rates of the models during grid search, we examine the Grid Search Model Accuracy results by ranking them with the same approach.","metadata":{}},{"cell_type":"code","source":"ranks = []\nfor i in list(leaderboard.Percentage.unique()):\n    temp = leaderboard.copy()\n    temp_ = temp[temp['Percentage']==i]\n    temp_['competitiveness_rank_for_train_set'] = temp_['Gridsearch_model_accuracy'].rank(ascending=False).astype(int)\n    ranks.append(temp_)\n    \nranks = pd.concat(ranks)\nranks = ranks.pivot(index='League',columns='Percentage', values='competitiveness_rank_for_train_set').reset_index()\nranks","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:47.573467Z","iopub.execute_input":"2021-06-30T12:23:47.573844Z","iopub.status.idle":"2021-06-30T12:23:47.608687Z","shell.execute_reply.started":"2021-06-30T12:23:47.573803Z","shell.execute_reply":"2021-06-30T12:23:47.607685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,5))\nparallel_coordinates(ranks, 'League',colormap=plt.get_cmap(\"tab20b_r\") ) #color=sns.color_palette()  \nplt.title(\"Competitiveness levels of the Leagues\", fontsize=12, fontweight='bold')\nplt.xlabel(\"Played Percentage of the Season\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T12:23:47.609946Z","iopub.execute_input":"2021-06-30T12:23:47.610323Z","iopub.status.idle":"2021-06-30T12:23:47.845304Z","shell.execute_reply.started":"2021-06-30T12:23:47.610284Z","shell.execute_reply":"2021-06-30T12:23:47.844284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions**: We have presented the competitiveness levels of the leagues by comparing several models applicable for binary target classification. As a result of our approach, it turned out that the **La Liga** league is the most competitive league compared to the others, maintaining the lowest balanced accuracy score until the end of the season. In addition to this work, further approximations can be made to this problem by incorporating various aspects of outcome over the course of the season, or different models can be applied, such as transforming the output into a multi-class classification problem to predict the position of each team. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}