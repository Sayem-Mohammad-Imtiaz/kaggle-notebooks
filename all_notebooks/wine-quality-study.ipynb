{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wine quality study<br>\n## Purpose of analysis\n- Understand the characteristics of the data by EDA.\n- Attempt to extract hard to understand information of data by cluster & Isomap analysis.\n- To predict the wine type and evaluation of quality, use 2way \"Logistic Regression\" and \"Decision Tree\"."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebraa\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Libraries\nimport os\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# For clustering\nfrom sklearn import cluster\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import Isomap\n\n# Data preprocessing library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Madhine learning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Evaluation library\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loadig"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/wine-quality/winequalityN.csv\", header=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data size\nprint(\"Data size:{}\".format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data info\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are null data in some columns. but the nuber is small."},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"type\" values\ndf[\"type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overview, Basic features\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling \"Null data\"\nNull data nuber is small from whole data. So, I decided to delete the null data by index."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"### Category counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wine type and Wine quality data count\nfig, ax = plt.subplots(1,2,figsize=(15,6))\n\n# Wine type\nsns.countplot(df[\"type\"], ax=ax[0])\nax[0].set_title(\"Wine type\")\n\n# Wine quality\nsns.countplot(df[\"quality\"], ax=ax[1])\nax[1].set_title(\"Wine type\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data pairplot by wine type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each wine type, plotting check of whole data, sample=1000.\nsns.pairplot(df.sample(n=1000, random_state=10), hue=\"type\", hue_order=['white', 'red'])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- \"white\" or \"red\" type distribution is split in some features.\n- For predict with Machine learning, they may need to drop outer data."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Data pairplot by wine quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each wine type, plotting check of whole data, sample=1000.\nsns.pairplot(df.sample(n=1000, random_state=10), hue=\"quality\", hue_order=[3,4,5,6,7,8,9])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- \"volatile acidity\", \"citric acid\", \"free sulfur dioxide\", \"density\", \"pH\", \"alcohol\" may be important to Wine quality.<br>\n### Next check the correlation with quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_values = df.iloc[:,1:]\n\n# Heatmap\nplt.figure(figsize=(15,15))\nhm = sns.heatmap(corr_values.corr(),\n                cbar=True,\n                annot=True,\n                square=True,\n                cmap=\"RdBu_r\",\n                fmt=\".2f\",\n                annot_kws={\"size\":10},\n                yticklabels=corr_values.columns,\n                vmax=1,\n                vmin=-1,\n                center=0)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"Variables\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- About quality, there is not clear correlation.\n- Relatively, 'free sulfur dioxide'&'total sulfur dioxide' have positive correlation, and 'density'&alcohol have a negative correlation."},{"metadata":{},"cell_type":"markdown","source":"### Cluster & Isomap analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cluster data set\ncluster_params = df.iloc[:,1:-1]\n\n# Standarized \nsc = StandardScaler()\nsc.fit(cluster_params)\nparams = sc.transform(cluster_params)\n\n# Create 5 clusters\nkmeans = cluster.KMeans(n_clusters=5, max_iter=30, init=\"random\", random_state=0)\nkmeans.fit(params)\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isomap, Compress to 2D.\niso = Isomap(n_components=2)\niso.fit(params)\ndata_projected = iso.transform(params)\ndata_projected.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2D plot by Wine quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,10))\nplt.scatter(data_projected[:,0], data_projected[:,1], c=df[\"quality\"], edgecolor='none', alpha=0.7, cmap=plt.cm.get_cmap('hsv', 6))\nplt.colorbar(label=\"quality\", ticks=range(6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- Around \"horizontal value=0\" & \"vertical value<-5\"is good score for quality like a trench.\n- There may be opotunity by 2 ways, the direction is from left upper side to there or right upper side to there."},{"metadata":{},"cell_type":"markdown","source":"## 2D plot Cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,10))\nplt.scatter(data_projected[:,0], data_projected[:,1], c=labels, edgecolor='none', alpha=0.8, cmap=plt.cm.get_cmap('nipy_spectral', 5))\nplt.colorbar(label=\"cluster\", ticks=range(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- Good quality cluster is cluster 1.\n- They can separate to right upper and left upper, next I confirm the 2 route.<br>\n1st route : cluster 2 ⇒ cluster 1 ⇒ cluster 4<br>\n2nd route : cluster 3 ⇒ cluster 0 ⇒ cluster 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Cluster\"] = labels\n\n# group by Cluster, confirm with mean value.\ndf.groupby(\"Cluster\").mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Analys of 1st route<br>\ncluster 2 ⇒ cluster 1 ⇒ cluster 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To make the dataframe\ndf_1st_route = df.groupby(\"Cluster\").mean().reset_index().query(\"Cluster==2 | Cluster==1 | Cluster==4\").sort_values(by=\"quality\")\n\n# sort index by cluster route\ndef cluster_route_flg(x):\n    if x[\"Cluster\"] == 2:\n        res=1\n    elif x[\"Cluster\"] == 1:\n        res=2\n    else:\n        res=3\n    return res\n\ndf_1st_route[\"Cluster_route\"] = df_1st_route.apply(cluster_route_flg, axis=1)\n\ndf_1st_route.reset_index(inplace=True)\ndf_1st_route.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'alcohol' can be a certain linear relationship."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing plot values\nx = df_1st_route.iloc[:,1:-1]\ny = df_1st_route[\"quality\"]\n\n# Visualization\nfig, ax = plt.subplots(1,5, figsize=(25,4))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\n# residual sugar\nax[0].plot(x[\"residual sugar\"], y, 'o',markersize=5)\nax[0].set_xlabel(\"residual sugar\")\nax[0].set_ylabel(\"quality\")\n\n# chlorides\nax[1].plot(x[\"chlorides\"], y, 'o',markersize=5)\nax[1].set_xlabel(\"chlorides\")\nax[1].set_ylabel(\"quality\")\n\n# free sulfur dioxide\nax[2].plot(x[\"free sulfur dioxide\"], y, 'o',markersize=5)\nax[2].set_xlabel(\"free sulfur dioxide\")\nax[2].set_ylabel(\"quality\")\n\n# total sulfur dioxide\nax[3].plot(x[\"total sulfur dioxide\"], y, 'o',markersize=5)\nax[3].set_xlabel(\"total sulfur dioxide\")\nax[3].set_ylabel(\"quality\")\n\n# alcohol\nax[4].plot(x[\"alcohol\"], y, 'o',markersize=5)\nax[4].set_xlabel(\"alcohol\")\nax[4].set_ylabel(\"quality\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"### Analys of 2nd route<br>\ncluster 3 ⇒ cluster 0 ⇒ cluster 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To make the dataframe\ndf_2nd_route = df.groupby(\"Cluster\").mean().reset_index().query(\"Cluster==3 | Cluster==0 | Cluster==4\")\n\n# sort index by cluster route\ndef cluster_route_flg(x):\n    if x[\"Cluster\"] == 3:\n        res=1\n    elif x[\"Cluster\"] == 0:\n        res=2\n    else:\n        res=3\n    return res\n\ndf_2nd_route[\"Cluster_route\"] = df_2nd_route.apply(cluster_route_flg, axis=1)\n\ndf_2nd_route.reset_index(inplace=True)\ndf_2nd_route.head()\n\ndf_2nd_route","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'volatile acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'alcohol' can be a certain linear relationship."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing plot values\nx = df_2nd_route.iloc[:,1:-1]\ny = df_2nd_route[\"quality\"]\n\n# Visualization\nfig, ax = plt.subplots(1,5, figsize=(25,4))\nplt.subplots_adjust(wspace=0.3, hspace=0.3)\n\n# volatile acidity\nax[0].scatter(x[\"volatile acidity\"], y, s=40)\nax[0].set_xlabel(\"volatile acidity\")\nax[0].set_ylabel(\"quality\")\n\n# residual sugar\nax[1].scatter(x[\"residual sugar\"], y, s=40)\nax[1].set_xlabel(\"residual sugar\")\nax[1].set_ylabel(\"quality\")\n\n# free sulfur dioxide\nax[2].scatter(x[\"free sulfur dioxide\"], y, s=40)\nax[2].set_xlabel(\"free sulfur dioxide\")\nax[2].set_ylabel(\"quality\")\n\n# total sulfur dioxide\nax[3].scatter(x[\"total sulfur dioxide\"], y, s=40)\nax[3].set_xlabel(\"total sulfur dioxide\")\nax[3].set_ylabel(\"quality\")\n\n# alcohol\nax[4].scatter(x[\"alcohol\"], y, s=40)\nax[4].set_xlabel(\"alcohol\")\nax[4].set_ylabel(\"quality\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- volatile acidity is different from 1st route.\n- It is interesting point that 'residual sugar' trend is reverse trend from 1st route.\n- This is suggest that the best cluster 1 is strong high quality cluster. And we can select if we add the specific feature, we can choose 2ways.\n*May be that wine type, white or red."},{"metadata":{},"cell_type":"markdown","source":"## Prediction of wine type by Logistic Regression & Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the flag of wine type\nmapping = {\"white\":0, \"red\":1}\ndf[\"type_flg\"] = df['type'].map(mapping)\ndf['type_flg'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data preparing\nX = df.iloc[:,1:12]\ny = df[\"type_flg\"]\n\n# Data splitting to make the training data and validation data\n# training data :70%, validation(test data) :30%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Taking veryfing to Standarlized data\nsc = StandardScaler()\nsc.fit(X_train)\n\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nlr = LogisticRegression()\n\nparam_range = [0.001, 0.01, 0.1, 1.0]\npenalty = ['l1', 'l2']\nparam_grid = [{\"C\":param_range, \"penalty\":penalty}]\n\ngs_lr = GridSearchCV(estimator=lr, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_lr = gs_lr.fit(X_train_std, y_train)\n\nprint(gs_lr.best_score_.round(3))\nprint(gs_lr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision tree\ntree = DecisionTreeClassifier(max_depth=4, random_state=10)\n\nparam_range = [3, 6, 9, 12]\nleaf = [10, 15, 20]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\ngs_tree = GridSearchCV(estimator=tree, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_tree = gs_tree.fit(X_train, y_train)\n\nprint(gs_tree.best_score_.round(3))\nprint(gs_tree.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*50)\n# Logistic Regression Result\ny_pred = gs_lr.best_estimator_.predict(X_test_std)\nprint(\"Logistic Regression Result\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# Decision tree\ny_pred = gs_tree.best_estimator_.predict(X_test)\nprint(\"Decision tree\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It was easy to identify the type of wine and it could be predicted with high accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Prediction of quality type by Logistic Regression & Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"To define binary classification, make the flag quality_flg.<br>\nHigh_quality : (flag=1, quality>=7)<br>\nLow_quality : (flag=1, quality<7)<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def quality_flag(x):\n    if x[\"quality\"] >= 7:\n        res = 1\n    else:\n        res = 0\n    return res\n\ndf[\"quality_flg\"] = df.apply(quality_flag, axis=1)\ndf[\"quality_flg\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"High quality data is fewer than Low quality data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data preparing\nX = df.iloc[:,1:12]\ny = df[\"quality_flg\"]\n\n# Data splitting to make the training data and validation data\n# training data :70%, validation(test data) :30%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Taking veryfing to Standarlized data\nsc = StandardScaler()\nsc.fit(X_train)\n\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nlr = LogisticRegression()\n\nparam_range = [0.001, 0.01, 0.1, 1.0]\npenalty = ['l1', 'l2']\nparam_grid = [{\"C\":param_range, \"penalty\":penalty}]\n\ngs_lr = GridSearchCV(estimator=lr, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_lr = gs_lr.fit(X_train_std, y_train)\n\nprint(gs_lr.best_score_.round(3))\nprint(gs_lr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision tree\ntree = DecisionTreeClassifier(max_depth=4, random_state=10)\n\nparam_range = [3, 6, 9, 12]\nleaf = [10, 15, 20]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\ngs_tree = GridSearchCV(estimator=tree, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_tree = gs_tree.fit(X_train, y_train)\n\nprint(gs_tree.best_score_.round(3))\nprint(gs_tree.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*50)\n# Logistic Regression Result\ny_pred = gs_lr.best_estimator_.predict(X_test_std)\nprint(\"Logistic Regression Result\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# Decision tree\ny_pred = gs_tree.best_estimator_.predict(X_test)\nprint(\"Decision tree\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It was more hard to identify the quality of wine than wine type.<br>\nRecall is low, I think it is from high quality data is few.<br>\n### So next, I tried \"over sampling method\" to recall sore up."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a RandomOverSampler\nros = RandomOverSampler(sampling_strategy = 'auto', random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the training data\nX_train_resampled, y_train_resampled = ros.fit_sample(X_train_std, y_train)\n\n# Logistic Regression\nlr = LogisticRegression()\n\nparam_range = [0.001, 0.01, 0.1, 1.0]\npenalty = ['l1', 'l2']\nparam_grid = [{\"C\":param_range, \"penalty\":penalty}]\n\ngs_lr = GridSearchCV(estimator=lr, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_lr = gs_lr.fit(X_train_resampled, y_train_resampled)\n\nprint(gs_lr.best_score_.round(3))\nprint(gs_lr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the training data\nX_train_resampled, y_train_resampled = ros.fit_sample(X_train, y_train)\n\n# Decision tree\ntree = DecisionTreeClassifier(max_depth=4, random_state=10)\n\nparam_range = [3, 6, 9, 12]\nleaf = [10, 15, 20]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\ngs_tree = GridSearchCV(estimator=tree, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_tree = gs_tree.fit(X_train_resampled, y_train_resampled)\n\nprint(gs_tree.best_score_.round(3))\nprint(gs_tree.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*50)\n# Logistic Regression Result\ny_pred = gs_lr.best_estimator_.predict(X_test_std)\nprint(\"Logistic Regression Result\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# Decision tree\ny_pred = gs_tree.best_estimator_.predict(X_test)\nprint(\"Decision tree\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall can be improved, but accuracy and precision is down.<br>\nBecause, there are trade off relation, so we decide that which is better in this situation. "},{"metadata":{},"cell_type":"markdown","source":"### Next try is separate the model, by wine type white or red."},{"metadata":{},"cell_type":"markdown","source":"I try for only red wine, and I checked."},{"metadata":{},"cell_type":"markdown","source":"### Red wine model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data preparing\nX = df.query(\"type=='red'\").iloc[:,1:12]\ny = df.query(\"type=='red'\")[\"quality_flg\"]\n\n# Data splitting to make the training data and validation data\n# training data :70%, validation(test data) :30%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Taking veryfing to Standarlized data\nsc = StandardScaler()\nsc.fit(X_train)\n\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nlr = LogisticRegression()\n\nparam_range = [0.001, 0.01, 0.1, 1.0]\npenalty = ['l1', 'l2']\nparam_grid = [{\"C\":param_range, \"penalty\":penalty}]\n\ngs_lr = GridSearchCV(estimator=lr, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_lr = gs_lr.fit(X_train_std, y_train)\n\nprint(gs_lr.best_score_.round(3))\nprint(gs_lr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision tree\ntree = DecisionTreeClassifier(max_depth=4, random_state=10)\n\nparam_range = [3, 6, 9, 12]\nleaf = [10, 15, 20]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\ngs_tree = GridSearchCV(estimator=tree, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_tree = gs_tree.fit(X_train, y_train)\n\nprint(gs_tree.best_score_.round(3))\nprint(gs_tree.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*50)\n# Logistic Regression Result\ny_pred = gs_lr.best_estimator_.predict(X_test_std)\nprint(\"Logistic Regression Result\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)\n\n# Decision tree\ny_pred = gs_tree.best_estimator_.predict(X_test)\nprint(\"Decision tree\")\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\nprint(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each model accuracy can be improved, precision and recall are improved,too.<br>\nIt can be a way for improving prediction accuracy. "},{"metadata":{},"cell_type":"markdown","source":"This Wine quality study ends here."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}