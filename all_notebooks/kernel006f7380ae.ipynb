{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Breast Cancer Detection from Tissue Cell Diagnostics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Breast Cancer datasets\nBCdata = pd.read_csv('../input/breastcancerdata.csv')\nBCdata.head(5).transpose()","execution_count":89,"outputs":[{"output_type":"execute_result","execution_count":89,"data":{"text/plain":"                                0         1    ...            3         4\nid                         842302    842517    ...     84348301  84358402\ndiagnosis                       M         M    ...            M         M\nradius_mean                 17.99     20.57    ...        11.42     20.29\ntexture_mean                10.38     17.77    ...        20.38     14.34\nperimeter_mean              122.8     132.9    ...        77.58     135.1\narea_mean                    1001      1326    ...        386.1      1297\nsmoothness_mean            0.1184   0.08474    ...       0.1425    0.1003\ncompactness_mean           0.2776   0.07864    ...       0.2839    0.1328\nconcavity_mean             0.3001    0.0869    ...       0.2414     0.198\nconcave points_mean        0.1471   0.07017    ...       0.1052    0.1043\nsymmetry_mean              0.2419    0.1812    ...       0.2597    0.1809\nfractal_dimension_mean    0.07871   0.05667    ...      0.09744   0.05883\nradius_se                   1.095    0.5435    ...       0.4956    0.7572\ntexture_se                 0.9053    0.7339    ...        1.156    0.7813\nperimeter_se                8.589     3.398    ...        3.445     5.438\narea_se                     153.4     74.08    ...        27.23     94.44\nsmoothness_se            0.006399  0.005225    ...      0.00911   0.01149\ncompactness_se            0.04904   0.01308    ...      0.07458   0.02461\nconcavity_se              0.05373    0.0186    ...      0.05661   0.05688\nconcave points_se         0.01587    0.0134    ...      0.01867   0.01885\nsymmetry_se               0.03003   0.01389    ...      0.05963   0.01756\nfractal_dimension_se     0.006193  0.003532    ...     0.009208  0.005115\nradius_worst                25.38     24.99    ...        14.91     22.54\ntexture_worst               17.33     23.41    ...         26.5     16.67\nperimeter_worst             184.6     158.8    ...        98.87     152.2\narea_worst                   2019      1956    ...        567.7      1575\nsmoothness_worst           0.1622    0.1238    ...       0.2098    0.1374\ncompactness_worst          0.6656    0.1866    ...       0.8663     0.205\nconcavity_worst            0.7119    0.2416    ...       0.6869       0.4\nconcave points_worst       0.2654     0.186    ...       0.2575    0.1625\nsymmetry_worst             0.4601     0.275    ...       0.6638    0.2364\nfractal_dimension_worst    0.1189   0.08902    ...        0.173   0.07678\nUnnamed: 32                   NaN       NaN    ...          NaN       NaN\n\n[33 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>842302</td>\n      <td>842517</td>\n      <td>84300903</td>\n      <td>84348301</td>\n      <td>84358402</td>\n    </tr>\n    <tr>\n      <th>diagnosis</th>\n      <td>M</td>\n      <td>M</td>\n      <td>M</td>\n      <td>M</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>radius_mean</th>\n      <td>17.99</td>\n      <td>20.57</td>\n      <td>19.69</td>\n      <td>11.42</td>\n      <td>20.29</td>\n    </tr>\n    <tr>\n      <th>texture_mean</th>\n      <td>10.38</td>\n      <td>17.77</td>\n      <td>21.25</td>\n      <td>20.38</td>\n      <td>14.34</td>\n    </tr>\n    <tr>\n      <th>perimeter_mean</th>\n      <td>122.8</td>\n      <td>132.9</td>\n      <td>130</td>\n      <td>77.58</td>\n      <td>135.1</td>\n    </tr>\n    <tr>\n      <th>area_mean</th>\n      <td>1001</td>\n      <td>1326</td>\n      <td>1203</td>\n      <td>386.1</td>\n      <td>1297</td>\n    </tr>\n    <tr>\n      <th>smoothness_mean</th>\n      <td>0.1184</td>\n      <td>0.08474</td>\n      <td>0.1096</td>\n      <td>0.1425</td>\n      <td>0.1003</td>\n    </tr>\n    <tr>\n      <th>compactness_mean</th>\n      <td>0.2776</td>\n      <td>0.07864</td>\n      <td>0.1599</td>\n      <td>0.2839</td>\n      <td>0.1328</td>\n    </tr>\n    <tr>\n      <th>concavity_mean</th>\n      <td>0.3001</td>\n      <td>0.0869</td>\n      <td>0.1974</td>\n      <td>0.2414</td>\n      <td>0.198</td>\n    </tr>\n    <tr>\n      <th>concave points_mean</th>\n      <td>0.1471</td>\n      <td>0.07017</td>\n      <td>0.1279</td>\n      <td>0.1052</td>\n      <td>0.1043</td>\n    </tr>\n    <tr>\n      <th>symmetry_mean</th>\n      <td>0.2419</td>\n      <td>0.1812</td>\n      <td>0.2069</td>\n      <td>0.2597</td>\n      <td>0.1809</td>\n    </tr>\n    <tr>\n      <th>fractal_dimension_mean</th>\n      <td>0.07871</td>\n      <td>0.05667</td>\n      <td>0.05999</td>\n      <td>0.09744</td>\n      <td>0.05883</td>\n    </tr>\n    <tr>\n      <th>radius_se</th>\n      <td>1.095</td>\n      <td>0.5435</td>\n      <td>0.7456</td>\n      <td>0.4956</td>\n      <td>0.7572</td>\n    </tr>\n    <tr>\n      <th>texture_se</th>\n      <td>0.9053</td>\n      <td>0.7339</td>\n      <td>0.7869</td>\n      <td>1.156</td>\n      <td>0.7813</td>\n    </tr>\n    <tr>\n      <th>perimeter_se</th>\n      <td>8.589</td>\n      <td>3.398</td>\n      <td>4.585</td>\n      <td>3.445</td>\n      <td>5.438</td>\n    </tr>\n    <tr>\n      <th>area_se</th>\n      <td>153.4</td>\n      <td>74.08</td>\n      <td>94.03</td>\n      <td>27.23</td>\n      <td>94.44</td>\n    </tr>\n    <tr>\n      <th>smoothness_se</th>\n      <td>0.006399</td>\n      <td>0.005225</td>\n      <td>0.00615</td>\n      <td>0.00911</td>\n      <td>0.01149</td>\n    </tr>\n    <tr>\n      <th>compactness_se</th>\n      <td>0.04904</td>\n      <td>0.01308</td>\n      <td>0.04006</td>\n      <td>0.07458</td>\n      <td>0.02461</td>\n    </tr>\n    <tr>\n      <th>concavity_se</th>\n      <td>0.05373</td>\n      <td>0.0186</td>\n      <td>0.03832</td>\n      <td>0.05661</td>\n      <td>0.05688</td>\n    </tr>\n    <tr>\n      <th>concave points_se</th>\n      <td>0.01587</td>\n      <td>0.0134</td>\n      <td>0.02058</td>\n      <td>0.01867</td>\n      <td>0.01885</td>\n    </tr>\n    <tr>\n      <th>symmetry_se</th>\n      <td>0.03003</td>\n      <td>0.01389</td>\n      <td>0.0225</td>\n      <td>0.05963</td>\n      <td>0.01756</td>\n    </tr>\n    <tr>\n      <th>fractal_dimension_se</th>\n      <td>0.006193</td>\n      <td>0.003532</td>\n      <td>0.004571</td>\n      <td>0.009208</td>\n      <td>0.005115</td>\n    </tr>\n    <tr>\n      <th>radius_worst</th>\n      <td>25.38</td>\n      <td>24.99</td>\n      <td>23.57</td>\n      <td>14.91</td>\n      <td>22.54</td>\n    </tr>\n    <tr>\n      <th>texture_worst</th>\n      <td>17.33</td>\n      <td>23.41</td>\n      <td>25.53</td>\n      <td>26.5</td>\n      <td>16.67</td>\n    </tr>\n    <tr>\n      <th>perimeter_worst</th>\n      <td>184.6</td>\n      <td>158.8</td>\n      <td>152.5</td>\n      <td>98.87</td>\n      <td>152.2</td>\n    </tr>\n    <tr>\n      <th>area_worst</th>\n      <td>2019</td>\n      <td>1956</td>\n      <td>1709</td>\n      <td>567.7</td>\n      <td>1575</td>\n    </tr>\n    <tr>\n      <th>smoothness_worst</th>\n      <td>0.1622</td>\n      <td>0.1238</td>\n      <td>0.1444</td>\n      <td>0.2098</td>\n      <td>0.1374</td>\n    </tr>\n    <tr>\n      <th>compactness_worst</th>\n      <td>0.6656</td>\n      <td>0.1866</td>\n      <td>0.4245</td>\n      <td>0.8663</td>\n      <td>0.205</td>\n    </tr>\n    <tr>\n      <th>concavity_worst</th>\n      <td>0.7119</td>\n      <td>0.2416</td>\n      <td>0.4504</td>\n      <td>0.6869</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>concave points_worst</th>\n      <td>0.2654</td>\n      <td>0.186</td>\n      <td>0.243</td>\n      <td>0.2575</td>\n      <td>0.1625</td>\n    </tr>\n    <tr>\n      <th>symmetry_worst</th>\n      <td>0.4601</td>\n      <td>0.275</td>\n      <td>0.3613</td>\n      <td>0.6638</td>\n      <td>0.2364</td>\n    </tr>\n    <tr>\n      <th>fractal_dimension_worst</th>\n      <td>0.1189</td>\n      <td>0.08902</td>\n      <td>0.08758</td>\n      <td>0.173</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>Unnamed: 32</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's check the dimensions of the dataframe\nprint(BCdata.shape)\n# Let's see the type of each column\nprint(BCdata.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# summarising number of missing values in each column\nBCdata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# summarising number of missing values in each row\nBCdata.isnull().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#checking for redundant duplicate rows\nprint(sum(BCdata.duplicated()))\n#Dropping Duplicate Rows\nBCdata.drop_duplicates(keep=False,inplace=True)\nprint(sum(BCdata.duplicated()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#dropping columns having null value \"Unnamed:32\"\nBCdata.drop(['Unnamed: 32'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# let's look at the outliers for numeric features in dataframe\nBCdata.describe(percentiles=[.25,.5,.75,.90,.95,.99]).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# correlation matrix\ncor = BCdata.corr()\ncor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plotting correlations on a heatmap post outlier treatment\n# figure size\nplt.figure(figsize=(20,15))\n# heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#pairplots for numerical data frames\nplt.figure(figsize=(20,12))\nsns.pairplot(BCdata)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# List of binary variables with M/B values using map converting these to 1/0\nvarlist =  ['diagnosis']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'M': 1, 'B': 0})\n\n# Applying the function to the leads score list\nBCdata[varlist] = BCdata[varlist].apply(binary_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Putting feature variables to X by first dropping y (Attrition) from HRdata\nX = BCdata.drop(['diagnosis'], axis=1)\n# Putting response variable to y\ny = BCdata['diagnosis']\nprint(y.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']] = scaler.fit_transform(X_train[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']])\n\n#verifying the scaled data in X_train dataframe\nX_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### Before we build the Logistic regression model, we need to know how much percent of Diagnosis as Malign is seen in the original data\n### Calculating the Diagnosis Rate\nDiagnosisRate = round((sum(BCdata['diagnosis'])/len(BCdata['diagnosis'].index))*100,2)\nDiagnosisRate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\n# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg,10)             # running RFE with 20 variables as output\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfe.support_\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"col = X_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#### Check for the VIF values of the feature variables.\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('radius_worst',1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#,'perimeter_mean','perimeter_worst','area_mean','area_worst','radius_se','perimeter_se'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('perimeter_worst',1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('area_se',1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('concave points_worst',1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Diagnosis':y_train.values, 'Diagnosis_Probability':y_train_pred})\ny_train_pred_final['PatientID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final.Diagnosis_Probability.map(lambda x: 1 if x > 0.8 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final.predicted )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Diagnosis, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity is:\")\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us calculate specificity\nprint(\"Specificity is:\")\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Calculate false postive rate - predicting Conversion when customer does not Convert\nprint(\"False Positive Rate is:\")\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# positive predictive value \nprint(\"Positive Predictive value is:\")\nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Negative predictive value\nprint(\"Negative Predictive value is:\")\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Probability, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"draw_roc(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Probability)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Diagnosis_Probability.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Diagnosis_Probability.map( lambda x: 1 if x > 0.35 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Diagnosis, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us calculate specificity - High specificity indicates the model can identify those who will not have attrition will have a negative test result.\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Calculate false postive rate - predicting Attrition when Employee is not Attrition\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Precision\nconfusion[1,1]/(confusion[0,1]+confusion[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Recall\nconfusion[1,1]/(confusion[1,0]+confusion[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Using sklearn to calculate above\nfrom sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_pred_final.Diagnosis, y_train_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recall_score(y_train_pred_final.Diagnosis, y_train_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\ny_train_pred_final.Diagnosis, y_train_pred_final.predicted\np, r, thresholds = precision_recall_curve(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Probability)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_test[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']] = scaler.fit_transform(X_test[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']])\n\nX_test = X_test[col]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)\n# Making predictions on the test set\ny_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\n# Let's see the head\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Putting LeadID to index\ny_test_df['PatientID'] = y_test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Diagnosis_Probability'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex_axis(['PatientID','diagnosis','Diagnosis_Probability'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of y_pred_final\ny_pred_final","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}