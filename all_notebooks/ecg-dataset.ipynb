{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\ntest_df  = pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv', header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[187] = train_df[187].astype(int)\n\ncategories_counts = train_df[187].value_counts()\nprint(categories_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(5,5))\nplt.pie(categories_counts, labels=['n','q','v','s','f'], colors=['red','green','blue','skyblue','orange'], autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\ndf_0 = train_df[train_df[187]==0]\ndf_1 = train_df[train_df[187]==1]\ndf_2 = train_df[train_df[187]==2]\ndf_3 = train_df[train_df[187]==3]\ndf_4 = train_df[train_df[187]==4]\n\ndf_0_downsample = resample(df_0,replace=True,n_samples=5000,random_state=122)\ndf_1_upsample   = resample(df_1,replace=True,n_samples=5000,random_state=123)\ndf_2_upsample   = resample(df_2,replace=True,n_samples=5000,random_state=124)\ndf_3_upsample   = resample(df_3,replace=True,n_samples=5000,random_state=125)\ndf_4_upsample   = resample(df_4,replace=True,n_samples=5000,random_state=126)\n\ntrain_df=pd.concat([df_0_downsample,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\n\nplt.subplot(2,2,1)\nplt.plot(df_0.iloc[100,:186])\n\nplt.subplot(2,2,2)\nplt.plot(df_1.iloc[100,:186])\n\nplt.subplot(2,2,3)\nplt.plot(df_2.iloc[100,:186])\n\nplt.subplot(2,2,4)\nplt.plot(df_3.iloc[100,:186])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical\n\ntarget_train = train_df[187]\ntarget_test  = test_df[187]\ny_train = to_categorical(target_train)\ny_test  = to_categorical(target_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.iloc[:,:186].values\nX_test  = test_df.iloc[:,:186].values\n\nX_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\nX_test  = X_test.reshape(len(X_test), X_test.shape[1], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.GaussianNoise(0.01, input_shape=(X_train.shape[1], X_train.shape[2])))\n\nmodel.add(layers.Conv1D(64, 16, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=4, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(64, 12, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=3, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(64, 8, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=2, strides=2, padding=\"same\"))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(5, activation='softmax'))\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\ncallbacks = [EarlyStopping(monitor='val_loss', patience=8)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, callbacks=[], validation_data=(X_test, y_test), epochs = 30, batch_size = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nepochs = range(1, len(acc)+1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nimport seaborn as sns\n\ny_pred = model.predict_classes(X_test)\n\ny_test_category = y_test.argmax(axis=-1)\n\n# Creates a confusion matrix\ncm = confusion_matrix(y_test_category, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index   = ['N', 'S', 'V', 'F', 'Q'], \n                     columns = ['N', 'S', 'V', 'F', 'Q'])\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm_df, annot=True, fmt=\"d\", linewidths=0.5, cmap='Blues', cbar=False, annot_kws={'size':14}, square=True)\nplt.title('Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_category, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_category, y_pred, target_names=['N', 'S', 'V', 'F', 'Q']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import notebook\n\ndef prep_data(data_frame):\n    df_list = []\n    label_list=[]\n    for k in notebook.tqdm(range(len(data_frame))):\n    # for k in range(2):\n        df = pd.DataFrame(data_frame.iloc[k,:186])\n        df.columns = [\"values\"]\n        df['id'] = k\n        df['time'] = df.index\n        df_list.append(df)\n        \n        df_label = pd.DataFrame(columns=['id', 'values'], data=[[k, data_frame.iloc[k,187].astype(int)]])\n        label_list.append(df_label)\n\n    df = pd.concat(df_list, ignore_index = True, sort = False)\n    df_label = pd.concat(label_list, ignore_index = True, sort = False)\n    \n    return df, df_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,y_train = prep_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test,y_test = prep_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tsfresh import extract_features\nfrom tsfresh import extract_relevant_features\nfrom tsfresh.feature_extraction.settings import from_columns\nfrom tsfresh.feature_extraction.settings import ComprehensiveFCParameters, MinimalFCParameters, EfficientFCParameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ComprehensiveFCParameters()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extraction_settings = dict({'median': None, 'mean': None, 'standard_deviation': None, 'variance': None, 'abs_energy': None, 'skewness': None, 'kurtosis': None, 'sample_entropy': None,  \n                            'spkt_welch_density': [{'coeff': 2}, {'coeff': 5}, {'coeff': 8}],\n                            'time_reversal_asymmetry_statistic': [{'lag': 1}, {'lag': 2}, {'lag': 3}],\n                            'fft_aggregated': [{'aggtype': 'centroid'},{'aggtype': 'variance'},{'aggtype': 'skew'},{'aggtype': 'kurtosis'}]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_features = extract_features(X_train, column_id='id', column_sort='time', default_fc_parameters=extraction_settings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_features = extract_features(X_test, column_id='id', column_sort='time', default_fc_parameters=extraction_settings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nmin_max_scaler   = preprocessing.MinMaxScaler()\nX_train_features = min_max_scaler.fit_transform(X_train_features)\nX_test_features  = min_max_scaler.transform(X_test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.GaussianNoise(0.01, input_shape=(X_train_features.shape[1],)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(5, activation='softmax'))\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train_features, to_categorical(y_train['values']), callbacks=[], validation_data=(X_test_features, to_categorical(y_test['values'])), epochs = 40, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nimport seaborn as sns\n\ny_pred = model.predict_classes(X_test_features)\n\ny_test_category = y_test['values']  \n\n# Creates a confusion matrix\ncm = confusion_matrix(y_test_category, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index   = ['N', 'S', 'V', 'F', 'Q'], \n                     columns = ['N', 'S', 'V', 'F', 'Q'])\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm_df, annot=True, fmt=\"d\", linewidths=0.5, cmap='Blues', cbar=False, annot_kws={'size':14}, square=True)\nplt.title('Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_category, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}