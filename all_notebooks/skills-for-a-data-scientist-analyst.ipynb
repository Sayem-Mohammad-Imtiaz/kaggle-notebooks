{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Job Skills - Building a (Network) Graph\n\n## This notebook will seek primarily to analyze the job description field of this dataset.  \n\nI want to thank [Kenza's article](https://medium.com/swlh/want-to-become-a-data-analyst-scientist-or-engineer-66902875bf9b) for providing the keywords that made this possible.  Her article also does a much better job than I of describing the dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"jobs = pd.read_csv('/kaggle/input/data-jobs/all_jobs.csv')\nskills = pd.read_csv('/kaggle/input/data-skills/skills_taxonomy.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport matplotlib.pyplot as plt\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.drop('Unnamed: 0', axis = 1, inplace = True)\njobs = jobs.drop_duplicates(subset = ['Job Description','Job Title','Location'], keep = 'first') \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The data was already pretty clean, thanks [picklesueat](https://github.com/picklesueat/data_jobs_data)! Now we can begin to parse the Job Descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's remove Capitals\njobs['Job Description'] = jobs['Job Description'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's remove all non-word charachters\n\nregex = re.compile('[^a-zA-Z\\']')\n\njobs['Job Description'] = jobs['Job Description'].apply(lambda x: regex.sub(' ', x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Equal Oppritunity tagline may skew our results, let's remove it\nequal_emp = 'Kelly is an equal opportunity employer committed to employing a diverse workforce, including, but not limited to, minorities, females, individuals with disabilities, protected veterans, sexual orientation, gender identity. Equal Employment Opportunity is The Law.'\nequal_emp = equal_emp.lower().split(' ')\n\njobs['Job Description'] = jobs['Job Description'].apply(lambda x: [item for item in x.split() if item.lower() not in equal_emp])\n\n#and then re-join our Job Descriptions\njobs['Job Description'] = jobs['Job Description'].apply(lambda x: ' '.join(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## There are way too many skills to analyze so we're going to group them by some arbitrary (but hopefully accurate) categories I came up with. "},{"metadata":{"trusted":true},"cell_type":"code","source":"skill_types= {}\n\nskill_types['Statistics'] = ['matlab',\n 'statistical',\n 'models',\n 'modeling',\n 'statistics',\n 'analytics',\n 'forecasting',\n 'predictive',\n 'r',\n 'pandas',\n 'statistics',\n 'statistical',\n 'Julia']\n\nskill_types['Machine Learning'] = ['datarobot',\n 'tensorflow',\n 'knime',\n 'rapidminer',\n 'mahout',\n 'logicalglue',\n 'nltk',\n 'networkx',\n 'rapidminer',\n 'scikit',\n 'pytorch',\n 'keras',\n 'caffe',\n 'weka',\n 'orange',\n 'qubole',\n 'ai',\n 'nlp',\n 'ml',\n 'neuralnetworks',\n 'deeplearning']\n\n\nskill_types['Data Visualization'] = ['tableau',\n 'powerpoint',\n 'Qlik',\n 'looker',\n 'powerbi',\n 'matplotlib',\n 'tibco',\n 'bokeh',\n 'd3',\n 'octave',\n 'shiny',\n 'microstrategy']\n\n\nskill_types['Data Engineering'] = ['etl',\n 'mining',\n 'warehousing',\n 'cloud',\n 'sap',\n 'salesforce',\n 'openrefine',\n 'redis',\n 'sybase',\n 'cassandra',\n 'msaccess',\n 'databasemanagement',\n 'aws',\n 'ibmcloud',\n 'azure',\n 'redshift',\n 's3',\n 'ec2',\n 'rds',\n 'bigquery',\n 'googlecloudplatform',\n 'googlecloudplatform',\n 'hadoop',\n 'hive',\n 'kafka',\n 'hbase',\n 'mesos',\n 'pig',\n 'storm',\n 'scala',\n 'hdfs',\n 'mapreduce',\n 'kinesis',\n 'flink']\n\n\nskill_types['Software Engineer'] = ['java',\n 'javascript',\n 'c#',\n 'c',\n 'docker',\n 'ansible',\n 'jenkins',\n 'nodejs',\n 'angularjs',\n 'css',\n 'html',\n 'terraform',\n 'kubernetes',\n 'lex',\n 'perl',\n 'cplusplus']\n\n\nskill_types['SQL'] = ['sql',\n 'oracle',\n 'mysql',\n 'oraclenosql',\n 'nosql',\n 'postgresql',\n 'plsql',\n 'mongodb']\n\n\n\n\nskill_types['Trait Skills'] = ['Learning',\n 'TimeManagement',\n 'AttentiontoDetail',\n 'ProblemSolving',\n 'criticalthinking']\n\n\n\nskill_types['Social Skills']= ['teamwork',\n 'team'\n 'communication',\n 'written',\n 'verbal',\n 'writing',\n 'leadership',\n 'interpersonal',\n 'personalmotivation',\n 'storytelling']\n\nskill_types['Business'] = ['excel',\n 'bi',\n 'reporting',\n 'reports',\n 'dashboards',\n 'dashboard',\n 'businessintelligence'\n 'business']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For some reason some of the dictionary values are uppercase so we have to correct that"},{"metadata":{"trusted":true},"cell_type":"code","source":"for k,v in skill_types.items():\n    skill_types[k] = [skill.lower() for skill in skill_types.get(k)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Refined Job descriptions\n\n## Now that we have our extensive list, we need to use it to modify our Job Descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def refiner(desc):\n    desc = desc.split()\n    \n    two_word = ''\n    \n    newskills = []\n    \n    for word in desc:\n        two_word = two_word + word \n        for key,value in skill_types.items():\n            if((word in value) or (two_word in value)):\n                newskills.append(key)\n                \n        #check for the two worders, like 'businessintelligence'        \n        two_word = word\n                \n    return list(set(newskills))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now all we have to do is apply this do our Job Description\njobs['refined skills'] = jobs['Job Description'].apply(refiner)\njobs['refined skills']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Great! Everything is ready, now we have our skills for each job description\n\n# Apriori Algorithm - Support"},{"metadata":{"trusted":true},"cell_type":"code","source":"def apriori_df(series, min_support):\n    lisolis =[]\n    series.apply(lambda x: lisolis.append(list(x)))\n    \n    from mlxtend.preprocessing import TransactionEncoder\n\n    te = TransactionEncoder()\n    te_ary = te.fit(lisolis).transform(lisolis)\n    df = pd.DataFrame(te_ary, columns=te.columns_)\n\n\n    from mlxtend.frequent_patterns import apriori\n\n    freq_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n    \n    return freq_itemsets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_itemsets = apriori_df(jobs['refined skills'],.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\n_ = frequent_itemsets[frequent_itemsets['length'] == 1]\n_['itemsets'] = _['itemsets'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\nax = sns.barplot(x=\"itemsets\", y=\"support\", data= _);\nax.set_xticklabels(ax.get_xticklabels(), rotation=75);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Although this tells us a lot, let's break this down by job, and try again\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobtypes = ['Data Analyst','Business Analyst','Data Scientist']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If it contains Data and Analyst we will classify that as Data Analyst\ndata_analyst = jobs[(jobs['Job Title'].str.contains('[Aa]nalyst', flags=re.IGNORECASE)) & (jobs['Job Title'].str.contains('[dD]ata ', flags=re.IGNORECASE))]\ndata_analyst['jobtype'] = jobtypes[0]\ntemp = temp.append(data_analyst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"business_analyst = jobs[(jobs['Job Title'].str.contains('[Aa]nalyst|[Ii]ntelligence|BI', regex = True, flags=re.IGNORECASE)) & (jobs['Job Title'].str.contains('Business |BI', flags=re.IGNORECASE))]\nbusiness_analyst['jobtype'] = jobtypes[1]\ntemp = temp.append(business_analyst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_scientist = jobs[(jobs['Job Title'].str.contains('[Ss]cientist|Science', regex = True, flags=re.IGNORECASE)) & (jobs['Job Title'].str.contains('[dD]ata ', flags=re.IGNORECASE))]\ndata_scientist['jobtype'] = jobtypes[2]\ntemp = temp.append(data_scientist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs = temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs.reset_index(inplace = True)\njobs.drop(['index'], axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Re-run apriori and put up chart 2.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We want to retain the categories we just made, run apriori, and then recombine them\nfrequent_itemsets = pd.DataFrame()\n\n\nfor job in jobtypes:\n    temp_frequent_itemsets = apriori_df(jobs.loc[jobs['jobtype'] == job,'refined skills'],.02)\n    temp_frequent_itemsets['jobtype'] = job\n    frequent_itemsets = frequent_itemsets.append(temp_frequent_itemsets)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_itemsets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_items = frequent_itemsets[frequent_itemsets['length'] == 1]\nsingle_items['itemsets'] = single_items['itemsets'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\nsingle_items['itemsets'] = single_items['itemsets'].str.replace(' ','\\n', regex = True)\nplt.figure(figsize =(25, 20)) \n\nplt.title('Chance of a skill appearing in a given Job Description', size = 35)\nax = sns.barplot(x=\"itemsets\", y=\"support\", hue = 'jobtype', data= single_items);\nax.set_xticklabels(ax.get_xticklabels(), rotation=25, size =20);\nax.set_yticklabels([0,0.2,0.4,0.6,0.8], size =25);\nplt.xlabel('Skill', size = 20)\nax.legend(fontsize = 20, frameon= False)\nplt.ylabel('Probability', size = 22)\nplt.savefig('chart1.png', dpi = 500, format = 'png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's note a few things\n\n*The support tells you the probability of that item (which actually represents a group of items, see above) is seen in a Job Description*\n\n\n### **There are a lot of things we would expect**\n* ML has a huge role in DS, but not in DA\n* The primary skills for BA are Business and Social Skills\n* The highest Data Visualization is seen in DA's\n* The highest SWE is seen in DS\n\n### **A few interesting things**\n* The difference in Data Visualization across jobs is small compared to other itemsets across jobs\n* Statistics knowledge is hugely correlated to job type \n* Trait Skills are much more important in DS jobs, where's social skills are slightly (slightly) less important\n* DA's use business knowledge more than BA or DS\n\n### Conclusions\nThat's up to you, although this chart clearly shows the difference between BA,DA,DS is not just semantic.  I'd rather work on getting more prescriptive advice. \n\n\n## Now I hear you saying, 'what a beautiful chart, it couldn't get any better', but there is one (or more) problems with the above chart:\nLet's say you want to be a DA so you work on your Data Viz, and your SQL skills since they are some of the highest at 40% and 60% respectivly, but then you go to apply to jobs and 40% of the roles contain Data Viz, and 60% SQL, but there is no overlap between which jobs contain each!!  SQL and Data Viz are only listed separately, and therefore you're not qualified for any of the jobs.\nWe need a way to determine the strength of the connections between these items, and then also a  way to visualize that...\n"},{"metadata":{},"cell_type":"markdown","source":"# Apriori Metrics: Conviction\n\nI'm just going to get the metric, and visualize it first  \n\nLet's start with our Goldilocks, the DA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"typ = 'Data Analyst'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rules = association_rules(apriori_df(jobs.loc[jobs['jobtype'] == typ,'refined skills'],.2), metric=\"conviction\", min_threshold=1.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 to 1 \nrules['alength'] = rules['antecedents'].apply(lambda x: len(x))\nrules['clength'] = rules['consequents'].apply(lambda x: len(x))\nrules = rules[(rules['alength'] == 1) & (rules['clength'] == 1)]\nrules['antecedents'] = rules['antecedents'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\nrules['antecedents'] = rules['antecedents'].str.replace(' ','\\n', regex = True)\nrules['consequents'] = rules['consequents'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\nrules['consequents'] = rules['consequents'].str.replace(' ','\\n', regex = True)\nrules","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rules.sort_values(by = 'conviction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make some edges, now that we've run apriori\nweighted_edges = []\n\nfor x in range(len(rules)):\n    weighted_edges.append((rules.iloc[x,0], rules.iloc[x,1], rules.iloc[x,8]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G = nx.DiGraph() \nG.add_weighted_edges_from(weighted_edges)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change node size according to support\nfor i in list(G.nodes()): \n    G.nodes[i]['support'] = single_items.loc[(single_items['jobtype'] == typ) & (single_items['itemsets'] == i), 'support'].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"node_size = [50000*nx.get_node_attributes(G, 'support')[v] for v in G] \nedge_width = [(G[u][v]['weight']- 1)*12 if((G[u][v]['weight']- 1) > .4) else 0 for u, v in G.edges() ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = nx.circular_layout(G)\nplt.figure(figsize=(20,20))\n\npos_shadow = copy.deepcopy(pos)\nshift_amount = 0.008\nfor idx in pos_shadow:\n    pos_shadow[idx][0] += shift_amount\n    pos_shadow[idx][1] -= shift_amount\n\nnx.draw_networkx_nodes(G, pos_shadow, node_color='k', alpha=0.3,  node_size = node_size)    \n    \nnx.draw_networkx_nodes(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, width = edge_width)\n\nnx.draw_networkx_labels(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, size = 25, width = edge_width)\n\nnx.draw_networkx_edges(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, width = edge_width)\nplt.title('Data Analyst Skills', size = 50)\nplt.axis('off')\nplt.plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_network_graph(jobtype, min_conviction):\n    rules = association_rules(apriori_df(jobs.loc[jobs['jobtype'] == jobtype,'refined skills'],.15), metric=\"conviction\", min_threshold=.5)\n    # 1 to 1 \n    rules['alength'] = rules['antecedents'].apply(lambda x: len(x))\n    rules['clength'] = rules['consequents'].apply(lambda x: len(x))\n    rules = rules[(rules['alength'] == 1) & (rules['clength'] == 1)]\n    rules['antecedents'] = rules['antecedents'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\n    rules['antecedents'] = rules['antecedents'].str.replace(' ','\\n', regex = True)\n    rules['consequents'] = rules['consequents'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\n    rules['consequents'] = rules['consequents'].str.replace(' ','\\n', regex = True)\n\n\n    #Make some edges, now that we've run apriori\n    weighted_edges = []\n\n    G = nx.DiGraph()\n    \n    for x in range(len(rules)):\n        if(rules.iloc[x,8] > min_conviction):\n            weighted_edges.append((rules.iloc[x,0], rules.iloc[x,1], rules.iloc[x,8]))\n        \n        else:\n            G.add_node(rules.iloc[x,1])\n            G.add_node(rules.iloc[x,0])\n        \n        \n     \n    G.add_weighted_edges_from(weighted_edges)\n    \n    #Change node size according to support\n    for i in list(G.nodes()): \n        G.nodes[i]['support'] = single_items.loc[(single_items['jobtype'] == jobtype) & (single_items['itemsets'] == i), 'support'].values\n        \n    node_size = [60000*nx.get_node_attributes(G, 'support')[v] for v in G] \n    edge_width = [(G[u][v]['weight']- 1)*10.5 if((G[u][v]['weight']- 1) > .1) else 0 for u, v in G.edges() ]\n\n    pos = nx.circular_layout(G)\n    plt.figure(figsize=(27,22))\n\n\n    pos_shadow = copy.deepcopy(pos)\n    shift_amount = 0.008\n    for idx in pos_shadow:\n        pos_shadow[idx][0] += shift_amount\n        pos_shadow[idx][1] -= shift_amount\n\n    nx.draw_networkx_nodes(G, pos_shadow, node_color='k', alpha=0.3,  node_size = node_size)   \n\n    nx.draw_networkx_nodes(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, width = edge_width)\n\n    nx.draw_networkx_labels(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, size = 20, font_size = 28, width = edge_width, font_weight = 'bold', font_color = 'darkorange')\n\n    nx.draw_networkx_edges(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, width = edge_width)\n    plt.title(jobtype + ' Skills', size = 50)\n    plt.axis('off')\n    plt.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\n## How do I read these\n\n### **Intuitively**\nThe size of the node tells you the probability of that word appearing in a Job Description.\n\nThe width of an arrow tells you the probability that if that word appears in a Job Description, the node it's pointing to will appear as well.\n\nThe arrows tell you what skill implies another, so if a node has many arrows pointing to it, then it is useful in all subtypes of that job.  \n\nIf you have a skill, use the arrows to see what other skills you should learn. If you get to a dead end node, then you can start looking at the arrows that point to it as new skills to branch out to.\n\nI want to note here, if you're familiar with apriori you will see that I haven't included any multi-item sets.  This makes it dangerous to go from one arrow to the next to the next, so be careful thinking about large chanins.  Because we can run into the same problem I talked about above.\n\n### **Mathematically**\n\nFirst let's look at two other measures\n\nConfidence:\n\n### $\\text{confidence}(A\\rightarrow B) = \\frac{P(A \\cap B)}{P(A)}, \\;\\;\\; \\text{range: } [0, 1]$\n\nThis tells us how often A and B occur over how often A occurs.  Note it doesn't take into account P(B), for this we use lift\n\n### $\\text{lift}(A\\rightarrow B) = \\frac{P(A \\cap B)}{P(A)P(B)}, \\;\\;\\; \\text{range: } [0, \\infty]$\n\nThis tells us how often A and  B occur together over how often we would expect them to occur together.  However, if P(A) is small and P(B) big, it may be that most A's include B, but because B is so large the lift is low.  Look for example at Data Visualization and Business in the BA chart below.  \nWe want something that combines confidence and lift, this is conviction.\n\nConviction is defined as: \n### $\\text{conviction}(A\\rightarrow B) = \\frac{P(A) P(-B)}{P(A \\cap -B)}, \\;\\;\\; \\text{range: } [0, \\infty]$\n\nThis solves the problem of the last example, because if most A's include B the conviction will capture this measure.  Consult [here]( https://michael.hahsler.net/research/association_rules/measures.html#conviction) for a better explanation. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"make_network_graph(jobtypes[1],1.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hopefully this speaks for itself.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"make_network_graph(jobtypes[0],1.45)\nplt.savefig('DAgraph.svg', format='svg', dpi=1200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note things that only have outward arrows, this means it cannot be used on its own  \n\nAlso, note the change in relative size of the statistics and business nodes"},{"metadata":{"trusted":true},"cell_type":"code","source":"make_network_graph(jobtypes[2],1.6)\nplt.savefig('DSgraph.svg', format='svg', dpi=1200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you so much for reading, I hope this was helpful.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}