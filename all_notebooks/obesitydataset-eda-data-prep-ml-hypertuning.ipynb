{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Part 1: Data Preperation and Exploration"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import packages and libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import shapiro\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read file\ndf = pd.read_csv(\"../input/obesity-levels/ObesityDataSet_raw_and_data_sinthetic.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About the Data\n\nThis data comes from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+). This dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition.\n\n### Notes:\n<ul> \n    <li> This data is created via survery. However the data was imbalanced and records were synthasized using WEKA to simulate additional responses.\n    <li> The synthasized data fits the distribution of the data but generates floats (ratio data) where ordinal and integer data were the original values in the survey.\n    <li> The target variable is comprised of the variables height and weight. Height and weight are used to classisfy a persons BMI. The BMI measure was then used to classify each observation. \n</ul>\n\n### Features & Descriptions\n\n<table>\n    <thead>\n        <tr><th>Category</th><th>Feature Name</th><th>Description</th><th>Variable Type</th></tr>\n    </thead>\n    <tbody>\n        <tr><td>Target Variable</td><td>NObesity</td><td>Based on BMI</td><td>Categorical</td></tr>\n        <tr><td>Eating Habits</td><td>FAVC</td><td>Frequent consumption of high caloric food</td><td>Categorical</td></tr>\n        <tr><td>Eating Habits</td><td>FCVC</td><td>Frequency of consumption of vegetables</td><td>Ordinal</td></tr>\n        <tr><td>Eating Habits</td><td>NCP</td><td>Number of main meals</td><td>Ordinal</td></tr>\n        <tr><td>Eating Habits</td><td>CAEC</td><td>Consumption of food between meals</td><td>Ordinal</td></tr>\n        <tr><td>Eating Habits</td><td>CH20</td><td>Consumption of water daily</td><td>Ordinal</td></tr>\n        <tr><td>Eating Habits</td><td>CALC</td><td>Consumption of alcohol</td><td>Ordinal</td></tr>\n        <tr><td>Physical Conditioning</td><td>SCC</td><td>Calories consumption monitoring</td><td>Categorical</td></tr>\n        <tr><td>Physical Conditioning</td><td>FAF</td><td>Pysical activity frequency</td><td>Ordinal</td></tr>\n        <tr><td>Physical Conditioning</td><td>TUE</td><td>Time using technology devices</td><td>Ordinal</td></tr>\n        <tr><td>Physical Conditioning</td><td>MTRANS</td><td>Transportation used</td><td>Categorical</td></tr>\n        <tr><td>Physical Conditioning</td><td>SMOKE</td><td>Smokes Yes or No</td><td>Categorical</td></tr>\n        <tr><td>Responder Charateristics</td><td>Family History with Overweight</td><td>Yes or No</td><td>Categorical</td></tr>\n        <tr><td>Responder Charateristics</td><td>Gender</td><td>Gender is Male or Female</td><td>Categorical</td></tr>\n        <tr><td>Responder Charateristics</td><td>Age</td><td>Age in years</td><td>Integer</td></tr>\n        <tr><td>Responder Charateristics</td><td>Height</td><td>Height in meters</td><td>Float</td></tr>\n        <tr><td>Responder Charateristics</td><td>Weight</td><td>Weight in kilograms</td><td>Float</td></tr>        \n     </tbody>\n</table>"},{"metadata":{},"cell_type":"markdown","source":"\n\n# Exploring the Data & Clean Up\n\n- The data contains 2111 records with 17 columns\n- The data loads as text and float objects for most of the objects. However we know that some are float, categorical and ordinal\n- All of the records are unique and contain no null values\n- Height and Weight are included however they have a direct correlation to each other and our target variable\n- The survey data is distinguishable from the synthazied data based on floats used for ordinal variables\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 5 rows show survey data\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bottom 5 rows show synthetic data\ndf.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional rows showing synthetic data\ndf.iloc[[501,518,516]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Prep / Clean Up\n\n- Drop Height and Weight columns as they are used in the BMI calculation for our target variable\n- Convert categorical variables to category instead of object/text\n- Convert synthetic floats & floats to whole integers to better reprsent the ordinal data from orignal survey data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Height and weight are highly correlated and they directly correlate to the BMI calc used for the target\n# Remove Height and Weight\ndf = df.drop(columns=['Height', 'Weight'])\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no nulls \ndf[df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert object/text variables to category variables\ncolumns = [\"Gender\", \"family_history_with_overweight\", \"FAVC\", \"CAEC\", \"SMOKE\", \"SCC\", \"CALC\", \"MTRANS\", \"NObeyesdad\"]\n\nfor col in columns:\n    df[col] = df[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to interigate data after conversion\n# provides min, max, unique counts\ndef variable_counts(columns, stage):\n\n    if stage == 'pre':\n        print(\"Pre Conversion to Integer\")\n    else:\n        print(\"Post Conversion to Integer\")\n\n    for col in columns:    \n        print(\"Variable:\", col, \"| Count Unique:\",df[col].nunique(),\"| Min: \", df[col].min(), \"| Max: \",df[col].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert float variables to integer to the nearest inter\ncolumns = [\"FCVC\", \"NCP\", \"CH2O\", \"TUE\", \"FAF\"]\n\n# pre conversion countss\nvariable_counts(columns, 'pre')\n\n# convert to int / nearest int value\nfor col in columns:\n    #round to nearest whole number\n    df[col] = round(df[col]).astype('int')  \n    \n# post conversion counts\nprint(\"\")\nvariable_counts(columns, 'post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirm types\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# review non synthetic are still the same\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Intuition & Further Exploration\n\n##### Categorical Variables\n- The categorical variables are not gaussian\n- Most of the categorical variables are bernoulli in nature\n- The target variables based on synthetic process are fairly balanced\n\n##### Ordinal Variables                                        \n- Will be treated as nominal         \n- Non are Gaussian distributed\n\n##### Ratio Variable\n- Age is the only ratio varible\n- Is not Gaussian\n\n##### Predictor Coorelations \n- The predictor variables not highly correlated\n- Height and Weight have been removed for their correlation to each other and the target\n\n##### Target Variable\n- Is categorical with > 2 classes\n- Is faily balanced in its distribution of weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns of interest\ncolumns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE',\n           'SCC', 'CALC', 'MTRANS', 'NObeyesdad']\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 10))\nfor col, subplot in zip(columns, ax.flatten()):\n    sns.countplot(df[col], ax=subplot)\n    \n    if col==\"MTRANS\":\n        sns.countplot(df[col],ax=subplot)\n        subplot.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.MTRANS)        \n        subplot.yaxis.label.set_text(\"Number of Records\")\n    elif col==\"NObeyesdad\":\n        sns.countplot(df[col],ax=subplot)\n        subplot.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.NObeyesdad)  \n        subplot.yaxis.label.set_text(\"Number of Records\")\n    else:\n        sns.countplot(df[col],ax=subplot)  \n        subplot.yaxis.label.set_text(\"Number of Records\")\n        \n# show figure & plots\nfig.suptitle(\"Categorigal Variables\", fontsize=20)\nplt.tight_layout(pad=5, w_pad=0.0, h_pad=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns of interest\ncolumns = [\"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n\nfig, ax = plt.subplots(1, 5, figsize=(15, 4))\nfor col, subplot in zip(columns, ax.flatten()):\n    sns.countplot(df[col], ax=subplot)\n    subplot.yaxis.label.set_text(\"Number of Records\")\n\n# show figure & plots\nfig.suptitle(\"Ordinal Variables\", fontsize=20)\nplt.tight_layout(pad=5, w_pad=0.7, h_pad=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio variable distribution \n\nfig = plt.figure(figsize = (16,5))\n\n#distplot\nax1 = fig.add_subplot(121)\nsns.distplot(df[\"Age\"], kde=True)\n\n#boxplot\nax1 = ax1 = fig.add_subplot(122)\nsns.boxplot(df.Age)\n\n# show figure & plots\nfig.suptitle(\"Distribution of Numeric (Ratio) Variable\", fontsize=20)\nplt.tight_layout(pad=5, w_pad=0.5, h_pad=.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create figure\nfig = plt.figure(figsize=(15, 5))\n\n# add subplot for one row 2 graphs first postion\nax1 = fig.add_subplot(121)\n\n# correlation data matrix\nmatrix = np.triu(df.corr())\n\n# set title \nax1.title.set_text(\"Coorelation Heatmap: Predictor Variables\")\n\n#define plot\nsns.heatmap(df.corr(), \n                 mask=matrix,\n                 annot = False,                 \n                 fmt='.1g', \n                 cmap=\"YlGnBu\", \n                 vmin=-1, vmax=1, center= 0,                 \n                 square=\"True\",\n                 ax=ax1)\n\n# add second subplot\nax2 = fig.add_subplot(122)\n\n# rotate axis label\nax2.set_xticklabels(rotation=45, horizontalalignment='right', labels=df.NObeyesdad)\n\n# Set title text\nax2.title.set_text(\"Weight Category Counts: Target Variable\")\n\n# define second plot\nsns.countplot(x=\"NObeyesdad\",                  \n                 palette=\"Blues_d\", \n                 order=df.NObeyesdad.value_counts().index,\n                 ax = ax2,\n                 data=df)\n\n# labels for x and y\nax2.xaxis.label.set_text(\"Level Category\")\nax2.yaxis.label.set_text(\"Number of Records\")\n\n# turn off top and right frame lines\nax2.spines['right'].set_visible(False)\nax2.spines['top'].set_visible(False)\n\n# show figure & plots\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n#print highly correlated variables\nprint(\"Number of variables with > 0.95 correlation: \", len(to_drop))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Intuition & Model Considerations\n\n##### Data Intuion\n- Models for this data need to cater to > 2 two classes\n- Data is limited so model will need to perform well with limited amount of data\n- Model will need to handle dummy variables well and not be dependent on ratio data\n- The ordinal variables may or may not be helful to the model\n- The data is not Gaussian so the model needs to be non-parametic or at least not strictly parametic \n- The data is pretty wide but not very deep, so reducing the factors may be necessary\n\n##### Potential Models\n- Decision Trees \n- Random Forest \n- Search Vector Machines (SVM)\n\n##### Experiments \nModels that may be used to evaluate data assumptions and model performance:\n- Is the data too limited for a Nerual Network? \n- Does Logistic Regression perform well if target variable reduced to 2 classes?\n- Do the other models perform better if target variable is reduced to 2 classes?"},{"metadata":{},"cell_type":"markdown","source":"# Part 2: Machine Learning\n\nThe target variable is NObesity. The attempt is is apply ML to find the best model for predicting NObesity. \nNObesity is a categorical variable that is a measure of a person weight ranging from under weight to very obese (Overweight Level II). "},{"metadata":{},"cell_type":"markdown","source":"# Data prep for ML models: General \n- Some models may require additional treatment \n- These steps will prep the data for the most viable models: Decision Trees, Random Forest, SVM, Nerual Network\n- Some models may benifit from scaling so this will be evaluated as well\n\n#### Data Treatment\n- Copy cleaned data to new dataframe\n- Create dummy variables out of categorical variables\n- Split the data into 70/30 train & test datasets     "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prep = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dummy variables\ndf_prep = pd.get_dummies(df_prep,columns=[\"Gender\",\"family_history_with_overweight\",\n                                          \"FAVC\",\"CAEC\",\"SMOKE\",\"SCC\",\"CALC\",\"MTRANS\"])\ndf_prep.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset in features and target variable\n\n# Features\nX = df_prep.drop(columns=[\"NObeyesdad\"])\n\n# Target variable\ny = df_prep['NObeyesdad'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import sklearn packages for data treatments\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Models\n\nFor this exercise we will take a look at \n- Decision Trees\n- Random Forest\n- Support Vector Machines (SVM)\n- K Nearest Neighbors \n\nCursory Look at the model results suggest that Random Forest will be our best initial model with an accuracy of 79%. However, many of the other models performance is not too far off. \n\nThe other interesting result is that all of the models classify Obesity_Type_III amazingly well with >= 98% accuracy across all models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.preprocessing import StandardScaler # Import for standard scaling of the data\nfrom sklearn.preprocessing import MinMaxScaler # Import for standard scaling of the data\n\n# standard scale data\nss = StandardScaler()\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.transform(X_test)\n\n# tested MinMaxScaler as KNN historically does better with MinMax\nmm = MinMaxScaler()\nX_train_mm_scaled = ss.fit_transform(X_train)\nX_test_mm_scaled = ss.transform(X_test)\n\n# program to run multilple models though sklearn \n# Default settings output accuracy and classification report\n# compares accuracy for scaled and unscaled data\ndef run_models(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame):\n    \n    models = [          \n          ('Random Forest', RandomForestClassifier(random_state=2020)),\n          ('Decision Tree', DecisionTreeClassifier()),                                                 \n          ('KNN', KNeighborsClassifier()),\n          ('SVM', SVC())\n        ]  \n    \n    for name, model in models:        \n        # unscaled data\n        clf = model.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        \n        # scaled data\n        clf_scaled = model.fit(X_train_scaled, y_train)\n        y_pred_scaled = clf_scaled.predict(X_test_scaled)\n        \n        # mm scaled data\n        clf_mm_scaled = model.fit(X_train_mm_scaled, y_train)\n        y_pred_mm_scaled = clf_scaled.predict(X_test_mm_scaled)\n        \n        # accuracy scores\n        accuracy = round(metrics.accuracy_score(y_test, y_pred),5)\n        scaled_accuracy = round(metrics.accuracy_score(y_test, y_pred_scaled),5)\n        scaled_mm_accuracy = round(metrics.accuracy_score(y_test, y_pred_mm_scaled),5)\n        \n        # output\n        print(name + ':')        \n        print(\"---------------------------------------------------------------\")      \n        print(\"Accuracy:\", accuracy)\n        print(\"Accuracy w/Scaled Data (ss):\", scaled_accuracy)\n        print(\"Accuracy w/Scaled Data (mm):\", scaled_mm_accuracy)\n        if (accuracy > scaled_accuracy) and (accuracy > scaled_mm_accuracy):\n            print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred))      \n            print(\"                            -----------------------------------               \\n\")      \n        elif (scaled_accuracy > scaled_mm_accuracy):\n            print(\"\\nClassification Report (ss):\\n\", metrics.classification_report(y_test, y_pred_scaled))      \n            print(\"                            -----------------------------------               \\n\")     \n        else:            \n            print(\"\\nClassification Report (mm):\\n\", metrics.classification_report(y_test, y_pred_mm_scaled))      \n            print(\"                            -----------------------------------               \\n\")      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run Decision Trees, Random Forest, KNN and SVM\nrun_models(X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning\n\nSearching for better performance out of the models with Gridsearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n#model name, classifier, parameters\n# function used to process models and parameters through gridsearch\ndef hyper_tune(name, clf, parameters, target_names=None): \n    \n    target_names = target_names\n    clf = clf\n    search = GridSearchCV(clf, parameters,verbose=True, n_jobs=15, cv=5)\n    search.fit(X_train_scaled,y_train)\n    y_pred_scaled = search.predict(X_test_scaled)\n    print (\"Accuracy Score = %3.2f\" %(search.score(X_test_scaled,y_test)))\n    print (search.best_params_)\n    print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred_scaled, target_names=target_names))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the KNN model performs better on the unscaled data this function\n# function for unscaled data\n#model name, classifier, parameters\n# function used to process models and parameters through gridsearch\ndef hyper_tune2(name, clf, parameters, target_names=None): \n    \n    target_names = target_names\n    clf = clf\n    search = GridSearchCV(clf, parameters,verbose=True, n_jobs=15, cv=5)\n    search.fit(X_train,y_train)\n    y_pred = search.predict(X_test)\n    print (\"Accuracy Score = %3.2f\" %(search.score(X_test,y_test)))\n    print (search.best_params_)\n    print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of neighbors\nn_neighbors = [int(x) for x in range(4, 15)]\n# weights\nweights = ['uniform','distance']\n# distance metric\nmetric = ['euclidean', 'manhattan', 'chebyshev']\n# computation algorithm\nalgorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n# power paramter\np=[1,2]\n\nparameters = { 'n_neighbors': n_neighbors,\n              'weights':weights,\n              'metric':metric,\n              'p':p,\n              'algorithm': algorithm              \n               }\n\nhyper_tune2('KNN', KNeighborsClassifier(), parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in range(10, 200,10)]\n# Criterion\ncriterion = ['gini','entropy']\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in range(10, 100, 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in range(2, 5)]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in range(2, 5)]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# random state\nrandom_state = [1010]\n\nparameters = { 'criterion':criterion,\n               'n_estimators': n_estimators,\n              'max_depth':max_depth,\n              #'random_state': random_state,\n              #'max_features':max_features,\n              #'min_samples_split':min_samples_split             \n               }\n\n\nhyper_tune('Random Forest',\n           RandomForestClassifier(), parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance w/ Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Decision Tree classifer object with optimized parameters\nclf = RandomForestClassifier(criterion='entropy',\n               n_estimators=52,\n              max_depth = 51,              \n              max_features='auto',\n              min_samples_split=2,\n              random_state=1010)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train_scaled,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test_scaled)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\nfig = plt.figure(figsize=(10, 5))\n\n# Creating a bar plot\nsns.barplot(x=feature_imp.index, y=feature_imp)\n\n# Add labels to your graph\nplt.xticks(rotation=45, horizontalalignment='right')\n\nplt.tight_layout()\nplt.show()\n\n# create features list\nfeatures_list = X.columns\nfeatures_list = features_list.tolist()\n\n# Get numerical feature importances\nimportances = list(clf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \nprint(\"\\nTop 10 Features:\")\ndisplay_top = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]\n\n# Sort the feature importances by least important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = False)\n# Print out the feature and importances \nprint(\"\\nBottom 10 Features:\")\ndisplay_bottom = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Two Category Targert Variable\n\n### Data Prep\n\n![](http://)Underweight mapped to being not obese. Arguably however it is just as much of a health concern as being overweight. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# map values \nweight_map = { 'Normal_Weight':0, 'Overweight_Level_I':0,\n               'Overweight_Level_II':0, 'Obesity_Type_I':1,\n               'Obesity_Type_II':1, 'Obesity_Type_III':1, 'Insufficient_Weight':0}\n\n# map values\ndf_prep['weight_cat'] = df_prep['NObeyesdad'].map(weight_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"weight_cat\",                  \n                 palette=\"Blues_d\", \n                 order=df_prep[\"weight_cat\"].value_counts().index,                 \n                 data=df_prep)\n\n\n# show figure & plots\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset in features and target variable\n\n# Features\nX = df_prep.drop(columns=[\"NObeyesdad\",\"weight_cat\"])\n\n# Target variable\ny = df_prep['weight_cat'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n\n# Scaled version of X train and X test\nss = StandardScaler()\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Random Forest with Two Category Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in range(10, 200,10)]\n# Criterion\ncriterion = ['gini','entropy']\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in range(10, 100, 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in range(2, 20,2)]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in range(2, 20, 2)]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# random state\nrandom_state = [1010]\n\ntarget_names = ['Not Obese', 'Obese']\n\nparameters = { 'criterion':criterion,\n               'n_estimators': n_estimators,\n              'max_depth':max_depth,\n              'random_state': random_state,\n              'max_features':max_features\n              #'min_samples_split':min_samples_split             \n               }\n\nhyper_tune('Random Forest', RandomForestClassifier(), parameters, target_names=target_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance with Random Forest: Two Category Variable Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Random Forest classifer object with optimized parameters\nclf = RandomForestClassifier(criterion='gini',\n               n_estimators=110,\n              max_depth = 20,              \n              max_features='auto',              \n              random_state=1010)\n\n# Train Random Forest classifer\nclf = clf.fit(X_train_scaled,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\nfig = plt.figure(figsize=(12, 5))\n\n# Creating a bar plot\nsns.barplot(x=feature_imp.index, y=feature_imp)\n\n# Add labels to your graph\nplt.xticks(rotation=45, horizontalalignment='right')\n\nplt.tight_layout()\nplt.show()\n\n# create features list\nfeatures_list = X.columns\nfeatures_list = features_list.tolist()\n\n# Get numerical feature importances\nimportances = list(clf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \nprint(\"\\nTop 10 Features:\")\ndisplay_top = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]\n\n# Sort the feature importances by least important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = False)\n# Print out the feature and importances \nprint(\"\\nBottom 10 Features:\")\ndisplay_bottom = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:10]]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}