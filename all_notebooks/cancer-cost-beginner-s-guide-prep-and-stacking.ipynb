{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Cancer Treatment Cost | Beginner's guide: Prep and Stacking**\n\nThis notebook is a simple overview that will go over basic skills for data science and demonstrate how a code should look like. "},{"metadata":{},"cell_type":"markdown","source":"## Libary import\nFirst we import all libaries we will need:"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Imported Libraries:\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing\nimport seaborn as sns  # visual data\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor  # ML models\nfrom sklearn.linear_model import Lasso  # ML model\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV  # Hyperparameter search\nfrom sklearn.ensemble import StackingRegressor  # Stacking for linear regression\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading data from CSV\nWe will be building this code using fucntions, this way its easy to change the code later on if needed and its more easy to read and go over. Later on we will write the formal main function.\n\nWe read the data from CSV file. We start by viewing the data summary and realize that something has gone horribly wrong with the rows and orders. The first rows needs to be removed because of the needless heading. Thus, we are changing the rows to have a normal dataframe"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def data_read():\n    df = pd.read_csv(r'../input/costs-for-cancer-treatment/DowloadableDataFull_2011.01.12.csv', header=None)\n    print(df.info(verbose=True, null_counts=True))  \n    col_index = df.iloc[3, :].values  # saving the features names for later.\n    df = df.iloc[4:, :].reset_index(drop=True)\n    df.columns = col_index  # returning the features names\n    print(\"***\"*5)\n    print(f\"The fixed data:\\n {df.info(verbose=True, null_counts=True)}\") \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true},"cell_type":"code","source":"df = data_read()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the data\n\nLooking at the data and the unique values in each row.\nWe found that the data itself is mostly made of repeated data.\nThus, we need to do a data preprocessing and feature engineering before the data could be visualized and analyzed further. The fact that all the numeric data is in strings prevents us to visual the data and understand it better.\n"},{"metadata":{"_kg_hide-output":false,"trusted":true,"scrolled":true},"cell_type":"code","source":"def data_inquiry(df):\n    print(f'the data has {df.isna().sum().sum()} NaNs')\n    for col in df.iloc[:, :-3]:  # running over the columns to check for repeated values, minus the last 3 numeric ones.\n        print(\"Column: \", col, df[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true},"cell_type":"code","source":"data_inquiry(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering of the different columns\n\nWe change the data to improove our models accuracy and to be able to visual it."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def feature_engineer(df):\n    df.drop(['Last Year of Life Cost', 'Continuing Phase Cost', 'Initial Year After Diagnosis Cost', 'Age'], axis=1,\n            inplace=True)  #Droping columns with repeated\n    df.rename(columns={'Incidence and Survival Assumptions': 'Incidence+Survival',\n                         'Annual Cost Increase (applied to initial and last phases)': 'Cost'}, inplace=True)\n    df['Sex'] = df['Sex'].map({'Both sexes': 0, 'Females': 1, 'Males': 2})  # Categorized data with no order\n    df['Cost'] = df['Cost'].map({'0%': 0, '2%': 1, '5%': 2}).astype('category')  # Changing the data to categories with an order\n    df['Total Costs'] = df['Total Costs'].astype('float64')  # Changing from an object type.\n    ax = sns.boxplot(x=\"Year\", y=\"Total Costs\", data=df)  # Visual the Year and Total Costs after changing the data type\n    print(f\"Data skewness: {df.skew()}\")  # Skewness check\n    x = df.iloc[:, :-1]\n    x = pd.get_dummies(x)   # Convert categorical variable into indicator variables\n    y = df.iloc[:, -1]\n    return x, y, df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x, y, df = feature_engineer(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization\n\nNow After we changed the data to numeric we are able to visual it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def visual(df):\n    sns.catplot(x='Cancer Site', y='Total Costs', color='#FB8861',kind=\"box\", data=df, legend_out=False, height=10, aspect=2)\n    sns.catplot(x='Sex', y='Total Costs', color='r',kind=\"violin\", data=df)\n    sns.catplot(x='Incidence+Survival', y='Total Costs',kind=\"box\", color='b', data=df, height=10, aspect=2 )\n    sns.catplot(x='Cost', y='Total Costs', kind=\"box\", color='g', data=df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"visual(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dividing the data into the train and test:\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def train_test_split_data(x, y):\n    x_train, x_test, y_train, y_test = train_test_split(x, y)\n    y_train = np.log1p(y_train)  # Using log to fix the skewness of the label (log1p is to help vs negative values)\n    print(f\"Data skewness after log: {df.skew()}\")\n    return x_train, x_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split_data(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross validation of different models:"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def cross_val_models(x_train, y_train, cv_param=5):\n    ABR = AdaBoostRegressor()\n    GBR = GradientBoostingRegressor()\n    RF = RandomForestRegressor()\n    Las = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))   \n    # Lasso is better used with RobustScaler and pipeline, thus we gave him his own parameters\n                                                                     \n    # best_est = hyperparam(ABR, GBR, RF)  # Look in the note below\n    \n    GBR = GradientBoostingRegressor()   # Surprisingly we got better results using the default parameters\n    ABR = AdaBoostRegressor(learning_rate=1, loss='square', n_estimators=100)\n    RF = RandomForestRegressor(max_depth=8, n_estimators=600)\n    models = [ABR, GBR, RF, Las]\n    for model in models:    # Cross validation of the train data with the different models\n        cv_results = -cross_val_score(model, x_train, y_train, cv=cv_param, scoring='neg_mean_squared_error')\n        mean_cv = cv_results.mean()\n        model_name = type(model).__name__\n        if model_name == 'Pipeline':\n            model_name = 'Lasso'\n        print(f'The mean_squared_error for {model_name} is {mean_cv}')\n    return models","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"models = cross_val_models(x_train, y_train, cv_param=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Note\n**The hyperparameter fucntion take some time, according to your hardware, so we added the hyperparameters manually after running the code, to run it, just remove the '#' from the start of the row and add the best_est according to the model.**"},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters scan using GridSearchCV\n\nWe utlize Scikit Learn function GridSearchCV to go over different hyperparameters and find the most ideal ones. The function is not perfect as we didnt use all the results in our data. Note that this process may take a couple of minutes, even with an 8 core CPU.\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def hyperparam(ABR, GBR, RF):\n    RF_param = {\n        'max_depth': [4, 6, 8],\n        'max_features': ['auto', 'sqrt'],\n        'min_samples_leaf': [1, 2, 4],\n        'min_samples_split': [2, 5, 10],\n        'n_estimators': [200, 400, 600, 800]}\n    GB_param = {\n        \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n        \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n        \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n        \"max_depth\": [3, 5, 8],\n        \"max_features\": [\"log2\", \"sqrt\"],\n        \"criterion\": [\"friedman_mse\", \"mae\"],\n        \"subsample\": [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n        \"n_estimators\": [10]}\n    AB_param = {\n        'n_estimators': [50, 100],\n        'learning_rate': [0.01, 0.05, 0.1, 0.3, 1],\n        'loss': ['linear', 'square', 'exponential']}\n    param_list = [RF_param, GB_param, AB_param]\n    model_list = [RF, GBR, ABR]\n    best_est = []\n    for param, model in zip(param_list, model_list):\n        clf = GridSearchCV(model, param, n_jobs=-1, scoring='neg_mean_squared_error')\n        clf.fit(x_train, y_train)\n        print(clf.best_estimator_)\n        best_est.append(clf)\n    return best_est","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking the models with a final regressor to achieve a better MSE:\n\nStacking allows us to use each individual estimator by using their output as input of a final estimator.\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def stacking(models, x_train, x_test, y_train, y_test):\n    estimators_ = []\n    for model in models:\n        estimators_.append((str(model), model))\n    stack = StackingRegressor(estimators=estimators_, final_estimator=RandomForestRegressor(n_estimators=10,\n                                                                                            random_state = 42))\n    stack.fit(x_train, y_train)\n    y_pred = stack.predict(x_test)\n    mse = np.square(y_pred-np.log1p(y_test)).mean()  # Final MSE calculation while remembering to adapt the y_test with\n                                                     # the log, like we did with the y_train\n    return mse","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"    mse = stacking(models, x_train, x_test, y_train, y_test)\n    print(f'The final MSE is {mse}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The main code\n\nFrom this code we will call all the functions above and print The final MSE of the stacked models y_pred, compared with the y_test."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"if __name__ == '__main__':\n\n    df = data_read()\n    data_inquiry(df)\n    x, y, df = feature_engineer(df)\n    visual(df)\n    x_train, x_test, y_train, y_test = train_test_split_data(x, y)\n    models = cross_val_models(x_train, y_train, cv_param=6)\n    mse = stacking(models, x_train, x_test, y_train, y_test)\n    print(f'The final MSE is {mse}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}