{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **In this notebook I intend to explore a dataset looking at different types of Ramen. I intend to do some basic exploratory data analysis and see what I can learn.**"},{"metadata":{},"cell_type":"markdown","source":"Here I explore what makes ramen great and where it comes from!\n\nTo start with I do some basic exploratory data analysis to try and better understand the data.\n\nGenerally, the data has a lot of catergorical variables, so the project focuses on feature engineering using NLP.\n\nThe final step includes trying to make a predictive model for the rating of sushi based on the features. I employ the use of a number of machine learning techniques.\n\nOverall I found that the models were not so accurate at predicting lower ratings. This is likely due to the dataset being skewed towards mostly higher ratings, a broader dataset may therefore increase accuracy. Nonetheless, this was a fun project"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.rcParams['font.size'] = '15'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ramen-ratings/ramen-ratings.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concerned that some of the catergories I would expect to be numeric are objects, espcially the stars"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Stars.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Stars.replace('Unrated',np.nan,inplace=True)\ndf['Stars'] = pd.to_numeric(df.Stars)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thats better, now the stars are listed as numeric, lets recheck the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Initial thoughts**\n\nThe review number column appears to just be an index so I will drop that. It seems like we could have a lot of infomation in the Variety column so it would be good to do some feature extraction on that. First, however, I want to do some basic exploratory data analysis to better understand the raw data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Review #',axis=1,inplace=True)\nprint('The number of unique Brands is '+ str(len(df.Brand.unique())))\nprint('The number of unique countrys is '+ str(len(df.Country.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted = df.groupby('Country').count().sort_values('Brand',ascending=False).Brand\ndf_sorted = df_sorted.reset_index().copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.xticks(rotation=90)\nplt.ylabel('Count')\nplt.bar(df_sorted.Country,df_sorted.Brand)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now I want to see the average ramen rating per country, and also add infomation on how many ramens are produced per country**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ave_stars= df[['Country','Stars']].groupby('Country').mean().sort_values('Stars',ascending=False).reset_index()\ndf_joined = df_ave_stars.set_index('Country').join(df_sorted.set_index('Country')).reset_index()\nplt.figure(figsize=(15,5))\nplt.xticks(rotation=90)\n#plt.hist(df_ave_stars.Stars,bins = 15)\nplt.ylabel('Average Ramen Rating')\nsizes = dict(zip(df_joined.Country,df_joined.Brand.values*10))\nsns.scatterplot('Country','Stars', data=df_joined,\n                size='Country',\n                sizes=sizes,\n                legend=False,\n                alpha=0.5,\n                edgecolor='black',\n                palette = 'Set1'\n                )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.grid()\nplt.hist(df.Stars,bins=20,edgecolor='k',align='mid')\nplt.xlabel('Stars')\nplt.ylabel('Number')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ave_stars_style = df[['Style','Stars']].groupby('Style').mean().sort_values('Stars',ascending=False).reset_index()\nplt.figure(figsize=(10,5))\n#plt.hist(df_ave_stars.Stars,bins = 15)\nplt.ylabel('Average Ramen Rating')\nplt.grid()\nsns.boxplot(df.Style,df.Stars,palette = 'Set2')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now to do some feature extraction from the variety column. It would be good to try and get an idea of flavour"},{"metadata":{},"cell_type":"markdown","source":"**First I need to remove the punctuation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string \ndef remove_punctuation(text):\n    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n    return no_punct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:,'Variety'] = df.loc[:,'Variety'].apply(lambda x: remove_punctuation(x.lower()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now to remove stopwords such as 'the', 'and' etc...**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\ndf.loc[:,'Variety'] = df.loc[:,'Variety'].apply(lambda x: tokenizer.tokenize(x.lower()))\ndf.Variety","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nspecified_extra = ['noodles','noodle','flavour','artificial','ramen','instant','flavor','sauce','cup','bowl','rice']\ndef remove_stopwords(text):\n    english_words = [w for w in text if w not in stopwords.words('english')]\n    additional_words = [w for w in english_words if w not in specified_extra]\n    return additional_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:,'Variety'] = df.loc[:,'Variety'].apply(lambda x: remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now to recombine for analysis. I also take the opportunity to remove any repeats in the lists, before recombining**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:,'Variety'] = df.loc[:,'Variety'].apply(lambda x: list(set(x)))\ndf.loc[:,'Variety'] = df.loc[:,'Variety'].apply(lambda x:\" \".join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Begin vectorisation\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to generate the sparce matrix with all the top words contained andto extract the top words and counts "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Might take awhile...\nmax_feature_length = 10\ntop_words = []\n\nbow_transformer = CountVectorizer(max_features=max_feature_length,ngram_range=(1,1)).fit(df.loc[:,'Variety'])\nbow = bow_transformer.transform([' '.join(df.loc[:,'Variety'].values)])#This joins all the words in all the rows \nword_list = bow_transformer.get_feature_names()\ncount_list = bow.toarray().sum(axis=0) \ntop_counts = pd.DataFrame(zip(word_list,count_list),columns=['term','count',])\ntop_counts.sort_values('count',axis=0,inplace=True, ascending=False)\ntop_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a new column with the flavours identified"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['flavour'] = df.Variety.apply(lambda y: np.array([x for x in y.split() if x in top_counts.term.values]))\ndf['flavour'] = df['flavour'].apply(lambda x :\" \".join(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now I have the flavours identified, I want to see how it varies with the rating\nFirst I create a new dataframe, with the flavours spilt and add the rating column "},{"metadata":{"trusted":true},"cell_type":"code","source":"flavour = df['flavour'].str.split(' ', 3,expand=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flavour['stars'] = df['Stars']\nflavour.replace('',np.nan,inplace=True)\nflavour.replace('tom','tom yum',inplace=True) # Change tom to tom yum \nflavour['flavour'] = flavour.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.xticks(rotation=90)\nsns.boxplot('flavour','stars',data=flavour, palette = 'Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It seems that curry noodles are often rated highly. Seafood and shrimp along with chicken seem to be generally worse.** Seeing as some rows also have secondary flavours, this could be a great predictor of ratings, along with country, style and brand."},{"metadata":{},"cell_type":"markdown","source":"## From here I begin to make dummy variables for the flavour and try and predict the rating.\n\nNow to create dummy variables for the top words found\n\nAs there are a large number of countries and brands I use the BaseN encoder to reduce the dimentionality. For style I simply use get dummies."},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BaseNEncoder(cols=['Brand'],return_df=True,base=5)\ndata_encoded_brand = encoder.fit_transform(df.Brand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BaseNEncoder(cols=['Country'],return_df=True,base=5)\ndata_encoded_Country = encoder.fit_transform(df.Country)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_encoded_styles = pd.get_dummies(data=df.Style,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.BaseNEncoder(cols=['flavour'],return_df=True,base=5)\ndata_encoded_flavour = encoder.fit_transform(flavour.flavour)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cleaned = data_encoded_flavour.join(data_encoded_brand).join(data_encoded_styles).join(data_encoded_Country)\nfinal_cleaned['Stars'] = df.Stars","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to check that all columns are numeric and ready to be used in prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cleaned.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Flavour_0, Brand_0 and Country_0 all appear to have no values"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cleaned.drop(['flavour_0','Brand_0','Country_0'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(final_cleaned.corr(),annot=True,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Overall it actually seems like nothing is very correlated to stars, it therefore seems like machine learning models may struggle to predict the rating of Ramen.** Nonetheless, I apply some to confirm this hypothesis."},{"metadata":{},"cell_type":"markdown","source":"# Now to employ some ML techniques to predict the rating \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cleaned.dropna(inplace=True)\nx = final_cleaned.iloc[:,0:-1].values\ny = final_cleaned.Stars.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train,  y_test = train_test_split(x, y, test_size=0.25,)\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression().fit(x_train, y_train)\ny_predict = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.scatter(y_test,y_predict)\nplt.xlim([-0.5,5.5])\nplt.xlabel('Y Actual')\nplt.ylabel('Y Predicted')\nplt.ylim([-0.5,5.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_val = [] #to store rmse values for different k\nfor K in range(20):\n    K += 1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n    model.fit(x_train, y_train)  #fit the model\n    pred = model.predict(x_test) #make prediction on test set\n    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = neighbors.KNeighborsRegressor(n_neighbors = 7)\nmodel.fit(x_train, y_train)  #fit the model\ny_predict_KNN = model.predict(x_test) #make prediction on test set\nplt.figure(figsize=(15,10))\nplt.scatter(y_test,y_predict_KNN)\nplt.xlim([-0.5,5.5])\nplt.xlabel('Y Actual')\nplt.ylabel('Y Predicted')\nplt.ylim([-0.5,5.5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As I expected the traditional machine learning models struggle to predict the rating of the ramen. We could try deep learning here to confirm this, but i'm not too hopeful**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(15, input_dim=15, activation= \"relu\"))\nmodel.add(Dense(10, activation= \"relu\"))\nmodel.add(Dense(10, activation= \"relu\"))\nmodel.add(Dense(5, activation= \"relu\"))\nmodel.add(Dense(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\nhistory = model.fit(x_train, y_train, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train= model.predict(x_train)\nprint(np.sqrt(mean_squared_error(y_train,pred_train)))\n\ny_pred_NN = model.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(np.arange(0,100),history.history.get('loss'))\nplt.xlabel('Epoch')\nplt.ylabel('MSE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.scatter(y_test,y_pred_NN)\nplt.xlim([-0.5,5.5])\nplt.xlabel('Y Actual')\nplt.ylabel('Y Predicted')\nplt.ylim([-0.5,5.5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall given the lack of correlation between the predictors and the rating it has proved hard to generate an accurate machine learning model. Nonetheless, its been fun to use some NLP, ML and generally explore the Ramen Data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}