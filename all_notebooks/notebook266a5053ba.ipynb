{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/creditcardfraud'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv',sep=',')\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud = data[data['Class']==1]\n\nnormal = data[data['Class']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fraud.shape,normal.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Frequency of fraudulent and non-fraudulent transactions against amount')\nbins = 30\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" plt.boxplot(data.Amount, labels = ['Amount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncorrelation = data.corr()\ncorr_features = correlation.index\nplt.figure(figsize=(25,25))\ng=sns.heatmap(data[corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data.V2, data.Amount)\nplt.xlabel('V2')\nplt.ylabel('Amount ($)')\nsns.regplot(data['V2'],data['Amount'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outlier detection model"},{"metadata":{"trusted":true},"cell_type":"code","source":"Fraud = data[data['Class']==1]\n\nValid = data[data['Class']==0]\n\noutlier_fraction = len(Fraud)/float(len(Valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(outlier_fraction)\n\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\n\nprint(\"Valid Cases : {}\".format(len(Valid)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = data.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = data[columns]\nY = data[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=state, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction),\n   \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nn_outliers = len(Fraud)\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Support Vector Machine\":\n        clf.fit(X)\n        y_pred = clf.predict(X)\n    else:    \n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Isolation Forest was able to detect 675 errors as compared to Local Outlier Factor that detected 935 that is significantly higher.\n* In comparing the accuracy score, it is visible that Isolation Forest is more reliable with a rate of 99.76% compared to that of Local Outlier Factor which has 99.67%. \n* On analysing the classification report for error precision, it's clear that Isolation Forest has a higher percentage of detecting fraud cases at 31% while that of LOF detects fraud at just 5%."},{"metadata":{},"cell_type":"markdown","source":"* The Isolation Forest method is an overall better way to detect fraud cases. It was able to detect the highest percentage overall of fraud cases in the dataset and with least margin for error. \n* It can be used in future to detect fraud cases via use of deep machine learning algorithms and also using anomaly detection models. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}