{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/file-pe-headers/file_pe.csv', sep=',')\nX = data.drop([\"Name\", \"Malware\"], axis=1).to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_standardized = StandardScaler().fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA()\npca.fit_transform(X_standardized)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pca.explained_variance_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We begin by reading in our dataset and then standardizing it, as in the recipe on\nstandardizing data (steps 1 and 2). (It is necessary to work with standardized data before\napplying PCA). We now instantiate a new PCA transformer instance, and use it to both\nlearn the transformation (fit) and also apply the transform to the dataset, using\nfit_transform (step 3). In step 4, we analyze our transformation. In particular, note that\nthe elements of pca.explained_variance_ratio_ indicate how much of the variance is\naccounted for in each direction. The sum is 1, indicating that all the variance is accounted\nfor if we consider the full space in which the data lives. However, just by taking the first\nfew directions, we can account for a large portion of the variance, while limiting our\ndimensionality. In our example, the first 40 directions account for 90% of the variance:\n","metadata":{}},{"cell_type":"code","source":"sum(pca.explained_variance_ratio_[0:40])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThis means that we can reduce our number of features to 40 (from 78) while preserving 90%\nof the variance. The implications of this are that many of the features of the PE header are\nclosely correlated, which is understandable, as they are not designed to be independent.","metadata":{}}]}