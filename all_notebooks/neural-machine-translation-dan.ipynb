{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import collections\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/language-translation-englishfrench/eng_-french.csv\")\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english = data[\"English words/sentences\"]\nfrench = data[\"French words/sentences\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(english):\n  stri = \"\"\n  txt = tokenizer.tokenize(text)\n  for j in txt:\n    j = j.lower()\n    stri = stri + j\n    stri = stri + \" \"\n  english[i] = stri","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(french):\n  stri = \"\"\n  txt = tokenizer.tokenize(text)\n  for j in txt:\n    j = j.lower()\n    stri = stri + j\n    stri = stri + \" \"\n  french[i] = stri","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(english[0:10])\nprint(french[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n1 = 9900\nn2=10000\neng = []\nfre = []\n\nfor data in english:\n  eng.append(data)\n\nfor data in french:\n  fre.append(data)\n\neng = np.asarray(eng)\nfre = np.asarray(fre)\n\neng = eng[0:175000]\nfre = fre[0:175000]\n\nfor i in range(n1,n2):\n  print(eng[i] + \"\\t->\\t\" + fre[i] + \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"English Shape is \" + str(eng.shape))\nprint(\"French Shape is \" + str(fre.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english_word_counter = collections.Counter([word for sentence in eng for word in sentence.split(\" \")])\nfrench_word_counter = collections.Counter([word for sentence in fre for word in sentence.split(\" \")])\n\nprint('{} English words.'.format(len([word for sentence in eng for word in sentence.split()])))\nprint('{} French words.'.format(len([word for sentence in fre for word in sentence.split()])))\nprint(\"\\n\")\nprint('{} unique English words.'.format(len(english_word_counter)))\nprint('{} unique French words.'.format(len(french_word_counter)))\nprint(\"\\n\")\nprint('10 Most common words in the English dataset:')\nprint('\"' + '\" \"'.join(list(zip(*english_word_counter.most_common(10)))[0]) + '\"')\nprint(\"\\n\")\nprint('10 Most common words in the French dataset:')\nprint('\"' + '\" \"'.join(list(zip(*french_word_counter.most_common(10)))[0]) + '\"')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(x):\n  tokenizer = Tokenizer(char_level=False,oov_token=\" \")\n  tokenizer.fit_on_texts(x)\n  return tokenizer.texts_to_sequences(x), tokenizer\n\ntext_sentences = [\n    'The quick brown fox jumps over the lazy dog .',\n    'By Jove , my quick study of lexicography won a prize .',\n    'This is a short sentence .']\n  \ntext , tokenizer = tokenize(text_sentences)\nprint(text)\nprint(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad(x,length=None):\n  if (length==None):\n    length = max([len(sentence) for sentence in x])\n  a = pad_sequences(x,maxlen=length,padding=\"post\")\n  return a\n\ntest_pad = pad(text)\nfor i, (token_sent, pad_sent) in enumerate(zip(text, test_pad)):\n    print('Sequence {} in x'.format(i + 1))\n    print('  Input:  {}'.format(np.array(token_sent)))\n    print('  Output: {}'.format(pad_sent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x,y):\n  preprocess_x,x_tk = tokenize(x)\n  preprocess_y,y_tk = tokenize(y)\n\n  preprocess_x = pad(preprocess_x)\n  preprocess_y = pad(preprocess_y)\n  preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n  return preprocess_x, preprocess_y, x_tk, y_tk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_eng,pre_fre,eng_tk,fre_tk = preprocess(eng,fre)\nmax_eng_seq_len = pre_eng.shape[1]\nmax_fr_seq_len = pre_fre.shape[1]\nenglish_vocab_size = len(eng_tk.word_index)\nfrench_vocab_size = len(fre_tk.word_index)\n\nprint('Data Preprocessed')\nprint(\"Max English sentence length:\", max_eng_seq_len)\nprint(\"Max French sentence length:\", max_fr_seq_len)\nprint(\"English vocabulary size:\", english_vocab_size)\nprint(\"French vocabulary size:\", french_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n\n    learning_rate = 1e-3\n    model = keras.Sequential([\n        Embedding(english_vocab_size+1, 128, input_length=input_shape[1]),\n        Bidirectional(GRU(128, return_sequences=True)),\n        tf.keras.layers.Dropout(0.25),\n        TimeDistributed(Dense(french_vocab_size, activation='softmax'))\n    ])\n    model.summary()\n    \n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    \n    return model\n\ntmp_x = pad(pre_eng, max_fr_seq_len)\nrnn_model = embed_model(tmp_x.shape,max_fr_seq_len,english_vocab_size,french_vocab_size)\nrnn_model.fit(tmp_x, pre_fre, batch_size=1024, epochs=50, validation_split=0.2)\nrnn_model.save_weights(\"rnn_model_weights.h5\")\n#rnn_model.load_weights(\"/content/rnn_model_weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logits_to_text(logits, tokenizer):\n  index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n  index_to_words[0] = '<PAD>'\n\n  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n  print(logits_to_text(rnn_model.predict(tmp_x[1000])[i], fre_tk))\nprint(tmp_x[10000])\n\ni = 10000\nprint(eng[i] + \"\\t->\\t\" + fre[i] + \"\\n\")\n\n\n#[24 ,80 ,159 ,0 ,  0 ,  0,   0,   0,   0,   0,   0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_words = []\nnum_arr = [0 for i in range(11)]\nsentence = \"is\"\nsentence = sentence.lower()\nwords = sentence.split(\" \")\n\nfor i,word in enumerate(words):\n  num_arr[i] = eng_tk.word_index[word]\n\n\narra = []\nfor i in range(10):\n  arra.append(logits_to_text(rnn_model.predict(num_arr)[i], fre_tk))\n\nsent = \"\"\nfor word in arra:\n  if word == \"<PAD>\":\n    break\n  else:\n    sent = sent + word\n    sent = sent + \" \"\n\nprint(sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}