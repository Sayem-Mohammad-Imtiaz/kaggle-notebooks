{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport sklearn\nimport scipy.sparse \nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.neighbors import NearestNeighbors\nfrom multiprocessing import Pool\nimport numpy as np\n\nclass NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n    '''\n    KNN Features\n    \n    Sample Usage:\n    \n    k_list = [3, 5, 7]\n    NNF = NearestNeighborsFeats(n_jobs=-1, k_list=k_list, metric='minkowski')\n    NNF.fit(X, Y)\n    test_knn_feats = NNF.predict(X_test[:50])\n    '''\n    def __init__(self, n_jobs, k_list, metric, n_classes=None, n_neighbors=None, eps=1e-6):\n        self.n_jobs = n_jobs\n        self.k_list = k_list\n        self.metric = metric\n        \n        if n_neighbors is None:\n            self.n_neighbors = max(k_list) \n        else:\n            self.n_neighbors = n_neighbors\n            \n        self.eps = eps        \n        self.n_classes_ = n_classes\n    \n    def fit(self, X, y):\n        '''\n        Set's up the train set and self.NN object\n        '''\n        # Create a NearestNeighbors (NN) object. We will use it in `predict` function \n        self.NN = NearestNeighbors(n_neighbors=max(self.k_list), \n                                      metric=self.metric, \n                                      n_jobs=-1, \n                                      algorithm='brute' if self.metric=='cosine' else 'auto')\n        self.NN.fit(X)\n        \n        # Store labels \n        self.y_train = y\n        \n        # Save how many classes we have\n        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n        \n        \n    def predict(self, X):       \n        '''\n        Produces KNN features for every object of a dataset X\n        '''\n        if self.n_jobs == 1:\n            test_feats = []\n            for i in range(X.shape[0]):\n                test_feats.append(self.get_features_for_one(X[i:i+1]))\n        else:\n            '''\n             *Make it parallel*\n             Number of threads should be controlled by `self.n_jobs`  \n\n\n             You can use whatever you want to do it\n             For Python 3 the simplest option would be to use \n             `multiprocessing.Pool` (but don't use `multiprocessing.dummy.Pool` here)\n             You may try use `joblib` but you will most likely encounter an error, \n             that you will need to google up (and eventually it will work slowly)\n\n             For Python 2 I also suggest using `multiprocessing.Pool` \n             You will need to use a hint from this blog \n             http://qingkaikong.blogspot.ru/2016/12/python-parallel-method-in-class.html\n             I could not get `joblib` working at all for this code \n             (but in general `joblib` is very convenient)\n                     \n            '''\n            test_feats = Pool(self.n_jobs).map(self.get_features_for_one,\n                                              (X[i:i+1] for i in range(X.shape[0])))       \n        return np.vstack(test_feats)\n        \n        \n    def get_features_for_one(self, x):\n        '''\n        Computes KNN features for a single object `x`\n        '''\n\n        NN_output = self.NN.kneighbors(x)\n        \n        # Vector of size `n_neighbors`\n        # Stores indices of the neighbors\n        neighs = NN_output[1][0]\n        \n        # Vector of size `n_neighbors`\n        # Stores distances to corresponding neighbors\n        neighs_dist = NN_output[0][0] \n\n        # Vector of size `n_neighbors`\n        # Stores labels of corresponding neighbors\n        neighs_y = self.y_train[neighs] \n        \n        # We will accumulate the computed features here\n        # Eventually it will be a list of lists or np.arrays\n        # and we will use np.hstack to concatenate those\n        return_list = [] \n        \n        \n        ''' \n        1. Fraction of objects of every class.\n           It is basically a KNNÐ¡lassifiers predictions.\n\n           Take a look at `np.bincount` function, it can be very helpful\n           Note that the values should sum up to one\n        '''\n        for k in self.k_list:\n            feats = np.bincount(neighs_y[:k], minlength=self.n_classes) / k\n            \n            assert len(feats) == self.n_classes\n            return_list += [feats]\n        \n        \n        '''\n        2. Same label streak: the largest number N, \n           such that N nearest neighbors have the same label.\n\n           What can help you: `np.where`\n        '''\n        \n        feats = 1 + \\\n                np.where(np.append(neighs_y[:-1] != neighs_y[1:], True))[0].min(keepdims=True)# YOUR CODE GOES HERE\n        \n        assert len(feats) == 1\n        return_list += [feats]\n        \n        '''\n        3. Minimum distance to objects of each class\n           Find the first instance of a class and take its distance as features.\n\n           If there are no neighboring objects of some classes, \n           Then set distance to that class to be 999.\n\n           `np.where` might be helpful\n        '''\n        feats = []\n        for c in range(self.n_classes):\n            feats.append(np.append(neighs_dist[neighs_y == c], 999).min())# YOUR CODE GOES HERE\n        \n        assert len(feats) == self.n_classes\n        return_list += [feats]\n        \n        '''\n        4. Minimum *normalized* distance to objects of each class\n           As 3. but we normalize (divide) the distances\n           by the distance to the closest neighbor.\n\n           If there are no neighboring objects of some classes, \n           Then set distance to that class to be 999.\n\n           Do not forget to add self.eps to denominator.\n        '''\n        feats = []\n        for c in range(self.n_classes):\n            # YOUR CODE GOES HERE\n            feat = neighs_dist[neighs_y == c] / (neighs_dist[0] + self.eps)\n            feats.append(feat.min() if feat.size else 999)\n        \n        assert len(feats) == self.n_classes\n        return_list += [feats]\n        \n        '''\n        5. \n           5.1 Distance to Kth neighbor\n               Think of this as of quantiles of a distribution\n           5.2 Distance to Kth neighbor normalized by \n               distance to the first neighbor\n\n           feat_51, feat_52 are answers to 5.1. and 5.2.\n           should be scalars\n\n           Do not forget to add self.eps to denominator.\n        '''\n        for k in self.k_list:\n            \n            feat_51 = neighs_dist[k-1] # YOUR CODE GOES HERE\n            feat_52 = neighs_dist[k-1] / (neighs_dist[0] + self.eps)# YOUR CODE GOES HERE\n            \n            return_list += [[feat_51, feat_52]]\n        \n        '''\n        6. Mean distance to neighbors of each class for each K from `k_list` \n               For each class select the neighbors of that class among K nearest neighbors \n               and compute the average distance to those objects\n\n               If there are no objects of a certain class among K neighbors, set mean distance to 999\n\n           You can use `np.bincount` with appropriate weights\n           Don't forget, that if you divide by something, \n           You need to add `self.eps` to denominator.\n        '''\n        for k in self.k_list:\n            \n            bincount = np.bincount(neighs_y[:k], minlength=self.n_classes)\n            feats = np.where(\n                bincount,\n                np.bincount(neighs_y[:k], weights=neighs_dist[:k], minlength=self.n_classes) / (bincount+self.eps),\n                999\n            )\n            \n            assert len(feats) == self.n_classes\n            return_list += [feats]\n        \n        \n        # merge\n        knn_feats = np.hstack(return_list)\n        \n        assert knn_feats.shape == (239,) or knn_feats.shape == (239, 1)\n        return knn_feats","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}