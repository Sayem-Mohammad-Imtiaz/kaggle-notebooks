{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load and summarize the mapping file for the planet dataset\nfrom pandas import read_csv\n# load file as CSV\n#train_classes = pd.read_csv('/kaggle/input/planets-dataset/planet/planet/train_classes.csv')\n#train_classes.head()\nfilename = '/kaggle/input/planets-dataset/planet/planet/train_classes.csv'\nmapping_csv = read_csv(filename)\n# summarize properties\nprint(mapping_csv.shape)\nprint(mapping_csv[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc  #importing the module\ngc.collect() #Clearing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print all unique tags\nfrom itertools import chain\ntag_list = list(chain.from_iterable([tags.split(' ')for tags in mapping_csv['tags'].values]))\ntag_set = set(tag_list)#for unique tags\nprint('There are {} unique labels including {}'.format(len(tag_set),tag_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the first 9 images in the planet dataset\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n# define location of dataset\nfolder = '/kaggle/input/planets-dataset/planet/planet/train-jpg/'# plot first few images\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # define filename\n    filename = folder + 'train_' + str(i) + '.jpg'\n    # load image pixels\n    image = imread(filename)\n    # plot raw pixel data\n    plt.imshow(image)\n# show the figure\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect() #Clearing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load and prepare planet dataset and save to file\nfrom os import listdir\nfrom numpy import zeros\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom pandas import read_csv\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n \ndef create_tag_mapping(mapping_csv):\n    # create a set of all known tags\n    labels = set()\n    for i in range(len(mapping_csv)):\n        # convert spaced separated tags into an array of tags\n        tags = mapping_csv['tags'][i].split(' ')\n        # add tags to the set of known labels\n        labels.update(tags)\n    # convert set of labels to a list to list\n    labels = list(labels)\n    # order set alphabetically\n    labels.sort()\n    # dict that maps labels to integers, and the reverse\n    labels_map = {labels[i]:i for i in range(len(labels))}\n    inv_labels_map = {i:labels[i] for i in range(len(labels))}\n    return labels_map, inv_labels_map\n # create a mapping of filename to a list of tags\ndef create_file_mapping(mapping_csv):\n    mapping = dict()\n    for i in range(len(mapping_csv)):\n        name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n        mapping[name] = tags.split(' ')\n    return mapping\n\n# create a one hot encoding for one list of tags\ndef one_hot_encode(tags, mapping):\n    # create empty vector\n    encoding = zeros(len(mapping), dtype='uint8')\n    # mark 1 for each tag in the vector\n    for tag in tags:\n        encoding[mapping[tag]] = 1\n    return encoding\n \n# load all images into memory\ndef load_dataset(path, file_mapping, tag_mapping):\n    photos, targets = list(), list()\n    # enumerate files in the directory\n    for filename in listdir(folder):\n        # load image\n        photo = load_img(path + filename, target_size=(64,64))\n        # convert to numpy array\n        photo = img_to_array(photo, dtype='uint8')\n        # get tags\n        tags = file_mapping[filename[:-4]]\n        # one hot encode tags\n        target = one_hot_encode(tags, tag_mapping)\n        # store\n        photos.append(photo)\n        targets.append(target)\n    X = asarray(photos, dtype='uint8')\n    y = asarray(targets, dtype='uint8')\n    return X, y\n# load the mapping file\n#filename = '/kaggle/input/planets-dataset/planet/planet/train_classes.csv'\n#mapping_csv = read_csv(filename)\n# create a mapping of tags to integers\ntag_mapping, _ = create_tag_mapping(mapping_csv)\n# create a mapping of filenames to tag lists\nfile_mapping = create_file_mapping(mapping_csv)\n# load the jpeg images\n#folder = '/kaggle/input/planets-dataset/planet/planet/train-jpg/'\nX, y = load_dataset(folder, file_mapping, tag_mapping)\nprint(X.shape, y.shape)\n# save both arrays to one file in compressed format\nsavez_compressed('planet_data.npz', X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del folder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del filename","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the final model to file\nfrom numpy import load\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD\n \n# load train and test dataset\ndef load_dataset():\n\t# load dataset\n\tdata = load('planet_data.npz')\n\tX, y = data['arr_0'], data['arr_1']\n\treturn X, y\n#gc.collect() #Clearing \n# define cnn model\ndef define_model(in_shape=(64, 64, 3), out_shape=17):\n\t# load model\n\tmodel = VGG16(include_top=False, input_shape=in_shape)\n\t# mark loaded layers as not trainable\n\tfor layer in model.layers:\n\t\tlayer.trainable = False\n\t# allow last vgg block to be trainable\n\tmodel.get_layer('block5_conv1').trainable = True\n\tmodel.get_layer('block5_conv2').trainable = True\n\tmodel.get_layer('block5_conv3').trainable = True\n\tmodel.get_layer('block5_pool').trainable = True\n\t# add new classifier layers\n\tflat1 = Flatten()(model.layers[-1].output)\n\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n\toutput = Dense(out_shape, activation='sigmoid')(class1)\n\t# define new model\n\tmodel = Model(inputs=model.inputs, outputs=output)\n\t# compile model\n\topt = SGD(lr=0.01, momentum=0.9)\n\tmodel.compile(optimizer=opt, loss='binary_crossentropy')\n\treturn model\n \n# run the test harness for evaluating a model\ndef run_test_harness():\n\t# load dataset\n\tX, y = load_dataset()\n#gc.collect() #Clearing\n\t# create data generator\n\tdatagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n\t# specify imagenet mean values for centering\n\tdatagen.mean = [123.68, 116.779, 103.939]\n\t# prepare iterator\n\ttrain_it = datagen.flow(X, y, batch_size=64)\n\t# define model\n\tmodel = define_model()\n\t# fit model\n\tmodel.fit_generator(train_it, steps_per_epoch=len(train_it), epochs=50, verbose=True)\n\t# save model\n\tmodel.save('final_model.h5')\n \n# entry point, run the test harness\nrun_test_harness()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction for a new image\nfrom pandas import read_csv\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import load_model\n#cp /input/planets-dataset/planet/planet/test-jpg/test_1.jpg\n#!cp train-jpg/train_1.jpg ./sample_image.jpg\n# create a mapping of tags to integers given the loaded mapping file\ndef create_tag_mapping(mapping_csv):\n\t# create a set of all known tags\n\tlabels = set()\n\tfor i in range(len(mapping_csv)):\n\t\t# convert spaced separated tags into an array of tags\n\t\ttags = mapping_csv['tags'][i].split(' ')\n\t\t# add tags to the set of known labels\n\t\tlabels.update(tags)\n\t# convert set of labels to a list to list\n\tlabels = list(labels)\n\t# order set alphabetically\n\tlabels.sort()\n\t# dict that maps labels to integers, and the reverse\n\tlabels_map = {labels[i]:i for i in range(len(labels))}\n\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n\treturn labels_map, inv_labels_map\n\n# convert a prediction to tags\ndef prediction_to_tags(inv_mapping, prediction):\n\t# round probabilities to {0, 1}\n\tvalues = prediction.round()\n\t# collect all predicted tags\n\ttags = [inv_mapping[i] for i in range(len(values)) if values[i] == 1.0]\n\treturn tags\n\n# load and prepare the image\ndef load_image(filename):\n\t# load the image\n\timg = load_img(filename, target_size=(64, 64))\n\t# convert to array\n\timg = img_to_array(img)\n\t# reshape into a single sample with 3 channels\n\timg = img.reshape(1, 64, 64, 3)\n\t# center pixel data\n\timg = img.astype('float32')\n\timg = img - [123.68, 116.779, 103.939]\n\treturn img\n\n# load an image and predict the class\ndef run_example(inv_mapping):\n\t# load the image\n\timg = load_image('../input/planets-dataset/planet/planet/test-jpg/test_1.jpg')\n\t# load model\n\tmodel = load_model('final_model.h5')\n\t# predict the class\n\tresult = model.predict(img)\n\tprint(result[0])\n\t# map prediction to tags\n\ttags = prediction_to_tags(inv_mapping, result[0])\n\tprint(tags)\n\n# load the mapping file\nfilename = '/kaggle/input/planets-dataset/planet/planet/test-jpg'\nmapping_csv = read_csv(filename)\n# create a mapping of tags to integers\n_, inv_mapping = create_tag_mapping(mapping_csv)\n# entry point, run the example\nrun_example(inv_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test f-beta score\nfrom numpy import load\nfrom numpy import ones\nfrom numpy import asarray\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import fbeta_score\n \n# load train and test dataset\ndef load_dataset():\n    # load dataset\n    data = load('planet_data.npz')\n    X, y = data['arr_0'], data['arr_1']\n    # separate into train and test datasets\n    trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.2, random_state=1)\n    print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n    return trainX, trainY, testX, testY\n \n# load dataset\ntrainX, trainY, testX, testY = load_dataset()\n# make all one predictions\ntrain_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\ntest_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])])\n# evaluate predictions\ntrain_score = fbeta_score(trainY, train_yhat, 2, average='samples')\ntest_score = fbeta_score(testY, test_yhat, 2, average='samples')\nprint('All Ones: train=%.3f, test=%.3f' % (train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect() #Clearing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend\n \n# calculate fbeta score for multi-class/label classification\ndef fbeta(y_true, y_pred, beta=2):\n    # clip predictions\n    y_pred = backend.clip(y_pred, 0, 1)\n    # calculate elements\n    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n    # calculate precision\n    p = tp / (tp + fp + backend.epsilon())\n    # calculate recall\n    r = tp / (tp + fn + backend.epsilon())\n    # calculate fbeta, averaged across each class\n    bb = beta ** 2\n    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n    return fbeta_score\n\n# evaluate predictions with keras\ntrain_score = fbeta(backend.variable(trainY), backend.variable(train_yhat))\ntest_score = fbeta(backend.variable(testY), backend.variable(test_yhat))\nprint('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect() #Clearing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nfrom numpy import load\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n# define cnn model\ndef define_model(in_shape=(32,32, 3), out_shape=17):\n    model = VGG16(include_top=False, input_shape=in_shape)\n    # mark loaded layers as not trainable\n    for layer in model.layers:\n        layer.trainable = False\n    # allow last vgg block to be trainable\n    model.get_layer('block5_conv1').trainable = True\n    model.get_layer('block5_conv2').trainable = True\n    model.get_layer('block5_conv3').trainable = True\n    model.get_layer('block5_pool').trainable = True\n    # add new classifier layers\n    flat1 = Flatten()(model.layers[-1].output)\n    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n    output = Dense(out_shape, activation='sigmoid')(class1)\n    # define new model\n    model = Model(inputs=model.inputs, outputs=output)\n    # compile model\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n    return model\n\n# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n    # plot loss\n    plt.subplot(211)\n    plt.title('Cross Entropy Loss')\n    plt.plot(history.history['loss'], color='blue', label='train')\n    plt.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    plt.subplot(212)\n    plt.title('Fbeta')\n    plt.plot(history.history['fbeta'], color='blue', label='train')\n    plt.plot(history.history['val_fbeta'], color='orange', label='test')\n    # save plot to file\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_plot.png')\n    plt.close()\n\n# run the test for evaluating a model\ndef run_test():\n\t# load dataset\n\t#trainX, trainY, testX, testY = load_dataset()\n\t# create data generator\n\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n\ttest_datagen = ImageDataGenerator(featurewise_center=True)\n\t# specify imagenet mean values for centering\n\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n\ttest_datagen.mean = [123.68, 116.779, 103.939]\n\t# prepare iterators\n\ttrain_it = train_datagen.flow(trainX, trainY, batch_size=128)\n\ttest_it = test_datagen.flow(testX, testY, batch_size=128)\n\t# define model\n\tmodel = define_model()\n\t# fit model\n\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose= True)\n    # evaluate model\n\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=True)\n\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n\t# learning curves\n\tsummarize_diagnostics(history)\n \n # entry point, run the test harness\nrun_test()\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lkjhg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD\nimport sys\nfrom numpy import load\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend\n#import gc  #importing the module\n#gc.collect() #Clearing \n\n\n# load train and test dataset\n#def load_dataset():\n    # load dataset\n #   data = load('planet_data.npz')\n  #  X, y = data['arr_0'], data['arr_1']\n    # separate into train and test datasets\n   # trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n    #print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n    #return trainX, trainY, testX, testY\n\n#gc.collect() #Clearing\n# calculate fbeta score for multi-class/label classification\n#def fbeta(y_true, y_pred, beta=2):\n    # clip predictions\n #   y_pred = backend.clip(y_pred, 0, 1)\n    # calculate elements\n  #  tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n   # fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n  #  fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n    # calculate precision\n   # p = tp / (tp + fp + backend.epsilon())\n    # calculate recall\n   # r = tp / (tp + fn + backend.epsilon())\n    # calculate fbeta, averaged across each class\n    #bb = beta ** 2\n    #fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n    #return fbeta_score\n\n# define cnn model\ndef define_model(in_shape=(128, 128, 3), out_shape=17):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(out_shape, activation='sigmoid'))\n    # compile model\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n    return model\n\n\n# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(history.history['loss'], color='blue', label='train')\n    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Fbeta')\n    pyplot.plot(history.history['fbeta'], color='blue', label='train')\n    pyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n    # save plot to file\n    filename = sys.argv[0].split('/')[-1]\n    pyplot.savefig(filename + '_plot.png')\n    pyplot.close()\n\n#gc.collect() #Clearing    \n# run the test harness for evaluating a model\ndef run_test_harness():\n    # load dataset\n    trainX, trainY, testX, testY = load_dataset()\n    gc.collect() #Clearing\n    # create data generator\n    datagen = ImageDataGenerator(rescale=1.0/255.0)\n    gc.collect() #Clearing \n    # prepare iterators\n    train_it = datagen.flow(trainX, trainY, batch_size=32)\n    test_it = datagen.flow(testX, testY, batch_size=32)\n    # define model\n    model = define_model()\n    # fit model\n    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n                                  validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)\n    # evaluate model\n    loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n    print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n    # learning curves\n    summarize_diagnostics(history)\n \n# entry point, run the test harness\nrun_test_harness()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}