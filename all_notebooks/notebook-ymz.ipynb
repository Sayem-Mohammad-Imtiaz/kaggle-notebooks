{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle\nimport pandas as pd\nimport pickle\nfrom matplotlib.pyplot import MultipleLocator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T05:39:34.491781Z","iopub.execute_input":"2021-06-03T05:39:34.492219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.TPUStrategy(resolver)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"读入数据\"\"\"\nthe_data_a = pd.read_csv(r'../input/datasets-ymz/train_4.csv')\nthe_data_b = pd.read_csv(r'../input/datasets-ymz/test_4.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = the_data_a.iloc[:105000,:]\nprint(len(train_data))\n\nvalidation_data = the_data_a.iloc[105000:,:]\nprint(len(validation_data))\n\ntest_data = the_data_b.iloc[:,:]\nprint(len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data = the_data_a \n# validation_data = the_data_b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载数据集train_data\"\"\"\nTrain_label_Sbp = train_data.iloc[:,1800]\nTrain_label_Sbp = Train_label_Sbp.values\n\nTrain_label_Dbp = train_data.iloc[:,1801]\nTrain_label_Dbp = Train_label_Dbp.values\n\nTrain_Dev_One = train_data.iloc[:,600:1200]\nTrain_Dev_One = Train_Dev_One.values\n\nTrain_Dev_Two = train_data.iloc[:,1200:1800]\nTrain_Dev_Two = Train_Dev_Two.values\n\nTrain_Data = train_data.iloc[:,:600]\nTrain_Data = Train_Data.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载数据集validation_data\"\"\"\nValidation_label_Sbp = validation_data.iloc[:,1800]\nValidation_label_Sbp = Validation_label_Sbp.values\n\nValidation_label_Dbp = validation_data.iloc[:,1801]\nValidation_label_Dbp = Validation_label_Dbp.values\n\nValidation_Dev_One = validation_data.iloc[:,600:1200]\nValidation_Dev_One = Validation_Dev_One.values\n\nValidation_Dev_Two = validation_data.iloc[:,1200:1800]\nValidation_Dev_Two = Validation_Dev_Two.values\n\nValidation_Data = validation_data.iloc[:,:600]\nValidation_Data = Validation_Data.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"数据格式、尺寸\"\"\"\nprint(\"data_information:\")\nprint(Train_Data.shape)\nprint(Train_Dev_One.shape)\nprint(Train_Dev_Two.shape)\nprint(Train_label_Sbp.shape)\nprint(Train_label_Dbp.shape)\n\nprint(\"data_information:\")\nprint(Validation_Data.shape)\nprint(Validation_Dev_One.shape)\nprint(Validation_Dev_Two.shape)\nprint(Validation_label_Sbp.shape)\nprint(Validation_label_Dbp.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"注意力机制模块\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n# 注意力机制：Relu6方式\n# 激活函数 relu6\ndef relu6(x):\n    return tf.keras.activations.relu(x, max_value=6)\n#   利用relu函数乘上x模拟sigmoid\ndef hard_swish(x):\n    return x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n\n#---------------------------------------#\n#   通道注意力机制单元\n#   利用两次全连接算出每个通道的比重\n#   可以连接在任意特征层后面\n#---------------------------------------#\n\ndef squeeze(inputs):\n    input_channels = int(inputs.shape[-1])\n    \n    x = layers.GlobalAveragePooling1D()(inputs)\n\n    x = layers.Dense(int(input_channels/4))(x)\n    x = relu6(x)\n\n    x = layers.Dense(input_channels)(x)\n    x = hard_swish(x)\n\n    x = layers.Reshape((1, input_channels))(x)\n    \n    x = layers.Multiply()([inputs, x])\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"下采样\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n#基础层—3x3_3x3—下采样——改变卷积层步长的方式\n#ps:目前试验结果中最好的下采样方式\ndef DownSampling(inputs,filters,stride_padding='same',Drop=False):\n    layer = layers.Conv1D(filters,3,padding='same')(inputs)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    \n    res_layer = layers.Conv1D(filters,1,padding='same')(inputs)\n    res_layer = layers.BatchNormalization()(res_layer)\n    layer = layers.add([layer,res_layer])\n    \n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    if Drop==True:\n        layer = layers.Dropout(0.25)(layer)\n    \n    pool_inputs = layers.Conv1D(filters,3,padding=stride_padding,strides=2)(layer)\n    pool = layers.BatchNormalization()(pool_inputs)\n    pool = layers.Activation(tf.nn.relu)(pool)\n\n    pool = layers.Conv1D(filters,3,padding='same')(pool)\n    pool = layers.BatchNormalization()(pool)\n    \n    res_pool = layers.Conv1D(filters,1,padding='same')(pool_inputs)\n    res_pool = layers.BatchNormalization()(res_pool)\n    pool = layers.add([pool,res_pool])\n    \n    pool = layers.Activation(tf.nn.relu)(pool)\n    \n    \n    return layer,pool\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"上采样\"\"\"\n#基础层上采样\ndef UpSampling(inputs,con_input,filters,need_zero=False):\n    \n    \n    up_layer = layers.UpSampling1D(size=2)(inputs)\n    \n    if need_zero==True:\n        up_layer = layers.ZeroPadding1D((0,1))(up_layer)\n    \n    layer = layers.Conv1D(filters,2,padding='same')(up_layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.Concatenate(axis=2)([layer,con_input])\n    \n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    output = layers.Activation(tf.nn.relu)(layer)\n    \n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x=32\n# def create_model():\n#     inputs_1 = keras.Input(shape=(600,1),name='inputs_1')\n#     inputs_dev1 = keras.Input(shape=(600,1),name='inputs_dev1')\n#     inputs_dev2 = keras.Input(shape=(600,1),name='inputs_dev2')\n    \n#     inputs = layers.Concatenate(axis=2)([inputs_1,inputs_dev1,inputs_dev2])\n\n#     conv1 = layers.Conv1D(x*2,3,padding='same')(inputs)\n#     conv1 = layers.BatchNormalization()(conv1)\n#     conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n#     conv1 = layers.Conv1D(x*2,3,padding='same')(conv1)\n#     conv1 = layers.BatchNormalization()(conv1)\n#     conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n#     pool1 = layers.MaxPooling1D(pool_size=2)(conv1)\n    \n#     conv2 = layers.Conv1D(x*4,3,padding='same')(pool1)\n#     conv2 = layers.BatchNormalization()(conv2)\n#     conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n#     conv2 = layers.Conv1D(x*4,3,padding='same')(conv2)\n#     conv2 = layers.BatchNormalization()(conv2)\n#     conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n#     pool2 = layers.MaxPooling1D(pool_size=2)(conv2)\n    \n    \n#     conv3 = layers.Conv1D(x*8,3,padding='same')(pool2)\n#     conv3 = layers.BatchNormalization()(conv3)\n#     conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n#     conv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\n#     conv3 = layers.BatchNormalization()(conv3)\n#     conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n#     pool3 = layers.MaxPooling1D(pool_size=2)(conv3)\n    \n    \n#     conv4 = layers.Conv1D(x*16,3,padding='same')(pool3)\n#     conv4 = layers.BatchNormalization()(conv4)\n#     conv4 = layers.Activation(tf.nn.relu)(conv4)\n\n#     conv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\n#     conv4 = layers.BatchNormalization()(conv4)\n#     conv4 = layers.Activation(tf.nn.relu)(conv4)\n    \n#     conv4 = layers.Dropout(0.5)(conv4)\n#     ####\n#     pool4 = layers.MaxPooling1D(pool_size=2)(conv4)\n    \n\n    \n#     conv5 = layers.Conv1D(x*32,3,padding='same')(pool4)\n#     conv5 = layers.BatchNormalization()(conv5)\n#     conv5 = layers.Activation(tf.nn.relu)(conv5)\n\n#     conv5 = layers.Conv1D(x*32,3,padding='same')(conv5)\n#     conv5 = layers.BatchNormalization()(conv5)\n#     conv5 = layers.Activation(tf.nn.relu)(conv5)\n    \n#     conv5 = layers.Dropout(0.5)(conv5)\n    \n    \n#     ####\n#     conv5 = layers.UpSampling1D(size=2)(conv5)\n#     conv5 = layers.ZeroPadding1D((0,1))(conv5)\n    \n#     conv5 = layers.Conv1D(x*16,2,padding='same')(conv5)\n    \n#     up6 = layers.Concatenate(axis=2)([conv5,conv4])\n    \n#     conv6 = layers.Conv1D(x*16,3,padding='same')(up6)\n#     conv6 = layers.BatchNormalization()(conv6)\n#     conv6 = layers.Activation(tf.nn.relu)(conv6)\n\n#     conv6 = layers.Conv1D(x*16,3,padding='same')(conv6)\n#     conv6 = layers.BatchNormalization()(conv6)\n#     conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n#     ####\n#     conv6 = layers.UpSampling1D(size=2)(conv6)\n    \n#     conv6 = layers.Conv1D(x*8,2,padding='same')(conv6)\n#     conv6 = layers.BatchNormalization()(conv6)\n#     conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n#     up7 = layers.Concatenate(axis=2)([conv6,conv3])\n\n#     conv7 = layers.Conv1D(x*8,3,padding='same')(up7)\n#     conv7 = layers.BatchNormalization()(conv7)\n#     conv7 = layers.Activation(tf.nn.relu)(conv7)\n\n#     conv7 = layers.Conv1D(x*8,3,padding='same')(conv7)\n#     conv7 = layers.BatchNormalization()(conv7)\n#     conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n#     ####\n    \n#     conv7 = layers.UpSampling1D(size=2)(conv7)\n    \n#     conv7 = layers.Conv1D(x*4,2,padding='same')(conv7)\n#     conv7 = layers.BatchNormalization()(conv7)\n#     conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n#     up8 = layers.Concatenate(axis=2)([conv7,conv2])\n\n#     conv8 = layers.Conv1D(x*4,3,padding='same')(up8)\n#     conv8 = layers.BatchNormalization()(conv8)\n#     conv8 = layers.Activation(tf.nn.relu)(conv8)\n\n#     conv8 = layers.Conv1D(x*4,3,padding='same')(conv8)\n#     conv8 = layers.BatchNormalization()(conv8)\n#     conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n#     ####\n    \n#     conv8 = layers.UpSampling1D(size=2)(conv8)\n    \n#     conv8 = layers.Conv1D(x*2,2,padding='same')(conv8)\n#     conv8 = layers.BatchNormalization()(conv8)\n#     conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n\n#     up9 = layers.Concatenate(axis=2)([conv8,conv1])\n\n#     conv9 = layers.Conv1D(x*2,3,padding='same')(up9)\n#     conv9 = layers.BatchNormalization()(conv9)\n#     conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n#     conv9 = layers.Conv1D(x*2,3,padding='same')(conv9)\n#     conv9 = layers.BatchNormalization()(conv9)\n#     conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n\n#     com_layer = layers.GlobalAveragePooling1D()(conv9)\n    \n#     com_layer = layers.Dense(32,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.4))(com_layer)\n    \n#     outputs_sbp = layers.Dense(1,name='Sbp')(com_layer)\n#     outputs_dbp = layers.Dense(1,name='Dbp')(com_layer)\n\n#     model = keras.Model(inputs=[inputs_1,inputs_dev1,inputs_dev2],outputs=[outputs_sbp,outputs_dbp])\n    \n#     return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"注意力机制模块\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n# 注意力机制：Relu6方式\n# 激活函数 relu6\ndef relu6(x):\n    return tf.keras.activations.relu(x, max_value=6)\n#   利用relu函数乘上x模拟sigmoid\ndef hard_swish(x):\n    return x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n\n#---------------------------------------#\n#   通道注意力机制单元\n#   利用两次全连接算出每个通道的比重\n#   可以连接在任意特征层后面\n#---------------------------------------#\n\ndef squeeze(inputs):\n    input_channels = int(inputs.shape[-1])\n    \n    x = layers.GlobalAveragePooling1D()(inputs)\n\n    x = layers.Dense(int(input_channels/4))(x)\n    x = relu6(x)\n\n    x = layers.Dense(input_channels)(x)\n    x = hard_swish(x)\n\n    x = layers.Reshape((1, input_channels))(x)\n    #print(x)\n    #print(inputs)\n    x = layers.Multiply()([inputs, x])\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"下采样\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n#基础层—3x3_3x3—下采样——改变卷积层步长的方式\n#ps:目前试验结果中最好的下采样方式\ndef DownSampling(inputs,filters,stride_padding='same',Drop=False):\n    layer = layers.Conv1D(filters,3,padding='same')(inputs)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    \n    res_layer = layers.Conv1D(filters,1,padding='same')(inputs)\n    res_layer = layers.BatchNormalization()(res_layer)\n    layer = layers.add([layer,res_layer])\n    \n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    if Drop==True:\n        layer = layers.Dropout(0.25)(layer)\n    \n    pool_inputs = layers.Conv1D(filters,3,padding=stride_padding,strides=2)(layer)\n    pool = layers.BatchNormalization()(pool_inputs)\n    pool = layers.Activation(tf.nn.relu)(pool)\n\n    pool = layers.Conv1D(filters,3,padding='same')(pool)\n    pool = layers.BatchNormalization()(pool)\n    \n    res_pool = layers.Conv1D(filters,1,padding='same')(pool_inputs)\n    res_pool = layers.BatchNormalization()(res_pool)\n    pool = layers.add([pool,res_pool])\n    \n    pool = layers.Activation(tf.nn.relu)(pool)\n    \n    \n    return layer,pool","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"上采样\"\"\"\n# #基础层上采样resblock\n# def UpSampling(inputs,con_input,filters,need_zero=False):\n    \n    \n#     up_layer = layers.UpSampling1D(size=2)(inputs)\n    \n#     if need_zero==True:\n#         up_layer = layers.ZeroPadding1D((0,1))(up_layer)\n    \n#     layer = layers.Conv1D(filters,2,padding='same')(up_layer)\n#     layer = layers.BatchNormalization()(layer)\n#     layer = layers.Activation(tf.nn.relu)(layer)\n    \n#     layer = layers.Concatenate(axis=2)([layer,con_input])\n    \n#     layer = layers.Conv1D(filters,3,padding='same')(layer)\n#     layer = layers.BatchNormalization()(layer)\n#     layer = layers.Activation(tf.nn.relu)(layer)\n\n#     layer = layers.Conv1D(filters,3,padding='same')(layer)\n#     layer = layers.BatchNormalization()(layer)\n#     layer = layers.Activation(tf.nn.relu)(layer)\n#     #output = layers.Activation(tf.nn.relu)(layer)\n#     #+\n#     res_layer = layers.Conv1D(filters,1,padding='same')(up_layer)\n#     res_layer = layers.BatchNormalization()(res_layer)\n#     layer = layers.add([layer,res_layer])\n    \n#     output = layers.Activation(tf.nn.relu)(layer)\n    \n#     return output\n\n\"\"\"上采样\"\"\"\n#基础层上采样\ndef UpSampling(inputs,con_input,filters,need_zero=False):\n    \n    \n    up_layer = layers.UpSampling1D(size=2)(inputs)\n    \n    if need_zero==True:\n        up_layer = layers.ZeroPadding1D((0,1))(up_layer)\n    \n    layer = layers.Conv1D(filters,2,padding='same')(up_layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.Concatenate(axis=2)([layer,con_input])\n    \n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    output = layers.Activation(tf.nn.relu)(layer)\n    \n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    #双输入模型\n    inputs_1 = keras.Input(shape=(600,1),name='inputs_1')\n    inputs_dev1 = keras.Input(shape=(600,1),name='inputs_dev1')\n    inputs_dev2 = keras.Input(shape=(600,1),name='inputs_dev2')\n    \n    inputs = layers.Concatenate(axis=2)([inputs_1,inputs_dev1,inputs_dev2])\n    \n    #下采样\n    layer1,pool1 = DownSampling(inputs,filters=64)\n    \n    layer2,pool2 = DownSampling(pool1,filters=128)\n        \n    layer3,pool3 = DownSampling(pool2,filters=256)\n    \n    layer4,pool4 = DownSampling(pool3,filters=512,stride_padding='valid',Drop=False)\n    \n    #最后一层不需要Pooling\n    layer51,pool51 = DownSampling(pool4,filters=1024,Drop=False)\n    layer52,pool52 = DownSampling(pool4,filters=1024,Drop=False)\n    layer53,pool53 = DownSampling(pool4,filters=1024,Drop=False)\n    \n    layer51 = squeeze(layer51)\n    layer52 = squeeze(layer52)\n    layer53 = squeeze(layer53)\n    \n    layer51 = layers.BatchNormalization()(layer51)\n    layer52 = layers.BatchNormalization()(layer52)\n    layer53 = layers.BatchNormalization()(layer53)\n    layer5 = layers.add([layer51,layer52])\n    layer5 = layers.add([layer5,layer53])\n    \n    #layer5 = layers.BatchNormalization()(layer5)\n    \n    #上采样\n    \n    #attention\n    layer4 = squeeze(layer4)\n    \n    layer44 = UpSampling(layer5,layer4,512,need_zero=True)\n    \n    #attention\n    layer3 = squeeze(layer3)\n                     \n    layer33 = UpSampling(layer44,layer3,256)\n    \n    #attention\n    layer2 = squeeze(layer2)\n    \n    layer22 = UpSampling(layer33,layer2,128)\n    \n    #attention\n    layer1 = squeeze(layer1)\n    \n    layer11 = UpSampling(layer22,layer1,64)\n    \n    #layer1= FinalConv1D(layer1,filters=64)\n    \n    #layer1 = squeeze(layer1)\n    \n    layer = layers.GlobalAveragePooling1D()(layer11)\n\n    outputs_sbp = layers.Dense(1,name='Sbp')(layer)\n    outputs_dbp = layers.Dense(1,name='Dbp')(layer)\n\n    model = keras.Model(inputs=[inputs_1,inputs_dev1,inputs_dev2],outputs=[outputs_sbp,outputs_dbp])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"自定义评价指标模块\"\"\"\ndef standard_deviation(y_true, y_pred):\n    u = keras.backend.mean(y_pred-y_true)\n    return keras.backend.sqrt(keras.backend.mean(keras.backend.square((y_pred-y_true) - u)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"回调函数\"\"\"\n#保存迭代周期内最好的模型\ncheckpoint_filepath = r'./model_struction.h5'\nSave_epochs = 100 #迭代多少层保存一次模型\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    # save_weights_only=True,\n    monitor='val_Sbp_mean_absolute_error',\n    mode='min',\n    save_best_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"建立模型\"\"\"\nwith strategy.scope():\n    model = create_model()\n    \n    model.compile(loss={'Sbp':\"mse\",'Dbp':\"mse\"}, optimizer=keras.optimizers.Adam(lr=0.0001),metrics=[tf.keras.metrics.MeanAbsoluteError(),standard_deviation])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"保存模型结构图片\"\"\"\ntf.keras.utils.plot_model(model, to_file=r'./model_graph.png', show_shapes=True, show_layer_names=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #1\n# history = model.fit({'inputs_1':Train_Data},{'Sbp':Train_label_Sbp,'Dbp':Train_label_Dbp},\n#                     batch_size=128*8,\n#                     epochs=500,\n#                     callbacks=model_checkpoint_callback,\n#                     validation_data=({'inputs_1':Validation_Data},{'Sbp':Validation_label_Sbp,'Dbp':Validation_label_Dbp})\n#                     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3\nhistory = model.fit({'inputs_1':Train_Data,'inputs_dev1':Train_Dev_One,'inputs_dev2':Train_Dev_Two},{'Sbp':Train_label_Sbp,'Dbp':Train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=({'inputs_1':Validation_Data,'inputs_dev1':Validation_Dev_One,'inputs_dev2':Validation_Dev_Two},{'Sbp':Validation_label_Sbp,'Dbp':Validation_label_Dbp})\n                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r'./last_model.h5.pickle', 'wb') as file_pi:\n \tpickle.dump(history.history, file_pi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}