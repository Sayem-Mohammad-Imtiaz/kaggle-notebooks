{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Membandingkan metode AdaBoost, RandomForest dan GradientBoost dengan dataset Indian Liver Disease\n\nMari kita fahami sedikit metode algoritma yang digunakan\n1. AdaBoost\n> Merupakan salah satu algoritma untuk model klasifikasi (supervised) pada data mining. Algoritma ini mengkombinasikan algoritma classifier lemah kemudian disusun menjadi algoritma classifier yang kuat. Adaboost ini men-train ulang algoritma sebelumnya berdasarkan nilai akurasi sebelumnya. Untuk menggunakan metode ini, pertama adalah lakukan dengan training data dengan metode classifier lain dulu dan ukur nilai akurasinya, jika akurasi 50% ini dinamakan bobot nol (weight zero) dan <50% maka ini dinamakan bobot negatif (weight negative). Secara garis bersar, Adaboost ini seperti Random Forest memberikan hasil yang lebih akurat karena bergantung pada banyak pengklasifikasi yang lemah untuk keputusan akhir. AdaBoost paling baik digunakan untuk meningkatkan kinerja Decision Tree pada masalah klasifikasi biner. Pada Adaboost Tree biasanya berupa simpul dan dua daun, yang disebut tunggul.\nAdaBoost dimulai dengan membangun pohon pendek yang disebut tunggul, dari data pelatihan. Dan jumlah mengatakan tunggul pada hasil akhir didasarkan pada seberapa baik itu dikompensasikan untuk kesalahan sebelumnya. Kemudian AdaBoost membangun tunggul baru berdasarkan kesalahan yang dibuat oleh tunggul sebelumnya. AdaBoost terus membuat tunggul dengan cara ini sampai menghasilkan jumlah tunggul yang kita minta.\nSalah satu contoh metode Adaboost adalah untuk sistem pengenalan wajah.\nSumber:\n[1](https://text-id.123dok.com/document/dzx31pndz-algoritma-adaboost-algoritma-viola-and-jones.html)\n[2](https://towardsdatascience.com/boosting-and-adaboost-clearly-explained-856e21152d3e)\n\n2. RandomForest\n> Random Forest (RF) adalah algoritma yang digunakan untuk mengklasifikasikan data dalam jumlah besar. Klasifikasi hutan secara acak dilakukan dengan menggabungkan pohon dengan melatih data sampel pohon. Peningkatan penggunaan pohon (pohon) akan berdampak pada diperolehnya akurasi yang lebih baik. Berdasarkan hasil pemungutan suara pohon yang terbentuk, digunakan hutan acak untuk menentukan klasifikasi. Pemenang dari pohon yang terbentuk ditentukan oleh suara terbanyak. Pohon dibangun di hutan acak hingga ukuran maksimum pohon data. Namun, struktur pohon sembarangan tidak dipangkas, yang merupakan salah satu cara untuk mengurangi kompleksitas ruang. Kembangkan dengan menerapkan metode pemilihan fitur acak untuk meminimalkan kesalahan. Pembentukan pohon (tree) dengan sample data menggunakan variable yang diambil secara acak dan menjalankan klasifikasi pada semua tree yang terbentuk. Random forest menggunakan Decision Tree untuk melakukan proses seleksi. Pohon yang dibangun dibagi secara rekursif dari data pada kelas yang sama. Pemecahan (split) digunakan untuk membagi data berdasarkan jenis atribut yang digunakan. Pembuatan decision tree pada saat penentuan klasifikasi,pohon yang buruk akan membuat prediksi acak yang saling bertentangan. Sehingga,beberapa decision tree akan menghasilkan jawaban yang baik. Random forest merupakan salah satu cara penerapan dari pendekatan diskriminasi stokastik pada klasifikasi. Proses Klasifikasi akan berjalan jika semua tree telah terbentuk.Pada saat proses klasifikasi selesai dilakukan, inisialisasi dilakukan dengan sebanyak data berdasarkan nilai akurasinya. Keuntungan penggunaan random forest yaitu mampu mengklasifiksi data yang memiliki atribut yang tidak lengkap,dapat digunakan untuk klasifikasi dan regresi akan tetapi tidak terlalu bagus untuk regresi, lebih cocok untuk pengklasifikasian data serta dapat digunakan untuk menangani data sampel yang banyak. Proses klasifikasi pada random forest berawal dari memecah data sampel yang ada kedalam decision tree secara acak. Setelah pohon terbentuk,maka akan dilakukan voting pada setiap kelas dari data sampel. Kemudian, mengkombinasikan vote dari setiap kelas kemudian diambil vote yang paling banyak.Dengan menggunakan random forest pada klasifikasi data maka, akan menghasilkan vote yang paling baik.\n\n3. GradientBoost\n> Gradient boosting adalah algoritma machine learning yang menggunakan ensamble dari decision tree untuk memprediksi nilai.Gradient boosting mampu menangani complex pattern dan data ketika linear model tidak dapat menangani. Ensamble learning algorithm adalah algortima yang menggunakan banyak simple machine learning model yang bekerja bersama untuk menghasilkan prediksi yang tepat. Sturtur data dari gradient boosting adalah decision tree. Decision tree adalah model dengan cabang pilihan (decision branch), nilai ditentukan dengan mengikuti alur cabang pilihan.\n> Gradient Boosting (Peningkatan Gradien) adalah algoritma pemodelan aditif. Secara matematis, konsep pemodelan aditif adalah menambahkan sekumpulan fungsi sederhana untuk mendekati fungsi yang kompleks. Disini, fungsi yang didekati tidak lain adalah model, yang memetakan serangkaian fitur masukan ke variabel target. Peningkatan gradien adalah algoritma pembelajaran mesin yang menggunakan seluruh pohon keputusan untuk memprediksi nilai. Jika model linier tidak dapat menangani, peningkatan gradien dapat menangani pola dan data yang kompleks. Algoritme pembelajaran pengkodean adalah algoritme yang menggunakan beberapa model pembelajaran mesin sederhana yang bekerja sama untuk menghasilkan prediksi yang akurat. Struktur data peningkatan gradien adalah Decision Tree.","metadata":{}},{"cell_type":"markdown","source":"![Confusion Matrix](https://miro.medium.com/max/1704/1*IzN36IDL95ASZcV7g_KRUg.jpeg)\nsumber: [Medium](https://medium.com/@ksnugroho/confusion-matrix-untuk-evaluasi-model-pada-unsupervised-machine-learning-bc4b1ae9ae3f)","metadata":{}},{"cell_type":"code","source":"# Source: https://www.kaggle.com/faruqaziz/bandingkan-ab-gb-rf/\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt# for Plotting graphs\nimport seaborn as sns# same as matplotlib but to make life easier\n#loading the dataset\ndf=pd.read_csv(\"/kaggle/input/indian-liver-patient-records/indian_liver_patient.csv\")\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T01:47:46.115819Z","iopub.execute_input":"2021-05-22T01:47:46.118849Z","iopub.status.idle":"2021-05-22T01:47:47.41207Z","shell.execute_reply.started":"2021-05-22T01:47:46.118017Z","shell.execute_reply":"2021-05-22T01:47:47.411347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing The dataset","metadata":{}},{"cell_type":"code","source":"#describing the data\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:47:47.413526Z","iopub.execute_input":"2021-05-22T01:47:47.413919Z","iopub.status.idle":"2021-05-22T01:47:47.457851Z","shell.execute_reply.started":"2021-05-22T01:47:47.413891Z","shell.execute_reply":"2021-05-22T01:47:47.45706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing the shape of data\nprint(df.shape)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:47:47.458852Z","iopub.execute_input":"2021-05-22T01:47:47.459225Z","iopub.status.idle":"2021-05-22T01:47:47.479272Z","shell.execute_reply.started":"2021-05-22T01:47:47.459196Z","shell.execute_reply":"2021-05-22T01:47:47.478501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Starting with EDA","metadata":{}},{"cell_type":"code","source":"# visualize number of patients diagonised with liver diesease\nplt.figure(figsize=(6,6))\nax = sns.countplot(x = df['Dataset'].apply(lambda x:'(1) Liver Disease' if x == 1 else '(0) Non-Liver Disease'), hue=df['Gender'])\nax.set_xlabel('Kondisi Pasien')\nfor p in ax.patches:\n  ax.annotate(f'{p.get_height()}',(p.get_x()+0.15, p.get_height()+3))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:47:47.48031Z","iopub.execute_input":"2021-05-22T01:47:47.480723Z","iopub.status.idle":"2021-05-22T01:47:47.692624Z","shell.execute_reply.started":"2021-05-22T01:47:47.480692Z","shell.execute_reply":"2021-05-22T01:47:47.691764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing data with liver disease along with Gender\nplt.figure(figsize=(6,6))\nax = sns.countplot(x = df['Dataset'].apply(lambda x:'Normal' if x == 1 else 'Liver Disease'), hue=df['Gender'])\nax.set_xlabel('Patient Condition')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:47:47.696025Z","iopub.execute_input":"2021-05-22T01:47:47.696324Z","iopub.status.idle":"2021-05-22T01:47:47.84662Z","shell.execute_reply.started":"2021-05-22T01:47:47.696294Z","shell.execute_reply":"2021-05-22T01:47:47.845688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding the Gender attribute\ndf['Gender'].replace({'Male':1,'Female':0},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:47:47.84915Z","iopub.execute_input":"2021-05-22T01:47:47.849495Z","iopub.status.idle":"2021-05-22T01:47:47.856005Z","shell.execute_reply.started":"2021-05-22T01:47:47.849463Z","shell.execute_reply":"2021-05-22T01:47:47.854944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting Correlation\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),cmap='Greens',annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:47:47.857279Z","iopub.execute_input":"2021-05-22T01:47:47.857665Z","iopub.status.idle":"2021-05-22T01:47:48.770293Z","shell.execute_reply.started":"2021-05-22T01:47:47.857622Z","shell.execute_reply":"2021-05-22T01:47:48.769571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, hue='Dataset')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:47:48.771485Z","iopub.execute_input":"2021-05-22T01:47:48.771985Z","iopub.status.idle":"2021-05-22T01:48:15.223306Z","shell.execute_reply.started":"2021-05-22T01:47:48.77195Z","shell.execute_reply":"2021-05-22T01:48:15.222297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Starting Data Preprocessing","metadata":{}},{"cell_type":"code","source":"#checking for missing values as per column\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:15.224515Z","iopub.execute_input":"2021-05-22T01:48:15.224803Z","iopub.status.idle":"2021-05-22T01:48:15.232882Z","shell.execute_reply.started":"2021-05-22T01:48:15.224777Z","shell.execute_reply":"2021-05-22T01:48:15.232011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the rows with the missing values\ndf[df['Albumin_and_Globulin_Ratio'].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:15.233779Z","iopub.execute_input":"2021-05-22T01:48:15.234062Z","iopub.status.idle":"2021-05-22T01:48:15.257041Z","shell.execute_reply.started":"2021-05-22T01:48:15.234028Z","shell.execute_reply":"2021-05-22T01:48:15.255902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets have a look for correlation of Albumin_and_Globulin_Ratio with other columns\nplt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),cmap='Greens',annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:15.258241Z","iopub.execute_input":"2021-05-22T01:48:15.25854Z","iopub.status.idle":"2021-05-22T01:48:16.36383Z","shell.execute_reply.started":"2021-05-22T01:48:15.258511Z","shell.execute_reply":"2021-05-22T01:48:16.362591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Albumin_and_Globulin_Ratio'].fillna(df['Albumin_and_Globulin_Ratio'].mean(),inplace=True)\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.365464Z","iopub.execute_input":"2021-05-22T01:48:16.365863Z","iopub.status.idle":"2021-05-22T01:48:16.376851Z","shell.execute_reply.started":"2021-05-22T01:48:16.365826Z","shell.execute_reply":"2021-05-22T01:48:16.375729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scaling the dataset using Min Max scaler:\n#Getting Numerical Columns\ncols=df.columns.to_list()\ncols.remove('Gender')\ncols.remove('Dataset')\nprint(\"Columns with numerical data:\")\ncols","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.378157Z","iopub.execute_input":"2021-05-22T01:48:16.378456Z","iopub.status.idle":"2021-05-22T01:48:16.390025Z","shell.execute_reply.started":"2021-05-22T01:48:16.378427Z","shell.execute_reply":"2021-05-22T01:48:16.389059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting Numerical columns:\ndf_numerical=df[cols]\n\n#starting scaling process:\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df_numerical)\nscaled=scaler.transform(df_numerical) #the variable scaled will be in numpy array \nx=pd.DataFrame(scaled, columns=cols) #converting the variable to dataframe.\nx['Gender']=df['Gender']# adding Gender to X or attribute list\ny=df['Dataset']# Getting the labels\n# x","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.391237Z","iopub.execute_input":"2021-05-22T01:48:16.391536Z","iopub.status.idle":"2021-05-22T01:48:16.550826Z","shell.execute_reply.started":"2021-05-22T01:48:16.39149Z","shell.execute_reply":"2021-05-22T01:48:16.549891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#moving for feature selection\nfrom sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectFromModel ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.552063Z","iopub.execute_input":"2021-05-22T01:48:16.552331Z","iopub.status.idle":"2021-05-22T01:48:16.814983Z","shell.execute_reply.started":"2021-05-22T01:48:16.552305Z","shell.execute_reply":"2021-05-22T01:48:16.814135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(x, y)\nprint(\"Showing feature importance values\")\nprint(clf.feature_importances_)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.816151Z","iopub.execute_input":"2021-05-22T01:48:16.816413Z","iopub.status.idle":"2021-05-22T01:48:16.914224Z","shell.execute_reply.started":"2021-05-22T01:48:16.816388Z","shell.execute_reply":"2021-05-22T01:48:16.91353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=SelectFromModel(clf, prefit=True) #getting features from  the above classifer as per the importances\ncols=x.columns.to_list()#getting list of columns\ntf=model.get_support()#getting which features are important\nselectedcols=[]\nfor i in range(len(cols)):\n    if tf[i]:\n        selectedcols.append(cols[i])\nprint(\"showing selected columns\")\nprint(selectedcols)\n#converting the data\nX_new = model.transform(x)\nX_new.shape ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.915352Z","iopub.execute_input":"2021-05-22T01:48:16.915835Z","iopub.status.idle":"2021-05-22T01:48:16.947584Z","shell.execute_reply.started":"2021-05-22T01:48:16.915803Z","shell.execute_reply":"2021-05-22T01:48:16.946855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying ML Algorithms","metadata":{}},{"cell_type":"code","source":"#splitting the dataset for Training and testing and using 10-fold Cross validation.\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\nkf.get_n_splits(x)\n\n#making a comparative study of 3 different ML Algorithms namely AB, Random Forest, GB\n#metrics for AdaBoost\nAB_accuracy=[]\nAB_confusion_matrix=[]\nAB_auc=[]\nAB_f1_score=[]\n\n#metrics for Random Forest\nRF_accuracy=[]\nRF_confusion_matrix=[]\nRF_auc=[]\nRF_f1_score=[]\n\n#metrics for GB\nGB_accuracy=[]\nGB_confusion_matrix=[]\nGB_auc=[]\nGB_f1_score=[]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.948869Z","iopub.execute_input":"2021-05-22T01:48:16.949393Z","iopub.status.idle":"2021-05-22T01:48:16.954927Z","shell.execute_reply.started":"2021-05-22T01:48:16.94936Z","shell.execute_reply":"2021-05-22T01:48:16.954152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initializing the models\n#importing libraries of performance metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n\n#Making the classifier Objects\nclf_ab=AdaBoostClassifier()\nclf_rf=RandomForestClassifier(max_depth=5, random_state=0)\nclf_gb=GradientBoostingClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.956169Z","iopub.execute_input":"2021-05-22T01:48:16.956721Z","iopub.status.idle":"2021-05-22T01:48:16.967179Z","shell.execute_reply.started":"2021-05-22T01:48:16.956687Z","shell.execute_reply":"2021-05-22T01:48:16.966349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count the number of folds\ni=1\n#starting the 10 fold cross validation\nfor train_index, test_index in kf.split(X_new):\n    print(\"%d Number of fold\"%i)\n    i+=1\n    #Splitting the data\n    X_train, X_test = X_new[train_index], X_new[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    #Training and Evaluating AB\n    model=clf_ab.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    AB_accuracy.append(accuracy_score(y_test,y_pred))\n    AB_confusion_matrix.append(confusion_matrix(y_test,y_pred))\n    AB_auc.append(roc_auc_score(y_test,y_pred))\n    AB_f1_score.append(f1_score(y_test,y_pred))\n    \n    #Training and Evaluating Random Forest\n    model=clf_rf.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    RF_accuracy.append(accuracy_score(y_test,y_pred))\n    RF_confusion_matrix.append(confusion_matrix(y_test,y_pred))\n    RF_auc.append(roc_auc_score(y_test,y_pred))\n    RF_f1_score.append(f1_score(y_test,y_pred))\n    \n    #Training and Evaluating GB\n    model=clf_gb.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    GB_accuracy.append(accuracy_score(y_test,y_pred))\n    GB_confusion_matrix.append(confusion_matrix(y_test,y_pred))\n    GB_auc.append(roc_auc_score(y_test,y_pred))\n    GB_f1_score.append(f1_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:16.968425Z","iopub.execute_input":"2021-05-22T01:48:16.968919Z","iopub.status.idle":"2021-05-22T01:48:21.326467Z","shell.execute_reply.started":"2021-05-22T01:48:16.96889Z","shell.execute_reply":"2021-05-22T01:48:21.32551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing the performance","metadata":{}},{"cell_type":"code","source":"#visualizing results of AdaBoost per fold\nx=list(range(1,11))\nplt.plot(x,AB_accuracy,label='Accuracy')\n# plt.plot(x,AB_confusion_matrix,label='Confusion Matrix')\nplt.plot(x,AB_auc, label='AUC')\nplt.plot(x,AB_f1_score,label='F1 Score')\nplt.title(\"Performance of AdaBoost\")\nplt.xlabel(\"Cross Validation Fold\")\nplt.ylabel(\"Performace\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:21.327689Z","iopub.execute_input":"2021-05-22T01:48:21.327942Z","iopub.status.idle":"2021-05-22T01:48:21.533119Z","shell.execute_reply.started":"2021-05-22T01:48:21.327916Z","shell.execute_reply":"2021-05-22T01:48:21.532005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ab_ac=(sum(AB_confusion_matrix)/len(AB_confusion_matrix))\nabcm= sns.heatmap(ab_ac,annot=True)\nabcm.set_title(\"AdaBoost Confusion Matrix\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:21.534673Z","iopub.execute_input":"2021-05-22T01:48:21.535083Z","iopub.status.idle":"2021-05-22T01:48:21.762053Z","shell.execute_reply.started":"2021-05-22T01:48:21.535043Z","shell.execute_reply":"2021-05-22T01:48:21.760922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing results of Random Forest per fold\nplt.plot(x,RF_accuracy,label='Accuracy')\n# plt.plot(x,RF_confusion_matrix,label='Confusion Matrix')\nplt.plot(x,RF_auc, label='AUC')\nplt.plot(x,RF_f1_score,label='F1 Score')\nplt.title(\"Performance of Random Forest\")\nplt.xlabel(\"Cross Validation Fold\")\nplt.ylabel(\"Performace\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:21.76314Z","iopub.execute_input":"2021-05-22T01:48:21.763434Z","iopub.status.idle":"2021-05-22T01:48:21.935073Z","shell.execute_reply.started":"2021-05-22T01:48:21.763407Z","shell.execute_reply":"2021-05-22T01:48:21.934139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_ac=(sum(RF_confusion_matrix)/len(RF_confusion_matrix))\nrfcm= sns.heatmap(rf_ac,annot=True)\nrfcm.set_title(\"Random Forest Confusion Matrix\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:21.936141Z","iopub.execute_input":"2021-05-22T01:48:21.936405Z","iopub.status.idle":"2021-05-22T01:48:22.168202Z","shell.execute_reply.started":"2021-05-22T01:48:21.936378Z","shell.execute_reply":"2021-05-22T01:48:22.167047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing results of GradientBoost\nplt.plot(x,GB_accuracy,label='Accuracy')\n# plt.plot(x,GB_confusion_matrix,label='Confusion Matrix')\nplt.plot(x,GB_auc, label='AUC')\nplt.plot(x,GB_f1_score,label='F1 Score')\nplt.title(\"Performance of GradientBoost\")\nplt.xlabel(\"Cross Validation Fold\")\nplt.ylabel(\"Performace\")\nplt.legend()\nplt.show","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:22.16967Z","iopub.execute_input":"2021-05-22T01:48:22.170076Z","iopub.status.idle":"2021-05-22T01:48:22.350997Z","shell.execute_reply.started":"2021-05-22T01:48:22.170031Z","shell.execute_reply":"2021-05-22T01:48:22.349981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_ac=(sum(GB_confusion_matrix)/len(GB_confusion_matrix))\ngbcm = sns.heatmap(gb_ac,annot=True)\ngbcm.set_title(\"Gradient Boost Confusion Matrix\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:22.352333Z","iopub.execute_input":"2021-05-22T01:48:22.352608Z","iopub.status.idle":"2021-05-22T01:48:22.580002Z","shell.execute_reply.started":"2021-05-22T01:48:22.352582Z","shell.execute_reply":"2021-05-22T01:48:22.579324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing average results:\nAB=[\"AB \", (sum(AB_accuracy)/len(AB_accuracy)), (sum(AB_confusion_matrix)/len(AB_confusion_matrix)), (sum(AB_auc)/len(AB_auc)), (sum(AB_f1_score)/len(AB_f1_score))]\nRF=[\"RF \", (sum(RF_accuracy)/len(RF_accuracy)), (sum(RF_confusion_matrix)/len(RF_confusion_matrix)), (sum(RF_auc)/len(RF_auc)), (sum(RF_f1_score)/len(RF_f1_score))]\nGB=[\"GB \", (sum(GB_accuracy)/len(GB_accuracy)), (sum(GB_confusion_matrix)/len(GB_confusion_matrix)), (sum(GB_auc)/len(GB_auc)), (sum(GB_f1_score)/len(GB_f1_score))]\ndata=[]\ndata.append(AB)\ndata.append(RF)\ndata.append(GB)\n#converting results to dataframe\nresults=pd.DataFrame(data,columns=[\"Algorithms\",\"Accuracy\", \"Confusion Matrix\", \"AUC\", \"F1 Score\"])\nresults","metadata":{"execution":{"iopub.status.busy":"2021-05-22T01:48:22.580964Z","iopub.execute_input":"2021-05-22T01:48:22.581396Z","iopub.status.idle":"2021-05-22T01:48:22.603306Z","shell.execute_reply.started":"2021-05-22T01:48:22.581367Z","shell.execute_reply":"2021-05-22T01:48:22.602214Z"},"trusted":true},"execution_count":null,"outputs":[]}]}