{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import unicode_literals # to print Unicode characters\n\nimport torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nfrom io import open\nimport itertools\nimport math\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nUSE_CUDA = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def printlines(file_path, n=10):\n    with open(file_path , 'rb') as datafile:\n        lines = datafile.readlines()\n        print('Shape of file is {}\\n'.format(len(lines)))\n    for line in lines[:n]:\n        print(line)\ncorpus_name = '/kaggle/input/cornell-moviedialog-corpus/'\nmovie_lines_path = '/kaggle/input/cornell-moviedialog-corpus/movie_lines.txt'\nmovie_conversations_path = '/kaggle/input/cornell-moviedialog-corpus/movie_conversations.txt'\nmovie_titles_path = '/kaggle/input/cornell-moviedialog-corpus/movie_titles_metadata.txt'\nmovie_charaters_metadata = '/kaggle/input/cornell-moviedialog-corpus/movie_characters_metadata.txt'\nprintlines(movie_conversations_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadLines(filename , fields):\n    lines = {}\n    with open(filename , 'r' , encoding = 'iso-8859-1') as f:\n        for line in f:\n            values = line.split(' +++$+++ ')\n            lineobj = {}\n            for i , field in enumerate(fields):\n                lineobj[field] = values[i]\n            lines[lineobj['lineID']] = lineobj\n    return lines\nMOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\nlines = loadLines(movie_lines_path, MOVIE_LINES_FIELDS)\nprint(lines['L1045'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadConversations(filename , lines , fields): # Loading movie_conversations to Structure the Conversations \n    conversations = []\n    with open(filename , 'r' , encoding = 'iso-8859-1') as f:\n        for line in f:\n            values = line.split(' +++$+++ ')\n            convObj = {}\n            for i , field in enumerate(fields):\n                convObj[field] = values[i]\n            utterance_id_pattern = re.compile('L[0-9]+')\n            lineIds = utterance_id_pattern.findall(convObj['utteranceIDs'])\n            convObj['lines'] = []\n            for lineId in lineIds:\n                convObj['lines'].append(lines[lineId])\n            conversations.append(convObj)\n    return conversations\nMOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n\nconversations = loadConversations(movie_conversations_path,lines, MOVIE_CONVERSATIONS_FIELDS)\nconversations[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str(codecs.decode('\\t' , 'unicode_escape'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract Sentence Pairs\ndef extractSentencePairs(conversations):\n    qa_pairs = []\n    for conversation in conversations:\n        for i in range(len(conversation['lines']) - 1):\n            inputLines = conversation['lines'][i]['text'].strip()\n            targetLines = conversation['lines'][i+1]['text'].strip()\n            if inputLines and targetLines:\n                qa_pairs.append([inputLines , targetLines])\n    return qa_pairs\n# Writing File\nwith open('formatted_movie_lines.txt' , 'w' , encoding = 'utf-8') as outputfile:\n    writer = csv.writer(outputfile ,lineterminator = '\\n' ,  delimiter = str(codecs.decode('\\t' , 'unicode_escape')))\n    for pair in extractSentencePairs(conversations):\n        writer.writerow(pair)\nprintlines('formatted_movie_lines.txt' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PAD_token = 0\nSOS_token = 1\nEOS_token = 2\nclass Voc:\n    def __init__(self , name):\n        self.name = name\n        self.trimmed = False\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\" , EOS_token : 'EOS'}\n        self.num_words = 3\n    def addSentence(self,sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n    \n    def addWord(self , word):\n        if word not in self.word2index:\n            self.word2index[word] = self.num_words\n            self.word2count[word] = 1\n            self.index2word[self.num_words] = word\n            self.num_words += 1\n        else:\n            self.word2count[word] += 1\n    def trim(self , min_count):\n#         if self.trimmed:\n#             return\n        self.trimmed = True\n        keep_words = []\n        for k,v in self.word2count.items():\n            if v >= min_count:\n                keep_words.append(k)\n        print('keep_words {} / {} = {:.4f}'.format(len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\" , EOS_token : 'EOS'}\n        self.num_words = 3\n        \n        for word in keep_words:\n            self.addWord(word)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 20\ndef unicodeToAscii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD' , s) if unicodedata.category(c) !='Mn')\n\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    s = re.sub(r\"\\s+\", r\" \", s).strip()\n    return s\ndef readVocs(datafile , corpus_name):\n    lines = open(datafile , encoding = 'utf-8').read().strip().split('\\n')\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n    voc = Voc(corpus_name)\n    return voc , pairs\ndef filterPair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\ndef loadPrepareData(corpus , corpus_name , datafile , save_dir):\n    voc , pairs = readVocs(datafile, corpus_name)\n    pairs = filterPairs(pairs)\n    for pair in pairs:\n        voc.addSentence(pair[0])\n        voc.addSentence(pair[1])\n    print(voc.num_words)\n    return voc , pairs\nvoc , pairs = loadPrepareData('' , '' , 'formatted_movie_lines.txt' , '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MIN_COUNT = 3\ndef trimRareWords(voc , pairs , MIN_COUNT):\n    voc.trim(MIN_COUNT)\n    keep_pairs = []\n    for pair in pairs:\n        input_sentence = pair[0]\n        output_sentence = pair[1]\n        keep_input = True\n        keep_output = True\n        for word in input_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_input = False\n                break\n        for word in output_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_output = False\n                break\n        if keep_input and keep_output:\n            keep_pairs.append(pair)\n    print('Trimmed from {} pairs to {} , {:.4f} of Total'.format(len(pairs) , len(keep_pairs) , len(keep_pairs)/len(pairs) ))\n    return keep_pairs\npairs = trimRareWords(voc, pairs , MIN_COUNT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def indexesFromSentence(voc , sentence):\n    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\ndef zeroPadding(l , fill_value = PAD_token):\n    return list(itertools.zip_longest(*l , fillvalue = fill_value))\ndef binaryMatrix(l, value=PAD_token):\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])\n        for token in seq:\n            if token == PAD_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m\n\ndef inputVar(l,voc):\n    indexes_batch = [indexesFromSentence(voc , sentence) for sentence in l] # Creating index matrix\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # Lenghts of each index\n    padList = zeroPadding(indexes_batch) # Zeropadding will pad the inputs\n    padVar = torch.LongTensor(padList)\n    return padVar , lengths\n\ndef outputVar(l ,voc):\n    indexes_batch = [indexesFromSentence(voc , sentence) for sentence in l]\n    max_length = max([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    mask = binaryMatrix(padList)\n    mask = torch.BoolTensor(mask)\n    padVar = torch.LongTensor(padList)\n    return padVar , mask , max_length\ndef batch2TrainData(voc , pair_batch):\n    pair_batch.sort(key = lambda x : len(x[0].split(\" \")) , reverse = True)\n    input_batch , output_batch = [] , []\n    for pair in pair_batch:\n        input_batch.append(pair[0])\n        output_batch.append(pair[1])\n    inp , lengths = inputVar(input_batch , voc)\n    output , mask , max_target_len = outputVar(output_batch , voc)\n    return inp , lengths , output , mask , max_target_len\nsmall_batch_size = 5\nbatches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\ninput_variable, lengths, target_variable, mask, max_target_len = batches\n\nprint(\"input_variable:\", input_variable)\nprint(\"lengths:\", lengths)\nprint(\"target_variable:\", target_variable)\nprint(\"mask:\", mask)\nprint(\"max_target_len:\", max_target_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#define-models\nclass EncoderRNN(nn.Module):\n    def __init__(self, hidden_size , embedding , n_layers=1 , dropout = 0):\n        super(EncoderRNN , self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n        self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout = dropout , bidirectional = True)\n    def forward(self , input_seq , input_lengths , hidden = None):\n        embedded = self.embedding(input_seq)\n        packed = nn.utils.rnn.pack_padded_sequence(embedded , input_lengths)\n        outputs , hidden = self.gru(packed , hidden)\n        outputs , _ =  nn.utils.rnn.pad_packed_sequence(outputs)\n        outputs = outputs[: , : , :self.hidden_size] + outputs[: , : , self.hidden_size:]\n        return outputs , hidden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Attention Layer\nclass Attn(nn.Module):\n    def __init__(self, method , hidden_size):\n        super(Attn , self).__init__()\n        self.method = method\n        if self.method not in ['dot' , 'general' , 'concat']:\n            raise ValueError(self.method , \"is not defined\")\n        if self.method == 'general':\n            self.attn = nn.Linear(self.hidden_size , hidden_size)\n        elif self.method == 'concat':\n            self.attn = nn.Linear(self.hidden_size*2 , hidden_size)\n            self.v = nn.Parameters(torch.FloatTensor(hidden_size))\n            \n    def dot_score(self , hidden , encoder_output):\n        return torch.sum(hidden*encoder_output , dim= 2)\n\n    def general_score(self, hidden, encoder_output):\n        energy = self.attn(encoder_output)\n        return torch.sum(hidden * energy, dim=2)\n\n    def concat_score(self, hidden, encoder_output):\n        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n        return torch.sum(self.v * energy, dim=2)\n    def forward(self, hidden , encoder_outputs):\n        if self.method == 'general':\n            attn_energies = self.general_score(hidden , encoder_outputs)\n        elif self.method == 'concat':\n            attn_energies = self.concat_score(hidden , encoder_outputs)\n        elif self.method == 'dot':\n            attn_energies = self.dot_score(hidden , encoder_outputs)\n        attn_energies = attn_energies.t()\n        return F.softmax(attn_energies , dim = 1).unsqueeze(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LuongAttnDecoderRNN(nn.Module):\n    def __init__(self , attn_model , embedding , hidden_size , output_size , n_layers = 1 , dropout = 0.1):\n        super(LuongAttnDecoderRNN , self).__init__()\n        self.attn_model = attn_model \n        self.hidden_size = hidden_size\n        self.output_size =output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.embedding = embedding \n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout = 0 , )\n        self.concat = nn.Linear(hidden_size*2 , hidden_size)\n        self.out = nn.Linear(hidden_size , output_size)\n        self.attn = Attn(attn_model , hidden_size)\n    def forward(self , input_step , last_hidden , encoder_outputs):\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        rnn_output , hidden = self.gru(embedded , last_hidden)\n        attn_weights = self.attn(rnn_output , encoder_outputs)\n        context = attn_weights.bmm(encoder_outputs.transpose(0,1)) #batch matrix-matrix product of matrices\n        rnn_output = rnn_output.squeeze(0)\n        context = context.squeeze(1)\n        concat_input = torch.cat((rnn_output , context) , 1)\n        concat_output = torch.tanh(self.concat(concat_input))\n        output = self.out(concat_output)\n        output = F.softmax(output , dim = 1)\n        return output , hidden \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def maskNLLLoss(inp , target , mask):\n    nTotal = mask.sum()\n    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n    loss = crossEntropy.masked_select(mask).mean()\n    loss = loss.to(device)\n    return loss , nTotal.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(input_variable , lengths , target_variable , mask , \n          max_target_len , encoder , decoder , embedding , encoder_optimizer , \n          decoder_optimizer , batch_size , clip , max_length = MAX_LENGTH):\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    \n    input_variable = input_variable.to(device)\n    target_variable = target_variable.to(device)\n    lengths = lengths.to(device)\n    mask = mask.to(device)\n    loss = 0\n    print_losses = []\n    n_totals = 0\n    encoder_outputs , encoder_hidden = encoder(input_variable , lengths)\n    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n    \n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n    if use_teacher_forcing:\n        for t in range(max_target_len):\n            decoder_output , decoder_hidden = decoder(decoder_input , decoder_hidden , encoder_outputs)\n            decoder_input = target_variable[t].view(1,-1)\n            mask_loss , nTotal = maskNLLLoss(decoder_output , target_variable[t] , mask[t])\n            loss +=mask_loss\n            print_losses.append(mask_loss.item()*nTotal)\n            n_totals +=nTotal\n    else:\n        for t in range(max_target_len):\n            decoder_ouput , decoder_hidden = decoder(decoder_input , decoder_hidden , encoder_outputs)\n            # No Teaching Forcing\n            _ , topi = decoder_output.topk(1)\n            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n            decoder_input = decoder_input.to(device)\n            mask_loss , nTotal = maskNLLLoss(decoder_output , target_variable[t] , mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item()*nTotal)\n            n_totals += nTotal\n    loss.backward()\n    \n    _ = nn.utils.clip_grad_norm_(encoder.parameters() , clip)\n    _ = nn.utils.clip_grad_norm_(decoder.parameters() , clip)\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    return sum(print_losses)/n_totals\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainIters(model_name , voc , pairs , encoder , decoder , encoder_optimizer , decoder_optimizer \n               , embedding , encoder_n_layers , decoder_n_layers , save_dir , n_iterations , \n              batch_size , print_every,save_every , clip , corpur_name , loadFilename):\n    training_batches = [batch2TrainData(voc , [random.choice(pairs) for _ in range(batch_size)])\n                       for _ in range(n_iteration)]\n    print('Initializing ...')\n    start_iterations = 1\n    print_loss = 0\n    if loadFilename:\n        startiterations = checkpoint['iteration'] + 1\n    print('Training ...')\n    for iteration in range(start_iterations , n_iteration + 1):\n        training_batch = training_batches[iteration - 1]\n        input_variable , lengths , target_variable , mask , max_target_len = training_batch\n        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)    \n        print_loss +=loss\n        if iteration % print_every == 0:\n            print_loss_avg = print_loss / print_every\n            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n            print_loss = 0\n        if (iteration % save_every == 0):\n            directory = os.path.join(save_dir , model_name , corpus_name , '{}-{}_{}'.format(encoder_n_layers , decoder_n_layers , hidden_size))\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            torch.save({\n                'iteration': iteration,\n                'en': encoder.state_dict(),\n                'de': decoder.state_dict(),\n                'en_opt': encoder_optimizer.state_dict(),\n                'de_opt': decoder_optimizer.state_dict(),\n                'loss': loss,\n                'voc_dict': voc.__dict__,\n                'embedding': embedding.state_dict()\n            } , os.path.join(directory, '{}_{}.tar'.format(iteration , 'checkpoint')))\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation\nclass GreedySearchDecoder(nn.Module):\n    def __init__(self,encoder,decoder):\n        super(GreedySearchDecoder , self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n    \n    def forward(self , input_seq , input_length , max_length):\n        encoder_outputs , encoder_hidden = self.encoder(input_seq , input_length)\n        decoder_hidden = encoder_hidden[:decoder.n_layers]\n        decoder_input = torch.ones(1,1,device = device , dtype = torch.long)*SOS_token\n        all_tokens = torch.zeros([0] , device = device , dtype = torch.long)\n        all_scores = torch.zeros([0] , device = device)\n        for _ in range(max_length):\n            decoder_output , decoder_hidden = self.decoder(decoder_input , decoder_hidden , encoder_outputs)\n            decoder_scores , decoder_input = torch.max(decoder_output , dim = 1)\n            all_tokens = torch.cat((all_tokens, decoder_input) , dim = 0)\n            all_scores = torch.cat((all_scores, decoder_scores) , dim = 0)\n            decoder_input = torch.unsqueeze(decoder_input , 0)\n        return all_tokens , all_scores\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(encoder , decoder , searcher , voc , sentence , max_length = MAX_LENGTH):\n    indexes_batch = [indexesFromSentence(voc , sentence)]\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    input_batch = torch.LongTensor(indexes_batch).transpose(0,1)\n    input_batch = input_batch.to(device)\n    tokens , scores = searcher(input_batch , lengths , max_length)\n    decoder_words = [voc.index2word[token.item()] for token in tokens]\n    return decoder_words\n\ndef evaluateInput(encoder, decoder , seracher , voc):\n    input_sentance = ''\n    while(1):\n        try:\n            input_sentence = input('> ')\n            if input_sentence == 'q' or input_sentence == 'quit':\n                break\n            input_sentence = normalizeString(input_sentence)\n            output_words = evaluate(encoder , decoder , searcher , voc , input_sentence)\n            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n            print('Bot: ',' '.join(output_words) )\n        except KeyError:\n            print(\"Error : Encounterd Unknown Word\")\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'cb_model'\nattn_model = 'dot'\n#attn_model = 'general'\n#attn_model = 'concat'\nhidden_size = 500\nencoder_n_layers = 2\ndecoder_n_layers = 2\ndropout = 0.1\nbatch_size = 64\nsave_dir = '/'\n# Set checkpoint to load from; set to None if starting from scratch\nloadFilename = None\ncheckpoint_iter = 4000\n#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n\n\n# Load model if a loadFilename is provided\nif loadFilename:\n    # If loading on same machine the model was trained on\n    checkpoint = torch.load(loadFilename)\n    # If loading a model trained on GPU to CPU\n    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n    encoder_sd = checkpoint['en']\n    decoder_sd = checkpoint['de']\n    encoder_optimizer_sd = checkpoint['en_opt']\n    decoder_optimizer_sd = checkpoint['de_opt']\n    embedding_sd = checkpoint['embedding']\n    voc.__dict__ = checkpoint['voc_dict']\n\n\nprint('Building encoder and decoder ...')\n# Initialize word embeddings\nembedding = nn.Embedding(voc.num_words, hidden_size)\nif loadFilename:\n    embedding.load_state_dict(embedding_sd)\n# Initialize encoder & decoder models\nencoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\ndecoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\nif loadFilename:\n    encoder.load_state_dict(encoder_sd)\n    decoder.load_state_dict(decoder_sd)\n# Use appropriate device\nencoder = encoder.to(device)\ndecoder = decoder.to(device)\nprint('Models built and ready to go!')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure training/optimization\nclip = 50.0\nteacher_forcing_ratio = 1.0\nlearning_rate = 0.0001\ndecoder_learning_ratio = 5.0\nn_iteration = 2000\nprint_every = 1\nsave_every = 4000\n\n# Ensure dropout layers are in train mode\nencoder.train()\ndecoder.train()\n\n# Initialize optimizers\nprint('Building optimizers ...')\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\nif loadFilename:\n    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n\n# If you have cuda, configure cuda to call\nfor state in encoder_optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()\n\nfor state in decoder_optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()\n\n# Run training iterations\nprint(\"Starting Training!\")\ntrainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n           print_every, save_every, clip, corpus_name, loadFilename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.eval()\ndecoder.eval()\n\n# Initialize search module\nsearcher = GreedySearchDecoder(encoder, decoder)\n\n# Begin chatting (uncomment and run the following line to begin)\n# evaluateInput(encoder, decoder, searcher, voc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}