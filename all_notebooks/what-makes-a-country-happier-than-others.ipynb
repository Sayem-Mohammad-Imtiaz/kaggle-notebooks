{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n## [World happiness report](https://worldhappiness.report/ed/2020/)\n\nThe World Happiness Report is a landmark survey of the state of global happiness that ranks 156 countries by how happy their citizens perceive themselves to be. The World Happiness Report 2020 for the first time ranks cities around the world by their subjective well-being and digs more deeply into how the social, urban and natural environments combine to affect our happiness. I urge you to check out their report if you already haven't!\n\n## Data\n\nThe data is available on Kaggle [here](https://www.kaggle.com/mathurinache/world-happiness-report)\n\n## This notebook\n\nIn this notebook we explore what makes the citizens of this planet happy. We also try predicting the happiness score of hypothetical countries not mentioned in this dataset. This is obtained by hyper-parameter tuning of various regression models. \n\n## Intended viewership\n\nI invite data enthusiasts from all regions of the world to tell me something about their countries which is not reflected in the datasets."},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n1. [Initialisation](#Inititialisation)\n  * [Libraries](#Libraries)\n  * [Data loading](#Data_loading)\n2. [Exploratory data analysis](#Exploratory_data_analysis)\n  * [Data preparation](#Data_preparation)\n  * [Visualization](#Visualization)\n3. [Model development](#Model_development)\n  * [Model testing](#Model_testing)  \n  * [Benchmarking models](#Benchmarking_models)  \n  * [Feature importance](#Feature_importance)\n4. [Conclusion](#Conclusion)  "},{"metadata":{},"cell_type":"markdown","source":"# 1. Initialisation"},{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 500)\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nfrom cycler import cycler # for cycling through colors in a graph\n\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn import neighbors as neigh\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn import metrics\nimport warnings; warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data_loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_yr = pd.read_csv('../input/world-happiness-report/2015.csv')\nsecond_yr = pd.read_csv('../input/world-happiness-report/2016.csv')\nthird_yr = pd.read_csv('../input/world-happiness-report/2017.csv')\nfourth_yr = pd.read_csv('../input/world-happiness-report/2018.csv')\nfifth_yr = pd.read_csv('../input/world-happiness-report/2019.csv')\nsixth_yr = pd.read_csv('../input/world-happiness-report/2020.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory_data_analysis"},{"metadata":{},"cell_type":"markdown","source":"## Data_preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sorting data by happiness ranks and dropping the happiness rank columns\nfirst_yr.sort_values('Happiness Rank', inplace=True)\nfirst_yr.drop('Happiness Rank', inplace=True, axis=1)\nsecond_yr.sort_values('Happiness Rank', inplace=True)\nsecond_yr['Standard Error'] = (second_yr['Upper Confidence Interval'] - second_yr['Lower Confidence Interval'])/2\nsecond_yr.drop(['Happiness Rank', 'Upper Confidence Interval', 'Lower Confidence Interval'], inplace=True, axis=1)\nthird_yr.sort_values('Happiness.Rank', inplace=True)\nthird_yr['Standard Error'] = (third_yr['Whisker.high'] - third_yr['Whisker.low'])/2\nthird_yr.drop(['Happiness.Rank', 'Whisker.high', 'Whisker.low'], inplace=True, axis=1)\nfourth_yr.sort_values('Overall rank', inplace=True)\nfourth_yr.drop('Overall rank', inplace=True, axis=1)\nfifth_yr.sort_values('Overall rank', inplace=True)\nfifth_yr.drop('Overall rank', inplace=True, axis=1)\n\n#Changing column names for consistency\nfourth_yr.rename(columns={'Country or region':'Country'}, inplace=True)\nfifth_yr.rename(columns={'Country or region':'Country'}, inplace=True)\nsixth_yr.rename(columns={'Country name':'Country', 'Regional indicator':'Region'}, inplace=True)\n\n#Changing conflicting region names\nsecond_yr.Region.replace({'East Asia':'Eastern Asia', 'South Asia':'Southern Asia', 'Southeast Asia':'Southeastern Asia', 'Middle East and North Africa':'Middle East and Northern Africa'}, inplace=True)\nfirst_yr.Region.replace({'East Asia':'Eastern Asia', 'South Asia':'Southern Asia', 'Southeast Asia':'Southeastern Asia', 'Middle East and North Africa':'Middle East and Northern Africa'}, inplace=True)\nsixth_yr.Region.replace({'East Asia':'Eastern Asia', 'South Asia':'Southern Asia', 'Southeast Asia':'Southeastern Asia', 'Middle East and North Africa':'Middle East and Northern Africa'}, inplace=True)\n\n\n#Adding region names in 2017,2018, 2019 where they were missing\nregions = pd.concat([second_yr[['Country','Region']], first_yr[['Country', 'Region']], sixth_yr[['Country','Region']]])\nregions.drop_duplicates(subset='Country', keep='first')\nthird_yr = third_yr.join(regions.set_index('Country'), on='Country')\nfourth_yr = fourth_yr.join(regions.set_index('Country'), on='Country')\nfifth_yr = fifth_yr.join(regions.set_index('Country'), on='Country')\n\n#Dropping duplicate rows\nthird_yr.drop_duplicates(subset='Country', inplace=True, keep='first')\nfourth_yr.drop_duplicates(subset='Country', inplace=True, keep='first')\nfifth_yr.drop_duplicates(subset='Country', inplace=True, keep='first')\n\n\n#Filling missing Region values\nthird_yr.Region.fillna('Southeastern Asia',inplace=True)\nfourth_yr.Region[fourth_yr.Country == 'Trinidad & Tobago'] = 'Latin America and Caribbean'\nfourth_yr.Region.iloc[fourth_yr.Country == 'Northern Cyprus'] = 'Middle East and Northern Africa'\nfifth_yr.Region[fifth_yr.Country == 'Trinidad & Tobago'] = 'Latin America and Caribbean'\nfifth_yr.Region.iloc[fifth_yr.Country == 'Northern Cyprus'] = 'Middle East and Northern Africa'\nfifth_yr.Region.iloc[fifth_yr.Country == 'North Macedonia'] = 'Western Europe'\n\n#Filling the only missing value remaining\nfourth_yr['Perceptions of corruption'].fillna(method='bfill', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing the rank data in a column and setting the index to the country names"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Aggregating ranks of countries over various years\nfirst_yr[\"Rank 2015\"]  = first_yr.index\nsecond_yr[\"Rank 2016\"] = second_yr.index\nthird_yr[\"Rank 2017\"]  = third_yr.index\nfourth_yr[\"Rank 2018\"] = fourth_yr.index\nfifth_yr[\"Rank 2019\"]  = fifth_yr.index\nsixth_yr[\"Rank 2020\"]  = sixth_yr.index\n\n#Setting the index to country names\nfirst_yr.set_index('Country', inplace=True)\nsecond_yr.set_index('Country', inplace=True)\nthird_yr.set_index('Country', inplace=True)\nfourth_yr.set_index('Country', inplace=True)\nfifth_yr.set_index('Country', inplace=True)\nsixth_yr.set_index('Country', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a Rank dataframe which stores ranks of happiness index across the years 2015-2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"Rank = pd.concat([first_yr[\"Rank 2015\"], second_yr[\"Rank 2016\"], third_yr[\"Rank 2017\"], fourth_yr[\"Rank 2018\"], fifth_yr[\"Rank 2019\"], sixth_yr[\"Rank 2020\"]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rc('axes', prop_cycle=(cycler('color', [(0,0,1),(0,1,0),(1,0,0),(0,1,1),(1,0.6,0),(0.3,0.5,1),(0,0.5,0.5),(0.3,0.3,0),(0.3,0.3,0.5),(0,0,0),(0.7,0.7,0.2)])))\n\nfor i in np.arange(0,11,1):\n    Rank.iloc[i].plot(figsize=(15,8))\n    plt.text(2.7,Rank.iloc[i,3], s=Rank.index[i])\n\nplt.title('Top 10 happiest countries during 2015-2020', fontsize=28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For further analysis we are only going to look at the dataset of 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping columns which just numerically add up to the happiness index\ndataset = sixth_yr.drop(['Explained by: Log GDP per capita', 'Explained by: Social support',\n       'Explained by: Healthy life expectancy',\n       'Explained by: Freedom to make life choices',\n       'Explained by: Generosity', 'Explained by: Perceptions of corruption',\n       'Dystopia + residual','upperwhisker','lowerwhisker','Standard error of ladder score'],axis=1)\n\n#Renaming columns\ndataset.rename(columns={'Ladder score':'Happiness index'}, inplace=True)\n\n#Score of Dystopia is same for all countries so let's drop it\ndataset.drop(['Ladder score in Dystopia'], axis=1, inplace=True)\n\n#Dropping the rank column as it does not contribute to the happiness index score, it is an outcome\ndataset.drop(['Rank 2020'], axis=1, inplace=True)\n\nplt.figure(figsize=(15,8))\nsns.kdeplot(dataset[\"Happiness index\"], shade=True)\nplt.title(\"Target variable distribution\")\nplt.text(6.6,0.3, s=\"Skew\")\nplt.text(7.2,0.3,s=np.round(skew(dataset[\"Happiness index\"]), 2))\nplt.text(6.6,0.28, s=\"Kurtosis\")\nplt.text(7.2,0.28,s=np.round(kurtosis(dataset[\"Happiness index\"]),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalising the target variable\ndataset[\"Happiness index\"] = (dataset[\"Happiness index\"]-dataset[\"Happiness index\"].mean())/dataset[\"Happiness index\"].std()\nplt.figure(figsize=(15,8))\nsns.kdeplot(dataset[\"Happiness index\"], shade=True, color='seagreen')\nplt.title(\"Normalised target variable distribution\")\nplt.text(1.5,0.3, s=\"Skew\")\nplt.text(2,0.3,s=np.round(skew(dataset[\"Happiness index\"]), 2))\nplt.text(1.5,0.28, s=\"Kurtosis\")\nplt.text(2,0.28,s=np.round(kurtosis(dataset[\"Happiness index\"]),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence we can conclude that our target variable roughly follows the binomial distribution"},{"metadata":{},"cell_type":"markdown","source":"Let us see which factors have maximum correlation with happiness in 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Corelation of each factor with \"Happiness Index\"\ndataset.corr().iloc[0,1:].to_frame().style.background_gradient(cmap=\"RdBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As is evident from the correlation matrix, the factors are moderate->highly correlated with the \"Happiness index\""},{"metadata":{},"cell_type":"markdown","source":"### Let us visualize the relationship between each feature and the \"Happiness index\""},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(21,25))\n\nsns.regplot(x=dataset[\"Happiness index\"], y=dataset[\"Logged GDP per capita\"], color='purple',ax=axes[0,0])\nsns.regplot(x=dataset[\"Happiness index\"], y=dataset[\"Social support\"], color='magenta',ax=axes[0,1], order=5)\nsns.regplot(x=dataset[\"Happiness index\"], y=dataset[\"Healthy life expectancy\"], color='red',ax=axes[1,0])\nsns.regplot(x=dataset[\"Happiness index\"], y=dataset[\"Freedom to make life choices\"], color='maroon',ax=axes[1,1])\nsns.regplot(x=dataset[\"Happiness index\"], y=dataset[\"Generosity\"], color='seagreen',ax=axes[2,0],  order=3)\nsns.regplot(x=dataset[\"Happiness index\"], y=dataset[\"Perceptions of corruption\"], color='green',ax=axes[2,1], order=5)\n\n\naxes[0,0].set_title('Regression plot of GDP per capita vs Happiness index')\naxes[0,1].set_title('Regression plot of Social support vs Happiness index')\naxes[1,0].set_title('Regression plot of Healthy life expectancy vs Happiness index')\naxes[1,1].set_title('Regression plot of Freedom to make life choices vs Happiness index')\naxes[2,0].set_title('Regression plot of Generosity vs Happiness index')\naxes[2,1].set_title('Regression plot of Perceptions of corruption vs Happiness index')\n\nplt.suptitle('Regression plots of features with the target variable', fontsize=28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\n#sns.countplot(dataset[\"Region\"], orient=\"v\")\nsns.boxplot(y='Region', x=\"Happiness index\", data=dataset)\nplt.title(\"Region wise variance in happiness index\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model_development"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding the regions\ndax=dataset #Keeping regions intact for feature importance analysis later\ndataset[pd.Series(dataset.Region.unique())] = pd.get_dummies(dataset.Region)\ndataset.drop(['Region'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the test and train splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tr, x_te, y_tr, y_te = train_test_split(dataset.drop(['Happiness index'], axis=1), dataset[\"Happiness index\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model_testing"},{"metadata":{},"cell_type":"markdown","source":"We are going to try fitting the following regression models with GridSearchCV:\n    1. Linear Regression\n    2. SVM\n    3. Decision Trees\n    4. Random Forest\n    5. XGBoost\n    6. CatBoost\n    \n    There could be other models which provide better performance, let me know if your model fits better!"},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression with/without penalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(x_tr, y_tr)\nlm_score = lm.score(x_te, y_te)\n\n#Let us try with lasso and regression\n\nlas = GridSearchCV(Lasso(), param_grid={\"alpha\":np.arange(0,10,0.1)}, cv=5, verbose=0)\nlas.fit(x_tr, y_tr)\nlas_score = las.score(x_te,y_te)\n\n#Let us try with ridge regression\nrid = GridSearchCV(Ridge(), param_grid={\"alpha\":np.arange(0,10,0.1),'solver':(\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\")}, cv=5, verbose=0)\nrid.fit(x_tr, y_tr)\nrid_score = rid.score(x_te,y_te)\n\n#Let us try with ElasticNet\nela = GridSearchCV(ElasticNet(), param_grid={\"alpha\":np.arange(0,10,0.1),\"l1_ratio\":np.arange(0,10,0.1)}, cv=5, verbose=0)\nela.fit(x_tr, y_tr)\nela_score = ela.score(x_te,y_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"sv = GridSearchCV(svm.SVR(),cv=5,param_grid={\"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \"degree\":np.arange(1,10,1)}, verbose=0)\nsv.fit(x_tr,y_tr)\nsvm_score = sv.score(x_te,y_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = GridSearchCV(DecisionTreeRegressor(),cv=5,param_grid={\"criterion\":[\"mse\", \"friedman_mse\", \"mae\", \"poisson\"], \"max_depth\":np.arange(1,16,1)}, verbose=0)\ntr.fit(x_tr,y_tr)\ndt_score = tr.score(x_te,y_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = GridSearchCV(RandomForestRegressor(verbose=0), cv=5, param_grid={\"n_estimators\":[1000], \"criterion\":[\"mse\",\"mae\"]}, verbose=0)\nrf.fit(x_tr,y_tr)\nrf_score = rf.score(x_te,y_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = GridSearchCV(XGBRegressor(), cv=5, param_grid={\"booster\":[\"gbtree\"],\"eta\":[0.1],\"max_depth\":np.arange(16)}, verbose=0)\nxgb.fit(x_tr,y_tr)\nxgb_score = xgb.score(x_te,y_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\nctb=GridSearchCV(CatBoostRegressor(verbose=0), cv=5,param_grid={\"max_depth\":np.arange(6,16,1) }, verbose=0)\nctb.fit(x_tr,y_tr)\nctb_score = ctb.score(x_te,y_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Benchmarking_models"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = pd.DataFrame([lm_score, las_score, rid_score, ela_score, svm_score, dt_score, rf_score, xgb_score, ctb_score])\nscores.index=['Linear Reg','Lasso','Ridge','ElasticNet','SVM','Decision trees','Random Forest','XG Boost','CatBoost']\nscores.columns=['Score']\nscores.sort_values('Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature_importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, mutual_info_regression\nfs = SelectKBest(score_func=mutual_info_regression, k=\"all\")\nfs.fit(dax.drop(['Happiness index'],axis=1), dax[\"Happiness index\"])\nX_n = fs.transform(dax.drop(['Happiness index'],axis=1))\nscore = pd.concat([pd.DataFrame(dax.columns),pd.DataFrame(fs.scores_)],axis=1)\nscore.columns = [\"feature\",\"scores\"]\nscore = score.sort_values(\"scores\", ascending=False)\nscore = score[score.feature != \"Happiness index\"]\nsns.barplot(x=score.scores, y=score.feature)\nplt.title(\"Importance of features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nAccording to our trials, CatBoost model had the best score. According to feature importances, we can see that GDP per capita, followed by Social support have the maximum effect in determining the happiness of people in a country."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}