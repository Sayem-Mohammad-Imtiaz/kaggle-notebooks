{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font color=\"#00adb5\">üëâüë®‚Äçüíª Catching Illegal Phishing ‚ò†</font>\n* Phishing is a cybercrime in which a target or targets are contacted by email, telephone or text message by someone posing as a legitimate institution to lure individuals into providing sensitive data such as personally identifiable information, banking and credit card details, and passwords.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nimport os\n!ls ../input/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phishing_data = pd.read_csv('/kaggle/input/phising-urls/phishing_site_urls.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phishing_data.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phishing_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#f21170\">üìä Regarding Dataset</font>\n* Data is containg 5,49,346 unique entries.\n* Label column is prediction col which has 2 categories:\n1. Good - which means the urls is not containing malicious stuff and this site is not a Phishing Site. \n2. Bad - which means the urls contains malicious stuffs and this site is a Phishing Site.\n* There is no missing value in the dataset.","metadata":{}},{"cell_type":"code","source":"# To get the information about 'phishing_site_urls.csv':\nphishing_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check if there is any missing values in dataset:\nphishing_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we create a DataFrame of classes counts\nlbl_counts = pd.DataFrame(phishing_data.Label.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's visualize the target column by using seaborn:\n# use for high-level interface for drawing attractive and informative statistical graphics \nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.barplot(lbl_counts.index, lbl_counts.Label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#ff8882\"> Preprocessing</font>\n* So, now that we have the data, we have to vectorize our URLs. So, I'm using CountVectorizer to gather words using tokenizer, since there are words in urls that are more important than other words such as:\n    1. -> Virus\n    2. -> .exe\n    3. -> .dat\n* Let's convert the URLs into a vector form.","metadata":{}},{"cell_type":"markdown","source":"### <font color=\"#1597bb\">Regexp Tokenizer</font>\n* A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.","metadata":{}},{"cell_type":"code","source":"# We use Regexp tokenizers to split words from text:\nfrom nltk.tokenize import RegexpTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In this expression we are spliting only alphabets\ntokenizer = RegexpTokenizer(r'[A-Za-z]+')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''For example here you can see lots of numbers, symbols, dots, etc which\nis not important to your data so we remove this and get only strings of alphabets.''' \nprint(phishing_data.URL[0]) # This 0 is first row","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This command will only pull all the alphabet strings present in URL:\nclean_text = tokenizer.tokenize(phishing_data.URL[0]) \nprint(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#ff1a75\">‚è∞ Time module</font>","metadata":{}},{"cell_type":"code","source":"# So now we transform all the URLs to clean_text:\n# To calculate time of execution we import time\nimport time\nstart = time.time()\nphishing_data['text_tokenized'] = phishing_data.URL.map(lambda text: tokenizer.tokenize(text))\nend = time.time()\ntime_req = end - start\nformatted_time = \"{:.2f}\".format(time_req)\nprint(f\"Time required to tokenize text is: \\n{formatted_time} sec\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's check some sample results of URLs conversion to tokenize text:\nphishing_data.sample(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#f875aa\">Snowball Stemmer NLTK</font>\n* Snowball is a small string processing language designed for creating stemming algorithms for use in Information Retrieval.","metadata":{}},{"cell_type":"code","source":"# Now we use Snowball stemmer to get the root words out of tokenized text:\nfrom nltk.stem.snowball import SnowballStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I am using english language for stemming purpose you can choose any language:\nsbs = SnowballStemmer(\"english\")\n\n# we will see the execution time to stem the tokenize text:\nstart = time.time()\nphishing_data['text_stemmed'] = phishing_data['text_tokenized'].map(lambda text: [sbs.stem(word) for word in text])\nend = time.time()\ntime_req = end - start\nformatted_time = \"{:.2f}\".format(time_req)\nprint(f\"‚è≥ Time required for stemming all the tokenized text is: \\n{formatted_time} sec\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's see the sample stemmed text:\nphishing_data.sample(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So, now we join the stemmed words together as a sentence:\nstart = time.time()\nphishing_data['text_to_sent'] = phishing_data['text_stemmed'].map(lambda text: ' '.join(text))\nend = time.time()\ntime_req = end - start\nformatted_time = \"{:.2f}\".format(time_req)\nprint(f\"Time required for joining text to sentence is: \\n{formatted_time} sec\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see some sample results of joined text to sentences:\nphishing_data.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#845ec2\">Visualization Part</font>\n##### <font color=\"#3aa6c5\">It is very important to know your data and visualize it to understand it better.</font>\n    1.Let's visualize some important key words using Wordcloud.","metadata":{}},{"cell_type":"code","source":"# First we slice the classes as:\nphishing_sites = phishing_data[phishing_data.Label == 'bad']\nnot_phishing_sites = phishing_data[phishing_data.Label == 'good']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using head() function to see the top 5 URLs of phishing sites:\nphishing_sites.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using head() function to see the top 5 URLs of Not phishing sites:\nnot_phishing_sites.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's create a function to visualize the important words from URLs:\n\ndef my_wordcloud(text, mask=None, max_words=500, max_font_size=70, figure_size=(8.0, 10.0),\n                title=None, title_size=70, image_color=False):\n    \n    stopwords = set(STOPWORDS)\n    my_stopwords = {'com', 'http'}\n    stopwords = stopwords.union(my_stopwords)\n    \n    wordcloud = WordCloud(background_color='#fff', \n                         stopwords = stopwords,\n                         max_words = max_words,\n                         random_state = 42,                            \n                         mask = mask)\n    \n    wordcloud.generate(text)\n    \n    plt.figure(figsize=figure_size)\n    \n    if image_color:\n        image_color = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors),\n                  interpolation='bilinear');\n        \n        plt.title(title,\n                 fontdict={'size': title_size,\n                          'verticalalignment': 'bottom'})\n        \n    else:\n        plt.imshow(wordcloud);\n        plt.title(title,\n                 fontdict={'size': title_size,\n                          'color': '#ff3333',\n                          'verticalalignment': 'bottom'})\n\n    plt.axis('off');\n    plt.tight_layout()\n    \nd = '../input/images/'\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_data = not_phishing_sites.text_to_sent\nmy_data.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_phishing_common_text = str(my_data)\ncommon_mask = np.array(Image.open(d+'idea.png'))\nmy_wordcloud(not_phishing_common_text,\n               common_mask,\n               max_words=400, \n               max_font_size=50, \n               title = 'The Most common words use in not phishing URLs:',\n               title_size=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for phishing sites common words are:\nmy_data = phishing_sites.text_to_sent\nmy_data.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's create a wordcloud for phishing sites:\nphishing_common_words = str(my_data)\ncommon_mask = np.array(Image.open(d+'target.png'))\nmy_wordcloud(phishing_common_words, \n             common_mask,\n             max_words=500, \n             max_font_size=20, \n             title='The Most common words use in phishing URLs:', \n             title_size=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#ff3366\">Creating Model</font>\n* Using CountVectorizer is used to transform a corpora of text to a vector of term / token counts.","metadata":{}},{"cell_type":"code","source":"# create sparse matrix of words using regexptokenizes: \nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we create a CV object:\nCV = CountVectorizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(CountVectorizer())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform all text which we tokenize and stemed:\nfeature = CV.fit_transform(phishing_data.text_to_sent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert sparse matrix into array to print transformed features:\nfeature[:5].toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spliting the data between feature and target:\nfrom sklearn.model_selection import train_test_split\n\n# gives whole report about metrics (e.g, recall,precision,f1_score,c_m):\nfrom sklearn.metrics import classification_report\n\n# gives info about actual and predicted:\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data:\ntrain_X, test_X, train_Y, test_Y = train_test_split(feature, phishing_data.Label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#e6e600\">Logistic Regression</font>\n* Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.","metadata":{}},{"cell_type":"code","source":"# algo use to predict not phishing site or phishing site: \nfrom sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an object for Logistic Regression()\nlr = LogisticRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.fit(train_X, train_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we are calculating the score of tests:\nlr.score(test_X, test_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#00ffaa\">LR Score</font>\n* Logistic Regression is giving us 96% accuracy, Now we will store scores in dict to see which model perform best of all","metadata":{}},{"cell_type":"code","source":"Score_ml = {}\nScore_ml['Logistic Regression'] = np.round(lr.score(test_X, test_Y), 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Accuracy: ',lr.score(train_X, train_Y))\nprint('Testing Accuracy: ',lr.score(test_X, test_Y))\n# here we create confusion matrix:\nconf_mat = pd.DataFrame(confusion_matrix(lr.predict(test_X), test_Y),\n                       columns = ['Predicted: Phishing', 'Predicted: Not Phishing'],\n                       index = ['Actual: Phishing', 'Actual: Not Phishing'])\n\nprint('\\nClassification Report: \\n')\nprint(classification_report(lr.predict(test_X), test_Y,\n                           target_names = ['Bad', 'Good']))\n\nprint('\\nconfusion Matrix: \\n')\nplt.figure(figsize = (6, 4))\nsns.heatmap(conf_mat, annot = True, fmt='d', cmap=\"RdYlBu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#00bfff\">MultinomialNB</font>\n* Applying Multinomial Naive Bayes to NLP Problems. Naive Bayes Classifier Algorithm is a family of probabilistic algorithms based on applying Bayes' theorem with the ‚Äúnaive‚Äù assumption of conditional independence between every pair of a feature.","metadata":{}},{"cell_type":"code","source":"# nlp algo use to predict not phishing site or phishing site:\nfrom sklearn.naive_bayes import MultinomialNB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create mnb object\nmnb = MultinomialNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb.fit(train_X, train_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb.score(test_X, test_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#ff5500\">MultinomialNB Score</font>\n* MultinomialNB is giving us 95% accuracy. So, now we will store scores in dict to see which model perform best of all.","metadata":{}},{"cell_type":"code","source":"Score_ml['MultinomialNB'] = np.round(mnb.score(test_X, test_Y), 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Accuracy: ',mnb.score(train_X, train_Y))\nprint('Testing Accuracy: ',mnb.score(test_X, test_Y))\n\nconf_mat = pd.DataFrame(confusion_matrix(mnb.predict(test_X), test_Y),\n                       columns = ['Predicted: Phishing', 'Predicted: Not Phishing'],\n                       index = ['Actual: Phishing', 'Actual: Not Phishing'])\n\nprint('\\nClassification Report\\n')\nprint(classification_report(mnb.predict(test_X), test_Y,\n                           target_names = ['Bad', 'Good']))\n\nprint('\\nConfusion Matrix\\n')\nplt.figure(figsize = (6,4))\nsns.heatmap(conf_mat, annot = True, fmt='d', cmap='PuBuGn_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame.from_dict(Score_ml, \n                                 orient = 'index', \n                                 columns = ['Accuracy'])\n\nprint(f\"Actual Score of Logistic Regression: \\n{lr.score(test_X, test_Y)}\\n\")\nprint(f\"Actual Score of MultinomialNB: \\n{mnb.score(test_X, test_Y)}\\n\")\nprint(f\"Final Rounded Score: \\n{results}\")\n\nsns.set_style('darkgrid')\nsns.barplot(results.index, results.Accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color=\"#ff5500\">Best fit Model</font>\n* In the above results we can clearly see that the Logistic Regression is the best fit model with actual score of 96%.\n* So, now we make sklearn pipeline using Logistic Regression.","metadata":{}},{"cell_type":"code","source":"# Used for combining all preprocessor techniques and algorithms:\nfrom sklearn.pipeline import make_pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a pipeline:\npipeline_ls = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize, stop_words='english'), LogisticRegression())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X, test_X, train_Y, test_Y = train_test_split(phishing_data.URL, phishing_data.Label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_ls.fit(train_X, train_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_ls.score(test_X, test_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training Accuracy: \",pipeline_ls.score(train_X, train_Y))\nprint(\"Testing Accuracy: \",pipeline_ls.score(test_X, test_Y))\n\nconf_mat = pd.DataFrame(confusion_matrix(pipeline_ls.predict(test_X), test_Y), \n                       columns = [\"Predicted: Phishing\", \"Predicted: Not Phishing\"],\n                       index = [\"Actual: Phishing\", \"Actual: Not Phishing\"])\n\nprint(\"\\nClassification Report \\n\")\nprint(classification_report(pipeline_ls.predict(test_X), test_Y,\n                            target_names = ['Bad', 'Good']))\n\nprint(\"\\nConfusion Matrix \\n\")\nplt.figure(figsize = (6,4))\nsns.heatmap(conf_mat, annot = True, fmt = 'd', cmap=\"Blues\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\npickle.dump(pipeline_ls,open('phishing.pkl','wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = pickle.load(open('phishing.pkl', 'rb'))\nresult = loaded_model.score(test_X,test_Y)\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#009999\">üôåConclusion</font>\n* So now, we get an accuracy of 96%. That‚Äôs a very high value for a machine to be able to detect a phishing sites. Want to test some links to see if the model gives good predictions? Sure. Let's do it","metadata":{}},{"cell_type":"markdown","source":"##### <font color=\"#ff0000\">‚ùå Bad Links</font> \n* __These are some Phishing links!__\n   1. www.yeniik.com.tr/wp-admin/js/login.alibaba.com/login.jsp.php\n   2. www.fazan-pacir.rs/temp/libraries/ipad\n   3. www.tubemoviez.exe\n   4. www.svision-online.de/mgfi/administrator/components/com_babackup/classes/fx29id1.txt\n   ","metadata":{}},{"cell_type":"markdown","source":"##### <font color=\"#77ff33\">‚úî Good Links</font>\n* __These are some not Phishing links!__\n   1. www.youtube.com/\n   2. www.python.org/\n   3. www.google.com/\n   4. www.kaggle.com/\n   ","metadata":{}},{"cell_type":"code","source":"predict_bad = ['www.yeniik.com.tr/wp-admin/js/login.alibaba.com/login.jsp.php',\n               'www.fazan-pacir.rs/temp/libraries/ipad',\n               'www.tubemoviez.exe/',\n               'www.svision-online.de/mgfi/administrator/components/com_babackup/classes/fx29id1.txt']\n\npredict_good = ['www.youtube.com/',\n                'www.python.org/',\n                'www.google.com/',\n                'www.kaggle.com/']\n\nloaded_model = pickle.load(open('phishing.pkl', 'rb'))\n#predict_bad = vectorizers.transform(predict_bad)\n# predict_good = vectorizer.transform(predict_good)\n\nresult_1 = loaded_model.predict(predict_bad)\nresult_2 = loaded_model.predict(predict_good)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{result_1} \\n {'-'*26} \\n{result_2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# `Project Terminated Here... üòé`","metadata":{}}]}