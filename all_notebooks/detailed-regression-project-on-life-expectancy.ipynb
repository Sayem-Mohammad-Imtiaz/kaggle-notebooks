{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <font color=\"red\"> <div align=\"center\"> REGRESSION PROJECT  \n    \n## <div align=\"center\"> Life Expectancy Data\n"},{"metadata":{},"cell_type":"markdown","source":"## Contents\n1.  Introduction\n2.  The Aim of Analysis\n3.  General Information of the Data\n4.  Data Exploration\n     * 4.1. Importing an External Data Frame\n     * 4.2. Merging Two Data Frame in One\n5.  Cleaning of Row Data \n6.  Filling of the Row Data \n7.  General Looking on Life Expectancy Values Based on Regions and Years\n8.  Overview about Outliers \n     * 8.1 Winsorization\n9.  Feature Engineering\n     * 9.1 Getting PCA Values\n     * 9.2 Getting PCA Values for all Elements by Switching Variables to Dummies*\n10.  Building Models\n     * 10.1 Building Model with All Numerical Variables\n      * 10.1. a)Residual Distributions on the Model\n      * 10.1. b)Jarque Bera Test\n     * 10.2 Adding Polinomial Features\n     * 10.3 Building Polinomial Regression Models\n      * 10.3 a)Checking the Best Polinomial Degree\n      * 10.3 b)Checking the Performance of Models within Polinomial Degree\n     * 10.4 Building Ridge Regression Models\n     * 10.5 Building Lasso Regression Models\n     * 10.6 Building ElasticNet Regression Models\n11. Evaluating the Model\n12. Predicting with the Best Model\n13. Conclusions"},{"metadata":{},"cell_type":"markdown","source":"### 0. Importing Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model as lm\nimport statsmodels.api as sm\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nfrom statsmodels.tools.eval_measures import mse, rmse\n\nimport warnings\nwarnings.filterwarnings(action= \"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import style\nstyle.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\">  **1. Introduction**"},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"gray\"> **Provided Information about The preparation of Row Data:**  The Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries The datasets are made available to public for the purpose of health data analysis. The dataset related to life expectancy, health factors for 193 countries has been collected from the same WHO data repository website and its corresponding economic data was collected from United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative. It has been observed that in the past 15 years , there has been a huge development in health sector resulting in improvement of human mortality rates especially in the developing nations in comparison to the past 30 years. Therefore, in this project we have considered data from year 2000-2015 for 193 countries for further analysis. The individual data files have been merged together into a single dataset.\n\n### <font color=\"gray\">On initial visual inspection of the data showed some missing values. As the datasets were from WHO,  None of evident errors are found. It is approved by puplishers that missing data was handled in R software by using Missmap command. The result indicated that most of the missing data was for population, Hepatitis B and GDP. The missing data were from less known countries like Vanuatu, Tonga, Togo,Cabo Verde etc. Finding all data for these countries was difficult and hence, it was decided that we exclude these countries from the final model dataset. The final merged file (final dataset) consists of 22 Columns and 2938 rows which meant 20 predicting variables. All predicting variables was then divided into several broad categories:Immunization related factors, Mortality factors, Economical factors and Social factors.\n\n### <font color=\"gray\"> In order to generate our regression models, I preferred  to merged an external data set to check  values based on  regions, sub-regions and countries to have deeper view on data. It also helped me to filled missing values accurately by using 'Sub-Region' values.\n\n### <font color=\"gray\"> **The preparation on Observations before Machine Learning:** Missing values were filled by interpolate method firstly, but the rest was filled grouping by 'Sub-Region' and 'Year' columns. "},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> **2. The Aim of Analysis**"},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"gray\"> This study aims to search for the elements which effects life expectancy by using statistical tools such as MSE, R squared, RMSE, ect. on different regression models."},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\">  **3. General Information of the Data**"},{"metadata":{},"cell_type":"markdown","source":"<font color=\"gray\">Country : Country\n \nYear : Year \n\nStatus : Developed or Developing status\n\nLife expectancy : Life Expectancy in age\n\nAdult Mortality : Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\n\ninfant deaths : Number of Infant Deaths per 1000 population\n\n\nAlcohol          : Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\n\nPercentage        :  Expenditure on health as a percentage of Gross Domestic Product per capita(%)\n\nHepatitis B        : Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\n\nMeasles : Measles - number of reported cases per 1000 population\n\nBMI : Average Body Mass Index of entire population\n\nUnder-five deaths : Number of under-five deaths per 1000 population\n\nPolio : Polio (Pol3) immunization coverage among 1-year-olds (%)\n\nTotal expenditure : General government expenditure on health as a percentage of total government expenditure (%)\n\nDiphtheria :  Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n\nHIV/AIDS : Deaths per 1 000 live births HIV/AIDS (0-4 years)\n\nGDP : Gross Domestic Product per capita (in USD)\n\nPopulation : Population of the country\n\nThinness 1-19 years : Prevalence of thinness among children and adolescents for Age 10 to 19 (% )\n\nIncome composition of resources : Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n\nThinness 5-9 years   : Prevalence of thinness among children for Age 5 to 9(%)\n\nSchooling : Number of years of Schooling(years)"},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> **4. Data Exploration**"},{"metadata":{},"cell_type":"markdown","source":"#### ***Getting Data***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from subprocess import check_output\n\nprint(check_output([\"ls\", \"../input/allcsv\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"LifeExpectancyData = pd.read_csv('../input/life-expectancy-who/led.csv')\nregions = pd.read_csv('../input/allcsv/all.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***First 5 rows***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***About data***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  ***Looking null values***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Checking for column names for further steps***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData.columns ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Manipulating column names for future steps***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData.columns= ['Country', 'Year', 'Status', 'Life_Expectancy', 'Adult_Mortality',\n       'infant_deaths', 'Alcohol', 'percentage_expenditure', 'Hepatitis_B',\n       'Measles', 'BMI', 'under_five_deaths', 'Polio', 'Total_Expenditure',\n       'Diphtheria', 'HIV/AIDS', 'GDP','Population', 'thinness_1_19_years', 'thinness_5_9_years',\n       'Income_composition_of_resources', 'Schooling']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Looking NAN values with heatmap***"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_missing_values = LifeExpectancyData.isnull().sum()\nmissing_values_per = LifeExpectancyData.isnull().sum()/LifeExpectancyData.isnull().count()\nnull_values = pd.concat([total_missing_values, missing_values_per], axis=1, keys=['total_null', 'total_null_perc'])\nnull_values = null_values.sort_values('total_null', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_cell(LifeExpectancyData):\n    total_missing_values = LifeExpectancyData.isnull().sum()\n    missing_values_per = LifeExpectancyData.isnull().sum()/LifeExpectancyData.isnull().count()\n    null_values = pd.concat([total_missing_values, missing_values_per], axis=1, keys=['total_null', 'total_null_perc'])\n    null_values = null_values.sort_values('total_null', ascending=False)\n    return null_values[null_values['total_null'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(LifeExpectancyData.isnull(), cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.1. Importing an External Data Frame**\n\n<font color=\"green\"> ***Further steps requires extra tools to compare such as regions and sub regions. Lack of those information does not help us to group data on necessary areas. Therefore, I imported an external data frame with only neccessary columns.***"},{"metadata":{},"cell_type":"markdown","source":"#### ***Getting new dataset***"},{"metadata":{"trusted":true},"cell_type":"code","source":"#regions = pd.read_csv('./data/all.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Looking new dataset - first 5 rows***"},{"metadata":{"trusted":true},"cell_type":"code","source":"regions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Checking NULL values***"},{"metadata":{"trusted":true},"cell_type":"code","source":"regions[['name', 'region', 'sub-region']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Last check on column names***"},{"metadata":{"trusted":true},"cell_type":"code","source":"regions.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.2. Merging two data frame in one, LifeExpectancyData_merged, will help us to fill in accurately.**"},{"metadata":{},"cell_type":"markdown","source":"#### ***Merging datasets***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData_merged = pd.merge(LifeExpectancyData, regions[['name', 'region', 'sub-region']],\n                                     left_on='Country', right_on='name')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Looking at NAN values***"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"null_cell(LifeExpectancyData_merged)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Checking merged dataset - first 5 rows***"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"LifeExpectancyData_merged.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> **5. Cleaning of the Row Data**"},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\">***We have a lot of missing population values in many countries. However, having GDP values from population for each country can help us as well. We also have status (Developed or Developing) for each country. Therefore, I preferred to drop column from data frame.***"},{"metadata":{},"cell_type":"markdown","source":"#### ***Dropping Population column***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData_merged.drop('Population', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Looking at columns of the new merged dataset***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData_merged.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Getting NAN values from index***"},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_list = (null_cell(LifeExpectancyData_merged)).index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> **6. Filling of the Row Data**"},{"metadata":{},"cell_type":"markdown","source":"#### ***Filling NAN values with interpolate method with both option as having values for some rows in each countries***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_interpolate = LifeExpectancyData_merged.copy()\n\nfor col in fill_list:\n    df_interpolate[col] = df_interpolate.groupby(['Country'])[col].transform(lambda x: x.interpolate(limit_direction = 'both'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Checking NAN values after interpolate***"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cell(df_interpolate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_interpolate[df_interpolate['Adult_Mortality'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\">  ***Applying interpolate method on both direction with grouping by Country, does not help on missing values. It only helped to decrease number of missing values at once.\nOn those rows, there is no previous information for relevant countries. Thus, I used interpolte method with grouping by sub-region and Year.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in fill_list:\n    df_interpolate[col] = df_interpolate.groupby(['sub-region', 'Year'])[col].transform(lambda x: x.interpolate(limit_direction='both'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Checking for NAN values***"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cell(df_interpolate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> ***Now data is ready for further steps.***"},{"metadata":{},"cell_type":"markdown","source":"####  ***Getting numeric values for mathematical and statistical operations.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData_num = df_interpolate._get_numeric_data() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Correlations Between All Variables.***"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"corr_matrix = LifeExpectancyData_num.corr()\ncorr_list = corr_matrix.Life_Expectancy.abs().sort_values(ascending=False).index[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(corr_matrix, annot=True, cmap='RdBu_r')\nplt.title('Correlation Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\">   ***As we see above 'Income_composition_of_resources' and 'Schooling' have high correlation, while  'Adult_Mortality' has high negative correlation between Life Expectancy.***\n\n\n***'HIV/AIDS', 'BMI', 'Diphtheria', 'thinness_1_19_years', 'thinness_5_9_years', 'Polio', 'GDP', and 'Alcohol' have medium correlation between Life Expectancy.***\n\n***And the rest of our columns; 'percentage_expenditure', ’Hepatitis_B', 'Total_Expenditure', 'under_five_deaths', 'infant_deaths', 'Year', and 'Measles' have low correlation between Life Expectancy.***"},{"metadata":{},"cell_type":"markdown","source":"#### ***Corellations between illnesses***"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = LifeExpectancyData_num[['Hepatitis_B','Measles', 'Polio','Diphtheria','HIV/AIDS', 'thinness_1_19_years',\n                                      'thinness_5_9_years','Life_Expectancy']].corr()\ncorr_list = corr_matrix.Life_Expectancy.abs().sort_values(ascending=False).index[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(corr_matrix, annot=True, cmap='RdBu_r')\nplt.title('Correlation Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\">   ***Life Expectancy has high negative correlation with HIV/AIDS when only consedering correlations based on sicknesses***\n    \n<font color=\"green\">   ***It also has medium correlation with thinness_1_19_years and thinness_5_9_years, Diphtheria and Polio.***\n    \n<font color=\"green\">   ***Life Expectancy has low correlation with Hepatitis_B and Measles.***\n"},{"metadata":{},"cell_type":"markdown","source":"###  <div align=\"center\"> 7. General Looking on Life Expectancy Values Based on Regions and Years "},{"metadata":{},"cell_type":"markdown","source":"### General Looking on Life Expectancy in Years"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.violinplot(x=df_interpolate[\"Year\"], y=df_interpolate[\"Life_Expectancy\"], data=df_interpolate)\nplt.title('General Looking on Life Expectancy in Years')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\"> ***As we see on the violin graph, general Life Expectancy value is decreasing after 2010 till 2014. Let’s have a look more detailed.***"},{"metadata":{},"cell_type":"markdown","source":"### **Life Expectancy Values in Years by Regions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.violinplot(x=df_interpolate.loc[df_interpolate['Year']>2009][\"Year\"], \n               y=df_interpolate[\"Life_Expectancy\"],\n               hue=df_interpolate[\"region\"], \n               data=df_interpolate.loc[df_interpolate['Year']>2010], \n               palette=\"muted\")\n\nplt.title('Life Expectancy Values in Years by Regions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"green\"> ***People in Africa and Asia regions have a stable Life Expectancy in general while Ocenania and Europe regions have decreasing trend between 2010-2014.***"},{"metadata":{},"cell_type":"markdown","source":"### General View in Life Expectancy by Grouping Countries with GDP Values Based on Regions"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='Life_Expectancy', \n                y='Alcohol', \n                hue='region',\n                data=df_interpolate, \n                s=df_interpolate.GDP/100);\nplt.xlabel('Life_expectancy',size=15)\nplt.ylabel('Alcohol', size =10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> 8. Overview about Outliers and Dealing with Them"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 60\nplt.rcParams['figure.figsize'] = (8,5.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_by_nineteen_variables = ['Year', 'Life_Expectancy','Adult_Mortality', 'infant_deaths', 'Alcohol', 'percentage_expenditure',\n                                    'Hepatitis_B','Measles', 'BMI',\n                                    'under_five_deaths', 'Polio', 'Total_Expenditure','Diphtheria', 'HIV/AIDS', 'GDP',\n                                    'thinness_1_19_years', 'thinness_5_9_years', 'Income_composition_of_resources', 'Schooling'] \nplt.figure(figsize=(25,25))\n\nfor i in range(0,19):\n    plt.subplot(5, 4, i+1)\n    plt.boxplot(df_interpolate[outliers_by_nineteen_variables[i]])\n    plt.title(outliers_by_nineteen_variables[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  **8.1 Winsorization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats.mstats import winsorize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Finding best limit for Winsorize for Each Variables***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def winsor(x, multiplier=3): \n    upper= x.median() + x.std()*multiplier\n    for limit in np.arange(0.001, 0.20, 0.001):\n        if np.max(winsorize(x,(0,limit))) < upper:\n            return limit\n    return None ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#An example to get limit value for winsorization\nlimit= winsor(df_interpolate['infant_deaths'])\nprint(limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_interpolate[\"Adult_Mortality\"]        = winsorize(df_interpolate[\"Adult_Mortality\"], (0, 0.018))\ndf_interpolate[\"infant_deaths\"]          = winsorize(df_interpolate[\"infant_deaths\"], (0, 0.018))\ndf_interpolate[\"percentage_expenditure\"] = winsorize(df_interpolate[\"percentage_expenditure\"], (0, 0.036))\ndf_interpolate[\"Hepatitis_B\"]            = winsorize(df_interpolate[\"Hepatitis_B\"], (0,0.001))\ndf_interpolate[\"Measles\"]                = winsorize(df_interpolate[\"Measles\"], (0, 0.018))\ndf_interpolate[\"under_five_deaths\"]      = winsorize(df_interpolate[\"under_five_deaths\"], (0, 0.013))\ndf_interpolate[\"Polio\"]                  = winsorize(df_interpolate[\"Polio\"], (0, 0.001))\ndf_interpolate[\"Total_Expenditure\"]      = winsorize(df_interpolate[\"Total_Expenditure\"], (0, 0.011))\ndf_interpolate[\"Diphtheria\"]             = winsorize(df_interpolate[\"Diphtheria\"], (0, 0.001))\ndf_interpolate[\"HIV/AIDS\"]               = winsorize(df_interpolate[\"HIV/AIDS\"], (0, 0.030))\ndf_interpolate[\"GDP\"]                    = winsorize(df_interpolate[\"GDP\"], (0, 0.43))\ndf_interpolate[\"thinness_1_19_years\"]    = winsorize(df_interpolate[\"thinness_1_19_years\"], (0, 0.026))\ndf_interpolate[\"thinness_5_9_years\"]     = winsorize(df_interpolate[\"thinness_5_9_years\"], (0, 0.27))\ndf_interpolate[\"Income_composition_of_resources\"] = winsorize(df_interpolate[\"Income_composition_of_resources\"], (0, 0.001))\ndf_interpolate[\"Schooling\"]              = winsorize(df_interpolate[\"Schooling\"], (0, 0.001))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> 9. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## **9.1 PCA Results with only numeric variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Getting PCA Model***"},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData_num = df_interpolate._get_numeric_data() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LifeExpectancyData_num = LifeExpectancyData_num.dropna()\n\nX = StandardScaler().fit_transform(LifeExpectancyData_num) #standardize the feature matrix\n\npca = PCA(n_components=0.90, whiten=True)\n\nX_pca = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Looking explained variance ratios***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ***Looking at results***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original Number of Features', X.shape[1]) \nprint('Reduced Number of Features',X_pca.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a scaler object\nsc = StandardScaler()\n\n#fit the scaler to the features and transform\nX_std = sc.fit_transform(X)\n\n# Fit the PCA and transform the data\nX_std_pca = pca.fit_transform(X_std)\n\n# View the new feature data's shape\nX_std_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn import decomposition, datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a PCA object with 12 components as a parameter\npca = decomposition.PCA(n_components=12) \n# Fit the PCA and transform the data\nX_std_pca = pca.fit_transform(X_std)\n\n# View the new feature data's shape\nX_std_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.title('Total variance explained: {}'.format(pca.explained_variance_ratio_.sum()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"green\"> At the further steps, I will search for the best model based on number of variables. This PCS formula above is just an example to get results quickly. I would rather check the best model with MSE and another related values on different regression models in this project. "},{"metadata":{},"cell_type":"markdown","source":"## **9.2 Getting PCA Values for all Elements by Switching Variables to Dummies**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_interpolate.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummies = pd.get_dummies(df_interpolate)\ndf_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  PCA Results with all features with dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummies = df_dummies.dropna()\n\nX = StandardScaler().fit_transform(df_dummies)#standardize the feature matrix\n\npca = PCA(n_components=0.95, whiten=True)\n\nX_pca = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original Number of Features', X.shape[1]) \nprint('Reduced Number of Features',X_pca.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a scaler object\nsc = StandardScaler()\n\n#fit the scaler to the features and transform\nX_std = sc.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a PCA object with 178 components as a parameter\npca = decomposition.PCA(n_components=178) \n# Fit the PCA and transform the data\nX_std_pca = pca.fit_transform(X_std)\n\n# View the new feature data's shape\nX_std_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.title('Total variance explained: {}'.format(pca.explained_variance_ratio_.sum()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA with 178 variables can explain of 95% of total variance."},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> 10. Building Regression Models"},{"metadata":{},"cell_type":"markdown","source":"## **10.1 Building Model with All Numerical Variables**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_allValues = LifeExpectancyData_num['Life_Expectancy']\nX_allValues = LifeExpectancyData_num[corr_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_allValues, y_allValues, test_size = 0.2, random_state = 101)\n\nprint(\" Observations in Training Group : {}\".format(X_train.shape[0]))\nprint(\" Observations in Test Group     : {}\".format(X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\"> ***We're splitting the data in two, so out of 100 rows, 80 rows will go into the training set, and 20 rows will go into the testing set.***"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train = sm.add_constant(X_train)\n\nModel_all = sm.OLS(y_train, X_train).fit()\n\nModel_all.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pValue = Model_all.pvalues\nsignificant_values = list(pValue[pValue<= 0.05].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10.1. a)Residual Distributions on the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model_all = linear_model.LinearRegression()\nModel_all.fit(X_allValues, y_allValues)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pred = Model_all.predict(X_allValues)\nResiduals = y_allValues - pred","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import acf\n\nacf_data = acf(Residuals)\n\nplt.figure(figsize=(9,6))\nplt.plot(acf_data[1:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_nums = np.random.normal(np.mean(Residuals), np.std(Residuals), len(Residuals))\n\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.scatter(np.sort(rand_nums), np.sort(Residuals))\nplt.xlabel(\"Normally Distributed Random Variable\")\nplt.ylabel(\"Residuals\")\nplt.title(\"QQ Plot\")\n\nplt.subplot(1,2,2)\nplt.hist(Residuals)\nplt.xlabel(\"Residuals\")\nplt.title(\"Residuals Histogram\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10.1. b) Jarque Bera Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import jarque_bera\nfrom scipy.stats import normaltest","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"jb_stats = jarque_bera(Residuals)\nnorm_stats = normaltest(Residuals)\n\nprint(\"Jarque-Bera test value : {0} ve p değeri : {1}\".format(jb_stats[0], jb_stats[1]))\nprint(\"Normal test value      : {0}  ve p değeri : {1:.30f}\".format(norm_stats[0], norm_stats[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\"> ***Jarque Bera shows us that residuals distributed normally.***"},{"metadata":{},"cell_type":"markdown","source":"## **10.2 Adding Polinomial Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = LifeExpectancyData_num.drop([\"Life_Expectancy\", \"Year\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **10.3 Building Polinomial Regression Models**"},{"metadata":{},"cell_type":"markdown","source":"**10.3 a)Checking the Performance of Models within Polynomial  Degree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def polynomial(df,pol):\n    poly = PolynomialFeatures(pol)\n    poly_array = poly.fit_transform(df.drop('Life_Expectancy', axis=1))\n    df_dropped = df.drop('Life_Expectancy', axis=1)\n    df_pol = pd.DataFrame(poly_array, columns= poly.get_feature_names(df_dropped.columns))\n    df_pol = pd.concat([df_pol, df['Life_Expectancy']], axis=1)\n    Feature_list = df_pol.corr()['Life_Expectancy'].abs().sort_values(ascending = False)[1:].index\n    return pd.concat([df_pol[Feature_list], df['Life_Expectancy']], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pol1 = polynomial(LifeExpectancyData_num,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_pol(df,pol):\n    y = df['Life_Expectancy']\n    Feature_list = Feature_list = df.columns[:500] #Having overfitting after 200 variables I prefer to limit until 500\n    MSE_list_test=[]\n    R_list=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    MSE_train_list=[]\n    adj_R_test=[]\n    adj_R_train=[]\n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n        \n        \n        model_poly = LinearRegression()\n        results = model_poly.fit(X_train, y_train)\n        y_pred  = model_poly.predict(X_test)\n        y_pred_train = model_poly.predict(X_train)\n\n        MSE_list_test.append(mse(y_test, y_pred))\n        MSE_train_list.append(mse(y_train, y_pred_train))\n\n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n\n\n        number_of_variables.append(len(selected_features))\n\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n\n        RMSE_list.append(rmse(y_test, y_pred))\n\n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) / y_test)) * 100)\n        \n    model_means = list(zip(number_of_variables, R_list,R_train_list,MSE_list_test,MSE_train_list,MAE_list,RMSE_list,MAPE_list,adj_R_test,adj_R_train))\n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables','R_list','R_train_list',\n                                                            'MSE_list_test','MSE_train_list','MAE_list', 'RMSE_list', 'MAPE_list','adj_R_test', 'adj_R_train'])\n    \n    return poly_means","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_poly_transform1 = polynomial(LifeExpectancyData_num,1)\ndf_pol1 = model_pol(df_poly_transform1,2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df_poly_transform2 = polynomial(LifeExpectancyData_num,2)\ndf_pol2 = model_pol(df_poly_transform2,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time #checking total time of process in Pyhton\ndf_poly_transform3 = polynomial(LifeExpectancyData_num,3)\ndf_pol3 = model_pol(df_poly_transform3,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Displaying 3 polynomial models with data frames "},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_pol1.sort_values(by='MSE_list_test').head())\ndisplay(df_pol2.sort_values(by='MSE_list_test').head())\ndisplay(df_pol3.sort_values(by='MSE_list_test').head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (25,10))\nplt.suptitle('MSE TEST TRAIN VALUES', size=20)\n\n\n\nplt.subplot(1,3,1)\nplt.plot(df_pol1.number_of_variables,df_pol1.MSE_list_test, label  = 'MSE Values', color='blue', linewidth=5)\nplt.plot(df_pol1.number_of_variables,df_pol1.MSE_train_list, label = 'MSE_train Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values ')\nplt.title('POL 1 MSE Test/Train Values')\nplt.ylim(0,30)\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(df_pol2.number_of_variables, df_pol2.MSE_list_test, label = 'MSE Values', color='blue', linewidth=5)\nplt.plot(df_pol2.number_of_variables, df_pol2.MSE_train_list,label = 'MSE_train Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values ')\nplt.ylim(0,30)\nplt.title('POL 2 MSE Test/Train Values')\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(df_pol3.number_of_variables, df_pol3.MSE_list_test, label = 'MSE Values', color='blue', linewidth=5)\nplt.plot(df_pol3.number_of_variables, df_pol3.MSE_train_list,label = 'MSE_train Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values ')\nplt.ylim(0,30)\nplt.title('POL 3 MSE Test/Train Values')\n\n\nplt.subplots_adjust()\nplt.legend()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nobjects = ('df_pol1', 'df_pol2', 'df_pol3')\n\ny_pos = np.arange(len(objects)) \nperformance  =[df_pol1.MSE_list_test.min() ,df_pol2.MSE_list_test.min(), df_pol3.MSE_list_test.min()]\nperformance2 =[df_pol1.MSE_train_list.min(), df_pol2.MSE_train_list.min(), df_pol3.MSE_train_list.min()]\n\nplt.subplot(121)\nplt.bar(y_pos, performance, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.xlabel('Model',size=10)\nplt.ylabel('MSE Values',size=10)\nplt.title('MSE TEST Values \\n', fontsize=10)\n\nplt.subplot(122)\nplt.bar(y_pos, performance2, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.title('MSE TRAIN Values \\n', size = 10)\n\n\nplt.xlabel('Model',size=10)\nplt.ylabel('MSE Values',size=10)\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nobjects = ('df_pol1', 'df_pol2', 'df_pol3')\n\ny_pos = np.arange(len(objects)) \nperformance  =[df_pol1.adj_R_test.max() ,df_pol2.adj_R_test.max(), df_pol3.adj_R_test.max()]\nperformance2 =[df_pol1.adj_R_train.max(), df_pol2.adj_R_train.max(), df_pol3.adj_R_train.max()]\n\nplt.subplot(121)\nplt.bar(y_pos, performance, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.xlabel('Model',size=10)\nplt.ylabel('Adj R Squared Test Values',size=10)\nplt.title('Adj R Squared Test Values \\n', fontsize=10)\n\nplt.subplot(122)\nplt.bar(y_pos, performance2, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.ylabel('Adj R Squared Train Values',size=10)\nplt.title('Adj R Squared Train Values \\n', size = 10)\nplt.xlabel('Model',size=10)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualization the three category based models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = LifeExpectancyData_num.drop([\"Life_Expectancy\", \"Year\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(2)\npoly_array = poly.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_poly2 = pd.DataFrame(poly_array, columns= poly.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = LifeExpectancyData_num['Life_Expectancy']\nX = df_poly2\n\nX_train_pol2, X_test_pol2, y_train_pol2, y_test_pol2 = train_test_split(X, y, test_size = 0.2, random_state = 101)\n\nprint(\"Eğitim kümesindeki gözlem sayısı : {}\".format(X_train.shape[0]))\nprint(\"Test kümesindeki gözlem sayısı   : {}\".format(X_test.shape[0]))\n\nX_train = sm.add_constant(X_train)\n\npoly_model_2 = sm.OLS(y_train_pol2, X_train_pol2).fit()\ny_preds_pol2 = poly_model_2.predict(X_test_pol2)\ny_preds_train_pol2 = poly_model_2.predict(X_train_pol2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(3)\npoly_array = poly.fit_transform(df)\ndf_poly3 = pd.DataFrame(poly_array, columns= poly.get_feature_names())\n\ny = LifeExpectancyData_num['Life_Expectancy']\nX = df_poly3\n\nX_train_pol3, X_test_pol3, y_train_pol3, y_test_pol3 = train_test_split(X, y, test_size = 0.2, random_state = 101)\n\nprint(\"Observations in Train Group : {}\".format(X_train.shape[0]))\nprint(\"Observations in Test Group  : {}\".format(X_test.shape[0]))\n\nX_train = sm.add_constant(X_train)\n\npoly_model_3 = sm.OLS(y_train_pol3, X_train_pol3).fit()\ny_preds_pol3 = poly_model_3.predict(X_test_pol3)\ny_preds_train_pol3 = poly_model_3.predict(X_train_pol3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(1)\npoly_array = poly.fit_transform(df)\ndf_poly1 = pd.DataFrame(poly_array, columns= poly.get_feature_names())\n\ny = LifeExpectancyData_num['Life_Expectancy']\nX = df_poly1\n\nX_train_pol1, X_test_pol1, y_train_pol1, y_test_pol1 = train_test_split(X, y, test_size = 0.2, random_state = 101)\n\nprint(\"Observations in Train Group : {}\".format(X_train.shape[0]))\nprint(\"Observations in Test Group  : {}\".format(X_test.shape[0]))\n\nX_train = sm.add_constant(X_train)\n\npoly_model_1 = sm.OLS(y_train_pol1, X_train_pol1).fit()\ny_preds_pol1 = poly_model_1.predict(X_test_pol1)\ny_preds_train_pol1 = poly_model_1.predict(X_train_pol1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.suptitle('Scatter Plots of Life Expectancy Predictions', size = 16)\n\nplt.subplot(1,3,1)\nplt.title('Poly 1 Model \\n', size = 14)\nplt.scatter(y_test_pol1, y_preds_pol1)\nplt.scatter(y_train_pol1, y_preds_train_pol1,alpha=0.10)\nplt.plot(y_test_pol1, y_test_pol1, color=\"red\")\nplt.ylim(0,90)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\nplt.subplot(1,3,2)\nplt.title('Poly 2 Model \\n', size = 14)\nplt.scatter(y_test_pol2, y_preds_pol2 )\nplt.scatter(y_train_pol2, y_preds_train_pol2,alpha=0.10)\nplt.plot(y_test_pol2, y_test_pol2, color=\"red\")\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\nplt.subplot(1,3,3)\nplt.title('Poly 3 Model \\n', size = 14)\nplt.scatter(y_test_pol3, y_preds_pol3)\nplt.scatter(y_train_pol3, y_preds_train_pol3,alpha=0.10)\nplt.plot(y_test_pol3, y_test_pol3, color=\"red\")\nplt.ylim(0,90)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\n\n\n\nplt.subplots_adjust()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\"> There are two critical characteristics of estimators to be considered: the bias and the variance. The bias is the difference between the true population parameter and the expected estimator.\nIt measures the accuracy of the estimates. Variance, on the other hand, measures the spread, or uncertainty, in these estimates. \n\n<font color=\"green\"> So, setting λ to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficients' size penalized\nas λ becomes larger, the variance decreases, and the bias increases.\n    \nA more traditional approach would be to choose λ such that some information criterion,Akaike or Bayesian(AIC or BIC), is the smallest. A more machine learning-like approach is to perform cross-validation and select the value of λ that minimizes the cross-validated sum of squared residuals.\n\nAs we see on scatter plots, True values of Poly 2 model are distributed better than Poly 3 Model on test and train group. Poly 3 Model is not enough to explain some of higher values. "},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\">  10.4 Building Ridge Regression Models"},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\">While Least Squares determines values for the parameters in an equation, it minimizes the sum of the squared residuals. On the other hand, Ridge Regression minimizes the sum of the squared residuals plus lambda and the slope of the regression line.\n\n#### <font color=\"green\"> As having mostly parameters important for my prediction, I am willing to use Ridge Model as well to keep all of components in my model. \n    \n#### <font color=\"red\"> I prefered not to use \" sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)\"  considering the size of my data set. However, this function also helps to estimate the most suitable variables within number of features in selected model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Ridge_model(df,pol, alpha, col=None):\n\n    y = df['Life_Expectancy']\n    Feature_list = df.columns[:500]\n    \n    MSE_list_test=[]\n    R_list=[]\n    adj_R_test=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    adj_R_train=[]\n    MSE_train_list=[]\n    model_list=[]\n    feature_list=[]\n        \n    \n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n                \n        model_poly = Ridge(alpha= alpha) \n        model_poly.fit(X_train, y_train)\n        results = model_poly.fit(X_train, y_train)\n               \n        y_pred  = model_poly.predict(X_test)\n        \n        y_pred_train = model_poly.predict(X_train)\n      \n        MSE_list_test.append(mse(y_test, y_pred))\n        \n        MSE_train_list.append(mse(y_train, y_pred_train))\n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n        \n        number_of_variables.append(len(selected_features))\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n        \n        RMSE_list.append(rmse(y_test, y_pred))\n        \n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) / y_test)) * 100)\n        model_list.append(model_poly)\n        feature_list.append(selected_features)\n        \n        \n    \n        \n        \n    model_means = list(zip(number_of_variables, R_list, adj_R_test, R_train_list, adj_R_train, MSE_list_test,\n                           MSE_train_list,MAE_list,RMSE_list,MAPE_list,model_list,feature_list))\n    \n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables', 'R_list','adj_R_test',\n                                                     'R_train_list','adj_R_train',\n                                                     'MSE_list_test','MSE_train_list','MAE_list','RMSE_list','MAPE_list',\n                                                     'model_list', 'feature_list'])\n    \n    \n    return poly_means, (y_pred,y_pred_train, X_train,y_train, X_test, y_test, MSE_list_test,MSE_train_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]: \n    df, _  = Ridge_model(df_poly_transform2,2,alpha)\n    print(alpha, df.MSE_list_test.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Model option with minimum MSE_test Value on Alpha 10-⁵ and polynomial 2nd degree.\n\ndf_Ridge_alpha_pol2, degerler1_2 = Ridge_model(df_poly_transform2,2,0.000001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Ridge_alpha_pol2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = Ridge_model(df_poly_transform3,3,alpha)\n    print(alpha, df.MSE_list_test.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Model option with minimum MSE_test Value on Alpha 10³ and polynomial 3rd degree.\ndf_Ridge_alpha_pol3, degerler1_3 = Ridge_model(df_poly_transform3,3,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Ridge_alpha_pol3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MSE_list_test_alpha_pol2  = df_Ridge_alpha_pol2['MSE_list_test']\nMSE_train_test_alpha_pol2 = df_Ridge_alpha_pol2['MSE_train_list']\nMSE_list_test_alpha_pol3  = df_Ridge_alpha_pol3['MSE_list_test']\nMSE_train_test_alpha_pol3 = df_Ridge_alpha_pol3['MSE_train_list']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, MSE_list_test_alpha_pol2,label  = 'MSE Test Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, MSE_train_test_alpha_pol2,label = 'MSE Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 2 MSE Test/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables,MSE_list_test_alpha_pol3,label  = 'MSE Test Alpha Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables, MSE_train_test_alpha_pol3,label = 'MSE Train Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 3 MSE Test/Train Values')\n\n\nplt.subplots_adjust()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adj_R_test_alpha_pol2  = df_Ridge_alpha_pol2['adj_R_test']\nadj_R_train_alpha_pol2 = df_Ridge_alpha_pol2['adj_R_train']\nadj_R_test_alpha_pol3  = df_Ridge_alpha_pol3['adj_R_test']\nadj_R_train_alpha_pol3 = df_Ridge_alpha_pol3['adj_R_train']\n\n\n\n\nplt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, adj_R_test_alpha_pol2,label  = 'Adjusted R² Test Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, adj_R_train_alpha_pol2,label = 'Adjusted R² Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted Values')\nplt.title('Ridge POLY 2 Adjusted R² Test/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables,adj_R_test_alpha_pol3,label  = 'Adjusted R² Test Alpha Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables, adj_R_train_alpha_pol3,label = 'Adjusted R² Train Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted Values')\nplt.title('Ridge POLY 3 Adjusted R² Test/Train Values')\n\n\nplt.subplots_adjust()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> 2nd polynomial degree gives higher Adjusted R squared values compering with 3rd polynomial degree."},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> While having the same trend until 125th variable on the Poly 2 MSE results, Poly 3 MSE results shows us that after the 200th variable trend is not good any more. \n    \n#### <font color=\"green\"> Because having the low MSE value, I will continue with 2nd polynomial degree ridge Model. Later on, we also compare R squared values as well\n"},{"metadata":{},"cell_type":"markdown","source":"# 10.5 Building Lasso Regression Models"},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> While Ridge Regression minimizes the sum of the squared residuals plus lambda and squaring the slope of the regression line, Lasso Regression minimizes the sum of the squared residuals, plus lambda and absolute value of slope of the regression line.\n    \n#### <font color=\"green\">In contrast, Ridge shrink the parameters by keeping all of them, Lasso Regression eliminates and creates a simpler model to explain. Therefore, I would like to have results of this model as well to have a wider range of elements for my prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Lasso_model(df,pol, alpha):\n\n    y = df['Life_Expectancy']\n    Feature_list = df.columns[:500]\n    \n    MSE_list_test=[]\n    R_list=[]\n    adj_R_test=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    adj_R_train=[]\n    MSE_train_list=[]\n    \n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n                \n        model_poly = Lasso(alpha= alpha) \n        model_poly.fit(X_train, y_train)\n        results = model_poly.fit(X_train, y_train)\n               \n        y_pred  = model_poly.predict(X_test)\n        \n        y_pred_train = model_poly.predict(X_train)\n      \n        MSE_list_test.append(mse(y_test, y_pred))\n        \n        MSE_train_list.append(mse(y_train, y_pred_train))\n        \n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n        \n        number_of_variables.append(len(selected_features))\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n        \n        RMSE_list.append(rmse(y_test, y_pred))\n        \n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) / y_test)) * 100)\n        \n        \n    model_means = list(zip(number_of_variables, R_list, adj_R_test, R_train_list, adj_R_train, MSE_list_test,MSE_train_list,MAE_list,RMSE_list,MAPE_list))\n    \n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables', 'R_list', 'adj_R_test', 'R_train_list', 'adj_R_train','MSE_list_test','MSE_train_list','MAE_list','RMSE_list','MAPE_list'])\n    \n    \n    return poly_means, (y_pred,y_pred_train, X_train,y_train, X_test, y_test, model_poly, MSE_list_test,MSE_train_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = Lasso_model(df_poly_transform2,2,alpha)\n    print(alpha, df.MSE_list_test.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Model option with minimum MSE_test Value on Alpha 10-⁵ and polynomial 2 degree\n\ndf_Lasso_alpha_pol2, degerler1_2 = Lasso_model(df_poly_transform2,2,0.000001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Lasso_alpha_pol2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = Lasso_model(df_poly_transform3,3,alpha)\n    print(alpha, df.MSE_list_test.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Best Model option with minimum MSE_test Value on Alpha 10³ and polynomial 3 degree.\n\ndf_Lasso_alpha_pol3, degerler1_3 = Lasso_model(df_poly_transform3,3,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Lasso_alpha_pol3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MSE_list_test_Lasso_alpha_pol2  = df_Lasso_alpha_pol2['MSE_list_test']\nMSE_train_test_Lasso_alpha_pol2 = df_Lasso_alpha_pol2['MSE_train_list']\nMSE_list_test_Lasso_alpha_pol3  = df_Lasso_alpha_pol3['MSE_list_test']\nMSE_train_test_Lasso_alpha_pol3 = df_Lasso_alpha_pol3['MSE_train_list']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,MSE_list_test_Lasso_alpha_pol2, label = 'MSE Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,MSE_train_test_Lasso_alpha_pol2,label = 'MSE Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('Lasso POLY 2 MSE Test/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, MSE_list_test_Lasso_alpha_pol3,label = 'MSE Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, MSE_train_test_Lasso_alpha_pol3,label = 'MSE Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('Lasso POLY 3 MSE Test/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adj_R_test_Lasso_alpha_pol2  = df_Lasso_alpha_pol2['adj_R_test']\nadj_R_train_Lasso_alpha_pol2 = df_Lasso_alpha_pol2['adj_R_train']\nadj_R_test_Lasso_alpha_pol3  = df_Lasso_alpha_pol3['adj_R_test']\nadj_R_train_Lasso_alpha_pol3 = df_Lasso_alpha_pol3['adj_R_train']\n\nplt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,adj_R_test_Lasso_alpha_pol2, label = 'Adjusted R² Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,adj_R_train_Lasso_alpha_pol2,label = 'Adjusted R² Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R² Values')\nplt.title('Lasso POLY 2 Adjusted R² Test/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, adj_R_test_Lasso_alpha_pol3,label = 'Adjusted R² Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, adj_R_train_Lasso_alpha_pol3,label = 'Adjusted R² Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R² Values')\nplt.title('Lasso POLY 3 Adjusted R² Test/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> Lasso 2nd Polynomial degree model gives higher Adjusted R² values than 3rd polynomial degree. With the same variables 3rd polynomial degree is not a good option for our regression model. "},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> As we see, Poly 2 Model is breaking after 130th variable and train and test values loses direction in Poly 3 Model after 400th variable. \n    \n#### <font color=\"green\"> As Lasso Model eliminates features in function, overfitting is not happening as before."},{"metadata":{},"cell_type":"markdown","source":"# 10.6 Building ElasticNet Regression Models"},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> This type of regression is a mixed of Ridge and Lasso Regression models for a huge data set while not keeping all elements in the model. This model also eliminates unnecessary variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ElasticNet_model(df,pol, alpha):\n\n    y = df['Life_Expectancy']\n    Feature_list = df.columns[:500]\n    \n    MSE_list_test=[]\n    R_list=[]\n    adj_R_test=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    adj_R_train=[]\n    MSE_train_list=[]\n    \n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n                \n        model_poly = ElasticNet(alpha=alpha, l1_ratio=0.5)\n        model_poly.fit(X_train, y_train)\n        results = model_poly.fit(X_train, y_train)\n               \n        y_pred  = model_poly.predict(X_test)\n        \n        y_pred_train = model_poly.predict(X_train)\n      \n        MSE_list_test.append(mse(y_test, y_pred))\n        \n        MSE_train_list.append(mse(y_train, y_pred_train))\n        \n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n                \n        number_of_variables.append(len(selected_features))\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n        \n        RMSE_list.append(rmse(y_test, y_pred))\n        \n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) / y_test)) * 100)\n        \n        \n    model_means = list(zip(number_of_variables, R_list, adj_R_test, R_train_list, adj_R_train, MSE_list_test,MSE_train_list,MAE_list,RMSE_list,MAPE_list))\n    \n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables', 'R_list', 'adj_R_test', 'R_train_list', 'adj_R_train', 'MSE_list_test','MSE_train_list','MAE_list','RMSE_list','MAPE_list'])\n    \n    \n    return poly_means, (y_pred,y_pred_train, X_train,y_train, X_test, y_test, model_poly, MSE_list_test,MSE_train_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = ElasticNet_model(df_poly_transform3,3,alpha)\n    print(alpha, df.MSE_list_test.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Model with minimum MSE_test Value on Alpha 10⁴ and polynomial 3 degree \ndf_ElasticNet_alpha_pol3, degerler1_3 = ElasticNet_model(df_poly_transform3,3,0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = ElasticNet_model(df_poly_transform2,2,alpha)\n    print(alpha, df.MSE_list_test.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Model with minimum MSE_test Value on Alpha 10-⁵ and polynomial 2 degree \n\ndf_ElasticNet_alpha_pol2, degerler1_2 = ElasticNet_model(df_poly_transform2,2,0.000001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MSE_list_test_ElasticNet_alpha_pol2  = df_ElasticNet_alpha_pol2['MSE_list_test']\nMSE_list_train_ElasticNet_alpha_pol2 = df_ElasticNet_alpha_pol2['MSE_train_list']\nMSE_list_test_ElasticNet_alpha_pol3  = df_ElasticNet_alpha_pol3['MSE_list_test']\nMSE_list_train_ElasticNet_alpha_pol3 = df_ElasticNet_alpha_pol3['MSE_train_list']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,MSE_list_test_ElasticNet_alpha_pol2, label = 'MSE Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,MSE_list_train_ElasticNet_alpha_pol2,label = 'MSE Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 2 MSE Test/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, MSE_list_test_ElasticNet_alpha_pol3,label = 'MSE Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, MSE_list_train_ElasticNet_alpha_pol3,label = 'MSE Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 3 MSE Test/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adj_R_test_ElasticNet_alpha_pol2  = df_ElasticNet_alpha_pol2['adj_R_test']\nadj_R_train_ElasticNet_alpha_pol2 = df_ElasticNet_alpha_pol2['adj_R_train']\nadj_R_test_ElasticNet_alpha_pol3  = df_ElasticNet_alpha_pol3['adj_R_test']\nadj_R_train_ElasticNet_alpha_pol3 = df_ElasticNet_alpha_pol3['adj_R_train']\n\n\nplt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,adj_R_test_ElasticNet_alpha_pol2, label = 'Adjusted R² Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,adj_R_train_ElasticNet_alpha_pol2,label = 'Adjusted R² Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R²')\nplt.title('Elastic Net POLY 2 Adjusted R² Test/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, adj_R_test_ElasticNet_alpha_pol3,label = 'Adjusted R² Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, adj_R_train_ElasticNet_alpha_pol3,label = 'Adjusted R² Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R²')\nplt.title('Elastic Net POLY 3 Adjusted R² Test/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font color=\"green\"> This models gives best adjusted R² values with 2nd polynomial degree."},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\">  11. Evaluating the Model"},{"metadata":{},"cell_type":"markdown","source":"#### Comparing All Results of our Models in one BarPlot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,20))\n\nobjects=('df_pol1', 'df_pol2', 'df_pol3',\n           'df_Ridge_alpha_pol2', 'df_Ridge_alpha_pol3',\n           'df_Lasso_alpha_pol2', 'df_Lasso_alpha_pol3',\n           'df_ElasticNet_alpha_pol2', 'df_ElasticNet_alpha_pol3' )\n\ny_pos = np.arange(len(objects)) \nperformance  =[df_pol1.MSE_list_test.min() ,df_pol2.MSE_list_test.min(), df_pol3.MSE_list_test.min(),\n               df_Ridge_alpha_pol2.MSE_list_test.min(),df_Ridge_alpha_pol3.MSE_list_test.min(),\n               df_Lasso_alpha_pol2.MSE_list_test.min(), df_Lasso_alpha_pol3.MSE_list_test.min(),\n               df_ElasticNet_alpha_pol2.MSE_list_test.min(), df_ElasticNet_alpha_pol3.MSE_list_test.min()]\n\nperformance2 =[df_pol1.MSE_train_list.min(), df_pol2.MSE_train_list.min(), df_pol3.MSE_train_list.min(),\n               df_Ridge_alpha_pol2.MSE_train_list.min(),df_Ridge_alpha_pol3.MSE_train_list.min(),\n               df_Lasso_alpha_pol2.MSE_train_list.min(), df_Lasso_alpha_pol3.MSE_train_list.min(),\n               df_ElasticNet_alpha_pol2.MSE_train_list.min(), df_ElasticNet_alpha_pol3.MSE_train_list.min()]\n               \n               \nperformance3 = [df_pol1.R_list.max() ,df_pol2.R_list.max(), df_pol3.R_list.max(),\n               df_Ridge_alpha_pol2.R_list.max(),df_Ridge_alpha_pol3.R_list.max(),\n               df_Lasso_alpha_pol2.R_list.max(), df_Lasso_alpha_pol3.R_list.max(),\n               df_ElasticNet_alpha_pol2.R_list.max(), df_ElasticNet_alpha_pol3.R_list.max()]\n\nperformance4 = [df_pol1.adj_R_test.max() ,df_pol2.adj_R_test.max(), df_pol3.adj_R_test.max(),\n               df_Ridge_alpha_pol2.adj_R_test.max(),df_Ridge_alpha_pol3.adj_R_test.max(),\n               df_Lasso_alpha_pol2.adj_R_test.max(), df_Lasso_alpha_pol3.adj_R_test.max(),\n               df_ElasticNet_alpha_pol2.adj_R_test.max(), df_ElasticNet_alpha_pol3.adj_R_test.max()]\n\nplt.subplot(411)\nplt.bar(y_pos, performance, align='center')\nplt.xticks(y_pos, objects,size=13)\n\nplt.ylabel('MSE Values',size=15)\nplt.title('MSE TEST Values \\n', fontsize=15)\n\n\nplt.subplots_adjust()\nplt.subplot(412)\nplt.bar(y_pos, performance2, align='center')\nplt.xticks(y_pos, objects,size=13)\n\nplt.ylabel('MSE TRAIN Values',size=15)\nplt.title('MSE  Values \\n', size = 15)\n\nplt.subplot(413)\nplt.bar(y_pos, performance3, align='center')\nplt.xticks(y_pos, objects,size=13)\nplt.title('R Squared Values \\n', size = 15)\n\nplt.ylabel('R Squared Values',size=15)\n\nplt.subplot(414)\nplt.bar(y_pos, performance4, align='center')\nplt.xticks(y_pos, objects,size=13)\nplt.title('Adjusted R Squared Values \\n', size = 15)\n\nplt.ylabel('Adjusted R Squared Values',size=15)\n\n\nplt.subplots_adjust()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting All Values of each Models in One Data Frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"objects =(df_pol1, df_pol2, df_pol3,\n             df_Ridge_alpha_pol2, df_Ridge_alpha_pol3,\n             df_Lasso_alpha_pol2, df_Lasso_alpha_pol3,\n             df_ElasticNet_alpha_pol2, df_ElasticNet_alpha_pol3)\n\ndf_results = pd.DataFrame()\nfor df in objects:\n    df_results= df_results.append(df.sort_values(by='MSE_list_test').head(1), ignore_index=True)\n    \n\ndf_results['Model'] = ['Linear Regression (Polynomial 1)',\n                           'Linear Regression (Polynomial 2)',\n                           'Linear Regression (Polynomial 3)',\n                           'Ridge Regression (Polynomial 2)',\n                           'Ridge Regression (Polynomial 3)',\n                           'Lasso Regression (Polynomial 2)',\n                           'Lasso Regression (Polynomial 3)',                           \n                           'ElasticNet Regression(Polynomial 2)',\n                           'ElasticNet Regression(Polynomial 3)']\n    \ndf_results.sort_values('MSE_list_test')[['Model', 'number_of_variables', 'MSE_list_test','MSE_train_list', 'R_list','adj_R_test', 'adj_R_train']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"green\"> After searching different type of regression models, we have the minimum MSE and the better adjusted R² values from  Linear Regression and Ridge Regression on two polynomial degree. Polynomial degree does not affect values on different type of regression models. \n\n\n### <font color=\"green\"> Low MSE values and highest adjusted R² came from two polynomial degree models. Applying other type of regressions with three polynomial degree only increased MSE Test values to a higher level. Therefore, I agree to choose the Ridge Regression with two polynomial degree."},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"green\">  **After selecting the best model of Ridge Regression with 2 polynomial degree on alpha 0.000001, here we will see the results of our model by applying coefficients on each variable as an example to check our model performance.**"},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\">  12. Predicting with the Best Model"},{"metadata":{},"cell_type":"markdown","source":"#### An Example from a Rondom Row to Check The Model Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#As we see on the graph of this model, best performance is starting after 125th variable.\n#Thus, I selected the first 126 variables from our model.\n\ndf_Ridge_alpha_pol2[df_Ridge_alpha_pol2['number_of_variables']== 126 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A rondom row[5] of our data set to find values for each columns as an example:\n\nSelected_Model = df_Ridge_alpha_pol2.iloc[5].model_list","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Here are the first 5 coeficiants from our model. \n\nSelected_Model.coef_[:5]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Switching our values to doctionary for the further step.\nLifeExpectancyData_num.iloc[5].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a dictionary to have values for each variables.\n\ndictionary = {'Year': 2010.0,\n 'Adult_Mortality': 279.0,\n 'infant_deaths': 74.0,\n 'Alcohol': 0.01,\n 'percentage_expenditure': 79.67936736,\n 'Hepatitis_B': 66.0,\n 'Measles': 1989.0,\n 'BMI': 16.7,\n 'under_five_deaths': 102.0,\n 'Polio': 66.0,\n 'Total_Expenditure': 9.2,\n 'Diphtheria': 66.0,\n 'HIV/AIDS': 0.1,\n 'GDP': 553.32894,\n 'thinness_1_19_years': 16.6,\n 'thinness_5_9_years': 6.9,\n 'Income_composition_of_resources': 0.45,\n 'Schooling': 9.2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Example = np.array(list(dictionary.values())).reshape(1,-1)\npoly = PolynomialFeatures(2)\ndf = LifeExpectancyData_num.drop('Life_Expectancy', axis=1)\npoly.fit_transform(df)\n\ndf_example = pd.DataFrame(poly.transform(Example), columns= poly.get_feature_names(df.columns))\n\ndf_Ridge_alpha_pol2, degerler1_2 = Ridge_model(df_poly_transform2,2,0.000001)\nselected_fetures = df_Ridge_alpha_pol2.iloc[5]['feature_list']\nselected_model = df_Ridge_alpha_pol2.iloc[5]['model_list']\n\nSelected_Model.predict(df_example[selected_fetures]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\">  13. Conclusions"},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"green\">  We can see that having values as following 'Year': 2010,  'Adult_Mortality': 279.0,  'infant_deaths': 74.0, 'Alcohol': 0.01, 'percentage_expenditure': 79.67936736, 'Hepatitis_B': 66.0, 'Measles': 1989.0, 'BMI': 16.7, 'under_five_deaths': 102.0, 'Polio': 66.0, 'Total_Expenditure': 9.2, 'Diphtheria': 66.0, 'HIV/AIDS': 0.1, 'GDP': 553.32894, 'thinness_1_19_years': 16.6, 'thinness_5_9_years': 6.9, 'Income_composition_of_resources': 0.45, 'Schooling': 9.2, gives the result of Life Expectancy as '61'. \n \n### <font color=\"green\">  The original value of Life Expectancy was 58.8 in 2010.  MSE Test value is 6.367 with average +-2.52 of RMSE value. Simply 61 minus 2.52 gives results as around 58 from the real value of Life Expectancy.\n    \n### <font color=\"green\">  Regression models is luckily helping us to predict our dependent variabl0 with using many parameters. In order to have an accurate result, we need to check as many as regression models. Having the lowest MSE and highest R squared values are helping us on our way. \n "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}