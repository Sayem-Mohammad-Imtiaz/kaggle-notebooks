{"cells":[{"metadata":{"_uuid":"18ce900f7b0190f4e5a7d0038096f776713456c3"},"cell_type":"markdown","source":"**Learning Sarcasm with Bidirectional GRU**\n\nThese are the codes that I used to explore the [Sarcasm Detection dataset](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home). I could make this kernel thanks to well-documented helpful kernels by [SRK](https://www.kaggle.com/sudalairajkumar)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_json(\"../input/Sarcasm_Headlines_Dataset.json\", lines = True)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cc5b7f2e09aac13e86c5f35a3219bf0e0c27608"},"cell_type":"code","source":"data[\"is_sarcastic\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"931e9a262688fc37ded181374ea5c888ba5e072a"},"cell_type":"code","source":"from tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44bc3f04f29621e6104cfce232b23a17306e1972"},"cell_type":"code","source":"data = data[[\"headline\", \"is_sarcastic\"]]\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb59b93aa24a7705d654fa2f1be45688f58cfdb6"},"cell_type":"code","source":"#https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/data\n\n## split to train and val\ntrain_df, test_df = train_test_split(data, test_size=0.1, random_state=2019)\ntrain_df, val_df = train_test_split(train_df, test_size=0.125, random_state=2019)\nprint(\"Train size:{}\".format(train_df.shape))\nprint(\"Validation size:{}\".format(val_df.shape))\nprint(\"Test size:{}\".format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a83b9377de0c4769ce916dbbccd3179d947571cd"},"cell_type":"code","source":"#https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"headline\"], title=\"Word Cloud of Questions\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b14be9f5513bf1fa3f656e31e7638a5205efd76"},"cell_type":"code","source":"from collections import defaultdict\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\ntrain1_df = train_df[train_df[\"is_sarcastic\"]==1]\ntrain0_df = train_df[train_df[\"is_sarcastic\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"headline\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"headline\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of not sarcastic headlines\", \n                                          \"Frequent words of sarcastic headlines\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45238224626e2110fae6fe549429ea552981a56b"},"cell_type":"code","source":"## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"headline\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"headline\"].apply(lambda x: len(str(x).split()))\n\n# ## Number of unique words in the text ##\n# train_df[\"num_unique_words\"] = train_df[\"headline\"].apply(lambda x: len(set(str(x).split())))\n# test_df[\"num_unique_words\"] = test_df[\"headline\"].apply(lambda x: len(set(str(x).split())))\n\n# ## Number of characters in the text ##\n# train_df[\"num_chars\"] = train_df[\"headline\"].apply(lambda x: len(str(x)))\n# test_df[\"num_chars\"] = test_df[\"headline\"].apply(lambda x: len(str(x)))\n\n# ## Number of stopwords in the text ##\n# train_df[\"num_stopwords\"] = train_df[\"headline\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n# test_df[\"num_stopwords\"] = test_df[\"headline\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# ## Number of punctuations in the text ##\n# train_df[\"num_punctuations\"] =train_df['headline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n# test_df[\"num_punctuations\"] =test_df['headline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n# ## Number of title case words in the text ##\n# train_df[\"num_words_upper\"] = train_df[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n# test_df[\"num_words_upper\"] = test_df[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n# ## Number of title case words in the text ##\n# train_df[\"num_words_title\"] = train_df[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n# test_df[\"num_words_title\"] = test_df[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n# ## Average length of the words in the text ##\n# train_df[\"mean_word_len\"] = train_df[\"headline\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n# test_df[\"mean_word_len\"] = test_df[\"headline\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36c2f123bbd03d980a68557c29ec48a6fc56317d"},"cell_type":"code","source":"#https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/data\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"headline\"].fillna(\"_na_\").values\nval_X = val_df[\"headline\"].fillna(\"_na_\").values\ntest_X = test_df[\"headline\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['is_sarcastic'].values\nval_y = val_df['is_sarcastic'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2825054916634fcb61e7f333c4f1318861b980c"},"cell_type":"code","source":"#https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/data\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f375f2a2b0b8fa301cc4be2970c1038e5b9eb77"},"cell_type":"code","source":"# https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/data\n\n## Train the model \nmodel.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"534da6bfc4b4e83478522fbd0e38fe3950baafcd"},"cell_type":"code","source":"# https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/data\n\npred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e3e0d36acd980f2a5b3006f099c64f447a0826f"},"cell_type":"code","source":"# https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/data\n\npred_noemb_test_y = model.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}