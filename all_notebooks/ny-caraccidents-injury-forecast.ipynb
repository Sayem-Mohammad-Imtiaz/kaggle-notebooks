{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"crash=pd.read_csv('../input/nys-motor-vehicle-crashes-and-insurance-reduction/motor-vehicle-crashes-case-information-three-year-window.csv')\nvehicle=pd.read_csv('../input/nys-motor-vehicle-crashes-and-insurance-reduction/motor-vehicle-crashes-vehicle-information-three-year-window.csv')\nperson=pd.read_csv('../input/nys-motor-vehicle-crashes-and-insurance-reduction/motor-vehicle-crashes-individual-information-three-year-window.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehper=vehicle.merge(person,left_on='Case Vehicle ID',right_on='Case Vehicle ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehper[:10].T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crash[:10].T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle[:10].T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nif True:   #labelencode instead of using average is better\n    for c in vehper.columns:\n        if vehper[c].dtype == 'object':\n            lbl = LabelEncoder()\n            lbl.fit(list(vehper[c].values)) \n            vehper[c] = lbl.transform(list(vehper[c].values))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehper[:10].T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clustertechniques(dtrain,label,indexv,dtest,lengte):\n    # One Hot Encode target mean()\n    cols=[ci for ci in dtrain.columns if ci not in [indexv,'index',label]]\n    coltype=dtrain.dtypes\n    for ci in cols:\n        if (coltype[ci]==\"object\"):\n            codes=dtrain[[ci,label]].groupby(ci).mean().sort_values(label)\n            codesdict=codes[label].to_dict()\n            dtrain[ci]=dtrain[ci].map(codesdict)\n            if len(dtest)>0:\n                dtest[ci]=dtest[ci].map(codesdict)\n    print(dtest)\n    #split data or use splitted data\n    if len(dtest)==0:\n        dtrain = dtrain[:lengte]\n        print(dtrain.info())\n        from sklearn.model_selection import train_test_split\n        X_train,X_test,Y_train,Y_test = train_test_split(dtrain.drop(label,axis=1),dtrain[label],test_size=0.25,random_state=0)\n        print('train - test',X_train.shape,X_test.shape)\n    else:\n\n        X_test=dtest.dropna(axis=1).drop(indexv,axis=1)\n        Y_test=dtest[indexv]\n        X_train=dtrain[X_test.columns]\n        Y_train=dtrain[label]        \n        print('train - test',X_train.shape,X_test.shape)\n    lenxtr=len(X_train)\n\n\n    import matplotlib.pyplot as plt \n    from sklearn import preprocessing\n    scale = preprocessing.MinMaxScaler().fit(X_train)\n    X_train = scale.transform(X_train)\n    X_test = scale.transform(X_test)\n\n        \n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE #limit number of records to 100000\n\n    clusters = [Dummy(1),\n                PCA(n_components=0.98,random_state=0,whiten=True),\n                #FastICA(n_components=7,random_state=0),\n                TruncatedSVD(n_components=10, n_iter=7, random_state=42),\n                #NMF(n_components=10,random_state=0),            \n                #UMAP(n_neighbors=5,n_components=10, min_dist=0.3,metric='minkowski'),\n                #TSNE(n_components=2,random_state=0)\n                ] \n    clunaam=[\"raw\",'PCA','tSVD']#,'ICA','tSVD','nmf','UMAP','tSNE']\n    \n    \n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import SVC, LinearSVC,NuSVC\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n    from sklearn.neural_network import MLPClassifier,MLPRegressor\n    from sklearn.linear_model import PassiveAggressiveClassifier,Perceptron,SGDClassifier,LogisticRegression\n    import xgboost as xgb\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n    \n    classifiers = [LogisticRegression( solver=\"lbfgs\",max_iter=500,n_jobs=-1),\n                   PassiveAggressiveClassifier(max_iter=50, tol=1e-3,n_jobs=-1),    \n                   KNeighborsClassifier(n_neighbors=5,n_jobs=-1),\n                   RandomForestClassifier(n_estimators=100, random_state=42,n_jobs=-1, oob_score=True),\n                   ExtraTreesClassifier(n_estimators=10, max_depth=50, min_samples_split=5, min_samples_leaf=1, random_state=None, min_impurity_decrease=1e-7),\n                   #SVC(gamma='auto'),                   \n                   MLPClassifier(alpha=0.510,activation='logistic'),\n                   SGDClassifier(average=True,max_iter=100),\n                   xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11),\n                    #SVC(kernel=\"rbf\", C=0.025, probability=True),\n                    #NuSVC(probability=True),\n                    DecisionTreeClassifier(),\n                    AdaBoostClassifier(),\n                    GradientBoostingClassifier(),\n                    GaussianNB(),\n                    LinearDiscriminantAnalysis(),\n                    QuadraticDiscriminantAnalysis()\n                  ]\n    clanaam= ['Logi','passiv','KNN','rFor','Xtr','MLP','SGD','xgb','Decis','Ada','GradB','GausNB','LinDis','QuadDis']#['Logi','KNN','rFor','SVC','MLP','SGD']\n    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n    \n    results=[]\n\n\n    #cluster data\n    for clu in clusters:\n        clunm=clunaam[clusters.index(clu)] #find naam\n        X_total_clu = clu.fit_transform(np.concatenate( (X_train,X_test),axis=0))\n        print(X_total_clu.shape)\n        plt.scatter(X_total_clu[:lenxtr,0],X_total_clu[:lenxtr,1],c=Y_train.values,cmap='prism')\n        plt.title(clu)\n        plt.show()\n        \n        #classifiy \n        for cla in classifiers:\n            import datetime\n            start = datetime.datetime.now()\n            clanm=clanaam[classifiers.index(cla)] #find naam\n            \n            print('    ',cla)\n            #cla.fit(X_total_clu,np.concatenate( (Y_train,Y_test)) )\n            cla.fit(X_total_clu[:lenxtr],Y_train )\n            \n            #predict\n            trainpredi=cla.predict(X_total_clu[:lenxtr])\n            print(classification_report(trainpredi,Y_train))            \n            testpredi=cla.predict(X_total_clu[lenxtr:])            \n\n            #testpredi=converging(pd.DataFrame(X_train),pd.DataFrame(X_test),Y_train,pd.DataFrame(testpredi),Y_test,clu,cla) #PCA(n_components=10,random_state=0,whiten=True),MLPClassifier(alpha=0.510,activation='logistic'))\n            \n            if len(dtest)==0:\n                test_score=cla.score(X_total_clu[lenxtr:],Y_test)\n                accscore=accuracy_score(testpredi,Y_test)\n                li = [clunm,clanm,test_score,accscore]\n                results.append(li)                \n                print(confusion_matrix(testpredi,Y_test))\n\n                plt.title(clanm+'test accuracy versus unknown:'+np.str(test_score)+' '+np.str(accscore)+' and test confusionmatrix')\n                plt.scatter(x=Y_test, y=testpredi, marker='.', alpha=1)\n                plt.scatter(x=[np.mean(Y_test)], y=[np.mean(testpredi)], marker='o', color='red')\n                plt.xlabel('Real test'); plt.ylabel('Pred. test')\n                plt.show()\n\n\n            else:\n                testpredlabel=le.inverse_transform(testpredi)  #use if you labellezid the classes \n#                testpredlabel=testpredi\n\n                submit = pd.DataFrame({indexv: dtest[indexv],label: testpredlabel})\n                filenaam='subm_'+clunm+'_'+clanm+'.csv'\n                submit.to_csv(path_or_buf =filenaam, index=False)\n                \n            print(clanm,'0 classifier time',datetime.datetime.now()-start)\n            \n    if len(dtest)==0:       \n        print(pd.DataFrame(results).sort_values(3))\n        submit=[]\n    return submit\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass Dummy( ):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self._feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def fit_transform( self, X, y = None ):\n        return X \n\n#clustertechniques(train[significant_features],'target','id',[],10000)\nclustertechniques(vehper.drop(['Case Vehicle ID','Partial VIN'],axis=1).fillna(0),'Injury Severity','Case Vehicle ID',[],100000)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}