{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport sklearn.metrics\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport numpy as np\nimport lightgbm as lgb\n    \nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['kc_house_data.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"root = Path(\"../input\")\ntrain = pd.read_csv(root.joinpath(\"kc_house_data.csv\"))\ntrain.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"           id             date     ...      sqft_living15  sqft_lot15\n0  7129300520  20141013T000000     ...               1340        5650\n1  6414100192  20141209T000000     ...               1690        7639\n2  5631500400  20150225T000000     ...               2720        8062\n3  2487200875  20141209T000000     ...               1360        5000\n4  1954400510  20150218T000000     ...               1800        7503\n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>condition</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>sqft_basement</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7129300520</td>\n      <td>20141013T000000</td>\n      <td>221900.0</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>1180</td>\n      <td>5650</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1180</td>\n      <td>0</td>\n      <td>1955</td>\n      <td>0</td>\n      <td>98178</td>\n      <td>47.5112</td>\n      <td>-122.257</td>\n      <td>1340</td>\n      <td>5650</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6414100192</td>\n      <td>20141209T000000</td>\n      <td>538000.0</td>\n      <td>3</td>\n      <td>2.25</td>\n      <td>2570</td>\n      <td>7242</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>2170</td>\n      <td>400</td>\n      <td>1951</td>\n      <td>1991</td>\n      <td>98125</td>\n      <td>47.7210</td>\n      <td>-122.319</td>\n      <td>1690</td>\n      <td>7639</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5631500400</td>\n      <td>20150225T000000</td>\n      <td>180000.0</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>770</td>\n      <td>10000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>6</td>\n      <td>770</td>\n      <td>0</td>\n      <td>1933</td>\n      <td>0</td>\n      <td>98028</td>\n      <td>47.7379</td>\n      <td>-122.233</td>\n      <td>2720</td>\n      <td>8062</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2487200875</td>\n      <td>20141209T000000</td>\n      <td>604000.0</td>\n      <td>4</td>\n      <td>3.00</td>\n      <td>1960</td>\n      <td>5000</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1050</td>\n      <td>910</td>\n      <td>1965</td>\n      <td>0</td>\n      <td>98136</td>\n      <td>47.5208</td>\n      <td>-122.393</td>\n      <td>1360</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1954400510</td>\n      <td>20150218T000000</td>\n      <td>510000.0</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>1680</td>\n      <td>8080</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1680</td>\n      <td>0</td>\n      <td>1987</td>\n      <td>0</td>\n      <td>98074</td>\n      <td>47.6168</td>\n      <td>-122.045</td>\n      <td>1800</td>\n      <td>7503</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train.drop(columns=[\"id\",'date'],inplace=True)\n\ntarget='price'\nX_train, X_test, y_train, y_test = train_test_split(train.drop(columns=['price']), train['price'], test_size=0.33, random_state=42)\nX_train.reset_index(inplace =True,drop=True)\nX_test.reset_index(inplace =True,drop=True)\ny_train.reset_index(inplace =True,drop=True)\ny_test.reset_index(inplace =True,drop=True)\nX_train[\"price\"]=y_train\nX_test.head()\n","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   bedrooms  bathrooms     ...      sqft_living15  sqft_lot15\n0         4       2.25     ...               2390        7700\n1         5       3.00     ...               2370        6283\n2         4       2.50     ...               3710        9685\n3         3       3.50     ...               4050       14226\n4         3       2.50     ...               2250        4050\n\n[5 rows x 18 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>sqft_living</th>\n      <th>sqft_lot</th>\n      <th>floors</th>\n      <th>waterfront</th>\n      <th>view</th>\n      <th>condition</th>\n      <th>grade</th>\n      <th>sqft_above</th>\n      <th>sqft_basement</th>\n      <th>yr_built</th>\n      <th>yr_renovated</th>\n      <th>zipcode</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>sqft_living15</th>\n      <th>sqft_lot15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>2.25</td>\n      <td>2070</td>\n      <td>8893</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>8</td>\n      <td>2070</td>\n      <td>0</td>\n      <td>1986</td>\n      <td>0</td>\n      <td>98058</td>\n      <td>47.4388</td>\n      <td>-122.162</td>\n      <td>2390</td>\n      <td>7700</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>3.00</td>\n      <td>2900</td>\n      <td>6730</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>8</td>\n      <td>1830</td>\n      <td>1070</td>\n      <td>1977</td>\n      <td>0</td>\n      <td>98115</td>\n      <td>47.6784</td>\n      <td>-122.285</td>\n      <td>2370</td>\n      <td>6283</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>2.50</td>\n      <td>3770</td>\n      <td>10893</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>11</td>\n      <td>3770</td>\n      <td>0</td>\n      <td>1997</td>\n      <td>0</td>\n      <td>98006</td>\n      <td>47.5646</td>\n      <td>-122.129</td>\n      <td>3710</td>\n      <td>9685</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3.50</td>\n      <td>4560</td>\n      <td>14608</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>12</td>\n      <td>4560</td>\n      <td>0</td>\n      <td>1990</td>\n      <td>0</td>\n      <td>98034</td>\n      <td>47.6995</td>\n      <td>-122.228</td>\n      <td>4050</td>\n      <td>14226</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>2.50</td>\n      <td>2550</td>\n      <td>5376</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>9</td>\n      <td>2550</td>\n      <td>0</td>\n      <td>2004</td>\n      <td>0</td>\n      <td>98052</td>\n      <td>47.6647</td>\n      <td>-122.083</td>\n      <td>2250</td>\n      <td>4050</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_xgb(X_train,Y_train,X_val,Y_val,X_test,parmaters,features_name): \n    d_train = xgb.Dataset(X_train, Y_train,feature_names=features_name)\n    d_valid = xgb.Dataset(X_val, Y_val,feature_names=features_name)\n    d_test = xgb.DMatrix(X_test,feature_names=features_name)\n    list_track = [(d_train, 'train'), (d_valid, 'valid')]\n    model = xgb.train(parmaters, d_train, 2000,  list_track, maximize=False, verbose_eval=50, early_stopping_rounds=50)\n    train_pred =model.predict(d_train)              \n    valid_pred =model.predict(d_valid)   \n    test_pred = model.predict(d_test)\n    return train_pred ,valid_pred,test_pred,model\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_LightGBM(X_train,Y_train,X_val,Y_val,X_test,parmaters): \n    d_train = lgb.Dataset(X_train,label=Y_train)\n    d_valid = lgb.Dataset(X_val, label=Y_val)\n    d_test = lgb.Dataset(X_test)\n    \n    model = lgb.train(params=parmaters, train_set=d_train, num_boost_round=2000,\n                      valid_sets=[d_valid], verbose_eval=50, early_stopping_rounds=50)\n    train_pred =model.predict(X_train)              \n    valid_pred =model.predict(X_val)   \n    test_pred = model.predict(X_test)\n    return train_pred ,valid_pred,test_pred,model","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_kfold_xgb(X_train,Y_train,X_test,parmaters,features_name,split=5):\n    final_train_pred=np.zeros_like(Y_train)\n    final_test_pred=np.zeros(len(X_test))\n    \n    kf = KFold(n_splits=split,random_state=2222)\n    i=1\n    for train_index, val_index in kf.split(X_train):\n        print(\"fold:\"+str(i))\n        train_fold_features, val_fold_features = X_train.loc[train_index], X_train.loc[val_index]\n        train_fold_target, val_fold_target = Y_train.loc[train_index], Y_train.loc[val_index] \n        train_pred ,valid_pred,test_pred,model=train_model_xgb( \n                                                        X_train=train_fold_features,\n                                                        Y_train= train_fold_target,\n                                                        X_val= val_fold_features,\n                                                        Y_val= val_fold_target,\n                                                        X_test= X_test,\n                                                        parmaters=parmaters,\n                                                        features_name=features_name \n                                                    )\n        \n        final_train_pred[val_index]=valid_pred\n        final_test_pred=final_test_pred+test_pred/split\n        i=i+1\n    return final_train_pred,final_train_pred,model\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_kfold_LightGBM(X_train,Y_train,X_test,parmaters,split=5):\n    final_train_pred=np.zeros_like(Y_train)\n    final_test_pred=np.zeros(len(X_test))\n    \n    kf = KFold(n_splits=split,random_state=2222)\n    i=1\n    for train_index, val_index in kf.split(X_train):\n        print(\"fold:\"+str(i))\n        train_fold_features, val_fold_features = X_train.loc[train_index], X_train.loc[val_index]\n        train_fold_target, val_fold_target = Y_train.loc[train_index], Y_train.loc[val_index] \n        train_pred ,valid_pred,test_pred,model=train_model_LightGBM( \n                                                        X_train=train_fold_features,\n                                                        Y_train= train_fold_target,\n                                                        X_val= val_fold_features,\n                                                        Y_val= val_fold_target,\n                                                        X_test= X_test,\n                                                        parmaters=parmaters\n                                                    )\n        \n        final_train_pred[val_index]=valid_pred\n        final_test_pred=final_test_pred+test_pred/split\n        i=i+1\n    return final_train_pred,final_train_pred,model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_pred,test_pred,xgb_best_params,xgb_model=train_Xgboost(train_df=X_train,test_df=X_test,target='price',boosting_type='dart',metric='rmse')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_res,test_res=train_Xgboost(train_df=X_train,test_df=X_test,target='price',boosting_type='dart')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_Xgboost(train_df,test_df,target,Y,boosting_type='gbdt',metric='rmse') :\n    \n    import gc # garbej collector for mempry optimisation\n    gc.enable()\n    from sklearn.metrics import accuracy_score # to be changed in case of AUC,...\n    from sklearn.metrics import roc_auc_score,mean_squared_error\n    from sklearn.model_selection import train_test_split    \n    from sklearn.metrics import mean_absolute_error\n\n    X=train_df.drop(columns=[target])\n    Y=train_df[target]\n    #use this in case classification\n\n    \n    dtrain = xgb.DMatrix(X, label=Y)\n    dtest = xgb.DMatrix(test_df)\n\n  \n    \n    params = {\n        # Parameters that we are going to tune.\n        'booster': boosting_type,\n        'max_depth':6,\n        'min_child_weight': 1,\n        'eta':.3,\n        'subsample': 1,\n        'colsample_bytree': 1,\n        # Other parameters\n        'objective':'reg:linear'\n    }\n    params['eval_metric'] = metric\n    num_boost_round = 999\n    \n    \n    \n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics=metric,\n        early_stopping_rounds=10\n    )\n    print('Best MAE with cv : '+str(cv_results['test-'+str(metric)+'-mean'].min()))\n    \n    \n    \n    print('--Tunning Parameters max_depth and min_child_weight--')\n    # Define initial best params and MAE\n    min_mae = float(\"Inf\")\n    best_params = None\n    \n    gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n    ]\n\n    for max_depth, min_child_weight in gridsearch_params:\n        print(\"CV with max_depth={}, min_child_weight={}\".format(\n                                 max_depth,\n                                 min_child_weight))\n        # Update our parameters\n        params['max_depth'] = max_depth\n        params['min_child_weight'] = min_child_weight\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=42,\n            nfold=5,\n            metrics=metric,\n            early_stopping_rounds=10\n        )\n        # Update best MAE\n        mean_mae = cv_results['test-'+str(metric)+'-mean'].min()\n        boost_rounds = cv_results['test-'+str(metric)+'-mean'].argmin()\n        print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = (max_depth,min_child_weight)\n        \n    params['max_depth'] = best_params[0]\n    params['min_child_weight'] = best_params[1]\n    print('--Tunning Parameters subsample and colsample_bytree--')\n    gridsearch_params = [\n        (subsample, colsample)\n        for subsample in [i/10. for i in range(7,11)]\n        for colsample in [i/10. for i in range(7,11)]\n    ]\n    min_mae = float(\"Inf\")\n    best_params = None\n    # We start by the largest values and go down to the smallest\n    for subsample, colsample in reversed(gridsearch_params):\n        print(\"CV with subsample={}, colsample={}\".format(\n                                 subsample,\n                                 colsample))\n        # We update our parameters\n        params['subsample'] = subsample\n        params['colsample_bytree'] = colsample\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=42,\n            nfold=5,\n            metrics=metric,\n            early_stopping_rounds=10\n        )\n        # Update best score\n        mean_mae = cv_results['test-'+str(metric)+'-mean'].min()\n        boost_rounds = cv_results['test-'+str(metric)+'-mean'].argmin()\n        print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = (subsample,colsample)\n    print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n    params['subsample'] = best_params[0]\n    params['colsample_bytree'] = best_params[1]\n    best_params=0.1\n    for eta in [.3, .2, .1, .05, .01, .005]:\n        print(\"CV with eta={}\".format(eta))\n        # We update our parameters\n        params['eta'] = eta\n        # Run and time CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=42,\n            nfold=5,\n            metrics=metric,\n            early_stopping_rounds=10\n              )\n        # Update best score\n        mean_mae = cv_results['test-'+str(metric)+'-mean'].min()\n        boost_rounds = cv_results['test-'+str(metric)+'-mean'].argmin()\n        print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = eta\n    print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))\n    params['eta'] = best_params\n    print(\"Final Best params: {}\".format(params))\n    model = xgb.train(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        evals=[(dtest, \"Test\")],\n        early_stopping_rounds=10\n        )\n    print(\"Best MAE: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))\n\n    final_train_pred,final_train_pred,model=train_kfold_xgb(X_train=X,Y_train=Y,X_test=test_df,parmaters=params,features_name=X.columns,split=5)\n\n    return final_train_pred,final_train_pred,model\n\n    ","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_res,test_res=train_Xgboost(train_df=X_train,test_df=X_test,Y=X_train[\"price\"],target='price',boosting_type='dart')\n","execution_count":18,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n  if getattr(data, 'base', None) is not None and \\\n/opt/conda/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n  data.base is not None and isinstance(data, np.ndarray) \\\n","name":"stderr"},{"output_type":"stream","text":"Best MAE with cv : 125034.0203124\n--Tunning Parameters max_depth and min_child_weight--\nCV with max_depth=9, min_child_weight=5\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:79: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\nwill be corrected to return the positional minimum in the future.\nUse 'series.values.argmin' to get the position of the minimum now.\n","name":"stderr"},{"output_type":"stream","text":"\tMAE 126224.5499998 for 26 rounds\nCV with max_depth=9, min_child_weight=6\n\tMAE 123323.384375 for 69 rounds\nCV with max_depth=9, min_child_weight=7\n\tMAE 128112.4203126 for 37 rounds\nCV with max_depth=10, min_child_weight=5\n\tMAE 125249.69375020001 for 31 rounds\nCV with max_depth=10, min_child_weight=6\n\tMAE 125897.3125002 for 79 rounds\nCV with max_depth=10, min_child_weight=7\n\tMAE 127099.39999979999 for 23 rounds\nCV with max_depth=11, min_child_weight=5\n\tMAE 126420.85 for 42 rounds\nCV with max_depth=11, min_child_weight=6\n\tMAE 127377.3640624 for 47 rounds\nCV with max_depth=11, min_child_weight=7\n\tMAE 127690.2140626 for 32 rounds\n--Tunning Parameters subsample and colsample_bytree--\nCV with subsample=1.0, colsample=1.0\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:115: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\nwill be corrected to return the positional minimum in the future.\nUse 'series.values.argmin' to get the position of the minimum now.\n","name":"stderr"},{"output_type":"stream","text":"\tMAE 123323.3875 for 69 rounds\nCV with subsample=1.0, colsample=0.9\n\tMAE 123614.4609376 for 87 rounds\nCV with subsample=1.0, colsample=0.8\n\tMAE 124712.8421876 for 38 rounds\nCV with subsample=1.0, colsample=0.7\n\tMAE 125133.1015626 for 63 rounds\nCV with subsample=0.9, colsample=1.0\n\tMAE 127901.16875 for 36 rounds\nCV with subsample=0.9, colsample=0.9\n\tMAE 128885.0640626 for 30 rounds\nCV with subsample=0.9, colsample=0.8\n\tMAE 126477.2156248 for 37 rounds\nCV with subsample=0.9, colsample=0.7\n\tMAE 128678.1765624 for 46 rounds\nCV with subsample=0.8, colsample=1.0\n\tMAE 128729.0187502 for 30 rounds\nCV with subsample=0.8, colsample=0.9\n\tMAE 130510.7640626 for 39 rounds\nCV with subsample=0.8, colsample=0.8\n\tMAE 130420.5609374 for 15 rounds\nCV with subsample=0.8, colsample=0.7\n\tMAE 128154.946875 for 51 rounds\nCV with subsample=0.7, colsample=1.0\n\tMAE 130481.38125 for 33 rounds\nCV with subsample=0.7, colsample=0.9\n\tMAE 127912.3375 for 77 rounds\nCV with subsample=0.7, colsample=0.8\n\tMAE 127595.3 for 33 rounds\nCV with subsample=0.7, colsample=0.7\n\tMAE 129087.6265626 for 47 rounds\nBest params: 1.0, 1.0, MAE: 123323.3875\nCV with eta=0.3\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:140: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\nwill be corrected to return the positional minimum in the future.\nUse 'series.values.argmin' to get the position of the minimum now.\n","name":"stderr"},{"output_type":"stream","text":"\tMAE 123323.3875 for 69 rounds\n\nCV with eta=0.2\n\tMAE 124677.0765624 for 99 rounds\n\nCV with eta=0.1\n\tMAE 122630.0843748 for 151 rounds\n\nCV with eta=0.05\n\tMAE 121434.8218752 for 472 rounds\n\nCV with eta=0.01\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-4331a5aa585b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_res\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_Xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboosting_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dart'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-d9f3222739b6>\u001b[0m in \u001b[0;36mtrain_Xgboost\u001b[0;34m(train_df, test_df, target, Y, boosting_type, metric)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m               )\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Update best score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, iteration, feval)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;34m\"\"\"\"Evaluate the CVPack for one iteration.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36meval_set\u001b[0;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                                               \u001b[0mdmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m                                               \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                                               ctypes.byref(msg)))\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_LightGBM(train_df,test_df,target,cat_features=[],num_boost_round_param=5000,\n                   early_stopping_rounds_param=100,boosting_type='dart',metric='rmse') :\n    \n    import gc # garbej collector for mempry optimisation\n    gc.enable()\n    import lightgbm as lgb\n    from sklearn.metrics import accuracy_score # to be changed in case of AUC,...\n    from sklearn.metrics import roc_auc_score,mean_squared_error\n    from sklearn.model_selection import train_test_split    \n    from sklearn.metrics import mean_absolute_error\n\n    X=train_df.drop(columns=[target])\n    Y=train_df[target]\n    #X_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.20, random_state=42)\n    #use this in case classification\n    '''X_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.33, stratify=Y,random_state=42)'''\n\n    \n    dtrain =lgb.Dataset(X, label=y_train)\n    #dtest = lgb.Dataset(Y, label=y_test)\n\n    \n    # \"Learn\" the mean from the training data\n    mean_train = np.mean(y_train)\n    # Get predictions on the test set\n    baseline_predictions = np.ones(y_test.shape) * mean_train\n    # Compute MAE\n    mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n    \n    print(\"Baseline MAE is {:.2f}\".format(mae_baseline))\n    \n    \n  \n    num_boost_round = num_boost_round_param\n    params = {}\n    params['learning_rate'] = 0.3\n    params['boosting_type'] = boosting_type,\n    params['objective'] = 'regression'\n    params['metric'] = metric\n \n   \n\n    \n    cv_results = lgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        stratified=False, # Default = TRUE !!!!\n        categorical_feature=cat_features,\n        nfold=5,\n        metrics=metric,\n        early_stopping_rounds=early_stopping_rounds_param\n    )\n    print('Best MAE with cv : '+str(np.min(cv_results[str(metric)+'-mean'])))    \n    \n    \n    print('--Tunning Parameters max_depth and min_child_weight and num_leaves--')\n    # Define initial best params and MAE\n    min_mae = float(\"Inf\")\n    best_params = (-1,20,31)\n    \n    gridsearch_params = [\n    (max_depth, min_data_in_leaf,num_leaves)\n    for max_depth in range(7,13)\n    for min_data_in_leaf in range(5,8)\n    for num_leaves in  [5,10,20,50]\n    ]\n\n    for max_depth, min_data_in_leaf,num_leaves in gridsearch_params:\n        print(\"CV with max_depth={}, min_data_in_leaf={},  num_leaves={}\".format(\n                                 max_depth,\n                                 min_data_in_leaf,\n                                num_leaves))\n        # Update our parameters\n        params['max_depth'] = max_depth\n        params['min_data_in_leaf'] = min_data_in_leaf\n        params['num_leaves'] = num_leaves\n        # Run CV\n        cv_results = lgb.cv(\n                    params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    seed=42,\n                    stratified=False, # Default = TRUE \n                    categorical_feature=cat_features,\n                    nfold=5,\n                    metrics=metric,\n                    early_stopping_rounds=early_stopping_rounds_param\n        )\n        # Update best MAE\n        mean_mae = np.min(cv_results[str(metric)+'-mean'])\n        boost_rounds = np.argmin(cv_results[str(metric)+'-mean'])\n        print(\"\\t\"+str(metric)+\" {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = (max_depth,min_data_in_leaf,num_leaves)\n        \n    params['max_depth'] = best_params[0]\n    params['min_data_in_leaf'] = best_params[1]\n    params['num_leaves'] = best_params[2]\n    \n    \n    \n    \n    print('--Tunning Parameters feature_fraction and bagging_fraction--')\n    \n    \n    # Define initial best params and MAE\n    min_mae = float(\"Inf\")\n    best_params = (1.0,1.0)\n    \n    gridsearch_params = [\n    (bagging_fraction, feature_fraction)\n    for bagging_fraction in [0.6,0.7,0.8,0.9]\n    for feature_fraction in [0.6,0.7,0.8,0.9]\n    ]\n\n    for feature_fraction, bagging_fraction in gridsearch_params:\n        print(\"CV with bagging_fraction={}, feature_fraction={}, \".format(\n                                 bagging_fraction,\n                                 feature_fraction\n                                ))\n        # Update our parameters\n        params['feature_fraction'] = feature_fraction\n        params['bagging_fraction'] = bagging_fraction\n        \n        # Run CV\n        cv_results = lgb.cv(\n                    params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    seed=42,\n                    stratified=False, # Default = TRUE \n                    categorical_feature=cat_features,\n                    nfold=5,\n                    metrics=metric,\n                    early_stopping_rounds=early_stopping_rounds_param\n        )\n        # Update best MAE\n        mean_mae = np.min(cv_results[str(metric)+'-mean'])\n        boost_rounds = np.argmin(cv_results[str(metric)+'-mean'])\n        print(\"\\t\"+str(metric)+\" {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = (feature_fraction,bagging_fraction)\n        \n    params['feature_fraction'] = best_params[0]\n    params['bagging_fraction'] = best_params[1]\n    \n    \n    \n    \n    \n    print('--Tunning Parameter Learning rate --')\n \n    \n    # Define initial best params and MAE\n    min_mae = float(\"Inf\")\n    best_params = 0.3\n\n    for learning_rate in [ .2, .1, .05, .01, .005]:\n        print(\"CV with Learning Rate={} \".format(\n                                 learning_rate\n                                ))\n        # Update our parameters\n        params['learning_rate'] = learning_rate\n        \n        # Run CV\n        cv_results = lgb.cv(\n                    params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    seed=42,\n                    stratified=False, # Default = TRUE \n                    categorical_feature=cat_features,\n                    nfold=5,\n                    metrics=metric,\n                    early_stopping_rounds=early_stopping_rounds_param\n        )\n        # Update best MAE\n        mean_mae = np.min(cv_results[str(metric)+'-mean'])\n        boost_rounds = np.argmin(cv_results[str(metric)+'-mean'])\n        print(\"\\t\"+str(metric)+\" {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = learning_rate\n        \n    params['learning_rate'] = best_params\n    \n    \n    \n        \n    print('--Tunning Parameter min_data_in_leaf and lambda_l1, lambda_l2  --')\n \n    \n    # Define initial best params and MAE\n    min_mae = float(\"Inf\")\n    best_params = (20,0,0)\n\n    gridsearch_params = [\n    (min_data_in_leaf , lambda_l1,lambda_l2)\n    for min_data_in_leaf in [30,40,50] \n    for lambda_l1 in [0.1,0.2]\n    for lambda_l2 in  [0.1,0.2]\n    ]\n\n    for min_data_in_leaf, lambda_l1,lambda_l2 in gridsearch_params:\n        print(\"CV with min_data_in_leaf={}, lambda_l1={},lambda_l2={}  \".format(\n                                 bagging_fraction,\n                                 lambda_l1,\n                                 lambda_l2\n                                ))\n        # Update our parameters\n        params['min_data_in_leaf'] = min_data_in_leaf\n        params['lambda_l1'] = lambda_l1\n        params['lambda_l2'] = lambda_l2\n        \n        # Run CV\n        cv_results = lgb.cv(\n                    params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    seed=42,\n                    stratified=False, # Default = TRUE \n                    categorical_feature=cat_features,\n                    nfold=5,\n                    metrics=metric,\n                    early_stopping_rounds=early_stopping_rounds_param\n        )\n        # Update best MAE\n        mean_mae = np.min(cv_results[str(metric)+'-mean'])\n        boost_rounds = np.argmin(cv_results[str(metric)+'-mean'])\n        print(\"\\t\"+str(metric)+\" {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = (min_data_in_leaf, lambda_l1,lambda_l2)\n        \n    params['min_data_in_leaf'] = best_params[0]\n    params['lambda_l1'] = best_params[1]\n    params['lambda_l2'] = best_params[2]\n    \n    \n    \n    #last : max_bin\n    print('--Tunning Parameters max_bin --')\n    # Define initial best params and MAE\n    min_mae = float(\"Inf\")\n    best_params = 63\n    for max_bin in [63,128,256]:\n        print(\"CV with max_bin={}\".format(max_bin))\n        # We update our parameters\n        params['max_bin'] = max_bin\n        # Run CV\n        cv_results = lgb.cv(\n                    params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    seed=42,\n                    stratified=False, # Default = TRUE \n                    categorical_feature=cat_features,\n                    nfold=5,\n                    metrics=metric,\n                    early_stopping_rounds=early_stopping_rounds_param\n        )\n         # Update best MAE\n        mean_mae = np.min(cv_results[str(metric)+'-mean'])\n        boost_rounds = np.argmin(cv_results[str(metric)+'-mean'])\n        print(\"\\t\"+str(metric)+\" {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = max_bin\n        \n    params['max_bin'] = best_params\n    \n    print(\"Final Best params: {}\".format(params))\n    cv_results = lgb.cv(\n                    params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    seed=42,\n                    stratified=False, # Default = TRUE \n                    categorical_feature=cat_features,\n                    nfold=5,\n                    metrics=metric,\n                    early_stopping_rounds=early_stopping_rounds_param\n        )\n    # Update best MAE\n    mean_mae = np.min(cv_results[str(metric)+'-mean'])\n    boost_rounds = np.argmin(cv_results[str(metric)+'-mean'])\n    print(\"\\t\"+str(metric)+\" {} for {} rounds\".format(mean_mae, boost_rounds))\n    \n\n    final_train_pred,final_train_pred,model=train_kfold_LightGBM(X_train=X,Y_train=Y,\n                                                           X_test=test_df,parmaters=params,\n                                                  split=5)\n\n    return final_train_pred,final_train_pred,model\n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#train_res,test_res,lgb_model=train_LightGBM(train_df=X_train,test_df=X_test,target='price',boosting_type='gbdt')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_Catboost(train_df,test_df,target,cat_features=[],num_boost_round_param=5000,\n                   early_stopping_rounds_param=100,boosting_type='dart',metric='rmse') :\n    \n    import gc # garbej collector for mempry optimisation\n    gc.enable()\n    import catboost as cat\n\n\n    print(format('How to find optimal parameters for CatBoost using GridSearchCV for Regression','*^82'))    \n    \n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    \n    # load libraries\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n    from sklearn.model_selection import GridSearchCV\n    import catboost \n\n    X=train_df.drop(columns=[target])\n    y=train_df[target]\n\n\n    model = CatBoostRegressor(eval_metric=metric,\n                              iterations=num_boost_round_param,\n                              boosting_type=boosting_type,\n                              categorical_feature=cat_features,\n                              task_type = \"GPU\")\n    parameters = {'depth'         : [6,8,10,13,16],\n                  'learning_rate' : [0.01, 0.05, 0.1],\n                  'l2_leaf_reg':[0,2,4,8],\n                  'bagging_temperature':[0,1,5,10,20],\n                  \n                 }\n    grid = GridSearchCV(estimator=model, param_grid = parameters, cv = 5, n_jobs=-1)\n    grid.fit(X_train, y_train)    \n\n    # Results from Grid Search\n    print(\"\\n========================================================\")\n    print(\" Results from Grid Search \" )\n    print(\"========================================================\")    \n    \n    print(\"\\n The best estimator across ALL searched params:\\n\",\n          grid.best_estimator_)\n    \n    print(\"\\n The best score across ALL searched params:\\n\",\n          grid.best_score_)\n    \n    print(\"\\n The best parameters across ALL searched params:\\n\",\n          grid.best_params_)\n    \n    print(\"\\n ========================================================\")\n    \n    \n    final_train_pred,final_train_pred,model=train_kfold_Catboost(X_train=X,Y_train=Y,\n                                                           X_test=test_df,parmaters=best_params_,\n                                                  split=5)\n\n    return final_train_pred,final_train_pred,model\n    \n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_res,test_res,lgb_model=train_Catboost(train_df=X_train,test_df=X_test,target='price',boosting_type='gbdt')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}