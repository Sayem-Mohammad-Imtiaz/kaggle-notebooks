{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings; warnings.filterwarnings('ignore')\nimport numpy as np,pylab as pl\nimport sys,h5py,urllib,zipfile\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom keras.datasets import mnist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Neuron","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions for neuron construction\ndef sigmoid(x): return 1.0/(1+np.exp(-x))\ndef sigmoid_derivation(x): return x*(1.0-x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# artificial data for the experiment\nX=np.array([[.11,.05,.95],[.09,.03,.08],[.01,.09,.91],\n            [.04,.92,.07],[.05,.02,.04],[.07,.97,.05],\n            [.06,.02,.98],[.02,.06,.03],[.01,.09,.03],\n            [.02,.94,.01],[.06,.03,.95],[.04,.91,.09]])\nY=np.array([[1,0,1,2,0,2,1,0,1,2,1,2]]).T\npl.imshow(X.T,cmap=pl.cm.cool); pl.xticks([])\npl.text(-.3,1,str(Y.T)[2:-2],c='white',fontsize=29);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# the start point for iterations\nsynapse0=np.random.randn(3,1); layer0=X\n# the steps of iteration\nfor iter in range(100):\n    layer1=sigmoid(np.dot(layer0,synapse0))\n    # finding errors and directions for correction\n    layer1_error=layer1-Y\n    layer1_delta=layer1_error*sigmoid_derivation(layer1)\n    # correction values & coefficients\n    synapse0_derivative=np.dot(layer0.T,layer1_delta)\n    synapse0-=synapse0_derivative\npl.scatter(list(range(1,13)),layer1,c=Y,cmap=pl.cm.cool);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing predictions and real data\n# the error in the start labeling (the 9th image) had been corrected\ndef labeling(x):\n    if (x<.7): return 0\n    if (x>.7) and (x<.99): return 1\n    else: return 2\npredict_Y=np.array([[labeling(layer1[i,0]) for i in range(12)]]).T\npl.imshow(X.T,cmap=pl.cm.cool); pl.xticks([])\npl.text(-.3,1,str(predict_Y.T)[2:-2],c='white',fontsize=29)\nprint(np.hstack((np.hstack((X,Y)),\n                (layer1.round(3)),(predict_Y))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Simple NN (MLP)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class NeuralNetMLP(object):\n    def __init__(self,n_output,n_features,n_hidden=30,\n                 l1=0.0,l2=0.0,epochs=500,eta=.001, \n                 alpha=0.0,decrease_const=0.0,shuffle=True, \n                 minibatches=1,random_state=None):\n        np.random.seed(random_state)\n        self.n_output=n_output; self.n_features=n_features\n        self.n_hidden=n_hidden; self.epochs=epochs\n        self.w1,self.w2=self._initialize_weights()\n        self.l1=l1; self.l2=l2; self.eta=eta; self.alpha=alpha\n        self.decrease_const=decrease_const\n        self.shuffle=shuffle\n        self.minibatches=minibatches\n    def _encode_labels(self,y,k):\n        onehot=np.zeros((k,y.shape[0]))\n        for idx,val in enumerate(y):\n            onehot[val,idx]=1.\n        return onehot\n    def _initialize_weights(self):\n        w1=np.random.uniform(-1.0,1.0,\n                             size=self.n_hidden*(self.n_features+1))\n        w1=w1.reshape(self.n_hidden,self.n_features+1)        \n        w2=np.random.uniform(-1.0,1.0,\n                             size=self.n_output*(self.n_hidden+1))\n        w2=w2.reshape(self.n_output,self.n_hidden+1)        \n        return w1,w2\n    def _sigmoid(self,z): return 1.0/(1.0+np.exp(-z))\n    def _sigmoid_gradient(self,z):\n        sg=self._sigmoid(z)\n        return sg*(1-sg)\n    def _add_bias_unit(self,X,how='column'):\n        if how=='column':\n            X_new=np.ones((X.shape[0],X.shape[1]+1))\n            X_new[:,1:]=X\n        elif how=='row':\n            X_new=np.ones((X.shape[0]+1,X.shape[1]))\n            X_new[1:,:]=X\n        else:\n            mes='`how` must be `column` or `row`'\n            raise AttributeError(mes)\n        return X_new\n    def _feedforward(self,X,w1,w2):\n        a1=self._add_bias_unit(X,how='column')\n        z2=w1.dot(a1.T)\n        a2=self._sigmoid(z2)\n        a2=self._add_bias_unit(a2,how='row')\n        z3=w2.dot(a2)\n        a3=self._sigmoid(z3)\n        return a1,z2,a2,z3,a3\n    def _L2_reg(self,lambda_,w1,w2):\n        return (lambda_/2.0)*(np.sum(w1[:,1:]**2)+\\\n                              np.sum(w2[:,1:]**2))\n    def _L1_reg(self,lambda_,w1,w2):\n        return (lambda_/2.0)*(np.abs(w1[:,1:]).sum()+\\\n                              np.abs(w2[:,1:]).sum())\n    def _get_cost(self,y_enc,output,w1,w2):\n        term1=-y_enc*(np.log(output))\n        term2=(1-y_enc)*np.log(1-output)\n        cost=np.sum(term1-term2)\n        L1_term=self._L1_reg(self.l1,w1,w2)\n        L2_term=self._L2_reg(self.l2,w1,w2)\n        cost=cost+L1_term+L2_term\n        return cost\n    def _get_gradient(self,a1,a2,a3,z2,y_enc,w1,w2):\n        sigma3=a3-y_enc\n        z2=self._add_bias_unit(z2,how='row')\n        sigma2=w2.T.dot(sigma3)*self._sigmoid_gradient(z2)\n        sigma2=sigma2[1:,:]\n        grad1=sigma2.dot(a1); grad2=sigma3.dot(a2.T)\n        grad1[:,1:]+=(w1[:,1:]*(self.l1+self.l2))\n        grad2[:,1:]+=(w2[:,1:]*(self.l1+self.l2))\n        return grad1,grad2\n    def predict(self,X):\n        a1,z2,a2,z3,a3=self._feedforward(X,self.w1,self.w2)\n        y_pred=np.argmax(z3,axis=0)\n        return y_pred\n    def fit(self,X,y,print_progress=False):\n        self.cost_=[]; X_data,y_data=X.copy(),y.copy()\n        y_enc=self._encode_labels(y,self.n_output)\n        delta_w1_prev=np.zeros(self.w1.shape)\n        delta_w2_prev=np.zeros(self.w2.shape)\n        for i in range(self.epochs):\n            self.eta/=(1+self.decrease_const*i)\n            if print_progress:\n                sys.stderr.write('\\rEpoch: %d/%d'%(i+1,self.epochs))\n                sys.stderr.flush()\n            if self.shuffle:\n                idx=np.random.permutation(y_data.shape[0])\n                X_data,y_enc=X_data[idx],y_enc[:,idx]\n            mini=np.array_split(range(y_data.shape[0]),self.minibatches)            \n            for idx in mini:\n                a1,z2,a2,z3,a3=self._feedforward(X_data[idx],\n                                                 self.w1,self.w2)\n                cost=self._get_cost(y_enc=y_enc[:,idx],\n                                    output=a3,w1=self.w1,w2=self.w2)                \n                self.cost_.append(cost)\n                grad1,grad2=self._get_gradient(a1=a1,a2=a2,a3=a3,z2=z2,\n                                               y_enc=y_enc[:,idx],\n                                               w1=self.w1,w2=self.w2)\n                delta_w1,delta_w2=self.eta*grad1,self.eta*grad2\n                self.w1-=(delta_w1+(self.alpha*delta_w1_prev))\n                self.w2-=(delta_w2+(self.alpha*delta_w2_prev))\n                delta_w1_prev,delta_w2_prev=delta_w1,delta_w2\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"digits=datasets.load_digits(n_class=10)\nX1,y1=digits.data,digits.target\nX_train1,X_test1,y_train1,y_test1=\\\ntrain_test_split(X1,y1,test_size=.2,random_state=1)\nimg=np.zeros((100,100))\nfor i in range(10):\n    ix=10*i+1\n    for j in range(10):\n        iy=10*j+1\n        img[ix:ix+8,iy:iy+8]=X1[i*10+j].reshape((8,8))\npl.figure(figsize=(5,5))\npl.imshow(img,cmap=pl.cm.cool)\npl.xticks([]); pl.yticks([]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn1=NeuralNetMLP(n_output=10,n_features=X1.shape[1], \n                 n_hidden=128,l2=.1,l1=0.0,epochs=1000, \n                 eta=.01,alpha=.01,decrease_const=.001,\n                 shuffle=True,minibatches=128,random_state=0)\nnn1.fit(X_train1,y_train1,print_progress=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"py_train1=nn1.predict(X_train1)\ntrain_accuracy1=np.sum(y_train1==py_train1,\n                       axis=0)/len(y_train1)\nprint('Digits. Train accuracy: %.2f%%'%(train_accuracy1*100))\npy_test1=nn1.predict(X_test1)\ntest_accuracy1=np.sum(y_test1==py_test1,\n                      axis=0)/len(y_test1)\nprint('Digits. Test accuracy: %.2f%%'%(test_accuracy1*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_train2,y_train2),(X_test2,y_test2)=mnist.load_data()\nfig,ax=pl.subplots(figsize=(11,4),nrows=2,ncols=5,\n                   sharex=True,sharey=True)\nax=ax.flatten()\nfor i in range(10):\n    ax[i].imshow(X_train2[i],cmap=pl.cm.cool)\n    ax[i].set_title(y_train2[i])\nax[0].set_xticks([]); ax[0].set_yticks([])\npl.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn2=NeuralNetMLP(n_output=10,n_features=784, \n                 n_hidden=196,l2=.01,l1=0.0,epochs=1000, \n                 eta=.001,alpha=.001,decrease_const=.00001,\n                shuffle=True,minibatches=50,random_state=1)\nnn2.fit(X_train2.reshape(-1,784),y_train2,print_progress=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"py_train2=nn2.predict(X_train2.reshape(-1,784))\ntrain_accuracy2=np.sum(y_train2==py_train2,\n                       axis=0)/len(y_train2)\nprint('MNIST. Train accuracy: %.2f%%'%(train_accuracy2*100))\npy_test2=nn2.predict(X_test2.reshape(-1,784))\ntest_accuracy2=np.sum(y_test2==py_test2,\n                      axis=0)/len(y_test2)\nprint('MNIST. Test accuracy: %.2f%%'%(test_accuracy2*100))\nbatches=np.array_split(range(len(nn2.cost_)),1000)\ncost_ar=np.array(nn2.cost_)\ncost_avg=[np.mean(cost_ar[i]) for i in batches]\npl.figure(figsize=(11,6))\npl.plot(range(len(cost_avg)),cost_avg,lw=3,color='#3333ff')\npl.xlabel('Epochs'); pl.ylim([0,2000]); pl.ylabel('Cost')\npl.tight_layout(); pl.grid(); pl.title('Cost Function');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_labeled=X_test2[y_test2!=py_test2][:15]\ncorrect_labels=y_test2[y_test2!=py_test2][:15]\nwrong_labels=py_test2[y_test2!=py_test2][:15]\nfig,ax=pl.subplots(nrows=2,ncols=5,sharex=True,\n                   sharey=True,figsize=(11,4))\nax=ax.flatten()\nfor i in range(10):\n    image=wrong_labeled[i].reshape(28,28)\n    ax[i].imshow(image,cmap=pl.cm.cool)\n    l=(i+1,correct_labels[i],wrong_labels[i])\n    ax[i].set_title('%d) r: %d p: %d'%l)\nax[0].set_xticks([]); ax[0].set_yticks([])\npl.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpath='../input/classification-of-handwritten-letters/'\nf=h5py.File(fpath+'LetterColorImages2.h5','r')\nkeys=list(f.keys()); rn=10**3\nletters=u'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\nX3=np.array(f[keys[1]])/255\n#X3=np.dot(X3[...,:3],[.299,.587,.114])\ny3=np.array(f[keys[2]])-1\nX_train3,X_test3,y_train3,y_test3=\\\ntrain_test_split(X3,y3,test_size=.2,random_state=1)\npl.figure(figsize=(2,3))\npl.title('label: \"%s\"'%letters[y3[rn]],\n         fontsize=20)\npl.imshow(X3[rn],cmap=pl.cm.cool);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn3=NeuralNetMLP(n_output=33,n_features=3072, \n                 n_hidden=512,l2=.01,l1=0.0,epochs=1000, \n                 eta=.001,alpha=.001,decrease_const=.00001,\n                 shuffle=True,minibatches=64,random_state=1)\nnn3.fit(X_train3.reshape(-1,3072),y_train3,print_progress=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"py_train3=nn3.predict(X_train3.reshape(-1,3072))\ntrain_accuracy3=np.sum(y_train3==py_train3,\n                       axis=0)/len(y_train3)\nprint('Letters. Train accuracy: %.2f%%'%(train_accuracy3*100))\npy_test3=nn3.predict(X_test3.reshape(-1,3072))\ntest_accuracy3=np.sum(y_test3==py_test3,\n                      axis=0)/len(y_test3)\nprint('Letters. Test accuracy: %.2f%%'%(test_accuracy3*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}