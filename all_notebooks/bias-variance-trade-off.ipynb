{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can download the data from here - http://faculty.marshall.usc.edu/gareth-james/ISL/data.html ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ad  = pd.read_csv(\"../input/advertising.csv/Advertising.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad.drop(\"Unnamed: 0\", axis=1, inplace=True)\nad.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Bias","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We will use Linear Regression to display bias or underfitting**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Linear Regression \nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(ad.TV.values.reshape(-1,1), ad.sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.subplot(111, facecolor='black')\nplt.scatter(ad.TV, ad.sales, color='gold')\nplt.plot(ad.TV,model.predict(ad.TV.values.reshape(-1,1)),color='red',linewidth=4)\nplt.text(0,22,\"Restrictive Model\\nHigh Bias\\nUnderfitting\", color='white',size=15)\nplt.text(220,1,\"@DataScienceWithSan\", color='white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple Linear Regression is an inflexible model that assumes a linear relationship between input and output variables. This assumption, approximation, and restriction introduce **bias** to this model.\n\n*Hence bias refers to the error which is observed while approximating a complex problem using a simple (or restrictive) model.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating random data\nnp.random.seed(0)\nx = np.random.normal(0,1,100)\ny = 30 + 4*x - 2*(x**2) + 3*(x**3) + np.random.normal(0,1,100)\nplt.figure(figsize=(12,4))\n\nplt.subplot(121)\nplt.ylim(10,60)\nplt.xlim(-1,2)\nplt.scatter(x,y,s=20,c='seagreen')\nplt.plot(x, 27+12*x, color='orange', linewidth=1)\n\nplt.subplot(122)\nplt.ylim(10,60)\nplt.xlim(-1,2)\nplt.scatter(x,y,s=20,c='seagreen')\nx2 = np.linspace(-3,3,50)\nplt.plot(x2,30 + 4*x2 - 2*(x2**2) + 3*(x2**3), c='orange', linewidth=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot on the right is quite more flexible than the one on the left. It fits more smoothly with the data. On the other hand, the plot on the left represents a poorly fitted model, which assumes a linear relationship in data. This poor-fitting due to high bias is also known as ***underfitting***. Underfitting results in poor performance and low accuracies and can be rectified if needed by using more flexible models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let’s summarise the key points about bias:\n* Bias is introduced when restrictive (inflexible) models are used to solve complex problems\n* As the flexibility of a model increases, the bias starts decreasing for training data.\n* Bias can cause underfitting, which further leads to poor performance and predictions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## The Variance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Performing KNN on above data to show high variance or overfitting**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=1, weights='distance')\nknn.fit(ad.TV.values.reshape(-1,1), ad.sales)\n\nx_points_ad = np.linspace(0,300,100)\ny_knn_ad = knn.predict(x_points_ad.reshape(-1,1))\n\nplt.figure(figsize=(8,5))\nplt.subplot(111, facecolor='black')\nplt.scatter(ad.TV, ad.sales, color='gold')\nplt.plot(x_points_ad, y_knn_ad, color='red',linewidth=4)\nplt.text(0,22,\"Complex Model\\nHigh Variance\\nOverfitting\", color='white', size=15)\nplt.text(220,1,\"@DataScienceWithSan\", color='white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Machine Learning, when a model performs so well on the training dataset, that it almost memorizes every outcome, it is likely to perform quite badly when running for testing dataset.\n\nThis phenomenon is known as ***overfitting*** and is generally observed while building very complex and flexible models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"What is Variance?\n\n***Variance is the amount by which our model will have to change if it were to make predictions on a different dataset.***\n\nLet’s simplify the above statement → Our model should not yield high errors if we use it to estimate the outputs of unseen data. That is if the model shows good results on the training dataset, but poor results on testing, it is said to have high variance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hence, building a very complex model can come at the cost of overfitting. One must understand that too much learning can bring high variance.\n\nLet’s see some of the key points about Variance.\n* Variance is the amount by which a model needs changing if it were to make predictions on unseen data.\n* High variance is equivalent to the overfitting of the model.\n* Restrictive models such as Linear Regression show low variance, whereas more complex and flexible models can introduce high variance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Making Random Data\n\nnp.random.seed(0)\nx = np.random.normal(0,10,50)\ny = 0.1*x + 0.01*(x**2) + 0.01*(x**3) + np.random.normal(0,10,50)\n\nx = np.array(x).reshape(-1,1)\ny = np.array(y).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\n\n## Fitting and plotting Linear Regression\nregression_model = LinearRegression()\nregression_model.fit(x,y)\nplt.subplot(221)\nplt.scatter(x, y, edgecolor='skyblue',color='royalblue')\nplt.plot(x,regression_model.predict(x),color='orange',linewidth=1)\nplt.title(\"Figure 1 - Linear Regression\")\n\n#############################################################################\n\n## Fitting and plotting polynomial regression\nx_points = np.linspace(-25,25,100)\ny2 =  0.1*x_points + 0.01*(x_points**2) + 0.01*(x_points**3)\nplt.ylim(-150,150)\nplt.subplot(222)\nplt.scatter(x, y, edgecolor='skyblue',color='royalblue')\nplt.plot(x_points,y2,color='orange',linewidth=2)\nplt.title(\"Figure 2 - Polynomial Regression\")\n\n#############################################################################\n\n## Fitting and plotting KNN with high k\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=9, weights='distance')\nknn.fit(x, y)\ny_knn_h = knn.predict(x_points.reshape(-1,1))\nplt.subplot(223)\nplt.scatter(x, y, edgecolor='skyblue', color='royalblue')\nplt.plot(x_points, y_knn_h, color='orange',linewidth=2)\nplt.title(\"Figure 3 - KNN with 9 nearest neighbors\")\n\n#############################################################################\n\n## Fitting and plotting KNN with k=1\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=1, weights='distance')\nknn.fit(x, y)\ny_knn_h = knn.predict(x_points.reshape(-1,1))\n#plt.subplot(111,facecolor='navy')\nplt.subplot(224)\nplt.scatter(x, y, edgecolor='skyblue',color='royalblue' )\nplt.plot(x_points, y_knn_h, color='orange', linewidth=2)\nplt.title(\"Figure 4 - KNN with 1 nearest neighbor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now take a few seconds to observe these plots and see how increasing the complexity of our model to train on the same data reduces bias and underfitting, thus reducing the training error.\n\n* Figure 1 shows the most restrictive model that is the linear model as we saw earlier. The bias in this model is certainly the highest.\n* Figure 2, the polynomial regression, is a smoother curve and reduces the training error further, hence showing a somewhat lesser bias than the linear model.\n* Figure 3 shows the K-Nearest Neighbors (KNN) model with K=9, which is able to classify the data more accurately than the previous two models.\n* Figure 4, the KNN model with K=1, closely follows the data and hardly misclassifies any samples. It gives the least amount of bias and underfitting as compared to all the previous models, but it shows very high variance and overfitting.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Bias Variance trade-off","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is important to keep in mind that some bias and some variance will always be there while building a Machine Learning model. Both bias and variance add to the overall error in the model.\n\nTo minimize the reducible error (bias+variance), we must find the sweet spot between the two, where both bias and variance coexist with minimum possible values. This is called Bias-Variance Trade-off. It’s more of a trade-off between Prediction Accuracy (Variance) and Model Interpretability (Bias).\n\nFind the full article here : https://towardsdatascience.com/bias-variance-tradeoff-7ca56ba182a","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}