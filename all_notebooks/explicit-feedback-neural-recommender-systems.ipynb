{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Explicit Feedback Neural Recommender Systems\n\nThis notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found [here](https://github.com/m2dsupsdlclass/lectures-labs). \n\n**Goals**\n\n* Understand recommender data\n* Build different models architectures using Keras\n* Retrieve Embeddings and visualize them\n* Add some metadata information as input to the models\n\n**Dataset used**\n\n* Anime Recommendations Database from Kaggle [link](https://www.kaggle.com/CooperUnion/anime-recommendations-database)."},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"%%bash\npip install -U keras-tuner","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load libraries\nimport umap\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport tensorflow as tf\n\nfrom collections import deque\n\nfrom kerastuner.tuners import RandomSearch\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom tensorflow.keras.layers import (Concatenate, Dense, Dot, Dropout, Embedding, Flatten, Softmax)\nfrom tensorflow.keras.models import Model, load_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and preprocess the data\n\n### Ratings file\n\nAfter loading the data, each line of the dataframe contains:\n * user_id - non identifiable randomly generated user id.\n * anime_id - the anime that this user has rated.\n * rating - rating out of $10$ this user has assigned ($-1$ if the user watched it but did not assign a rating)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load and preprocess rating files\ndf_raw = pd.read_csv('../input/anime-recommendations-database/rating.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of the ratings data: {df_raw.shape}.\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Anime metadata file\n\nThe anime metadata file contains the following metadata: \n * anime_id - myanimelist.net's unique id identifying an anime.\n * name - full name of the anime.\n * genre - comma separated list of genres for this anime.\n * type - movie, TV, OVA, etc.\n * episodes - how many episodes in this show ($1$ if it's a movie).\n * rating - average rating out of $10$ for this anime.\n * members - number of community members that are in this anime's group."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load metadata file\nmetadata = pd.read_csv('../input/anime-recommendations-database/anime.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of the metadata: {metadata.shape}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merge ratings and metadata\n\nLet's enrich the raw ratings with the collected items metadata by merging the two dataframes on `anime_id`."},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings = df_raw.merge(metadata.loc[:, ['name', 'anime_id', 'type', 'episodes']], left_on='anime_id', right_on='anime_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of the complete data: {ratings.shape}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing\n\nTo understand well the distribution of the data, the following statistics are computed:\n* the number of users\n* the number of items\n* the rating distribution\n* the popularity of each anime"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of unique users: {ratings['user_id'].unique().size}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of unique animes: {ratings['anime_id'].unique().size}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of the ratings\nx, height = np.unique(ratings['rating'], return_counts=True)\n\nfig, ax = plt.subplots()\nax.bar(x, height, align='center')\nax.set(xticks=np.arange(-1, 11), xlim=[-1.5, 10.5])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's compute the popularity of each anime, defined as the number of ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of ratings for each movie\npopularity = ratings.groupby('anime_id').size().reset_index(name='popularity')\nmetadata = metadata.merge(popularity, left_on='anime_id', right_on='anime_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Speed-up the computation\n\nIn order to speed up the computation, we will subset the dataset using three criteria:\n* Remove the $-1$ ratings (people who watch the anime but without giving a rate).\n* Get only TV shows (because I like TV show).\n* Get the most popular ones (more than $5000$ ratings)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get most popular anime id and TV shows\nmetadata_5000 = metadata.loc[(metadata['popularity'] > 5000) & (metadata['type'] == 'TV')]\n# Remove -1 ratings and user id less than 10000\nratings = ratings[(ratings['rating'] > -1) & (ratings['user_id'] < 10000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean id\n\nAdd a new column to metadata_5000 in order to clean up id of the anime."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe for anime_id\nmetadata_5000 = metadata_5000.assign(new_anime_id=pd.Series(np.arange(metadata_5000.shape[0])).values)\nmetadata_5000_indexed = metadata_5000.set_index('new_anime_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the dataframe\nratings = ratings.merge(metadata_5000.loc[:, ['anime_id', 'new_anime_id', 'popularity']], left_on='anime_id', right_on='anime_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe for user_if\nuser = pd.DataFrame({'user_id': np.unique(ratings['user_id'])})\nuser = user.assign(new_user_id=pd.Series(np.arange(user.shape[0])).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the dataframe\nratings = ratings.merge(user, left_on='user_id', right_on='user_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of the rating dataset: {ratings.shape}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Later in the analysis, we will assume that this popularity does not come from the ratings themselves but from an external metadata, *e.g.* box office numbers in the month after the release in movie theaters.\n\n### Split the dataset into train/test sets\n\nLet's split the enriched data in a train/test split to make it possible to do predictive modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n\nuser_id_train = np.array(train['new_user_id'])\nanime_id_train = np.array(train['new_anime_id'])\nratings_train = np.array(train['rating'])\n\nuser_id_test = np.array(test['new_user_id'])\nanime_id_test = np.array(test['new_anime_id'])\nratings_test = np.array(test['rating'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explicit feedback: supervised ratings prediction\n\nFor each pair of (user, movie), we would like to predict the rating the user would give to the item.\n\nThis is the classical setup for building recommender systems from offline data with explicit supervision signal.\n\n### Predictive ratings as a regression problem\n\nThe following code implements the following architecture:\n\n![](https://raw.githubusercontent.com/m2dsupsdlclass/lectures-labs/3cb7df3a75b144b4cb812bc6eacec8e27daa5214/labs/03_neural_recsys/images/rec_archi_1.svg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each sample, we input the integer identifiers\n# of a a single user and a single items.\nclass RegressionModel(Model):\n    \"\"\"Define a regression model for items recommendation.\n    \n    Parameters\n    ----------\n    embedding_size: integer\n        Size the embedding vector\n    max_user_id: integer\n        Number of user in the dataset\n    max_item_id: integer\n        Number of item in the dataset\n    \n    Arguments\n    ---------\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    flatten: Flatten\n        Flatten layer\n    dot: Dot\n        Dot layer\n    \"\"\"\n    def __init__(self, embedding_size, max_user_id, max_item_id, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.user_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_user_id + 1,\n                                       input_length=1,\n                                       name='user_embedding')\n        self.item_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_item_id + 1,\n                                       input_length=1,\n                                       name='item_embedding')\n        self.flatten = Flatten()\n        self.dot = Dot(axes=1)\n    \n    def call(self, inputs, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with two elements\n            First element corresponds to the users\n            Second element corresponds to the items\n        \"\"\"\n        user_inputs = inputs[0]\n        item_inputs = inputs[1]\n        \n        # Definition of the user vectors\n        user_vecs = self.flatten(self.user_embedding(user_inputs))\n        # Definition of the item vectors\n        item_vecs = self.flatten(self.item_embedding(item_inputs))\n        \n        # Compute the dot product of the previous vectors\n        y = self.dot([user_vecs, item_vecs])\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nEMBEDDING_SIZE = 64\nMAX_USER_ID = np.max(user_id_train)\nMAX_ITEM_ID = np.max(anime_id_train)\n\n# Define and run the model\nmodel = RegressionModel(EMBEDDING_SIZE, MAX_USER_ID, MAX_ITEM_ID)\nmodel.compile(optimizer='adam', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial prediction\ninitial_train_preds = model.predict([user_id_train, anime_id_train])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model error\n\nUsing `initial_train_preds`, compute the model errors:\n* mean absolute error\n* mean squared error\n\nConverting a pandas Series to numpy array is usually implicit, but you may use `ratings_train.values` to do so explicitely. Be sure to monitor the shapes of each object you deal with by using `object.shape`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def mae(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\ndef mse(y_true, y_pred):\n    return np.mean((y_true - y_pred)**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_train, initial_train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_train, initial_train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Monitoring runs\n\nKeras enables to monitor various variables during training.\n\n`history.history` returned by the `model.fit` function is a dictionary containing the `'loss'` and validation loss `'val_loss'` after each epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nBATCH_SIZE = 64\nEPOCHS = 10\nVALIDATION_SPLIT = 0.1\n\n# Train the model\nhistory = model.fit(x=[user_id_train, anime_id_train], y=ratings_train,\n                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_split=VALIDATION_SPLIT, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training and test losses\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.ylim(0, 2)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model\n#model.save('model', save_format='tf')\n#model = load_model('../input/embeddings-model/model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train loss is higher then the validation loss in the first few epochs because the training loss is not computed on the complete training set. Keras does not compute the train loss on the full training set at the end of each epoch to prevent overfitting.\n\nNow that the model is trained, let's look back at the MSE and MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_prediction(y_true, y_pred):\n    \"\"\"Plot of the prediction.\n    :param y_true: Vector of true label\n    :param y_pred: Vector of predicted label\n    \"\"\"\n    plt.scatter(y_true, y_pred, s=60, alpha=0.01)\n    plt.xlim(0.5, 10.5)\n    plt.xlabel('True rating')\n    plt.ylim(-1, 14)\n    plt.ylabel('Predicted rating')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* On the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform predition on the test set\ntest_preds = model.predict([user_id_test, anime_id_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the prediction\nplot_prediction(ratings_test, test_preds.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* On the train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform predition on the train set\ntrain_preds = model.predict([user_id_train, anime_id_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_train, train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_train, train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the prediction\nplot_prediction(ratings_train, train_preds.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Embeddings\n\n* It is possible to retrieve the embeddings by simply using the Keras function `model.get_weights` which returns all the model learnable parameters.\n* The weights are returned in the same order as they were build in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the weights\nweights = model.get_weights()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'The shape of the different weights matrices are: {[w.shape for w in weights]}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'There are {np.sum([w.shape[0] * w.shape[1] for w in weights])} trainable parameters in the model.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieve the different embeddings\nuser_embeddings = weights[0]\nitem_embeddings = weights[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get embedding vector for a particular anime_id\nANIME_ID = 8\nprint(f'Title for ANIME_ID={ANIME_ID}: {metadata_5000[\"name\"].iloc[ANIME_ID]}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Embedding vector for ANIME_ID={ANIME_ID}.')\nprint(item_embeddings[ANIME_ID])\nprint(f'Shape of the embedding vector: {item_embeddings[ANIME_ID].shape}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the most similar items\n\nFinding the $k$ most similar items to a point in embedding space:\n* Write a function to compute cosine similarity between two animes in embedding space.\n* Test it on the following cells to check the similarities between popular animes.\n* Try to generalize the function to compute similarities between one anime and all the others and return the most related animes.\n\nNotes:\n* We may use `np.linalg.norm` to compute norm of vectors, and we may specidy the `axis`.\n* The numpy function `np.argsort(...)` enables to compute the sorted indices of a vector.\n* `items[\"name\"][idxs]` returns the `name` of the items indexed by array `idxs`."},{"metadata":{"trusted":true},"cell_type":"code","source":"EPSILON = 1e-07\n\ndef cosine(x, y):\n    \"\"\"Compute cosine similarities.\n    :param x: Vector\n    :param y: Vector\n    :return: Cosine similarity\n    \"\"\"\n    dot_prod = np.dot(x, y.T)\n    norm = np.linalg.norm(x) * np.linalg.norm(y)\n    return dot_prod / (norm + EPSILON)\n\ndef cosine_similarities(item_id, item_embeddings):\n    \"\"\"Compute cosine similarities between item_id and all items embeddings.\n    :param item_id: Item id (integer)\n    :param item_embeddings: Matrix of weights of embeddings\n    :return: Vector of cosine similarities\n    \"\"\"\n    query_vector = item_embeddings[item_id]\n    dot_products = item_embeddings @ query_vector\n    \n    query_vector_norm = np.linalg.norm(query_vector)\n    all_item_norms = np.linalg.norm(item_embeddings, axis=1)\n    norm_products = query_vector_norm * all_item_norms\n    return dot_products / (norm_products + EPSILON)\n    \ndef most_similar(item_id, item_embeddings, titles, top_n=10):\n    \"\"\"Find the `top_n` most similar items to `item_id`.\n    :param item_id: Item id (integer)\n    :param item_embeddings: Matrix of weights of embeddings\n    :param titles: Vector of titles\n    :param top_n: Number of anime to return (default=10)\n    :return: A list with the most similar items by increasing order\n    \"\"\"\n    similarities = cosine_similarities(item_id, item_embeddings)\n    sorted_indexes = np.argsort(similarities)[::-1]\n    idxs = sorted_indexes[0:top_n]\n    return list(zip(idxs, titles.iloc[idxs], similarities[idxs]))\n    \ndef print_similarity(item_a, item_b, item_embeddings, titles):\n    \"\"\"Print a summary of similarity between 2 items\n    :param item_a: First item (integer)\n    :param item_b: Second item (integer)\n    :param item_embeddings: Matrix of weights of embeddings\n    :param titles: Vector of titles\n    \"\"\"\n    similarity = cosine(item_embeddings[item_a], item_embeddings[item_b])\n    print(f'Cosine similarity between {titles.iloc[item_a]} and {titles.iloc[item_b]}: {similarity:.3}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_similarity(8, 102, item_embeddings, metadata_5000['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_similarity(8, 14, item_embeddings, metadata_5000['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_similarity(8, 8, item_embeddings, metadata_5000['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of cosine similarities\nplt.hist(cosine_similarities(8, item_embeddings), bins=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the most similar items to One Punch Man\nmost_similar(8, item_embeddings, metadata_5000['name'], top_n=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The similarities do not always make sense: the number of ratings is low and the embedding does not automatically capture semantic relationships in that context. Better representations arise with higher number of ratings, and less overfitting in models or maybe better loss function, such as those based on implicit feedback.\n\n### Visualizing embeddings using t-SNE\n\nWe can use scikit-learn to visualize item embeddings via [t-SNE](https://lvdmaaten.github.io/tsne/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_tsne(item_embeddings, perplexities):\n    \"\"\"Plot tSNE representations of embeddings\n    :param item_embeddings: Matrix of weights of embeddings\n    :param perplexities: Vector of perplexity\n    \"\"\"\n    (fig, subplots) = plt.subplots(1, 4, figsize=(16, 4))\n    for i, perplexity in enumerate(perplexities):\n        ax = subplots[i]\n        item_tsne = TSNE(perplexity=perplexity).fit_transform(item_embeddings)\n        ax.set_title(f\"Perplexity = {perplexity}\")\n        ax.scatter(item_tsne[:, 0], item_tsne[:, 1])\n        ax.axis('tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne(item_embeddings, [5, 30, 50, 100])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t-SNE visualisation using plotly\nitem_tsne = TSNE(perplexity=5).fit_transform(item_embeddings)\ntsne_df = pd.DataFrame(item_tsne, columns=['tsne_1', 'tsne_2'])\ntsne_df = tsne_df.assign(item_id=pd.Series(np.arange(item_tsne.shape[0])).values)\ntsne_df = tsne_df.merge(metadata_5000, left_on='item_id', right_on='new_anime_id')\n\npx.scatter(tsne_df, x='tsne_1', y='tsne_2', \n           color='rating', \n           hover_data=['name', 'type', 'rating', 'episodes'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can do similar things with [Uniform Manifold Approximation and Projection (UMAP)](https://github.com/lmcinnes/umap)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot UMAP\nitem_umap = umap.UMAP().fit_transform(item_embeddings)\n\nplt.scatter(item_umap[:, 0], item_umap[:, 1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Deep Recommender Model\n\nUsing a similar framework as previously, the following deep model described in the course was built (with only two fully connected layers).\n\n![](https://raw.githubusercontent.com/m2dsupsdlclass/lectures-labs/3cb7df3a75b144b4cb812bc6eacec8e27daa5214/labs/03_neural_recsys/images/rec_archi_2.svg)\n\nTo build this model, we will need a new king of layer, namely `Concatenate`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a class for the deep recommender model\nclass DeepRegressionModel(Model):\n    \"\"\"Define a deep regression model for items recommendation.\n    \n    Parameters\n    ----------\n    embedding_size: integer\n        Size the embedding vector\n    max_user_id: integer\n        Number of user in the dataset\n    max_item_id: integer\n        Number of item in the dataset\n    dropout_size: float\n        Probablity to dropout the neuron (between 0 and 1)\n    layer_size: integer\n        Size of the first hidden dense layer\n        \n    Arguments\n    ---------\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    flatten: Flatten\n        Flatten layer\n    concat: Concatenate\n        Concatenate layer\n    dropout: Dropout\n        Dropout layer\n    dense1: Dense\n        First dense layer\n    dense2: Dense\n        Second dense layer\n    \"\"\"\n    def __init__(self, embedding_size, max_user_id, max_item_id, dropout_size, layer_size, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.user_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_user_id + 1,\n                                       input_length=1,\n                                       name='user_embedding')\n        self.item_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_item_id + 1,\n                                       input_length=1,\n                                       name='item_embedding')\n        self.flatten = Flatten()\n        self.concat = Concatenate()\n        \n        # Too much dropout lead to underfitting\n        self.dropout = Dropout(dropout_size)\n        \n        self.dense1 = Dense(layer_size, activation=\"relu\")\n        # We predict one dimensional rating.\n        # No activation needed as we want to predict between 0 and 10.\n        self.dense2 = Dense(1)\n    \n    def call(self, inputs, training=False, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with two elements\n            First element corresponds to the users\n            Second element corresponds to the items\n        \"\"\"\n        user_inputs = inputs[0]\n        item_inputs = inputs[1]\n        \n        # Definition of the user vectors\n        user_vecs = self.flatten(self.user_embedding(user_inputs))\n        # Definition of the item vectors\n        item_vecs = self.flatten(self.item_embedding(item_inputs))\n        \n        # Contenate user and item vectors (fc1)\n        input_vecs = self.concat([user_vecs, item_vecs])\n        \n        # Build the network\n        y = self.dropout(input_vecs, training=training)\n        y = self.dense1(y) # fc2\n        y = self.dense2(y) # fc3\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nEMBEDDING_SIZE = 64\nMAX_USER_ID = np.max(user_id_train)\nMAX_ITEM_ID = np.max(anime_id_train)\nDROPOUT_SIZE = 0.2\nLAYER_SIZE = 64\n\n# Define and run the model\nmodel = DeepRegressionModel(EMBEDDING_SIZE, MAX_USER_ID, MAX_ITEM_ID, DROPOUT_SIZE, LAYER_SIZE)\nmodel.compile(optimizer='adam', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial prediction\ninitial_train_preds = model.predict([user_id_train, anime_id_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\n\nBATCH_SIZE = 64\nEPOCHS = 10\nVALIDATION_SPLIT = 0.1\n\n# Train the model\nhistory = model.fit(x=[user_id_train, anime_id_train], y=ratings_train,\n                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_split=VALIDATION_SPLIT, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training and test losses\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.ylim(0, 1.25)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform predition on the train set\ntrain_preds = model.predict([user_id_train, anime_id_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_train, train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_train, train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the prediction\nplot_prediction(ratings_train, train_preds.squeeze())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform predition on the test set\ntest_preds = model.predict([user_id_test, anime_id_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the prediction\nplot_prediction(ratings_test, test_preds.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the model not necessarily significantly better than the previous model but you can notice that the gap between train and test is lower, probably thanks to the use of dropout.\nFurthermore, this model is more flexible in the sense that we can extend it to include metadata for hybrid recommendation system as we will see in the following.\n\nBut before that, let's do some hyperparameters tuning. Manual tuning of so many hyperparameters is tedious. In practice, it is better to automate the design of the model using an hyperparameter search tool such as:\n* https://keras-team.github.io/keras-tuner/ (Keras specific)\n* https://optuna.org/ (any machine learning framework, Keras included)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a model to do hyperparameters tuning\nEMBEDDING_SIZE = 64\nMAX_USER_ID = np.max(user_id_train)\nMAX_ITEM_ID = np.max(anime_id_train)\n\ndef build_model(hp):\n    model = DeepRegressionModel(EMBEDDING_SIZE, MAX_USER_ID, MAX_ITEM_ID, \n                                dropout_size=hp.Float('dropout_size', min_value=0.1, max_value=0.5, step=0.1), \n                                layer_size=hp.Int('layer_size', min_value=32, max_value=64, step=32))\n\n    model.compile(optimizer='adam', loss='mae')\n    return model\n\n# Instantiate a tuner\ntuner = RandomSearch(build_model, \n                     objective='val_loss', \n                     max_trials=5, executions_per_trial=1, \n                     directory='.', project_name='Embedding')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Start the search for the best hyperparameter configuration\nEPOCHS = 5\n\ntuner.search(x=[user_id_train, anime_id_train], y=ratings_train,\n             batch_size=BATCH_SIZE, epochs=EPOCHS,\n             validation_split=VALIDATION_SPLIT, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_best = tuner.get_best_models(1)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nBATCH_SIZE = 64\nEPOCHS = 10\nVALIDATION_SPLIT = 0.1\n\n# Train the model\nhistory = model_best.fit(x=[user_id_train, anime_id_train], y=ratings_train,\n                         batch_size=BATCH_SIZE, epochs=EPOCHS,\n                         validation_split=VALIDATION_SPLIT, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training and test losses\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.ylim(0, 1.25)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform predition on the test set\ntest_preds = model_best.predict([user_id_test, anime_id_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the prediction\nplot_prediction(ratings_test, test_preds.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Item Metadata in the Model\n\nUsing a similar framework as previously, we will build another depp model that can also leverage additional metadata. The resulting system is therefor an **Hybrid Recommender System** that does both **Collaborative Filtering** and **Content-based recommendations**.\n\n![](https://raw.githubusercontent.com/m2dsupsdlclass/lectures-labs/3cb7df3a75b144b4cb812bc6eacec8e27daa5214/labs/03_neural_recsys/images/rec_archi_3.svg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define some metadata\nmeta_columns = ['episodes', 'popularity']\n\nscaler = QuantileTransformer()\nitem_meta_train = scaler.fit_transform(train[meta_columns])\nitem_meta_test = scaler.transform(test[meta_columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a class for the Hybrid model\nclass HybridModel(Model):\n    \"\"\"Define a deep regression model for items recommendation.\n    \n    Parameters\n    ----------\n    embedding_size: integer\n        Size the embedding vector\n    max_user_id: integer\n        Number of user in the dataset\n    max_item_id: integer\n        Number of item in the dataset\n        \n    Arguments\n    ---------\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    flatten: Flatten\n        Flatten layer\n    concat: Concatenate\n        Concatenate layer\n    dropout: Dropout\n        Dropout layer\n    dense1: Dense\n        First dense layer\n    dense2: Dense\n        Second dense layer\n    \"\"\"\n    def __init__(self, embedding_size, max_user_id, max_item_id, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.user_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_user_id + 1,\n                                       input_length=1,\n                                       name='user_embedding')\n        self.item_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_item_id + 1,\n                                       input_length=1,\n                                       name='item_embedding')\n        self.flatten = Flatten()\n        self.concat = Concatenate()\n        \n        self.dropout = Dropout(0.3)\n        self.dense1 = Dense(64, activation=\"relu\")\n        self.dense2 = Dense(64, activation=\"relu\")\n        self.dense3 = Dense(1)\n    \n    def call(self, inputs, training=False, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with two elements\n            First element corresponds to the users\n            Second element corresponds to the items\n            Third element corresponds to the metadata\n        \"\"\"\n        user_inputs = inputs[0]\n        item_inputs = inputs[1]\n        meta_inputs = inputs[2]\n        \n        # Definition of the user vectors\n        user_vecs = self.flatten(self.user_embedding(user_inputs))\n        user_vecs = self.dropout(user_vecs, training=training)\n        \n        # Definition of the item vectors\n        item_vecs = self.flatten(self.item_embedding(item_inputs))\n        item_vecs = self.dropout(item_vecs, training=training)\n        \n        # Contenate user, item and meta vectors (fc1)\n        input_vecs = self.concat([user_vecs, item_vecs, meta_inputs])\n        \n        # Build the network\n        y = self.dense1(input_vecs) # fc2\n        y = self.dropout(y, training=training)\n        y = self.dense2(y) # fc3\n        y = self.dropout(y, training=training)\n        y = self.dense3(y)\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nEMBEDDING_SIZE = 64\nMAX_USER_ID = np.max(user_id_train)\nMAX_ITEM_ID = np.max(anime_id_train)\n\n# Define and run the model\nmodel = HybridModel(EMBEDDING_SIZE, MAX_USER_ID, MAX_ITEM_ID)\nmodel.compile(optimizer='adam', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial prediction\ninitial_train_preds = model.predict([user_id_train, anime_id_train, item_meta_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nBATCH_SIZE = 64\nEPOCHS = 10\nVALIDATION_SPLIT = 0.1\n\n# Train the model\nhistory = model.fit(x=[user_id_train, anime_id_train, item_meta_train], y=ratings_train,\n                         batch_size=BATCH_SIZE, epochs=EPOCHS,\n                         validation_split=VALIDATION_SPLIT, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training and test losses\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.ylim(0, 1.5)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform predition on the test set\ntest_preds = model.predict([user_id_test, anime_id_test, item_meta_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the prediction\nplot_prediction(ratings_test, test_preds.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The additional metadata seems to improve the predictive power of the model a bit but this should be re-run several times to see the impact of the random initialization of the model. However, the variance of the prediction seems to be improved."},{"metadata":{},"cell_type":"markdown","source":"## A recommendation function for a given user\n\nOnce the model is trained, the system can be used to recommend a few items for a user, that he/she has not already seen:\n* We use the `model.predict` to compute the ratings a user would have given to all items.\n* We build a recommendation function that sorts these items and exclude those the user has already seen."},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommend(user_id, top_n=10):\n    \"\"\"Recommend anime for a given user id.\n    :param user_id: Id of a user\n    :param top_n: Number of anime to recommend\n    \"\"\"\n    item_ids = range(1, MAX_ITEM_ID)\n    seen_mask = ratings['new_user_id'] == user_id\n    seen_animes = set(ratings[seen_mask]['new_anime_id'])\n    item_ids = list(filter(lambda x: x not in seen_animes, item_ids))\n    \n    print(f'User {user_id} has seen {len(item_ids)} animes, including:')\n    for title in ratings[seen_mask].nlargest(20, 'popularity')['name']:\n        print(f'\\t{title}')\n    print(f'Computing ratings for {len(item_ids)} other animes:')\n    \n    item_ids = np.array(item_ids)\n    user_ids = np.zeros_like(item_ids)\n    user_ids[:] = user_id\n    items_meta = scaler.transform(metadata_5000_indexed[meta_columns].loc[item_ids])\n    \n    ratings_preds = model.predict([user_ids, item_ids, items_meta])\n    \n    item_ids = np.argsort(ratings_preds[:, 0])[::-1].tolist()\n    rec_items = item_ids[:top_n]\n    return [(metadata_5000_indexed['name'][anime], ratings_preds[anime][0]) for anime in rec_items]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for title, pred_rating in recommend(5):\n    print(f'\\t{pred_rating}: {title}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting ratings a classification problem\n\nIn this dataset, the ratings all belong  to a finite set of possible values: $1$ to $10$.\n\nMaybe, we can help the model by forcing it to predict those values by treating the problem as a multiclassification problem. The only required changes are:\n* setting the final layer to output class membership probabilities using a softmax activation with $10$ outputs;\n* optimize the categorical cross-entropy classification loss instead of a regression loss suwh as MSE or MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a class for the Hybrid model\nclass HybridClassificationModel(Model):\n    \"\"\"Define a deep regression model for items recommendation.\n    \n    Parameters\n    ----------\n    embedding_size: integer\n        Size the embedding vector\n    max_user_id: integer\n        Number of user in the dataset\n    max_item_id: integer\n        Number of item in the dataset\n        \n    Arguments\n    ---------\n    user_embedding: Embedding\n        Embedding layer of user \n    item_embedding: Embedding\n        Embedding layer of item\n    flatten: Flatten\n        Flatten layer\n    concat: Concatenate\n        Concatenate layer\n    dropout: Dropout\n        Dropout layer\n    dense1: Dense\n        First dense layer\n    dense2: Dense\n        Second dense layer\n    \"\"\"\n    def __init__(self, embedding_size, max_user_id, max_item_id, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.user_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_user_id + 1,\n                                       input_length=1,\n                                       name='user_embedding')\n        self.item_embedding = Embedding(output_dim=embedding_size,\n                                       input_dim=max_item_id + 1,\n                                       input_length=1,\n                                       name='item_embedding')\n        self.flatten = Flatten()\n        self.concat = Concatenate()\n        \n        self.dropout = Dropout(0.3)\n        self.dense1 = Dense(64, activation=\"relu\")\n        self.dense2 = Dense(64, activation=\"relu\")\n        self.dense3 = Dense(10, activation='softmax')\n    \n    def call(self, inputs, training=False, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs: list with two elements\n            First element corresponds to the users\n            Second element corresponds to the items\n            Third element corresponds to the metadata\n        \"\"\"\n        user_inputs = inputs[0]\n        item_inputs = inputs[1]\n        meta_inputs = inputs[2]\n        \n        # Definition of the user vectors\n        user_vecs = self.flatten(self.user_embedding(user_inputs))\n        user_vecs = self.dropout(user_vecs, training=training)\n        \n        # Definition of the item vectors\n        item_vecs = self.flatten(self.item_embedding(item_inputs))\n        item_vecs = self.dropout(item_vecs, training=training)\n        \n        # Contenate user, item and meta vectors (fc1)\n        input_vecs = self.concat([user_vecs, item_vecs, meta_inputs])\n        \n        # Build the network\n        y = self.dense1(input_vecs) # fc2\n        y = self.dropout(y, training=training)\n        y = self.dense2(y) # fc3\n        y = self.dropout(y, training=training)\n        y = self.dense3(y)\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nEMBEDDING_SIZE = 64\nMAX_USER_ID = np.max(user_id_train)\nMAX_ITEM_ID = np.max(anime_id_train)\n\n# Define and run the model\nmodel = HybridClassificationModel(EMBEDDING_SIZE, MAX_USER_ID, MAX_ITEM_ID)\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial prediction\ninitial_train_preds = model.predict([user_id_train, anime_id_train, item_meta_train]).argmax(axis=1) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_train, initial_train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_train, initial_train_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define parameters\nBATCH_SIZE = 64\nEPOCHS = 10\nVALIDATION_SPLIT = 0.1\n\n# Train the model\nhistory = model.fit(x=[user_id_train, anime_id_train, item_meta_train], y=ratings_train - 1,\n                         batch_size=BATCH_SIZE, epochs=EPOCHS,\n                         validation_split=VALIDATION_SPLIT, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training and test losses\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.ylim(0, 1.5)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform predition on the test set\ntest_preds = model.predict([user_id_test, anime_id_test, item_meta_test]).argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Absolute Error: {mae(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean Squared Error: {mse(ratings_test, test_preds.squeeze())}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the prediction\nplot_prediction(ratings_test, test_preds.squeeze())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}