{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prp_df is the dataframe containing the main csv file dataframe\nprp_df = pd.read_csv(\"../input/delhi-house-price-prediction/MagicBricks.csv\")\nprp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This contains 11 columns and 1259 rows \nprint(prp_df.shape)\n# This is used to find that dataframe contains some nan values\nprint(prp_df.isnull().values.any())\n\n# Try to find which columns contains nan values\nprp_df.columns\n\nfor i in prp_df.columns:\n    print(i+' ' , end='')\n    print(prp_df[i].isnull().values.any())\n    \n# Now will see how many rows contains nan values\n# we get that 254 rows have some nan values\nprint()\nnan_val=sum([True for idx,row in prp_df.iterrows() if any(row.isnull())])\nprint(nan_val)\n\n# That is around 20 percent of dataset rows have nan values. \nprint(nan_val/prp_df.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### We have issue in parking add mean in nan val or median =1\n# bathroom add mean in nan val or median =2\n# Furnishing and Type drop missing values\n# Locality\n# Per_Sqft might we can do is area/price for that column\n\n#Handling missing values\n\nprp_df['Bathroom'] = prp_df['Bathroom'].fillna(2)\nprp_df['Parking'] = prp_df['Parking'].fillna(1)\n\nprp_df = prp_df[prp_df['Furnishing'].notna()]\nprp_df = prp_df[prp_df['Type'].notna()]\n\nprp_df['Per_Sqft'].fillna(prp_df['Price']/prp_df['Area'], inplace=True)\n\nprint(prp_df.shape)\nfor i in prp_df.columns:\n    print(i+' ' , end='')\n    print(prp_df[i].isnull().values.any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values are successfully handled\nprp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to see the correlation between we need to remove or convert categorical variables\n#instead of this we can directly replace cat with a val for each col\n\ncleanup_nums = {\"Status\":     {\"Ready_to_move\": 0, \"Almost_ready\": 1},\"Furnishing\":{\"Semi-Furnished\": 0, \"Furnished\": 1,\"Unfurnished\":2}, \n                \"Transaction\":     {\"New_Property\": 0, \"Resale\": 1}, \"Type\":     {\"Builder_Floor\": 0, \"Apartment\": 1}}\n\nprp_df = prp_df.replace(cleanup_nums)\nprp_df.head()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=prp_df.corr()\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's create a heat map to see the correlation between different columns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax=plt.subplots(figsize=(15, 10))\ndf=prp_df.corr()\nsns.heatmap(df)\n\n#Hence we can observe strong correlation between  Area, BHK,bathroom, persqft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.unique(prp_df['Locality'])\n#prp_df.drop('Locality',axis='columns', inplace=True)\n#based on correlation we can drop\nprp_df.drop('Furnishing',axis='columns', inplace=True)\nprp_df.drop('Parking',axis='columns', inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #locality obs\n\n#location_stats = prp_df.groupby('Locality')['Locality'].agg('count').sort_values(ascending = False)\n#location_stats\n# len(pd.unique(prp_df['Locality']))\n# #pd.unique(prp_df['Locality'])\n\n# lis=[]\n# for i in prp_df['Locality']:\n#     temp_i=i.split()\n#     #sw=i.split(' ', 1)[1]\n#     fw=temp_i[0]\n#     #sw=temp_i[1]\n#     i=fw\n#     lis.append(i)\n\n# #print(set(lis))\n\n\n# prp_df['Locality']=lis\n\n# prp_df.head()\n# # #len(pd.unique(lis))\n\nlocation_stats = prp_df.groupby('Locality')['Locality'].agg('count').sort_values(ascending = False)\nlocation_stats\n\n#reduced the unique values from 363 to 199\n\n# Classify less than 20 as others\nlen(location_stats[location_stats < 11])\n\n#reduced the unique values from 199 to 22\nlocation_stas_less_than = location_stats[location_stats < 11]\nprp_df.Locality = prp_df.Locality.apply(lambda x : 'Other' if x in location_stas_less_than else x)\n\n\nprp_df.Locality = prp_df.Locality.apply(lambda x : 'J R Designers Floors' if x =='J' else x)\n\n# prp_df.head()\n# len(pd.unique(prp_df['Locality']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pd.unique(prp_df['Locality']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Assign numerical value to locality\nlist_prp=[]\nlist_prp=list(prp_df.Locality.unique())\nlist_prp\n\nprp_df['Locality'] = prp_df['Locality'].astype('category')\nprp_df['Locality']=prp_df['Locality'].cat.codes\nprp_df.Locality.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prp_df['BHK']=prp_df['BHK'].astype(float)\nprp_df['Locality']=prp_df['Locality'].astype(float)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = prp_df.drop(['Price'],axis='columns')\nprint(X.head(3))\n\ny = prp_df.Price\nprint(y.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr_clf = LinearRegression()\nlr_clf.fit(X_train,y_train)\nlr_clf.score(X_test,y_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.4814762972281953 accuracy(without locality)\n0.4817679934752995 accuracy (with locality 120 class)\n0.4821458736518861 accuracy (with locality 360 class) \n0.7382633342552736 accuracy (after removing outlier from area and per_sqrt) \n","metadata":{}},{"cell_type":"code","source":"#Outlier detection and handling\nimport matplotlib\n\nflag=0\nfor i in list(prp_df.Per_Sqft):\n    if i>40000:\n        flag=flag+1\nprint(flag)\nprp_df = prp_df[prp_df['Per_Sqft'].values <40000]\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nplt.hist(prp_df.Per_Sqft,rwidth=0.8)\nplt.xlabel(\"Price Per Square Feet\")\nplt.ylabel(\"Count\")\nprint(len(list(prp_df.Per_Sqft)))\n\n#prp_df['Per_Sqft']= np.sqrt(prp_df['Per_Sqft'])\n#Normalise each column\n#prp_df['Per_Sqft']=(prp_df['Per_Sqft']-prp_df['Per_Sqft'].mean())/prp_df['Per_Sqft'].std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Outlier detection and handling\nimport matplotlib\nflag=0\nfor i in list(prp_df.Area):\n    if i>6000:\n        flag=flag+1\nprint(flag)\nprp_df = prp_df[prp_df['Area'].values <6000]\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nplt.hist(prp_df.Area,rwidth=0.8)\nplt.xlabel(\"Area\")\nplt.ylabel(\"Count\")\nprint(len(list(prp_df.Area)))\n\n\n#prp_df['Area']=np.sqrt(prp_df['Area'])\n#Normalise each column\n#prp_df['Area']=(prp_df['Area']-prp_df['Area'].mean())/prp_df['Area'].std()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = prp_df.drop(['Price'],axis='columns')\nprint(X.head(3))\ny = prp_df.Price\nprint(y.head(3))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nlr_clf = Ridge(alpha=0.1)\nlr_clf.fit(X_train,y_train)\nprint(lr_clf.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\ndegree=2\npolyreg=make_pipeline(PolynomialFeatures(degree),LinearRegression())\npolyreg.fit(X_train,y_train)\nprint(polyreg.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}