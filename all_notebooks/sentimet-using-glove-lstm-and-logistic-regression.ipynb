{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string \nstring.punctuation\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport unicodedata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Downloading pre-stored stopwords in English language from NLTK Library\n\nstop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loading Data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/joe-biden-tweets/JoeBidenTweets.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nsentences = data['tweet'].tolist()\njoined_sentences = ''.join(sentences)\n\nplt.figure(figsize = (15,15))\nplt.imshow(WordCloud(colormap='Dark2').generate(joined_sentences) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Pre-Processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping irrelevant columns from the data\ndata = data.drop(columns=['id', 'url', 'timestamp', 'replies', 'retweets', 'likes', 'quotes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to perform data pre-processing \n\ndef preprocessing(text):\n    lowercase = text.lower()\n    punc_removal = [char for char in lowercase if char not in string.punctuation]\n    punc_removal_joined = ''.join(punc_removal)\n    url_removal = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', punc_removal_joined, flags=re.MULTILINE)\n    emoji_removal = url_removal.encode('ascii', 'ignore').decode('ascii')\n    rt_removal = re.sub(\"RT\", \"\", emoji_removal)\n    email_removal = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', rt_removal)\n    numbers_removal = re.sub(r'[0-9]', \"\", email_removal)\n    stopwords_removal = [word for word in numbers_removal.split() if word not in stopwords.words('english')]\n    return stopwords_removal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['processed_tweet'] = data['tweet'].apply(preprocessing).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning polarity scores using TextBlob \n\nfrom textblob import TextBlob\ndata['polarity'] = data['processed_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying conditions to the polarity score and assigning target 0 and 1 (Negative & Positive) respectively\n\nconditionList = [\n                 data['polarity'] > 0,\n                 data['polarity'] <= 0\n                 ]\nchoiceList = ['1', '0']\ndata['target'] = np.select(conditionList, choiceList, default='no_label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout, Activation, Bidirectional, SimpleRNN\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tokeinzing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenizing the words and sentences.. Followed by padding to set a fixed length for all tweets \n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['processed_tweet'].values)\n\nword_index = tokenizer.word_index\n\nsentence = tokenizer.texts_to_sequences(data['processed_tweet'].values)\npadding = pad_sequences(sentence, padding='post', maxlen = 22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining the size of vocabulary \n\nvocab_size = len(word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning X, y features and splitting them for training and testing \n\nX = padding\ny = pd.get_dummies(data['target']).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **GloVe Word Embedding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initializing GloVe word embedding \n\nembeddings_index = dict()\nf = open('../input/glove-twitter/glove.twitter.27B.200d.txt')\nfor line in f:\n\tvalues = line.split()\n\tword = values[0]\n\tcoefs = np.asarray(values[1:], dtype='float32')\n\tembeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, 200))\nfor word, i in tokenizer.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Deep Learning / Neural Network Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 200\nlstm_out = 128\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embed_dim,input_length = X.shape[1], weights=[embedding_matrix],trainable=False))\nmodel.add(LSTM(lstm_out, return_sequences=True))\nmodel.add(LSTM(lstm_out))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs = 10, verbose = 1, validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Logisitic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Test split for Logistic Regression\n\nX = padding\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LR Score of Training Set\" ,lr.score(X_train, y_train))\nprint(\"LR Score of Test Set\" ,lr.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"expected = y_test\npredicted = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn import metrics\ncm = metrics.confusion_matrix(expected, predicted, labels = ['1','0'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[1,0])\ndisp = disp.plot()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(expected, predicted))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}