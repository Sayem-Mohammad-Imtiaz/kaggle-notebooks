{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Internet News Prediction & EDA\nWelcome to today's notebook, where we will be visualising and predicting dataset which includes different newspaper articles and their details."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the data\nThe first step will be to fill the null values of the data and drop the 'Unnamed: 0' feature which is useless."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/internet-articles-data-with-users-engagement/articles_data.csv')\ndf = df.drop('Unnamed: 0', axis=1)\n\ndf['title'] = df['title'].fillna('NaN')\ndf['description'] = df['description'].fillna('NaN')\ndf['content'] = df['content'].fillna('NaN')\ndf['published_at'] = df['published_at'].fillna('NaN')\n\ndf['engagement_reaction_count'] = df['engagement_reaction_count'].fillna(0)\ndf['engagement_comment_count'] = df['engagement_comment_count'].fillna(0)\ndf['engagement_share_count'] = df['engagement_share_count'].fillna(0)\ndf['engagement_comment_plugin_count'] = df['engagement_comment_plugin_count'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising the data\nNext, we will perform EDA on our features."},{"metadata":{},"cell_type":"markdown","source":"The following cell is a procedure which plots out a bar chart that can tell us the distribution of the different variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar_charts(title, x, y, colour, values, keys, figsize=(10, 5), fontsize=12):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    bars = plt.bar(keys, values, color=colour)\n\n    for bar in bars:\n        label = list(count)[list(bars).index(bar)]\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, height, label, ha='center', va='bottom', \n                 fontsize=fontsize)\n\n    plt.title(title, fontsize=fontsize)\n    plt.xlabel(x, fontsize=fontsize)\n    plt.ylabel(y, fontsize=fontsize)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afterwards, another procedure is defined, though this time in the form of a line graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plots(title, x, y, values, keys, figsize=(10, 5), fontsize=12):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    plt.plot(list(keys), list(values))\n    plt.title(title)\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A very useful technique for visualisation is the WordCloud, which shows what words are the most frequently occurring."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 20))\n\nfor ax in [[ax1, 'title'], [ax2, 'description'], [ax3, 'content']]:\n    wordcloud = WordCloud(background_color='white').generate(' '.join(df[ax[1]]))\n    ax[0].set_title(ax[1], fontsize=20)\n    ax[0].imshow(wordcloud)\n    ax[0].axis('off')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bar charts"},{"metadata":{},"cell_type":"markdown","source":"The first variable that we will display is the 'source_name' column which describes the originators of the article. As seen below, the 'Reuters', 'BBC News' and 'Irish Times' have written the most articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"count = Counter(df['source_name'])\ncount = pd.Series(count).sort_values(ascending=False)\n\nkeys = list(count.keys())\nkeys[keys.index('The New York Times')] =  'The NY Times'\nkeys[keys.index('Al Jazeera English')] = 'Al Jazeera'\nkeys[keys.index('The Wall Street Journal')] =  'Wall Street Journal'\n\nbar_charts('Articles per source', 'Source name', 'Number of articles', 'blue', count, keys,\n          (20, 13), 18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Subsequently, we now show the distribution of how many articles were written in September and October. The amount written in October was less than a third of that written in September."},{"metadata":{"trusted":true},"cell_type":"code","source":"month = [i[5:7] for i in df['published_at']]\ncount = Counter(month)\ncount = pd.Series(count).sort_values(ascending=False)[:2]\n\nbar_charts('Distribution of articles released per month', 'Month number', 'Number of articles',\n          'orange', count, count.keys(), figsize=(15, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following EDA is how many pieces were written per day, only looking at those in September. The most that was written on one day was on the third of September."},{"metadata":{"trusted":true},"cell_type":"code","source":"day = [i[8:10] for i in df['published_at']]\ncount = Counter(day)\ncount = pd.Series(count).sort_values(ascending=False)[:13]\n\nbar_charts('Day that articles were released', 'Day released', 'Number of articles', 'purple', \n           count, count.keys(), figsize=(15, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use a bar chart to take a look into which hours are the most popular for releasing a news piece. It seems that around 2-4 pm is the most regular time that people publish their content."},{"metadata":{"trusted":true},"cell_type":"code","source":"hour = [i[11:13] for i in df['published_at']]\ncount = Counter(hour)\ncount = pd.Series(count).sort_values(ascending=False)[:20]\n\nbar_charts('Hours that articles were released', 'Hour released', 'Number of articles',\n          'green', count, count.keys(), figsize=(13, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Line graphs"},{"metadata":{},"cell_type":"markdown","source":"Furthermore, we switch our attention to line graphs, where we will look at a how many articles were released."},{"metadata":{"trusted":true},"cell_type":"code","source":"day_and_month = pd.DataFrame([])\nday_and_month['day'] = day\nday_and_month['month'] = month\n\ncount1 = Counter(day_and_month[day_and_month['month']=='09']['day'])\ncount2 = Counter(day_and_month[day_and_month['month']=='10']['day'])\n\nkeys = pd.concat([pd.Series(count1.keys()), pd.Series(count2.keys())[:2]])\nvalues = pd.concat([pd.Series(count1.values()), pd.Series(count2.values())[:2]])\ncount = dict(zip(keys, values))\n\nplots('Articles released over the days', 'Days', 'Number of articles', count.values(),\n      count.keys(), (13, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next four line graphs are about the engagement of the reaction, comment, share and comment-plugin."},{"metadata":{},"cell_type":"markdown","source":"There are many spikes in these visualisations, which show that there is no steady order of the variables and that these increases in the data are seemingly unexpected."},{"metadata":{"trusted":true},"cell_type":"code","source":"plots('Engagement reaction count over time', 'Time', 'Engagement reaction count', \n      df['engagement_reaction_count'], df['engagement_reaction_count'].keys(), (13, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plots('Engagement comment count over time', 'Time', 'Engagement comment count', \n      df['engagement_comment_count'], df['engagement_comment_count'].keys(), (13, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plots('Engagement share count over time', 'Time', 'Engagement share count', \n      df['engagement_share_count'], df['engagement_share_count'].keys(), (13, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plots('Engagement comment plugin count over time', 'Time', 'Engagement comment plugin count', \n      df['engagement_comment_plugin_count'], df['engagement_comment_plugin_count'].keys(), \n      (13, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation"},{"metadata":{},"cell_type":"markdown","source":"Next, we use a heatmap to check out whether there are correlations between any of the variables. We see that there are three sets of features that do have connections, which means that they have a dependency on each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr(), annot=True)\nplt.title('Correlation of variables')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, we will scatter the datapoints and create a line of best fit to have a closer look at how the different variables correlate to each other."},{"metadata":{},"cell_type":"markdown","source":"We remove some outliers in the columns because they could skew our results."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['engagement_reaction_count'] = df['engagement_reaction_count'][df['engagement_reaction_count']<100000]\ndf['engagement_share_count'] = df['engagement_share_count'][df['engagement_share_count']<20000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fontsize=15\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 7))\n\nfor ax in [[ax1, ['engagement_reaction_count', 'engagement_comment_count']],\n           [ax2, ['engagement_reaction_count', 'engagement_share_count']],\n           [ax3, ['engagement_share_count', 'engagement_comment_count']]]:\n    sns.regplot(data=df, x=ax[1][0], y=ax[1][1], ax=ax[0])\n    ax[0].set_xlabel(ax[1][0], fontsize=fontsize)\n    ax[0].set_ylabel(ax[1][1], fontsize=fontsize)\n\nax2.set_title('Correlation of variables', fontsize=30, pad=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting the data"},{"metadata":{},"cell_type":"markdown","source":"## Splitting our dataset"},{"metadata":{},"cell_type":"markdown","source":"We assign an 'X' variable to the 'content' feature and 'y' to our 'source_name' feature. They then go on to be further split into train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df['content']\ny = df['source_name']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using NLP"},{"metadata":{},"cell_type":"markdown","source":"We must first convert our data from textual to numerical format in order for us to input it into a predictor. The way this is done is through a 'CountVectorizer' and then a 'TFIDF' model."},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer()\ntfidf = TfidfTransformer()\n\nX_train = cv.fit_transform(X_train)\nX_test = cv.transform(X_test)\n\nX_train = tfidf.fit_transform(X_train)\nX_test = tfidf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using TruncatedSVD"},{"metadata":{},"cell_type":"markdown","source":"Now we will reduce the unwanted parts of our data using a TruncatedSVD model, which is basically a PCA for text."},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components=2000)\nX_train = svd.fit_transform(X_train)\nX_test = svd.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating and evaluating classifiers"},{"metadata":{},"cell_type":"markdown","source":"Next, will train three different classifiers: 'SGD', 'Random Forest' and 'Linear SVC' and then evaluate their performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [['SGD', SGDClassifier()], ['Random Forest', RandomForestClassifier()],\n              ['Linear SVC', LinearSVC()]]\nscores = []\ncross_vals = []\n\nfor classifier in classifiers:\n    model = classifier[1]\n    model.fit(X_train, y_train)\n\n    score = model.score(X_test, y_test)\n    cross_val = cross_val_score(model, X_test, y_test).mean()\n    scores.append(score)\n    cross_vals.append(cross_val)\n    \n    print(classifier[0])\n    print(score)\n    print(cross_val)\n    if model != classifiers[-1][1]:\n        print('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we use bar charts to visualise how well each classifier has performed in relation to model score and cross val score. We can see that the best predictor for this data is the Linear SVC, followed by SGD Classifier and then the Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['SGD', 'Random Forest', 'Linear SVC']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 9))\n\nfor ax in [[ax1, scores, 'model score'], [ax2, cross_vals, 'cross validation score']]:\n    metric = ax[1]\n    bars = ax[0].bar(names, metric, color='blue')\n    for bar in bars:\n        label = str(metric[list(bars).index(bar)])[:4]\n        height = bar.get_height()\n        ax[0].text(bar.get_x() + bar.get_width()/2, height, label, ha='center', va='bottom')\n    ax[0].set_title(ax[2])\n    ax[0].set_xlabel('model')\n    ax[0].set_ylabel('accuracy')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thank you for reading my notebook.\n### If you enjoyed this notebook and found it helpful, please upvote it and give feedback as it will help me make more of these."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}