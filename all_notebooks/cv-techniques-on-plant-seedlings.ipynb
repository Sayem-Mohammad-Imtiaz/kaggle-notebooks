{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport random\n\nfrom PIL import Image\nfrom imageio import imread\n\nimport seaborn as sns\nsns.set_style(\"dark\")\nsns.set()\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CyclicLR\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\n\nfrom skimage.morphology import closing, disk, opening\n\nimport cv2\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom os import listdir\nfrom skimage.segmentation import mark_boundaries\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\ntorch.backends.cudnn.deterministic=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training=False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data\n\nLet's take a look at our input folder:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that I have added pretrained neural networks for solving this task.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = \"../input/v2-plant-seedlings-dataset/nonsegmentedv2/\"\nOUTPUT_PATH = \"segmented_seedlings\"\nMODEL_PATH = \"../input/seedlingsmodel/segmented_seedlings\"\nLOSSES_PATH = \"../input/seedlingsmodel/\"\nsubfolders = listdir(base_path)\nsubfolders","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting image paths and target species","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"All target species have different subfolders. Consequently we can extract the target as well as the path for each image by collecting them for each subfolder. After doing so we end up with a pandas dataframe that holds the path, the species as well as the width and height of all images:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"total_images = 0\nfor folder in subfolders:\n    total_images += len(listdir(base_path + folder))\n\nplantstate = pd.DataFrame(index=np.arange(0, total_images), columns=[\"width\", \"height\", \"species\"])\n\nk = 0\nall_images = []\nfor m in range(len(subfolders)):\n    folder = subfolders[m]\n    \n    images = listdir(base_path + folder)\n    all_images.extend(images)\n    n_images = len(images)\n    \n    for n in range(0, n_images):\n        image = imread(base_path + folder + \"/\" + images[n])\n        plantstate.loc[k, \"width\"] = image.shape[1]\n        plantstate.loc[k, \"height\"] = image.shape[0]\n        plantstate.loc[k, \"species\"] = folder\n        plantstate.loc[k, \"image_name\"] = images[n]\n        k+=1\n\nplantstate.width = plantstate.width.astype(np.int)\nplantstate.height = plantstate.height.astype(np.int)\nplantstate.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX = np.log(plantstate.width.values).reshape(-1,1)\nX = scaler.fit_transform(X)\n\nkm = KMeans(n_clusters=5)\nplantstate[\"growth_state\"] = km.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_states = plantstate.groupby(\"growth_state\").width.mean().values\nstate_order = np.argsort(mean_states)\nmean_states","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(len(state_order), 5, figsize=(20,5*len(state_order)))\n\nfor n in range(len(state_order)):\n    your_state=state_order[n]\n\n    example = np.random.choice(plantstate[plantstate.growth_state==your_state].index.values, size=5)\n    species = plantstate.loc[example].species.values\n    for m in range(5):\n        image_id = all_images[example[m]]\n        image = imread(base_path + species[m] + \"/\" + image_id)\n        ax[n,m].imshow(image)\n        ax[n,m].set_title(species[m] + \" \" + image_id + \"\\n\" + \"growth state: \" + str(your_state))\n        ax[n,m].grid(False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def segment_plant1(np_image, threshold, radius):\n    image_lab = cv2.cvtColor(np_image, cv2.COLOR_BGR2HSV)\n  #  image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity , 255, 255])\n\n    mask = cv2.inRange(image_lab, lower_hsv, upper_hsv)\n    edges = cv2.Canny(mask,100,200)\n    #kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n   # mask = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n    #output = cv2.bitwise_and(np_image, np_image, mask = mask)\n    #image_blurred = cv2.GaussianBlur(output, (0, 0), 3)\n    #image_sharp = cv2.addWeighted(output, 1.5, image_blurred, -0.5, 0)\n   # img = cv2.imread('messi5.jpg',0)\n   # edges = cv2.Canny(image_sharp,100,200)\n    #def canny_edge_detection(img):\n    #img = cv2.imread('messi5.jpg',0)\n    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n    #ret, thresh = cv2.threshold(image_sharp,127,255,cv2.THRESH_TOZERO_INV)\n    #return thresh\n   # mask = get_mask(image_lab[:,:,1], threshold, radius)\n  #  masked_image = np_image.copy()\n#    for n in range(3):\n#        masked_image[:,:,n] = np_image[:,:,n] * image\n    return edges","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Edge detection on growth rate 4 images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots(len(state_order), 5, figsize=(20,5*len(state_order)))\n\nfor n in range(len(state_order)):\n   # your_state=state_order[n]\n    your_state=4\n    example = np.random.choice(plantstate[plantstate.growth_state==your_state].index.values, size=5)\n    species = plantstate.loc[example].species.values\n    for m in range(5):\n        image_id = all_images[example[m]]\n        image = imread(base_path + species[m] + \"/\" + image_id)\n        ax[n,m].imshow(image)\n        ax[n,m].imshow(segment_plant1(image, my_threshold, my_radius))\n        ax[n,m].set_title(species[m] + \" \" + image_id + \"\\n\" + \"growth state: \" + str(your_state))\n        ax[n,m].grid(False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plantstate[\"growth_state\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"growth4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_leakage = plantstate.groupby([\"growth_state\", \"species\"]).size().unstack().fillna(0) \ntarget_leakage = target_leakage / plantstate.species.value_counts() * 100\ntarget_leakage = target_leakage.apply(np.round).astype(np.int)\n\nplt.figure(figsize=(20,5))\nsns.heatmap(target_leakage, cmap=\"YlGnBu\", annot=True)\nplt.title(\"The growth state is related to the species!\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Segmentation beginssss...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"example_path = base_path + \"Sugar beet/27.png\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_path = base_path + \"Sugar beet/27.png\" \nfig,ax = plt.subplots(3,3,figsize=(20,17))\n\ntitles = [[\"Red\", \"Green\", \"Blue\"],\n         [\"Hue\", \"Saturation\", \"Value\"],\n         [\"\\n lightness from black to white\", \"\\n A - from green to red\", \"\\n B - from blue to yellow\"]]\npil_image = Image.open(example_path)\nnp_image = np.array(pil_image)\nimage_hvs = cv2.cvtColor(np_image, cv2.COLOR_BGR2HSV)\nimage_lab = cv2.cvtColor(np_image, cv2.COLOR_BGR2LAB)\n\nfor n in range(3):\n    ax[0,n].imshow(np_image[:,:,n], cmap=\"RdYlGn\")\n    ax[0,n].grid(False)\n    ax[0,n].set_title(\"Sugar beet/27\" + \" - \" + titles[0][n]);\n    ax[1,n].imshow(image_hvs[:,:,n], cmap=\"RdYlGn\")\n    ax[1,n].set_title(\"Sugar beet/27\" + \" - \" + titles[1][n]);\n    ax[2,n].imshow(image_lab[:,:,n], cmap=\"RdYlGn\")\n    ax[2,n].set_title(\"Sugar beet/27\" + \" - \" + titles[2][n]);\nplt.savefig(\"Colorspace\", dpi=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_threshold = 121\nmy_radius = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mask(image, threshold, radius):\n    mask = np.where(image < threshold, 1, 0)\n    selem = disk(radius)\n    mask = closing(mask, selem)\n    return mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def segment_plant(np_image, threshold, radius):\n   # edges = cv2.Canny(np_image,100,200)\n    image_lab = cv2.cvtColor(np_image, cv2.COLOR_BGR2HSV)\n  #  image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity , 255, 255])\n\n    mask = cv2.inRange(image_lab, lower_hsv, upper_hsv)\n  #  edges = cv2.Canny(mask,100,200)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    output = cv2.bitwise_and(np_image, np_image, mask = mask)\n    image_blurred = cv2.GaussianBlur(output, (0, 0), 3)\n    image_sharp = cv2.addWeighted(output, 1.5, image_blurred, -0.5, 0)\n   # img = cv2.imread('messi5.jpg',0)\n   # edges = cv2.Canny(image_sharp,100,200)\n    #def canny_edge_detection(img):\n    #img = cv2.imread('messi5.jpg',0)\n    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n    #ret, thresh = cv2.threshold(image_sharp,127,255,cv2.THRESH_TOZERO_INV)\n    #return thresh\n   # mask = get_mask(image_lab[:,:,1], threshold, radius)\n  #  masked_image = np_image.copy()\n#    for n in range(3):\n#        masked_image[:,:,n] = np_image[:,:,n] * image\n    return image_sharp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.distplot(image_lab[:,:,1].flatten(), ax=ax[0], kde=False)\nmask = get_mask(image_lab[:,:,1], my_threshold, my_radius)\nax[1].imshow(mask);\nax[2].imshow(segment_plant(np_image, my_threshold, my_radius))\nax[0].grid(False)\nax[1].grid(False)\nax[2].grid(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4,6,figsize=(20,14))\n\nfor m in range(6):\n    folder = subfolders[m]\n    files = listdir(base_path + folder + \"/\")\n    image = np.array(Image.open(base_path + folder + \"/\" + files[0]))\n    ax[0,m].imshow(image)\n    ax[1,m].imshow(segment_plant(image, my_threshold, my_radius))\n    ax[0,m].grid(False)\n    ax[1,m].grid(False)\n    ax[0,m].set_title(folder + \"/\" + files[0])\n    \n    folder = subfolders[m+6]\n    files = listdir(base_path + folder + \"/\")\n    image = np.array(Image.open(base_path + folder + \"/\" + files[0]))\n    ax[2,m].imshow(image)\n    ax[3,m].imshow(segment_plant(image, my_threshold, my_radius))\n    ax[2,m].grid(False)\n    ax[3,m].grid(False)\n    ax[2,m].set_title(folder + \"/\" + files[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SegmentPlant(object):\n    \n    def __call__(self, image):\n        np_image = np.array(image)\n        image = segment_plant(np_image, my_threshold, my_radius)\n        pil_image = Image.fromarray(image)\n        return pil_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RandomZoom(object):\n    \n    def __call__(self, image):\n        zoom_factor = np.random.uniform(1, 1)\n        height = image.size[0]\n        width = image.size[1]\n        new_size = (np.int(zoom_factor*height), np.int(zoom_factor*width))\n        return transforms.Resize(new_size)(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_transform(key=\"train\", plot=False):\n    train_sequence = [RandomZoom(), transforms.Resize(size=256),\n            transforms.CenterCrop(224),\n            SegmentPlant(),\n            transforms.RandomAffine(30),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip()]\n    val_sequence = [transforms.Resize(size=256),\n            transforms.CenterCrop(224),\n            SegmentPlant()]\n    if plot==False:\n        train_sequence.extend([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        val_sequence.extend([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        \n    data_transforms = {'train': transforms.Compose(train_sequence),'val': transforms.Compose(val_sequence)}\n    return data_transforms[key]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class SeedlingsDataset(Dataset):\n    \n    def __init__(self, root_dir, df, transform=None):\n        self.root_dir = root_dir\n        self.states = df\n        self.transform=transform\n      \n    def __len__(self):\n        return len(self.states)\n        \n    def __getitem__(self, idx):\n        image_path = self.root_dir + self.states.species.values[idx] + \"/\" \n        image_path += self.states.image_name.values[idx]\n        image = Image.open(image_path)\n        image = image.convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n         \n        target = self.states.target.values[idx]\n        return {\"image\": image, \"label\": target}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nlabels = encoder.fit_transform(plantstate.species.values)\nplantstate[\"target\"] = labels\nNUM_CLASSES = plantstate.target.nunique()\nplantstate.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain the training data by splitting into 60% train and 40% for the next split\ntrain_idx, sub_test_idx = train_test_split(plantstate.index.values,\n                                           test_size=0.4,\n                                           random_state=2019,\n                                           stratify=plantstate.target.values)\n\n# Split the residual 40% into two parts (each 20% of the original data): \ndev_idx, test_idx = train_test_split(sub_test_idx,\n                                     test_size=0.5,\n                                     random_state=2019,\n                                     stratify=plantstate.loc[sub_test_idx, \"target\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = plantstate.loc[train_idx].copy()\ndev_df = plantstate.loc[dev_idx].copy()\ntest_df = plantstate.loc[test_idx].copy()\n\ntrain_dataset = SeedlingsDataset(base_path, train_df, transform=my_transform(key=\"train\"))\ndev_dataset = SeedlingsDataset(base_path, dev_df, transform=my_transform(key=\"val\"))\ntest_dataset = SeedlingsDataset(base_path, test_df, transform=my_transform(key=\"val\"))\n\nimage_datasets = {\"train\": train_dataset, \"dev\": dev_dataset, \"test\": test_dataset}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"dev\", \"test\"]}\n\nprint(len(train_dataset), len(dev_dataset), len(test_dataset))\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ndev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n\ndataloaders = {\"train\": train_dataloader, \"dev\": dev_dataloader, \"test\": test_dataloader}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,6,figsize=(20,11))\n\ntrain_transform = my_transform(key=\"train\", plot=True)\nval_transform = my_transform(key=\"val\", plot=True)\n\nfor m in range(6):\n    folder = subfolders[m]\n    files = listdir(base_path + folder + \"/\")\n    image = Image.open(base_path + folder + \"/\" + files[0])\n    ax[0,m].imshow(image)\n    transformed_img = train_transform(image)\n    ax[1,m].imshow(transformed_img)\n    ax[2,m].imshow(val_transform(image))\n    ax[0,m].grid(False)\n    ax[1,m].grid(False)\n    ax[2,m].grid(False)\n    ax[0,m].set_title(folder + \"/\" + files[0])\n    ax[1,m].set_title(\"Preprocessing for train\")\n    ax[2,m].set_title(\"Preprocessing for val\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input/pretrained-pytorch-models/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.resnet18(pretrained=False)\n\nif run_training:\n    model.load_state_dict(torch.load(\"../input/pretrained-pytorch-models/resnet18-5c106cde.pth\"))\nnum_features = model.fc.in_features\nprint(num_features)\n\nmodel.fc = nn.Sequential(\n    nn.Linear(num_features, 512),\n    nn.ReLU(),\n    nn.BatchNorm1d(512),\n    nn.Dropout(0.5),\n    \n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.BatchNorm1d(256),\n    nn.Dropout(0.5),\n    \n    nn.Linear(256, NUM_CLASSES))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n\nmodel.apply(init_weights)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = compute_class_weight(y=train_df.target.values, class_weight=\"balanced\", classes=train_df.target.unique())    \nclass_weights = torch.FloatTensor(weights)\nif device.type==\"cuda\":\n    class_weights = class_weights.cuda()\nprint(class_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.SGD(model.fc.parameters(), lr=0.09, momentum=0.9)\nscheduler = CyclicLR(optimizer, base_lr=0.01, max_lr=0.09)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if run_training:\n    model, loss_dict, running_loss_dict = train_loop(model, criterion, optimizer, scheduler=scheduler, num_epochs = 10)\n    \n    if device == \"cpu\":\n        OUTPUT_PATH += \".pth\"\n    else:\n        OUTPUT_PATH += \"_cuda.pth\"\n        \n    torch.save(model.state_dict(), OUTPUT_PATH)\n    \n    losses_df = pd.DataFrame(loss_dict[\"train\"],columns=[\"train\"])\n    losses_df.loc[:, \"dev\"] = loss_dict[\"dev\"]\n    losses_df.loc[:, \"test\"] = loss_dict[\"test\"]\n    losses_df.to_csv(\"losses_segmented_seedlings.csv\", index=False)\n    \n    running_losses_df = pd.DataFrame(running_loss_dict[\"train\"], columns=[\"train\"])\n    running_losses_df.loc[0:len(running_loss_dict[\"dev\"])-1, \"dev\"] = running_loss_dict[\"dev\"]\n    running_losses_df.loc[0:len(running_loss_dict[\"test\"])-1, \"test\"] = running_loss_dict[\"test\"]\n    running_losses_df.to_csv(\"running_losses_segmented_seedlings.csv\", index=False)\nelse:\n    if device == \"cpu\":\n        MODEL_PATH += \".pth\"\n    else:\n        MODEL_PATH += \"_cuda.pth\"\n    model.load_state_dict(torch.load(MODEL_PATH))\n    model.eval()\n    \n    losses_df = pd.read_csv(LOSSES_PATH + \"losses_segmented_seedlings.csv\")\n    running_losses_df = pd.read_csv(LOSSES_PATH + \"running_losses_segmented_seedlings.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,1,figsize=(20,15))\n\nax[0].plot(running_losses_df[\"train\"], '-o', label=\"train\")\nax[0].set_xlabel(\"Step\")\nax[0].set_ylabel(\"Weighted x-entropy\")\nax[0].set_title(\"Loss change over steps\")\nax[0].legend();\n\nax[1].plot(running_losses_df[\"dev\"], '-o', label=\"dev\", color=\"orange\")\nax[1].set_xlabel(\"Step\")\nax[1].set_ylabel(\"Weighted x-entropy\")\nax[1].set_title(\"Loss change over steps\")\nax[1].legend();\n\nax[2].plot(running_losses_df[\"test\"], '-o', label=\"test\", color=\"mediumseagreen\")\nax[2].set_xlabel(\"Step\")\nax[2].set_ylabel(\"Weighted x-entropy\")\nax[2].set_title(\"Loss change over steps\")\nax[2].legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(losses_df[\"train\"], '-o', label=\"train\")\nplt.plot(losses_df[\"dev\"], '-o', label=\"dev\")\nplt.plot(losses_df[\"test\"], '-o', label=\"test\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Weighted x-entropy\")\nplt.title(\"Loss change over epochs\");\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_predictions = pd.DataFrame(index = np.arange(0, dataset_sizes[\"dev\"]), columns = [\"true\", \"predicted\"])\ntest_predictions = pd.DataFrame(index = np.arange(0, dataset_sizes[\"test\"]), columns = [\"true\",\"predicted\"])\ntest_predictions = pd.DataFrame(index = np.arange(0, dataset_sizes[\"test\"]), columns = [\"true\", \"predicted\"])\n\nplt.ion()\n\ndef imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\ndef evaluate_model(model, predictions_df, key):\n    was_training = model.training\n    model.eval()\n\n    with torch.no_grad():\n        for i, data in enumerate(dataloaders[key]):\n            inputs = data[\"image\"].to(device)\n            labels = data[\"label\"].to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"true\"] = data[\"label\"].numpy().astype(np.int)\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"predicted\"] = preds.cpu().numpy().astype(np.int)\n    predictions_df = predictions_df.dropna()\n    return predictions_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nspecies_map = {0: \"Black-grass\",\n               1: \"Charlock\",\n               2: \"Cleavers\",\n               3: \"Common Chickweed\",\n               4: \"Common wheat\",\n               5: \"Fat Hen\",\n               6: \"Loose Silky-bent\",\n               7: \"Maize\",\n               8: \"Scentless Mayweed\",\n               9: \"Shepherd's Purse\",\n               10: \"Small-flowered Cranesbill\",\n               11: \"Sugar beet\"}\n\ndev_predictions = evaluate_model(model, dev_predictions, \"dev\")\ndev_predictions.loc[:,\"true\"] = dev_predictions.loc[:, \"true\"].astype(np.int)\ndev_predictions.loc[:, \"predicted\"] = dev_predictions.loc[:, \"predicted\"].astype(np.int)\ndev_predictions.loc[:, \"true species\"] = dev_predictions.loc[:, \"true\"].map(species_map)\ndev_predictions.loc[:, \"predicted species\"] = dev_predictions.loc[:, \"predicted\"].map(species_map)\ndev_predictions.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_predictions.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(dev_predictions.true.values, dev_predictions.predicted.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = evaluate_model(model, test_predictions, \"test\")\ntest_predictions.loc[:,\"true\"] = test_predictions.loc[:, \"true\"].astype(np.int)\ntest_predictions.loc[:, \"predicted\"] = test_predictions.loc[:, \"predicted\"].astype(np.int)\ntest_predictions.loc[:, \"true species\"] = test_predictions.loc[:, \"true\"].map(species_map)\ntest_predictions.loc[:, \"predicted species\"] = test_predictions.loc[:, \"predicted\"].map(species_map)\ntest_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(test_predictions.true.values, test_predictions.predicted.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_confusion = confusion_matrix(dev_predictions[\"true species\"].values, dev_predictions[\"predicted species\"].values)\ntest_confusion = confusion_matrix(test_predictions[\"true species\"].values, test_predictions[\"predicted species\"].values)\n\nfig, ax = plt.subplots(1,2,figsize=(25,10))\nsns.heatmap(dev_confusion, annot=True, cmap=\"Oranges\", square=True, cbar=False, linewidths=1, ax=ax[0]);\nsns.heatmap(test_confusion, annot=True, cmap=\"Greens\", square=True, cbar=False, linewidths=1, ax=ax[1]);\nax[0].set_title(\"Confusion matrix of Dev-data\");\nax[0].set_xticklabels([name for val, name in species_map.items()], rotation=90)\nax[0].set_yticklabels([name for val, name in species_map.items()], rotation=45)\nax[0].set_xlabel(\"predicted\")\nax[0].set_ylabel(\"true\")\nax[1].set_title(\"Confusion matrix of Test-data\");\nax[1].set_xticklabels([name for val, name in species_map.items()], rotation=90)\nax[1].set_yticklabels([name for val, name in species_map.items()], rotation=45);\nax[1].set_xlabel(\"predicted\")\nax[1].set_ylabel(\"true\");\nplt.savefig(\"Confusion\", dpi=200)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}