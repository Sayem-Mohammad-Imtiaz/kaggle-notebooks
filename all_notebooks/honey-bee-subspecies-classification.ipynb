{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center><font size=\"6\">Honye Bee Subspecies Classification</font></center></h1>\n\n<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Bee-apis.jpg/337px-Bee-apis.jpg\"></img><center>\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Prepare the data analysis</a>  \n - <a href='#21'>Load packages</a>  \n - <a href='#21'>Load the data</a>  \n - <a href='#21'>Preprocessing data</a>  \n- <a href='#3'>Data exploration</a>   \n - <a href='#31'>Check for missing data</a>  \n - <a href='#32'>Explore image data</a>  \n - <a href='#33'>Location</a>  \n - <a href='#34'>Date and Time</a>  \n - <a href='#35'>Subspecies</a>  \n  - <a href='#36'>Health</a>  \n  - <a href='#37'>Pollen carrying</a>  \n  - <a href='#38'>Caste</a>  \n- <a href='#4'>Subspecies classification</a>  \n - <a href='#40'>Split the data</a>  \n - <a href='#41'>Build a baseline model</a>  \n - <a href='#42'>Model evaluation</a>    \n - <a href='#43'>Add Dropout</a>  \n - <a href='#44'>Model refinement</a>  \n- <a href='#6'>Conclusions</a>    \n- <a href='#7'>References</a>    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\n\nIn this Kernel, we will explore a dataset with adnotated images of bees from various locations of US, captured over several months during 2018, at different hours, from various bees subspecies, and with different health problems. \n\nThe objective of the Kernel is to take us through the steps of a machine learning analysis.   \nWe start by preparing the analysis (load the libraries and the data), continue with an Exploratory Data Analysis (EDA) where we highlight various data features, spending some time to try to understand the data and also get an idea about various features predictive potential and correlation with other features.   \nWe follow then with features engineering and preparation for creation of a model. The dataset is split in training, validation and test set. We start then with a simple model to classify the bees subspecies, something we are calling a baseline model.   \nWe evaluate the model, estimating the training error and accuracy and also the validation error and accuracy. With these, and with an rough estimate of what will be the (human) error rate for classification of bees subspecies, we decide how to follow our machine learning for image classification work. If we have at start a high bias, we will try first to improve our model so that will learn better the train images dataset. If we have a small bias but large variance (the model learns well the training data but fails to generalize, that means our model is overfitting. Based on these kind of observation, we make decission for how to adjust the model.   \nWe run few models with the improvements decided based on analysis or error and accuracy and we decide at the end for a final model. This model will be used for classification of fresh, new data, not used for training or validation, the test set.\n\nNote: this Kernel is using GPU acceleration. If you will fork and run, this will require few minutes of GPU run from your 30 h/week free GPU quota.\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>  ","metadata":{"_uuid":"a8e77ace65f04c89a878bf18249e4d8e23fec996"}},{"cell_type":"markdown","source":"# <a id='2'>Prepare the data analysis</a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n\n","metadata":{"_uuid":"4e97555eb77978a29a51c41f39cec67136b18157"}},{"cell_type":"markdown","source":"# <a id='21'>Load packages</a>\n\nWe load the packages used for the analysis.\n","metadata":{"_uuid":"cb2e73fe056a3dda7eb48eeac2facf0c441816d1"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport tensorflow","metadata":{"_uuid":"af08260bfbe163f9132f39d09627899bbc4c1dae","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_PATH = '../input/bee_imgs/bee_imgs/'\nIMAGE_WIDTH = 100\nIMAGE_HEIGHT = 100\nIMAGE_CHANNELS = 3\nRANDOM_STATE = 2018\nTEST_SIZE = 0.2\nVAL_SIZE = 0.2\nCONV_2D_DIM_1 = 16\nCONV_2D_DIM_2 = 16\nCONV_2D_DIM_3 = 32\nCONV_2D_DIM_4 = 64\nMAX_POOL_DIM = 2\nKERNEL_SIZE = 3\nBATCH_SIZE = 32\nNO_EPOCHS_1 = 5\nNO_EPOCHS_2 = 10\nNO_EPOCHS_3 = 50\nPATIENCE = 5\nVERBOSE = 1","metadata":{"_kg_hide-input":true,"_uuid":"a2082fb1e56fc6cfc91d40820b905267bc1ca468","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n# <a id='22'>Load the data</a>  \n\nLet's see first what data files do we have in the root directory.","metadata":{"_uuid":"307f656565365ff05faf226e5a447875dd0dfead"}},{"cell_type":"code","source":"os.listdir(\"../input\")","metadata":{"_uuid":"9f1df6658b17558179d8a9016f544410de16c354","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a dataset file and a folder with images.  \n\nLet's load the dataset file first.","metadata":{"_uuid":"241b8735a85a25e16421fda8c35bc3d3c69e7ea8"}},{"cell_type":"code","source":"honey_bee_df=pd.read_csv('../input/bee_data.csv')","metadata":{"_uuid":"d7b9f11a014428e56e422d97a5b3ef70efec007e","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's glimpse the data. First, let's check the number of columns and rows.","metadata":{"_uuid":"22b3984ccc3e29daaf77a796d9d7966cd798e1a8"}},{"cell_type":"code","source":"honey_bee_df.shape","metadata":{"_uuid":"535f3f9cea3b26428bec3ede4ed49009bdb91889","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 5172 rows and 9 columns. Let's look to the data.","metadata":{"_uuid":"5b4405ddcce03ee722f05234d508188997817f8d"}},{"cell_type":"code","source":"honey_bee_df.sample(100).head()","metadata":{"_uuid":"4d326f747f0a14580b20c2e034e6c3368edcd18b","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data contains the following values:  \n\n* file - the image file name;  \n* date - the date when the picture was taken;\n* time - the time when the picture was taken;\n* location - the US location, with city, state and country names;  \n* zip code - the ZIP code associated with the location;  \n* subspecies - the subspecies to whom the bee in the current image belongs;  \n* health - this is the health state of the bee in the current image;  \n* pollen_carrying - indicates if the picture shows the bee with pollen attached to the legs;  \n* caste - the bee caste;  \n\nIt is important, before going to create a model, to have a good understanding of the data. We will therefore explore the various features, not only the images.","metadata":{"_uuid":"c97047b17cda76e346e444229485ac91ec966423"}},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n# <a id='3'>Data exploration</a>  \n\n\n\nLet's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. \n","metadata":{"_uuid":"55dd26f919decca9d67daec9895a5d9e11f1d28b"}},{"cell_type":"markdown","source":"## <a id='31'>Check for missing data</a>  \n\nLet's create a function that check for missing data in the dataset.","metadata":{"_uuid":"14443450ba96e12ad8e18ce4dd1779f18d5f914b"}},{"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(honey_bee_df)","metadata":{"_uuid":"4544dd470d743c54f815faaee863038ad5e8398f","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing (null) data in the dataset. Still it might be that some of the data labels are misspelled; we will check this when we will analyze each data feature.","metadata":{"_uuid":"3cb0410e8b9afd75ac7b50d0489d90eda6e1b109"}},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='32'>Explore image data</a>  \n\nLet's also check the image data. First, we check how many images are stored in the image folder.","metadata":{"_uuid":"1fbab44688fb2ab073aac8f964e534f90ce1dfff"}},{"cell_type":"code","source":"image_files = list(os.listdir(IMAGE_PATH))\nprint(\"Number of image files: {}\".format(len(image_files)))","metadata":{"_uuid":"46f15681887fa82ab13224e52df69d91119fc9ad","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also check that each line in the dataset has a corresponding image in the image list.","metadata":{"_uuid":"68523860593e9a64059b51d40a316454e6937a68"}},{"cell_type":"code","source":"file_names = list(honey_bee_df['file'])\nprint(\"Matching image names: {}\".format(len(set(file_names).intersection(image_files))))","metadata":{"_uuid":"457cd17212904bb96f86ec1770cbdbefc5ffb395","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also check the image sizes.","metadata":{"_uuid":"4b20cd791ede3f23d0c9275aafc75827b9424df4"}},{"cell_type":"code","source":"def read_image_sizes(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    return list(image.shape)","metadata":{"_kg_hide-input":true,"_uuid":"64f4416a8e20197d60f7dbc9dd41a5e73049bfd0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = np.stack(honey_bee_df['file'].apply(read_image_sizes))\ndf = pd.DataFrame(m,columns=['w','h','c'])\nhoney_bee_df = pd.concat([honey_bee_df,df],axis=1, sort=False)","metadata":{"_uuid":"2c72a7d125efb51e00a58554692dbd99adc74b55","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's represent the distribution of the images sizes (width and height).","metadata":{"_uuid":"afb79c117b2c613b9be54f3921a5980531f8400d"}},{"cell_type":"code","source":"traceW = go.Box(\n    x = honey_bee_df['w'],\n    name=\"Width\",\n     marker=dict(\n                color='rgba(238,23,11,0.5)',\n                line=dict(\n                    color='red',\n                    width=1.2),\n            ),\n    orientation='h')\ntraceH = go.Box(\n    x = honey_bee_df['h'],\n    name=\"Height\",\n    marker=dict(\n                color='rgba(11,23,245,0.5)',\n                line=dict(\n                    color='blue',\n                    width=1.2),\n            ),\n    orientation='h')\ndata = [traceW, traceH]\nlayout = dict(title = 'Width & Heights of images',\n          xaxis = dict(title = 'Size', showticklabels=True), \n          yaxis = dict(title = 'Image dimmension'),\n          hovermode = 'closest',\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='width-height')","metadata":{"_kg_hide-input":true,"_uuid":"2719c8c64be44d0750f2f325d2492fbce2ee8332","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also load images associated with each features categories, in the following sections.","metadata":{"_uuid":"6253c64c9d971e624772e8be06eb35d913c328de"}},{"cell_type":"markdown","source":"## <a id='33'>Locations</a>  \n\nLet's check the locations of the images. For this, we will group by `location` and `zip code`.","metadata":{"_uuid":"a2b88af0c239ca3d9e37e159889836a4f38913c8"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['zip code'])['location'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\ndf","metadata":{"_uuid":"6f1c39d0398275215f92f61542544132a0d574a0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that `Athens, GA, USA` is actually the same location as `Athens, Georgia, USA`, only written differently (the `zip code` is the same as well). Let's modify the data to have the same location name for both.","metadata":{"_uuid":"fd421a7d1872af204c26588d1a15eaddca08a396"}},{"cell_type":"code","source":"honey_bee_df = honey_bee_df.replace({'location':'Athens, Georgia, USA'}, 'Athens, GA, USA')","metadata":{"_uuid":"57c4f7bac571131392374b462f69cfcd24ec5f79","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['zip code'])['location'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\ndf['code'] = df['location'].map(lambda x: x.split(',', 2)[1])\ndf","metadata":{"_uuid":"fdf585a7e29da16a9043b969e3ca9d2afabae16c","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize now the **location** data.","metadata":{"_uuid":"974c1ba8e721ca1879864b701bdf41bc84e668cc"}},{"cell_type":"code","source":"trace = go.Bar(\n        x = df['location'],\n        y = df['Images'],\n        marker=dict(color=\"Tomato\"),\n        text=df['location']\n    )\ndata = [trace]\n    \nlayout = dict(title = 'Number of bees images per location',\n          xaxis = dict(title = 'Subspecies', showticklabels=True, tickangle=15), \n          yaxis = dict(title = 'Number of images'),\n          hovermode = 'closest'\n         )\nfig = dict(data = data, layout = layout)\niplot(fig, filename='images-location')","metadata":{"_kg_hide-input":true,"_uuid":"b978a0e04e5ef3fa15c99cd9c0e67045af6e8452","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the images (2000) were captured in `Saratoga, California`. On second place is `Athens, Georgia` (1051), followed by `Des Moines, Iowa` (973).   \n\nLet's load few images, one per each location.","metadata":{"_uuid":"44620aed21e3412cd95493e8034d95b1095d3700"}},{"cell_type":"code","source":"#list of locations\nlocations = (honey_bee_df.groupby(['location'])['location'].nunique()).index","metadata":{"_kg_hide-input":true,"_uuid":"b8cf9ce34783bda8b121c67bb81461b75b071fb1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_category_images(var,cols=5):\n    categories = (honey_bee_df.groupby([var])[var].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=cols, figsize=(2*cols,2*len(categories)))\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = honey_bee_df[honey_bee_df[var]==cat].sample(cols)\n        for j in range(0,cols):\n            file=IMAGE_PATH + sample.iloc[j]['file']\n            im=imageio.imread(file)\n            ax[i, j].imshow(im, resample=True)\n            ax[i, j].set_title(cat, fontsize=9)  \n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"078d947f264182dee5db0b172b91715ac6c98969","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_category_images(\"location\")","metadata":{"_uuid":"9df59a4228f0838c73b67f3e4579041a373b8e4f","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='34'>Date and time</a>   \n\nLet's first convert date to datetime and extract year, month and day.  We also convert time and extract hour and minute.","metadata":{"_uuid":"d9c080eae3157070f2526a140e4becdfcfb7bf23"}},{"cell_type":"code","source":"honey_bee_df['date_time'] = pd.to_datetime(honey_bee_df['date'] + ' ' + honey_bee_df['time'])\nhoney_bee_df[\"year\"] = honey_bee_df['date_time'].dt.year\nhoney_bee_df[\"month\"] = honey_bee_df['date_time'].dt.month\nhoney_bee_df[\"day\"] = honey_bee_df['date_time'].dt.day\nhoney_bee_df[\"hour\"] = honey_bee_df['date_time'].dt.hour\nhoney_bee_df[\"minute\"] = honey_bee_df['date_time'].dt.minute","metadata":{"_uuid":"f103bccb829b9c909a251ddf301ba74e1b11713d","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the date/time distribution of the data. We group the data on `date_time` and count the images. We will represent the date on x-axis, the time (with hours resolution) on y-axis, and one circle with area proportional with the number of images taken for each location corresponding to the date and hour.","metadata":{"_uuid":"f36bdc8d4cf9cf0cb4e917a4951949ad836d5019"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['date_time', 'hour'])['location'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\nhover_text = []\nfor index, row in df.iterrows():\n    hover_text.append(('Date/time: {}<br>'+\n                      'Hour: {}<br>'+\n                      'Location: {}<br>'+\n                      'Images: {}').format(row['date_time'],\n                                            row['hour'],\n                                            row['location'],\n                                            row['Images']))\ndf['hover_text'] = hover_text\nlocations = (honey_bee_df.groupby(['location'])['location'].nunique()).index\ndata = []\nfor location in locations:\n    dfL = df[df['location']==location]\n    trace = go.Scatter(\n        x = dfL['date_time'],y = dfL['hour'],\n        name=location,\n        marker=dict(\n            symbol='circle',\n            sizemode='area',\n            sizeref=0.2,\n            size=dfL['Images'],\n            line=dict(\n                width=2\n            ),),\n        mode = \"markers\",\n        text=dfL['hover_text'],\n    )\n    data.append(trace)\n    \nlayout = dict(title = 'Number of bees images per date, approx. hour and location',\n          xaxis = dict(title = 'Date', showticklabels=True), \n          yaxis = dict(title = 'Hour'),\n          hovermode = 'closest'\n         )\nfig = dict(data = data, layout = layout)\n\niplot(fig, filename='images-date_time')","metadata":{"_uuid":"a3a4f369c2bb46ae2761f4508188327a2de72571","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also show now a number of images per each day **hour**.","metadata":{"_uuid":"987e3a00c68b5b318ebac6663eaef1ac500fab22"}},{"cell_type":"code","source":"draw_category_images(\"hour\")","metadata":{"_kg_hide-input":true,"_uuid":"8c5af9e5ed8a6d721e54208881d688e192fcef41","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='34'>Subspecies</a>   \n\nLet's plot now the subspecies distribution.","metadata":{"_uuid":"61bc75e33d136a5b63382838bf5766d847c6d01b"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['subspecies'])['year'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\ndf","metadata":{"_kg_hide-input":true,"_uuid":"40e0f13ffb3bcf217954095fb7578ddf35c48973","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace = go.Bar(\n        x = df['subspecies'],\n        y = df['Images'],\n        marker=dict(color=\"Green\"),\n        text=df['subspecies']\n    )\ndata = [trace]\n    \nlayout = dict(title = 'Number of bees images per subspecies',\n          xaxis = dict(title = 'Subspecies', showticklabels=True, tickangle=15), \n          yaxis = dict(title = 'Number of images'),\n          hovermode = 'closest'\n         )\nfig = dict(data = data, layout = layout)\niplot(fig, filename='images-subspecies')","metadata":{"_kg_hide-input":true,"_uuid":"70546715903314d4a4ce253488aff0602dbf89c5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The majority of subspecies are `Italian honey bee`,  with 3008 images, followed by `Russian honey bee` (527) and `Carniolan honey bee` (501). There is a relativelly large number of subspecies not identified, marked with `-1` (428).","metadata":{"_uuid":"2327b89ba2ee56d44b7b2a496d679b71b3ff3b28"}},{"cell_type":"markdown","source":"Let's show few images of each **subspecies**.","metadata":{"_uuid":"075e1932fa0321a4c86ef839cf85dac4a50ef79a"}},{"cell_type":"code","source":"draw_category_images(\"subspecies\")","metadata":{"_kg_hide-input":true,"_uuid":"2604b66b64099487ad2aeabecfcd2b129bafa1e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Subspecies and location","metadata":{"_uuid":"8506f84198402b23ad00a96b51a3dd094c23a3c9"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['subspecies'])['location'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()","metadata":{"_kg_hide-input":true,"_uuid":"c7655288d4ec0a5aeae9a6ed82eec4f1645e00bc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"piv = pd.pivot_table(df, values=\"Images\",index=[\"subspecies\"], columns=[\"location\"], fill_value=0)\nm = piv.values","metadata":{"_kg_hide-input":true,"_uuid":"c645c52b8316d2b9996d1b85b1ef1d21d9d4c793","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace = go.Heatmap(z = m, y= list(piv.index), x=list(piv.columns),colorscale='Rainbow',reversescale=False)\n    \ndata=[trace]\nlayout = dict(title = \"Number of images per subspecies and location\",\n              xaxis = dict(title = 'Location',\n                        showticklabels=True,\n                           tickangle = 45,\n                        tickfont=dict(\n                                size=10,\n                                color='black'),\n                          ),\n              yaxis = dict(title = 'Subspecies', \n                        showticklabels=True, \n                           tickangle = 45,\n                        tickfont=dict(\n                            size=10,\n                            color='black'),\n                      ), \n              hovermode = 'closest',\n              showlegend=False,\n                  width=600,\n                  height=600,\n             )\nfig = dict(data = data, layout = layout)\niplot(fig, filename='images-location_subspecies')","metadata":{"_kg_hide-input":true,"_uuid":"0e6134df06c9fbed6bc77e477a53091297cb1c44","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Subspecies and hour","metadata":{"_uuid":"b737f86069bfd20aae0e4b3dbd5a3948935c932f"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['subspecies'])['hour'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()","metadata":{"_uuid":"8516e7736d65a4e93a639b3e85325987a51b6e27","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"piv = pd.pivot_table(df, values=\"Images\",index=[\"subspecies\"], columns=[\"hour\"], fill_value=0)\nm = piv.values","metadata":{"_uuid":"a59d81f661c590c15e25a2e92e9262bfc3ce7cba","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace = go.Heatmap(z = m, y= list(piv.index), x=list(piv.columns),colorscale='Rainbow',reversescale=False)\n    \ndata=[trace]\nlayout = dict(title = \"Number of images per subspecies and hour\",\n              xaxis = dict(title = 'Hour',\n                        showticklabels=True,\n                           tickangle = 0,\n                        tickfont=dict(\n                                size=10,\n                                color='black'),\n                          ),\n              yaxis = dict(title = 'Subspecies', \n                        showticklabels=True, \n                           tickangle = 45,\n                        tickfont=dict(\n                            size=10,\n                            color='black'),\n                      ), \n              hovermode = 'closest',\n              showlegend=False,\n                  width=600,\n                  height=600,\n             )\nfig = dict(data = data, layout = layout)\niplot(fig, filename='images-location_subspecies')","metadata":{"_uuid":"9938a3b9df402df38c15a9f674a8f93d02f86503","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"9f82846a2f247ce9964b873050259f0a4582958b"}},{"cell_type":"markdown","source":"### Subspecies and image size \n\nLet's draw the image size distribution (width and height) grouped by subspecies.","metadata":{"_uuid":"8912a7cf7d2b0e8637edb80fb8ade62affa89210"}},{"cell_type":"code","source":"def draw_trace_box(dataset,var, subspecies):\n    dfS = dataset[dataset['subspecies']==subspecies];\n    trace = go.Box(\n        x = dfS[var],\n        name=subspecies,\n        marker=dict(\n                    line=dict(\n                        color='black',\n                        width=0.8),\n                ),\n        text=dfS['subspecies'], \n        orientation = 'h'\n    )\n    return trace\n\nsubspecies = (honey_bee_df.groupby(['subspecies'])['subspecies'].nunique()).index\ndef draw_group(dataset, var, title,height=500):\n    data = list()\n    for subs in subspecies:\n        data.append(draw_trace_box(dataset, var, subs))\n        \n    layout = dict(title = title,\n              xaxis = dict(title = 'Size',showticklabels=True),\n              yaxis = dict(title = 'Subspecies', showticklabels=True, tickfont=dict(\n                family='Old Standard TT, serif',\n                size=8,\n                color='black'),), \n              hovermode = 'closest',\n              showlegend=False,\n                  width=600,\n                  height=height,\n             )\n    fig = dict(data=data, layout=layout)\n    iplot(fig, filename='subspecies-image')\n\n\ndraw_group(honey_bee_df, 'w', \"Width of images per subspecies\")\ndraw_group(honey_bee_df, 'h', \"Height of images per subspecies\")","metadata":{"_kg_hide-input":true,"_uuid":"546d737e1558b83c5704ee15af89e231cc5c73ff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the averages values for the images sizes are between 50 and 100 pixels and the extreme values are from few pixels to approximatelly 500. We will scale all images to 100 x 100 pixels.\n\nLet's draw as well the scatter plot of the image width and height, grouped by subspecies.","metadata":{"_uuid":"111ce7a5b4dcf00d09c12f7a8a3564cbf1869439"}},{"cell_type":"code","source":"def draw_trace_scatter(dataset, subspecies):\n    dfS = dataset[dataset['subspecies']==subspecies];\n    trace = go.Scatter(\n        x = dfS['w'],y = dfS['h'],\n        name=subspecies,\n        mode = \"markers\",\n        marker = dict(opacity=0.8),\n        text=dfS['subspecies'], \n    )\n    return trace\n\nsubspecies = (honey_bee_df.groupby(['subspecies'])['subspecies'].nunique()).index\ndef draw_group(dataset, title,height=600):\n    data = list()\n    for subs in subspecies:\n        data.append(draw_trace_scatter(dataset, subs))\n        \n    layout = dict(title = title,\n              xaxis = dict(title = 'Width',showticklabels=True),\n              yaxis = dict(title = 'Height', showticklabels=True, tickfont=dict(\n                family='Old Standard TT, serif',\n                size=8,\n                color='black'),), \n              hovermode = 'closest',\n              showlegend=True,\n                  width=800,\n                  height=height,\n             )\n    fig = dict(data=data, layout=layout)\n    iplot(fig, filename='subspecies-image')\n\n\ndraw_group(honey_bee_df,  \"Width and height of images per subspecies\")","metadata":{"_uuid":"90a6683ea075b3456b7ff4e1d9ebab4ef30e7ae0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"42f8cfd4d51c7ae29308ad00199491d8ecc1ff51"}},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='35'>Health</a>   \n\nLet's plot now the health distribution.","metadata":{"_uuid":"07479c4b334dd003ab27ce73c8bd7225e69d7830"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['health'])['year'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\ndf","metadata":{"_kg_hide-input":true,"_uuid":"3e42de343a40852ec54356682ae95de15c351ff8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace = go.Bar(\n        x = df['health'],\n        y = df['Images'],\n        marker=dict(color=\"Red\"),\n        text=df['health']\n    )\ndata = [trace]\n    \nlayout = dict(title = 'Number of bees images per health',\n          xaxis = dict(title = 'Health', showticklabels=True, tickangle=15), \n          yaxis = dict(title = 'Number of images'),\n          hovermode = 'closest'\n         )\nfig = dict(data = data, layout = layout)\niplot(fig, filename='images-health')","metadata":{"_kg_hide-input":true,"_uuid":"2883d13b37f33f4a4c00bee03cbd78570d7267ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Majority of images are for healthy bees (3384), followed by `feq varrao, hive beetles` (579) and `Varroa, Small Hive Beetles` (472) and `ant problems` (457).","metadata":{"_uuid":"52ca32ede63b574d5963e2f18d803caab1c690bd"}},{"cell_type":"markdown","source":"Let's plot on the same graph subspecies and health.","metadata":{"_uuid":"fb5e9297c114944fefaafa56a6ab211c4dff3df8"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['subspecies'])['health'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\ndf","metadata":{"_kg_hide-input":true,"_uuid":"af3dcc128760e91e1a2f761cdbf03e8df8ac854c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"piv = pd.pivot_table(df, values=\"Images\",index=[\"subspecies\"], columns=[\"health\"], fill_value=0)\nm = piv.values","metadata":{"_uuid":"c80ab3a07989d44cc78a03ee66f9eb95e94566d0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace = go.Heatmap(z = m, y= list(piv.index), x=list(piv.columns),colorscale='Rainbow',reversescale=False)\n    \ndata=[trace]\nlayout = dict(title = \"Number of images per subspecies and health\",\n              xaxis = dict(title = 'Subspecies',\n                        showticklabels=True,\n                           tickangle = 45,\n                        tickfont=dict(\n                                size=10,\n                                color='black'),\n                          ),\n              yaxis = dict(title = 'Health', \n                        showticklabels=True, \n                           tickangle = 45,\n                        tickfont=dict(\n                            size=10,\n                            color='black'),\n                      ), \n              hovermode = 'closest',\n              showlegend=False,\n                  width=600,\n                  height=600,\n             )\nfig = dict(data = data, layout = layout)\niplot(fig, filename='images-health_subspecies')","metadata":{"_kg_hide-input":true,"_uuid":"1f6e0f190b2a4c326ff37f0b762c29fd3f547dc6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only a reduced number of combination of health and subspecies values are present. The majority of images are of `healthy` `Italian honey bee` (1972), followed by `few varrao, hive beetles`  `Italian honey bee` (579) and  `healthy` `Russian honey bee` (527). The unknown subspecies are either `healthy` (177) or `hive being robbed` (251).   \n\nLet's plot on the same graph the number of images per location and health, grouped by subspecies.\n","metadata":{"_uuid":"4af46caf797b70aaab2728bacb710dc0bedf6d82"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['health', 'location'])['subspecies'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\nhover_text = []\nfor index, row in df.iterrows():\n    hover_text.append(('Subspecies: {}<br>'+\n                      'Health: {}<br>'+\n                      'Location: {}<br>'+\n                      'Images: {}').format(row['subspecies'],\n                                            row['health'],\n                                            row['location'],\n                                            row['Images']))\ndf['hover_text'] = hover_text\nsubspecies = (honey_bee_df.groupby(['subspecies'])['subspecies'].nunique()).index\ndata = []\nfor subs in subspecies:\n    dfL = df[df['subspecies']==subs]\n    trace = go.Scatter(\n        x = dfL['location'],y = dfL['health'],\n        name=subs,\n        marker=dict(\n            symbol='circle',\n            sizemode='area',\n            sizeref=0.2,\n            size=dfL['Images'],\n            line=dict(\n                width=2\n            ),),\n        mode = \"markers\",\n        text=dfL['hover_text'],\n    )\n    data.append(trace)\n    \nlayout = dict(title = 'Number of bees images per location, health and subspecies',\n          xaxis = dict(title = 'Location', showticklabels=True), \n          yaxis = dict(title = 'Health', tickangle=45),\n          hovermode = 'closest'\n         )\nfig = dict(data = data, layout = layout)\niplot(fig, filename='images-subspecies-health-location')","metadata":{"_uuid":"c0db04f3d19d09c261ae58998e8020cb1f435d4f","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot few images for each **health** category.","metadata":{"_uuid":"6d7c5d7c4f208b1accc096bfb04fbdb4e7abf15c"}},{"cell_type":"code","source":"draw_category_images(\"health\")","metadata":{"_kg_hide-input":true,"_uuid":"5a55bf50eee2c193284a6d1cf83605eabf92687f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='36'>Pollen carying</a>   \n\nLet's check now the pollen carying distribution.","metadata":{"_uuid":"995b4083742a1371787e1c614ee843ea52225795"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['pollen_carrying'])['year'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\ndf","metadata":{"_kg_hide-input":true,"_uuid":"ad7da88a35c5fc6872d80e7dd676163fc5e66d2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 18 out of 5172 (0.34%) of the images are of bees carrying pollen. We will not try to predict this, as the data is highly unballanced. Let's see what species have pollen carrying.","metadata":{"_uuid":"79694b1ccb952b53062d99fc49fae1d93ec1dbd8"}},{"cell_type":"code","source":"tmp = honey_bee_df.groupby(['pollen_carrying'])['subspecies'].value_counts()\ndf = pd.DataFrame(data={'Images': tmp.values}, index=tmp.index).reset_index()\ndf[df['pollen_carrying']==True]","metadata":{"_kg_hide-input":true,"_uuid":"e52e1625e7602fd1ad352a820e23097971f8cdd7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Majority of pollen carrying bees are of unknown species (67%) and the rest (33%) are `Italian honey bee`.  \n\nLet's plot few images with honey bees either **carrying pollen** or not.","metadata":{"_uuid":"72f2130a17b81ee39e604d0b93aafd8a0990f37f"}},{"cell_type":"code","source":"draw_category_images(\"pollen_carrying\")","metadata":{"_kg_hide-input":true,"_uuid":"5899b06541c1888b2918cd1963663441c81d001a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='37'>Caste</a>   \n\nLet's check now the`Caste` distribution.","metadata":{"_uuid":"037c7be0ed553383d169d4465459a84e60b2ff6b"}},{"cell_type":"code","source":"honey_bee_df.groupby(['caste'])['caste'].nunique()","metadata":{"_kg_hide-input":true,"_uuid":"866a725b3f3ed7f9815cf9ee442eb18b172a44e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the bees are of `worker` caste.   \n\n\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>  ","metadata":{"_uuid":"d3cbdebbc697f1ff8161fb38eb4bf1d640619af0"}},{"cell_type":"markdown","source":"# <a id='4'>Subspecies classification</a>\n\nOur objective is to use the images that we investigated until now to correctly identify the subspecies. We have a unique dataset and we will have to split this dataset in **train** and **test**. The **train** set will be used for training a model and the test will be used for testing the model accuracy against new, fresh data, not used in training.\n\n","metadata":{"_uuid":"c2a5e2401b418f1723c859ee9e0b4ad5071e4a82"}},{"cell_type":"markdown","source":"## <a id='40'>Split the data</a>  \n\nFirst, we split the whole dataset in train and test. We will use **random_state** to ensure reproductibility of results.   \n\nThe train-test split is **80%** for training set and **20%** for test set.\n","metadata":{"_uuid":"e8c0a6df4bb85bcdf90f7c908decab07304d660f"}},{"cell_type":"code","source":"train_df, test_df = train_test_split(honey_bee_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, \n                                     stratify=honey_bee_df['subspecies'])","metadata":{"_kg_hide-input":true,"_uuid":"352d452d5212d8c9eff074f11820b03a0d44387b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will split further the **train** set in **train** and **validation**. We want to use as well a validation set to be able to measure not only how well fits the model the train data during training (or how well `learns` the training data) but also how well the model is able to generalize so that we are able to understands not only the bias but also the variance of the model.  \n\nThe train-validation split is **80%** for training set and **20%** for validation set.","metadata":{"_uuid":"856060cc500db00e472b7755c91aba20c953a5f6"}},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=train_df['subspecies'])","metadata":{"_kg_hide-input":true,"_uuid":"83d0be04ae5a4ad5834631bf18e21917d6313bcd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the shape of the three datasets.","metadata":{"_kg_hide-input":true,"_uuid":"0dcaa8c2c5423ab8fc2898d4a4aa937801592c2c"}},{"cell_type":"code","source":"print(\"Train set rows: {}\".format(train_df.shape[0]))\nprint(\"Test  set rows: {}\".format(test_df.shape[0]))\nprint(\"Val   set rows: {}\".format(val_df.shape[0]))","metadata":{"_uuid":"8247f70b4deb4600fe322f004733234ed37617f0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now ready to start building our first model.","metadata":{"_uuid":"ee768c083f40fcbd109425182bc55ce86173b69d"}},{"cell_type":"markdown","source":"## <a id='41'>Build a baseline model</a>    \n\n\nNext step in our creation of a predictive model is to create a simple model, a **baseline model**.  \n\n Why start with a simple model (as simple as possible, but not simpler :-) )?\n \n With a simple model, we can get fast insight in how well will the data predict our target value. Looking to the training results (the training error and accuracy, the validation error and accuracy), we can understand if we need to add more data (because the training accuracy is small) or if we need to optimize the model (by adding more convolutional layers) or if we need to add Dropout layers (because the validation error is increasing after few steps - the model is overfitting) etc.\n \nLet's define few auxiliary functions that we will need for creation of our models.\n\nA function for reading images from the image files, scale all images to 100 x 100 x 3 (channels).","metadata":{"_uuid":"d76e822dd76565d29fcfed323cb034939f307581"}},{"cell_type":"code","source":"def read_image(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    image = skimage.transform.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), mode='reflect')\n    return image[:,:,:IMAGE_CHANNELS]","metadata":{"_kg_hide-input":true,"_uuid":"f80b4e20e98ce5bf328fba3a22457c4a994de06b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A function to create the dummy variables corresponding to the categorical target variable.","metadata":{"_uuid":"e396e0cd23633af2169e4d50985f1987654205a9"}},{"cell_type":"code","source":"def categories_encoder(dataset, var='subspecies'):\n    X = np.stack(dataset['file'].apply(read_image))\n    y = pd.get_dummies(dataset[var], drop_first=False)\n    return X, y","metadata":{"_kg_hide-input":true,"_uuid":"0f7a2146ca93aef9367ecd64300980005d89911b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's populate now the train, val and test sets with the image data and create the  dummy variables corresponding to the categorical target variable, in our case `subspecies`.","metadata":{"_kg_hide-input":true,"_uuid":"b40c205d5189b23cbbc4ef0cda8798721d504ff9"}},{"cell_type":"code","source":"X_train, y_train = categories_encoder(train_df)\nX_val, y_val = categories_encoder(val_df)\nX_test, y_test = categories_encoder(test_df)","metadata":{"_kg_hide-input":true,"_uuid":"70acefcd6dc5d494b1c7db6dc90bae5f8c856d94","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to start creating our model.  \n\nWe will add the folllowing elements to our model: \n* One convolutional layer, with 16 filters of dimmension 3;  \n* One maxpoll2d layer, with reduction factor 2;  \n* One convolutional layer, with 16 filters of dimmension 3;  \n* A flatten layer;  \n* A dense layer;  ","metadata":{"_uuid":"5093354dc9c7f0510ba54a254690db45e38d0bcc"}},{"cell_type":"code","source":"model1=Sequential()\nmodel1.add(Conv2D(CONV_2D_DIM_1, kernel_size=KERNEL_SIZE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS), activation='relu', padding='same'))\nmodel1.add(MaxPool2D(MAX_POOL_DIM))\nmodel1.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\nmodel1.add(Flatten())\nmodel1.add(Dense(y_train.columns.size, activation='softmax'))\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"_uuid":"bd7147baf9c45217988df92cf631202684fb3609","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.summary()","metadata":{"_uuid":"046612eda2408801ded03cc6a5e2357a99298969","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are also using a **ImageDataGenerator** that creates random variation of the training dataset, by applying various techniques, including:\n* rotation (in a range of 0-180 degrees) of the original images;  \n- zoom (10%);  \n- shift in horizontal and in vertical direction (10%);  \n- horizontal and vertical flip;  \n","metadata":{"_uuid":"725cef4194ab0eeb3901ba68f5226aa94ccda331"}},{"cell_type":"code","source":"image_generator = ImageDataGenerator(\n        featurewise_center=False,\n        samplewise_center=False,\n        featurewise_std_normalization=False,\n        samplewise_std_normalization=False,\n        zca_whitening=False,\n        rotation_range=180,\n        zoom_range = 0.1, \n        width_shift_range=0.1,\n        height_shift_range=0.1, \n        horizontal_flip=True,\n        vertical_flip=True)\nimage_generator.fit(X_train)","metadata":{"_uuid":"d2e402a92832cfd95cb3668431482d40854bb93a","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We train the first model using **fit_generator** and a predefined batch size. The **steps_per_epoch** is calculated to be size of the training set divided by the batch size. We are using the predefined epoch number for this first experiment (5 steps) and as well validation, using the validation set. ","metadata":{"_uuid":"0149020f748dc9eaa89ebf732829031d6d9d35a2"}},{"cell_type":"code","source":"train_model1  = model1.fit_generator(image_generator.flow(X_train, y_train, batch_size=BATCH_SIZE),\n                        epochs=NO_EPOCHS_1,\n                        validation_data=[X_val, y_val],\n                        steps_per_epoch=len(X_train)/BATCH_SIZE)","metadata":{"_uuid":"19ac76b9eed3495049a0546402368a529c5db2cb","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='42'>Model evaluation</a> \n\n\nLet's start by plotting the loss error for the train and validation set. \nWe define a function to visualize these values.","metadata":{"_uuid":"5837b3e9cb131fab2c58f4b9de92f147f48b59ae"}},{"cell_type":"code","source":"def create_trace(x,y,ylabel,color):\n        trace = go.Scatter(\n            x = x,y = y,\n            name=ylabel,\n            marker=dict(color=color),\n            mode = \"markers+lines\",\n            text=x\n        )\n        return trace\n    \ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['acc']\n    val_acc = hist['val_acc']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = list(range(1,len(acc)+1))\n    #define the traces\n    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n                                                             'Training and validation loss'))\n    #add traces to the figure\n    fig.append_trace(trace_ta,1,1)\n    fig.append_trace(trace_va,1,1)\n    fig.append_trace(trace_tl,1,2)\n    fig.append_trace(trace_vl,1,2)\n    #set the layout for the figure\n    fig['layout']['xaxis'].update(title = 'Epoch')\n    fig['layout']['xaxis2'].update(title = 'Epoch')\n    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n    #plot\n    iplot(fig, filename='accuracy-loss')\n\nplot_accuracy_and_loss(train_model1)","metadata":{"_uuid":"a87e3beea44a87a806893b798a38d26904d10718","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nLet's continue by evaluating the **test** set **loss** and **accuracy**. We will use here the test set.","metadata":{"_uuid":"a77c0127288f090233e70831e6b909c1618efa35"}},{"cell_type":"code","source":"score = model1.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"_kg_hide-input":true,"_uuid":"1f54e33fcba0e3054d364f35a22f69ef350e8e0d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check also the test accuracy per class.","metadata":{"_uuid":"46cd7e86e92ac2ec484f0c38c451465cc16a2736"}},{"cell_type":"code","source":"def test_accuracy_report(model):\n    predicted = model.predict(X_test)\n    test_predicted = np.argmax(predicted, axis=1)\n    test_truth = np.argmax(y_test.values, axis=1)\n    print(metrics.classification_report(test_truth, test_predicted, target_names=y_test.columns)) \n    test_res = model.evaluate(X_test, y_test.values, verbose=0)\n    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])","metadata":{"_kg_hide-input":true,"_uuid":"55e96cfdaf488df5bc3d5511fa062563926227ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy_report(model1)","metadata":{"_uuid":"68c06c36ae2f89e070c878bf5f660e765d23878b","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We used a simple model. We separated 20% of the data for testing. From the training data, 80% is used for actual training and 20% for testing.   \nThe data is unbalanced with respect of the classes of subspecies.   \nThe accuracy of the training set obtained after only 5 epochs was 0.84, with a loss of 0.38.    \nThe accuracy of the validation set remained around 0.84 and the loss remained constant after epoch 4.  \n\nAdding additional data will only slightly increase the accuracy of the training set (it is already very good).   \nTo reduce the loss of the validation set (which is a sign of overfitting), we can have three strategies:  \n* add Dropout layers;  \n* introduce strides;  \n* modify the learning rate during the training;  \n","metadata":{"_uuid":"28293f20eb559420e61e052397bd998fe6e6ff05"}},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='43'>Add Dropout</a>  \n\nWe add two Dropout layers.  The role of the Dropout layers is to reduce the overfitting, by dropping, each training epoch, a certain percent of the nodes connections (by rotation). This is equivalent of using less training data and in the same time training the network with various data as well as using `parallel` alternative networks, thus reducing the likelihood that the network will overfit the train data.  \n\nThe definition of the second model is:","metadata":{"_uuid":"74de02a417db465b77495147c810911d81f491a9"}},{"cell_type":"code","source":"model2=Sequential()\nmodel2.add(Conv2D(CONV_2D_DIM_1, kernel_size=KERNEL_SIZE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS), activation='relu', padding='same'))\nmodel2.add(MaxPool2D(MAX_POOL_DIM))\n# Add dropouts to the model\nmodel2.add(Dropout(0.4))\nmodel2.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n# Add dropouts to the model\nmodel2.add(Dropout(0.4))\nmodel2.add(Flatten())\nmodel2.add(Dense(y_train.columns.size, activation='softmax'))\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"_kg_hide-input":true,"_uuid":"ca30aceff3500c10383761962a7908f0b2b558f3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's inspect the new model.","metadata":{"_uuid":"cff8a87a7e837de5dd0b001ace6249933027d95b"}},{"cell_type":"code","source":"model2.summary()","metadata":{"_uuid":"63dff8b355758d2d55d3e7aff154e9ed6f23d961","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that this model has the same number of parameters and trainable parameters as  the previous model.","metadata":{"_uuid":"dd8b5497fa489c35d7ff77319f7d05de46186ac1"}},{"cell_type":"code","source":"train_model2  = model2.fit_generator(image_generator.flow(X_train, y_train, batch_size=BATCH_SIZE),\n                        epochs=NO_EPOCHS_2,\n                        validation_data=[X_val, y_val],\n                        steps_per_epoch=len(X_train)/BATCH_SIZE)","metadata":{"_uuid":"b97e8d966f8a369c4503bffd419d57c1d113bd1b","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate model accuracy and loss","metadata":{"_uuid":"2254e9082c703f5744f61bc59018f107aca6757c"}},{"cell_type":"code","source":"plot_accuracy_and_loss(train_model2)","metadata":{"_uuid":"9a69aeb83cee0c57a2e362349925ff509c7af7ee","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test accuracy and loss\n\nLet's evaluare as well the test accuracy and loss.","metadata":{"_uuid":"dc0fb043121e04f92f1af7d7821b8581cca8c572"}},{"cell_type":"code","source":"test_accuracy_report(model2)","metadata":{"_uuid":"19285aa59a4a6dabbf55e81ebdc235ef50c46411","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='45'>Model refinement</a>  \n\n\nWe define now also a refined model. \n\nWe add an early stopping condition (monitor the loss error and stops the training if for a number of stept given in the `patience` parameters the loss is not improving).\n\nWe are also saving a model checkpoint after each epoch when accuracy improves; if accuracy degrades, no new model is saved. Thus, Model Checkpoint saves all the time the best model in terms of accuracy.  \n\nWe adjust as well the learning rate with the training epochs.\n\nAlso, we increase the number of training epochs to 50.\n\n","metadata":{"_uuid":"349c34acbba6d761b959a0a7a5b31df9abdf722b"}},{"cell_type":"code","source":"annealer3 = LearningRateScheduler(lambda x: 1e-3 * 0.995 ** (x+NO_EPOCHS_3))\nearlystopper3 = EarlyStopping(monitor='loss', patience=PATIENCE, verbose=VERBOSE)\ncheckpointer3 = ModelCheckpoint('best_model_3.h5',\n                                monitor='val_acc',\n                                verbose=VERBOSE,\n                                save_best_only=True,\n                                save_weights_only=True)","metadata":{"_kg_hide-input":true,"_uuid":"deb1e2d4ced60d163ae5257830a3b60dc2d8fc0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3=Sequential()\nmodel3.add(Conv2D(CONV_2D_DIM_1, kernel_size=KERNEL_SIZE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS), activation='relu', padding='same'))\nmodel3.add(MaxPool2D(MAX_POOL_DIM))\n# Add dropouts to the model\nmodel3.add(Dropout(0.4))\nmodel3.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n# Add dropouts to the model\nmodel3.add(Dropout(0.4))\nmodel3.add(Flatten())\nmodel3.add(Dense(y_train.columns.size, activation='softmax'))\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"_uuid":"83da1405441c237be0537abd81baf8f90638ce40","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's inspect the refined model.","metadata":{"_uuid":"66c5f7c6d62ee01b17b85960456f7b0502415aa8"}},{"cell_type":"code","source":"model3.summary()","metadata":{"_uuid":"b521fad3c6db559e77eeee1147f0b7a02fd91a13","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's train the model.","metadata":{"_kg_hide-input":true,"_uuid":"c773e443457fb4999513b5d0eb7d2454bae419e7"}},{"cell_type":"code","source":"train_model3  = model3.fit_generator(image_generator.flow(X_train, y_train, batch_size=BATCH_SIZE),\n                        epochs=NO_EPOCHS_3,\n                        validation_data=[X_val, y_val],\n                        steps_per_epoch=len(X_train)/BATCH_SIZE,\n                        callbacks=[earlystopper3, checkpointer3, annealer3])","metadata":{"_uuid":"a7ac74a25bed9338906effff9d7df171d7b8b154","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model accuracy and loss","metadata":{"_uuid":"c1d333286edff47c2452cdd54df91c5ad7959ef7"}},{"cell_type":"code","source":"plot_accuracy_and_loss(train_model3)","metadata":{"_uuid":"8adfacee0f01e584915a712f0501c105287e70dc","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test accuracy and loss","metadata":{"_uuid":"c0a45ee7288e16e571110dd134248ec880ccaadc"}},{"cell_type":"code","source":"test_accuracy_report(model3)","metadata":{"_uuid":"1ad166de21bdd7e097fb73e64ec564b61358a6c1","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n# <a id='6'>Conclusions</a>  \n\nAfter exploring the data to understand its various features, a baseline model is created.   \nEvaluation of the baseline model  results for valid set and test set allows us to decide, based on analysis of bias and variance, how to conduct furher our experiments.\n\nFrom the possible solutions for overfitting, we choose to add Dropout layers. Adding Dropout layers improve a bit the algorithm performance (reduce overfitting).  \n\nA third model, with adjustable learning rate, early stoping based on validation accuracy measurement and saving the model with best accuracy was also created. With this model, accuracy of prediction for the test set was further improved.\n\nThe key lessons learned from this Kernel are the following:   \n\n* start by analyzing the data;   \n* follow with a simple baseline model;   \n* refine gradually the model, by making corrections based on the analysis of the (partial) results.\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>","metadata":{"_uuid":"cd6bd3fba3f98e9775dac4f313371af4701febf3"}},{"cell_type":"markdown","source":"# <a id='7'>References</a>  \n\n[1] Gabriel Preda, RSNA Pneumonia Detection EDA, https://www.kaggle.com/gpreda/rsna-pneumonia-detection-eda     \n[2] Gabriel Preda, CNN with Tensorflow|Keras for Fashion-MNIST, https://www.kaggle.com/gpreda/cnn-with-tensorflow-keras-for-fashion-mnist    \n[3] DanB, CollinMoris, Deep Learning From Scratch, https://www.kaggle.com/dansbecker/deep-learning-from-scratch  \n[4] DanB, Dropout and Strides for Larger Models, https://www.kaggle.com/dansbecker/dropout-and-strides-for-larger-models  \n[5] BGO, CNN with Keras, https://www.kaggle.com/bugraokcu/cnn-with-keras  \n[6] Dmitri Pukhov, Hony Bee health detection using CNN, https://www.kaggle.com/gpreda/honey-bee-health-detection-with-cnn/notebook     \n[7] Why Dropounts prevent overfitting in Deep Neural Networks, https://medium.com/@vivek.yadav/why-dropouts-prevent-overfitting-in-deep-neural-networks-937e2543a701  \n[8] Dropout: A Simple Way to Prevent Neural Networks from Overfitting, https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf  \n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"}}]}