{"cells":[{"metadata":{},"cell_type":"markdown","source":"<p><center><span style=\"color:steelblue; font-size:3em;\">Stop it before it starts. </span></center></p>\n<p><center><span style=\"color:steelblue; font-size:3em;\">Predict who will leave next.</span></center></p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://www.salesforcesearch.com/wp-content/uploads/2017/12/employee-in-a-hurry-to-leave.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> | Table Of Contents </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Understanding the Business Goal: Project Description & Objectives\n2. Exploratory Data Analysis\n3. Data Preparation\n4. Algorithms Definition\n5. K-NN for Dataset with All Features (Statistically Significant and Insignificant)\n *    5.1.Modeling\n *    5.2 Model Evaluation\n6. K-NN for Dataset with Only Statistically Significant Features\n *    6.1 Modeling\n *    6.2.Model Evaluation\n7. Random Forest\n *    7.1 Modeling\n *    7.2 Model Evaluation\n8. Conclusion\n9. Possible Business Use and Potential Improvements\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://)<span style=\"color:steelblue; font-size:1.5em;\"> || Project participants </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Olena Kudrenko (1787681)\n* Jessica Wendler (1779385)\n* Anda Gurabardhi (1778674)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 1. Understanding the Business Goals </span>\n<p>\n  <span style=\"color:steelblue; font-size:1.5em;\">  Project Description & Objectives </span>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"\n#### Project Description & Objectives:\n\nThe multinational technology company, IBM, headquartered in the USA but with more than 350,000 employees worldwide, has met with unexpected attrition of their employees (Statista, 2020). To find out the reasons why employees are leaving the company, IBM decided to analyze the profiles of those employees. In order to be able to react promptly and avoid any further attrition, IBM decided to make use of artificial intelligence (AI) and wants to implement a machine learning (ML) model that can predict which employee will most likely leave the company next. Research has increasingly shown that AI can accomplish this type of task (Saranya and Sharmila Devi, 2018; Sri Harsha et al., 2020).\n\nThis is important to know because in a competitive market, companies are fighting for the most talented employees and recruiting new employees costs time and resources. Moreover, employees carry tacit knowledge when they leave a company, which is highly valuable and hard to replace (Alao and Adeyemo, 2013; Sri Harsha et al., 2020). \n\nThe goal of this project is to predict which valuable employee will leave IBM next. The data set “IBM HR Analytics Employee Attrition & Performance” (2017) was created by an IBM data scientist and will be the foundation for this project. \n\n\nTo accomplish this task, a machine learning model will be introduced. \n\nMachine Learning\n“Machine learning can be broadly defined as computational methods using experience to improve performance or to make accurate predictions.” (Mohri et al., 2018, p.1). Data that contains past information is used as experience from which the program can learn from and make predictions (Mohrl et al., 2018). Machine learning is a type of AI and can help with finding solutions for many different problems. It can be broadly grouped into 3 categories:  \n\nSupervised learning\n\nUnsupervised learning\n\nReinforcement learning\n\nA predictive ML model (supervised learning) aims to make future predictions, whereas a descriptive ML model (unsupervised learning) aims to gain knowledge from the data (Alpaydin, 2020, p. 3). Reinforcement learning means that through continuous feedback (trial and error) the algorithm learns to optimize the model (Igual and Seguí, 2017, p. 67). In this project we used supervised learning algorithms of two types which we will elaborate on below.\n\nSupervised Learning\nIn supervised learning, the ML algorithms learn from labeled training data. The algorithms used for this project are called k-nearest neighbor and random forest classification algorithms, data is categorized into different classes and the algorithm predicts how likely it is that data will fall into one of the classes. K-nearest neighbor (k-NN) classification is where an object is sorted into a class that is most common amongst its k nearest neighbors. Random forest classification is a decision-tree based algorithm that, after training, takes a random assortment of decision trees and classifies an object according to the mode average result of that assortment. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pandas\nPandas library is used for data manipulation, analysis and cleaning. Python pandas is well suited for different kinds of data, such as: tabular data with heterogeneously-typed columns;Ordered and unordered time series data;Arbitrary matrix data with row & column labels;Unlabelled data; Any other form of observational or statistical data sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NumPy\n\n“The NumPy library is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays.” (Willems, 2017). NumPy arrays are more compact and convenient than Python lists, as they are faster in reading and writing items, which makes them very efficient. (Willems, 2017). It is also useful in linear algebra, random number capability, etc.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization libraries\n\n“Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.” (“seaborn: statistical data visualization”, n.d.)\n“Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.” (“Matplotlib: Visualization with Python”, n.d.)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\">2. Exploratory Data Analysis </span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Import\n\nFirst we found the correct dataset \"IBM HR Analytics Employee Attrition & Performance\" in the kaggle database and added it into the input folder of our project.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We read dataset (dataset is an csv file),features are listed in the very first row in bold.\ndf = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.head() shows the heading of the table + first 5 rows\n# alternative way: df.tail() or print(df.head(1470))-less UX friendly\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have noticed that not all column names from a Pandas DataFrame have been shown, some features are missing. \n# We can reproduce them in different ways: \n\n# Option 1: use key method\n# print(df.keys())\n\n# Option 2:  use the columns method: iterating over the columns to get all their names\n# for col_name in df.columns: \n    # print(col_name)\n    \n# Option 3: use the list() method to print the column names as a list.\nprint(list(df.columns))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To get more explicite information about our dataframe we use\n\ndf.info()\n\n# Additionally, we can receive similar information using different functions\n\n# 1. To define the total number of columns to double check, we use built-in function len()\n# len(df.columns)\n\n# 2. To check if we have any missing data\n# df.count()\n\n# 3. To define data types of the columns of the dataframe\n# df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descriptive statistics of the numerical attributes\n# df.describe(include='all')\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explanation of Attributes\n\n* unique - the amount of unique values in the column (df.describe(include='all')). \n* freq- how often a value value occurs (df.describe(include='all')).\n* mean - average of values, can be applied only in case if datatype is int64.\n* std - the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range. In case of high standard deviation (close to max) - feature can be dropped, as it does not influence the employee decision.\n* 25% -  first / lower quartile.\n* 50% - median, which is the second quartile.\n* 75% - third / upper quartile.\n* min and max represent the highest and the lowest value in the column. Comparing min. and max. helps us to understand if we need to scale. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Description\n\nWe have 34 feature columns and  1 column with target property. \nOur dataset includes 35 feature columns that include categorical features (e.g. Sex, Department, EducationField, etc.), features with continuous values like Age, StockOptionLevel, StandardHours, etc.   \nThe target feature **Attrition** is binary.\n  \nEach feature has 1470 entries.\nThere is not missing data in our DataFrame.\n25 features have the data type int64.\n9 features have the data type object: text or mixed numeric and non-numeric values.\n \nAll features with an object data type will be encoded (see section 3) (Moffitt, 2018). ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Minor data preparation (transformation) is implemented for further better data visualization.\n\n#Transformation of values: Attrition,Gender and OverTime values into \"0\" and \"1\" & obj into int \ndf.replace({'Attrition' : { 'Yes': 1, 'No': 0 }}, regex=True, inplace=True)\ndf.replace({'Gender' : { 'Female': 1, 'Male': 0 }}, regex=True, inplace=True)\ndf.replace({'OverTime' : { 'Yes': 1, 'No': 0 }}, regex=True, inplace=True)\n#df to check Attrition column values \n#df.info() #to check type\n#df['Attrition']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that our data set is imbalanced. (imbalance in class: attrition)\n\nAn imbalanced classification problem occurs whenever the number of data for the individual class labels is not balanced. “An imbalance occurs when one or more classes have very low proportions in the training data as compared to the other classes” and the distribution of data is biased or skewed (Kuhn and Johnson, 2013, p. 419). A balanced dataset would have an equal or close to equal distribution of data throughout the different class labels (Browniee, 2019).\n\nA possible solution to this issue will be described in section 5.\n\n____________\n\nAt IBM, 16.1% of the workforce accounts for attrition. In order to better understand causal relationships between attrition, the target variables and relevant features, we used data visualization. We think that following features might influence attrition at IMB: Age, Monthly Income, Overtime, and Salary Hike. To see the distribution of attrition across these features we created the following plots:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n=len(df)\nattr_0=len(df[df['Attrition']==0])\nattr_1=len(df[df['Attrition']==1])\nleft=attr_1*100/n\nstayed =attr_0*100/n\nprint(\"Percentage of employees that left the company: \",left)\nprint(\"Percentage of employees that decided to keep working for IBM: \",stayed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = 'Employee Attrition', ''\nsizes = left, stayed\ncolors = ['skyblue', 'steelblue']\n\nexplode = (0, 0.1)  # only \"explode\" the 2nd slice \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, colors=colors, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(\"Distribution of Attrition at IBM\", fontsize=15, y=1.02,\n          color='steelblue')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following graph shows how the target variable attrition is distributed with respect to the age of the employees. Grouping by the means of attrition, we can notice that prevalently employees between 18 and 35 years of age tend to leave the company. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,4))\nsns.countplot(x='Age',hue='Attrition', data=df, palette = 'Blues')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['Age'])['Attrition'].mean().plot(kind='bar', color='steelblue',grid=False, figsize=(12,4))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When plotting the graphs for Attrition against Overtime, we can see that Attrition is prevalent within employees that work overtime. When grouping the target variable and feature by means, attrition for overtime working employees is three times more. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(5,4))\nsns.countplot(x='OverTime',hue='Attrition', data=df, palette = 'Blues')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['OverTime'])['Attrition'].mean().plot(kind='bar', color='steelblue',grid=False, figsize=(5,4))\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To analyze the relationship between our target variable and continuous features such as Monthly Income or Salary Hike, we decided to plot Boxplots. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(5, 6)) \nsns.boxplot(x ='Attrition', y ='MonthlyIncome', data = df, palette = 'Blues')\nplt.xlabel('Attrition',fontsize=10)\nplt.ylabel('Monthly Income',fontsize=10, rotation=90)\nplt.title(\"Attrition and Monthly Income\", fontsize=15, y=1.02,\n          color='steelblue')\n\nplt.figure(figsize =(5,6)) \nsns.boxplot(x ='Attrition', y ='PercentSalaryHike', data = df, palette = 'Blues')\nplt.xlabel('Attrition',fontsize=10)\nplt.ylabel('Salary Hike in Percentage',fontsize=10, rotation=90)\nplt.title(\"Attrition and Salary Hike\", fontsize=15, y=1.02,\n          color='steelblue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We visualized the distribution of attrition among five relevant factors. We think that following features (based on employees’ background) might influence employees’ decision to quit or retain their job at IBM. To see the correlation of attrition with these features we created the following plots.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Attrition vs:\n* #### Employee Job Satisfaction Rating \n* #### Environment Satisfaction \n* #### Work Life Balance\n* #### JobInvolvement\n* #### JobLevel","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#<--Dear Prof.Mueller, please run the code cell one more time if you see the empty graph.\n\nfeatures_rating = ['JobSatisfaction','EnvironmentSatisfaction','WorkLifeBalance',\n                  'JobInvolvement', 'JobLevel']\n\ntitle= ['Employee Job Satisfaction Rating - Attrition vs No Attrition',\n        'Employee Environment Satisfaction Rating - Attrition vs No Attrition',\n       'Work Life Balance Rating - Attrition vs No Attrition',\n             'Job Involvement Rating - Attrition vs No Attrition',\n        'Job Level Rating - Attrition vs No Attrition']\n\nj = 0\nfor f in features_rating:\n        fig1=plt.figure(figsize=(15,4)) \n        ax=sns.kdeplot(df.loc[(df['Attrition']==0),f],color='steelblue',shade=False,label='No Attrition') \n        ax=sns.kdeplot(df.loc[(df['Attrition']==1),f],color='skyblue',shade=True,label='Attrition') \n        ax.set(xlabel= features_rating[j] ,ylabel='Frequency') \n        title_string = title[j]\n        plt.title(title_string,fontsize=20, color='steelblue', y=1.02)\n        j = j+1\n#fig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\">3.Data preparation </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Steps to conduct preparing data:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Missing data\nOne very important task when working with machine learning algorithms is dealing with missing data. Many algorithms do not support data with any missing values, therefore first missing data needs to be handled before proceeding with modeling (Browniee, 2017). \nUsing the function df.info() we identified that our data set has no missing values. To double-check it, we proceeded as following: First we checked if there any “0” values in the DataFrame where they should not be “0”, then we also checked if there any “NaN” values in the DataFrame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_missing = (df == 0).sum()\nprint(num_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_missing = (df == 'NaN').sum()\nprint(\"Total amount of values checked\", len(num_missing))\nprint(num_missing)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can se that the features: NumCompaniesWorked, StockOptionLevel,TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager have \"0\" values. However, these values should not be replaced by NaN. These features can have the value 0, as for example an employee could work less than 1 year or have 0 stock. \n\n* df.replace(0, np.nan) #replace all values of 0 with NaN\n\nOnce we found out that we do not have any \"0\" value in our dataframe, we have to check if there any \"NaN\" values in the dataframe.\nIn Python, specifically Pandas, NumPy and Scikit-Learn, we mark missing values as NaN.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()#returns a boolean value answering the question if Daraframe has missing data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.isnull().sum() shows the total sum of missing data for each feature\ndf.isnull().sum().sum() #total number of missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df)\nm=sns.heatmap(df.isnull(),cmap=sns.color_palette(\"Blues\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we did not identify any missing values in our dataset, we did not need to take any additional measure. However if we had missing data, we would have implemented the following steps:\n* Define the total sum of missing data for each feature: df.isnull().sum() \n* In case that the missing values are more than 60% of the total data of the feature, we would have to drop the feature as it would be insignificant for our model. The following function would then be applied: df_missing= df.drop(columns=['X=Missing_more_than_60%']\n* In case the amount of missing data is less than 60% of the total data of the feature, we would have replaced missing values with sensible values. One way do this is using the function fillna() of Pandas. Here missing values can be replaced by: \n     * a special value. The function would be df.fillna('X')\nthe value before. It could be .ffill-Nearest proceeding non-NaN entry in each column or after .bfill’ in case of time-series datasets.\n     * or an aggreate value such as mean, median, etc., which could also be taken. The function would be mean=df['column_with missing_data'].mean() df.['column_with missing_data'].fillna(mean) \n(Browniee, 2017).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Important features for our model:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Categorical features that should be transform into binary data for each level of feture with one-hot encoding:\nCategorical features that should be transformed into binary data for each level of feature with one-hot encoding:\n\n* **Department**: 3 unique values: Sales, Research,, Others\n* **EducationField**: 5 unique values: Technical Degree, Marketing, Life Sciences, Medical , Other\n* **Education**: 5 unique values: Below College, College , Bachelor, Master, Doctor\n* **MaritalStatus** 3 unique values\n* **BusinessTravel**: 3 unique values\n* **JobRole**: 9 unique values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(df.JobRole.unique()) to check unique values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df[['MaritalStatus','JobRole', 'Department','Education','EducationField','BusinessTravel']].copy()\n#Department_bi=pd.get_dummies(df2.Department, prefix='Department')\n\ntemp = pd.concat([pd.get_dummies(df2[col],prefix=col) for col in df2], axis=1)\n                    \n#Attaching to the DataFrame\ndf_dummies = pd.concat([df,temp],axis=1)\n\n#Dropping features that are repeating as well as label feture\ndf_dummies= df_dummies.drop(columns=['MaritalStatus', 'JobRole','Department','Education','EducationField',\n'BusinessTravel' ])\n\n#Another Version, by doing it all manually 1 by 1: \n#Creating dymmies: Department_bi=pd.get_dummies(df2.Department, prefix='Department')\n#Attaching to the DataFrame: df_new = pd.concat([df, Education_bi], axis=1)\n\n#check:\n#df_dummies.head()\n#print(list(df_dummies.columns))\n#df_dummies.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features that should be scaled/normalized:\n\n* **Percent Salary Hike**: (scale 100%)\n\n**Categorical Features**: \n* **Job Level**\n* **Performance Rating**\n* **Relationship Satisfaction**\nFollowing features have semantic meaning:\n* **Environment Satisfaction**\n* **Job Involvement**\n* **Job Satisfaction**\n* **Work Life Balance**\n* **Relationship Satisfaction**\n\n\n**Continious Features**:\n* **Age**\n* **Daily Rate**: values. min:102 to max:1499\n* **Hourly Rate** probably correlated with  Daily Rate -> plot \n* **Monthly Rate** probably correlated with  Daily Rate -> plot\n* **Distance From Home**\n* **Monthly Income**\n* **Stock Option Level**\n* **Total Working Years**\n* **Training Times LastYear**\t\n* **Years Since Last Promotion** \t\n* **Number of  Compnies** \n* **Years At Company**\n* **Training Times last year**\n\n\nAs for some algorithm e.g. KNN, which uses the Euclidean distance, we have to ensure that all our features have the same range of values; have equal importance when calculating the distance. \nTo do so, we  use so-called Min-Max scaling (often also simply called \"normalization\" - a common cause for ambiguities). In this approach, the data is scaled to a fixed range - we took range from 0 to 1. Backside effect of it - in contrast to standardization - that eventually we will have smaller standard deviations, which can suppress the effect of outliers.\n\nA Min-Max scaling is typically done via the following equation:\n\n$x=\\frac{x-x_{min}}{x_{max}-x_{min}}$\n\n(“MinMax Scaling”, n.d.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from mlxtend.preprocessing import minmax_scaling\n#minmax_scaling(df_to_scale_part , columns=data_to_scale)\n\ndf_scaled = df_dummies.copy()\n\ndata_to_scale=['Age','DailyRate', 'DistanceFromHome', 'EnvironmentSatisfaction', 'HourlyRate', \n'JobInvolvement','JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', \n'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', \n'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n\n\nfor i in range(len(data_to_scale)):\n    column_maxes = df_scaled[data_to_scale[i]].max() \n    column_min = df_scaled[data_to_scale[i]].min() \n    df_scaled[data_to_scale[i]] = (df_scaled[data_to_scale[i]] - column_min) / (column_maxes-column_min)\n\n#Another version, by doing it all manually 1 by 1:\n#column_maxes = df['DailyRate'].max()\n#normalized_df = df['DailyRate'] / column_maxes\n#normalized_df -> check    \n#Check:\n#df_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chi-Square Test\n\nA chi-square test identifies a possible relationship between two **categorical variables**. It is the mostly used when testing the independence or dependence with a bivariate table / cross tabulation. Requirement is that the variables are independent from each other. The chi-square test shows whether the distribution of the variables differs from one to another (“Using Chi-Square Statistic in Research”, n.d.; “Statistics How To”, n.d.).\n \nWe used the chi-square test to compute the p-value in order to understand the relationship between categorical features and the target variable. \nA p-value ≤ α shows that the correlation is statistically significant and a correlation exists. A p-value > α shows that the correlation is not statistically significant (“Interpret the key results for correlation”, n.d.).\n \nWe have 31 categorical features in total in our data set and as it can be seen below\n16 features are statistically significant, \n15 features are statistically insignificant\n \nDespite the findings we think that the 15 features, which are statistically insignificant based on the p-value, still might be relevant for our model. Therefore, we decided to proceed with the testing and further check this. For this we will create two data sets: one set with only statistically significant values, and the other set with all categorical values. Afterwards we will compare the results. In this way we will be able to prove if the 15 values are indeed statistically insignificant (Browniee, 2018).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import chi2_contingency\n\ndf_categorical= temp.copy()\n\ndf_categorical['Gender']=df_scaled['Gender']\ndf_categorical['OverTime']=df_scaled['OverTime']\n\nstat_significant=[]\nstat_insignificant=[]\n\nfor i, item in enumerate (df_categorical):\n    csq=chi2_contingency(pd.crosstab(df[\"Attrition\"], df_categorical[item]))\n    #print(csq[0]) #chi2\n    if csq[1]<= 0.05: #p-value\n        stat_significant.append((item, csq[1])) #appened->add new value\n    else:\n        stat_insignificant.append((item, csq[1]))\n        \nprint(len(stat_significant),len(stat_insignificant))       \nprint(\"statistically significant values:\")\nprint(stat_significant)\nprint(\"statistically insignificant values:\")\nprint(stat_insignificant)\n\n#Alternative version to check p-value\n#csq=chi2_contingency(pd.crosstab(df['Attrition'], df['Gender']))\n#print(\"P-value: \",csq[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Check\n\nWhen we want to compare data or check their correlations, we need to make sure that data is **normalized first**. As correlation only works with numeric variables, we will transform our categorical variables into binary variables (Igual and Seguí, 2017, p. 47).\n\nChecking correlation helps to define the data columns with similar trends, which are likely to carry similar information and thus are useless for a robust model and should be removed.\nTo detect for high correlated variables, we used the function cor(). By default standard correlation coefficient will be computed by **Pearson**. \n\nFormula: \n$ r=\\frac{n(\\sum_{}^{}xy) - (\\sum_{}^{}x)(\\sum_{}^{}y)}{\\sqrt{[n\\sum_{}^{}x^2-(\\sum_{}^{}x)^2][n\\sum_{}^{}y^2-(\\sum_{}^{}y)^2]} }$\n\nCorrelatio always has a value between -1 and +1. A correlation of -1 or +1 indicates perfect correlated variables (Igual and Seguí, 2017, p. 47).\n \n* “A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n* A value closer to 1 implies stronger positive correlation\n* A value closer to -1 implies stronger negative correlation”\n(Shetye, 2019)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,15))\nplt.title(\"Correlation Plot before data normalization\", fontsize=30, y=1.02, color='steelblue') \ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=sns.color_palette(\"Blues\"))      \nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=df_scaled.corr()\ncorr=(corr)\n\nplt.figure(figsize=(21,15))\nplt.title(\"Correlation Plot with Normalised Data\",fontsize=30, y=1.02,color='steelblue')\ncor = df_scaled.corr()\nsns.heatmap(cor, cmap=sns.color_palette(\"Blues\"))      \nplt.show()\n\ncorr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(list(temp))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it was mentioned before, correlation is scale sensitive. However, we made a comparison between scaled and not scaled data. Analyzing it, we came to the conclusion that the overall correlation pattern has similarities. This gave us a chance to see feature correlation in a \"big picture\" and helps us to make assumption about potentially highly correlated feature combinations.\n \nIn order to create our model as accurate as possible, it is essential to go deeper into the correlation analysis through detailed comparison of potentially highly correlated feature combinations. This is further accomplished through grouping and checking correlation between closely related features. The grouping process is based on our personal assumptions as well as above correlation visualizations graphs. Following correlation groups / features combination will be analyzed:\n* Years metrics\n* Education Field vs. Department","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Years metics \ndf_Years= df_scaled[['TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', \n'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']].copy()\nplt.figure(figsize=(6,4))\nplt.title(\"Years Mertics\",fontsize=20, y=1.02,color='steelblue')\ncor = df_Years.corr()\nsns.heatmap(cor,annot=True, cmap=sns.color_palette(\"Blues\"))      \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Following features are highly correlated :\n'YearsAtCompany' and 'YearsWithCurrManager' and 'YearsInCurrentRole' \n\nAs was mentioned before - highly correlated features should be avoided for better model result accuracy. However, in order to keep the yet existence difference in their values, we decided to combine them in 1 future: YersCompanyManagerRole","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cor_years = df_scaled.copy()\n\ndf_cor_years['YersCompanyManagerRole']=df_cor_years.apply(lambda x: x['YearsAtCompany']+x['YearsWithCurrManager']+x['YearsInCurrentRole'], axis=1)\n\ndf_cor_years= df_cor_years.drop(columns=['YearsAtCompany','YearsWithCurrManager','YearsInCurrentRole'])\n#df_cor_years\n#df_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#'PercentSalaryHike' VS 'PerformanceRating'\ndf_Years= df_scaled[['PercentSalaryHike', 'PerformanceRating']].copy()\nplt.figure(figsize=(6,4))\nplt.title(\"Salary VS Performance\",fontsize=20, y=1.02,color='steelblue')\ncor = df_Years.corr()\nsns.heatmap(cor,annot=True, cmap=sns.color_palette(\"Blues\"))      \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following features are highly correlated: 'PerformanceRating' and 'PercentSalaryHike' and will be combined in one feature: PerformanceRatingSalaryHik","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_perf_rait_sal = df_cor_years.copy()\n\ndf_cor_years['PerformanceRatingSalaryHik']=df_cor_years.apply(lambda x: x['PerformanceRating']+x['PercentSalaryHike'], axis=1)\n\ndf_cor_years= df_cor_years.drop(columns=['PerformanceRating','PercentSalaryHike'])\n#df_cor_years\n#df_perf_rait_sal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Education Field vs Department \ndf_DepartnentVSEducationField = df_scaled[['JobRole_Healthcare Representative', \n'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', \n'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', \n'JobRole_Sales Executive', 'JobRole_Sales Representative','Department_Human Resources', \n'Department_Research & Development', 'Department_Sales','EducationField_Human Resources', \n'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', \n'EducationField_Other', 'EducationField_Technical Degree']].copy()\nplt.figure(figsize=(15,10))\nplt.title(\"Correlation Education Field VS Departmentvs VS Job Role \",fontsize=20, y=1.02,color='steelblue')\ncor = df_DepartnentVSEducationField.corr()\nsns.heatmap(cor,annot=True, cmap=sns.color_palette(\"Blues\"))      \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following features are highly correlated: \n* 'Department_Human Resources' and 'JobRole_Human Resources' and will be combined in one feature: Department_JobRole_Human Resources\n* 'Department_Sales' and 'JobRole_Sales Executive' and will be combined in one feature: Department_JobRole_Sales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Department_JobRole_Human Resources\n\ndf_dep_role_hum = df_perf_rait_sal.copy()\n\ndf_dep_role_hum['Department_JobRole_Human Resources']=df_cor_years.apply(lambda x: x['Department_Human Resources']+x['JobRole_Human Resources'], axis=1)\n\ndf_dep_role_hum = df_dep_role_hum.drop(columns=['Department_Human Resources','JobRole_Human Resources'])\n#df_dep_role_hum\n#df_perf_rait_sal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Department_JobRole_Sales\n\ndf_dep_role_sal = df_dep_role_hum.copy()\n\ndf_dep_role_sal['Department_JobRole_Sales']=df_dep_role_sal.apply(lambda x: x['Department_Sales']+x['JobRole_Sales Executive'], axis=1)\n\ndf_dep_role_sal = df_dep_role_sal.drop(columns=['Department_Sales','JobRole_Sales Executive'])\n#df_dep_role_hum\n#df_dep_role_sal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(df_dep_role_sal))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#'Age' VS Job Role VS 'YearsAtCompany'vs JobLevel\n\ndf_Position_Years = df_dep_role_sal[['Department_JobRole_Human Resources','Department_JobRole_Sales',  'Age','JobLevel',\n'JobRole_Healthcare Representative', 'YersCompanyManagerRole',\n'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', \n'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Representative',\n'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsSinceLastPromotion', 'MonthlyIncome']].copy()\n\nplt.figure(figsize=(12,7))\nplt.title(\"Position VS Years\",fontsize=20, y=1.02,color='steelblue')\ncor = df_Position_Years.corr()\nsns.heatmap(cor,annot=True, cmap=sns.color_palette(\"Blues\"))      \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following features are highly correlated :\n\n'TotalWorkingYears' and 'JobLevel' and will be combined in one feature: TotalWorkingYears_JobLevel Resources\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TotalWorkingYears_JobLevel_MonthlyIncome \n\ndf_TotalWorkingYears_JobLevel_MonthlyIncome = df_dep_role_sal.copy()\n\ndf_TotalWorkingYears_JobLevel_MonthlyIncome['TotalWorkingYears_JobLevel_MonthlyIncome']=df_TotalWorkingYears_JobLevel_MonthlyIncome.apply(lambda x: x['TotalWorkingYears']+x['JobLevel']+x['MonthlyIncome'], axis=1)\n\ndf_TotalWorkingYears_JobLevel_MonthlyIncome = df_TotalWorkingYears_JobLevel_MonthlyIncome.drop(columns=['TotalWorkingYears','JobLevel','MonthlyIncome'])\n#df_TotalWorkingYears_JobLevel_MonthlyIncome\n#df_dep_role_sal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Not Relevant Features \n\nFollowing features are NOT relevant for our model and will be dropped as well:  \n* 'EmployeeCount', 'Over18', 'StandardHours': each features has 1 unique variable. \n* 'EmployeeNumber' and 'EmployeeCount' are not relevant","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m = (df['StandardHours'] == 80).sum()\nn = (df['Over18'] == 'Y').sum()\nprint(m,n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Droping all before defined Not relevant features for our model\ndf_clean= df_TotalWorkingYears_JobLevel_MonthlyIncome.drop(columns=['Attrition', \n'EmployeeCount', 'Over18', 'StandardHours', 'EmployeeNumber'])\ndf_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Label\n\n**Attrition**: Is our target label. To be able to train our model we have to separate it from the rest of the features by creating a new dataset and transform it to one-hot encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We created a separate dataframe for our label\ny = df['Attrition']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation Summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features:\", len(list(df_clean)))\nprint(list(df_clean))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"During the data preparation phase, we defined a set of variable that are statistically insignificant, following the chi-square test. However, as we mentioned before we believe that all 47 features are relevant for our model. Therefore, we decided to proceed and compare the results of two datasets - one with all variables vs. one data set with only statistically significant features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(len(stat_significant),len(stat_insignificant))       \n#print(\"statistically significant values:\")\n#print(stat_significant)\n#print(\"statistically insignificant values:\")\n#print(stat_insignificant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all=df_clean.copy()\n#df_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat_sig_only=df_clean.copy()\ndf_stat_sig_only=df_stat_sig_only.drop(columns=['Department_JobRole_Human Resources','JobRole_Research Scientist',\n'Education_1', 'Education_2','Education_3','Education_4', 'Education_5',\n'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Medical','EducationField_Other',\n'BusinessTravel_Travel_Rarely', 'Gender'])\ndf_stat_sig_only","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 4. Algorithm Description </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To achieve our goal and define who will be the next employee who would leave the company, we decided to develop and compare two models, using two different algorithms:\n* K-Nearest Neighbors (KNN)\n* Random Forest (RF)\n\nBefore beginning with the data preparation, we first have to define what algorithm we will use to develop our models.\n \n### K-Nearest Neighbors Algorithm\n \n#### What is KNN?\nK-Nearest Neighbor is a machine learning algorithm, which can be used for classification and regression problems. It is a non-paramedic classification method. For the classification of data, the nearest neighbors are retrieved (K), which form a neighborhood. The classification for the data is based on a majority voting among the data in the neighborhood (Guo et al., 2003; Hastie et al., 2017, p. 463-468). So basically, the KNN algorithm assumes that similar values are all located in close proximity. The algorithm first usually measures the distance between two data points (a test data point and every training data point). Then the distances are sorted in ascending order and the nearest distance is K nearest neighbor (Harrison, 2018).\n \n#### Advantages and Disadvantages\nThere are several advantages of KNN, such as (“Pros and Cons of K-Nearest Neighbors”, 2018):\n* It is an easy to use und easy to understand algorithm\n* It can not only be implemented for binary problems,like ours, but also works with multi-class problems\n* It has no assumptions as it is a non-parametric algorithm.\n* There is no training step, as the algorithm does not build a model, but rather tags new data based on learning from past data.\n\nHowever, there are also some disadvantages of KNN, such as (Guo et al., 2003; “Pros and Cons of K-Nearest Neighbors”, 2018):\n* It has low efficiency once the number of data increases\n* It works very well with a small amount of variables, but once the amount of variables increases the algorithm struggles with predicting the output\n* The results and the success of the classification depend on a good value for K \n* The algorithm only works well with balanced data, but struggles with imbalanced data\n* It might be difficult to define the optimal number of neighbors\n* In case of imbalanced data, which is especially relevant in our case: KNN doesn’t perform well on imbalanced data. If we consider two classes, Attrition: Yes and No , and the majority of the training data is labeled as NO, then the model will ultimately give a lot of preference to NO. This might result in getting the less common class Yes wrongly classified. Therefore, to compare the result we will use also Random Forest.[will be described further]\n\n#### What has to be done?\n* Scaling needs to be done in order to normalize data \n* Data needs to be transformed into binary data\n\n\n \n### Random Forest Algorithm\n\n#### What is Random Forest?\nRandom Forest is a supervised learning algorithm, which is based on an ensemble technique. Ensemble technique means that the algorithm relies on the combination of different classifiers, as well as on an aggregation technique, like majority voting (Igual and Seguí, 2017, p. 90). Random Forest works based on the bagging method and CART, which stands for Classification and Regression Trees. Bagging means bootstrap aggregating and is an ensemble of classifications that are based on random sampling with replacement (Berk, 2017, p. 206-210). It builds several decision trees and combines them together, which gives better results, as the predictions are more accurate and stable. Random Forest can be used for both classification and regression tasks, which makes it diverse, but also is easy to use. This makes it one of the most used machine learning algorithms (Calica, 2019).\n \n#### Advantages and Disadvantages \nIn our case we are trying to solve a classification problem. We chose this algorithm for our second model because Random Forest has several advantages, such as (Calica, 2019): \n* It is a flexible and easy to use machine learning algorithm\n* It usually produces good prediction results\n* As it is a ensemble technique and uses bagging, it has good properties for combating overfitting (Igual and Seguí, 2017, p. 90)\n \nNevertheless, there are also a few disadvantages of Random Forest (Calica, 2019):\n* If there is a large number of trees, the algorithm might be too slow and thus making it inefficient for real-times predictions. However, to have accurate predictions, quite a large number of trees is needed \n* As it is tool for predictive modeling, it does not describe the relationship between different data.\n\n#### What has to be done?\nRole of Scaling is mostly important in algorithms that are distance based and require Euclidean Distance e.g. KNN. Random Forest is a tree-based model and hence does not require feature scaling.\nThis algorithm requires partitioning, even if you apply Normalization then also> the result would be the same.\n \nAccording to the algorithm pre-requirements, all relevant data transformation steps were implemented in part 3.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 5. K-NN for Dataset with All Features (Statistically Significant and Insignificant)</span>\n### df_all dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 5.1 Modeling  </span>\n### df_all dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are going to perform k-NN classifier using scikit-learn.\nk-NN is a type of instance-based learning, or lazy learning, as there are no parameters to learn. The algorithm classifies based on the distance to other data. Normalizing the training data can improve its accuracy dramatically. Therefore, data preparation in section 3 was implemented very carefully (Harrison, 2019, p. 116).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score \nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nimport scikitplot as skplt\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before beginning with training the model, we split our datasets\ndf_all\ndf_stat_sig_only \ninto train and test sets.\n \nTo spit our datasets we use the Train Test Split approach. In this approach we randomly split the complete data into training and test sets. Then we perform the model training on the training set and use the test set for validation purpose. We split our datasets into 70:30, 70 being the training data and 30 being the test data. With this approach there is the possibility of high bias when having limited data, because then one might miss some information about the data that was not used for training. As mentioned before in section 1, our dataset is not very big and imbalanced. Therefore, we used shuffle parameter to mix out data. Additionally we use the stratify approach, which aims to split a data set so that each split is similar with respect to something. This is done so that the train and test sets have similar percentages of the samples of the target class as the complete dataset (Igual and Seguí, 2017, pp. 82-86; “Train Test Split”, n.d.)\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#add pick ![image.png](attachment:image.png)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_train, df_all_test, y_train, y_test = train_test_split(df_all, y, test_size=0.3, random_state=88, \n            stratify=y, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of df_all_train:\", df_all_train.shape)\nprint(\"Shape of df_all_test:\", df_all_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat_sig_only_train, df_stat_sig_only_test, y_train, y_test = train_test_split(df_stat_sig_only,y, test_size=0.3, random_state=88, stratify=y, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of df_stat_sig_only_train:\", df_stat_sig_only_train.shape)\nprint(\"Shape of df_stat_sig_only_test:\", df_stat_sig_only_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-Validation\nCross-Validation is used to estimate the skill of a machine learning model.\nIt is often used to compare and select a model for a given predictive modeling problem as it is easy to understand and to implement (Browniee, 2018b).\n\nThere are different types of Cross-Validation such as k-Fold Cross-Validation or Stratified k-Fold Cross Validation.\n\nKNN algorithm has only one hyperparameter, namely number of neighbours. Therefore at first we will do a cross validation for different numbers of neighbours. This will let us define the optimal number of neighbors [k=1:10]. To make sure, our classifier generalizes to unseen data, we will use 5-fold cross-validation on the train data and pick the model with highest average performance. All our scores we will be saved in the beforeahand created list:scores.\n\nKnowing that our dataset is imbalanced (see section 1. Visualization), we assumed that accuracy metric is not the best metric in our case and could be misleading. Therefore, we decided to use F1-weighted score metric for our k parameter estimation, as it is a more reliable metric for our dataset (Browniee, 2020).\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Accuracy \nAccuracy shows the correct classification in percentage. It can be calculated as following:  \n\n$Accuracy =\\frac{(tp + tn)}{(tp + tn + fp + fn)}$\n\nTo know if the accurancy is good enough for the model, one might need to check the other metrics, such as sensitivity (Harrison, 2019, p. 164).  \nWhen having a binary or multiclass classification, the function is equal to the jaccard_score function (\"Accuracy classification score\", n.d.). The Jaccard Similarity Index shows the similarity and the diversity of two sets of data (\"Jaccard Index / Similarity Coefficient\", 2016).\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=[] \nn_neighbors_range = np.arange(1,10)\n\nfor k in n_neighbors_range:\n        knn = KNeighborsClassifier(n_neighbors = k)\n        score = cross_val_score(knn,df_all_train,y_train, cv = 5)\n        scores.append(score.mean())\nprint(scores)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\n\n#Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 10)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(df_all_train, y_train)\n\n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(df_all_train, y_train )\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] =knn.score(df_all_test, y_test)\n\n    \nplt.title('k-NN: Varying Number of Neighbors',fontsize=15, color='steelblue', y=1.02)\nplt.plot(neighbors,test_accuracy, label = 'Testing Accuracy', color=\"steelblue\")\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy', color=\"skyblue\")\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n\n#2\n\nplt.figure()\nplt.xlabel('k-neighbours')\nplt.ylabel('accuracy')\nplt.scatter(n_neighbors_range, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel('k-neighbours')\nplt.ylabel('accuracy')\nplt.scatter(n_neighbors_range, scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it was mentioned before our dataset is imbalanced. Because of the dispraity between the number of positive and negative labels, accuracy does not represent the reality. Therefore, accuracy is not the best measure to use when having imbalanced data. In cases like this, precision and recall are better options (Harrison, 2019, p. 99).\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### F1-weighted score\nThe F1 score calculates the harmonic mean of recall and precision. The formula for F1 is as following (Harrison, 2019, p. 165):  \n$F1 = \\frac{(2 * precision * recall)}{(precision + recall)}$\nThe best score is the value 1, the worst possible score a 0 (\"F1 score\", n.d.).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_weighted=[] \n\nn_neighbors_range = np.arange(1,10)\n\nfor k in n_neighbors_range:\n        knn = KNeighborsClassifier(n_neighbors = k)\n        score = cross_val_score(knn,df_all_train,y_train, scoring=\"f1_weighted\", cv = 5)\n        f1_weighted.append(score.mean())\nprint(f1_weighted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel('k-neighbours')\nplt.ylabel('F1 weighted')\nplt.scatter(n_neighbors_range, f1_weighted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search \nGrid search is a „cross-validation using an exhaustive search over all combinations of parameters provided” (Igual and Seguí, 2017, p. 91). It searches for the optimal hyperparameter of a model, which will give the best values. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors_range = np.arange(2,10)\np_range = np.arange(1,4) #p=1->Manhattan distance.\nbest_n_neighbors= 0    \nbest_p_range = 0    \n#cv_results=0\n\nparameters= {'p': p_range,'n_neighbors': n_neighbors_range}\n    \nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn, parameters, cv=5, scoring=\"f1_weighted\")\ngrid.fit(df_all_train, y_train)\n    \nbest_n_neighbors = grid.best_params_[\"n_neighbors\"]\nbest_p_range = grid.best_params_[\"p\"]\nF1_weighted_score = grid.best_score_ #based on training dataset\n#cv_results = grid.cv_results_ #cross-validation results using different parameters  \n    \nprint(\"Grid search results: \")\nprint(\"k:best n neighbors:\",best_n_neighbors,\";\", \"best p range\",best_p_range,\";\", \"F1-weighted score of training data\", F1_weighted_score)\n#print(\"cvresults:\" , cv_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nAs with our first dataset d_all, for our second data set df_df_stat_sig_only_train, we utilized two methods to find the optimal hyperparameter for our algorithm, namely: pure cross-validation and a combination of cross-validation and grid search. \nBased on our results, we found out that parameters k=7, p=1 will be used to train our model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Instantiate KNN classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_optimal = KNeighborsClassifier(n_neighbors=best_n_neighbors, p=best_p_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training of the KNN classifier\nTrain a KNN classifier for the existing training data y_train and df_all_train.\nThe fit method trains the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_optimal.fit(df_all_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply trained classifier to test data\nThe predict method applies the trained model to new data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn_optimal.predict(df_all_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 weighted score for KNN predicted: \", f1_score(y_test,y_pred, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 5.2 Model Evaluation  </span>\n### df_all dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To evaluate our kNN model we computed the confusion matrix and the F1-weighted score. We also considered the training errors (that refer to optimization problem) and testing error (that refer to generalization problem) (Goodfellow et al., 2016, pp. 108-114).\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Training Error","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix\n\nA confusion matrix shows us the correct classification, the false positive and the false negative classifications. One might want to optimize the results towards a false positive or a false negative result, depending on the context (Harrison, 2019, p. 35).\n\nIn our case, false postive classifications are worse than false negative. False positive classifications mean that the model recognizes certain employees to stay at the company, while in reality they are most likely to leave IBM. As the model will falsely classify them as employees who are staying, the HR department will not take any actions and give them any incentives to stay. This makes it even more likely that these employees will leave. On the other hand, the false negative classifications are not so bad. The HR department will think that these employees are likely to leave the company and therefore will try to retain them. Giving them incentives to stay at IBM might cost the company some money, but beyond that has no negative impact on the company. More importantly, it might even increase the loyalty of those employees.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train= knn_optimal.predict(df_all_train) #to count training error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=y_train.copy()\n\ncmtx = pd.DataFrame(\n    confusion_matrix(y_true, y_pred_train, labels=[0, 1]), \n    index=['reality: NO Attrition', 'reality: Attrition'], \n    columns=['predicted:NO Attrition', 'predicred:Attrition'])\n             \nprint(cmtx)\ncf = confusion_matrix(y_train,y_pred_train )\n\n#plot\nax= plt.subplot()\nsns.heatmap(cf, annot=True, ax = ax,cmap=plt.cm.Blues, fmt=\"d\", vmin=0, vmax=180,); #annot=True to annotate cells\n\n# labels, title and ticks\n\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix',fontsize=15, color='steelblue', y=1.04); \nax.xaxis.set_ticklabels(['NOT Attrition', 'Attrition']); \nax.yaxis.set_ticklabels(['NOT Attrition', 'Attrition']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1-weighted score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 weighted score for KNN on train data:\", f1_score(y_train,y_pred_train, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total number of test samples equals 30% of the total number of samples (1470): 441\n\nBased on the confusion matrix results, we estemated that 16% of our data was perdicted incorrectly, which is 1% more than results were for df_all_train dataset.\n\n**Note.** The reason why our right hand side part of our confusion matrix is in the same color, is because the indicators true negative and false negative are both less than 50.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Testing Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=y_test.copy()\n\ncmtx = pd.DataFrame(\n    confusion_matrix(y_true, y_pred, labels=[0, 1]), \n    index=['reality: NO Attrition', 'reality: Attrition'], \n    columns=['predicted:NO Attrition', 'predicred:Attrition']\n)\n\nprint(cmtx)\ncf = confusion_matrix(y_test,y_pred )\n\n\n#plot\nax= plt.subplot()\nsns.heatmap(cf, annot=True, ax = ax,cmap=plt.cm.Blues, fmt=\"d\", vmin=0, vmax=90,); #annot=True to annotate cells\n\n# labels, title and ticks\n\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix',fontsize=15, color='steelblue', y=1.04); \nax.xaxis.set_ticklabels(['NOT Attrition', 'Attrition']); \nax.yaxis.set_ticklabels(['NOT Attrition', 'Attrition']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### F1-weighted score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 weighted score for KNN: \", f1_score(y_test,y_pred, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Count of expected vs predicted: Not Attrition vs Attrition\")\n\ndf=pd.DataFrame()\ndf['expected: Not Attrition vs Attrition']=y_test\ndf['predicted: Not Attrition vs Attrition']= y_pred\n\nfig, ax =plt.subplots(1,2)\n\n\nsns.countplot(df['expected: Not Attrition vs Attrition'], ax=ax[0], palette=\"Blues\")\ns = df['expected: Not Attrition vs Attrition'].value_counts()\nfor i, v in s.reset_index().iterrows():\n    ax[0].text(i, v.loc['expected: Not Attrition vs Attrition'] + 0.2 , v.loc['expected: Not Attrition vs Attrition'], color='black')\n    \nsns.countplot(df['predicted: Not Attrition vs Attrition'], ax=ax[1], palette=\"Blues\")\ns = df['predicted: Not Attrition vs Attrition'].value_counts()\nfor i, v in s.reset_index().iterrows():\n    ax[1].text(i, v.loc['predicted: Not Attrition vs Attrition'] + 0.2 , v.loc['predicted: Not Attrition vs Attrition'] , color='black')\n    \n    \nfig.suptitle(\"Count of expected vs predicted :Not Attrition vs Attrition\",fontsize=20, color='steelblue', y=1.02)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total number of test samples equal to 30% of total number of samples (1470): 441\n \nBased on the confusion matrix results, we estemated that 15.6% of our data was perdicted incorrectly. In comparison to train dataset with 13.7% incorrectly perdicted labels. \n\n**Note.** The reson our right hand side part of our confusion matrix is in same color, ist that both of the indicators: True negative and False negative are less than 50.\n\n  \n    \n      \nAs with machine learning algorithms training sets are first tested and then their training error is reduced, and only afterwards the test set is used, the expected test error is usually greater than or at least equal to the expected training error.\nHow well the algorithm for the machine learning works, dependents on the ability of making the training error small, as well as decreasing the gap between the training and test error. As mentioned before, this is also closely linked to over- and underfitting.  \nIn our case, with our imbalanced dataset, we think that our training error is quite low (only 10% incorrectly predicted labels).   While the test error is higher than the training error, the difference is not significant. Therefore, we can assume that our model was trained properly (Goodfellow et al., 2016, pp. 108-114).\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Metrics computed from a confusion matrix\n\n#### Sensitivity & Specificity\n \nSensitivity, also known as Recall, shows how often the test correctly generates a positive result. It is the true positive rate in a confusion matrix. A test that’s highly sensitive will flag almost everyone who might not leave as true positive and will not generate many false negative results. However, in our case we care more about the employees that might quit, to proceed with actions and try to convince them to stay at the company. Therefore, the true negative rate is really important for us. The true negative rate, also known as  **specificity**  shows all employees that are actually likely to leave the company (Igual and Seguí, 2017, pp. 72-73; Ragan, 2018).\n \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(cf)\nTP = cf[1, 1]\nTN = cf[0, 0]\nFP = cf[0, 1]\nFN = cf[1, 0]\n\nspecificity = TN / float(TN + FP)# we use float to perform true division, not integer division\nprint(\"Specificity=\", specificity)\n\n#Specificity: When the actual value is negative, how often is the prediction correct?\n#Something we want to maximize\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\nprint(\"Classification error=\", classification_error)\n\n#False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\nprint(\"False Positive Rate=\",1 - specificity)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC-AUC Curve\n\nThe ROC is the probability curve, the AUC shows the degree of separability. \nA ROC curve, the receiver operating characteristic curve, shows the performance of a classification model at all thresholds. “[It] illustrates how the classifier performs by tracking the true positive rate (recall/sensitivity) as the false positive rate (inverted specificity) changes” (Harrison, 2019, p. 166). It is able to distinguish between classes.\n \nThe higher the AUC is, the higher the probability that it will correctly classify the employees who will leave and those who will not. A very good AUC will have a value close to 1, a poor model will have a value close to 0. For example, a value of 0.7 means that the model will correctly distinguish between positive and negative in 70% of the cases (Narkhede, 2018; Igual and Seguí, 2017, pp. 166-168).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores = knn_optimal.predict_proba(df_all_test)\nfpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\n#print(y_scores)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc, color=\"steelblue\")\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--',color=\"skyblue\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC - AUC Curve of kNN',fontsize=15, color='steelblue', y=1.02)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Report\n\nPrecision and Recall are two important model evaluation metrics. \nPrecision, also known as Positive Predictive Value, refers to the percentage of the positive results which was correctly classified. It has the following formula:\nPrecision = True Positive / (True Positive + False Positive)\nRecall, also known as Sensitivity,  refers to the percentage of actual positive results correctly classified by your algorithm. It has the following formula:  \n**Recall = True Positive / (True Positive + False Negative)**  \n(Igual and Seguí, 2017, p. 73)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 6. K-NN for dataset with only statistically significant features </span> \n### df_stat_sig_only dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 6.1. Modeling </span> \n### df_stat_sig_only_train dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Cross-Validation\n### F1-score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_weighted_2=[] \n\nn_neighbors_range = np.arange(2,10)\n\nfor k in n_neighbors_range:\n        knn = KNeighborsClassifier(n_neighbors = k)\n        score = cross_val_score(knn,df_stat_sig_only_train, y_train, scoring=\"f1_weighted\", cv = 5)\n        f1_weighted_2.append(score.mean())\nprint(f1_weighted_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel('k-neighbours')\nplt.ylabel('F1 weighted')\nplt.scatter(n_neighbors_range, f1_weighted_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid search ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors_range = np.arange(2,10)\np_range = np.arange(1,4) \nbest_n_neighbors= 0    \nbest_p_range = 0    \n#cv_results=0\n\nparameters= {'p': p_range,'n_neighbors': n_neighbors_range}\n    \nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn, parameters, cv=5, scoring=\"f1_weighted\")\ngrid.fit(df_stat_sig_only_train, y_train)\n    \nbest_n_neighbors_2 = grid.best_params_[\"n_neighbors\"]\nbest_p_range_2 = grid.best_params_[\"p\"]\nF1_weighted_score_2 = grid.best_score_ #based on training dataset\n#cv_results = grid.cv_results_ #cross-validation results using different parameters  \n    \nprint(\"Grid search results: \")\nprint(\"k:best n neighbors:\",best_n_neighbors_2,\";\", \"best p range\",best_p_range_2,\";\", \"F1-weighted score of training data\", F1_weighted_score_2)\n#print(\"cvresults:\" , cv_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nAs with our first dataset d_all, for our second data set df_df_stat_sig_only_train, we utilized two methods to find the optimal hyperparameter for our algorithm, namely: pure cross-validation and a combination of cross-validation and grid search.\nBased on our results, we found out that in this case both metrics provide the same result. Consequently, the parameters k=7, p=1 will be used to train our model.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Instantiate KNN classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_optimal_2 = KNeighborsClassifier(n_neighbors=best_n_neighbors_2, p=best_p_range_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training of the KNN classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_optimal_2.fit(df_stat_sig_only_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply trained classifier to test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_2 = knn_optimal_2.predict(df_stat_sig_only_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 weighted score for KNN: \", f1_score(y_test,y_pred_2, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 6.2 Model Evaluation </span>\n### df_stat_sig_only_train dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Training Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train_2= knn_optimal_2.predict(df_stat_sig_only_train )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=y_train.copy()\n\ncmtx = pd.DataFrame(\n    confusion_matrix(y_true, y_pred_train_2, labels=[0, 1]), \n    index=['reality: NO Attrition', 'reality: Attrition'], \n    columns=['predicted:NO Attrition', 'predicred:Attrition'])\n                         \nprint(cmtx)\ncf = confusion_matrix(y_train,y_pred_train_2 )\n\n#plot\nax= plt.subplot()\nsns.heatmap(cf, annot=True, ax = ax,cmap=plt.cm.Blues, fmt=\"d\", vmin=0, vmax=170,); #annot=True to annotate cells\n\n# labels, title and ticks\n\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix',fontsize=15, color='steelblue', y=1.04); \nax.xaxis.set_ticklabels(['NOT Attrition', 'Attrition']); \nax.yaxis.set_ticklabels(['NOT Attrition', 'Attrition']);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 weighted score for KNN: \", f1_score(y_train, y_pred_train_2, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Testing Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=y_test.copy()\n\ncmtx = pd.DataFrame(\n    confusion_matrix(y_true, y_pred_2, labels=[0, 1]), \n    index=['reality: NO Attrition', 'reality: Attrition'], \n    columns=['predicted:NO Attrition', 'predicred:Attrition']\n)\n\nprint(cmtx)\ncf = confusion_matrix(y_test,y_pred_2 )\n\n#plot\nax= plt.subplot()\nsns.heatmap(cf, annot=True, ax = ax,cmap=plt.cm.Blues, fmt=\"d\", vmin=0, vmax=90,); #annot=True to annotate cells\n\n# labels, title and ticks\n\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix',fontsize=15, color='steelblue', y=1.04); \nax.xaxis.set_ticklabels(['NOT Attrition', 'Attrition']); \nax.yaxis.set_ticklabels(['NOT Attrition', 'Attrition']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total number of test samples equals 30% of the total number of samples (1470): 441\n\n\n**Note.** The reason why our right hand side part of our confusion matrix is in the same color, is because the indicators true negative and false negative are both less than 50.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Count of expected vs predicted: Not Attrition vs Attrition\")\n\ndf=pd.DataFrame()\ndf['expected: Not Attrition vs Attrition']=y_test\ndf['predicted: Not Attrition vs Attrition']= y_pred_2\n\nfig, ax =plt.subplots(1,2)\n\n\nsns.countplot(df['expected: Not Attrition vs Attrition'], ax=ax[0], palette=\"Blues\")\ns = df['expected: Not Attrition vs Attrition'].value_counts()\nfor i, v in s.reset_index().iterrows():\n    ax[0].text(i, v.loc['expected: Not Attrition vs Attrition'] + 0.2 , v.loc['expected: Not Attrition vs Attrition'], color='black')\n    \nsns.countplot(df['predicted: Not Attrition vs Attrition'], ax=ax[1], palette=\"Blues\")\ns = df['predicted: Not Attrition vs Attrition'].value_counts()\nfor i, v in s.reset_index().iterrows():\n    ax[1].text(i, v.loc['predicted: Not Attrition vs Attrition'] + 0.2 , v.loc['predicted: Not Attrition vs Attrition'] , color='black')\n    \n    \nfig.suptitle(\"Count of expected vs predicted :Not Attrition vs Attrition\",fontsize=20, color='steelblue', y=1.02)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metrics computed from a confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(cf)\nTP = cf[1, 1]\nTN = cf[0, 0]\nFP = cf[0, 1]\nFN = cf[1, 0]\n\nspecificity = TN / float(TN + FP)# we use float to perform true division, not integer division\nprint(\"Specificity=\", specificity)\n\n#Specificity: When the actual value is negative, how often is the prediction correct?\n#Something we want to maximize\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\nprint(\"Classification error=\", classification_error)\n\n#False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\nprint(\"False Positive Rate=\",1 - specificity)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC-AUC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores = knn_optimal_2.predict_proba(df_stat_sig_only_test)\nfpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\n#print(y_scores)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc, color=\"steelblue\")\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--',color=\"skyblue\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC - AUC Curve of kNN',fontsize=15, color='steelblue', y=1.02)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To compare our 2 datasets we use classification reports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_best_train=df_stat_sig_only_train.copy()\ndf_best_test=df_stat_sig_only_test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 7. Random Forest (RF)</span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 7.1 Modeling</span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"By proceeding with the kNN algorithm for two datasets: d_all and df_df_stat_sig_only_train we utilized two methods to find the optimal hyperparameter for our algorithm, namely: pure cross-validation and a combination of cross-validation and Gridsearch.  Through comparative analysis of the two parameters we found out that we received a better F1 score using a parameter combination of cross-validation and Gridsearch. Therefore, for our comparative algorithm Random Forest we will only apply a parameter combination of cross-validation and grid search. \n  \nAdditionally, comparing the kNN algorithm prediction for our two data sets df_all and df_df_stat_sig_only_train, we learned that with using df_all dataset we received better results. Therefore, for Random Forest algorithm only df_all dataset will be used.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Grid search ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier(random_state=88)\n\nparameters = {\n    \"n_estimators\": [8,18,28, 38, 58, 88,108],\n    \"max_features\": ['auto', 'sqrt', 'log2'],# we take log 47 from amount of features https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n    \"criterion\": [\"gini\", \"entropy\"],#criteria that define how good features were defined\n    \"max_depth\" : [2,5,10,15,20, None]# None: till it reaches min_samples_split\n}\n    \ngr_srch_rfc = GridSearchCV(rfc, param_grid=parameters, cv=5, scoring='roc_auc')#scoring='roc_auc' --> the best. acc=0.8503401360544217\n#scoring=\"f1_weighted\" # we tried for f1_weigted score and received lower score in comparison to accuracy, which is stained by default \n\ngr_srch_rfc.fit(df_best_train, y_train)\n\ncrit_param = gr_srch_rfc.best_params_[\"criterion\"]#looking for optimal parameter,we \n#create variables e.g.crit_param that automatically will be sent to the RF classificator\n\ndepth_max = gr_srch_rfc.best_params_[\"max_depth\"]\nn_est = gr_srch_rfc.best_params_[\"n_estimators\"]\nn_max = gr_srch_rfc.best_params_[\"max_features\"]\ngr_srch_rfc.best_params_# to print out\n\n\n\nprint(\"Best parameters based on Gridsearch:\\n\",\n       gr_srch_rfc.best_params_,\"\\n Mean cross-validated score of the best_estimator\",gr_srch_rfc.best_score_ )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Training of the Random Forest Classifier\n\nWe used the out of bag (OOB) score, which is one way of validating a Random Forest model. \n\"The OOB score is computed as the number of correctly predicted rows from the out of bag sample.\" (Bhatia, 2019).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#Instantiate RF classifier\n\nrfc_optim = RandomForestClassifier(random_state=88, n_estimators=n_est, max_depth=depth_max, \ncriterion=crit_param,max_features=n_max, class_weight='balanced_subsample', oob_score=True)\n\n#Training \nrfc_optim.fit(df_best_train, y_train)\n\n#Predictions\ny_pred_rf= rfc_optim.predict(df_best_test)\n#print(y_pred_rf)\n\n#print(\"OOB for Random Forest: \", rfc_optim.oob_score_)\n\noob_score=rfc_optim.oob_score_\ny_for_auc = rfc_optim.predict_proba(df_best_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display classification tree","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 7.2 Model Evaluation</span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix for Training Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rf_train= rfc_optim.predict(df_best_train)\n\ny_true=y_train.copy()\n\ncmtx = pd.DataFrame(\n    confusion_matrix(y_true, y_pred_rf_train, labels=[0, 1]), \n    index=['reality: NO Attrition', 'reality: Attrition'], \n    columns=['predicted:NO Attrition', 'predicred:Attrition']\n)\n\nprint(cmtx)\nrfc_optim_train = confusion_matrix(y_train,y_pred_rf_train )\n\n#plot\nax= plt.subplot()\nsns.heatmap(rfc_optim_train, annot=True, ax = ax,cmap=plt.cm.Blues, fmt=\"d\", vmin=0, vmax=90,); \n\n# labels, title and ticks\n\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix',fontsize=20, color='steelblue', y=1.04); \nax.xaxis.set_ticklabels(['NOT Attrition', 'Attrition']); \nax.yaxis.set_ticklabels(['NOT Attrition', 'Attrition']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix for Test Error","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Metrics computed from a confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=y_test.copy()\n\ncmtx = pd.DataFrame(\n    confusion_matrix(y_true, y_pred_rf, labels=[0, 1]), \n    index=['reality: NO Attrition', 'reality: Attrition'], \n    columns=['predicted:NO Attrition', 'predicred:Attrition']\n)\n\nprint(cmtx)\nrfc_optim = confusion_matrix(y_test,y_pred_rf )\n\n#plot\nax= plt.subplot()\nsns.heatmap(rfc_optim, annot=True, ax = ax,cmap=plt.cm.Blues, fmt=\"d\", vmin=0, vmax=90,); #annot=True to annotate cells\n\n# labels, title and ticks\n\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix',fontsize=20, color='steelblue', y=1.04); \nax.xaxis.set_ticklabels(['NOT Attrition', 'Attrition']); \nax.yaxis.set_ticklabels(['NOT Attrition', 'Attrition']);\n\ncf=rfc_optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_rf))\n\n#print(\"Accuracy for Random Forest:\",accuracy_score(y_test,y_pred_rf))\n#print(\"F1 weighted for Random Forest: \", f1_score(y_test,y_pred_rf, average=\"weighted\"))\nprint(\"OOB for Random Forest: \", oob_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Count of expected vs predicted: Not Attrition vs Attrition\")\n\ndf=pd.DataFrame()\ndf['expected: Not Attrition vs Attrition']=y_test\ndf['predicted: Not Attrition vs Attrition']= y_pred_rf\n\nfig, ax =plt.subplots(1,2)\n\n\nsns.countplot(df['expected: Not Attrition vs Attrition'], ax=ax[0], palette=\"Blues\")\ns = df['expected: Not Attrition vs Attrition'].value_counts()\nfor i, v in s.reset_index().iterrows():\n    ax[0].text(i, v.loc['expected: Not Attrition vs Attrition'] + 0.2 , v.loc['expected: Not Attrition vs Attrition'], color='black')\n    \nsns.countplot(df['predicted: Not Attrition vs Attrition'], ax=ax[1], palette=\"Blues\")\ns = df['predicted: Not Attrition vs Attrition'].value_counts()\nfor i, v in s.reset_index().iterrows():\n    ax[1].text(i, v.loc['predicted: Not Attrition vs Attrition'] + 0.2 , v.loc['predicted: Not Attrition vs Attrition'] , color='black')\n    \n    \nfig.suptitle(\"Count of expected vs predicted :Not Attrition vs Attrition\",fontsize=20, color='steelblue', y=1.02)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTP = cf[1, 1]\nTN = cf[0, 0]\nFP = cf[0, 1]\nFN = cf[1, 0]\n\nspecificity = TN / float(TN + FP)# we use float to perform true division, not integer division\nprint(\"Specificity=\", specificity)\n\n#Specificity: When the actual value is negative, how often is the prediction correct?\n#Something we want to maximize\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\nprint(\"Classification error=\", classification_error)\n\n#False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\nprint(\"False Positive Rate=\",1 - specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC-AUC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_scores = rfc_optim.predict_proba(df_best_test)\nfpr, tpr, threshold = roc_curve(y_test, y_for_auc[:, 1])\nroc_auc = auc(fpr, tpr)\n\n#print(y_scores)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc, color=\"steelblue\")\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--',color=\"skyblue\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC - AUC Curve of kNN',fontsize=15, color='steelblue', y=1.02)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importance of the attributes \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nimportances = rfc_optim.feature_importances_\nindices = np.argsort(rfc_optim.feature_importances_)[::-1]\n\n#print(indices.shape)\n\nindices = indices[0:10]\n\n#print(indices.shape)\nfeature_list=[]\nfor string in df_best_test.columns[df_best_test.columns != \"Attrition\"][indices]:\n    feature_list.append(re.sub('([A-Z])', r' \\1', string))\n\nplt.figure(figsize=(12,10))\n\nplt.barh(range(1,11), importances[indices], align=\"center\",color=sns.color_palette(\"Blues\"))\n         \nplt.yticks(range(1,11), feature_list, rotation=0, fontsize=13)\nplt.ylabel('Feature', fontsize=14)\nplt.title(\"IBM Employee Attrition Feature Importance\",fontsize=23, color='steelblue', y=1.02 );\nplt.tight_layout()\nplt.gca().invert_yaxis()\n#plt.savefig('./graphs/important_features.png', dpi=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nimportances = rfc_optim.feature_importances_\nindices = np.argsort(rfc_optim.feature_importances_)[::-1]\n\n#print(indices.shape)\n\nindices = indices[0:34]\n\n#print(indices.shape)\nfeature_list=[]\nfor string in df_best_test.columns[df_best_test.columns != \"Attrition\"][indices]:\n    feature_list.append(re.sub('([A-Z])', r' \\1', string))\n\nplt.figure(figsize=(12,10))\n\nplt.barh(range(1,35), importances[indices], align=\"center\",color=sns.color_palette(\"Blues\"))\n         \nplt.yticks(range(1,35), feature_list, rotation=0, fontsize=13)\nplt.ylabel('Feature', fontsize=14)\nplt.title(\"IBM Employee Attrition Feature Importance\",fontsize=23, color='steelblue', y=1.02 );\nplt.tight_layout()\nplt.gca().invert_yaxis()\n#plt.savefig('./graphs/important_features.png', dpi=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 8. Conclusion </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Precision can be seen as a measure of exactness or quality, whereas recall is a measure of completeness or quantity. In simple terms, high precision means that an algorithm returns substantially more relevant results than irrelevant ones, while high recall means that an algorithm returns most of the relevant results (whether irrelevant ones are also returned).\n\nIn this project, we tried to predict which valuable employees will leave the company next, by analyzing IBM HR Analytics Employee Attrition & Performance dataset and applying Machine Learning algorithms. \nOur first step was data preparation. We analyzed all our features, the correlation between them and their significance. For the correlation part we proceeded with the help of correlation plots. By identifying highly correlated features, we merged them together in order to avoid any variable value loss. All insignificant features were dropped, and categorical as well as continuous features were normalized using a min-max scaling method. This step was especially relevant for one of our algorithms (KNN classifier) that was applied later. Because irrelevant as well as differently scaled features might have a negative impact on our model. Additionally, proceeding with the Chi-square test, we identified 15 statistically insignificant features. We considered that dropping these features might not have any significant impact on our model, or even affect it.  Therefore, we decided to create two datasets: one with only statistically significant features and the other one with all features from our initial dataset. The idea was to compare the predicted results of the models trained on these datasets in order to identify the better dataset. \n\nOur second step was splitting our model into training (70%) and test (30%) sets. We then defined the hyperparameters respectively to the algorithms we applied.\nFor our kNN classifier, optimal k (number of next neighbours) and p (distance between neighbors) parameter were defined using grid search. For both datasets, optimal hyperparameters were equal, which let us compare the predictive power of our models trained on these datasets using the aforementioned hyperparameters. For the evaluation of our models we took the F1-weighted score as the main estimating parameter, as in sklearn documentation this parameter was defined as suitable for the imbalanced dataset, and our dataset is strongly imbalanced (we have two classes of Attrition: “No”, “Yes”, 84% and 16% respectively). According to the results we received, the model trained on the dataset with only statistically significant features. The fitted model generalizes better and consequently results in higher prediction accuracy. Therefore, for Random Forest classifier dataset with statistically significant was taken. \n\nTo proceed with the model built utilizing Random Forest classifier we took similar steps to the aforementioned steps. We found optimal hyperparameters using grid search. (Random Forest has the following hyper parameters: max depth (the depth of our tree), N-estimators (number of trees), max features (number of features to consider to look for the best split), and criterion.) Afterwards, we fixed our model. The area under the curve score (AUC) was taken as a main estimating parameter for our model. Similarly to the F1-weighted parameter, according to sklearn documentation, AUC is more suitable for an imbalanced dataset, than the default (accuracy).  \n  \nFinally, we can only roughly estimate which model (kNN or Random Forest classifier) is better, as those were built utilizing different hyper parameters. Nevertheless we pulled the same evaluation metrics: AUC, F1-weighted score and calculated classification error. According to all aforementioned metrics, ***the Random Forest classifier model performed better.*** \nHowever, a lot of metrics we were able to pull, we are not able to use for our evaluation. For instance, some of them (recall, closely related to specificity for the label “Yes”) were performing very poor results, which we assumed were caused by the imbalanced dataset.\n\nUnfortunately, we were not able to solve this problem in our project. However, we consider a possible solution to this problem:\nTo balance the dataset by generating more sample with label Attrition “YES” pr use class weights to \nresample the training set. Over-sampling an under-sampling  (20 to 80 and 40 to 60) and compare results.\nTo try to change True Positive from  Attrition “NO” to “Yes”. As currently, True Negative is more relevant for our problem.\nImplement  ensemble different resampled datasets (creating smaller balanced dataset; train separate models and then chain them together) \nImplementa regression algorithm for the classification problem. Our idea is that we can overcome the problem, which allows us to utilize move the threshold down and more correctly predict employees that are likely to leave, which is more relevant for our case. Even though we will probably also increase the incorrectly predicted number of employees who are likely to stay (False Positive). However, in our opinion it is more relevant for the company to be aware of employees that are likely to leave to proceed with different steps of improvement: working conditions and satisfaction as well as investigate what factors cause employee attrition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> 9. Possible Business Use </span>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The tech. business landscape nowadays is highly competitive and the retention of employees can be difficult due to highly educated employees having the ability to be mobile in the job market. Attrition may be expensive to the company both in terms of money and time because of new employee recruiting and training costs. Moreover, the loss of experienced employees that are familiar with the business practices and have unwritten knowledge that isn't easily passed on to new employees can have a serious impact on productivity and the company’s profit. A machine learning model provides insights into patterns that can help understand the reasons and causes which drive employees to leave the company. By understanding what conditions contribute to attrition, the companies can work on improving them to better retain talent. \n\nUpon further inspection (this by simply plotting graphs), some of the most important features which greatly impact attrition are: working hours, job level, monthly income, and age. Companies like IBM need to better tailor retention programmes to specifically target these criteria for various employee groups.\n\nBeyond the scope of this study, our model can be applied in marketing for various purposes. One would be to potentially reduce customer churn by identifying the features driving it, in the same way as we did with employee attrition. Another would be the use of the ML models in direct marketing, specifically email marketing. By focusing on a target variable such as the click through rate, these models can help marketers optimize their email campaigns by targeting relevant customer segments, better understanding the optimal time of traffic, and managing the content of the emails. \n\n\nA survey from Forbes (Press, 2016) found that data scientists spend 80% of the time on data preparation. As Luca Massaron, data scientist and author, stated \"nothing influences the result more than the features you use\" (Rençberoğlu, 2019). That is why we believe feature engineering would be a good approach to predict IBM’s attrition for further analysis. We decided not to drop features, despite running tests that showed which categorical features are not statistically significant, to keep a complete picture. Instead, to deal with multicollinearity, we combined highly correlated features. Furthermore, no new features were derived. \no","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:steelblue; font-size:2em;\"> ||| References </span>\n\n“Accuracy classification score\" (n.d.). Online: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score (29.07.2020).\n\nAlao, D. and Adeyemo, A. B. (2013): Analyzing Employee Attrition Using Decision Tree Algorithms. Computing, Information Systems & Development Informatics, 4(1), 17-28.\n\nAlpaydin, E. (2020): Introduction to Machine Learning, fourth edition. Cambridge, MA: The MIT Press.\n\nBerk, R. A. (2017): Statistical Learning from a Regression Perspective, second edition. Cham, Switzerland: Springer.\n\nBhatia, N. (2019): What is Out of Bag (OOB) score in Random Forest? Online: https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710 (29.07.2020).\n\nBrowniee, J. (2017): How to Handle Missing Data with Python. Online: https://machinelearningmastery.com/handle-missing-data-python/ (26.07.2020).\n\nBrowniee, J. (2018a): A Gentle Introduction to the Chi-Squared Test for Machine Learning. Online: https://machinelearningmastery.com/chi-squared-test-for-machine-learning/ (26.07.2020).\n\nBrowniee, J. (2018b): A Gentle Introduction to k-fold Cross-Validation. Online: https://machinelearningmastery.com/k-fold-cross-validation/ (29.07.2020).\n\nBrowniee, J. (2019): A Gentle Introduction to Imbalanced Classification. Online: https://machinelearningmastery.com/what-is-imbalanced-classification/ (26.07.2020).\n\nBrowniee, J. (2020): Tour of Evaluation Metrics for Imbalanced Classification. Online: https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/ (29.07.2020).\n\nCalica, A. (2019): A complete guide to the random forest algorithm. Online: https://www.datacamp.com/community/news/a-complete-guide-to-the-random-forest-algorithm-n7lwwult5op (27.07.2020).\n\n\"F1 score\" (n.d.). Online: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html (29.07.2020).\n\nGoodfellow, I., Bengio, Y. and Courville, A. (2016): Deep Learning. Cambridge, MA: MIT Press.\n\nGuo, G., Wang, H., Bell, D., Bi, Y. and Greer, K. (2003): KNN Model-Based Approach in Classification. In: Meersman, R., Tari, Z. and Schmidt, D. C. (Eds): One The Move to Meaningful Internet Systems in 2003: CoopIS, DOA, and ODBASE. Heidelberg: Springer.\n\nHarrison, M. (2019): Machine Learning Pocket Reference. Sebastopol, CA: O’Reilly Media.\n\nHarrison, O. (2018): Machine Learning Basics with the K-Nearest Neighbors Algorithm. Online: https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761 (27.07.2020).\n\nHastie, T., Tibshirani, R. and Friedman, J. (2017): The Elements of Statistical Learning, second edition. New York: Springer.\n\nIgual, L. and Seguí, S. (2017): Introduction to Data Science. Cham, Switzerland: Springer International Publishing. \n\n“Interpret the key results for correlation” (n.d.). Online: https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/correlation/interpret-the-results/ (26.07.2020).\n \n\"Jaccard Index / Similarity Coefficient\" (2016). Online: https://www.statisticshowto.com/jaccard-index (29.07.2020).\n\nJoseph, R. (2018): Grid Search for model tuning. Online: https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e (29.07.2020).\n\nKoehrsen, W. (2018): Overfitting vs. Underfitting: A Complete Example. Online: https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765 (28.07.2020).\n\nKuhn, M. and Johnson, K. (2013): Applied Predictive Modeling. New York: Springer.\n\n“Matplotlib: Visualization with Python” (n.d.). Online: https://matplotlib.org (26.07.2020).\n\n“MinMax Scaling” (n.d.). Online: http://rasbt.github.io/mlxtend/user_guide/preprocessing/minmax_scaling/ (29.07.2020)\n\nMoffitt, C. (2018): Overview of Pandas Data Types. Online: https://pbpython.com/pandas_dtypes.html (26.07.2020).\n\nMohri, M., Rostamizadeh, A. and Talwalkar, A. (2018): Foundations of Machine Learning, second edition. Cambridge, MA: The MIT Press.\n\nNarkhede, S. (2018): Understanding AUC - ROC Curve. Online: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 (29.07.2020).\n\nPress, G. (2016): Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says. Online: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#1d353fbd6f63 (29.07.2020).\n\n“Pros and Cons of K-Nearest Neighbors” (2018). Online: https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/ (27.07.2020).\n\nRagan, A. (2018): Taking the Confusion Out of Confusion Matrices. Online: https://towardsdatascience.com/taking-the-confusion-out-of-confusion-matrices-c1ce054b3d3e (29.07.2020).\n\nRençberoğlu, E. (2019): Fundamental Techniques of Feature Engineering for Machine Learning. Online: https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114 (29.07.2020).\n\nSaranya, S. and Sharmila Devi, J. (2018): Predicting Employee Attrition Using Machine Learning Algorithms and Analyzing Reasons for Attrition. International Journal of Advanced Engineering Research and Technology, 6(9), 475-478.\n\nSathya, R. and Abraham, A. (2013): Comparison of Supervised and Unsupervised Learning Algorithms for Pattern Classification. International Journal of Advanced Research in Artificial Intelligence, 2(2), 34-38.\n\n“seaborn: statistical data visualization” (n.d.). Online: https://seaborn.pydata.org/introduction.html (26.07.2020).\n\nShetye, A. (2019): Feature Selection with sklearn and Pandas. Online: https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b (26.07.2020). \n\nSri Harsha, B., Jithendra Varaprasad, A. and Pavan Sai Sujith, L V. N. (2020): Early Prediction of Employee Attrition. International Journal of Scientific & Technology Research, 9(3), 3374-3379.\n\nStatista (2020): IBM number of employees worldwide from 2000 to 2019. Online: https://www.statista.com/statistics/265007/number-of-employees-at-ibm-since-2000/ (25.07.2020).\n\n“Statistics How To” (n.d.). Online: https://www.statisticshowto.com/probability-and-statistics/chi-square/ (26.07.2020).\n\n“Train Test Split” (n.d.). Online: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html (29.07.2020).\n\n“Using Chi-Square Statistic in Research” (n.d.). Online: https://www.statisticssolutions.com/using-chi-square-statistic-in-research/ (26.07.2020). \n\nWaseem, M. (2020): How To Implement Classification in Machine Learning? Online: https://www.edureka.co/blog/classification-in-machine-learning/#classification (24.07.2020).\n\nWillems, K. (2017): NumPy Cheat Sheet: Data Analysis in Python. Online: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet (25.07.2020).\n\nWillems, K. (2019): Pandas Tutorial: DataFrames in Python. Online: https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python (25.07.2020).\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}