{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<hr style=\"width:100%;height:3px;border-width:0;background-color:silver\">\n<h1 style=\"text-align:center\">   \n      <font color = MidnightBlue >\n                Fake News Detection with NLP and LSTM \n        </font>    \n</h1>   \n<hr style=\"width:100%;height:3px;border-width:0;background-color:silver\">\n<center><img style = \"height:550px;\" src=\"https://images.livemint.com/rf/Image-621x414/LiveMint/Period2/2018/05/05/Photos/Processed/fakereal-k2QC--621x414@LiveMint.jpg\"></center>\n<br>\n<h2><font color = MidnightBlue>What is \"Fake News\"?</font></h2>\n<p>“Fake news” is a term that has come to mean different things to different people. At its core, we are defining “fake news” as those news stories that are false: the story itself is fabricated, with no verifiable facts, sources or quotes. Sometimes these stories may be propaganda that is intentionally designed to mislead the reader, or may be designed as “clickbait” written for economic incentives (the writer profits on the number of people who click on the story). In recent years, fake news stories have proliferated via social media, in part because they are so easily and quickly shared online.</p>\n<h2><font color = MidnightBlue>About Dataset</font></h2>\n<p>This data set consists of 40000 fake and real news. Our goal is to train our model to accurately predict whether a particular piece of news is real or fake. Fake and real news data are given in two separate data sets, with each data set consisting of approximately 20000 articles.</p>\n<h2><font color = MidnightBlue>Content:</font></h2>\n<br>\n \n1. [Import Libraries](#1)\n1. [Load and Check Data](#2)\n1. [Visualization](#3)\n1. [Data Cleaning](#4)\n    * [Removal of HTML Contents](#5)\n    * [Removal of Punctuation Marks and Special Characters](#6)\n    * [Removal of Stopwords](#7)\n    * [Lemmatization](#8)\n    * [Perform it for all the examples](#9)  \n1. [N-Gram Analysis](#10)\n    * [Unigram Analysis](#11)\n    * [Bigram Analysis](#12)\n    * [Trigram Analysis](#13)\n1. [Modeling](#14)\n    * [Train - Test Split](#15)\n    * [Tokenizing](#16)\n    * [Training LSTM Model](#17)\n    * [Analysis After Training](#18) ","metadata":{}},{"cell_type":"markdown","source":"<a id = 1></a>\n<h1><font color = MidnightBlue>Import Libraries</font></h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nimport re\nimport string \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport keras\nfrom keras.preprocessing import text,sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 2></a>\n<h1><font color = MidnightBlue>Load and Check Data</font></h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","metadata":{}},{"cell_type":"code","source":"real_data = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake_data = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add column \nreal_data['target'] = 1\nfake_data['target'] = 0 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merging the 2 datasets\ndata = pd.concat([real_data, fake_data], ignore_index=True, sort=False)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 3></a>\n<h1><font color = MidnightBlue>Visualization</font></h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","metadata":{}},{"cell_type":"markdown","source":"**1.Count of Fake and Real Data**","metadata":{}},{"cell_type":"code","source":"print(data[\"target\"].value_counts())\nfig, ax = plt.subplots(1,2, figsize=(19, 5))\ng1 = sns.countplot(data.target,ax=ax[0],palette=\"pastel\");\ng1.set_title(\"Count of real and fake data\")\ng1.set_ylabel(\"Count\")\ng1.set_xlabel(\"Target\")\ng2 = plt.pie(data[\"target\"].value_counts().values,explode=[0,0],labels=data.target.value_counts().index, autopct='%1.1f%%',colors=['SkyBlue','PeachPuff'])\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.Distribution of The Subject According to Real and Fake Data**","metadata":{}},{"cell_type":"code","source":"print(data.subject.value_counts())\nplt.figure(figsize=(10, 5))\n\nax = sns.countplot(x=\"subject\",  hue='target', data=data, palette=\"pastel\")\nplt.title(\"Distribution of The Subject According to Real and Fake Data\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 4></a>\n<h1><font color = MidnightBlue>Data Cleaning</font></h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","metadata":{}},{"cell_type":"code","source":"data['text']= data['subject'] + \" \" + data['title'] + \" \" + data['text']\ndel data['title']\ndel data['subject']\ndel data['date']\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_text = data.text[10]\nfirst_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 5></a>\n<h2><font color = MidnightBlue>Removal of HTML Contents</font></h2>","metadata":{}},{"cell_type":"markdown","source":"**First, let's remove HTML content.**","metadata":{}},{"cell_type":"code","source":"pip install bs4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(first_text, \"html.parser\")\nfirst_text = soup.get_text()\nfirst_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 6></a>\n<h2><font color = MidnightBlue>Removal of Punctuation Marks and Special Characters</font></h2>","metadata":{}},{"cell_type":"markdown","source":"**Let's now remove everything except uppercase / lowercase letters using Regular Expressions.**","metadata":{}},{"cell_type":"code","source":"first_text = re.sub('\\[[^]]*\\]', ' ', first_text)\nfirst_text = re.sub('[^a-zA-Z]',' ',first_text)  # replaces non-alphabets with spaces\nfirst_text = first_text.lower() # Converting from uppercase to lowercase\nfirst_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 7></a>\n<h2><font color = MidnightBlue>Removal of Stopwords</font></h2>","metadata":{}},{"cell_type":"markdown","source":"**Let's remove stopwords like is,a,the... Which do not offer much insight.**","metadata":{}},{"cell_type":"code","source":"nltk.download(\"stopwords\")   \nfrom nltk.corpus import stopwords  \n\n# we can use tokenizer instead of split\nfirst_text = nltk.word_tokenize(first_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_text = [ word for word in first_text if not word in set(stopwords.words(\"english\"))]","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 8></a>\n<h2><font color = MidnightBlue>Lemmatization</font></h2>","metadata":{}},{"cell_type":"markdown","source":"**Lemmatization to bring back multiple forms of same word to their common root like 'coming', 'comes' into 'come'.**","metadata":{}},{"cell_type":"code","source":"lemma = nltk.WordNetLemmatizer()\nfirst_text = [ lemma.lemmatize(word) for word in first_text] \n\nfirst_text = \" \".join(first_text)\nfirst_text","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 9></a>\n<h2><font color = MidnightBlue>Perform it for all the examples</font></h2>\n<b>We performed the steps for a single example. Now let's perform it for all the examples in the data.</b>","metadata":{}},{"cell_type":"code","source":"#Removal of HTML Contents\ndef remove_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removal of Punctuation Marks\ndef remove_punctuations(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removal of Special Characters\ndef remove_characters(text):\n    return re.sub(\"[^a-zA-Z]\",\" \",text)\n\n#Removal of stopwords \ndef remove_stopwords_and_lemmatization(text):\n    final_text = []\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    \n    for word in text:\n        if word not in set(stopwords.words('english')):\n            lemma = nltk.WordNetLemmatizer()\n            word = lemma.lemmatize(word) \n            final_text.append(word)\n    return \" \".join(final_text)\n\n#Total function\ndef cleaning(text):\n    text = remove_html(text)\n    text = remove_punctuations(text)\n    text = remove_characters(text)\n    text = remove_stopwords_and_lemmatization(text)\n    return text\n\n#Apply function on text column\ndata['text']=data['text'].apply(cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Let's make some visualization with new data.</b></h2>","metadata":{}},{"cell_type":"markdown","source":"<h3><b>1.WordCloud for Real News </b></h3>","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\nplt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(data[data.target == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><b>2.WordCloud for Fake News </b></h3>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(data[data.target == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><b>Number of words in each text</b></h3>","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=data[data['target']==0]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='SkyBlue')\nax1.set_title('Fake news text')\ntext_len=data[data['target']==1]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='PeachPuff')\nax2.set_title('Real news text')\nfig.suptitle('Words in texts')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The number of words seems to be a bit different. 500 words  are most common in real news category while around 250 words are most common in fake news category.**","metadata":{}},{"cell_type":"markdown","source":"<a id = 10></a>\n<h2><font color = MidnightBlue>N-Gram Analysis</font></h2>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">\n<center><img style = \"height:450px;\" src=\"https://devopedia.org/images/article/219/7356.1569499094.png\"></center>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","metadata":{}},{"cell_type":"code","source":"texts = ' '.join(data['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = texts.split(\" \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_n_gram(string,i):\n    n_gram = (pd.Series(nltk.ngrams(string, i)).value_counts())[:15]\n    n_gram_df=pd.DataFrame(n_gram)\n    n_gram_df = n_gram_df.reset_index()\n    n_gram_df = n_gram_df.rename(columns={\"index\": \"word\", 0: \"count\"})\n    print(n_gram_df.head())\n    plt.figure(figsize = (16,9))\n    return sns.barplot(x='count',y='word', data=n_gram_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 11></a>\n<h2><font color = MidnightBlue>Unigram Analysis</font></h2>","metadata":{}},{"cell_type":"code","source":"draw_n_gram(string,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 12></a>\n<h2><font color = MidnightBlue>Bigram Analysis</font></h2>","metadata":{}},{"cell_type":"code","source":"draw_n_gram(string,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 13></a>\n<h2><font color = MidnightBlue>Trigram Analysis</font></h2>","metadata":{}},{"cell_type":"code","source":"draw_n_gram(string,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 14></a>\n<h2><font color = MidnightBlue>Modeling</font></h2>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">","metadata":{}},{"cell_type":"markdown","source":"<a id = 15></a>\n<h2><font color = MidnightBlue>Train Test Split</font></h2>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 16></a>\n<h2><font color = MidnightBlue>Tokenizing</font></h2>","metadata":{}},{"cell_type":"markdown","source":"* **Tokenizing Text -> Repsesenting each word by a number**\n\n* **Mapping of orginal word to number is preserved in word_index property of tokenizer**\n\n<h3><b>Lets keep all news to 300, add padding to news with less than 300 words and truncating long ones </b></h3>","metadata":{}},{"cell_type":"code","source":"max_features = 10000\nmaxlen = 300","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 17></a>\n<h2><font color = MidnightBlue>Training LSTM Model</font></h2>","metadata":{}},{"cell_type":"code","source":"batch_size = 256\nepochs = 10\nembed_size = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, validation_split=0.3, epochs=10, batch_size=batch_size, shuffle=True, verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 18></a>\n<h2><font color = MidnightBlue>Analysis After Training </font></h2>","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.plot(history.history[\"accuracy\"], label = \"Train\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Test\")\nplt.title(\"Accuracy\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.plot(history.history[\"loss\"], label = \"Train\")\nplt.plot(history.history[\"val_loss\"], label = \"Test\")\nplt.title(\"Loss\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"epochs\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict_classes(X_test)\nprint(classification_report(y_test, pred, target_names = ['Fake','Real']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}