{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d26307df-f430-c5df-0764-cabfbae3e1ae"},"source":"Parkinson's Disease is a progressive disorder of the nervous system that affects movement. It develops gradually and cannot be cured, although treatment may help. It requires medical diagnosis and it can occur starting from the age of 6. Thus, this is a preliminary study that tries to identify correlations between Parkinson disease variables for easy identification of the disease.A"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b71e01e8-d89c-49a9-dac1-be5ec8284b38"},"outputs":[],"source":"#Krishna Thiyagarajan\n#Abhinav Jain\n#Linear Reg. Min Project\n#ECE-411: Stat Learning\n#Prof. Keene\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import lasso_path\nimport matplotlib.pyplot as plt\n\nnp.set_printoptions(suppress = True, precision=3); #Options for NumPy\n\ndf = pd.read_csv('../input/Data.csv', sep = ',', header = 0);  #load CSV file using pandas\n\ndf.describe() # Summary of data set indicating mean, std, min etc."},{"cell_type":"markdown","metadata":{"_cell_guid":"f5d9dd72-be6c-8890-b5bd-b18e73c72a56"},"source":"**Analyzing Variables to Find the Dependent Variable**\n\nAs it can be seen, the table doesn't tell you what the dependent variable of this data set is. So we must discover it. To do so, we find the correlation matrix and see which column has the highest average correlation constant. This is obviously for demonstration purposes and it cannot be used as a way of determining a \"dependent variable\". #"},{"cell_type":"markdown","metadata":{"_cell_guid":"1eca4507-88ff-b3e4-ea2c-7be8da526de6"},"source":"**Qualitative Look at Correlation Matrix**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a76ad88e-1476-1fc9-4b7f-8994c815f72e"},"outputs":[],"source":"corrMat = df[::].corr(); \nsns.heatmap(corrMat, vmax=0.8, square = True);"},{"cell_type":"markdown","metadata":{"_cell_guid":"f381c7f4-dc99-71d8-7a0e-edea03b73121"},"source":"**Quantitative Look at Correlation Matrix**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91566928-1620-d978-466f-f830428aeea8"},"outputs":[],"source":"print(corrMat);"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c35c44f-d457-57d0-8e12-57ead7962883"},"source":"After some simple calculations, one can see that Jitter(Percent) seems to generally have the highest percent of correlation with all of the other variables. Thus, let us use that as the dependent variable and the rest as the independent variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a752e76-6490-d7b5-0ac1-f406c4c7bd00"},"outputs":[],"source":"\nX = df.values[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]] # First 19 columns of data\ny = df.values[:, 19]; # Last col of data\nX = np.array(X); \nX = sm.add_constant(X); \nresults = sm.OLS(endog=y, exog = X).fit(); \nprint(results.summary())"},{"cell_type":"markdown","metadata":{"_cell_guid":"95703ce3-fbef-027d-124a-ff0670bb467e"},"source":"As it can be, only some of the above variables are truly contributing to the regression. In particular, the following do not seem to contribute to the model as much as the other variables because they are less than 2 standard deviations from the mean: x1, x2, x6, x8, x11, x14 and x18. In other words, these variables are accepted by the null hypothesis. Thus, let us see how things change if we ignore these variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6cdc7207-184d-534d-441f-6c10e54ca21f"},"outputs":[],"source":"X = df.values[:,[2,3,4,6,8,9,11,12,14,15,16,18]] # First 19 columns of data\ny = df.values[:, 19]; # Last col of data\nX = np.array(X); \nX = sm.add_constant(X); \nresults1 = sm.OLS(endog=y, exog = X).fit(); \n\nprint(results1.summary())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd69c52a-185c-672a-fcef-ba1b0846a730"},"outputs":[],"source":"print(\"\\n\\n\\nÎ² =\", results1.params) #new beta vector for linear regression"},{"cell_type":"markdown","metadata":{"_cell_guid":"c87d0210-3037-809b-0181-a65c7bc67b7e"},"source":"Some of the t-values have changed and the R-squared variable has decreased, but the new model seems to predict the Jitter percentage as well as the old model. Thus, ignoring those certain variables with the low t-scores did not do much damage."},{"cell_type":"markdown","metadata":{"_cell_guid":"d41a0105-a7a9-1597-37a0-ec2eeb4e5556"},"source":"**Let us See how Ridge Regression Works on this Data set**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88fe1ea3-84b7-c923-b0be-ec2db51080b8"},"outputs":[],"source":"X = df.values[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]] # First 19 columns of data\ny = df.values[:, 19]; # Last col of data\n\nn_alphas = 200\n\nalphas = np.logspace(-10, -2, n_alphas)\nregr = linear_model.Ridge(fit_intercept = False)\n\ncoefs = []; \nfor a in alphas:\n    regr.set_params(alpha = a)\n    regr.fit(X,y)\n    coefs.append(regr.coef_)\n    \nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nax.set_xlim(ax.get_xlim()[::-1])\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization')\nplt.axis('tight')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fbb36a16-cfca-0d00-cb97-ee642a21b906"},"outputs":[],"source":"from itertools import cycle\n\nX = df.values[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]] # First 19 columns of data\ny = df.values[:, 19]; # Last col of data\n\neps = 5e-20\nalphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept = False)\n\nplt.figure(1)\nax = plt.gca()\ncolors = cycle(['b', 'r', 'g', 'c', 'k']);\nneg_log_alphas_lasso = -np.log10(alphas_lasso)\nfor coef_l, c in zip(coefs_lasso, colors):\n    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c);\n   \nplt.xlabel('-Log(alpha)')\nplt.ylabel('coefficients')\nplt.title('Lasso Paths')\nplt.axis('tight')\n\n\n\n\n#lassocoefs = []; \n#for a in alphas:\n#    regr.set_params(alpha = a)\n#    regr.fit(X,y)\n#    lassocoefs.append(regr.coef_)\n\n#plt.xlabel('-Log(alpha)');\n#plt.ylabel('coefficients');\n#plt.title('Lasso Paths');\n#plt.axis('tight');"},{"cell_type":"markdown","metadata":{"_cell_guid":"51bd7fe4-d525-fc35-aafb-78331dab54a0"},"source":"**Comparison of Least Squares, Ridge and Lasso**#"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d96f2af-d6ac-6b72-39d9-a16244e00eab"},"outputs":[],"source":"print(\"Ridge   \", \"Lasso    \", \"OLS\")\n\nA  = [coefs[0], coefs_lasso.T[::-1][0], results.params[1:]]\n\nprint(np.matrix(A).T)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}