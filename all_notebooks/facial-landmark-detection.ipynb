{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing the dependencies\nfrom sklearn.model_selection import train_test_split \nfrom matplotlib import pyplot as plt\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the Lookup Table\nIdLookupTable = pd.read_csv('/kaggle/input/IdLookupTable.csv')\nIdLookupTable.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IdLookupTable.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the training dataset\ntraining = pd.read_csv('/kaggle/input/training/training.csv')\ntraining.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/test/test.csv')\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are lot of null values in the training dataset. Let's drop all the rows which have null values. \ntraining = training.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training.shape, type(training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us seperate images and keypoints seperately for the train dataset\n\n#Writing a function to seperate the images from the training data\ndef load_images(image_data):\n    images = []\n    for idx, sample in image_data.iterrows():\n        image = np.array(sample['Image'].split(' '), dtype=int)\n        image = np.reshape(image, (96,96,1))\n        images.append(image)\n    images = np.array(images)/255.\n    return images\n\n#Writing a function to seperate the keypoints from the training data\ndef load_keypoints(keypoint_data):\n    keypoint_data = keypoint_data.drop('Image',axis = 1)\n    keypoint_features = []\n    for idx, sample_keypoints in keypoint_data.iterrows():\n        keypoint_features.append(sample_keypoints)\n    keypoint_features = np.array(keypoint_features, dtype = 'float')\n    return keypoint_features\n\n\nclean_train_images =load_images(training)\nprint(\"Shape of clean_train_images: {}\".format(np.shape(clean_train_images)))\n\nclean_train_keypoints = load_keypoints(training)\nprint(\"Shape of clean_train_keypoints: {}\".format(np.shape(clean_train_keypoints)))\n\ntest_images = load_images(test)\nprint(\"Shape of test_images: {}\".format(np.shape(test_images)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's create a function to plot the image\ndef plot_sample(image, keypoint, axis, title):\n    image = image.reshape(96,96)\n    axis.imshow(image, cmap='gray')\n    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's create a new label for images and keypoints\ntrain_images = clean_train_images\ntrain_keypoints = clean_train_keypoints\nfig, axis = plt.subplots()\nplot_sample(clean_train_images[25], clean_train_keypoints[25], axis, \"Sample Image & Keypoints\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Horizonal Flipping of Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Various Image Agumentation choices\nsample = 25\nhorizontal_flip = True\nrotation_augmentation = True\nbrightness_augmentation = True\nshift_augmentation = True\nrandom_noise_augmentation = True\n\n#Function for Horizantal flipping of images\ndef left_right_flip(images, keypoints):\n    flipped_keypoints = []\n    flipped_images = np.flip(images, axis=2)   # Flip column-wise (axis=2)\n    for idx, sample_keypoints in enumerate(keypoints):\n        flipped_keypoints.append([96.-coor if idx%2==0 else coor for idx,coor in enumerate(sample_keypoints)])    # Subtract only X co-ordinates of keypoints from 96 for horizontal flipping\n    return flipped_images, flipped_keypoints\n\nif horizontal_flip:\n    flipped_train_images, flipped_train_keypoints = left_right_flip(clean_train_images, clean_train_keypoints)\n    print(\"Shape of flipped_train_images:\",np.shape(flipped_train_images))\n    print(\"Shape of flipped_train_keypoints:\",np.shape(flipped_train_keypoints))\n    \n    #Concatenating the train images with flipped image & train keypoints with flipped train points\n    train_images = np.concatenate((train_images, flipped_train_images))\n    train_keypoints = np.concatenate((train_keypoints, flipped_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(flipped_train_images[sample], flipped_train_keypoints[sample], axis, \"Horizontally Flipped\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rotating of Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Example for converting rotated image\n#img = cv2.imread('messi5.jpg',0)\n#rows,cols = img.shape\n\n#M = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)\n#dst = cv2.warpAffine(img,M,(cols,rows))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom math import sin, cos, pi\n\n\nrotation_angles = [12]    # Rotation angle in degrees (includes both clockwise & anti-clockwise rotations)\npixel_shifts = [12]    # Horizontal & vertical shift amount in pixels (includes shift from all 4 corners)\n\n#Writing a function for Rotation of the Images\ndef rotate_augmentation(images, keypoints):\n    rotated_images = []\n    rotated_keypoints = []\n    print(\"Augmenting for angles (in degrees): \")\n    \n    for angle in rotation_angles:    # Rotation augmentation for a list of angle values\n        for angle in [angle,-angle]:\n            print(f'{angle}', end='  ')\n            M = cv2.getRotationMatrix2D((48,48), angle, 1.0)\n            angle_rad = -angle*pi/180.     # Obtain angle in radians from angle in degrees (notice negative sign for change in clockwise vs anti-clockwise directions from conventional rotation to cv2's image rotation)\n            \n            # For train_images\n            for image in images:\n                rotated_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)\n                rotated_images.append(rotated_image)\n            \n            # For train_keypoints\n            for keypoint in keypoints:\n                rotated_keypoint = keypoint - 48.    # Subtract the middle value of the image dimension\n                for idx in range(0,len(rotated_keypoint),2):\n                    # https://in.mathworks.com/matlabcentral/answers/93554-how-can-i-rotate-a-set-of-points-in-a-plane-by-a-certain-angle-about-an-arbitrary-point\n                    rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\n                    rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\n                rotated_keypoint += 48.   # Add the earlier subtracted value\n                rotated_keypoints.append(rotated_keypoint)\n            \n    return np.reshape(rotated_images,(-1,96,96,1)), rotated_keypoints\n\n#For more details on the transformation of the images below is the link.\n#https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html\n\nif rotation_augmentation:\n    rotated_train_images, rotated_train_keypoints = rotate_augmentation(clean_train_images, clean_train_keypoints)\n    print(\"\\nShape of rotated_train_images:\",np.shape(rotated_train_images))\n    print(\"Shape of rotated_train_keypoints:\\n\",np.shape(rotated_train_keypoints))\n    \n    #Concatenating the train images with rotated image & train keypoints with rotated train points\n    train_images = np.concatenate((train_images, rotated_train_images))\n    train_keypoints = np.concatenate((train_keypoints, rotated_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(rotated_train_images[sample], rotated_train_keypoints[sample], axis, \"Rotation Augmentation\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Brightness Alteration of Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Writing a function for Brightness Alteration\ndef alter_brightness(images, keypoints):\n    altered_brightness_images = []\n    inc_brightness_images = np.clip(images*1.2, 0.0, 1.0)    # Increased brightness by a factor of 1.2 & clip any values outside the range of [-1,1]\n    dec_brightness_images = np.clip(images*0.6, 0.0, 1.0)    # Decreased brightness by a factor of 0.6 & clip any values outside the range of [-1,1]\n    altered_brightness_images.extend(inc_brightness_images)\n    altered_brightness_images.extend(dec_brightness_images)\n    return altered_brightness_images, np.concatenate((keypoints, keypoints))\n\nif brightness_augmentation:\n    altered_brightness_train_images, altered_brightness_train_keypoints = alter_brightness(clean_train_images, clean_train_keypoints)\n    print(\"Shape of altered_brightness_train_images:\",np.shape(altered_brightness_train_images))\n    print(\"Shape of altered_brightness_train_keypoints:\",np.shape(altered_brightness_train_keypoints))\n    \n    #Concatenating the train images with brightness altered images & train keypoints with brightness altered train points\n    train_images = np.concatenate((train_images, altered_brightness_train_images))\n    train_keypoints = np.concatenate((train_keypoints, altered_brightness_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(altered_brightness_train_images[sample], altered_brightness_train_keypoints[sample], axis, \"Increased Brightness\") \n    fig, axis = plt.subplots()\n    plot_sample(altered_brightness_train_images[len(altered_brightness_train_images)//2+sample], altered_brightness_train_keypoints[len(altered_brightness_train_images)//2+sample], axis, \"Decreased Brightness\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Horizontal Left and Vertical Shift"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Writing a function for shift the image horizontal and verical\ndef shift_images(images, keypoints):\n    shifted_images = []\n    shifted_keypoints = []\n    for shift in pixel_shifts:    # Augmenting over several pixel shift values\n        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\n            for image, keypoint in zip(images, keypoints):\n                shifted_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)\n                shifted_keypoint = np.array([(point+shift_x) if idx%2==0 else (point+shift_y) for idx, point in enumerate(keypoint)])\n                if np.all(0.0<shifted_keypoint) and np.all(shifted_keypoint<96.0):\n                    shifted_images.append(shifted_image.reshape(96,96,1))\n                    shifted_keypoints.append(shifted_keypoint)\n    shifted_keypoints = np.clip(shifted_keypoints,0.0,96.0)\n    return shifted_images, shifted_keypoints\n\nif shift_augmentation:\n    shifted_train_images, shifted_train_keypoints = shift_images(clean_train_images, clean_train_keypoints)\n    print(f\"Shape of shifted_train_images:\",np.shape(shifted_train_images))\n    print(f\"Shape of shifted_train_keypoints:\",np.shape(shifted_train_keypoints))\n    \n    train_images = np.concatenate((train_images, shifted_train_images))\n    train_keypoints = np.concatenate((train_keypoints, shifted_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(shifted_train_images[sample], shifted_train_keypoints[sample], axis, \"Shift Augmentation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding Noise to the images"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Writing a function to add noise\ndef add_noise(images):\n    noisy_images = []\n    for image in images:\n        noisy_image = cv2.add(image, 0.008*np.random.randn(96,96,1))    # Adding random normal noise to the input image & clip the resulting noisy image between [-1,1]\n        noisy_images.append(noisy_image.reshape(96,96,1))\n    return noisy_images\n\nif random_noise_augmentation:\n    noisy_train_images = add_noise(clean_train_images)\n    print(\"Shape of noisy_train_images:\",np.shape(noisy_train_images))\n    \n    train_images = np.concatenate((train_images, noisy_train_images))\n    train_keypoints = np.concatenate((train_keypoints, clean_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(noisy_train_images[sample], clean_train_keypoints[sample], axis, \"Random Noise Augmentation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing all the train images with keypoints"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of final train_images: {}\".format(np.shape(train_images)))\nprint(\"Shape of final train_keypoints: {}\".format(np.shape(train_keypoints)))\n\nif horizontal_flip:\n    print(\"Horizontal Flip Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(flipped_train_images[i], flipped_train_keypoints[i], axis, \"\")\n    plt.show()\n\nif rotation_augmentation:\n    print(\"Rotation Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(rotated_train_images[i], rotated_train_keypoints[i], axis, \"\")\n    plt.show()\n    \nif brightness_augmentation:\n    print(\"Brightness Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(altered_brightness_train_images[i], altered_brightness_train_keypoints[i], axis, \"\")\n    plt.show()\n\nif shift_augmentation:\n    print(\"Shift Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(shifted_train_images[i], shifted_train_keypoints[i], axis, \"\")\n    plt.show()\n    \nif random_noise_augmentation:\n    print(\"Random Noise Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(noisy_train_images[i], clean_train_keypoints[i], axis, \"\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, AvgPool2D, BatchNormalization, Dropout, Activation, MaxPooling2D\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras import regularizers\nfrom keras.layers.advanced_activations import ReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\n\nmodel = Sequential()\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n#model.add(Dropout(0.1))\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n#model.add(Dropout(0.1))\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n#model.add(Dropout(0.1))\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n# model.add(BatchNormalization())\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n#model.add(Dropout(0.1))\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n#model.add(Dropout(0.1))\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(ReLU())\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\nadam =optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\nmodel.compile(optimizer=adam, \n              loss='mse', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n\ncheckpointer = ModelCheckpoint(filepath = 'best_model.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\nhist = model.fit(train_images, train_keypoints,epochs=100,batch_size =64, validation_split=0.20, callbacks=[checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('keypoint_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model \n\ntest_preds = model.predict(test_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Test Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,16))\nfor i in range(15):\n    axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n    plot_sample(test_images[i], test_preds[i], axis, \"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}