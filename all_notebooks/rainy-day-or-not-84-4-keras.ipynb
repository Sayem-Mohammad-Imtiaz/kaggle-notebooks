{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Initial step\nImport required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical, normalize\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring data\nLoad the data and explore it."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/weatherAUS.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we have \"NaN\"s.\nLet's find out how many nans df have."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First 4 columns have a lot of nans, let's drop them and drop RISK_MM"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['Sunshine','Evaporation','Cloud3pm','Cloud9am', 'RISK_MM'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally drop NaNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much cleaner.\n\nConvert date column in DateTime format, set date as index and sort in.\n\nFor machine learning it's not necessary, it's just fun."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True)\ndf.sort_index(inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot MaxTemp."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MaxTemp'].rolling(365).mean().plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Back to machine learning.\n\nWe have a lot of \"strings\".\n\nConvert them to numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Location'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Location'] = df['Location'].astype('category').cat.codes\ndf['WindGustDir'] = df['WindGustDir'].astype('category').cat.codes\ndf['WindDir9am'] = df['WindDir9am'].astype('category').cat.codes\ndf['WindDir3pm'] = df['WindDir3pm'].astype('category').cat.codes\ndf['RainToday'] = df['RainToday'].astype('category').cat.codes\ndf['RainTomorrow'] = df['RainTomorrow'].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the Date columm. We don't need it any more ."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we sort the data we need to shuffle it.\n\nAnyway shuffling data it's always good practice. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = shuffle(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create train data and labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('RainTomorrow', axis=1)\ny = df['RainTomorrow']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We definitely need to normalize the data.\n\nConvert it to np array and use normalize utils from keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.values\nX = normalize(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the dataset into three sets.\n\n\ntrain - 80% valid - 10% test - 10%"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create the model.\nOur playground. Feel free to try a different variation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n       \n    tf.keras.layers.Dense(128, input_shape=(17,), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n\n            \n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(0.00001),\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\nFinally, train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=10,\n                    validation_data=(X_val, y_val),\n                    verbose=1,\n                   )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model\nPlot our accuracy and loss for understanding problems: \"high bias\" and \"high variance\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After finishing playing with model and we are happy with achieved accuracy, evaluate your model on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = accuracy * 100\nplt.bar(1, acc)\nplt.text(0.92,45,f'{acc:.2f}%', fontsize=20)\nplt.title('Accuracy')\nplt.xticks([])\nplt.ylabel('Percent')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center>Thanks for reading.</center>\n\n<center>Vote if you like it.</center>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}