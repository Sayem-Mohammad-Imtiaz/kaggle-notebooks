{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Telecom Churn - Advance Classification","metadata":{}},{"cell_type":"markdown","source":"## Business Problem Overview\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\n \n\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\n\n \n\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n\n \n\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.","metadata":{}},{"cell_type":"markdown","source":"### Business Objective\n\nTo predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.","metadata":{}},{"cell_type":"markdown","source":"<table><tr><th></th><th>Acronyms</th><th>Descriptions</th></tr>\n<tr><td>0</td><td>*.6</td><td>KPI for the month of June</td></tr>\n<tr><td>1</td><td>*.7</td><td>KPI for the month of July</td></tr>\n<tr><td>2</td><td>*.8</td><td>KPI for the month of August</td></tr>\n<tr><td>3</td><td>*.9</td><td>KPI for the month of September</td></tr>\n<tr><td>4</td><td>2G</td><td>2G network</td></tr>\n<tr><td>5</td><td>3G</td><td>3G network</td></tr>\n<tr><td>6</td><td>AMT</td><td>Amount in local currency</td></tr>\n<tr><td>7</td><td>AON</td><td>Age on network - number of days the customer is using the operator T network</td></tr>\n<tr><td>8</td><td>ARPU</td><td>Average revenue per user</td></tr>\n<tr><td>9</td><td>AV</td><td>Average</td></tr>\n<tr><td>10</td><td>CIRCLE_ID</td><td>Telecom circle area to which the customer belongs to</td></tr>\n<tr><td>11</td><td>DATA</td><td>Mobile internet</td></tr>\n<tr><td>12</td><td>FB_USER</td><td>Service scheme to avail services of Facebook and similar social networking sites</td></tr>\n<tr><td>13</td><td>IC</td><td>Incoming calls</td></tr>\n<tr><td>14</td><td>ISD</td><td>ISD calls</td></tr>\n<tr><td>15</td><td>LOC</td><td>Local calls - within same telecom circle</td></tr>\n<tr><td>16</td><td>MAX</td><td>Maximum</td></tr>\n<tr><td>17</td><td>MOBILE_NUMBER</td><td>Customer phone number</td></tr>\n<tr><td>18</td><td>MONTHLY</td><td>Service schemes with validity equivalent to a month</td></tr>\n<tr><td>19</td><td>MOU</td><td>Minutes of usage - voice calls</td></tr>\n<tr><td>20</td><td>NIGHT</td><td>Scheme to use during specific night hours only</td></tr>\n<tr><td>21</td><td>NUM</td><td>Number</td></tr>\n<tr><td>22</td><td>OFFNET</td><td>All kind of calls outside the operator T network</td></tr>\n<tr><td>23</td><td>OG</td><td>Outgoing calls</td></tr>\n<tr><td>24</td><td>ONNET</td><td>All kind of calls within the same operator network</td></tr>\n<tr><td>25</td><td>PCK</td><td>Prepaid service schemes called - PACKS</td></tr>\n<tr><td>26</td><td>RECH</td><td>Recharge</td></tr>\n<tr><td>27</td><td>ROAM</td><td>Indicates that customer is in roaming zone during the call</td></tr>\n<tr><td>28</td><td>SACHET</td><td>Service schemes with validity smaller than a month</td></tr>\n<tr><td>29</td><td>SPL</td><td>Special calls</td></tr>\n<tr><td>30</td><td>STD</td><td>STD calls - outside the calling circle</td></tr>\n<tr><td>31</td><td>T2C</td><td>Operator T to itâ€™s own call center</td></tr>\n<tr><td>32</td><td>T2F</td><td>Operator T to fixed lines of T</td></tr>\n<tr><td>33</td><td>T2M</td><td>Operator T to other operator mobile</td></tr>\n<tr><td>34</td><td>T2O</td><td>Operator T to other operator fixed line</td></tr>\n<tr><td>35</td><td>T2T</td><td>Operator T to T, i.e. within same operator (mobile to mobile)</td></tr>\n<tr><td>36</td><td>VBC</td><td>Volume based cost - when no specific scheme is not purchased and paid as per usage</td></tr>\n<tr><td>37</td><td>VOL</td><td>Mobile internet usage volume (in MB)</td></tr></table>","metadata":{}},{"cell_type":"code","source":"#import necessary packae/library needed for this assignment\nimport pandas as pd,numpy as np,seaborn as sns,matplotlib.pyplot as plt\n\nfrom datetime import datetime , timedelta\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting max column size to view all column data\npd.set_option(\"display.max_rows\", 230)\npd.set_option(\"display.max_columns\", 230)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATA LOAD","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets load the CSV data\ntelecom_data=pd.read_csv('/kaggle/input/telecom-churn/telecom_churn_data.csv',engine='python')\n\n#lets now check for the info of the loaded CSV\ntelecom_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see how the loaded data looks\ntelecom_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets pull out all features having Object as datatype\nobject_columns=telecom_data.select_dtypes(['object']).columns\ntelecom_data[list(object_columns)].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(object_columns)\n#We notice that above Object columns are all date and hence can be converted to date Format\n\nfor date_col in object_columns:\n    telecom_data[date_col]=pd.to_datetime(telecom_data[date_col], format='%m/%d/%Y')\n    \n#Now that we converted the datatype to Datetime, lets again check the info of the dataframe\ntelecom_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets rename the column month names to month number\ntelecom_data.rename(columns={'jun_vbc_3g': 'vbc_3g_6', 'jul_vbc_3g': 'vbc_3g_7', 'aug_vbc_3g': 'vbc_3g_8', 'sep_vbc_3g': 'vbc_3g_9'}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATA PREPERATION","metadata":{}},{"cell_type":"markdown","source":"Lets Clean up some features to extract high value customers based on the recharge amount and recharge data","metadata":{}},{"cell_type":"code","source":"#lets drop Mobile Number as its only unique\ntelecom_data.drop('mobile_number',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets pull out all the features related to recharge\nrecharge_cols=telecom_data.columns[telecom_data.columns.str.contains('rech_amt|rech_data')]\ntelecom_data[list(recharge_cols)].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obsere NAN under `Average data amount` Column, Hence for the dependent columns to be valid, NAN needs to be present under `Max recharge data` and `total recharge data`\n\nNow lets validate this use case","metadata":{}},{"cell_type":"code","source":"print(telecom_data.loc[telecom_data['av_rech_amt_data_6'].isnull(),['av_rech_amt_data_6','max_rech_data_6','total_rech_data_6']].sum())\nprint('\\n',telecom_data.loc[telecom_data['av_rech_amt_data_7'].isnull(),['av_rech_amt_data_7','max_rech_data_7','total_rech_data_7']].sum())\nprint('\\n',telecom_data.loc[telecom_data['av_rech_amt_data_8'].isnull(),['av_rech_amt_data_8','max_rech_data_8','total_rech_data_8']].sum())\nprint('\\n',telecom_data.loc[telecom_data['av_rech_amt_data_9'].isnull(),['av_rech_amt_data_9','max_rech_data_9','total_rech_data_9']].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All 3 Columns are in Sync for NAN data and lets impute NAN with 0 to proceed with further Analysis","metadata":{}},{"cell_type":"code","source":"#Method to impute specific Col_name with impute_val passed as argument\ndef impute_value(col_name,impute_val):\n    telecom_data[col_name+'_6'].fillna(impute_val,inplace=True)\n    telecom_data[col_name+'_7'].fillna(impute_val,inplace=True)\n    telecom_data[col_name+'_8'].fillna(impute_val,inplace=True)\n    telecom_data[col_name+'_9'].fillna(impute_val,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_value('av_rech_amt_data',0)\nimpute_value('max_rech_data',0)\nimpute_value('total_rech_data',0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(telecom_data.shape)\ntelecom_data.drop_duplicates(inplace=True)\nprint(telecom_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. DERIVED FEATURES","metadata":{}},{"cell_type":"markdown","source":"Now lets derive the Total data recharge based on total_rech_data and av_rech_amt_data","metadata":{}},{"cell_type":"code","source":"# Multiplying total recharge data and avergae recharge amount for data\ntelecom_data['total_data_rech_6']=telecom_data['total_rech_data_6']*telecom_data['av_rech_amt_data_6']\ntelecom_data['total_data_rech_7']=telecom_data['total_rech_data_7']*telecom_data['av_rech_amt_data_7']\ntelecom_data['total_data_rech_8']=telecom_data['total_rech_data_8']*telecom_data['av_rech_amt_data_8']\n\n## We are not perfroming this for month 9 as its our target month\n# adding total recharge amount with total data rechrage to obtain overall recharge amount for month\ntelecom_data['total_rech_6']=telecom_data['total_data_rech_6']+telecom_data['total_rech_amt_6']\ntelecom_data['total_rech_7']=telecom_data['total_data_rech_7']+telecom_data['total_rech_amt_7']\ntelecom_data['total_rech_8']=telecom_data['total_data_rech_8']+telecom_data['total_rech_amt_8']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can derieved columns for total minutes of usage for each month","metadata":{}},{"cell_type":"code","source":"#Get list of all mins of usage columns\nminutes_of_usage_cols = [cols for cols in telecom_data.columns.tolist() if 'mou' in cols]\n\n# If mins of usage is NAN we can fill it with 0\ntelecom_data[minutes_of_usage_cols] = telecom_data[minutes_of_usage_cols].apply(lambda x: x.fillna(0))\n\n# Deriving columns for total minutes of usage on each month\n# Adding minutes of usage for incoming and outgoing \ntelecom_data['total_mou_6'] = telecom_data['total_og_mou_6'] + telecom_data['total_ic_mou_6']\ntelecom_data['total_mou_7'] = telecom_data['total_og_mou_7'] + telecom_data['total_ic_mou_7']\ntelecom_data['total_mou_8'] = telecom_data['total_og_mou_8'] + telecom_data['total_ic_mou_8']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets derive some Features out of the Last Recharge date Columns**","metadata":{}},{"cell_type":"code","source":"#method to calculate the days since last recharge\ndef last_recharge_days(cols,col_name):\n    \n    #get the last recharge date\n    telecom_data['last_rech_date'] = telecom_data[cols].max(axis=1)\n\n    #Now lets get the no of days since last recharge as we are restricted till 8th month, we subract from 31st Aug\n    telecom_data[col_name] = np.floor(( pd.to_datetime('2014-08-31', format='%Y-%m-%d') - telecom_data['last_rech_date'] ).astype('timedelta64[D]'))\n    \n    telecom_data.drop('last_rech_date',axis=1,inplace=True)\n    \n    #impute the missing values with 0\n    telecom_data[col_name] = telecom_data[col_name].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listing the date of last recharge for imputation\nrech_date_cols = ['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8']\nlast_recharge_days(rech_date_cols,'days_since_last_rech')\n\n# listing the date of last data recharge for imputation\nrech_data_date_cols = ['date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8']\nlast_recharge_days(rech_data_date_cols,'days_since_last_data_rech')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets also derive a column to see if a customr has reached in a month","metadata":{}},{"cell_type":"code","source":"# deriving columns as boolean for 1 if customer has recharged and 0 if customer has not recharged\ntelecom_data['rech_6'] = np.where(telecom_data[['date_of_last_rech_6','date_of_last_rech_data_6']].apply(pd.isnull).astype(int).sum(axis=1) > 0, 1, 0)\ntelecom_data['rech_7'] = np.where(telecom_data[['date_of_last_rech_7','date_of_last_rech_data_7']].apply(pd.isnull).astype(int).sum(axis=1) > 0, 1, 0)\ntelecom_data['rech_8'] = np.where(telecom_data[['date_of_last_rech_8','date_of_last_rech_data_8']].apply(pd.isnull).astype(int).sum(axis=1) > 0, 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to drop the columns that no longer need as we have derieved some information in a new column","metadata":{}},{"cell_type":"code","source":"#We no longer need these date cols and hence we can drop them\ntelecom_data.drop(rech_date_cols,axis=1,inplace=True)\n\ntelecom_data.drop(rech_data_date_cols,axis=1,inplace=True)\n\ntelecom_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets calculate the date from which the customer is using the service","metadata":{}},{"cell_type":"code","source":"# since we have our data till the month of September we will set the \n# current date as the last day of September\ncurrent_date = datetime.strptime('Sep\\xa030 2014', '%b\\xa0%d %Y')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data.aon.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calulate the date first available\ntelecom_data['date_fst_atv'] = current_date - telecom_data.aon.apply(timedelta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data['date_fst_atv'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data['customer_group'] = pd.cut(telecom_data['date_fst_atv'],bins=3)\ntelecom_data['customer_group'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data['customer_group']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"code, _ = pd.factorize(telecom_data['customer_group'],sort=True)\ntelecom_data['customer_group'] = pd.Series(code)\ntelecom_data['customer_group']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(telecom_data.shape)\n#Lets now drop the columns that we no longer use\n#telecom_data.drop(['total_rech_data_6','total_rech_data_7','total_rech_data_8','av_rech_amt_data_6','av_rech_amt_data_7','av_rech_amt_data_8'],inplace=True,axis=1)\n#telecom_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. FILTER HIGH-VALUED CUSTOMERS","metadata":{}},{"cell_type":"code","source":"#Now lets find the average of month 6 and 7 and place it in a column\ntelecom_data['av_rech_good_phase']=(telecom_data['total_rech_6']+telecom_data['total_rech_7'])/2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rech_70th_perc=np.percentile(telecom_data['av_rech_good_phase'],70)\nprint(rech_70th_perc)\n\n#Now lets filter out high value customers using the 70th percentile and save it to our DF\ntelecom_data=telecom_data[telecom_data['av_rech_good_phase']>=rech_70th_perc]\n\nprint(telecom_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get close to 29.9K records as provided in the assignment","metadata":{}},{"cell_type":"markdown","source":"#### 3. CHURN PREDICTION","metadata":{}},{"cell_type":"markdown","source":"Lets predict the Churn Customers based on `total_ic_mou_9`, `total_og_mou_9`, `vol_2g_mb_9`, `vol_3g_mb_9`","metadata":{}},{"cell_type":"code","source":"#lets see the missing values in these Feature columns\nmonth_9_cols=['total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9']\ntelecom_data[month_9_cols].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A Customer is considered to have Churned if Sum of all of these above mentioned Features is 0","metadata":{}},{"cell_type":"code","source":"telecom_data['churn']=np.where(telecom_data[month_9_cols].sum(axis=1)==0, 1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data['churn'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a lesser percent of churn data and Thus it falls under Class imbalance probllem, which we will handle in the upcoming blocks","metadata":{}},{"cell_type":"markdown","source":"### DATA CLEANUP","metadata":{}},{"cell_type":"markdown","source":"Since Month 9 acts as Target Column and as we are predicting Churn for that month , we no longer require Features representing 9th month and lets go ahead and drop them","metadata":{}},{"cell_type":"code","source":"month_9_cols_drop=telecom_data.columns[telecom_data.columns.str.contains('_9')]\nprint(month_9_cols_drop)\n\ntelecom_data.drop(month_9_cols_drop,axis=1,inplace=True)\nprint(telecom_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets go ahead and drop the Single value Columns First as it provides no use full information for prediction","metadata":{}},{"cell_type":"code","source":"single_val_cols=list(filter(lambda x : len(telecom_data[x].value_counts()) < 2, telecom_data.columns))\nsingle_val_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data.drop(single_val_cols,axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets also differentiate the Categorical and Numerical Columns ","metadata":{}},{"cell_type":"code","source":"# Extration columns specific to fb_user|night|sachet|monthly|pck|churn for the available features\ncat_cols=telecom_data.columns[telecom_data.columns.str.contains('fb_user|night|sachet|monthly|pck|churn')].tolist()\ncat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting the names of all numeric columns \nnumeric_cols=[x for x in telecom_data.columns if (x not in cat_cols and x not in object_columns)]\nnumeric_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Method to check missing vlaues percentage\ndef missing_vals(df,threshold=0.0):\n    return pd.DataFrame(100 * (df.isnull().sum()/df.shape[0])).sort_values(by=0,ascending=False)\\\n            .set_axis(['Missing_Percentage'],axis=1).query('Missing_Percentage > {}'.format(threshold))\n#return dataframe with missing value % which is more than the specified threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see the missing values in these Categorical Columns","metadata":{}},{"cell_type":"code","source":"#Lets see the missing values of Categorical Columns which has more than 30% missing values\nmissing_vals(telecom_data[cat_cols],30.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Not imputing with 0 as its already present as one of the data in the Categorical columns\n#lets impute the missing values in these columns with -1 \ntelecom_data[cat_cols] = telecom_data[cat_cols].apply(lambda x: x.fillna(-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val_df=missing_vals(telecom_data[numeric_cols])\nmissing_val_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets impute the missing values in the Numerical columns with 0","metadata":{}},{"cell_type":"code","source":"#lets impute the missing values in these columns with 0\ntelecom_data[numeric_cols] = telecom_data[numeric_cols].apply(lambda x: x.fillna(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_vals(telecom_data)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have clean data with no columns having missing values ","metadata":{}},{"cell_type":"markdown","source":"Lets try to find the most common value repetiton in each column with its percentage of occurance","metadata":{}},{"cell_type":"code","source":"#Method to get the list of all columns (whose max category's value count % is more than num%). \ndef show_unique_column_values(df_var,num):\n    unique_col_dict={}\n    for col_unique in df_var.columns:\n        #biased_data=len(df_var[col_unique].unique())\n        biased_data=round((df_var[col_unique].value_counts(dropna=False).max()/df_var.shape[0])*100,2) #pull the category with max count in the column\n        if(biased_data>=num and col_unique!='churn'):\n            unique_col_dict[col_unique]=biased_data\n    df_final=pd.DataFrame(unique_col_dict.items())\n    df_final.columns=['cols','percent']\n    return df_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets pull out all the columns with biased data where one category is occupying more than 80% of the data in that column\nmost_repeated_data_cols=show_unique_column_values(telecom_data,80)\nmost_repeated_data_cols=most_repeated_data_cols.sort_values(by='percent',ascending=False)\nmost_repeated_data_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since 80% of the values in the column are repeted with the same data we can drop this as its biased ","metadata":{}},{"cell_type":"code","source":"print(telecom_data.shape)\n#lets go ahead and drop these biased columns\ntelecom_data.drop(list(most_repeated_data_cols.cols),axis=1,inplace=True)\n#lets see the shape of DF \nprint(telecom_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have handled the missing values lets also make sure we have grouped the Categorical data accordingly","metadata":{}},{"cell_type":"code","source":"cat_cols=set(cat_cols).intersection(set(telecom_data.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#method group Sachets into different sets\ndef group_sachets(x):\n    \n    if(int(x)<=5):\n        return int(1)\n    elif(int(x)<=10):\n        return int(2)\n    elif(int(x)<=15):\n        return int(3)\n    elif(int(x)<=20):\n        return int(4)\n    else:\n        return int(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data['sachet_2g_6']=telecom_data['sachet_2g_6'].map(group_sachets)\ntelecom_data['sachet_2g_7']=telecom_data['sachet_2g_7'].map(group_sachets)\ntelecom_data['sachet_2g_8']=telecom_data['sachet_2g_8'].map(group_sachets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outlier Treatment","metadata":{}},{"cell_type":"markdown","source":"Lets hadle the outliers by checking the skewness of the data and then remove the outlier that are above 0.999 quantile range","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all numeric columns\nnumeric_cols=[x for x in telecom_data.columns if (x not in cat_cols and x not in object_columns and x not in ['date_fst_atv'])]\n\n# get the skewness of the numeric columns\nskewed_feats = telecom_data[numeric_cols].apply(lambda x: skew(x.dropna())) #compute skewness","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort the columns based on skewness\nskewed_feats = skewed_feats.sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loop thrugh the columns list\nfor index,value in zip(skewed_feats.index,skewed_feats):\n    # if the skewness is above 5 then remove the values that are above 0.999 quantile\n    if skew(telecom_data[index]) > 5:\n        print(skew(telecom_data[index]),telecom_data.shape[0],sum(telecom_data[index] < telecom_data[index].quantile(0.999)))\n        # set the filtered values to our dataset\n        telecom_data = telecom_data[telecom_data[index] < telecom_data[index].quantile(0.999)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATA VISUALISATION","metadata":{}},{"cell_type":"code","source":"sns.set_style('darkgrid') #setting background style for plots\n\n#custom color\ndark_green=(0.21568627450980393, 0.6366013071895424, 0.39869281045751637)\nlight_green=(0.5913725490196078, 0.8433371780084583, 0.7819761630142252)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TARGET COLUMN  VISUALISATION (CHURN)","metadata":{}},{"cell_type":"code","source":"# Define a function for print text on each pie\ndef pie_label(val):\n    return f'{val:.0f}% {val / 100 * len(telecom_data):.0f}'\n\n# Create a Pie chart for Loan Status\nplt.figure(figsize=(9,10))\nax = telecom_data.churn.value_counts().plot(kind='pie', autopct=pie_label,\\\n                        startangle=140,labels=['Non-churn','Churn'],colors=[dark_green,light_green],\\\n                        textprops={'fontsize': 15,'ha':'center','va':'top'},\\\n                        shadow=False, figsize=(10,10),explode=(0.1,0));\n\ninner_circle=plt.Circle( (0.05,-0.09), 0.5, color='white')\nax.add_artist(inner_circle)\n# if len(img):\n#     ax.imshow(img,zorder=2,extent=[-0.4, 0.45, -0.55, 0.35],aspect='auto')\n\n# set title and labels\nplt.title(\"Propotion of Churn vs Non-Churn (Univariate Analysis)\",size='15')\nplt.ylabel('');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n- We notice a class imbalance problem where we have very minimal data with respect to Churn.\n\n- We are taking care/handling of this Class imbalance in the upcoming blocks","metadata":{}},{"cell_type":"markdown","source":"Plotting Correlation with Target variable","metadata":{}},{"cell_type":"code","source":"sns.heatmap(telecom_data.corr());","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are not able to observe clear view of the correlation since there are higher number of features, so lets split the features into group and check the correlation","metadata":{}},{"cell_type":"code","source":"def plot_correlation():\n    # define subplot and figure size\n    _, axes = plt.subplots(3,1,figsize=(30,15))\n    \n    # loop through each subplot\n    for n,axis in enumerate(axes):\n        \n        cols = []\n        \n        # Get the features specific to each month\n        cols = sorted([cols for cols in telecom_data.columns.tolist() if cols.endswith('_'+str(n+6))]) + ['churn']\n        \n        # Get the correlation and sort it in ascending order , Plot a bar chart for the data\n        (telecom_data[cols].corr()['churn']*100).sort_values().drop('churn').plot(kind='bar',color=sns.color_palette('viridis',telecom_data[cols][:-1].shape[1]),ax=axis)\n        \n        # get the tick labels\n        ticks = [tick for tick in axis.get_xticklabels()]\n        \n        # loop through each bar\n        for i,patch in enumerate(axis.patches):\n            \n            # Get the tick label for each bar\n            label = ticks[i].get_text()[:-2] if ticks[i].get_text() != 'churn' else ticks[i].get_text()\n            \n            # If the correlation is positive lop the tick label below else above\n            if patch.get_height() < 0:\n                axis.text(patch.get_x(),patch.get_height()-2,label,rotation=90,va='top')\n                axis.text(patch.get_x(),3,round(patch.get_height(),1),ha='center')\n            else:\n                axis.text(patch.get_x(),patch.get_height()+3,label,rotation=90)\n                axis.text(patch.get_x(),-5,round(patch.get_height(),1))\n                \n        # set x label , y label and y limit\n        axis.set(xlabel='',ylabel='Correlation with Churn in %',ylim=(-50,50),xticklabels=[])\n        axis.text((telecom_data[cols].shape[1]/2) - 2,25,'Month '+str(n+6),size=18)\n        \n    # plot X label and title    \n    axes[2].text((telecom_data[cols].shape[1]/2) - 2,-60,'Feature Variables',size=15)\n    plt.suptitle('Correlation btw Features and Churn variabale in %',x=0.5,y=0.9,size=25)\n    \n    # show plot\n    plt.show()\n    \n    # Get the features which are not pecific to months\n    cols = set(sorted([col for col in telecom_data.columns.tolist() if not (col[-2:] in ['_6','_7','_8']) and (telecom_data[col].dtype.name in ['float64','int64'])]) + ['churn'])\n    \n    \n    # Get the correlation and sort it in ascending order , Plot a bar chart for the data\n    ax = (telecom_data[cols].corr()['churn']*100).drop('churn').plot(kind='bar',color=sns.color_palette('viridis',len(cols)))\n    \n    # set x label , y label and y limit\n    ax.set(ylabel='Correlation with Churn in %',ylim=(-50,50),title=\"Other Variables\",xlabel='Features')\n    \n    # plot bar values\n    for i,patch in enumerate(ax.patches):\n        if patch.get_height() < 0:\n            ax.text(patch.get_x()+0.1,3,round(patch.get_height(),1))\n        else:\n            ax.text(patch.get_x()+0.1,-5,round(patch.get_height(),1))\n    1# show plot\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_correlation()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n \n- `Std_og_mou`, `loc_in_mou`, `rech` and `fb_user` are some of the highly correlated features with respect to Churn\n \n\n- `aon`, `days since last recharge` and `days since last data recharge` are displaying higher correlation with Churn","metadata":{}},{"cell_type":"code","source":"# set the default color for the plots\nsns.set(palette='Set2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box plot \ndef plot_box(col):\n    # create subplots and set figuresize with shared y axis\n    _, axes = plt.subplots(1,3,figsize=(20,4),sharey=True)\n\n    # loop through each subplot    \n    for n,axis in enumerate(axes):\n        \n        # create BoxPlot for churn vs the feature\n        sns.boxplot(x='churn',y=col+'_'+str(n+6),data=telecom_data,ax=axis,showfliers=False);\n        \n        # set title, xlabel and ylabel\n        axis.set(title='Month '+str(n+6), xlabel=col,ylabel='Count')\n        \n    # set subplot title\n    plt.suptitle(str.title(col) +' vs Churn')\n    \ndef plot_hist(col):\n    # create subplots and set figuresize with shared y axis\n    _, axes = plt.subplots(1,3,figsize=(20,4),sharey=True)\n\n    # loop through each subplot    \n    for n,axis in enumerate(axes):\n        \n        # create histogram plot\n        sns.histplot(x=col+'_'+str(n+6),hue='churn',data=telecom_data,bins=15,ax=axis);\n        \n        # set title, xlabel and ylabel\n        axis.set(title='Month '+str(n+6), xlabel=col,ylabel='Count')\n        \n    # set subplot title\n    plt.suptitle(str.title(col) +' vs Churn')\n    \ndef plot_scatter(col):\n    # create subplots and set figuresize with shared y axis\n    _, axes = plt.subplots(1,3,figsize=(20,4))\n\n    # loop through each subplot    \n    for n,axis in enumerate(axes):\n        \n        # create scatterplot\n        sns.scatterplot(x=col+'_'+str(n+6),y='total_rech_'+str(n+6),hue='churn',data=telecom_data,ax=axis,alpha=0.8);\n        \n        # set title, xlabel and ylabel\n        axis.set(title='Month '+str(n+6), xlabel=col, ylabel='Total Recharge Value')\n        \n    # set subplot title\n    plt.suptitle(str.title(col) +' vs Total Recharge Value')\n\ndef plot_bar(col,segment=True):\n    # create subplots and set figuresize with shared y axis\n    _, axes = plt.subplots(1,3,figsize=(20,4),sharey=True)\n\n    # loop through each subplot    \n    for n,axis in enumerate(axes):\n        if segment:\n            x_data = pd.cut(telecom_data[col+'_'+str(n+6)].astype(int),bins=3)\n            \n        else:\n            x_data = telecom_data[col+'_'+str(n+6)]\n            \n        # create bar plot\n        sns.barplot(y='total_rech_'+str(n+6), x=x_data,data=telecom_data,ci=None,hue='churn',ax=axis)\n        \n        # set title, xlabel and ylabel\n        axis.set(title='Month '+str(n+6), xlabel=col, ylabel='Total Recharge Value')\n        \n    # set subplot title\n    plt.suptitle(str.title(col) +' vs Total Recharge Value')\n    \ndef plot_graphs(col):\n    plot_box(col)\n    plot_hist(col)\n    plot_scatter(col)\n    plot_bar(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CATEGORICAL DATA VISUALISATION","metadata":{}},{"cell_type":"markdown","source":"#### GENERIC METHODS FOR CATEGORICAL DATA VISUALISATION","metadata":{}},{"cell_type":"code","source":"#Method for setting value at top of each bar in Count Plot\ndef display_val(ax,bar_h,f_size,percentage):\n    rects=ax.patches\n    for rect in rects:\n        r_height=rect.get_height()\n        if(r_height!=0.0 and r_height>0):\n            ax.text(rect.get_x()+rect.get_width()/2,bar_h*r_height,str(int(r_height))+percentage,ha='center',va='bottom',fontsize=f_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Generic Method for Categorical columns to display distplot\ndef distplot_com(col_name,fsize):\n    \n    # create subplots and set figure size\n    fig , ax = plt.subplots(1,2,figsize=(15,5))\n    \n    # create stacked bar for feature\n    #telecom_data.groupby([col_name])['churn'].value_counts().unstack().plot(kind='bar',stacked=True,ax=ax[0],width=1.0,color=[(0.21568627450980393, 0.6366013071895424, 0.39869281045751637),(0.5913725490196078, 0.8433371780084583, 0.7819761630142252)])\n    sns.countplot(x=col_name,hue='churn',data=telecom_data,ax=ax[0])   \n    \n    display_val(ax[0],1,fsize,\"\")\n    \n    # creating a bar chart showing percentage ratio of churn for each categorical value\n    category_table=telecom_data.groupby([col_name,'churn'])[col_name].count().unstack('churn').T\n    ((category_table/category_table.sum()).round(2)*100).T.plot.bar(stacked=False,ax=ax[1],width=0.8)\n    \n    # annotate bar values\n    display_val(ax[1],1,fsize,\"%\")\n        \n    # set margins\n    ax[1].margins(0.05,0.05)\n    \n    #set title for each of the sub-plot\n    #plt.suptitle(\"Bivariate Analysis on \"+str(col_name).upper(),size=18,y=1.03)\n    ax[0].set_title(\"Number of Churn in each \"+str(col_name).upper(),size=13)\n    ax[1].set_title(\"Percentage Ratio of Churn in each category of \"+str(col_name).upper(),size=13)\n    ax[0].set_xlabel(str(col_name).upper())\n    ax[1].set_xlabel(str(col_name).upper())\n\n    \n    # make the plot tight layout\n    plt.tight_layout()\n    \n    plt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets sort the Categorical columns\nlist(cat_cols).sort()\n\n#iterate different categorical columns \nfor cols in cat_cols:\n    #count plot and percentage ratio of churn for each of the categorical cols\n    if(cols!='churn'):\n        distplot_com(cols,12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n \n- Over a period of 3 months, there is an increase in number of users utlising different `sachet schemes`\n \n\n- Users opting for `night pack scheme` under scheme 0 and 1 also shows an increase over 3 months along with decrease in Churn rate\n \n\n- here is an increase in no of users using different `Fb_users` scheme which is accompanied by decrease in Churn rate","metadata":{}},{"cell_type":"code","source":"# set figure size\nplt.figure(figsize=(10,5))\n\n# create countplot\nax = sns.countplot(telecom_data.date_fst_atv.dt.year,hue='churn',data=telecom_data)\n\n# set axis labels and title\nax.set(xlabel='Date Joined Service',title='Churn vs Activated Service Date');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n- We observe steady rise in churn for the period of 2009 to 2013","metadata":{}},{"cell_type":"code","source":"# dropping date_fst_atv as we already have days since customer active\ntelecom_data = telecom_data.drop(columns=['date_fst_atv'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set figure size\nplt.figure(figsize=(10,5))\n\n# create countplot\nax = sns.countplot(telecom_data.customer_group,hue='churn',data=telecom_data)\n\n# set axis labels and title\nax.set(xlabel='Customer Group',title='Churn vs Customer Group',xticklabels=['2002-2006','2006-2010','2010-2014']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n- We see that the recently joined customers between timeframe (2010 - 2014) are the ones that with higher churn rate","metadata":{}},{"cell_type":"markdown","source":"### NUMERICAL DATA VISUALISATION","metadata":{}},{"cell_type":"code","source":"plot_graphs('total_rech_num')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs('total_mou')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar('rech',segment=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs('arpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets group different numerical columns based on feature names\nstring_list=['arpu','others','count','days','last_day','total','max','vol','vbc','isd','loc_ic','spl','roam','std_ic','loc_og','std_og']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate over above list and the commonly grouped columns are then ploted as one boxplot\nfor x in string_list:\n    col_list=telecom_data.columns[telecom_data.columns.str.contains(x)]   #pull out the columns having common name (x)\n    plt.figure(figsize=(15,5))\n    sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(telecom_data[col_list])) #display boxplot \n    sns.stripplot(x=\"variable\", y=\"value\", data=pd.melt(telecom_data[col_list]),alpha=0.4)\n    if(x in ['std_ic','loc_og','std_og']):\n        plt.xticks(rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,[[ax1, ax2],[ax3, ax4],[ax5,ax6],[ax7,ax8]]=plt.subplots(4,2,figsize=(20,20))\n\n#pull out last day recharge columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8'].median()\ntemp_df.plot.bar(ax=ax1)\n#set title and x,y labels\nax1.set_title('Churn vs Last Day Recharge Amount', fontsize=10)\nax1.set_xlabel(\"Churn\")\nax1.set_ylabel(\"Last Day Recharge Amount\")\n\n#pull out max recharge columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['max_rech_amt_6', 'max_rech_amt_7', 'max_rech_amt_8'].median()\ntemp_df.plot.bar(ax=ax2)\n#set title and x,y labels\nax2.set_title('Churn vs Max Recharge Amount', fontsize=10)\nax1.set_xlabel(\"Churn\")\nax1.set_ylabel(\"Max Recharge Amount\")\n\n#pull out std outgoing columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['std_og_mou_6', 'std_og_mou_7', 'std_og_mou_8'].median()\ntemp_df.plot.bar(ax=ax3)\n#set title and x,y labels\nax3.set_title('Churn vs STD Outgoing Call', fontsize=10)\nax3.set_xlabel(\"Churn\")\nax3.set_ylabel(\"STD Outgoing Call\")\n\n#pull out loc incoming t2f columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['loc_ic_t2f_mou_6', 'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8'].median()\ntemp_df.plot.bar(ax=ax4)\n#set title and x,y labels\nax4.set_title('Churn vs LOC incoming', fontsize=10)\nax4.set_xlabel(\"Churn\")\nax4.set_ylabel(\"LOC incoming\")\n\n#pull out loc incoming t2f columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', 'std_ic_t2m_mou_8'].median()\ntemp_df.plot.bar(ax=ax5)\n#set title and x,y labels\nax5.set_title('Churn vs STD incoming T2M', fontsize=10)\nax5.set_xlabel(\"Churn\")\nax5.set_ylabel(\"STD incoming T2M\")\n\n#pull out dasys since last recharge columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['days_since_last_rech','days_since_last_data_rech'].median()\ntemp_df.plot.bar(ax=ax6)\n#set title and x,y labels\nax6.set_title('Churn vs LOC incoming T2M', fontsize=10)\nax6.set_xlabel(\"Churn\")\nax6.set_ylabel(\"LOC incoming T2M\")\n\n#pull out ARPU columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['arpu_6', 'arpu_7', 'arpu_8'].median()\ntemp_df.plot.bar(ax=ax7)\n#set title and x,y labels\nax7.set_title('Churn vs ARPU', fontsize=10)\nax7.set_xlabel(\"Churn\")\nax7.set_ylabel(\"ARPU\")\n\n\n# #pull out Total data recahrge columns for month 6,7,8\n# temp_df = telecom_data.groupby(['churn'])['total_data_rech_6', 'total_data_rech_7', 'total_data_rech_8'].median()\n# temp_df.plot.bar(ax=ax8)\n# #set title and x,y labels\n# ax8.set_title('Churn vs Total data recahrge', fontsize=10)\n# ax8.set_xlabel(\"Churn\")\n# ax8.set_ylabel(\"Total data recahrge\")\n\n \n\n#pull out ARPU columns for month 6,7,8\ntemp_df = telecom_data.groupby(['churn'])['total_rech_num_6', 'total_rech_num_7', 'total_rech_num_8'].median()\ntemp_df.plot.bar(ax=ax8)\n#set title and x,y labels\nax8.set_title('Churn vs Total data recahrge', fontsize=10)\nax8.set_xlabel(\"Churn\")\nax8.set_ylabel(\"Total data recahrge\")\n\n# #pull out Total data recahrge columns for month 6,7,8\n# temp_df = telecom_data.groupby(['churn'])['max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8'].median()\n# temp_df.plot.bar(ax=ax10)\n# #set title and x,y labels\n# ax10.set_title('Churn vs Max data recahrge', fontsize=10)\n# ax10.set_xlabel(\"Churn\")\n# ax10.set_ylabel(\"Max data recahrge\")\n\n# temp_df = telecom_data.groupby(['churn'])['arpu_6', 'arpu_7', 'arpu_8'].median()\n# temp_df.plot.bar()\n# plt.title('Churn vs ARPU', fontsize=20)\n# plt.show()\n\n \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n \n- Max recharge is considerable reduced for Churners\n \n\n- Loc incoming T2F is never utilised by Churners\n \n\n- last day recharge amount decreases for CHurners over period of 3 months\n \n\n- Standard Outgoing MOU has a steep decrease from good phase to action phase which is good indicator of churners\n \n\n- STD incoming T2M also also decrease for chruners from month 6 to 8\n \n\n- If Days since Last data recharge is high then the possiblility of Churners will also be high\n \n\n- ARPU decreases over month 6 to 8 indicating the Churners\n \n\n- Total recharge number gradually decreases for CHurn customers","metadata":{}},{"cell_type":"markdown","source":"### Handling highly correlated Featues","metadata":{}},{"cell_type":"markdown","source":"lets check the correlation amongst the features, drop the highly correlated ones","metadata":{}},{"cell_type":"code","source":"# get correlation matrix\ncor = telecom_data.corr()\n\n# get the Lower triangle of an matrix. \ncor.loc[:,:] = np.tril(cor, k=-1)\n\n# stack the correlation array\ncor = cor.stack()\n\n# Filter the list which has correlation above |80| percent \ncorr_df = cor[(cor > 0.80) | (cor < -0.80)].sort_values(ascending=False).to_frame().reset_index().rename(columns={0:'corr'})\n\n# get the low correlated feature with Target between the multicollinearity variables, So to \n# prevent lossing a high information feature from being dropped in multicollenarity\ncorr_value = []\nfor index , row in corr_df.iterrows():\n    corr_value.append(row[1] if telecom_data[['churn',row[0]]].corr()[row[0]][0] > telecom_data[['churn',row[1]]].corr()[row[1]][0] else row[0])\n\n# concatingate to see the low correlated feature in the Low_churn_corr column\ncorr_df = pd.concat([corr_df,pd.Series(corr_value,name='low_churn_corr')],axis=1)\ncorr_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have now obtained the multicollinearlity features that can be dropped which has lower correlation with target variable","metadata":{}},{"cell_type":"code","source":"# dropping featuresfrom columns low_churn_corr\ntelecom_data.drop(columns=list(set(corr_df.low_churn_corr)),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"telecom_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE, ADASYN\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n\nfrom sklearn.decomposition import PCA,IncrementalPCA\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_curve,f1_score,auc\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SPLIT DATAFRAME AS X AND Y","metadata":{}},{"cell_type":"code","source":"# get target vaiable\ny = telecom_data.churn\n# get dependent features\nX = telecom_data.drop('churn',axis=1)\n\n# print shape of x and y\nprint(X.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SPLITTING DATA INTO TRAIN AND TEST DATA SET","metadata":{}},{"cell_type":"code","source":"# set random state value which can be used globally for models and data imputaion\nrandm_st=0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets split the data into train and test split.\n\nWe will stratify the target variable to make a equal class split balance between traina and test data","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=randm_st,stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rescaling of Features using StandardScaler","metadata":{}},{"cell_type":"code","source":"#initialising the scalar\nscaler=StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Applying StandardScaler on TRAIN DATASET","metadata":{}},{"cell_type":"code","source":"num_cols=list(X_train.columns)\n\n#apply fit and transform on train dataframe\nX_train[num_cols]=scaler.fit_transform(X_train[num_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Applying StandardScaler to TEST DATASET\n\nLet's apply Standard scaling on test dataset and **only transform the data**","metadata":{}},{"cell_type":"code","source":"#transform the test data with Scalar\nX_test[num_cols]=scaler.transform(X_test[num_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HANDLING CLASS IMBALANCE USING ADASYN","metadata":{}},{"cell_type":"markdown","source":"Since the class is imbalance we will use a techinque to handle the imbalance data whihc will increate the sample size of the number of churned customer \n\nThis can be done usinf serve method like over-sampling, SMOTE , ADASYN ... etc\n\nWe will use ADASYN as we assume that it works well with this data set","metadata":{}},{"cell_type":"code","source":"# initializing ADASYN\nadasyn = ADASYN(random_state=randm_st,n_jobs=-1)\n\n# Fit and resmapel the data to get balance classes\nX_train, y_train = adasyn.fit_resample(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now lets print the shape and see the size of our data set\nprint(X_train.shape)\nprint(y_train.shape)\nprint(\"\\n Churn and Non-Churn data count \\n\",y_train.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have now **balanced** the Churn data w.r.t no Non-churn","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection / Dimension Reduction","metadata":{}},{"cell_type":"markdown","source":"We have number of ways to handle feature selection for feeding the model varaibles\n\nWe have selcted few imputation methods \n\n    - RFE \n        > Reduce the number of columns to give feature number thorughh recursive feature elimination\n    - VIF / P-Value\n        > Adding columns one by one to logistic model and Filtering features based on the P - Value and VIF of a feature\n    - PCA\n        > Fedding PCA with the data set and combine uncorrelated columns to provide a valuabe infromation by shrinking the columns","metadata":{}},{"cell_type":"markdown","source":"## RFE\n#### Lets reduce the columns to 20","metadata":{}},{"cell_type":"code","source":"# define a logistic model\nlr = LogisticRegression()\n\n# running RFE with 20 variables as output\nrfe = RFE(lr, 20)\n\n# fit the data\nrfe = rfe.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we have the features that has been selected by RFE\nrfe_selected_columns = X.columns[rfe.support_]\n\n# view the selcted features\nprint(\"Features identified by RFE \", rfe_selected_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Test and Training data with RFE features selection","metadata":{}},{"cell_type":"code","source":"X_train_rfe = X_train[rfe_selected_columns.values.tolist()]\nX_test_rfe = X_test[rfe_selected_columns.values.tolist()]\nX_train_rfe.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VIF / P-Value","metadata":{}},{"cell_type":"code","source":"# Creating a method to get the top correlated features\ndef get_top_corr_features():\n    # add target column to find correlation\n    correlation_df = pd.concat([X_train,y_train],axis=1)\n    # creating a correlation list for the top features with SalesPrice\n    corr_cols = correlation_df.corr().loc[:,'churn'].sort_values(ascending=False)\n    corr_cols = corr_cols.reset_index()\n    # order the data with positive corr first and negative corr last\n    corr_cols = corr_cols[corr_cols.churn>0].append(corr_cols[corr_cols.churn<0])\n    return corr_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort the correlated features with top 15 and last 15 \ncorr_cols = get_top_corr_features().head(15).append(get_top_corr_features().tail(15)\\\n                                    .sort_values('churn',ascending=True))[1:].reset_index(drop=True)\\\n                                    .rename(columns={'index':'features','churn':'correlation'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_cols.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defina method to caluclate VIF\ndef VIF(X_train_rfe):\n    # create a dummy dataframe\n    vif = pd.DataFrame(columns=['Features','VIF'])\n    \n    # extract the column values to vif features column value\n    vif['Features'] = X_train_rfe.columns\n\n    # calculate vif for the train data for the added features\n    vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\n    \n    # round the value to 2 decimals\n    vif['VIF'] = round(vif['VIF'], 2)\n    \n    # sort values by hightevif value first\n    vif = vif.sort_values(by = \"VIF\", ascending = False).reset_index(drop=True)\n    \n    # print vif table\n    #display(vif)\n    \n    # retrun vif object\n    return vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building Model based on Logit (Logistic) model from statsmodels library","metadata":{}},{"cell_type":"code","source":"# this function can be resued to build ols model for given features\ndef build_model(X_train):\n    # adding a constant variable for intercept\n    X_train_vif = sm.add_constant(X_train)\n\n    # Initialize an OLS model for our dataset and fit the data to model\n    lm = sm.Logit(y_train,X_train).fit()\n\n    # view the summary of the model for selected features\n    #print(lm.summary())\n\n    return lm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have two way to build a model to find the best features selected by RFE that would fit.\n<ul><li>Dropping a feature one by one from the model built with 50 features until it shows good performane without overfitting</li><li>Adding a feature one by one to the model until it shows a good performance metrics</li></ul>\n\nLets add features one by one and build our model\n\nLets create a custom logic based on the below condition\n\n<table>\n<tr><th style=\"text-align:center\">Order</th><th style=\"text-align:center\">P-value</th><th style=\"text-align:center\">VIF</th><th style=\"text-align:center\">Action</th></tr>\n<tr><td>1</td><td>High</td><td>High</td><td>Drop these columns First</td></tr>\n<tr><td>2</td><td>High</td><td>Low</td><td>Drop these columns one by one, because this could lower the VIF values of other columns to prevent it from being dropped in next step </td></tr>\n<tr><td>3</td><td>Low</td><td>High</td><td>Drop the colums with VIF greater than 5</td></tr>\n<tr><td>4</td><td>Low</td><td>Low</td><td>Keep these features</td></tr>\n</table>\t","metadata":{}},{"cell_type":"code","source":"# deifne a method to make a incremental feature selection \n# by adding feature one by one and evaluating the logistic model\n# to check p value and vif value \ndef perform_feature_selection(train_data):\n    # create a empty data frame for xtrain and vif\n    X_train_vif = pd.DataFrame()\n    vif = pd.DataFrame()\n\n    # creating this object to ignore vif for a single feature\n    count = 1\n\n    # created this varible to stop the outer loop of adding futher features for model\n    stop = False\n\n    # prev r2score\n    r2score = 0.0\n        \n    for col in corr_cols.features:\n        print(\"adding :\"+col)\n        # add the column to the traing data set\n        if col in train_data.columns.values:\n            X_train_vif[col] = train_data[col]\n\n            # rebuild the model again to ckeck for high vifs and p-values \n            # once a feature is dropped on the above conditions after adding \n            # the new feature from the previous step to the model\n            while True:\n                # build the model\n                lm = build_model(X_train_vif)\n\n                # Drop the previous column if r2score doesn't increase\n                if round(r2score,3) == round(lm.prsquared,3):\n\n                    print(\"\\n\\n Dropping \"+X_train_vif.columns.values[-1]+\" and rebuilding the model as it did not add any info to model \\n\\n\")\n                    count = count - 1\n                    X_train_vif.drop(X_train_vif.columns.values[-1],axis=1, inplace=True)\n                    \n                    # build the model again as we have removed a feature \n                    lm = build_model(X_train_vif)\n\n                # Assign new r2score to check for the next build on adding new feature\n                r2score = lm.prsquared\n\n                # ignore vif and p-value check since there will\n                # be only 1 column on first iteration\n                if count != 1:\n\n                    # calculate VIF\n                    vif = VIF(X_train_vif)\n\n                    # if the model reaches required r2 score stop the model from executing furher steps\n                    if lm.prsquared >= 0.35:\n                        stop = True\n                        break\n\n                    # Check if the p-value if high\n                    if (lm.pvalues > 0.05).sum() > 0:\n\n                        # extract feature fo high p-value\n                        feature = lm.pvalues[lm.pvalues > 0.05].index\n\n                        # check if this feature is not const\n                        if feature[0] != 'const':\n\n                            # if the VIF value is aslo high drop this columns first\n                            if feature[0] in vif.loc[vif.VIF > 5,'Features']:\n                                X_train_vif.drop(feature[0],axis=1,inplace=True)                # order 1\n                            else:\n                                # if only the p-value is high drop it\n                                X_train_vif.drop(feature[0],axis=1,inplace=True)                # order 2\n\n                        # if the p-value column is 2nd in the list extract \n                        # that feature name to drop if from dataset if there is \n                        # a third value with high p-value it will be\n                        # validated in the next loop after rebuild on dropping the current feature\n                        elif (feature[0] == 'const') & (len(feature) > 1):\n                            X_train_vif.drop(feature[1],axis=1,inplace=True)                    # order 2\n\n                    # if VIF value is high drop it\n                    if ((vif.VIF > 5).sum() > 0) & (col in X_train_vif.columns.values):\n                        X_train_vif.drop(col,axis=1,inplace=True)   # order 3\n                    else:\n                        break                                                                   # order 4\n                else:\n                    break\n            # stop the process\n            if stop:\n                break\n\n            # increment count on adding new feature\n            count = count + 1\n            \n    return X_train_vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the Train and Test data based on VIF / P-Value feature selection","metadata":{}},{"cell_type":"code","source":"X_train_vif = perform_feature_selection(X_train);\nX_test_vif = X_test[X_train_vif.columns.tolist()];","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PCA\n#### Lets proceed with Dimensionality reduction using PCA","metadata":{}},{"cell_type":"code","source":"#lets initialise PCA\npca=PCA(svd_solver='randomized',random_state=randm_st)\n\n# fit the data to PCA\npca.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets do ahead with scree plot to decide the right number of features needed","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,7))\nplt.plot(np.cumsum(pca.explained_variance_ratio_),c=dark_green)\nplt.xlabel('No Of variables')\nplt.ylabel('Total Variance Explained')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Scree plot we can see that it needs 50 Components to explain 90% of variance.","metadata":{}},{"cell_type":"code","source":"#initialising PCA with 50 components\npca_inc=IncrementalPCA(n_components=50)\n\n#fit and transform the X_train DF\nX_train_pca=pca_inc.fit_transform(X_train)\n\n#Transform the Test DF\nX_test_pca=pca_inc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL BUILDING","metadata":{}},{"cell_type":"markdown","source":"#### LETS DEFINE SOME GENERIC METHODS","metadata":{}},{"cell_type":"code","source":"def fit_predict(classifier_model,params,mode=None):\n    #fit the pca train data\n    train_data = X_train if mode==None else eval(\"X_train_\"+mode)\n    test_data = X_test if mode==None else eval(\"X_test_\"+mode)\n    \n    # initialzie a StratifiedKFold \n    skf = StratifiedKFold(n_splits=3, shuffle = True, random_state = randm_st)\n    \n    model = GridSearchCV(estimator=classifier_model, param_grid=params, cv=skf, n_jobs=-1)\n    \n    model.fit(train_data,y_train)\n\n    #lets now predict on Test and Train data set\n    y_train_pred=model.predict(train_data)\n    y_test_pred=model.predict(test_data)\n    y_pred_proba=model.predict_proba(test_data)\n    \n    return y_train_pred,y_test_pred,y_pred_proba,model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Method to print Accuracy,Precision,recall and ROC score\nmodel_eval = {}\ndef calculate_metrics():\n\n    # initialize a dict\n    metric = {}\n\n    # calcuate Accuracy Score for Training data\n    metric['Train Accuracy'] = round(accuracy_score(y_train,y_train_pred)*100,2)\n    metric['Test Accuracy'] = round(accuracy_score(y_test,y_test_pred)*100,2)\n\n\n    # calculate Recall for Test data\n    metric['Train Recall'] = round(recall_score(y_train,y_train_pred)*100,2)\n    metric['Test Recall'] = round(recall_score(y_test,y_test_pred)*100,2)\n\n\n    # calcuate Precision for Test Data\n    metric['Train Precision'] = round(precision_score(y_train,y_train_pred)*100,2)\n    metric['Test Precision'] = round(precision_score(y_test,y_test_pred)*100,2)\n\n\n    # calcuate AUC for Test Data\n    metric['Train AUC'] = round(roc_auc_score(y_train,y_train_pred)*100,2)\n    metric['Test AUC'] = round(roc_auc_score(y_test,y_test_pred)*100,2)\n\n\n    # calcuate F1-Score for Test Data\n    metric['Train F1-Score'] = round(f1_score(y_train,y_train_pred)*100,2)\n    metric['Test F1-Score'] = round(f1_score(y_test,y_test_pred)*100,2)\n\n    return metric\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Method to plot the Confusion matrix\ndef confusion_matrix_plot(X_test_arg,classifier_arg):\n    \n    plt.style.use('default')\n    # create subplots\n    fig,(axes1,axes2)=plt.subplots(1,2,figsize=(10,5))\n    \n    # plot a confusion matrix for test data\n    plot_confusion_matrix(classifier_arg,X_test_arg,y_test,normalize='true',cmap=plt.cm.Blues,ax=axes1)\n    \n    # set title for the plot\n    axes1.set_title('Confusion Matrix')\n    axes2.set(xlabel='',ylabel='',xticklabels=[],yticklabels=[],xticks=[],yticks=[],title='Metrics')\n    axes2.text(0.1,0.1,'Accuracy Score for Train set: '+str(metric['Train Accuracy'])+'\\n\\nAccuracy Score for Test set: '+str(metric['Test Accuracy'])+'\\n\\nRecall Score for Train set: '+str(metric['Train Recall'])+'\\n\\nRecall Score for Test set: '+str(metric['Test Recall'])+'\\n\\nPrecision Score for Train set: '+str(metric['Train Precision'])+'\\n\\nPrecision Score for Test set: '+str(metric['Test Precision'])+'\\n\\nAUC Score for Train set: '+str(metric['Train AUC'])+'\\n\\nAUC Score for Test set: '+str(metric['Test AUC'])+'\\n\\nF1-Score for Train set: '+str(metric['Train F1-Score'])+'\\n\\nF1-Score for Test set: '+str(metric['Test F1-Score']))\n    # show plot\n    plt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def roc_curve_prec_curve(name):\n    \n    sns.set_style('darkgrid')\n    # create subplots\n    fig,(axes1,axes2) = plt.subplots(1,2,figsize=(10,5))\n    \n    # get the probability value\n    y_pred_prb =y_pred_proba[:,1]\n    \n    # plot roc curve for y_test and predicted probaility\n    fpr,tpr,thres=roc_curve(y_test,y_pred_prb)\n    axes1.plot([0,1],[0,1],'k--')\n    \n    # plt False Positive Rate and True Positive Rate\n    axes1.plot(fpr,tpr)\n    \n    # set labels and title for the subplot\n    axes1.set_xlabel('False positive Rate')\n    axes1.set_ylabel('True Positive rate')\n    axes1.set_title('ROC Curve')\n    \n    # plot precision recall curve\n    prec,recal,thres=precision_recall_curve(y_test,y_pred_prb)\n    \n    # plt False Positive Rate and True Positive Rate\n    axes2.plot(fpr,tpr)\n    \n    # set axis labels and title\n    axes2.set_xlabel('Recall')\n    axes2.set_ylabel('Precision')\n    axes2.set_title('Precision-Recall Curve')\n    \n    plt.show()\n    \n    # add the AUC Recall value as the final metric to model\n    auc_scr=round(auc(recal,prec)*100,2)\n    metric['AUC recall'] = auc_scr\n    model_eval[name] = metric","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### METRICS","metadata":{}},{"cell_type":"markdown","source":"`Accuracy score:` Gives the overall accuracy of the model for test/train data set\n\n`ROC Curve:` Compares true positive rate (TPR) and false positive rate (FPR) for different thresholds of class predictions \n\n`AUC (for ROC):` Measures the overall separability between classes of the model related to the ROC curve\n\n`Precision-Recall-Curve:` Compares false positive rate (FPR) and false negative rate (FNR) for different thresholds of class predictions. It is suitable for data sets with high class imbalances Precision and recall as does not dependent on the number of true negatives and thereby excludes the imbalance\n\n`F1 Score:` Builds the harmonic mean of precision and recall and thereby measures the compromise between both.\n\n`AUC (for PRC):` Measures the overall separability between classes of the model related to the Precision-Recall curve","metadata":{}},{"cell_type":"markdown","source":"## LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"# initialize hybper parameters\nhyperparameters = {\"penalty\":[\"l1\",\"l2\"]}\n\n#create an instance of linear regression\nlog_reg=LogisticRegression(class_weight='balanced', random_state=randm_st,n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Logistic model to dataset obtained through RFE\ny_train_pred,y_test_pred,y_pred_proba,log_model= fit_predict(log_reg,hyperparameters,'rfe')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_rfe,log_model)\n\n# plot a ROC cure and Precision-recall curve for the Logistic Model\nroc_curve_prec_curve(\"Logistic Regression RFE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Logistic model to dataset obtained through VIF\ny_train_pred,y_test_pred,y_pred_proba,log_model= fit_predict(log_reg,hyperparameters,'vif')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_vif,log_model)\n\n# plot a ROC cure and Precision-recall curve for the Logistic Model\nroc_curve_prec_curve(\"Logistic Regression VIF\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Logistic model to dataset obtained through PCA\ny_train_pred,y_test_pred,y_pred_proba,log_model= fit_predict(log_reg,hyperparameters,'pca')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_pca,log_model)\n\n# plot a ROC cure and Precision-recall curve for the Logistic Model\nroc_curve_prec_curve(\"Logistic Regression PCA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K NEAREST NEIGHBORS","metadata":{}},{"cell_type":"code","source":"#List Hyperparameters that we want to tune.\nleaf_size = [25,30,35]\nn_neighbors = [5,10,15]\n\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors)\n\n#create an instance of knn\nknn=KNeighborsClassifier(n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict KNN model to dataset obtained through RFE\ny_train_pred,y_test_pred,y_pred_proba,knn_model= fit_predict(knn,hyperparameters,'rfe')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_rfe,knn_model)\n\n# plot a ROC cure and Precision-recall curve for the KNN Model\nroc_curve_prec_curve(\"K Nearest Neighbours RFE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict KNN model to dataset obtained through VIF\ny_train_pred,y_test_pred,y_pred_proba,knn_model= fit_predict(knn,hyperparameters,'vif')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_vif,knn_model)\n\n# plot a ROC cure and Precision-recall curve for the KNN Model\nroc_curve_prec_curve(\"K Nearest Neighbours VIF\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict KNN model to dataset obtained through PCA\ny_train_pred,y_test_pred,y_pred_proba,knn_model= fit_predict(knn,hyperparameters,'pca')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_pca,knn_model)\n\n# plot a ROC cure and Precision-recall curve for the KNN Model\nroc_curve_prec_curve(\"K Nearest Neighbours PCA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DECISION TREE","metadata":{}},{"cell_type":"code","source":"#List Hyperparameters that we want to tune.\nmin_samples_split = [10,15,20]\nmin_samples_leaf = [5,10,15]\n\n#Convert to dictionary\nhyperparameters = dict(min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n\n#create an instance of decisiontree\ndec_tree=DecisionTreeClassifier(random_state=randm_st)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Decision Tree model to dataset obtained through RFE\ny_train_pred,y_test_pred,y_pred_proba,dec_tree_model= fit_predict(dec_tree,hyperparameters,'rfe')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_rfe,dec_tree_model)\n\n# plot a ROC cure and Precision-recall curve for the Decision Tree Model\nroc_curve_prec_curve(\"Decision Tree RFE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Decision Tree model to dataset obtained through VIF\ny_train_pred,y_test_pred,y_pred_proba,dec_tree_model= fit_predict(dec_tree,hyperparameters,'vif')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_vif,dec_tree_model)\n\n# plot a ROC cure and Precision-recall curve for the Decision Tree Model\nroc_curve_prec_curve(\"Decision Tree VIF\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Decision Tree model to dataset obtained through PCA\ny_train_pred,y_test_pred,y_pred_proba,dec_tree_model= fit_predict(dec_tree,hyperparameters,'pca')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_pca,dec_tree_model)\n\n# plot a ROC cure and Precision-recall curve for the Decision Tree Model\nroc_curve_prec_curve(\"Decision Tree PCA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"#List Hyperparameters that we want to tune.\nmin_samples_split = [10,15,20]\nmin_samples_leaf = [5,10,15]\nn_estimators= [50,100,150]\n\n#Convert to dictionary\nhyperparameters = dict(min_samples_split=min_samples_split, \\\n                       min_samples_leaf=min_samples_leaf,\\\n                       n_estimators=n_estimators)\n\n#create an instance of randomforest\nrandm_frst=RandomForestClassifier(random_state=randm_st,n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Random Forest model to dataset obtained through RFE\ny_train_pred,y_test_pred,y_pred_proba,randm_frst_model= fit_predict(randm_frst,hyperparameters,'rfe')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_rfe,randm_frst_model)\n\n# plot a ROC cure and Precision-recall curve for the Random Forest Model\nroc_curve_prec_curve(\"Random Forest RFE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Random Forest model to dataset obtained through VIF\ny_train_pred,y_test_pred,y_pred_proba,randm_frst_model= fit_predict(randm_frst,hyperparameters,'vif')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_vif,randm_frst_model)\n\n# plot a ROC cure and Precision-recall curve for the Random Forest Model\nroc_curve_prec_curve(\"Random Forest VIF\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Random Forest model to dataset obtained through PCA\ny_train_pred,y_test_pred,y_pred_proba,randm_frst_model= fit_predict(randm_frst,hyperparameters,'pca')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_pca,randm_frst_model)\n\n# plot a ROC cure and Precision-recall curve for the Random Forest Model\nroc_curve_prec_curve(\"Random Forest PCA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRADIENT BOOSTING","metadata":{}},{"cell_type":"code","source":"#List Hyperparameters that we want to tune.\nn_estimators= [50,100,150]\nlearning_rate = [0.1,0.5,1]\nmax_depth = [2,3,4]\n\n#Convert to dictionary\nhyperparameters = dict(n_estimators=n_estimators,\\\n                      max_depth=max_depth,\\\n                      learning_rate=learning_rate)\n\ngrd_bst = GradientBoostingClassifier(random_state=randm_st)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Gradient Boost model to dataset obtained through RFE\ny_train_pred,y_test_pred,y_pred_proba,grd_bst_model= fit_predict(grd_bst,hyperparameters,'rfe')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_rfe,grd_bst_model)\n\n# plot a ROC cure and Precision-recall curve for the Gradient Boost Model\nroc_curve_prec_curve(\"Gradient Boost RFE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Gradient Boost model to dataset obtained through VIF\ny_train_pred,y_test_pred,y_pred_proba,grd_bst_model= fit_predict(grd_bst,hyperparameters,'vif')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_vif,grd_bst_model)\n\n# plot a ROC cure and Precision-recall curve for the Gradient Boost Model\nroc_curve_prec_curve(\"Gradient Boost VIF\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict Gradient Boost model to dataset obtained through PCA\ny_train_pred,y_test_pred,y_pred_proba,grd_bst_model= fit_predict(grd_bst,hyperparameters,'pca')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_pca,grd_bst_model)\n\n# plot a ROC cure and Precision-recall curve for the Gradient Boost Model\nroc_curve_prec_curve(\"Gradient Boost PCA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBOOSTING","metadata":{}},{"cell_type":"code","source":"hyperparameters = {  \"learning_rate\"    : [0.05, 0.1, 0.15],\n                     \"max_depth\"        : [3, 4, 5],\n                     \"gamma\"            : [0.0, 0.2 ,0.4],\n                     'eval_metric'      : ['auc'],\n                  }\n\nxg_bst = XGBClassifier(random_state=randm_st,n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict XGBoost model to dataset obtained through RFE\ny_train_pred,y_test_pred,y_pred_proba,xg_bst_model= fit_predict(xg_bst,hyperparameters,'rfe')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_rfe,xg_bst_model)\n\n# plot a ROC cure and Precision-recall curve for the XGBoost Model\nroc_curve_prec_curve(\"XGBoost RFE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict XGBoost model to dataset obtained through VIF\ny_train_pred,y_test_pred,y_pred_proba,xg_bst_model= fit_predict(xg_bst,hyperparameters,'vif')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_vif,xg_bst_model)\n\n# plot a ROC cure and Precision-recall curve for the XGBoost Model\nroc_curve_prec_curve(\"XGBoost VIF\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and predict XGBoost model to dataset obtained through PCA\ny_train_pred,y_test_pred,y_pred_proba,xg_bst_model= fit_predict(xg_bst,hyperparameters,'pca')\n\n# Lets evaluate the model by caltulating all the metrics\nmetric=calculate_metrics()\n\n# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_pca,xg_bst_model)\n\n# plot a ROC cure and Precision-recall curve for the XGBoost Model\nroc_curve_prec_curve(\"XGBoost PCA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modal Summary\n\n","metadata":{}},{"cell_type":"markdown","source":"##### Creating a Dataframe Table with all the metrics from different models","metadata":{}},{"cell_type":"code","source":"# Convert the model summary dictionray to DataFrame\nmodel_summary = pd.DataFrame(model_eval).T\nmodel_summary.style.set_properties(\n    subset=['Test Recall'], \n    **{'font-weight': 'bold'}\n).highlight_max(subset=['Test Recall'], color = 'lightgreen',axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We could see that KNN outperforms other models and has a good recall score, Hence Lets take this as our final Model.**\n\nNow that we have identified the final model, lets try different hyperparameters in KNN to see if it improves the model further","metadata":{}},{"cell_type":"markdown","source":"### MODEL EVALUATION","metadata":{}},{"cell_type":"markdown","source":"#### HYPERPARAMETER TUNING for the best Model : Logistic Regression RFE","metadata":{}},{"cell_type":"code","source":"#List Hyperparameters \npenalty = ['l1', 'l2']\nC = [0.001,0.01,0.1,1,10,100]\nmax_iter = [100,500,1000]\n#convert it to dictionary\nhyperparameters = dict(penalty=penalty, C=C, max_iter=max_iter)\n\n#create an instance of linear regression\nlog_reg=LogisticRegression(class_weight='balanced', random_state=randm_st,n_jobs=-1)\n\n# set number of folds to 5\nsplit_val=5\n\n# initialzie a StratifiedKFold \nskf = StratifiedKFold(n_splits=split_val, shuffle = True, random_state = randm_st)\n\n# Create a Grid Search CV and making use of already created logistic object\nlog_grid=GridSearchCV(estimator=log_reg,param_grid=hyperparameters,cv=skf,scoring='recall', verbose=True)\n\n# fit data obtained through RFE \nlog_grid.fit(X_train_rfe,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the grid results\nprint('\\n Best estimator:')\nprint(log_grid.best_estimator_)\nprint('\\n Best score:')\nprint(log_grid.best_score_)\nprint('\\n Best parameters:')\nprint(log_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets now predict on Test and Train data set\ny_train_pred=log_grid.predict(X_train_rfe)\ny_test_pred=log_grid.predict(X_test_rfe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets evaluate the model by caltulating all the metrics\nmetric = calculate_metrics()\nmetric","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot a confusion matirx for model \nconfusion_matrix_plot(X_test_rfe,log_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot a ROC cure and Precision-recall curve for the Logistic RFE Model\nroc_curve_prec_curve(\"Best Estimator : Logistic RFE hypertuned\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Final Model produces high Recall Score and good Accuracy as well, hence we recommend this model**\n\n**Now that we have finalised the Model, Lets also make sure we have identify the Probability Cut-off and see if it can be utilised**","metadata":{}},{"cell_type":"markdown","source":"We know that the **default Cut-off value is 0.5**, lets see if the same can be used or we could find a better Cut-off with High Specificity","metadata":{}},{"cell_type":"markdown","source":"### OPTIMAL PROBABILTY CUTOFF CALCULATION","metadata":{}},{"cell_type":"code","source":"y_pred_proba = log_grid.best_estimator_.predict_proba(X_test_rfe)\n\ny_pred_df= pd.DataFrame({'actual_churn':y_test, 'churn_Prob':y_pred_proba[:,1]})\n# Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\n\n# create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_pred_df[i]= y_pred_df['churn_Prob'].map( lambda x: 1 if x > i else 0)\n\n    \n#y_pred_df['final_predicted'] = y_pred_df.churn_Prob.map( lambda x: 1 if x > prob else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets calculate the accuracy,sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = confusion_matrix( y_pred_df['actual_churn'], y_pred_df[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    sensi = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    speci = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We see 0.7 is giving better sensitivity, lets try using 0.7 as cut-off**","metadata":{}},{"cell_type":"code","source":"y_pred_df['final_predicted'] = y_pred_df['churn_Prob'].map( lambda x: 1 if x > 0.7 else 0)\ny_test_pred=y_pred_df['final_predicted']\n\n# Confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_df['final_predicted']).ravel()\nprint('\\nTN = {0}, FP = {1}, FN = {2}, TP = {3}\\n\\n'.format(tn, fp, fn, tp))\n\n# calcuate Accuracy Score for Training data\nprint(\"Test Accuracy : \"+str(round(accuracy_score(y_test,y_test_pred)*100,2)))\n\n\n# calculate Recall for Test data\nprint(\"Test Recall : \"+str(round(recall_score(y_test,y_test_pred)*100,2)))\n\n\n# calcuate Precision for Test Data\nprint(\"Test Precision : \"+str(round(precision_score(y_test,y_test_pred)*100,2)))\n\n\n# calcuate AUC for Test Data\nprint(\"Test ROC : \"+str(round(roc_auc_score(y_test,y_test_pred)*100,2)))\n\n\n# calcuate F1-Score for Test Data\nprint(\"Test F1-Score : \"+str(round(f1_score(y_test,y_test_pred)*100,2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_df['final_predicted'] = y_pred_df['churn_Prob'].map( lambda x: 1 if x > 0.6 else 0)\ny_test_pred=y_pred_df['final_predicted']\n# Confusion matrix\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_df['final_predicted']).ravel()\nprint('\\nTN = {0}, FP = {1}, FN = {2}, TP = {3}\\n\\n'.format(tn, fp, fn, tp))\n\n# calcuate Accuracy Score for Training data\nprint(\"Test Accuracy : \"+str(round(accuracy_score(y_test,y_test_pred)*100,2)))\n\n\n# calculate Recall for Test data\nprint(\"Test Recall : \"+str(round(recall_score(y_test,y_test_pred)*100,2)))\n\n\n# calcuate Precision for Test Data\nprint(\"Test Precision : \"+str(round(precision_score(y_test,y_test_pred)*100,2)))\n\n\n# calcuate AUC for Test Data\nprint(\"Test ROC : \"+str(round(roc_auc_score(y_test,y_test_pred)*100,2)))\n\n\n# calcuate F1-Score for Test Data\nprint(\"Test F1-Score : \"+str(round(f1_score(y_test,y_test_pred)*100,2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We See that with **0.6, the No of Churn customers predicted as Non-Churn (FN) increases and Hence we can stick to default cut-off of 0.9 which has lesser FN**","metadata":{}},{"cell_type":"markdown","source":"## 2. FEATURE IMPORTANCE","metadata":{}},{"cell_type":"markdown","source":"As we are trying to figure out the best features, we can no longer use PCA and hence we use normal train and test data","metadata":{}},{"cell_type":"code","source":"#create an instance of randomforest\nrandm_frst=RandomForestClassifier(random_state=randm_st,n_jobs=-1)\n\n#fit the pca train data\nrandm_frst.fit(X_train,y_train)\n\n#lets now predict on Test data set\ny_test_pred=randm_frst.predict(X_test)\n\n# get the prediction probability\ny_pred_proba=randm_frst.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot confusion matrix\nconfusion_matrix_plot(X_test,randm_frst)\n\n# Lets evaluate the model by caltulating all the metrics\nmetric = calculate_metrics()\n\n# plot a ROC cure and Precision-recall curve for the XGBoost Model\nroc_curve_prec_curve(\"Random Forest without RFE/VIF/PCA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp_df = pd.DataFrame({'Feature':telecom_data.drop(['churn'],axis=1).columns, 'Score':randm_frst.feature_importances_})\n\n# Order the features by max score\nfeature_imp_df = feature_imp_df.sort_values('Score', ascending=False).reset_index()\n\n#Taking absolute to see the co-eff weightage\nfeature_imp_df['Abs_Coefficient']=feature_imp_df['Score'].apply(lambda x:np.abs(x))\n\n#sorting based on aboslute co-eff value\nfeature_imp_df=feature_imp_df.sort_values(by=['Abs_Coefficient'], ascending=False)\n\nfeature_imp_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preivew the impotant features\nfeature_imp_df.loc[:10,'Feature'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set figure size\nplt.figure(figsize=(12,6))\n\n# create a horizaontal barplot showing the features importance\nsns.barplot(y='Feature',x='Abs_Coefficient',data=feature_imp_df[:10])\n\n# show plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fetch the top 10 features\nfeature_imp_df_top_10 = feature_imp_df.loc[:10,:]\n\n# normalize the feature to 100% ratio\nfeature_imp_df_top_10.Abs_Coefficient = (feature_imp_df_top_10.Abs_Coefficient)/max(feature_imp_df_top_10.Abs_Coefficient)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize a color map\ncmap = plt.get_cmap('Spectral')\n\n# extract 10 colors for spectral space \ncolors = [cmap(i) for i in np.linspace(0, 1, 9)]\n\n# set figure size\nplt.figure(figsize=(6,6))\n\n# plot a pie chart for top 10 important features\nplt.pie(feature_imp_df_top_10.Abs_Coefficient, labels=feature_imp_df_top_10['Feature'], autopct='%1.2f%%', shadow=False, colors=colors)\n\n# set title\nplt.title('Top 10 Best Predictors')\n\n# set axis as equal\nplt.axis('equal')\n\n# show plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below are the top 10 features which are high indicators of Churning\n \n`1.Days since last recharge:`\n \n- If Days since Last recharge is high then the possiblility of user churning will also be high\n \n`2.last_day_rch_amt_8:`\n \n- Users wanting to churn have lower last day recharge amount over period of 3 months\n \n`3.Max Recharge:`\n \n- lower the Max recharge amount, higher the churning chances\n \n`4.Arpu:`\n \n- ARPU decreases over month 6 to 8 indicating the higher chance of churning\n \n`5.Total data recharge` and `6.Max Recharge:`\n \n- Total recharge number and Max recharge gradually decreases from month 6 to 8 indicating high probability for customers to churn\n \n`7. Local Incoming T2F:`\n \n- If this Feature is not used by user in good and action phase then chancces of user to churn is very high\n \n`8. Days since last data recharge:`\n \n- Higher the last data recharge data, higher the possiblity of churning\n \n`9.STD Incoming T2M` and `10. STD Outgoing MOU:`\n \n- Decrease in Std incoming T2m/ STD outgoing MOU shows possiblity of user turning into a churner","metadata":{}},{"cell_type":"markdown","source":"## 3. Recommendation Strategy","metadata":{}},{"cell_type":"markdown","source":"As we could see the most important columns are from the 8th Month which decides if the Customer will churn or not.\n\n \n\n- If the days since last recharge goes beyond 60 days, then the user can be provided with a **recharge discount to attract back the recharge** \n\n \n\n\n- **Providing free caller tunes and free roaming to users having less or no Local Incoming T2F/STD Incoming T2M** can attract users to get back to the network\n\n \n\n\n- Users can be provided with a **additional data coupons if they havent recharged for more than 60 days**\n\n \n\n\n- Providing **additional talk time to users with steeply reduced Outgoing MOU** will a good chance to opt them out of churning\n\n \n\n\n- Continuous Monitoring of High valued ARPU users, with high inactivity, the **Customer service can call the users to check for issues and provide better offers if the user raises a complaint/feedback**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}