{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport seaborn as sns\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n%matplotlib inline\n\nfrom numpy.random import seed\nimport tensorflow\nfrom keras.layers import Input, Dropout\nfrom keras.layers.core import Dense \nfrom keras.models import Model, Sequential, load_model\nfrom keras import regularizers\nfrom keras.models import model_from_json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('../input/nasa-bearing-dataset-aggregated-sets-no-1-2-3'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset preprocessing","metadata":{}},{"cell_type":"code","source":"# Read the CSV file and set first column as the dataframe index\ndataset = pd.read_csv(\"../input/nasa-bearing-dataset-aggregated-sets-no-1-2-3/merged_dataset_BearingTest_1.csv\", index_col=0)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normaliza data","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\n# Decide on what normalizer function to use\n## https://www.geeksforgeeks.org/standardscaler-minmaxscaler-and-robustscaler-techniques-ml\nscaler = preprocessing.MinMaxScaler() # scales all the data features in the range [0, 1] or if there are negative values to [-1, 1] \n#scaler = preprocessing.StandardScaler() # It follows Standard Normal Distribution (SND). Therefore, it makes mean = 0 and scales the data to unit variance\n\n# If you needed to operate in the whole dataset, you could apply normalization to the full time series\n#X_all = scaler.fit_transform(dataset)\n#X_all = pd.DataFrame(dataset)\n#X_all.columns = dataset.columns\n\n# Dataset is scaled so that maximum for every column is 1\ndataset_scaled = pd.DataFrame(scaler.fit_transform(dataset), \n                              columns=dataset.columns, \n                              index=dataset.index)\ndataset_scaled.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split into training and test datasets\n- We want the training set contains only \"normal\" data\n- The rest of points will be in the test set, that will contain both \"normal\" and anomalous data","metadata":{}},{"cell_type":"code","source":"print(\"dataset_scaled shape is\",dataset_scaled.shape,\"\\n\\n\", dataset_scaled.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will split into training and test sets:\n \n - The **training set** corresponds to the first part of the time serie (25% approximately), where bearing status is healthy\n     - It will train the **Autoencoder model**\n     - So the training step will provide with the **baseline** that we will use to flag anomalies later\n     \n - The **test set** covers the remaining 75% of the of the serie (right part)\n     - We will apply on it the threshold value provided by the autoencoder model (baseline)\n     - Then we will flag as anomalous every point whose score is above the threshold","metadata":{}},{"cell_type":"code","source":"# Split baseline and analysis set with a ratio 1:3 (25% : 75%)\nrow_slice = round( 0.25*dataset_scaled.shape[0] )\nindex_slice = dataset_scaled.index[row_slice]\nindex_slice_ = dataset_scaled.index[row_slice + 1]\n\nprint(\"dataset_scaled shape is\",dataset_scaled.shape,\"and will be slice at timestamp\", index_slice)\nprint(\"Analysis set will start at timestamp\", index_slice_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = dataset_scaled[:index_slice]\nX_test  = dataset_scaled[index_slice_:]\n# Random shuffle training data\nX_train.sample(frac=1)\n\nprint(\"Train dataset has lenght\", X_train.shape[0], \"while test dataset is\", X_test.shape[0],\n      \"\\nIn TOTAL there are\", X_train.shape[0]+X_test.shape[0],\"rows\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ticks_span = 50\n\nX_train.plot(figsize = (6,6), title ='Left time series with \"normal\" data (normalized signals)')\nplt.xticks(np.arange(0, X_train.shape[0], x_ticks_span), fontsize=10, rotation = 30)\nplt.ylim(0,1)\nplt.legend(loc=\"upper left\")  \nplt.show()\n\nX_test.plot(figsize = (18,6), title='Right time series with \"normal\" & \"anomalous\" data (normalized signals)')\nplt.xticks(np.arange(0, X_test.shape[0], x_ticks_span), fontsize=10, rotation = 30)\nplt.ylim(0,1)\nplt.legend(loc=\"upper left\")  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## All components (PCA) just for visualization purposes\nTraining of the model will use the 4 bearings data, without PCA dimensional reduction.\n\nIn fact, the Autoencoder model will have a central (hidden) layer with two nodes (that play the role of the 2 PCA components). Neural Network have the advantage of being able to deal with both linear & non linear models. ","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(8)\nx_pca = pca.fit_transform(X_train)\nx_pca = pd.DataFrame(x_pca)\nx_pca.columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8']\n\n# Plot\nplt.scatter(x_pca['PC1'], x_pca['PC2'])\nplt.title('Training dataset projected onto 2 Principal Components')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Explained variance of each PCA component","metadata":{}},{"cell_type":"code","source":"np.set_printoptions(precision=3, suppress=True) # 3 decimal places and don't use scientific\nprint(pca.explained_variance_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build autoencoder model\n\nWe don't need to apply dimensional reduction, it's done by the Autoencoder model (central layer of two nodes in the Neural Network are the equivalent to the 2 Principal Components)","metadata":{}},{"cell_type":"code","source":"seed(10)\ntensorflow.random.set_seed(10)\nact_func = 'elu'\n\n# Input layer:\nmodel=Sequential()\n# First hidden layer, connected to input vector X. \nmodel.add(Dense(2,activation=act_func,\n                kernel_initializer='glorot_uniform',\n                kernel_regularizer=regularizers.l2(0.0),\n                input_shape=(X_train.shape[1],)\n               )\n         )\n\nmodel.add(Dense(1,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\nmodel.add(Dense(2,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\nmodel.add(Dense(X_train.shape[1],\n                kernel_initializer='glorot_uniform'))\n\nmodel.compile(loss='mse',optimizer='adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model for 30 epochs, batch size of 30: \nNUM_EPOCHS=30\nBATCH_SIZE=10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import model_to_dot\nfrom IPython.display import SVG\nSVG(model_to_dot(model, dpi=70, show_shapes=True, show_layer_names=True, rankdir='TB').create(prog='dot', format='svg'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting the model\nTo keep track of the accuracy during training, we use 5% of the training data for validation after each epoch (validation_split = 0.05)","metadata":{}},{"cell_type":"code","source":"history=model.fit(np.array(X_train),np.array(X_train),\n                  batch_size=BATCH_SIZE, \n                  epochs=NUM_EPOCHS,\n                  validation_split=0.05,\n                  verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model: validation vs. training loss","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['loss'],\n         'b',\n         label='Training loss')\nplt.plot(history.history['val_loss'],\n         'r',\n         label='Validation loss')\nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Loss, [mse]')\n#plt.ylim([0,.1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of loss function in the training set\nBy plotting the distribution of the calculated loss in the training set, one can use this to identify a suitable threshold value for identifying an anomaly.\n\nIn doing this, one can make sure that this threshold is set above the “noise level”, and that any flagged anomalies should be statistically significant above the noise background.","metadata":{}},{"cell_type":"code","source":"X_pred = model.predict(np.array(X_train))\nX_pred = pd.DataFrame(X_pred, \n                      columns=X_train.columns)\nX_pred.index = X_train.index\n\nscored = pd.DataFrame(index=X_train.index)\nscored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\nscored.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nsns.distplot(scored['Loss_mae'],\n             bins = 40, \n             kde= True,\n            color = 'blue');\nplt.xlim([0.0,.1])\nplt.ylim([0,10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above loss distribution, let us try a threshold of 0.08 for flagging an anomaly.","metadata":{}},{"cell_type":"code","source":"threshold = 0.06","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" We can then calculate the loss in the test set, to check when the output crosses the anomaly threshold.","metadata":{}},{"cell_type":"code","source":"X_pred = model.predict(np.array(X_test))\nX_pred = pd.DataFrame(X_pred, \n                      columns=X_test.columns)\nX_pred.index = X_test.index\n\nscored = pd.DataFrame(index=X_test.index)\nscored['Loss_mae'] = np.mean(np.abs(X_pred-X_test), axis = 1)\nscored['Threshold'] = threshold\nscored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\nscored.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then calculate the same metrics also for the training set, and merge all data in a single dataframe:","metadata":{}},{"cell_type":"code","source":"X_pred_train = model.predict(np.array(X_train))\nX_pred_train = pd.DataFrame(X_pred_train, \n                      columns=X_train.columns)\nX_pred_train.index = X_train.index\n\nscored_train = pd.DataFrame(index=X_train.index)\nscored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\nscored_train['Threshold'] = threshold\nscored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n\nscored = pd.concat([scored_train, scored])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An outlier is a point that is distant from others, so the **score** value can be understood *as a distance*. Let's add a column in the training set to flag the anomalies.","metadata":{}},{"cell_type":"markdown","source":"# Results from Autoencoder model\nHaving calculated the loss distribution and the anomaly threshold, we can visualize the model output in the time leading up to the bearing failure:","metadata":{}},{"cell_type":"code","source":"scored.plot(logy=True,  figsize = (18,6), ylim = [1e-3,1e2], color = ['blue','red'])\nplt.xticks(np.arange(0, scored.shape[0], 50), fontsize=10, rotation = 30)\n#plt.gca().grid(True)\nplt.ylabel('Reconstruction error')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n- The Autoencoder model performs as well as the PCA + Mahalanobis distance model.\nThe dataset is not complex enough as to find an improved result if using a neural network.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}