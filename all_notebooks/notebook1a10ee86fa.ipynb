{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\n#Loading data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.iloc[:,:32]\ndf.shape\n# to remove Unamed column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['diagnosis'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['diagnosis']= df['diagnosis'].replace('M', 1)\ndf['diagnosis']= df['diagnosis'].replace('B', 0)\n# to convert target variable to number","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nfigure=sns.heatmap(df.corr(),annot=True,cmap='cubehelix_r')\n#plt.figure(figsize(15,10))\nplt.show()\n# to get correlation between different columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('diagnosis', axis=1).corrwith(df.diagnosis).plot(kind='bar', grid=True, figsize=(12, 10), title=\"Correlation with target\",color=\"green\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr=df.corr()\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.98:\n            if columns[j]:\n                columns[j] = False\nselected_columns = df.columns[columns]\ndf = df[selected_columns]\ndf.shape\n# to only take one column between the two columns which have strong correlation with each other\n# for example radius, perimeter and area are dependent on each other","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr1=df.corr()\ncorr1[abs(corr1['diagnosis']) > 0.59].index\n# to only take those columns which have good correlation with target column\n# this works better than taking all columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test=train_test_split(df,test_size=0.2)\ntrain_X = train[['radius_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'radius_worst', 'compactness_worst',\n       'concavity_worst', 'concave points_worst']]# taking the training data features\ntrain_y=train.diagnosis# output of our training data\ntest_X= test[['radius_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'radius_worst', 'compactness_worst',\n       'concavity_worst', 'concave points_worst']] # taking test data features\ntest_y =test.diagnosis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVC()\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_index=list(range(1,16))\na=pd.Series()\nx=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\nfor i in list(range(1,16)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(accuracy_score(prediction,test_y)))\nplt.plot(a_index, a)\nplt.xticks(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AdaBoostClassifier()\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CatBoostClassifier()\nmodel.fit(train_X,train_y) \nprediction=model.predict(test_X) \nprint('The accuracy is:',accuracy_score(prediction,test_y))\nprint(\"The classification report is\")\nprint(classification_report(prediction,test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}