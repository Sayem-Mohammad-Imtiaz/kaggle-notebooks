{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tutorial: Machine Learning with Text in scikit-learn\n\n***\n### **I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES</b></font> would be highly appreciated**\n***\n"},{"metadata":{},"cell_type":"markdown","source":"## Agenda\n\n1. Representing text as numerical data\n2. Reading a text-based dataset into pandas\n3. Vectorizing our dataset\n4. Building and evaluating a model\n5. Comparing models\n6. Examining a model for further insight\n7. Practicing this workflow on another dataset\n8. Tuning the vectorizer (discussion)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Representing text as numerical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# example text for model training (SMS messages)\nsimple_train = ['call you tonight', 'Call me a cab', 'Please call me... PLEASE!']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n\n> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n\nWe will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import and instantiate CountVectorizer (with the default parameters)\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn the 'vocabulary' of the training data (occurs in-place)\nvect.fit(simple_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the fitted vocabulary\nvect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(simple_train)\nsimple_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert sparse matrix to a dense matrix\nsimple_train_dtm.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\npd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n\n> In this scheme, features and samples are defined as follows:\n\n> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n\n> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n\n> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the type of the document-term matrix\ntype(simple_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the sparse matrix contents\nprint(simple_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n\n> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n\n> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n\n> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."},{"metadata":{"trusted":true},"cell_type":"code","source":"# example text for model testing\nsimple_test = [\"please don't call me\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform testing data into a document-term matrix (using existing vocabulary)\nsimple_test_dtm = vect.transform(simple_test)\nsimple_test_dtm.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\npd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary:**\n\n- `vect.fit(train)` **learns the vocabulary** of the training data\n\n- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n\n- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data (and **ignores tokens** it hasn't seen before)"},{"metadata":{},"cell_type":"markdown","source":"# 2. Reading a text-based dataset into pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read file into pandas using a relative path\n# sms = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', delimiter='\\t', names=['label', 'message'])\nsms = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.dropna(how=\"any\", inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.columns = ['label', 'message']\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the shape\nsms.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the class distribution\nsms.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert label to a numerical variable\nsms['label_num'] = sms.label.map({'ham':0, 'spam':1})\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\nX = sms.message\ny = sms.label_num\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split X and y into training and testing sets \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Vectorizing our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the vectorizer\nvect = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn training data vocabulary, then use it to create a document-term matrix\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# equivalently: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the document-term matrix\nX_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Building and evaluating a model\n\nWe will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n\n> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import and instantiate a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n%time nb.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for false positives (ham incorrectly classifier)\n# X_test[(y_pred_class==1) & (y_test==0)]\nX_test[y_pred_class > y_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for false negatives (spam incorrectly classifier)\nX_test[y_pred_class < y_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of false negative \nX_test[3132]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm (poorly calibrated)\ny_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Caomparing models\n\nWe will compare multinomial Naive Bayes with [logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression):\n\n> Logistic regression, despite its name, is a **linear model for classification** rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import an instantiate a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model using X_train_dtm\n%time logreg.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\ny_pred_class = logreg.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm (well calibrated)\ny_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Examining a model for further insight\n\nWe will examine the our **trained Naive Bayes model** to calculate the approximate **\"spamminess\" of each token**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# store the vocabulary of X_train\nX_train_tokens = vect.get_feature_names()\nlen(X_train_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the first 50 tokens\nprint(X_train_tokens[:50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the last 50 tokens\nprint(X_train_tokens[-50:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes counts the number of times each token appears in each class\nnb.feature_count_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rows represent classes, columns represent tokens\nnb.feature_count_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of times each tokens appears across all HAM meassages\nham_token_count = nb.feature_count_[0, :]\nham_token_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of times each tokens appears across all SPAM meassages\nspam_token_count = nb.feature_count_[1, :]\nspam_token_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a DataFrame of tokens with their separate ham and spam counts\ntokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, \n                       'spam':spam_token_count}).set_index('token')\ntokens.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine 5 random DataFrame rows\ntokens.sample(5, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes counts the number of observations in each class \nnb.class_count_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we can calculate the \"spamminess\" of each token, we need to avoid **dividing by zero** and account for the **class imbalance**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# add 1 to ham and spam counts to avoid dividing by 0\ntokens['ham'] = tokens.ham + 1\ntokens['spam'] = tokens.spam + 1\ntokens.sample(5, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the ham and spam counts into frequencies\ntokens['ham'] = tokens.ham / nb.class_count_[0]\ntokens['spam'] = tokens.spam / nb.class_count_[1]\ntokens.sample(5, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the ratio of spam-to-ham for each token\ntokens['spam_ratio'] = tokens.spam / tokens.ham\ntokens.sample(5, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the DataFrame sorted by spam_ratio\ntokens.sort_values(by='spam_ratio', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look up spam_ratio for a given token\ntokens.loc['dating', 'spam_ratio']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Tuning the vectorizer\n\nThus far, we have been using the default parameters of [CountVectorizer:](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# show default parameters for CountVectorizer\nvect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n\n- **stop_words**: string {'english'}, list, or None (default)\n    - If 'english', a built-in stop word list for English is used.\n    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n    - If None, no stop words will be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove English stop words\nvect = CountVectorizer(stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **ngram_range**: tuple (min_n, max_n), default=(1, 1)\n    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n    - All values of n such that min_n <= n <= max_n will be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"# include 1-grams and 2-grams\nvect = CountVectorizer(ngram_range=(1, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **max_df**: float in range [0.0, 1.0] or int, default=1.0\n    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n    - If float, the parameter represents a proportion of documents.\n    - If integer, the parameter represents an absolute count."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ignore terms that appear in more than 50% of the documents\nvect = CountVectorizer(max_df=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **min_df**: float in range [0.0, 1.0] or int, default=1\n    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n    - If float, the parameter represents a proportion of documents.\n    - If integer, the parameter represents an absolute count."},{"metadata":{"trusted":true},"cell_type":"code","source":"# only keep terms that appear in at least 2 documents\nvect = CountVectorizer(min_df=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Guidelines for tuning CountVectorizer**:\n\n    - Use your knowledge of the problem and the text, and your understanding of the tuning parameters, to help you decide what parameters to tune and how to tune them.\n    - Experiment, and let the data tell you the best approach!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}