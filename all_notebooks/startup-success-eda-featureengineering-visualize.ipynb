{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/startup-success-prediction/startup data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n\n# We have 49 columns and 923 rows, some columns have missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values\n\ndf.isnull().sum().sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at percentage of missing values\n\nprint(\"Percentage of missing values in 'closed_at' column: % {:.2f}\". format((df.closed_at.isnull().sum())/len(df)*100))\nprint(\"Percentage of missing values in 'Unnamed: 6' column: % {:.2f}\". format((df[\"Unnamed: 6\"].isnull().sum())/len(df)*100))\nprint(\"Percentage of missing values in 'age_last_milestone_year' column  : % {:.2f}\". format((df.age_last_milestone_year.isnull().sum())/len(df)*100))\nprint(\"percentage of missing values in 'age_first_milestone_year' column : % {:.2f}\". format((df.age_first_milestone_year.isnull().sum())/len(df)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  \"Unnamed: 6\", \"Unnamed: 0\", \"id\", \"closed_at\" columns are not necessary so drop it\ndf.drop([\"Unnamed: 6\"],axis=1, inplace=True)\ndf.drop([\"Unnamed: 0\"], axis=1, inplace=True)\ndf.drop([\"id\"], axis=1, inplace=True)\ndf.drop([\"closed_at\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in df.iterrows():\n    if row['state_code']!=row['state_code.1']:\n        print(index, row['state_code'], row['state_code.1'])\n\n# \"state_code\" column and \"state_code.1\" column must be the same, so we should drop the \"state_code.1\" and also, \n# \"state_code.1\" column has a one missing value in the 515. row. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"state_code.1\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"status_closed\" column is for prediction of startup success and this is binary classification so we should convert numerical variable by using get_dummies( function) in pandas\ndf=pd.get_dummies(df, columns=[\"status\"], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Year columns must be converted to datetime type\n\ndf.founded_at=pd.to_datetime(df.founded_at)\ndf.first_funding_at=pd.to_datetime(df.first_funding_at)\ndf.last_funding_at=pd.to_datetime(df.last_funding_at)\n\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What does it mean \"age_first_funding_year\", \"age_last_funding_year\", \"age_first_milestone_year\", \"age_last_milestone_year\" , let's together analyze it\n\nplt.figure(figsize=(18,3),dpi=100)\n\nplt.subplot(1,4,1)\nsns.scatterplot((df[\"first_funding_at\"].dt.year - df[\"founded_at\"].dt.year), df[\"age_first_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'First Funding'\")\n\nplt.subplot(1,4,2)\nsns.scatterplot((df[\"last_funding_at\"].dt.year- df[\"founded_at\"].dt.year), df[\"age_last_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'Last Funding'\");\n\nplt.subplot(1,4,3)\nsns.scatterplot(df[\"age_first_funding_year\"], df[\"age_first_milestone_year\"])\n\nplt.subplot(1,4,4)\nsns.scatterplot(df[\"age_last_funding_year\"], df[\"age_last_milestone_year\"]);\n\n\n# As we see the graph, we can say high correlation between funding date and age funding. Difference between \"last_funding_at\" and \"founded_at\" is related \"age_last_funding_year\".\n# \"age_first_funding_year\" and \"age_last_funding_year\" have negative values,it shouldn't be and also it can not be that \"founded\" date higher than \"first_funding_at\" and \"last_funding_at\"\n# So we must get the absolute value of columns including negative value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nfor a in range(len(age)):\n    print(\"Is there any negative value in '{}' column  : {} \".format(age[a],(df[age[a]]<0).any()))\n          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Which rows have negative values? look at one of them\nfor index, rows in df.iterrows():\n    if rows[\"age_first_funding_year\"]<0:\n        print(index, rows[\"age_first_funding_year\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we must get the absolute value of columns including negative value\n\ndf[\"age_first_funding_year\"]=np.abs(df[\"age_first_funding_year\"])\ndf[\"age_last_funding_year\"]=np.abs(df[\"age_last_funding_year\"])\ndf[\"age_first_milestone_year\"]=np.abs(df[\"age_first_milestone_year\"])\ndf[\"age_last_milestone_year\"]=np.abs(df[\"age_last_milestone_year\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nfor a in range(len(age)):\n    print(\"Is there any negative value in '{}' column  : {} \".format(age[a],(df[age[a]]<0).any()))\n    \n# Now, we get rid of negative values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After we get the absolute, visualize relationships\n\nplt.figure(figsize=(16,3),dpi=100)\n\nplt.subplot(1,4,1)\nsns.scatterplot(np.abs(df[\"first_funding_at\"].dt.year - df[\"founded_at\"].dt.year), df[\"age_first_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'First Funding'\")\n\nplt.subplot(1,4,2)\nsns.scatterplot(np.abs(df[\"last_funding_at\"].dt.year- df[\"founded_at\"].dt.year), df[\"age_last_funding_year\"])\nplt.xlabel(\"Difference 'Founded' and 'Last Funding'\");\n\nplt.subplot(1,4,3)\nsns.scatterplot(df[\"age_first_funding_year\"], df[\"age_first_milestone_year\"])\n\nplt.subplot(1,4,4)\nsns.scatterplot(df[\"age_last_funding_year\"], df[\"age_last_milestone_year\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will fill the missing values by mean() function\n\ndf[\"age_first_milestone_year\"].fillna((df[\"age_first_milestone_year\"].mean()), inplace=True)\ndf[\"age_last_milestone_year\"].fillna((df[\"age_last_milestone_year\"].mean()), inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After we get the absolute, visualize relationships\n\nplt.figure(figsize=(16,3),dpi=100)\n\nplt.subplot(1,2,1)\nsns.scatterplot(df[\"age_first_funding_year\"], df[\"age_first_milestone_year\"])\n\nplt.subplot(1,2,2)\nsns.scatterplot(df[\"age_last_funding_year\"], df[\"age_last_milestone_year\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling The Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To find how much there are outliers in dataset, we should use only continuous variables, because rest of numerical variables are binary variables including 0 and 1\n\nvariable=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nplt.figure(figsize=(17,3),dpi=100)\nfor i in range(len(variable)):\n    plt.subplot(1,4,i+1)\n    plt.title(\"{}\". format(variable[i]))\n    plt.boxplot(df[variable[i]]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variable=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_last_milestone_year\"]\n\nplt.figure(figsize=(17,3),dpi=100)\nfor i in range(len(variable)):\n    plt.subplot(1,4,i+1)\n    plt.title(\"{}\". format(variable[i]))\n    sns.distplot(df[variable[i]], color=\"orange\");\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For only one column, analyze that number of outliers\n\nfrom scipy.stats import zscore\n\nzscores=zscore(df[\"age_first_funding_year\"])\n\nfor threshold in range(1,8,1):\n    print(\"Threshold value: {}\". format(threshold))\n    print(\"Number of outliers: {}\".format(len(np.where(zscores>threshold)[0])))\n    print(\"------------------------\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7),dpi=100)\n\ndf[\"log_first_fundig\"]=np.log(df[\"age_first_funding_year\"]+1)\nplt.subplot(2,4,1)\nplt.xlabel(\"log_first_fundig\")\nplt.boxplot(df[\"log_first_fundig\"])   \n\nplt.subplot(2,4,5)\nsns.distplot(df[\"log_first_fundig\"] , color=\"green\");\n\n\ndf[\"log_last_fundig\"]=np.log(df[\"age_last_funding_year\"]+1)\nplt.subplot(2,4,2)\nplt.xlabel(\"log_last_fundig\")\nplt.boxplot(df[\"log_last_fundig\"])   \n\nplt.subplot(2,4,6)\nsns.distplot(df[\"log_last_fundig\"], color=\"green\")\n\n\ndf[\"log_first_milestone\"]=np.log(df[\"age_first_milestone_year\"]+1)\nplt.subplot(2,4,3)\nplt.xlabel(\"log_first_milestone\")\nplt.boxplot(df[\"log_first_milestone\"])   \n\nplt.subplot(2,4,7)\nsns.distplot(df[\"log_first_milestone\"], color=\"green\")\n\n\ndf[\"log_last_milestone\"]=np.log(df[\"age_last_milestone_year\"]+1)\nplt.subplot(2,4,4)\nplt.xlabel(\"log_last_milestone\")\nplt.boxplot(df[\"log_first_fundig\"])   \n\nplt.subplot(2,4,8)\nsns.distplot(df[\"log_last_milestone\"], color=\"green\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4),dpi=100)\n\n\n# \"avg_participants\"  column has negative value but it shouldn't be, so firstly we should get the absolute of the column\ndf[\"avg_participants\"]=np.abs(df[\"avg_participants\"])\n\n\nplt.subplot(1,4,1)\nplt.title(\"Avg Participant Outliers\")\nplt.boxplot(df[\"avg_participants\"])\n\nplt.subplot(1,4,2)\nplt.title(\"Histogram of Avg Participants\")\nsns.distplot(df[\"avg_participants\"], color=\"green\")\n\nplt.subplot(1,4,3)\ndf[\"log_avg_participants\"]=np.log(df[\"avg_participants\"]+1)\nplt.title(\"Logaritmic Avg Participants\")\nplt.boxplot(np.log(df[\"log_avg_participants\"]))\n\nplt.subplot(1,4,4)\nplt.title(\"Histogram of Logaritmic Avg Participants\")\nsns.distplot(np.log(df[\"log_avg_participants\"]), color=\"green\");\n\n# After we get the logaritmic of \"avg_participant\" column, we get rid of the outliers but anyway this column still is not normal distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_state=df.groupby([\"state_code\"])[\"funding_total_usd\"].sum().sort_values(ascending=False).reset_index().head(12)\n\nplt.figure(figsize=(18,6), dpi=100)\nplt.subplot(2,2,1)\nplt.ylabel(\"First 10 state\")\nplt.xlabel(\"Total USD of Funding\")\nsns.barplot(df_state[\"state_code\"],df_state[\"funding_total_usd\"], palette=\"Greens\")\n\n\nplt.subplot(2,2,2)\ndf_funding=df.groupby([\"state_code\"])[\"funding_rounds\"].sum().sort_values(ascending=False).reset_index().head(12)\nsns.barplot(df_funding[\"state_code\"], df_funding[\"funding_rounds\"], palette=\"Greens\")\n\nplt.subplot(2,2,3)\nsns.countplot(df[\"state_code\"])\nplt.xticks(rotation=55);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,4),dpi=100)\nplt.xticks(rotation=45)\nplt.title(\"Category Type Counts\")\nsns.countplot(df[\"category_code\"], edgecolor=sns.color_palette(\"dark\"));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category top500\")\nsns.countplot(x=df[\"category_code\"], hue=df[\"is_top500\"], palette=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category has_angel\")\nsns.countplot(x=df[\"category_code\"], hue=df[\"has_angel\"], palette=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category status_closed\")\nsns.countplot(x=df[\"category_code\"], hue=df[\"status_closed\"], palette=\"Greens\")\nplt.legend(loc=1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category total USD\")\nsns.barplot(x=df[\"category_code\"], y=df[\"funding_total_usd\"], palette=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,4),dpi=100)\n\nplt.xticks(rotation=42)\nplt.title(\"According to category avg_participants\")\nsns.barplot(x=df[\"category_code\"], y=df[\"avg_participants\"], palette=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4),dpi=100)\n\nplt.subplot(1,3,1)\nsns.barplot(df[\"is_top500\"], df[\"funding_total_usd\"], palette=\"Greens\")\n\nplt.subplot(1,3,2)\nsns.barplot(df[\"has_angel\"], df[\"funding_total_usd\"], palette=\"Greens\")\n\nplt.subplot(1,3,3)\nsns.countplot(df.milestones, palette=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,4),dpi=100)\n\n\ncol=[\"log_first_fundig\",\"log_last_fundig\",\"log_first_milestone\",\"log_last_milestone\",\"log_avg_participants\"]\n\n\nfor i in range(len(col)):\n    plt.subplot(1,5,i+1)\n    sns.barplot(df[\"funding_rounds\"],df[col[i]], palette=\"Greens\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,3),dpi=100)\nplt.subplot(1,3,1)\nsns.scatterplot(df[\"age_first_funding_year\"],df[\"age_last_funding_year\"], label=\"first&last funding\", palette=\"Greens\")\nsns.scatterplot(df[\"age_first_milestone_year\"], df[\"age_last_milestone_year\"], label=\"first&last milestone\", palette=\"Blues\")\nplt.legend()\n\nplt.subplot(1,3,2)\nsns.distplot(df[\"age_first_funding_year\"], label=\"first_funding\")\nsns.distplot(df[\"age_last_funding_year\"], label=\"last_funding\")\nsns.distplot(df[\"age_first_milestone_year\"], label=\"first_milestone\")\nsns.distplot(df[\"age_last_milestone_year\"], label=\"last_milestone\")\nplt.xlabel(\"first_funding, last_funding, first_milestone, last_milestone\")\nplt.legend()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The most relational columns with target variable(status_closed) are below:\n\nplt.figure(figsize=(4,8),dpi=100)\n\nfocus_cols = ['status_closed']\ndf_corr=df.corr().filter(focus_cols).drop(focus_cols)\nsns.heatmap(df_corr, annot=True, fmt='.2f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test it Statistically"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import ttest_ind\n\n# we get the null hypothesis that both groups have equal means.\n\nttest=ttest_ind(df[\"has_angel\"],df[\"funding_total_usd\"])\nprint(\"Is there any differences between means of has_angel and funding_total_usd?\")\nprint(\"--\"*40)\nprint(\"t statistic: {:3f} p_value: {:3f}\". format(ttest[0],ttest[1]),\"\\n\",\"\\n\")\n\n\nprint(\"Is there any differences between means of is_top500 and funding_total_usd?\")\nprint(\"--\"*40)\nttest2=ttest_ind(df[\"is_top500\"],df[\"funding_total_usd\"])\nprint(\"t statistic: {:3f} p_value: {:3f}\". format(ttest2[0],ttest2[1]))\n\n\n# In order to p_value is less than 0.05, we reject the H0 hypothesis so, there is not differences between mean of variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test whether group differences are significant.\n\n\nttest_3=ttest_ind(df[\"funding_rounds\"], df[\"log_first_fundig\"])    \nprint(\"'funding_rounds' and 'log_first_fundig' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_3[0], ttest_3[1]))\n   \n    \nttest_4=ttest_ind(df[\"funding_rounds\"], df[\"log_last_fundig\"])    \nprint(\"'funding_rounds' and 'log_last_fundig' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_4[0], ttest_4[1]))\n\nttest_5=ttest_ind(df[\"funding_rounds\"], df[\"log_first_milestone\"])    \nprint(\"'funding_rounds' and 'log_first_milestone' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_5[0], ttest_5[1]))\n\nttest_6=ttest_ind(df[\"funding_rounds\"], df[\"log_last_milestone\"])    \nprint(\"'funding_rounds' and 'log_last_milestone' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_6[0], ttest_6[1]))\n\nttest_7=ttest_ind(df[\"funding_rounds\"], df[\"log_avg_participants\"])    \nprint(\"'funding_rounds' and 'log_avg_participants' t statistic: {:.4f}, p_value: {:.4f}\". format(ttest_7[0], ttest_7[1]))\n\n# In order to p_value is less than 0.05, rejected H0 hypothesis so, there is not difference between means","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Before the feature selection, we have to normalize the some columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"column=[\"log_first_fundig\",\"log_last_fundig\",\"log_first_milestone\",\"log_last_milestone\",\"log_avg_participants\"]\ncolu=[\"age_first_funding_year\",\"age_last_funding_year\",\"age_first_milestone_year\",\"age_first_milestone_year\",\"avg_participants\"]\n\nplt.figure(figsize=(18,3), dpi=100)\nfor j in range(len(colu)):\n    plt.subplot(1,5,j+1)\n    sns.distplot(df[colu[j]], color=\"orange\")\n    \nplt.figure(figsize=(18,3), dpi=100)\nfor i in range(len(column)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column[i]], color=\"green\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Are these variables really normal distribution, calculate it statistically?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test it, whether these variables are normal distribution, for this i will use jarque-bera test function \n\nfrom scipy.stats import jarque_bera\n\ndist=[\"log_first_fundig\", \"log_last_fundig\", \"log_first_milestone\", \"log_last_milestone\", \"log_avg_participants\"]\njarq_df=pd.DataFrame(columns=[\"variable\",\"test statistic\",\"p_value\"])\n\n\nfor d in range(len(dist)):\n    jarq=jarque_bera(df[dist[d]])\n    jarq_df=jarq_df.append({\"variable\":dist[d],\n                   \"test statistic\":jarq[0],\n                   \"p_value\":jarq[1]}, ignore_index=True)\n\ndisplay(jarq_df)    \n\n\n# All of the variables are not the normal distribution because of rejected the H0 hypothesis.\n# H0 --> have normal dstribution\n# HA --> not normal distribution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\ndf[\"norm_log_first_funding\"]=normalize(np.array(df[\"log_first_fundig\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_last_funding\"]=normalize(np.array(df[\"log_last_fundig\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_first_milestone\"]=normalize(np.array(df[\"log_first_milestone\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_last_milestone\"]=normalize(np.array(df[\"log_last_milestone\"]).reshape(1,-1)).reshape(-1,1)\ndf[\"norm_log_avg_participants\"]=normalize(np.array(df[\"log_avg_participants\"]).reshape(1,-1)).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column2=[\"norm_log_first_funding\",\"norm_log_last_funding\",\"norm_log_first_milestone\",\"norm_log_last_milestone\",\"norm_log_avg_participants\"]\n\nplt.figure(figsize=(18,3), dpi=100)\nfor i in range(len(column2)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column2[i]], color=\"orange\");\n    \nprint(\"Minimum values is norm_log_first_funding\", df[\"norm_log_first_funding\"].min())\nprint(\"Maximum values is norm_log_first_funding\", df[\"norm_log_first_funding\"].max())   \n# Still these columns are not normal distribution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now, try StandardScaler()\n\nfrom sklearn.preprocessing import scale\n\ndf[\"scaled_log_first_funding\"]=scale(df[\"log_first_fundig\"])\ndf[\"scaled_log_last_funding\"]=scale(df[\"log_last_fundig\"])\ndf[\"scaled_log_first_milestone\"]=scale(df[\"log_first_milestone\"])\ndf[\"scaled_log_last_milestone\"]=scale(df[\"log_last_milestone\"])\ndf[\"scaled_log_avg_participants\"]=scale(df[\"log_avg_participants\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column3=[\"scaled_log_first_funding\",\"scaled_log_last_funding\",\"scaled_log_first_milestone\",\"scaled_log_last_milestone\",\"scaled_log_avg_participants\"]\n\nplt.figure(figsize=(18,3), dpi=100)\nfor i in range(len(column3)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column3[i]], color=\"orange\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats.mstats import winsorize\n\ndf[\"winsorize_first_funding\"]=winsorize(df[\"age_first_funding_year\"], (0,0.10))\n\n# For \"age_first_funding\" column we analyze whether there are normal distribution\n\ncolumn4=[\"age_first_funding_year\",\"log_first_fundig\",\"winsorize_first_funding\",\"norm_log_first_funding\",\"scaled_log_first_funding\"]\n\nplt.figure(figsize=(18,3),dpi=100)\n\nfor i in range(len(column4)):\n    plt.subplot(1,5,i+1)\n    sns.distplot(df[column4[i]], color=\"orange\");    \n\n# None of this columns are not normal distribution but we need to select columns closer to normal distribution and these are logaritmic columns.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I have decided to continue with only logaritmic columns in this dataset, so i will drop that is created new\n\ndf.drop([\"winsorize_first_funding\",\"scaled_log_avg_participants\",\"scaled_log_last_milestone\",\"scaled_log_first_milestone\",\"scaled_log_last_funding\",\"scaled_log_first_funding\"],\n             axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variable must be in the end\n\ncols = [col for col in df if col != 'status_closed'] + ['status_closed'] \ndf=df[cols]\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We have applied exploratory data analysis in this dataset for now, then we will try to predict classification problem using various machine learning algorithms"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}