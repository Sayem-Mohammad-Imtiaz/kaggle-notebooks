{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Machine Learning: Fundamentals"},{"metadata":{},"cell_type":"markdown","source":"### What is Machine Learning and how can Machine Learning be useful?\n\nDiscussion!\n\nMachine Learning is the process of building an algorithm which can learn without explicit instructions, this is accomplished with a mathematical model - a model is based on data, and is usually designed to exploit our current understanding of data to find a generalizable \"trend\" that applies to all instances of similar data. \n\nWhat is an example of a problem that can be handled with machine learning?\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\ndf = pd.read_csv('../data/fifa19.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_sub = df[['Composure','Overall']].sample(20, random_state = 42)\ndf_sub.dropna(inplace = True)\ndf_sub.plot(x = 'Composure', y='Overall', kind = 'scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nX = np.array(df_sub['Composure'])\ny = df_sub['Overall']\ndegrees = [1,2,12]\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i],\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n\n    X_test = np.linspace(25, 90, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((25, 90))\n    plt.ylim((60, 80))\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Validation\n\nModel validation is the process of assessing the fidelity of the model by confirming that the outputs from a machine learning model are representative of the population. This is done by evaluating the model on data it has not \"seen.\"\n\nWhat are some ways we could incorrectly validate our model?"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns\ndf['not_awful'] = (df.Overall > 62)*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df[['not_awful','Position','Strength','Composure',\n        'LongPassing','GKReflexes']]\nX.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_1 = X[X.Position != 'GK']\nX_train_1.drop(['Position'],axis=1, inplace= True)\nY_train_1 = X_train_1.pop('not_awful')\n\n\nX_test_1 = X[X.Position == 'GK']\nX_test_1.drop(['Position'],axis=1, inplace= True)\nY_test_1 = X_test_1.pop('not_awful')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(gamma ='auto')\nsvm.fit(X_train_1,Y_train_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred_train = svm.predict(X_train_1)\nprint(str(accuracy_score(Y_train_1, y_pred_train)))\n\ny_pred_test = svm.predict(X_test_1)\nprint(str(accuracy_score(Y_test_1, y_pred_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try a more reasonable split"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df[['not_awful','Position','Strength','Composure',\n        'LongPassing','GKReflexes']]\nX.dropna(inplace = True)\n\nY = X.pop('not_awful')\nX.drop('Position', axis = 1, inplace = True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, Y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svm_tt = SVC(gamma ='auto')\nsvm_tt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_train = svm_tt.predict(X_train)\nprint(str(accuracy_score(y_train, y_pred_train)))\n\ny_pred_test = svm_tt.predict(X_test)\nprint(str(accuracy_score(y_test, y_pred_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\n\nsvm = SVC(gamma ='auto')\ncv_results = cross_validate(svm, X_train, y_train, cv = 3, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, n_jobs = -1) \ngrid.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid.cv_results_['mean_test_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install hyperopt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\ndef hyperopt_train_test(params):\n    X_ = X_train[:]\n    clf = SVC(**params)\n    return cross_val_score(clf, X_,y_train).mean()\n\nspace4svm = {\n    'C': hp.uniform('C', 7.5, 12.5),\n    'kernel': hp.choice('kernel', ['rbf']),\n    'gamma': hp.uniform('gamma', 0.00001, .005),\n}\n\ndef f(params):\n    acc = hyperopt_train_test(params)\n    return {'loss': -acc, 'status': STATUS_OK}\n\ntrials = Trials()\nbest = fmin(f, space4svm, algo=tpe.suggest, max_evals=10, trials=trials)\nprint('best:')\nprint(best)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":4}