{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport lightgbm as lgbm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-14T07:40:35.630827Z","iopub.execute_input":"2021-09-14T07:40:35.631424Z","iopub.status.idle":"2021-09-14T07:40:38.658583Z","shell.execute_reply.started":"2021-09-14T07:40:35.631301Z","shell.execute_reply":"2021-09-14T07:40:38.657247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/train-folds-5/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsubmission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# Training the models on the prediction set from the base level\nxgb_df = pd.read_csv('../input/1st-layer-model-stacking/xgb_train_pred (1).csv')\nxgb_df.columns = [\"id\", \"xgbpred\"]\n\nlgb_df = pd.read_csv('../input/1st-layer-model-stacking/lgb_train_pred (1).csv')\nlgb_df.columns = [\"id\", \"lgbpred\"]\n\ngbr_df = pd.read_csv('../input/1st-layer-model-stacking/gbr_train_pred (1).csv')\ngbr_df.columns= [\"id\", \"gbrpred\"]\n\nrf_df = pd.read_csv('../input/1st-layer-model-stacking/rf_train_pred (2).csv')\nrf_df.columns = ['id', 'rfpred']\n\nlass_df = pd.read_csv('../input/ridge-and-lasso-preds-30-days-of-ml/lass_train_pred.csv')\nlass_df.columns = ['id','lasspred']\n\nridge_df = pd.read_csv('../input/ridge-and-lasso-preds-30-days-of-ml/ridge_train_pred.csv')\nridge_df.columns = ['id','ridgepred']\n\nxgb_df_test = pd.read_csv('../input/1st-layer-model-stacking/xgb_test_pred (1).csv')\nxgb_df_test.columns = [\"id\", \"xgbpred\"]\n\nrf_df_test = pd.read_csv('../input/1st-layer-model-stacking/rf_test_pred (2).csv')\nrf_df_test.columns = ['id', 'rfpred']\n\nlgb_df_test = pd.read_csv('../input/1st-layer-model-stacking/lgb_test_pred (1).csv')\nlgb_df_test.columns = [\"id\", \"lgbpred\"]\n\ngbr_df_test = pd.read_csv('../input/1st-layer-model-stacking/gbr_test_pred (2).csv')\ngbr_df_test.columns = [\"id\", \"gbrpred\"]\n\nlass_df_test = pd.read_csv('../input/ridge-and-lasso-preds-30-days-of-ml/lass_test_pred.csv')\nlass_df_test.columns = ['id','lasspred']\n\nridge_df_test = pd.read_csv('../input/ridge-and-lasso-preds-30-days-of-ml/ridge_train_pred.csv')\nridge_df_test.columns = ['id','ridgepred']\n# *** Merging dataframes\ndf = df.merge(xgb_df, on=\"id\", how=\"left\")\ndf = df.merge(lgb_df, on=\"id\", how=\"left\")\ndf = df.merge(rf_df, on=\"id\", how=\"left\")\ndf = df.merge(gbr_df,on=\"id\", how=\"left\" )\ndf = df.merge(lass_df,on=\"id\", how=\"left\")\ndf = df.merge (ridge_df,on=\"id\", how=\"left\" )\ndf_test = df_test.merge(xgb_df_test, on='id',how=\"left\")\ndf_test = df_test.merge(lgb_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(rf_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(gbr_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(lass_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(ridge_df_test, on='id', how=\"left\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:38.660523Z","iopub.execute_input":"2021-09-14T07:40:38.661053Z","iopub.status.idle":"2021-09-14T07:40:46.37285Z","shell.execute_reply.started":"2021-09-14T07:40:38.661004Z","shell.execute_reply":"2021-09-14T07:40:46.372016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:46.374204Z","iopub.execute_input":"2021-09-14T07:40:46.374646Z","iopub.status.idle":"2021-09-14T07:40:46.412441Z","shell.execute_reply.started":"2021-09-14T07:40:46.374616Z","shell.execute_reply":"2021-09-14T07:40:46.41133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:46.413775Z","iopub.execute_input":"2021-09-14T07:40:46.414259Z","iopub.status.idle":"2021-09-14T07:40:46.442327Z","shell.execute_reply.started":"2021-09-14T07:40:46.414225Z","shell.execute_reply":"2021-09-14T07:40:46.441078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return any column with missing values. No columns with missing values found\ndf.columns[df.isnull().any()]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:46.4438Z","iopub.execute_input":"2021-09-14T07:40:46.444201Z","iopub.status.idle":"2021-09-14T07:40:46.756164Z","shell.execute_reply.started":"2021-09-14T07:40:46.444166Z","shell.execute_reply":"2021-09-14T07:40:46.755101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Histograms for each var\ndf_hist = df.hist(bins=10,figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:46.757493Z","iopub.execute_input":"2021-09-14T07:40:46.75783Z","iopub.status.idle":"2021-09-14T07:40:50.069948Z","shell.execute_reply.started":"2021-09-14T07:40:46.757773Z","shell.execute_reply":"2021-09-14T07:40:50.069182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating a heatmap to show correlation\nfig,axes = plt.subplots(1,1,figsize=(16,14))\nsns.heatmap(df.corr(),annot=True, cmap=\"RdYlGn\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:50.071169Z","iopub.execute_input":"2021-09-14T07:40:50.071672Z","iopub.status.idle":"2021-09-14T07:40:53.169599Z","shell.execute_reply.started":"2021-09-14T07:40:50.071627Z","shell.execute_reply":"2021-09-14T07:40:53.167987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select categorical vars only\n\ndf_cat = df.select_dtypes(include = 'object').copy()\n# counts of each var value\ndf_cat.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:53.173139Z","iopub.execute_input":"2021-09-14T07:40:53.173557Z","iopub.status.idle":"2021-09-14T07:40:53.753361Z","shell.execute_reply.started":"2021-09-14T07:40:53.173519Z","shell.execute_reply":"2021-09-14T07:40:53.752321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the numbers of each unique values for each categorical var using lambda expression\ndf_cat.apply(lambda x:x.value_counts()).T.stack()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:53.755751Z","iopub.execute_input":"2021-09-14T07:40:53.756224Z","iopub.status.idle":"2021-09-14T07:40:54.42857Z","shell.execute_reply.started":"2021-09-14T07:40:53.756173Z","shell.execute_reply":"2021-09-14T07:40:54.427877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kept_features = ['xgbpred', 'lgbpred','rfpred','gbrpred','lasspred','ridgepred']\ndf_test = df_test[kept_features]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:54.429925Z","iopub.execute_input":"2021-09-14T07:40:54.430501Z","iopub.status.idle":"2021-09-14T07:40:54.46281Z","shell.execute_reply.started":"2021-09-14T07:40:54.430454Z","shell.execute_reply":"2021-09-14T07:40:54.461855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params ={'random_state': 1, \n        'booster': 'gbtree',\n        'n_estimators': 10000,\n        'learning_rate': 0.03628302216953097,\n        'reg_lambda': 0.0008746338866473539,\n        'reg_alpha': 23.13181079976304,\n        'subsample': 0.7875490025178415,\n        'colsample_bytree': 0.11807135201147481,\n        'max_depth': 3\n             }","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:54.46426Z","iopub.execute_input":"2021-09-14T07:40:54.464678Z","iopub.status.idle":"2021-09-14T07:40:54.471049Z","shell.execute_reply.started":"2021-09-14T07:40:54.464637Z","shell.execute_reply":"2021-09-14T07:40:54.469892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/train-folds-5/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsubmission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\nkept_features = [useful_cols for useful_cols in df.columns if useful_cols not in ('id','kfold','target')]\nobject_cols = [col for col in kept_features if 'cat' in col]\n# Creating a var for numerical columns for feature engineering\nnumerical_cols = [col for col in kept_features if 'cont' in col]\ndf_test=df_test[kept_features]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:54.472615Z","iopub.execute_input":"2021-09-14T07:40:54.472999Z","iopub.status.idle":"2021-09-14T07:40:57.459127Z","shell.execute_reply.started":"2021-09-14T07:40:54.472967Z","shell.execute_reply":"2021-09-14T07:40:57.45787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building an XGBoost model for stacking\n\nxgb_final_test_predictions = []\nxgb_final_valid_predictions = {}\nscores = []\n# creating a for loop to loop over fold, reserving -1 fold for training data\nfor fold in range(5):\n    # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n    #creating a list of validation set's indices\n    valid_id = x_valid.id.values.tolist()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n    # ordinal-encode categorical columns\n    OE = preprocessing.OrdinalEncoder()\n    \n    # always fit_transform on the training data\n    x_train[object_cols] = OE.fit_transform(x_train[object_cols])\n    \n    #transform on the validation and test sets\n    x_valid[object_cols] = OE.transform(x_valid[object_cols])\n    x_test[object_cols] = OE.transform(x_test[object_cols])\n    \n    # Feature engineering for numerical var : standardisation\n    \n    scaler = preprocessing.StandardScaler()\n    # standardise training data, using .fit_transform()\n    x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n    #standardise validation and test data using .transform()\n    x_valid[numerical_cols] = scaler.transform(x_valid[numerical_cols])\n    x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n    \n    \n    #using XGBRegressor \n    xgb_model = XGBRegressor(**xgb_params,\n                              n_jobs =4\n                              )\n    xgb_model.fit(x_train, y_train, early_stopping_rounds=300, eval_set=[(x_valid, y_valid)], verbose=1000)\n    xgb_preds_valid = xgb_model.predict(x_valid)\n    xgb_test_preds = xgb_model.predict(x_test)\n    xgb_final_test_predictions.append(xgb_test_preds)\n    xgb_final_valid_predictions.update(dict(zip(valid_id, xgb_preds_valid)))\n    RMSE = mean_squared_error(y_valid, xgb_preds_valid, squared=False)\n    print(fold, RMSE)\n    scores.append(RMSE)\n# Printing out of the loop \nprint(np.mean(scores), np.std(scores))\nxgb_final_valid_predictions = pd.DataFrame.from_dict(xgb_final_valid_predictions, orient=\"index\").reset_index()\nxgb_final_valid_predictions.columns = [\"id\", \"xgbpred_1\"]\nxgb_final_valid_predictions.to_csv(\"xgb_train_pred_1.csv\", index=False)\n\nsubmission.target = np.mean(np.column_stack(xgb_final_test_predictions), axis=1)\nsubmission.columns = [\"id\", \"xgbpred_1\"]\nsubmission.to_csv(\"xgb_test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T07:40:57.460502Z","iopub.execute_input":"2021-09-14T07:40:57.460837Z","iopub.status.idle":"2021-09-14T08:25:30.974566Z","shell.execute_reply.started":"2021-09-14T07:40:57.460797Z","shell.execute_reply":"2021-09-14T08:25:30.973631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = { \"objective\": \"regression\",\n             \"metric\": \"rmse\",\n             \"boosting_type\": \"gbdt\",\n             \"n_estimators\": 10000,\n             \"early_stopping_round\": 300,\n   'colsample_tree': 0.9966937316093348,\n 'learning_rate': 0.23205382167938451,\n 'max_depth': 2,\n 'reg_alpha': 21.312138571025006,\n 'reg_lambda': 4.4379149320083925e-08,\n 'subsample': 0.21475469764965427}","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:25:30.97606Z","iopub.execute_input":"2021-09-14T08:25:30.976407Z","iopub.status.idle":"2021-09-14T08:25:30.983363Z","shell.execute_reply.started":"2021-09-14T08:25:30.976372Z","shell.execute_reply":"2021-09-14T08:25:30.982081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building a LightGBM\nimport lightgbm as lgbm\n\nlgb_final_test_predictions = []\nlgb_final_valid_predictions = {}\nscores = []\n# creating a for loop to loop over fold, reserving -1 fold for training data\nfor fold in range(5):\n    # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n\n    valid_id = x_valid.id.values.tolist()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n     # ordinal-encode categorical columns\n    OE = preprocessing.OrdinalEncoder()\n    \n    # always fit_transform on the training data\n    x_train[object_cols] = OE.fit_transform(x_train[object_cols])\n    \n    #transform on the validation and test sets\n    x_valid[object_cols] = OE.transform(x_valid[object_cols])\n    x_test[object_cols] = OE.transform(x_test[object_cols])\n    \n    # Feature engineering for numerical var : standardisation\n    \n    scaler = preprocessing.StandardScaler()\n    # standardise training data, using .fit_transform()\n    x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n    #standardise validation and test data using .transform()\n    x_valid[numerical_cols] = scaler.transform(x_valid[numerical_cols])\n    x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n    lgb_train = lgbm.Dataset(x_train, y_train)\n    lgb_valid = lgbm.Dataset(x_valid, y_valid, reference = lgb_train)\n   \n    #using LGBM  \n    lgb_model = lgbm.train(lgb_params, lgb_train, \n                                 valid_sets = [lgb_valid],\n                                  verbose_eval = 1000\n                              )\n  \n    lgb_preds_valid = lgb_model.predict(x_valid)\n    lgb_test_preds = lgb_model.predict(x_test)\n    lgb_final_test_predictions.append(lgb_test_preds)\n    lgb_final_valid_predictions.update(dict(zip(valid_id, lgb_preds_valid)))\n    print(mean_squared_error(y_valid, lgb_preds_valid, squared=False))\n    RMSE = mean_squared_error(y_valid, lgb_preds_valid, squared=False)\n    print(fold,RMSE)\n    scores.append(RMSE)\n\nprint (np.mean(scores),np.std(scores))\nlgb_final_valid_predictions = pd.DataFrame.from_dict(lgb_final_valid_predictions, orient=\"index\").reset_index()\nlgb_final_valid_predictions.columns = [\"id\", \"lgbpred_1\"]\nlgb_final_valid_predictions.to_csv(\"lgb_train_pred_1.csv\", index=False)\n\nsubmission.target = np.mean(np.column_stack(lgb_final_test_predictions), axis=1)\nsubmission.columns = [\"id\", \"lgbpred_1\"]\nsubmission.to_csv(\"lgb_test_pred_1.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:25:30.985089Z","iopub.execute_input":"2021-09-14T08:25:30.985401Z","iopub.status.idle":"2021-09-14T08:27:52.606742Z","shell.execute_reply.started":"2021-09-14T08:25:30.985373Z","shell.execute_reply":"2021-09-14T08:27:52.605718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_params = {'min_samples_split': 2,\n             'min_samples_leaf': 4, \n             'max_features': 'sqrt', \n             'max_depth': None, \n             'bootstrap': False}","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:27:52.608334Z","iopub.execute_input":"2021-09-14T08:27:52.608776Z","iopub.status.idle":"2021-09-14T08:27:52.614199Z","shell.execute_reply.started":"2021-09-14T08:27:52.608729Z","shell.execute_reply":"2021-09-14T08:27:52.613046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building a random forest model\n\nrf_final_test_predictions = []\nrf_final_valid_predictions = {}\nscores =[]\n\nfor fold in range(5):\n    # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n    #creating a list of validation set's indices\n    valid_id = x_valid.id.values.tolist()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n     # ordinal-encode categorical columns\n    OE = preprocessing.OrdinalEncoder()\n    \n    # always fit_transform on the training data\n    x_train[object_cols] = OE.fit_transform(x_train[object_cols])\n    \n    #transform on the validation and test sets\n    x_valid[object_cols] = OE.transform(x_valid[object_cols])\n    x_test[object_cols] = OE.transform(x_test[object_cols])\n    \n    # Feature engineering for numerical var : standardisation\n    \n    scaler = preprocessing.StandardScaler()\n    # standardise training data, using .fit_transform()\n    x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n    #standardise validation and test data using .transform()\n    x_valid[numerical_cols] = scaler.transform(x_valid[numerical_cols])\n    x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n    #using RandomForestRegressor\n    rf_model = RandomForestRegressor(**rf_params, random_state=1)\n    rf_model.fit(x_train, y_train)\n    rf_preds_valid = rf_model.predict(x_valid)\n    rf_test_preds = rf_model.predict(x_test)\n    rf_final_test_predictions.append(rf_test_preds)\n    # use function zip() to create a zip object and store it in a dictionary\n    rf_final_valid_predictions.update(dict(zip(valid_id, rf_preds_valid)))\n    RMSE = mean_squared_error(y_valid, rf_preds_valid, squared=False)\n    print(fold,RMSE)\n    scores.append(RMSE)\nprint (np.mean(scores),np.std(scores))\nrf_final_valid_predictions = pd.DataFrame.from_dict(rf_final_valid_predictions, orient='index').reset_index()\nrf_final_valid_predictions.columns = ['id', 'rfpred_1']\nrf_final_valid_predictions.to_csv('rf_train_pred_1.csv', index=False)\n    \nsubmission.target = np.mean(np.column_stack(rf_final_test_predictions), axis=1)\nsubmission.columns = ['id', 'rfpred_1']\nsubmission.to_csv (\"rf_test_pred_1.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:27:52.615519Z","iopub.execute_input":"2021-09-14T08:27:52.615834Z","iopub.status.idle":"2021-09-14T08:44:42.647971Z","shell.execute_reply.started":"2021-09-14T08:27:52.615795Z","shell.execute_reply":"2021-09-14T08:44:42.646891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr_params= {'learning_rate': 0.021030330096493244, \n 'max_depth': 4, 'n_estimators': 968, \n 'subsample': 0.579236968137102}","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:44:42.649369Z","iopub.execute_input":"2021-09-14T08:44:42.649644Z","iopub.status.idle":"2021-09-14T08:44:42.654878Z","shell.execute_reply.started":"2021-09-14T08:44:42.649618Z","shell.execute_reply":"2021-09-14T08:44:42.653809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building a Gradient Boosting Regressor\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbr_final_test_predictions = []\ngbr_final_valid_predictions = {}\nscores = []\n# creating a for loop to loop over fold, reserving -1 fold for training data\nfor fold in range(5):\n    # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n\n    valid_id = x_valid.id.values.tolist()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n     # ordinal-encode categorical columns\n    OE = preprocessing.OrdinalEncoder()\n    \n    # always fit_transform on the training data\n    x_train[object_cols] = OE.fit_transform(x_train[object_cols])\n    \n    #transform on the validation and test sets\n    x_valid[object_cols] = OE.transform(x_valid[object_cols])\n    x_test[object_cols] = OE.transform(x_test[object_cols])\n    \n    # Feature engineering for numerical var : standardisation\n    \n    scaler = preprocessing.StandardScaler()\n    # standardise training data, using .fit_transform()\n    x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n    #standardise validation and test data using .transform()\n    x_valid[numerical_cols] = scaler.transform(x_valid[numerical_cols])\n    x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n   \n    #using GBR  \n    gbr_model = GradientBoostingRegressor(**gbr_params, random_state=1)\n    gbr_model.fit(x_train,y_train)\n                      \n  \n    gbr_preds_valid = gbr_model.predict(x_valid)\n    gbr_test_preds = gbr_model.predict(x_test)\n    gbr_final_test_predictions.append(gbr_test_preds)\n    gbr_final_valid_predictions.update(dict(zip(valid_id, gbr_preds_valid)))\n    print(mean_squared_error(y_valid, gbr_preds_valid, squared=False))\n    RMSE = mean_squared_error(y_valid, gbr_preds_valid, squared=False)\n    print(fold,RMSE)\n    scores.append(RMSE)\n\nprint (np.mean(scores),np.std(scores))\ngbr_final_valid_predictions = pd.DataFrame.from_dict(gbr_final_valid_predictions, orient=\"index\").reset_index()\ngbr_final_valid_predictions.columns = [\"id\", \"gbrpred_1\"]\ngbr_final_valid_predictions.to_csv(\"gbr_train_pred_1.csv\", index=False)\n\nsubmission.target = np.mean(np.column_stack(gbr_final_test_predictions), axis=1)\nsubmission.columns = [\"id\", \"gbrpred_1\"]\nsubmission.to_csv(\"gbr_test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T08:44:42.656592Z","iopub.execute_input":"2021-09-14T08:44:42.657073Z","iopub.status.idle":"2021-09-14T10:56:54.204442Z","shell.execute_reply.started":"2021-09-14T08:44:42.657031Z","shell.execute_reply":"2021-09-14T10:56:54.193935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lasso regression\nfrom sklearn.linear_model import Lasso\nlass_final_test_predictions = []\nlass_final_valid_predictions = {}\nscores = []\n# creating a for loop to loop over fold, reserving -1 fold for training data\nfor fold in range(5):\n    # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n    valid_id = x_valid.id.values.tolist()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n     # ordinal-encode categorical columns\n    OE = preprocessing.OrdinalEncoder()\n    \n    # always fit_transform on the training data\n    x_train[object_cols] = OE.fit_transform(x_train[object_cols])\n    \n    #transform on the validation and test sets\n    x_valid[object_cols] = OE.transform(x_valid[object_cols])\n    x_test[object_cols] = OE.transform(x_test[object_cols])\n    \n    # Feature engineering for numerical var : standardisation\n    \n    scaler = preprocessing.StandardScaler()\n    # standardise training data, using .fit_transform()\n    x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n    #standardise validation and test data using .transform()\n    x_valid[numerical_cols] = scaler.transform(x_valid[numerical_cols])\n    x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n    #using Lasso \n    lass_model = Lasso(alpha = 0.0,\n                              random_state=0)\n    lass_model.fit(x_train, y_train)\n    lass_preds_valid = lass_model.predict(x_valid)\n    lass_test_preds = lass_model.predict(x_test)\n    print(mean_squared_error(y_valid, lass_preds_valid, squared=False))\n    lass_final_test_predictions.append(lass_test_preds)\n    lass_final_valid_predictions.update(dict(zip(valid_id, lass_preds_valid)))\n    RMSE = mean_squared_error(y_valid, lass_preds_valid, squared=False)\n    print(fold,RMSE)\n    scores.append(RMSE)\n\n    print (np.mean(scores),np.std(scores))\nprint (np.mean(scores),np.std(scores))\nlass_final_valid_predictions = pd.DataFrame.from_dict(lass_final_valid_predictions, orient=\"index\").reset_index()\nlass_final_valid_predictions.columns = [\"id\", \"lasspred_1\"]\nlass_final_valid_predictions.to_csv(\"lass_train_pred_1.csv\", index=False)\n\nsubmission.target = np.mean(np.column_stack(lass_final_test_predictions), axis=1)\nsubmission.columns = [\"id\", \"lasspred_1\"]\nsubmission.to_csv(\"lass_test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:56:54.21396Z","iopub.execute_input":"2021-09-14T10:56:54.214338Z","iopub.status.idle":"2021-09-14T10:58:07.87902Z","shell.execute_reply.started":"2021-09-14T10:56:54.214305Z","shell.execute_reply":"2021-09-14T10:58:07.877864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge Regression\nfrom sklearn.linear_model import Ridge,RidgeCV\nridge_final_test_predictions = []\nridge_final_valid_predictions = {}\nscores = []\n# creating a for loop to loop over fold, reserving -1 fold for training data\nfor fold in range(5):\n    # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n    valid_id = x_valid.id.values.tolist()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n     # ordinal-encode categorical columns\n    OE = preprocessing.OrdinalEncoder()\n    \n    # always fit_transform on the training data\n    x_train[object_cols] = OE.fit_transform(x_train[object_cols])\n    \n    #transform on the validation and test sets\n    x_valid[object_cols] = OE.transform(x_valid[object_cols])\n    x_test[object_cols] = OE.transform(x_test[object_cols])\n    \n    # Feature engineering for numerical var : standardisation\n    \n    scaler = preprocessing.StandardScaler()\n    # standardise training data, using .fit_transform()\n    x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n    #standardise validation and test data using .transform()\n    x_valid[numerical_cols] = scaler.transform(x_valid[numerical_cols])\n    x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n    #using Lasso \n    ridge_model = Ridge(alpha = 0.0,\n                              random_state=0)\n    ridge_model.fit(x_train, y_train)\n    ridge_preds_valid = ridge_model.predict(x_valid)\n    ridge_test_preds = ridge_model.predict(x_test)\n    print(mean_squared_error(y_valid, ridge_preds_valid, squared=False))\n    ridge_final_test_predictions.append(ridge_test_preds)\n    ridge_final_valid_predictions.update(dict(zip(valid_id, ridge_preds_valid)))\n    RMSE = mean_squared_error(y_valid, ridge_preds_valid, squared=False)\n    print(fold,RMSE)\n    scores.append(RMSE)\n\nprint (np.mean(scores),np.std(scores))\n\nridge_final_valid_predictions = pd.DataFrame.from_dict(ridge_final_valid_predictions, orient=\"index\").reset_index()\nridge_final_valid_predictions.columns = [\"id\", \"ridgepred_1\"]\nridge_final_valid_predictions.to_csv(\"ridge_train_pred_1.csv\", index=False)\n\nsubmission.target = np.mean(np.column_stack(ridge_final_test_predictions), axis=1)\nsubmission.columns = [\"id\", \"ridgepred_1\"]\nsubmission.to_csv(\"ridge_test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:58:07.880755Z","iopub.execute_input":"2021-09-14T10:58:07.881181Z","iopub.status.idle":"2021-09-14T10:58:31.779521Z","shell.execute_reply.started":"2021-09-14T10:58:07.881137Z","shell.execute_reply":"2021-09-14T10:58:31.778423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating submission\ndf = pd.read_csv(\"../input/train-folds-5/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsubmission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nxgb_df = pd.read_csv('xgb_train_pred_1.csv')\nrf_df = pd.read_csv('rf_train_pred_1.csv')\n\nxgb_df_test = pd.read_csv('xgb_test_pred_1.csv')\nrf_df_test = pd.read_csv('rf_test_pred_1.csv')\n\nlgb_df = pd.read_csv('lgb_train_pred_1.csv')\nlgb_df_test = pd.read_csv('lgb_test_pred_1.csv')\n\ngbr_df = pd.read_csv('gbr_train_pred_1.csv')\ngbr_df_test = pd.read_csv('gbr_test_pred_1.csv')\n\nlass_df = pd.read_csv('lass_train_pred_1.csv')\nlass_df_test = pd.read_csv('lass_test_pred_1.csv')\n\nridge_df = pd.read_csv('ridge_train_pred_1.csv')\nridge_df_test = pd.read_csv('ridge_test_pred_1.csv')\n\n# *** Merging dataframes\ndf = df.merge(xgb_df, on=\"id\", how=\"left\")\ndf = df.merge(lgb_df, on=\"id\", how=\"left\")\ndf = df.merge(rf_df, on=\"id\", how=\"left\")\ndf = df.merge(gbr_df,on=\"id\", how=\"left\" )\ndf = df.merge(lass_df,on=\"id\", how=\"left\")\ndf = df.merge(ridge_df,on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(xgb_df_test, on='id',how=\"left\")\ndf_test = df_test.merge(lgb_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(rf_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(gbr_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(lass_df_test, on='id', how=\"left\")\ndf_test = df_test.merge(ridge_df_test, on='id', how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:58:31.781028Z","iopub.execute_input":"2021-09-14T10:58:31.781327Z","iopub.status.idle":"2021-09-14T10:58:36.874668Z","shell.execute_reply.started":"2021-09-14T10:58:31.781296Z","shell.execute_reply":"2021-09-14T10:58:36.87361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building a Linear Regression model \n\nkept_features = ['xgbpred_1','lgbpred_1', 'rfpred_1', 'gbrpred_1', 'lasspred_1', 'ridgepred_1']\ndf_test = df_test[kept_features]\nfinal_predictions = []\nscores = []\n\nfor fold in range(5):\n   # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n\n    valid_id = x_valid.id.values.tolist()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n\n    model = LinearRegression()\n    model.fit(x_train, y_train)\n    test_pred = model.predict(x_test)\n    valid_pred = model.predict(x_valid)\n    final_predictions.append(test_pred)\n    RMSE = mean_squared_error(y_valid, valid_pred, squared=False)\n    print (fold, RMSE)\n    scores.append(RMSE)\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:58:36.881637Z","iopub.execute_input":"2021-09-14T10:58:36.882002Z","iopub.status.idle":"2021-09-14T10:58:37.915945Z","shell.execute_reply.started":"2021-09-14T10:58:36.88195Z","shell.execute_reply":"2021-09-14T10:58:37.914896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.mean(np.column_stack( final_predictions), axis = 1)\n\nsubmission.target = preds\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:58:38.908079Z","iopub.execute_input":"2021-09-14T10:58:38.908907Z","iopub.status.idle":"2021-09-14T10:58:39.669524Z","shell.execute_reply.started":"2021-09-14T10:58:38.908864Z","shell.execute_reply":"2021-09-14T10:58:39.668313Z"},"trusted":true},"execution_count":null,"outputs":[]}]}