{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport calendar\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium.plugins import HeatMap\nfrom IPython.display import IFrame\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 500)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview (understanding the data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the study provided in the link below I've found description of column headers.<br><br>\nhttps://www.slideshare.net/PawanShivhare1/predicting-king-county-house-prices\n\nid - Unique ID for each home sold<br>\ndate - Date of the home sale<br>\nprice - Price of the home sale<br>\nbedrooms - Number of bedrooms<br>\nbathrooms - Number of bathrooms<br>\nsqft_living - Square footage of the apartments interior living space<br>\nsqft_lot - Square footage of lot (area around the house) space<br>\nfloors - Number of floors<br>\nwaterfront - Dummy variable whether house is located next to water body<br>\nview - Index from 0 to 4 of describing how good the view of the house is<br>\ncondition - Index from 1 to 5 describing what the condition of the building (1 is worst)<br>\ngrade - Index from 1 to 13 describing quality level of construction and design<br>\nsqft_above - Square footage of house interior that is above the ground level<br>\nsqft_basement - Square footage of house interior that is below the ground level<br>\nyr_built - Year in which house was built<br>\nyr_renovated - Year of last house renovation<br>\nzipcode - Zipcode<br>\nlat - Latitude<br>\nlong - Longitude<br>\nsqft_living15 - The average house square footage for the closest 15 houses<br>\nsqft_lot - The average lot square footage for the closest 15 houses<br><br>\nAdditional explanation you can find in the link below:<br>\nhttps://<span>info.kingcounty.gov/assessor/esales/Glossary.as</span>px?type=r"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing and initial analysis"},{"metadata":{},"cell_type":"markdown","source":"This part will be devoted to transform columns from dataframe to desired form (or create new ones from existing). This includes early form of both visualization and feature engineering."},{"metadata":{},"cell_type":"markdown","source":"<br>Check whether there are any NA or NULL values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Date</b><br>\nValues in date column are in form of datetime. My purpose is to create columns with years, months (abbreviation) and full date format."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['year_sale'] = df['date'].str[:4].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['month_sale_num'] = df['date'].str[4:6].astype(int)\ndf['month_sale_name'] = df['month_sale_num'].apply(lambda x: calendar.month_abbr[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['year_month_day_sale'] = pd.to_datetime(df['date'].str[:4] +\"-\"+ df['date'].str[4:6] + \"-\" +  df['date'].str[6:8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['month_sale_name'].sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Bathrooms</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bathrooms'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Floating point may indicate that bathrooms are not only counted how many of them are in the house. Some bathrooms may have more facalities (bathtub, shower)."},{"metadata":{},"cell_type":"markdown","source":"<br><b>Bedrooms</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bedrooms'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['bedrooms'] >= 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For modeling purposes it is resonable to change top five categories to one."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bedrooms'] = df['bedrooms'].apply(lambda x: 8 if x>=8 else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Floors</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['floors'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In my opinion there should be alternative floor column with only integer values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['floors_int'] = df['floors'].round(0).astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Waterfront</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['waterfront'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>View<b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['view'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Condition<b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['condition'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Grade</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['grade'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to kingcounty.gov grades 1-6 are low quality, 7 - 8 average, high 9 - 11 and 12 - 13 very high. Therefore I am going to put these grades into four categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"def grades_to_categories(col):\n    if col in [1,2,3,4,5,6]:\n        return 1\n    elif col in [7,8]:\n        return 2\n    elif col in [9,10,11]:\n        return 3\n    elif col in [12,13]:\n        return 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['grade_category'] = df['grade'].apply(grades_to_categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column display just to check..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['grade_category'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Year built and year renovated</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Year built:\",df['yr_built'].value_counts().count(),\"Year renovated:\",df['yr_renovated'].value_counts().count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So both year built and year renovated have large amount of unique values. Better idea is to display them on histograms."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['yr_built'], bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['yr_renovated'], bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From histograms I can read two conclusions. First is that during world war II there was significant drop in house building in the area. That may be not helpful but category indicating houses built before 1945 may help improve model.<br>\nSecond is kind of obvious. Most houses were not renovated. In my opinion best use of this column is to make another column indicating whether house was ever renovated."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['built_after_ww2'] = df['yr_built'].map(lambda x: x>1945)\ndf['house_renovated'] = df['yr_renovated'].map(lambda x: x != 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Year built and year renovated</b><br>\nIn the dataset description it is said that data was collected between May 2014 and May 2015. It is resonable to subtract date of built from appropriate date in year_sale column to see how many years have passed since."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['years_since_construction'] = df['year_sale'] - df['yr_built']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['years_since_construction'], bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, this histogram is reversed yr_built column."},{"metadata":{},"cell_type":"markdown","source":"<br><b>Zipcode</b>"},{"metadata":{},"cell_type":"markdown","source":"As long as in dataset there are geological coordinates, zipcode may not be useful to place households on the geological map but rather reveal better or worse districts."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['zipcode'].unique())\nprint(\"Numer of unique districts:\",df['zipcode'].unique().size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These numbers don't tell too much. My idea is just try to put them into model on later stage."},{"metadata":{},"cell_type":"markdown","source":"### Visualization and Outliers\n<br>The reason why I place this two issues into one topic is that visualzation techiques can easily help to catch outliers. This part may also contain further feature engineering."},{"metadata":{},"cell_type":"markdown","source":"<br><b>Price</b><br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\nsns.boxplot(y = df['price'], ax=axes[0])\n\nsns.distplot(df['price'], ax=axes[1])\nsns.despine(left=True, bottom=True)\n\naxes[0].set(ylabel='Price')\naxes[0].yaxis.tick_left()\n\naxes[1].yaxis.set_label_position(\"left\")\naxes[1].yaxis.tick_left()\naxes[1].set(xlabel='Price', ylabel='Distribution');\n\nfig, axes = plt.subplots(1,2,figsize=(15,10))\nsns.scatterplot(y = df['price'],x=df['sqft_living'], ax=axes[0])\nsns.scatterplot(y = df['price'],x=df['sqft_lot'], ax = axes[1])\naxes[0].set(xlabel = 'Square foot of living area',ylabel=\"Price\")\naxes[1].set(xlabel = 'Square foot of lot',ylabel=\"Price\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.loc[df['price'] >=4000000].shape[0],df.loc[df['price'] >=3000000].shape[0],df.loc[df['price'] >=2000000].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My conclusions are:<br>\nSquare footage of living area looks much more correlated to the price than area around the house. Therefore I will create the same scatterplots for each individual zipcode.\n\nThere are only 12 estates with price higher than 4m, 50 higher than 3 and 205 with price over 2m. I will keep this in mind and decide later whether to exclude these outliers (this may improve models)."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col = \"zipcode\", height=5,col_wrap=5)\ng.map(plt.scatter, \"price\",'sqft_living', color = 'red');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, col = \"zipcode\", height=5,col_wrap=5)\ng.map(plt.scatter, \"price\",'sqft_lot', color = 'blue');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at these scatterplots it is possible to recognize better or worse zipcodes but what's interesting I can select zipcodes that are probably in downtown (area around the house doesn't increase with the price). I will make another column with 'urban' zipcodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"zipcode_list = [98004,98006,98007,98008,98033,98034,98039,98040,98056,98102,98103,98105,98106,98107,98108,98109,98112,98115,98116,\n               98117,98118,98119,98122,98125,98126,98133,98136,98144,98146,98148,98155,98166,98168,98177,98178,98188,98198,98199]\n\ndf['urban_zipcode'] = df['zipcode'].map(lambda x: x in zipcode_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Different approach is to check how many houses has lot area equal to 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"sqft_lot equal to zero:\",df.loc[df['sqft_lot']==0].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no such cases."},{"metadata":{},"cell_type":"markdown","source":"<br><b>Longitude and Lattitude</b>\n<br>First thing to do is to check boundaries of both columns. The purpose is to check correctness of data (difference more than two degrees of parallels or meridians will could be suspicious)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Min:\",min(df['lat']), \"Max:\",max(df['lat']), \"Difference:\", max(df['lat']) - min(df['lat']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Min:\",min(df['long']), \"Max:\",max(df['long']), \"Difference:\",max(df['long']) - min(df['long']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this data looks okay."},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium.plugins import HeatMap\n\n\nm = folium.Map(location=[df['lat'].mean(), df['long'].mean(),],\n                        zoom_start=9.4,\n                        tiles=\"CartoDB dark_matter\")\n\n\nHeatMap(data=df[['lat','long']].groupby(['lat','long']).sum().reset_index().values.tolist(),radius=11.5).add_to(m)\n\n\n#m.save(\"map.html\")\n\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IFrame(src='map.html', width=700, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<i>Note: to generate html file folium module is needed. In order to do this, please use pip (pip install folium).</i>\n<br><br>The heatmap above is displaying density of house offers. If only I could find geojson file (marks districts borders) for King County, then I might write code for choropleth map. Choropleth looks like a patchwork. Areas have different colors that corresponds to the chosen attributes or statistics."},{"metadata":{},"cell_type":"markdown","source":"<br><b>Price vs House age</br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axe = plt.subplots(1, 1,figsize=(15,7))\nreg = sns.regplot(y = df['price'],x = df['years_since_construction'], scatter_kws={\"s\": 0.3})\naxes = reg.axes\naxes.set_ylim(0,1500000)\n\naxe.yaxis.set_label_position(\"left\")\naxe.yaxis.tick_left()\naxe.set(xlabel='House age', ylabel='Price');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, house age does not affect price so much. Regression line is quite parallel to the X axis descenging slightly with house age."},{"metadata":{},"cell_type":"markdown","source":"<br><b>Price vs Square footages</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sqft_regplot(outlier_limit,features):\n    \n    df_copy = df.copy()\n    df_copy = df_copy.loc[df_copy['price'] <= outlier_limit]\n    \n    fig, axes = plt.subplots(len(features), 1,figsize=(20,60))\n    \n    for i, feature in enumerate(features):\n        \n        reg = sns.regplot(x=df_copy[feature],y=df_copy['price'], ax=axes[i], fit_reg=True, scatter_kws={\"s\": 0.5})\n        reg.tick_params(labelsize=15)\n        ax = reg.axes\n        ax.set_xlabel(feature, fontsize = 30)\n        ax.set_ylabel('Price',fontsize= 30)\n        ax.grid(True)\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plot_sqft_regplot(3000000,['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As this check was made with sqft_lot, I will check whether sqft_basement and sqft_lot15 has values equal to 0. If so, there will be needed another column for such cases."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"sqft_basement equal to zero:\",df.loc[df['sqft_basement']==0].shape[0])\nprint(\"sqft_lot15 equal to zero:\",df.loc[df['sqft_lot15']==0].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over a half of houses have no basement. That deserves a separate category."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['no_basement'] = df['sqft_basement'].map(lambda x: int(x==0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Price vs categorical or quasi-categorical features</b><br>\nMy visualizations code is inspired by work from link below. Boxplots are doing amazing job with presenting a few statistics on one plot while there are only several categories to compare. For better visibility of boxplots I encapsulate these box plots into function with outlier limit (cutoff) option.\n<br>\nLink:\n<i>https://www.kaggle.com/burhanykiyakoglu/predicting-house-prices</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_categorical_features(outlier_limit):\n    \n    df_copy = df.copy()\n    df_copy = df_copy.loc[df_copy['price'] <= outlier_limit]\n    fig, axes = plt.subplots(5, 2,figsize=(17,45))\n\n    sns.boxplot(x=df_copy['grade'],y=df_copy['price'], ax=axes[0][0])\n    axes[0][0].set(xlabel='Grade', ylabel='Price')\n    axes[0][0].yaxis.tick_left()\n    axes[0][0].grid(True)\n\n    sns.boxplot(x=df_copy['grade_category'],y=df_copy['price'], ax=axes[0][1])\n    axes[0][1].yaxis.set_label_position(\"right\")\n    axes[0][1].yaxis.tick_right()\n    axes[0][1].set(xlabel='Grade categorized', ylabel='Price')\n    axes[0][1].grid(True)\n\n\n    sns.boxplot(x=df_copy['view'],y=df_copy['price'], ax=axes[1][0])\n    axes[1][0].yaxis.tick_right()\n    axes[1][0].set(xlabel='View', ylabel='Price')\n    axes[1][0].grid(True)\n\n    sns.boxplot(x=df_copy['waterfront'],y=df_copy['price'], ax=axes[1][1])\n    axes[1][1].yaxis.set_label_position(\"right\")\n    axes[1][1].yaxis.tick_right()\n    axes[1][1].set(xlabel='Waterfront', ylabel='Price')\n    axes[1][1].grid(True)\n\n\n    sns.boxplot(x=df_copy['built_after_ww2'],y=df_copy['price'], ax=axes[2][0])\n    axes[2][0].yaxis.tick_right()\n    axes[2][0].set(xlabel='Built after WW2?', ylabel='Price')\n    axes[2][0].grid(True)\n\n    sns.boxplot(x=df_copy['house_renovated'],y=df_copy['price'], ax=axes[2][1])\n    axes[2][1].yaxis.set_label_position(\"right\")\n    axes[2][1].yaxis.tick_right()\n    axes[2][1].set(xlabel='House renovated?', ylabel='Price')\n    axes[2][1].grid(True)\n\n    sns.boxplot(x=df_copy['condition'],y=df_copy['price'], ax=axes[3][0])\n    axes[3][0].yaxis.tick_right()\n    axes[3][0].set(xlabel='Condition', ylabel='Price')\n    axes[3][0].grid(True)\n\n    sns.boxplot(x=df_copy['urban_zipcode'],y=df_copy['price'], ax=axes[3][1])\n    axes[3][1].yaxis.set_label_position(\"right\")\n    axes[3][1].yaxis.tick_right()\n    axes[3][1].set(xlabel='Has urban zipcode?', ylabel='Price')\n    axes[3][1].grid(True)\n    \n    sns.boxplot(x=df_copy['month_sale_name'],y=df_copy['price'], ax=axes[4][0])\n    axes[4][0].yaxis.tick_right()\n    axes[4][0].set(xlabel='Month of sale', ylabel='Price')\n    axes[4][0].grid(True)\n\n    sns.boxplot(x=df_copy['no_basement'],y=df_copy['price'], ax=axes[4][1])\n    axes[4][1].yaxis.set_label_position(\"right\")\n    axes[4][1].yaxis.tick_right()\n    axes[4][1].set(xlabel='Has basement?', ylabel='Price')\n    axes[4][1].grid(True)\n\n    fig, axes = plt.subplots(3, 1,figsize=(17,25))\n\n    sns.boxplot(x=df_copy['bathrooms'],y=df_copy['price'], ax=axes[0])\n    axes[0].yaxis.tick_left()\n    axes[0].set(xlabel='Bathrooms', ylabel='Price')\n    axes[0].grid(True)\n\n    sns.boxplot(x=df_copy['bedrooms'],y=df_copy['price'], ax=axes[1])\n    axes[1].yaxis.tick_left()\n    axes[1].set(xlabel='Bedrooms', ylabel='Price')\n    axes[1].grid(True)\n    \n    sns.boxplot(x=df_copy['floors'],y=df_copy['price'], ax=axes[2])\n    axes[2].yaxis.tick_left()\n    axes[2].set(xlabel='Floors', ylabel='Price')\n    axes[2].grid(True);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categorical_features(2000000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusions:<br>\nGrade, waterfront, view, condition, number of bedrooms, number of bathrooms, house renovation, urban zipcode. These features, in differend degree, lift price up. What is suprising, unlike main trend - house age, houses built before WW2 are slightly more expensive."},{"metadata":{},"cell_type":"markdown","source":"## Ideas (feature engineering)"},{"metadata":{},"cell_type":"markdown","source":"In this part I am going to produce some fancy features. Multiplying, adding together, division two different columns may help inprove the model in unexpected way. Altogether with prevoiusly created features I plan to use only original (not processed) or resulting column in order to not confuse (overfit) model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def divide_bathrooms_bedrooms(bathrooms,bedrooms):\n    if bedrooms != 0:\n        return bathrooms/bedrooms\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bathrooms/bedrooms'] = df.apply(lambda x: divide_bathrooms_bedrooms(x.bathrooms,x.bedrooms),axis=1)\ndf.loc[df['bedrooms'] == 0].head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bathrooms*bedrooms'] = df['bathrooms']*df['bedrooms']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['waterfront+view'] = df['waterfront'] + df['view']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['over_one_floor'] = df['floors'].map(lambda x: int(x>1.))\ndf['over_two_floors'] = df['floors'].map(lambda x: int(x>2.))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['view_over_zero'] = df['view'].map(lambda x: int(x>0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation"},{"metadata":{},"cell_type":"markdown","source":"Correlation between a target variable (price) and other is a good indicator which features may be worth of using in model. On the other hand when two features correlate with each other strongly, that may lead model to overfitting. So resonable is to use only one of them.\nI make two correlation heatmap. One with Spearman more categorical variables and Pearson (default value) with continuous data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_correlation = df[['price','bedrooms','bathrooms', 'over_one_floor','over_two_floors','view_over_zero',\n                     'waterfront','view','condition', 'grade','house_renovated',\n                    'grade_category', 'built_after_ww2','urban_zipcode','no_basement','waterfront+view']].copy()\nplt.rcParams['figure.figsize']=(15,10)\nsns.heatmap(df_correlation.corr(method='spearman'), vmax=1., vmin=-1., annot=True, linewidths=.8, cmap=\"YlGnBu\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_correlation = df[['price','sqft_living','sqft_lot','sqft_above', 'sqft_basement','sqft_living15', 'sqft_lot15',\n                    'year_sale','month_sale_num','years_since_construction','bathrooms/bedrooms',\n                     'bathrooms*bedrooms', 'yr_built', 'yr_renovated','floors']].copy()\nplt.rcParams['figure.figsize']=(15,10)\nsns.heatmap(df_correlation.corr(), vmax=1., vmin=-1., annot=True, linewidths=.8, cmap=\"YlGnBu\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally one big correlation heatmap to see how features correlate with each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_correlation = df[['price','sqft_living','sqft_lot','sqft_above', 'sqft_basement','sqft_living15', 'sqft_lot15',\n                    'year_sale','month_sale_num','years_since_construction','bathrooms/bedrooms',\n                     'bathrooms*bedrooms', 'yr_built', 'yr_renovated','floors',\n                    'bedrooms','bathrooms', 'over_one_floor','over_two_floors','view_over_zero',\n                     'waterfront','view','condition', 'grade','house_renovated',\n                    'grade_category', 'built_after_ww2','urban_zipcode','no_basement','waterfront+view']].copy()\nplt.rcParams['figure.figsize']=(15,10)\nsns.heatmap(df_correlation.corr(), vmax=1., vmin=-1., annot=True, linewidths=.8, cmap=\"YlGnBu\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_curve, roc_auc_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\n\nfrom functools import partial\nfrom hyperopt import hp\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nimport random\nfrom math import sqrt\n\nrandom.seed(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Declare target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['price'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below there is declarated Cross Validation function. Cross Validation is used to comapre different models on one dataset. If we were dividing dataset on train and testing sets, there is a chance that there might be inequalities that could favor some algorithms. Stratified K Fold tries to distribute values and classes evenly between training and validation parts."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_validate(model, metric, X, y):\n    skf = KFold(n_splits = 8, shuffle= True)\n    \n    scores_metric = []\n    for train_idx, test_idx in skf.split(X,y):\n        model.fit(X[train_idx],y[train_idx])\n        y_pred = model.predict(X[test_idx])\n        \n        score = metric(y[test_idx],y_pred)\n        \n        scores_metric.append(score)\n\n        \n    result = np.mean(scores_metric)\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My experimental function. It is randomly trying to find what set of features gives best result for given model and metric. This should be treated just as additional help. I will not run it for all model as it takes a lot of time to perfom one run. This could be deveolped in future."},{"metadata":{"trusted":true},"cell_type":"code","source":"def best_features(dataframe, model,metric, features, repeats = 20, min_features = 1, max_features = 18):\n\n    best_score = 100000000000000000\n    best_feats = []\n    np.random.seed(2000)\n    \n    y = dataframe['price'].values\n    \n    if max_features > len(features):\n        max_features = len(features)\n        \n\n    for i in range(min_features,max_features): \n        for a in range(repeats): # repeat n times for this number of features\n            feats = np.random.choice(features,i,replace = False).tolist()\n            X = dataframe[feats].values\n            score = train_validate(model,metric,X,y)\n            if score < best_score:\n                best_score = score\n                best_feats = feats\n        print(\"Best score for {0} features is {1}\".format(len(feats),best_score))\n        print(feats)\n\n\n    print('\\n\\nBest score is {0} with features: {1}'.format(best_score,best_feats))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Metrics</b>"},{"metadata":{},"cell_type":"markdown","source":"In regression problems there are several metrics used to estimate how good the model is. Most common used are Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).<br>MAE, is just average absolute error (distance) between model estimation and real point.<br>RMSE on the other hand squares these errors and finally takes square root of its sum.\nRMSE is more fragile on large errors than MAE.\n<br>In both cases the lower value it is better.\n<br><br>\nR squared measures what is the quality of the model. It expands from 0 to 1. In regression if R^2 is equal to 1, then we can say that all positions of points on Y axis can be perfectly explained by their position on X axis, so model strictly fits the data.\n"},{"metadata":{},"cell_type":"markdown","source":"<br><b>Multilinear Regression</b>"},{"metadata":{},"cell_type":"markdown","source":"<b>Standarization</b><br><br>\nIf dataset contains features in different scales these both techniques allows to rescale them. This usualy improves some models (regression, k-nearest neighbours, SVM).<br>\nThere are two mainly used techniques to do this :<br> - normalization scales all values between 0 and 1, <br> - standarization (z-score) maps mean of feature values as 0 and unit is standard deviation.<br>\nStandarization is doing better when we deal with features containing outliers so I will use this one.<br>\nI propose to create separate pandas dataframe with standarized values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df.copy()\ndf_copy = df_copy.drop(columns=['id','date','month_sale_name','year_month_day_sale','zipcode'])\nnames = df_copy.columns\n\nscaler = StandardScaler()\nstandarized_df = pd.DataFrame(scaler.fit_transform(df_copy), columns = names)\nstandarized_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standarized_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = standarized_df.columns.to_list()\nfeatures.remove('price')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#function below is hashed because it takes several minutes to get the result.\n#best_features(standarized_df, LinearRegression(),mean_squared_error, features, max_features=25, repeats = 35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best score is 0.2824923891898764 with features: ['bathrooms', 'sqft_above', 'over_one_floor', 'waterfront+view', 'bathrooms*bedrooms', 'lat', 'month_sale_num', 'year_sale', 'sqft_living15', 'years_since_construction', 'sqft_basement', 'condition', 'sqft_living', 'no_basement', 'urban_zipcode', 'grade', 'house_renovated', 'bedrooms', 'over_two_floors', 'waterfront', 'yr_built', 'long', 'floors']"},{"metadata":{},"cell_type":"markdown","source":"R squared:"},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['bathrooms', 'sqft_above', 'over_one_floor', 'waterfront+view', 'bathrooms*bedrooms', 'lat', \n         'month_sale_num', 'year_sale', 'sqft_living15', 'years_since_construction', 'sqft_basement', \n         'condition', 'sqft_living', 'no_basement', 'urban_zipcode', 'grade', 'house_renovated', 'bedrooms', \n         'over_two_floors', 'waterfront', 'yr_built', 'long', 'floors']\n\ny = standarized_df['price'].values\nX = standarized_df[feats].values\n\nlin_reg = LinearRegression()\nlin_reg.fit(X,y)\nlin_reg.score(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now do the same steps with unstandarized dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df.columns.to_list()\n\nremove_list = ['price','id','month_sale_name','year_month_day_sale','date','zipcode']\n\nfor elem in remove_list:\n    features.remove(elem)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#function below is hashed because it takes several minutes to get the result.\n#best_features(df, LinearRegression(),mean_squared_error, features, max_features=25, repeats = 35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best score is 38082141181.18445 with features: ['bathrooms', 'sqft_above', 'over_one_floor', 'waterfront+view', 'bathrooms*bedrooms', 'lat', 'month_sale_num', 'year_sale', 'sqft_living15', 'years_since_construction', 'sqft_basement', 'condition', 'sqft_living', 'no_basement', 'urban_zipcode', 'grade', 'house_renovated', 'bedrooms', 'over_two_floors', 'waterfront', 'yr_built', 'long', 'floors']"},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['bathrooms', 'sqft_above', 'over_one_floor', 'waterfront+view', 'bathrooms*bedrooms', 'lat', \n         'month_sale_num', 'year_sale', 'sqft_living15', 'years_since_construction', 'sqft_basement', \n         'condition', 'sqft_living', 'no_basement', 'urban_zipcode', 'grade', 'house_renovated', 'bedrooms', \n         'over_two_floors', 'waterfront', 'yr_built', 'long', 'floors']\n\ny = df['price'].values\nX = df[feats].values\n\nlin_reg = LinearRegression()\nlin_reg.fit(X,y)\nlin_reg.score(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function has found the same sets of features in both cases. R squared is quite the same. Standarization didn't improve regression."},{"metadata":{},"cell_type":"markdown","source":"RMSE is just square root of MSE."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = round(sqrt(38082141181.18445),None)\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Ridge Regression</b> can improve model adding some Bias by alpha penalty in order to lower variance between datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nparameters = {'alpha':[0,1e-15,1e-10,1e-8,1e-4,1e-3,1e-2,1,2,3,5,10,15,20]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_regressor = GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error', cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_regressor.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ridge_regressor.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ridge_regressor.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = round(sqrt(38214893294.43146),None)\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ridge regression has slightly worse RMSE metric value but it's due to adding bias to the model."},{"metadata":{},"cell_type":"markdown","source":"<br><b>K Nearest Neighbors</b>"},{"metadata":{},"cell_type":"markdown","source":"Having GPS coordinates (lattitude and longitude) it comes to mind to use them somehow. Maybe expensive neighborhood lifts price high. Here comes KNeighborsRegressor aglorithm.\n<br>Let's try first with a few featuers and determine how many neighbors are doing best job."},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['lat','long','sqft_living']\nX = df[feats].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,16):\n\n    KNR = KNeighborsRegressor(n_neighbors=i)\n    score = train_validate(KNR,mean_squared_error,X,y)\n    rmse = sqrt(score)\n    print('Neighbors: {0}, MSE: {1}'.format(i,rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df.columns.to_list()\n\nremove_list = ['price','id','month_sale_name','year_month_day_sale','date','zipcode']\n\nfor elem in remove_list:\n    features.remove(elem)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#function below is hashed because it takes several minutes to get the result.\n#best_features(df, KNeighborsRegressor(n_neighbors=9),mean_squared_error, features, max_features=25, repeats = 35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best score is 29912478092.967567 with features: ['lat', 'grade', 'view_over_zero', 'long']"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['lat', 'grade', 'view_over_zero','long']\n\nX = df[features].values\ny = df['price']\n\nKNR = KNeighborsRegressor(n_neighbors=9)\nscore = train_validate(KNR,mean_squared_error,X,y)\nrmse = round(sqrt(score),None)\nprint('Neighbors: {0}, RMSE: {1}'.format(9,rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, this algorithm has better performance than regression. Finding best features selection manually could bring better solutions but it also takes more time.<br>\nFinally there is a chance that models overfits data, so it should be runned again on testing dataset to verify results."},{"metadata":{},"cell_type":"markdown","source":"<br><b>Random Forest Regressor and XGBoost Regressor</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['lat','long','sqft_living']\nX = df[feats].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB\nprint(sqrt(train_validate(xgb.XGBRegressor(),mean_squared_error,X,y)))\n# Random Forest Regressor\nprint(sqrt(train_validate(RandomForestRegressor(),mean_squared_error,X,y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even without feature selection and hyperparameters adjustment these models are doing better than previous aglorithms."},{"metadata":{},"cell_type":"markdown","source":"<b>XGBoost Regressor</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['sqft_living','sqft_lot','sqft_living15', 'sqft_lot15','years_since_construction',\n                     'bathrooms*bedrooms', 'yr_built', 'yr_renovated','floors_int',\n                     'over_one_floor','over_two_floors', 'sqft_basement',\n                     'waterfront','view','condition', 'grade','house_renovated',\n                     'built_after_ww2','urban_zipcode','no_basement', 'lat','long']\nX = df[feats].values\nprint(sqrt(train_validate(xgb.XGBRegressor(),mean_squared_error,X,y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyperparameters Optimization<br>\n<i>It takes significant amount of time to run hyperopt loop, so I hash code below (the same with Random Forest Regressor) and paste results in markdown</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function below is hashed because it takes several minutes to get the result.\n# def objective(space):\n#     params = {\n#         'eta':space['eta'],\n#         'max_depth':int(space['max_depth']),\n#         'min_child_weight':int(space['min_child_weight']),\n        \n#     }\n    \n#     model = xgb.XGBRegressor(**params)\n    \n#     score = sqrt(train_validate(model,mean_squared_error,X,y))\n#     print('Score: {0}'.format(score))\n#     return {'loss':score,'status':STATUS_OK}\n\n\n# space = {\n#     'eta':hp.uniform('eta',0.1,1),\n#     'max_depth':hp.quniform('max_depth',1,70,1),\n#     'min_child_weight':hp.quniform('min_child_weight',0,150,1)\n# }\n\n\n# trials = Trials()\n# best_params = fmin(fn = objective,\n#                   space = space,\n#                   algo=partial(tpe.suggest, n_startup_jobs = 10),\n#                   max_evals = 20,\n#                   trials = trials)\n\n# print('Best params: ', best_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"best loss: 124122.52545991719\nBest params:  {'eta': 0.43924184444710274, 'max_depth': 6.0, 'min_child_weight': 30.0}"},{"metadata":{},"cell_type":"markdown","source":"<br><b>Random Forest Regressor</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['sqft_living','sqft_lot','sqft_living15', 'sqft_lot15','years_since_construction',\n                     'bathrooms*bedrooms', 'yr_built', 'yr_renovated','floors_int',\n                     'over_one_floor','over_two_floors', 'sqft_basement',\n                     'waterfront','view','condition', 'grade','house_renovated',\n                     'built_after_ww2','urban_zipcode','no_basement', 'lat','long']\nX = df[feats].values\nprint(sqrt(train_validate(RandomForestRegressor(),mean_squared_error,X,y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyperparameter Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function below is hashed because it takes several minutes to get the result.\n# def objective(space):\n#     params = {\n#         'max_depth':int(space['max_depth']),\n#         'min_samples_split':int(space['min_samples_split']),\n#         'max_features':int(space['max_features'])\n#     }\n    \n#     model = RandomForestRegressor(**params)\n    \n#     score = sqrt(train_validate(model,mean_squared_error,X,y))\n#     print('Score: {0}'.format(score))\n#     return {'loss':score,'status':STATUS_OK}\n\n\n# space = {\n#     'max_depth':hp.quniform('max_depth',1,35,1),\n#     'min_samples_split':hp.quniform('min_samples_split',2,100,1),\n#     'max_features':hp.quniform('max_features',1,10,1)\n# }\n\n\n# trials = Trials()\n# best_params = fmin(fn = objective,\n#                   space = space,\n#                   algo=partial(tpe.suggest, n_startup_jobs = 50),\n#                   max_evals = 150,\n#                   trials = trials)\n\n# print('Best params: ', best_params)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'max_depth': 30.0, 'max_features': 10.0, 'min_samples_split': 2.0"},{"metadata":{},"cell_type":"markdown","source":"<br><b>Dataset split and testing</b>"},{"metadata":{},"cell_type":"markdown","source":"Now it is time to put hyperparameters to models and simulate (by dividing dataset on train and test part) how would they work in reallife problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_feature_importances(model, features):\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    plt.figure(figsize=(10, 5))\n    plt.title(\"Feature importances\")\n    plt.bar(range(X.shape[1]), model.feature_importances_[indices],\n           color=\"b\", align=\"center\")\n    plt.xticks(range(X.shape[1]), [ features[x] for x in indices] )\n    #plt.xticks(range(X.shape[1]), model.feature_importances_[indices])\n    plt.xticks(rotation=90)\n    plt.xlim([-1, X.shape[1]])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<i>Feature importance function above</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = ['sqft_living','sqft_lot','sqft_living15', 'sqft_lot15','years_since_construction',\n                     'bathrooms*bedrooms', 'yr_built', 'yr_renovated','floors_int',\n                     'over_one_floor','over_two_floors', 'sqft_basement',\n                     'waterfront','view','condition', 'grade','house_renovated',\n                     'built_after_ww2','urban_zipcode','no_basement', 'lat','long']\nX = df[feats].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, test_X, train_y, test_y = train_test_split(X,y,test_size = 0.3, random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>XGBoost Regressor result model</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_XGBoostRegressor = xgb.XGBRegressor(eta = 0.4392, max_depth = 6, min_child_weight = 30)\nmodel_XGBoostRegressor.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_XGBoostRegressor.predict(test_X)\nprint(\"RMSE error: {0}\".format(sqrt(mean_squared_error(test_y,y_pred))))\nprint(\"MAE error: {0}\".format(mean_absolute_error(test_y,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_feature_importances(model_XGBoostRegressor,feats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br><b>Random Forest Regressor result model</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_RandomForestRegressor = RandomForestRegressor(max_depth=30, max_features='auto',min_samples_split=2)\nmodel_RandomForestRegressor.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_RandomForestRegressor.predict(test_X)\nprint(\"RMSE error: {0}\".format(sqrt(mean_squared_error(test_y,y_pred))))\nprint(\"MAE error: {0}\".format(mean_absolute_error(test_y,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_feature_importances(model_RandomForestRegressor,feats)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig, axe = plt.subplots(1, 1,figsize=(15,7))\nscatter = sns.scatterplot(x = y_pred,y = test_y)\naxes = scatter.axes\nplt.title('Random Forest Price actual vs predicted')\nplt.grid(True)\naxe.yaxis.set_label_position(\"left\")\naxe.yaxis.tick_left()\naxe.set(xlabel= 'Price predicted', ylabel='Price actual');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatterplot above compares actual house price with predicted price. It is good way to catch wrongly predicted cases."},{"metadata":{},"cell_type":"markdown","source":"## Conclusions and further steps"},{"metadata":{},"cell_type":"markdown","source":"Finally XGBoost Regressor has lower RMSE error and a little bit MAE error. This result does not mean that this model is better for the task. Results may change in effect of further adjustment. Moreover XGBoost is much slower than Random Forest (or should be used more computation power). <br><br>\nThis analysis and modeling were made to cleary show steps of how the analyst approaches regression problem. In real life conditions analysis and visualizations should go parallel to data preparation, feature engineering and modeling. Analyst chooses only best solutions, does not use jupyter notebook for this purposes, possibly takes advantages of OOP.\nModel I presented here can be developed. Here are some of my suggestions how: <br>\n\n- find real utility and public building locations (schools, hospitals, shop centres etc...). This can explain price differences,\n- compare different models (CatBoost LightGBM),\n- spend more time on feature selection and hyperparameter optimization (it takes significant amount of time to run hyperopt loop),\n- use two or more models to receive output value."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}