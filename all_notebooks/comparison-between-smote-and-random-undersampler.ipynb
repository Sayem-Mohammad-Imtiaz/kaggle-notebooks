{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import required datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import relevant datasets","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ntypes = {\n    'gender':'category',\n    'age': 'int64',\n    'ever_married':'category', \n    'work_type':'category',\n    'Residence_type':'category',\n    'smoking_status':'category'}\n\ndf = df.astype(types)\ndf.drop(columns='id', inplace=True)\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill for null in BMI data\ndf['bmi'] = df['bmi'].fillna(df['bmi'].mean())\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#Continuous variables\ntarget = df['stroke']\ncont = df.drop(columns=['hypertension', 'heart_disease'])\n\nsns.pairplot(cont, hue='stroke', diag_kind='kde')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Categorical variables\ncat = df[['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'stroke']]\n#cat = pd.get_dummies(cat)\n\nfig,((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(nrows=3,ncols=2, figsize=(20,12))\nsns.violinplot(data=df, x='gender', y='age', hue='stroke', split=True, inner='box', ax=ax1)\nsns.violinplot(data=df, x='ever_married', y='age', hue='stroke', split=True, inner='box', ax=ax2)\nsns.violinplot(data=df, x='work_type', y='age', hue='stroke', split=True, inner='box', ax=ax3)\nsns.violinplot(data=df, x='Residence_type', y='age', hue='stroke', split=True, inner='box', ax=ax4)\nsns.violinplot(data=df, x='smoking_status', y='age', hue='stroke', split=True, inner='box', ax=ax5)\nsns.kdeplot(data=df, x='bmi', y='age', hue='stroke', ax=ax6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA conclusions\n\nFrom the results, we can see that as age increases the probability of stroke increases.  \nHowever, even though age is a major contributing factor, a current smoker also has an elevated stroke risk level than other age groups.  \nWe also see that a BMI>25 (overweight) is an indicator of stroke","metadata":{}},{"cell_type":"code","source":"stroke = df['stroke'].value_counts()[1]\nno_stroke = df['stroke'].value_counts()[0]\npct_stroke = stroke/(stroke+no_stroke)*100\n\nprint('The number of positive stoke cases in dataset is {:0.2f}%'.format(pct_stroke))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n\nPrepare for 2 cases \nCase 1 -> SMOTE Oversampling to oversample positive stroke cases to compensate for imbalanced data.  \nCase 2 -> Random Under Sampler to undersample negative stroke cases to compensate for imbalanced data","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE \nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.svm import SVC\nfrom imblearn.pipeline import make_pipeline\n\n\nvariable = df.drop(columns='stroke')\ntarget = df['stroke']\n\nX_train, X_test, y_train, y_test = train_test_split(variable, target, random_state=10, test_size=0.3)\n\n\nct = make_column_transformer((OneHotEncoder(handle_unknown='ignore'), ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']),\n                       (MaxAbsScaler(), ['avg_glucose_level', 'bmi', 'age']),\n                       remainder='passthrough')\n\nsmote = SMOTE(random_state=10)\nrus = RandomUnderSampler(random_state=10)\nsvc = SVC()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning\n\nAs the dataset is imbalanced (positive stroke cases much lower than total).  \nWe can compensate using Synthetic Minority Oversampling Technique (SMOTE) to balance the target dataset\n\n\nThe first case would test the effect of a StandardScaler and the second case would test the effect of Max Absolute Scaler","metadata":{}},{"cell_type":"code","source":"# Case 1 SMOTE\ncase1 = make_pipeline(ct, smote,svc)\ncase1.fit(X_train, y_train)\ny_pred1 = case1.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Case 2 Random Under Sampler\ncase2 = make_pipeline(ct, rus,svc)\ncase2.fit(X_train, y_train)\ny_pred2 = case2.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nac1 = accuracy_score(y_test, y_pred1)\nac2 = accuracy_score(y_test, y_pred2)\n\nprint('The accuracy score of case 1 (SMOTE) is: {:.2f}% and case 2 (Random Under Sampler) is: {:.2f}%'.format(ac1*100,ac2*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report_imbalanced(y_test, y_pred1))\nprint(classification_report_imbalanced(y_test, y_pred2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my first time using SMOTE and Random Under Sampler to compensate for imbalanced data\n\nMany thanks to Aditi Mulye and lakshman raj as i refered quite abit to their submission.\n\nI would appreciate any comments on how to improve the scores as well as introducing any scoring techniques for imbalanced datasets. Ususally I would use an ROC curve to compare between two models but that is not possible with imblearn.","metadata":{}}]}