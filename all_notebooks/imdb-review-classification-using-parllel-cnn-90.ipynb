{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"# import required packages\nimport os\nimport re\nimport tarfile\nimport numpy as np\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom keras.utils import get_file\nfrom bs4 import BeautifulSoup\nfrom tensorflow import keras\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preperation:**\n\n*   The data set consists of 25k training and testing samples.The first step is to read the files from the directory and form the dataset by assigning labels to the reviews.\n\n*   The follwing helper functions are created in order to help the data pre-processing set. We will discuss one by one.","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to download data\ndef download_data():\n    data_dir = get_file('aclImdb_v1.tar.gz', 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n                        cache_subdir=\"datasets\", hash_algorithm=\"auto\", extract=True, archive_format=\"auto\")\n    my_tar = tarfile.open(data_dir)\n    my_tar.extractall('./data/')  # specify which folder to extract to\n    my_tar.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.   **Removal of HTML tags:** The reviews in the data set has html tags in the movie reviews in the dataset. The first step is to remove them as they are noise in our data.Below function helps us to achieve that.","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to remove html tags from the reviews\ndef remove_html_tags(data_raw):\n    pro_data = []\n    for i in range(len(data_raw)):\n        soup = BeautifulSoup(data_raw[i], \"html.parser\")\n        pro_data.append(soup.get_text())\n\n    return pro_data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n2.   **Removing special charecters and numbers:** The reviews in the dataset are ven assoicated with special charecters like /,\\,%,$,# which cannot contribute anything to the sentiment of the review. So below function will help us in removing them using regex.\n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to remove special charecters from the reviews\ndef remove_special_char(data_raw):\n    pro_data = []\n    for i in range(len(data_raw)):\n        review = re.sub('\\[[^]]*\\]', ' ', data_raw[i])\n        review = re.sub('[^a-zA-Z]', ' ', data_raw[i])\n        pro_data.append(review)\n\n    return pro_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.   **Removing Stop Words:** “stop words” usually refers to the most common words in a language. These words occur too often and doesn't contribute nuch to the sentiment of the review. But removing too many stop words will result in lossing the meaning od the text. So in order not to sacrifice meaning of the text, at first a tokenizer is used to fit the sentences of the training set. Then when observed the top words in the tokenizer appeared to be :\n\n\n\n```\n'': 1, 'the': 2, 'and': 3, 'a': 4, 'of': 5, 'to': 6, 'is': 7, 'it': 8, 'in': 9,\n 'i': 10, 'this': 11, 'that': 12, 's': 13, 'was': 14, 'as': 15, 'for': 16, \n 'with': 17, 'movie': 18, 'but': 19, 'film': 20, 't': 21, 'you': 22, 'on': 23, \n 'not': 24, 'he': 25, 'are': 26, 'his': 27, 'have': 28, 'be': 29, 'one': 30, \n 'all': 31, 'at': 32, 'they': 33, 'by': 34, 'an': 35, 'who': 36, 'so': 37, \n 'from': 38, 'like': 39, 'there': 40, 'her': 41, 'or': 42, 'just': 43,\n```\n\nWe can observe that the words which occured mostly in the dataset text are words like 'the','and','a','of'.... which doesn't consitute much information in the text. So a custom stop word list as below has been choosen:\n\n```\n['the','and','a','of','to', 'is', 'it', 'in', 'i', 'this', 'that', 's', 'was',\n 'as', 'for', 'with']\n```\n\nThe below helper function achieves the tas of removing stop words from the corpus and is as shown below:\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to remove stop words\ndef remove_stop_words(data_raw):\n    pro_data = []\n    for i in range(len(data_raw)):\n        text =  ' '.join([word for word in \n        data_raw[i].split() if word.lower() not in my_stop_words])\n        pro_data.append(text)\n\n    return pro_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.   **Padding the sequences:** In the model that will be used will be learning Word embeddings. To learn word embedding, input to that layer should be documents as a sequences of numbers for words taken from vocab and with constant size.\n\nConversion to sequences of tokens will be discussed next and the below function helps us to padd the sequences to constant length, so that they can be fed to Neural network to learn word embeddings:\n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to pad the sequences to constant length\ndef pad_sentence(data_raw, sentence_length):   \n    return pad_sequences(data_raw, maxlen=sentence_length, dtype='int32', \n    padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nA post truncation with a '0' value is choosen.\n\n\n5.   **Tokenization of the documents:** The next step taken is to tokenize the corpus documents so that all documents can be represented as list of numbers, where each word is assigned a unique value. A vocubalory or 10,000 size has been choosen so that all necessary words can be caputered. An Out of Vocabulary token is used to represent all the words that are not present in the vocabulary dictonary built using tokenzier.\n\nNext all the documents are converted to sequences of numbers using text_to_sequences helper function from TensorFlow. Before tokenization we will remove the stop wors from the corpus. The helper function is as below:\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to tokenize the sentences after removing html,secail cahrecters and stop words\ndef tokenize_sentences(train, test, vocab_size, sentence_length):\n#     train = remove_stop_words(train)\n#     test = remove_stop_words(test)\n    tokenizer = Tokenizer(num_words=vocab_size, lower=True, oov_token=\"<OOV>\")\n    tokenizer.fit_on_texts(train)\n\n    train = tokenizer.texts_to_sequences(train)\n    test = tokenizer.texts_to_sequences(test)\n\n    train = pad_sentence(train, sentence_length)\n    test = pad_sentence(test, sentence_length)\n    print(train.shape)\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n6.   **All pre-processing steps:** The below function cobines all the above processing steps in the order of reading the data from the directory, assigning labels to the documents,removing html tags, removing special charecters and numbers followed by stop words removal and tokenization.\n\nThis function returns the train set in the form ready to be fed to the Neural Net model that we are goinig to choose.s\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to perform all data preprocessing techniques and return the processed data\ndef get_data_preprocessed(train_data_dir, test_data_dir, vocab_size=6000, max_seq_length=600):\n    # dictionary mapping label name to numeric id\n    labels_index = {'pos': 1, 'neg': 0}\n\n    # reading train directory\n    train_texts = []  # list of text samples\n    train_labels = []  # list of label ids\n    for name in [\"pos\", \"neg\"]:\n        if name == \"pos\" or \"neg\":\n            path = os.path.join(train_data_dir, name)\n            if os.path.isdir(path):\n                label_id = labels_index[name]\n                for fname in sorted(os.listdir(path)):\n                    fpath = os.path.join(path, fname)\n                    text = open(fpath).read()\n                    train_texts.append(text)\n                    train_labels.append(label_id)\n\n    # reading test directory\n    test_texts = []  # list of text samples\n    test_labels = []  # list of label ids\n    for name in [\"pos\", \"neg\"]:\n        if name == \"pos\" or \"neg\":\n            path = os.path.join(test_data_dir, name)\n            if os.path.isdir(path):\n                label_id = labels_index[name]\n                for fname in sorted(os.listdir(path)):\n                    fpath = os.path.join(path, fname)\n                    text = open(fpath).read()\n                    test_texts.append(text)\n                    test_labels.append(label_id)\n\n    print(\"Data loading from directory finished, proceeding to pre-processing\")\n    train_texts = remove_html_tags(train_texts)\n    train_texts = remove_special_char(train_texts)\n    print(\"Removed html tags from the data\")\n    test_texts = remove_html_tags(test_texts)\n    test_texts = remove_special_char(test_texts)\n    print(\"Removed special charecters from the data\")\n\n    train_texts, test_texts = tokenize_sentences(train_texts, test_texts, vocab_size, max_seq_length)\n    print(train_texts.shape)\n    print(\"Tokenized and padded the sequences to constant length\")\n\n    return np.array(train_texts), np.array(np.eye(2)[train_labels]),np.array(test_texts), np.array(np.eye(2)[test_labels])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Selection:**\n\nThe model has been selected based on the paper **[\"Effective Use of Word Order for Text Categorization with Convolutional Neural Networks\"](https://arxiv.org/pdf/1412.1058.pdf) by  Rie Johnson and Tong Zhang**\n\n**Reference:**\n\nJohnson, Rie & Zhang, Tong. (2014). Effective Use of Word Order for Text Categorization with Convolutional Neural Networks'\n\n\n\n**The below is the summary of the paper and major considerations explored that made choose the model for classification:**\n\n\n\n*   Convolutionary Neural Network (CNN) is a neural network that can make use of the internal data structure, such as the 2D image data structure. The model selcted is based on CNN for exploring the 1D structure (namely, word order) of text data for accurate prediction.\n*   Rather than use low-dimensional word vectors as inputs, as is often the case, we apply CNN directly to high-dimensional text data which leads to the direct learning of word embedding of small text regions for classification use.\n*   The usage of n-grams and phrases to understand the meaning of the text,so that application like topic classifcation can be benifited. But i=most of the time the meaning of the words is not very effective in such cases for improving the accuracy of the model.\n*   In order to benoft from word order to understand the context CNN are used in this case. CNN is used on text categorization in order to make use the 1D structure (word order) of document data,so that each unit in the convolution layer responds to a small region of a document (a sequence of words). \n*   The authors directly used a embedding layer as th input to CNN network, so that the network can learn the embeddings on its own. Then the vector of sequences of embeddings is passed to the CNN network for performing classification.\n\nA helper function to create such model is as shown below:\n\n#####  **Imporatant Note:  Usage of Parllel CNN model:**\n*   The original model is a simple CNN network with just couple of 1D convolution layers.While this can be extended in several ways, the experiment of extending it to have parllel CNN i.e. it has two or more parllel layers as shown below:\n\n<!-- ![alt text](https://drive.google.com/uc?id=1PaiKfV3Ecb8M5aAe8DgH8LMFLVTqurkv) -->\n\n\n*   The idea is to learn multiple types of embedding of small text regions.So that they can complement each other to improve model accuracy. In this architecture, multiple convolution-pooling pairs with different region sizes (and possibly different region vector representations) are given one-hot vectors as input and produce feature vectors for each region; \n*  The concatenation of the produced feature vectors is done in the top layer.\n*  After the concatenation layer, we use batch normalization and drop out layer to reqularize the model well for unseen data.\n*  Then we use a Dense layer with 2 neurons as output layer with softmax as activation function.\n\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def create_model(emb_dim, num_words, sentence_length, hid_dim, class_dim, dropout_rate):\n    input_layer = tf.keras.layers.Input(shape=(sentence_length,), dtype=tf.int32)\n\n    layer = tf.keras.layers.Embedding(num_words, output_dim=emb_dim)(input_layer)\n\n    layer_conv3 = tf.keras.layers.Conv1D(hid_dim, 3, activation=\"relu\")(layer)\n    layer_conv3 = tf.keras.layers.GlobalMaxPooling1D()(layer_conv3)\n\n    layer_conv4 = tf.keras.layers.Conv1D(hid_dim, 2, activation=\"relu\")(layer)\n    layer_conv4 = tf.keras.layers.GlobalMaxPooling1D()(layer_conv4)\n\n    layer = tf.keras.layers.concatenate([layer_conv4, layer_conv3], axis=1)\n    layer = tf.keras.layers.BatchNormalization()(layer)\n    layer = tf.keras.layers.Dropout(dropout_rate)(layer)\n\n    output = tf.keras.layers.Dense(class_dim, activation=\"softmax\")(layer)\n\n    model = Model(inputs=input_layer, outputs=output)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training the Model:**","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to preprocess data set, create ,compile and train model\ndef run_model(x_train, y_train, emb_dim, hid_dim, batch_size, epochs, model_save_dir, num_of_classes, vocab_size,max_seq_length):\n    # creating a model with required parameters\n    model = create_model(emb_dim, vocab_size, max_seq_length, hid_dim, 2, DROPOUT_RATE)\n    model.summary()\n\n    # compiling the model\n    model.compile(loss=\"categorical_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                  metrics=[\"accuracy\"])\n\n    # declaring path to save models\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n\n    filepath = model_save_dir + \"/model-{epoch:02d}.hdf5\"\n\n    # declaring call back function\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy',\n                                                             verbose=1, save_best_only=True,\n                                                             save_weights_only=True, mode='auto')\n\n    # training the model with a validation split of 0.2\n    history = model.fit(x_train, y_train, batch_size=batch_size,\n                        validation_split=0.2, epochs=epochs, callbacks=[checkpoint_callback], verbose=1)\n\n\n    return history,model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to plot accuracy comparision graphs\ndef plot_graph_accuracy(hist, title_string):\n    plt.figure(figsize=(12, 8))\n    plt.plot(hist.history['accuracy'])\n    plt.plot(hist.history['val_accuracy'])\n    plt.title(title_string)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'val_accuracy'], loc='lower right')\n    plt.show()\n\n\n# function to plot accuracy comparision graphs\ndef plot_graph_loss(hist, title_string):\n    plt.figure(figsize=(12, 8))\n    plt.plot(hist.history['loss'])\n    plt.plot(hist.history['val_loss'])\n    plt.title(title_string)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'val_loss'], loc='lower right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"TRAIN_DATA_DIR = \"./data/aclImdb/train\"  # source: http://ai.stanford.edu/~amaas/data/sentiment/\nTEST_DATA_DIR = \"./data/aclImdb/test\"\nlabels_index = {'pos': 1, 'neg': 0}\n\n#declaring a custom stop words list to be removed from the reviews\nmy_stop_words = ['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for', 'with']\n\n#declaring drop out rate\nDROPOUT_RATE = 0.9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"if __name__ == \"__main__\":\n    vocab_size = 10000\n    max_seq_length = 1000\n    # 1. load your training data\n    # expecting that the dat is already available , so commenting the function\n    download_data()\n    print(\"downloaded data\")\n\n    # loading the train data and getting the data after pre-processing\n    # creating data sets\n    x_train, y_train, x_test, y_test = get_data_preprocessed(TRAIN_DATA_DIR, TEST_DATA_DIR,vocab_size, max_seq_length)\n    print(x_train.shape)\n    print(\"train data preprocessed 1\")\n    x_train, y_train = shuffle(x_train, y_train)\n    print(\"train data preprocessed 2\")\n\n    # 2. Train your network\n    # Make sure to print your training loss and accuracy within training to show progress\n    history, model = run_model(x_train, y_train, 500, 1024, 512, 5, \"./data/\", 2, 10000, 1000)\n    \n       # print accuracy and loss graphs\n    # Make sure you print the final training accuracy\n    print(\"final training accuracy: \",history.history['accuracy'][-1])\n    print(\"final training loss: \",history.history['loss'][-1])\n    \n    score = model.evaluate(x_test, y_test, batch_size=16)\n    print(\"test_accuracy:\", score)\n\n\n    # 3. Save your model\n    model.save(\"NLP_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"    # print accuracy and loss graphs\n#     plot_graph_accuracy(history, \"Accuracy\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#     plot_graph_loss(history, \"Loss\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}