{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Read the dataset using pandas and see some entries\ndf = pd.read_csv('/kaggle/input/ISLR-Auto/Advertising.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check for different attributes\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## See for point summaries\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check whether any column has null entries\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Find the total Advertising budget\ndf['Total_Advertising_Budget'] = df['TV'] + df['Radio'] + df['Newspaper']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Visualize the total advertising budget vs Sales\nsns.scatterplot(x='Total_Advertising_Budget', y='Sales', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check for correlation and draw the heat-map\ncorr = df.corr()\nsns.heatmap(corr, vmax=1, vmin=-1, annot=True, cmap='plasma')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We can also draw the pairplot for checking the relationship\nsns.pairplot(df, diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Prepare for linear regression\ncData = df.drop(['Unnamed: 0', 'Total_Advertising_Budget', 'Sales'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Identify X & Y\nX = cData\ny = df.Sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Prepare model\nreg_model = LinearRegression()\nreg_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, col_name in enumerate(X_train.columns):\n    print(\"Coefficeint of {} is {}\".format(col_name, reg_model.coef_[idx]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Check the scores over training data\nreg_model.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Check the r2 score over test data. This is really awesome\nreg_model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = reg_model.predict(X_test)\ny_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's also see the residual plots for ei = y-y^ vs y^\ny_predict_train = reg_model.predict(X_train)\nE = y_train - y_predict_train\nfigure, ax = plt.subplots(1,1)\nax.set_xlabel('Fitted Values')\nax.set_ylabel('Residuals')\nsns.scatterplot(y_predict_train, E.values, ax=ax)\n## From residual plots its clear that there seems to be some relationship between \n## residuals and predicted value. This means there is some level of non-linearity among predictors\n\n### Let's also indentify the outliers too \nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_train, y_predict_train)\nrmse = np.sqrt(mse)\n\n\nE[E/rmse >= 2]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's create a new data frame with actual and predicted value\nnew_df = X_test.copy()\nnew_df['Sales_predict'] = y_predict\nnew_df['Sales_actual'] = y_test\nnew_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's plot the pairplot with this new data frame\n## If you look from below graphs, test between each attribute and Sales_actual vs Sales_predict seems to be \n## quite close to each other\nsns.pairplot(new_df, diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n## Find the root mean square error and r2_score\nmse = mean_squared_error(y_test, y_predict)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_predict)\nprint(\"Root mean square error = \", rmse)\nprint(\"R2 score\", r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression assumes that predictors are independent or additive. However this is not always true. For e.g increasing budget for RADIO effects the sales but it also increases the effectiveness of TV advertising. So just main effects cannot capture this. In order in include such relationship we can add interaction terms. A linear model that uses radio, TV, and an interaction between the two to predict sales takes the form<br>\n* > sales = β0 + β1 × TV + β2 × radio + β3 × (radio × TV)\n* > = β0 + (β1 + β3 × radio) × TV + β2 × radio \n\n### We can interpret β3 as the increase in the effectiveness of TV advertising for a one unit increase in radio advertising (or vice-versa)."},{"metadata":{"trusted":true},"cell_type":"code","source":"### let's add more interaction terms and see that our predict model get's better\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\n## interaction_only states that only includes the interaction terms\npoly = PolynomialFeatures(degree=2, interaction_only=True)\n\n## Create the new training/test data with added interaction terms\nX_train2 = poly.fit_transform(X_train)\nX_test2 = poly.fit_transform(X_test)\n\n## Again fit the linear regression model on such train data\npoly_clf = linear_model.LinearRegression()\npoly_clf.fit(X_train2, y_train)\n\n#In sample (training) R^2 will always improve with the number of variables!\n## See the effects of the interaction terms\nprint(poly_clf.score(X_train2, y_train))\nprint(poly_clf.score(X_test2, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As we see above adding more interaction terms made the results better "},{"metadata":{"trusted":true},"cell_type":"code","source":"### let's add higher degree polynomial terms and see that our predict model get's better\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\npoly = PolynomialFeatures(degree=2)\n\n## Create the new training/test data with added interaction terms\nX_train2 = poly.fit_transform(X_train)\nX_test2 = poly.fit_transform(X_test)\n\n## Again fit the linear regression model on such train data\npoly_clf = linear_model.LinearRegression()\npoly_clf.fit(X_train2, y_train)\n\n#In sample (training) R^2 will always improve with the number of variables!\n## See the effects of the interaction terms\nprint(poly_clf.score(X_train2, y_train))\nprint(poly_clf.score(X_test2, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's also see the residual plots for ei = y-y^ vs y^\ny_predict_train = poly_clf.predict(X_train2)\nE = y_train - y_predict_train\nfigure, ax = plt.subplots(1,1)\nax.set_xlabel('Fitted Values')\nax.set_ylabel('Residuals')\nsns.scatterplot(y_predict_train, E.values, ax=ax)\n\n## from below plot there doesn't seem be any relationship between Residuals and Fitted Values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's also check distribution of the residuals\nsns.distplot(E.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's also build a OLS based model for regression\nimport statsmodels.api as sm\nfrom statsmodels.api import add_constant\nX = cData\nY = df.Sales\nX2 = add_constant(X)\nX_train, X_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.2, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(Y_train, X_train)\nlm2 = lm.fit()\nlm2.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## This will check the presence of multi-collinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}