{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read CSV and Display Options"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/allenunger-global-commodity-prices/all_commodities.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows',None)\n#col width option is not necessary, but can be usefull for looking at large amounts of text in columns such as sources.\n#pd.set_option('max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Establish DataFrames for Wheat Prices\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to analyze Wheat in the 100 years from 1801-1900  \n#Using loc indexing in combination with boolean expressions returns that snapshot\nwheat_df=df.loc[(df.Commodity == \"Wheat\")& (df['Item Year'] >1800)& (df['Item Year'] <1901)]\nwheat_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to check which locations have the most data for wheat\nwheat_counts=wheat_df.groupby([\"Location\",\"Commodity\"]).size().reset_index(name=\"Wheat_Counts\").sort_values(by=\"Wheat_Counts\",ascending=False)\nwheat_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I have identified locations that have 85 or more Data points for wheat in this time period\n#I want to take a look at if there will be potential issues with having different sources or varieties before proceeding.\n#I also want to make sure the data is continuous if possible. If not I will have to fill in with appropriate method\n\n# Porto, Portugal 1801-1854 two sets of data 2 sources, variety NA, too many missing data points,45, to continue with\n#wheat_df[wheat_df.Location==\"Porto\"]\n\n# Low, Ukraine 1801-1900 variety NA\n#wheat_df[wheat_df.Location==\"Lwow\"]\n\n# Tours France variety NA 1801-1900 variety NA\n#wheat_df[wheat_df.Location==\"Tours\"]\n\n# england/southengland dataset 1800-1900 variety NA for both and I will investigate before combining\n#wheat_df[wheat_df.Location==\"England\"]\n#wheat_df[wheat_df.Location==\"Southern England\"]\n\n# Arnhem, Netherlands 1800-1900 variety NA\n#wheat_df[wheat_df.Location==\"Arnhem\"]\n\n#Ghent, Belgium is NA variety and 1816-1900\n#wheat_df[wheat_df.Location==\"Ghent\"]\n\n#Krakow,Poland variety NA 1800-1900 2 different sources, so I will have to investigate before combining\n#wheat_df[wheat_df.Location==\"Krakow\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making dataframes for cities. At this point both England locations and both Krakow sources are seperate\n#England\nengland=wheat_df.loc[(wheat_df.Location==\"England\")].reset_index()\nsouthern_england=wheat_df.loc[(wheat_df.Location==\"Southern England\")].reset_index()\n#Poland\nkrakow_unger=wheat_df.loc[(wheat_df.Location==\"Krakow\") & (wheat_df.Sources==\"(Richard Unger)\")].reset_index()\nkrakow_gorkiewicz=wheat_df.loc[(wheat_df.Location==\"Krakow\") & (wheat_df.Sources!=\"(Richard Unger)\")].reset_index()\n#France\ntours=wheat_df.loc[(wheat_df.Location==\"Tours\")].reset_index()\n#Belgium\nghent=wheat_df.loc[(wheat_df.Location == \"Ghent\")].reset_index()\n#Netherlands\narnhem=wheat_df.loc[(wheat_df.Location == \"Arnhem\")].reset_index()\n#Ukraine\nlwow=wheat_df.loc[wheat_df.Location==\"Lwow\"].reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine Data for England and Krakow"},{"metadata":{"trusted":true},"cell_type":"code","source":"krakow_combined=krakow_gorkiewicz.copy()\nkrakow_combined[\"Standard Value\"]=(krakow_gorkiewicz[\"Standard Value\"] + krakow_unger[\"Standard Value\"])/2\nkrakow_combined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"england_combined=england.copy()\nengland_combined[\"Standard Value\"]=(england[\"Standard Value\"] + southern_england[\"Standard Value\"])/2\nengland_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the Percent Change Relative to 1816"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to calculate the Wheat price % change per year relative to the first year of data that all locations have, 1816\n#This could prove usefull both for filling in missing data points for Ghent which is missing values before 1816\n#It can be more resistant to violatility than a standard mean calculation in some cases.\n\n#Add a column for % change by year relative to the FIRST year by accessing the zero index value in \"Standard Value\" column\nengland_combined[\"england_pct_change\"] = england_combined[\"Standard Value\"] / england_combined[england_combined['Item Year'] == 1816]['Standard Value'].iat[0]-1\ntours[\"tours_pct_change\"] = tours[\"Standard Value\"] / tours[tours['Item Year'] == 1816]['Standard Value'].iat[0]-1\nkrakow_combined[\"krakow_pct_change\"] = krakow_combined[\"Standard Value\"] / krakow_combined[krakow_combined['Item Year'] == 1816]['Standard Value'].iat[0]-1\nlwow[\"lwow_pct_change\"] = lwow[\"Standard Value\"] / lwow[lwow['Item Year'] == 1816]['Standard Value'].iat[0]-1\narnhem[\"arnhem_pct_change\"] = arnhem[\"Standard Value\"] / arnhem[arnhem['Item Year'] == 1816]['Standard Value'].iat[0]-1\nghent[\"ghent_pct_change\"] = ghent[\"Standard Value\"] / ghent[ghent['Item Year'] == 1816]['Standard Value'].iat[0]-1\nengland_combined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating this new dataframe will streamline the aggregation process\nengland_combined2=england_combined[[\"england_pct_change\",\"Item Year\"]].copy() \ntours2=tours[[\"tours_pct_change\",\"Item Year\"]].copy() \nkrakow_combined2=krakow_combined[[\"krakow_pct_change\",\"Item Year\"]].copy() \nlwow2=lwow[[\"lwow_pct_change\",\"Item Year\"]].copy()  \narnhem2=arnhem[[\"arnhem_pct_change\",\"Item Year\"]].copy()  \nghent2=ghent[[\"ghent_pct_change\",\"Item Year\"]].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combine all the relevant data\ndflist=[england_combined2,krakow_combined2,lwow2,tours2,arnhem2,ghent2]\ndfs = [df.set_index('Item Year') for df in dflist]\nEurope= pd.concat(dfs, axis=1)\n#Europe= pd.concat(dflist, axis=1)\nEurope[\"avg_pct_change\"]=Europe.agg(\"mean\",axis=1)\nEurope=Europe.reset_index()\nEurope.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Impute Missing Values for Ghent"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I now have reasonable values for the missing values of Ghent\n\nghent3 = pd.merge(Europe[['Item Year', 'avg_pct_change']], ghent, how='outer', on=['Item Year'])\nghent3.loc[ghent3['Standard Value'].isnull(), 'Standard Value'] = (1+ghent3['avg_pct_change']) * ghent3[ghent3['Item Year'] == 1816]['Standard Value'].iat[0]\n#Need to fill in percent change for ghent as well\n\nghent3[\"ghent_pct_change\"] = ghent3[\"Standard Value\"] / ghent3[ghent3['Item Year'] == 1801]['Standard Value'].iat[0]-1\n#ghent3=ghent3.drop([\"level_0\",\"index\"],axis=1)\nghent3[\"Commodity\"].fillna(\"Wheat\",inplace=True)\nghent3[\"Location\"].fillna(\"Ghent\",inplace=True)\nghent3[\"Original Currency\"].fillna(\"Belgian Franc\",inplace=True)\nghent3[\"Standard Currency\"].fillna(\"Silver\",inplace=True)\nghent3[\"Orignal Measure\"].fillna(\"Kilogram\",inplace=True)\nghent3[\"Standard Measure\"].fillna(\"Kilogram\",inplace=True)\nghent3[\"Sources\"].fillna(\"(G. Avondts-P. Scholliers) (De Gentse Textielarbeiders in de 19e en 20e Eeuw dossier 5) (Brussels: Centrum voor Hedendaagse Sociale Geschiedenis-1977)\",inplace=True)\nghent3[\"Notes\"].fillna(\"(gpih.ucdavis.edu)-(D.S.Jacks_2001-P.H. Lindert_2008-J.W.Ambrosin_2007)\",inplace=True)\n\nghent3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build 95% Confidence Intervals"},{"metadata":{"trusted":true},"cell_type":"code","source":"#england_combined=england_combined.reset_index()\nengland_combined['20_yr_rolling_avg_price'] = england_combined[\"Standard Value\"].rolling(window=20).mean()\nengland_combined['20_yr_rolling_std_price'] = england_combined['Standard Value'].rolling(window=20).std()\nengland_combined['2_std_price_decline'] = england_combined['20_yr_rolling_avg_price'] - 2 * england_combined['20_yr_rolling_std_price']\nengland_combined['2_std_price_rise'] = england_combined['20_yr_rolling_avg_price'] + 2 * england_combined['20_yr_rolling_std_price']\nengland_combined.loc[england_combined['Standard Value'] >= england_combined['2_std_price_rise'], '95_pct_sig_move'] = 'sig_price_increase'\nengland_combined.loc[england_combined['Standard Value'] <= england_combined['2_std_price_decline'], '95_pct_sig_move'] = 'sig_price_decrease'\n\nengland_combined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recalculate Avg Percent Change for Europe 1801-1900\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"england_combined3=england_combined.copy() \ntours3=tours.copy() \nkrakow_combined3=krakow_combined.copy() \nlwow3=lwow.copy()  \narnhem3=arnhem.copy()  \nghent4=ghent3.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Same idea as above, but I need this to be with respect to 1801 this time.\nengland_combined3[\"england_pct_change\"] = england_combined[\"Standard Value\"] / england_combined[england_combined['Item Year'] == 1801]['Standard Value'].iat[0]-1\ntours3[\"tours_pct_change\"] = tours[\"Standard Value\"] / tours[tours['Item Year'] == 1801]['Standard Value'].iat[0]-1\nkrakow_combined3[\"krakow_pct_change\"] = krakow_combined[\"Standard Value\"] / krakow_combined[krakow_combined['Item Year'] == 1801]['Standard Value'].iat[0]-1\nlwow3[\"lwow_pct_change\"] = lwow[\"Standard Value\"] / lwow[lwow['Item Year'] == 1801]['Standard Value'].iat[0]-1\narnhem3[\"arnhem_pct_change\"] = arnhem[\"Standard Value\"] / arnhem[arnhem['Item Year'] == 1801]['Standard Value'].iat[0]-1\n#ghent2[\"ghent_pct_change\"] = ghent3[\"Standard Value\"] / ghent3[ghent3['Item Year'] == 1801]['Standard Value'].iat[0]-1\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"england_combined3=england_combined3[[\"england_pct_change\",\"Item Year\"]] \ntours3=tours3[[\"tours_pct_change\",\"Item Year\"]] \nkrakow_combined3=krakow_combined3[[\"krakow_pct_change\",\"Item Year\"]] \nlwow3=lwow3[[\"lwow_pct_change\",\"Item Year\"]]  \narnhem3=arnhem3[[\"arnhem_pct_change\",\"Item Year\"]]  \nghent3=ghent3[[\"ghent_pct_change\",\"Item Year\"]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With respect to the year 1801 I now have the percent differences and average percent difference for Europe.\ndflist2=[england_combined3,krakow_combined3,lwow3,tours3,arnhem3,ghent3]\ndfs2 = [df.set_index('Item Year') for df in dflist2]\nEurope_1801= pd.concat(dfs2, axis=1)\n\nEurope_1801[\"avg_pct_dif\"]=Europe_1801.agg(\"mean\",axis=1)\n\nEurope_1801=Europe.reset_index()\nEurope_1801.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}