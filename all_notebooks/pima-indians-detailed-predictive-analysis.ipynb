{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# Pregnancies Number of times pregnant\n#Glucose Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n#BloodPressure Diastolic blood pressure (mm Hg)\n#SkinThickness Triceps skin fold thickness (mm)\n#Insulin 2-Hour serum insulin (mu U/ml)\n#BMI Body mass index (weight in kg/(height in m)^2)\n#DiabetesPedigreeFunction Diabetes pedigree function\n#Age Age (years)\n#Outcome Class variable (0 or 1) 268 of 768 are 1, the others are 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#univariate Analysis\ndata.iloc[:,:-1].hist(bins=20, figsize=(20,10), grid=False, edgecolor='black', alpha=0.5, color='pink')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(data['Outcome'], palette='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"###################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Upsampling\n#As the data is less for outcome 1 so its better if we do upsampling for data\nfrom sklearn.utils import resample\ndata_majority= data.loc[data['Outcome']==0]\ndata_minority= data.loc[data['Outcome']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_majority.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_minority_resampled= resample(data_minority, replace=True, n_samples=500, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1= pd.concat([data_majority, data_minority_resampled])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#######################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#BOXPlot #univariate analysis\nfig,ax = plt.subplots(nrows=2, ncols=4, figsize=(20,10))\nfor i in range(0,4):\n    sns.boxplot(y=data1.iloc[:,i], ax=ax[0,i])\nfor j in range(4,8):\n    sns.boxplot(y=data1.iloc[:,j], ax=ax[1,j-4])\n#it seems that there are outliers in my dataset. But lets try to build model with outliers only            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bivariate Analysis\nsns.pairplot(data1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Predictive Modelling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X= data1.iloc[:,:-1]\nY= data1.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X, Y, random_state=0, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Logistic Regression\nlogreg= LogisticRegression()\nlogreg.fit(X_train, Y_train)\nfrom sklearn import metrics\nmetrics.accuracy_score(Y_test, logreg.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#SVM\nsvc= SVC(kernel='rbf', random_state=1)\nsvc.fit(X_train, Y_train)\nfrom sklearn import metrics\nmetrics.accuracy_score(Y_test, svc.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Decisison tree\ndtree= DecisionTreeClassifier(criterion='entropy', random_state=0)\ndtree.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, dtree.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Random Forest\nrforest= RandomForestClassifier(n_estimators=50, criterion='entropy', random_state=0)\nrforest.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, rforest.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#naive Bayes\nnb= GaussianNB()\nnb.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, nb.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#KNN\n#using optimum value of k to predict the accuracy \naccuracy=[]\nfor k in np.arange(2,20,1):\n    knn= KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n    knn.fit(X_train, Y_train)\n    acc=metrics.accuracy_score(Y_test, knn.predict(X_test))  \n    accuracy.append(acc)\n    \nprint(accuracy)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(np.arange(2,20,1), accuracy)\nplt.xlim(1,21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#KNN\n#taking sqrt of len of X_train values as k value\nknn= KNeighborsClassifier(n_neighbors=int(np.sqrt(X_train.shape[0])), metric='minkowski', p=2)\nknn.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, knn.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"###############################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#In a nutshell\nac= []\nlist=[LogisticRegression(), SVC(), DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(), GaussianNB()]\nfor i in list:\n    model= i\n    model.fit(X_train, Y_train)\n    a=metrics.accuracy_score(Y_test, model.predict(X_test))\n    ac.append(a)\n    \nprint(pd.Series(data=ac, index=['LogisticRegression', 'SVC', 'DecisionTreeClassifier', 'RandomForestClassifier', 'KNeighborsClassifier', 'NaiveBayes']))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##########################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets build each model in depth\n#check the corr\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Heatmap\nsns.heatmap(data.corr(), linecolor='black', linewidths=0.2, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Logistic regression\nlogreg1= LogisticRegression()\nlogreg1.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"logreg1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"metrics.confusion_matrix(Y_test, logreg1.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"metrics.accuracy_score(Y_test, logreg1.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"logreg1.coef_.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot=sns.barplot(x=X_train.columns.tolist(),y=logreg1.coef_.ravel())\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#as per coffecient using only 4 features to predict the linear model\na=pd.Series(logreg1.coef_.ravel(), index=X_train.columns.tolist()).sort_values(ascending=False)\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"var1= a.head(4).index.tolist()\nvar1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"logreg2= LogisticRegression()\nlogreg2.fit(X_train[var1], Y_train)\nmetrics.confusion_matrix(Y_test, logreg2.predict(X_test[var1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('The model accuracy for Log regression is {}'.format(metrics.accuracy_score(Y_test, logreg2.predict(X_test[var1]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#ROC-AUC curve\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfpr, tpr, thres= roc_curve(Y_test, logreg2.predict_proba(X_test[var1])[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"roc_auc= roc_auc_score(Y_test, logreg2.predict(X_test[var1]))\nprint(roc_auc)\nplt.plot(fpr, tpr, label='area={}'.format(roc_auc))\nplt.plot(np.arange(0,1.1,0.1), np.arange(0,1.1,0.1), linestyle='--')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"############################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#SVC with rbf kernel\nsvc1= SVC(kernel='rbf',random_state=1)\nsvc1.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, svc1.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"######Performing K fold validations with diff kernels#######"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nsvc1= SVC(kernel='rbf',random_state=1)\ncv_score= cross_val_score(svc1, X, Y, cv=10)\ncv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets try to find optimum value of C for Linear kernel\nacc=[]\nfor c in np.arange(0.1,100,10).tolist():\n    svc2= SVC(kernel='rbf',C=c,random_state=1)\n    cv_score= cross_val_score(svc2, X, Y, cv=10)\n    acc.append(cv_score.mean())\n    \nprint(acc)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(np.arange(0.1,100,10).tolist(), acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# C value in between 1 to 10\nacc=[]\nfor c in np.arange(1,10,1).tolist():\n    svc2= SVC(kernel='rbf',C=c,random_state=1)\n    cv_score= cross_val_score(svc2, X, Y, cv=10)\n    acc.append(cv_score.mean())\n    \nprint(acc)    \n#lets take c=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#optimal value of gaama\nac=[]\ngaama=[0.0001, 0.001, 0.01, 0.1, 1, 10]\nfor g in gaama:\n    svc2= SVC(kernel='rbf',gamma=g ,random_state=1)\n    cvs_score=cross_val_score(svc2, X, Y, cv=10)\n    ac.append(cvs_score.mean())\n    \nprint(ac)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(gaama, ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets perform Grid search cv for optimum value of c and gaama\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'C':[0.1, 1, 10, 100], 'gamma':[1,0.1,0.01,0.001],'kernel':['rbf']}\nsvc_model=SVC()\ngrid= GridSearchCV(svc_model, param_grid=param_grid, cv=5, refit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid.fit(X_train, Y_train)\ngrid.predict(X_test)\nmetrics.accuracy_score(Y_test, grid.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Print Hyperparameter\nprint('Best parameter : {}'.format(grid.best_params_))\nprint('Best Score: {}'.format(grid.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"########################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt= DecisionTreeClassifier(criterion='entropy', random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dt.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, dt.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ndt_roc_auc= roc_auc_score(Y_test,dt.predict_proba(X_test)[:,1])\ndt_roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thres= roc_curve(Y_test,dt.predict_proba(X_test)[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(fpr, tpr)\nplt.plot(np.arange(0,1.1,0.1), np.arange(0,1.1,0.1), linestyle='--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets draw the tree to analyse better\nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(dt, out_file=dot_data, feature_names=X_train.columns.tolist(), class_names=['0','1'], rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot1=sns.barplot(X_train.columns.tolist(), dt.feature_importances_)\nplot1.set_xticklabels(plot.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Now lets try to tune the hyper parameters in Decision tree. We have 4 parameters to tune\n#1 max_depth\n#2 min_samples_split\n#3 min_samples_leaf\n#4 max_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets keep max_depth from 1 to 10 and check accuracy\nmax_depth= np.linspace(1,20,20).tolist()\ntrain_result=[]\ntest_result=[]\nfor max_depth in max_depth:\n    dtr=DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n    dtr.fit(X_train, Y_train)\n    \n    acc_train= metrics.accuracy_score(Y_train, dtr.predict(X_train))\n    acc_test= metrics.accuracy_score(Y_test, dtr.predict(X_test))\n    \n    train_result.append(acc_train)\n    test_result.append(acc_test)\n    \nprint(train_result)\nprint('\\n')\nprint(test_result) \nplt.bar(np.linspace(1,20,20).tolist(), train_result, )\nplt.bar(np.linspace(1,20,20).tolist(), test_result, color='pink')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets check min_samples_split to tune the model\n#lets take it to 10 to 100%\nmin_samples_split= np.linspace(0.1,1,10).tolist()\ntrain_result=[]\ntest_result=[]\nfor min_samples_split in min_samples_split:\n    dtr=DecisionTreeClassifier(criterion='entropy', min_samples_split=min_samples_split)\n    dtr.fit(X_train, Y_train)\n    \n    acc_train= metrics.accuracy_score(Y_train, dtr.predict(X_train))\n    acc_test= metrics.accuracy_score(Y_test, dtr.predict(X_test))\n    \n    train_result.append(acc_train)\n    test_result.append(acc_test)\n    \nprint(train_result)\nprint('\\n')\nprint(test_result)\nplt.bar(np.arange(0.1, 1.1, 0.1).tolist(), train_result, edgecolor='black',linewidth=0.2, width=0.08)\nplt.bar(np.arange(0.1, 1.1, 0.1).tolist(), test_result, color='pink', edgecolor='black',linewidth=0.2, width=0.08)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets check max_features to tune the model\n\nmax_features= np.arange(1, (len(X.columns.tolist())+1)).tolist()\ntrain_result=[]\ntest_result=[]\nfor max_features in max_features:\n    dtr=DecisionTreeClassifier(criterion='entropy', max_features= max_features)\n    dtr.fit(X_train, Y_train)\n    \n    acc_train= metrics.accuracy_score(Y_train, dtr.predict(X_train))\n    acc_test= metrics.accuracy_score(Y_test, dtr.predict(X_test))\n    \n    train_result.append(acc_train)\n    test_result.append(acc_test)\n    \nprint(train_result)\nprint('\\n')\nprint(test_result)\nplt.bar(np.arange(1, (len(X.columns.tolist())+1)).tolist(), train_result, edgecolor='black',linewidth=0.2, )\nplt.bar(np.arange(1, (len(X.columns.tolist())+1)).tolist(), test_result, color='pink', edgecolor='black',linewidth=0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Grid Search CV\nfrom sklearn.model_selection import GridSearchCV\nparam={'max_depth':np.arange(1,21).tolist(), 'min_samples_split':np.linspace(0.1,1,10).tolist(),\n       'max_features':[\"auto\", \"sqrt\",\"log2\"]}\nmodel= DecisionTreeClassifier(criterion='entropy', )\ngrid= GridSearchCV(model, param_grid=param, refit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid_pred= grid.predict(X_test)\ngrid_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Tuned HyperParameter K : {}'.format(grid.best_params_))\nprint('Best Score : {}'.format(grid.best_score_))\nprint('Accuracy Score: {}'.format(metrics.accuracy_score(Y_test, grid_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Random forest\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nacc=[]\nfor i in np.arange(10,300,10).tolist():\n    rfr= RandomForestClassifier(n_estimators=i, criterion='entropy', random_state=1)\n    rfr.fit(X_train, Y_train)\n    a= metrics.accuracy_score(Y_test, rfr.predict(X_test))\n    acc.append(a)\n\nprint(acc)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(np.arange(10,300,10), acc)\nplt.show()\n#Seems like n_estimator= 20 has max accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Grid Search CV\nrfr= RandomForestClassifier(n_estimators=20, criterion='entropy', random_state=1)\nparam_grid= {'max_depth':np.arange(1,21).tolist(), 'min_samples_split':np.linspace(0.1,1,10).tolist(), \n             'max_features':[\"auto\", \"sqrt\",\"log2\"]}\nfrom sklearn.model_selection import GridSearchCV\ngrid_random= GridSearchCV(rfr, param_grid=param_grid, refit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid_random.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Tuned HyperParameter K : {}'.format(grid.best_params_))\nprint('Best Score : {}'.format(grid.best_score_))\nprint('Accuracy Score: {}'.format(metrics.accuracy_score(Y_test, grid_random.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"###################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using boosting techniques\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Adaboost Classifier\nkfold = model_selection.KFold(n_splits=10, random_state=5)\nmodel1= AdaBoostClassifier(n_estimators=20, random_state=5)\ncvscore= model_selection.cross_val_score(model1, X, Y, cv=kfold)\ncvscore.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#XGBoost Classifier\nk= model_selection.KFold(n_splits=10, random_state=10)\nmodel2= XGBClassifier(random_state=10)\ncvs= model_selection.cross_val_score(model2, X, Y, cv=k)\ncvs.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel3= GradientBoostingClassifier(n_estimators=150, random_state=1)\nmodel3.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, model3.predict(X_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}