{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Recognizing Traffic Signals with Keras CNN"},{"metadata":{},"cell_type":"markdown","source":"![traffic_signal](https://www.rhinocarhire.com/CorporateSite/media/Drive-Smart/Road-Signs/Spain-Road-Signs.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Hello Kagglers,\n\nIn this notebook I present you a simple convolutional networks model for the classification of about 40 classes of German traffic signs.\n\nThe development of the model and preprocessing of the images will be done with the Keras library. I will always try to prioritize simplicity, so that those who are starting with neural networks can benefit from it.\n\nWithout further ado, let's get started. I hope you enjoy it."},{"metadata":{},"cell_type":"markdown","source":"## The data\n\nExtracted from the Kaggle dataset description:\n\nThe German Traffic Sign Benchmark is a multi-class, single-image classification challenge held at the International Joint Conference on Neural Networks (IJCNN) 2011. The benchmark has the following properties:\n\n- Single-image, multi-class classification problem\n- More than 40 classes\n- More than 50,000 images in total\n- Large, lifelike database"},{"metadata":{},"cell_type":"markdown","source":"## Index\n\n1. [Importing the necessary libraries](#section1)\n2. [Load and format the data](#section2)\n3. [Preprocessing the data](#section3)\n4. [The model](#section4)\n5. [Validation of the model](#section5)"},{"metadata":{},"cell_type":"markdown","source":"### <a id='section1'>1. Importing the necessary libraries</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport cv2\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport time\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='section2'>2. Load and format the data</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will time our model\nstart = time.time()\n\ninput_path = \"../input/gtsrb-german-traffic-sign/\"\n\nimage_data = []\nimage_labels = []\n\n# Number of total classes\ntotal_classes = 43\n\n# Dimensions of our images\nheight = 32\nwidth = 32\nchannels = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the images from the correct path\nfor i in range(total_classes):\n    path = input_path + \"Train/\" + str(i)\n    images = os.listdir(path)\n    \n    for img in images:\n        try:\n            image = cv2.imread(path + '/' + img)\n            image_fromarray = Image.fromarray(image, \"RGB\")\n            resize_image = image_fromarray.resize((height, width))\n            image_data.append(np.array(resize_image))\n            image_labels.append(i)\n        except:\n            print(\"Error in Image loading\")\n            \n# Converting lists into numpy arrays\nimage_data = np.array(image_data)\nimage_labels = np.array(image_labels)\n\n# Time taken to load our images in seconds\nend = time.time()\nprint(\"Time taken: \", round(end-start, 5), \"seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='section3'>3. Preprocessing the data</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffling data\nshuffle_indexes = np.arange(image_data.shape[0])\nnp.random.shuffle(shuffle_indexes)\n\nimage_data = image_data[shuffle_indexes]\nimage_labels = image_labels[shuffle_indexes]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting training and testing dataset\nX_train, X_valid, y_train, y_valid = train_test_split(image_data, image_labels, test_size=0.2,\n                                                     random_state=2666, shuffle=True)\n\n# Scale the values between 0 and 1\nX_train = X_train / 255\nX_valid = X_valid / 255\n\n# The dimensions concur\nprint(\"X_train.shape\", X_train.shape)\nprint(\"X_valid.shape\", X_valid.shape)\nprint(\"y_train.shape\", y_train.shape)\nprint(\"y_valid.shape\", y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the labels into one hot encoding\ny_train = keras.utils.to_categorical(y_train, total_classes)\ny_valid = keras.utils.to_categorical(y_valid, total_classes)\n\n# The dimensions concur\nprint(y_train.shape)\nprint(y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.backend.clear_session() # Clearing previous session if there was any\nnp.random.seed(2666)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='section4'>4. The model</a>\n\n**Why convolutional networks**\n\nColor images commonly have over 30 million pixels per channel. Quickly we understand why ordinary neural networks were not feasible for comupting these figures. \n\nConvolution layers are the crux of the convolutional neural network. They use filters to extract features from the input image. A filter is also referred to as a kernel or feature detector. It can be thought of as a sliding window of weights. These weights are learned during training. That is, a CNN “learns” filters that are proficient at detecting certain types of visual features, such as a straight line, a semi-circle, and other image features that may be meaningless to humans, but that are helpful in determining a correct outcome.\n\nFilters usually have small width and height and, of course, they share the same depth as the input. For a color image, the filter depth is 3, for the three color channels of blue, green, and red. For a black and white image, the filter depth is 1.\n\nWe slide the filter over the image at a certain stride, and the dot product is computed. Stride is simply the number of pixels we slide over each time we move the window. The dot products create a new matrix called the convolved feature or activation map or feature map."},{"metadata":{},"cell_type":"markdown","source":"![Convlayer](https://miro.medium.com/max/1206/1*ZPXWZDIHFbTxs-6KVPS5gg.png)"},{"metadata":{},"cell_type":"markdown","source":"**LeNet** is a convolutional neural network structure proposed by Yann LeCun et al. in 1998. and is a simple convolutional neural network structure. This is the one we will implement here: In general, LeNet refers to lenet-5 and is a simple convolutional neural network. Note that LeNet uses `tahn` activation function and here we will use `reLu`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create our model with Keras is straightforward\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(filters=18, kernel_size=(5,5), strides=1, activation=\"relu\", \n                        input_shape=(height, width, channels)),\n    keras.layers.MaxPooling2D(pool_size=(2,2), strides=2),\n    keras.layers.Conv2D(filters=36, kernel_size=(5,5), strides=1, activation=\"relu\"),\n    keras.layers.MaxPooling2D(pool_size=(2,2), strides=2),\n    keras.layers.Conv2D(filters=36, kernel_size=(5, 5), activation=\"relu\"),\n    keras.layers.MaxPooling2D(pool_size=(1,1)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(units=72, activation=\"relu\"),\n    keras.layers.Dense(units=43, activation=\"softmax\"),\n])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compilation of our model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nepochs = 20\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_data=(X_valid, y_valid))\nvalidation_data = (X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='section5'> 5. Validation of the model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing accuracy on the reserved test set\ntest = pd.read_csv(input_path + \"/Test.csv\")\n\nlabels = test[\"ClassId\"].values\ntest_imgs = test[\"Path\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How an image looks like\nimg_index = 25\nimage = Image.open(input_path + test_imgs[img_index])\nimg = image.resize((height,width))\nimg = np.array(img) / 255.\nimg = img.reshape(1, height, width, channels)\n\nprint(img.shape)\nprint(labels[img_index])\nplt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't know what that 11 of the class means. Let us map these values and predict this same image afterwards"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dictionary to map classes.\nclasses = { \n    0:'Speed limit (20km/h)',\n    1:'Speed limit (30km/h)', \n    2:'Speed limit (50km/h)', \n    3:'Speed limit (60km/h)', \n    4:'Speed limit (70km/h)', \n    5:'Speed limit (80km/h)', \n    6:'End of speed limit (80km/h)', \n    7:'Speed limit (100km/h)', \n    8:'Speed limit (120km/h)', \n    9:'No passing', \n    10:'No passing veh over 3.5 tons', \n    11:'Right-of-way at intersection', \n    12:'Priority road', \n    13:'Yield', \n    14:'Stop', \n    15:'No vehicles', \n    16:'Veh > 3.5 tons prohibited', \n    17:'No entry', \n    18:'General caution', \n    19:'Dangerous curve left', \n    20:'Dangerous curve right', \n    21:'Double curve', \n    22:'Bumpy road', \n    23:'Slippery road', \n    24:'Road narrows on the right', \n    25:'Road work', \n    26:'Traffic signals', \n    27:'Pedestrians', \n    28:'Children crossing', \n    29:'Bicycles crossing', \n    30:'Beware of ice/snow',\n    31:'Wild animals crossing', \n    32:'End speed + passing limits', \n    33:'Turn right ahead', \n    34:'Turn left ahead', \n    35:'Ahead only', \n    36:'Go straight or right', \n    37:'Go straight or left', \n    38:'Keep right', \n    39:'Keep left', \n    40:'Roundabout mandatory', \n    41:'End of no passing', \n    42:'End no passing veh > 3.5 tons'\n          }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction of this image\npred = model.predict_classes(img)[0]\nprint(pred)\n\nsign = classes[pred]\nprint(sign)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load and preprocess test set\nstart = time.time()\ntest = pd.read_csv(input_path + 'Test.csv')\n\nlabels = test[\"ClassId\"].values\nimgs = test[\"Path\"].values\n\ndata = []\n\nfor img in imgs:\n    try:\n        image = cv2.imread(input_path + img)\n        image_fromarray = Image.fromarray(image, 'RGB')\n        resize_image = image_fromarray.resize((height, width))\n        data.append(np.array(resize_image))\n    except:\n        print(\"Error\")\n        \nX_test = np.array(data)\nX_test = X_test / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction of test set\npred = model.predict_classes(X_test)\n\n#Accuracy with the test data\nprint(accuracy_score(labels, pred))\nend = time.time()\nprint(\"Time taken: \", round(end-start,5), \"seconds\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}