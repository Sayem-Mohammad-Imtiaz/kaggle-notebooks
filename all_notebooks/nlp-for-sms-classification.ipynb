{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#for visualising\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataset=pd.read_csv(\"../input/spam.csv\",encoding=\"latin-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ab78875feb6ae4967deecbe973f169f02a7da95"},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4ef905be89f0df897a57a9e2bf8ebd6c8ecba71"},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"139be8d93fd1d73f3353a930de23f0b531f8b143"},"cell_type":"code","source":"dataset = dataset.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68bda9c05c42f244079876c7a6a5f28a5ae966bc"},"cell_type":"code","source":"dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a064c6c6b25ea7e7840d61e198c5b703c3bdc60"},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e798e5c8381773e4dd94eccfa0acbb421465879"},"cell_type":"code","source":"#for text preprocessing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d47a0ce05e04ad442383ca7cd24f19c6f3c1c99a"},"cell_type":"code","source":"corpus=[]\nfor i in range(0,len(dataset)):\n    review=re.sub(\"[^a-zA-Z]\",\" \",dataset.iloc[i][1])\n    review=review.lower()\n    review=review.split()\n    ps=PorterStemmer()\n    review=[ps.stem(word) for word in review if not word in set(stopwords.words(\"english\"))]\n    review=\" \".join(review)\n    corpus.append(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f340b6c3c56b48032e889f889058ab943c6c29f"},"cell_type":"code","source":"#before and after text preprocessing\nprint(dataset.iloc[6,1])\nprint(30*\"*\")\nprint(corpus[6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dc97ce3e50fef7e7755fc67ea3861db7bce3998"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\nX=cv.fit_transform(corpus).toarray()\ny=dataset.iloc[:,0].values\ny=[0 if each==\"ham\" else 1 for each in y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10238e3ecc7a855d5abf7f2db9233ffc62c9d3ae"},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab8fad7c79490725c712b81dbad96f293bd6a52b"},"cell_type":"code","source":"#Classification algorithms\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n#These are for visualising\nalgoritma=[]\naccuracy=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3816b6b7a9ac7c479dce1f0880832be0a5eea6bb"},"cell_type":"code","source":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier=DecisionTreeClassifier(random_state=0,criterion=\"entropy\")\nclassifier.fit(X_train,y_train)\n\n#predict\ny_pred=classifier.predict(X_test)\n\n\nfrom sklearn.metrics import confusion_matrix\ncm_dt=confusion_matrix(y_test,y_pred)\ndtscore=classifier.score(X_test,y_test)*100\nprint(\"Decision Tree accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Decision Tree\")\naccuracy.append(dtscore)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_dt,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Decision Tree Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3030b32e569d787b02fd770222799dea29939ca"},"cell_type":"code","source":"#Find the optimal  k-value\n\n#error=[]\n#knn_accuracy=[]\n#for i in range(3, 9): \n#    classifier=KNeighborsClassifier(n_neighbors=i)\n#    classifier.fit(X_train,y_train)\n#    pred_i = classifier.predict(X_test)\n#    error.append(np.mean(pred_i != y_test))\n#    knn_accuracy.append(classifier.score(X_test,y_test)*100)\n#plt.figure(figsize=(12, 6))  \n#plt.plot(range(3, 9), error, color='red', linestyle='dashed', marker='o',  \n#         markeclassifieracecolor='blue', markersize=10)\n#plt.title('Error Rate K Value')  \n#plt.xlabel('K Value')  \n#plt.ylabel('Mean Error')  \n#\n#plt.figure(figsize=(12, 6))  \n#plt.plot(range(3, 9), knn_accuracy, color='red', linestyle='dashed', marker='o',  \n#         markeclassifieracecolor='blue', markersize=10)\n#plt.title('Accuracy of K Value')  \n#plt.xlabel('K Value')  \n#plt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6c65178ff162b6ce787a823ebec04f908f47e16"},"cell_type":"markdown","source":"**If you're wondering how I found the k value, you can run the above code.\nThis process takes a very long time.**"},{"metadata":{"trusted":true,"_uuid":"f832524d43a0158eadfd3828534b4642308a8592"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nclassifier=KNeighborsClassifier(n_neighbors=3)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn= confusion_matrix(y_test, y_pred)\nknn=classifier.score(X_test,y_test)*100\nprint(\"KNN accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"KNN\")\naccuracy.append(knn)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_knn,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"KNN Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6250735d1bcfb317f04d578c082e3bdf8a1726d"},"cell_type":"code","source":"#Find the optimal n_estimators value\n\n#n_estimators = np.arange(1,200,5)\n#train_results = []\n#test_results = []\n#from sklearn.metrics import roc_curve, auc\n#for estimator in n_estimators:\n#   classifier = RandomForestClassifier(n_estimators=estimator)\n#   classifier.fit(X_train, y_train)\n#   train_pred = classifier.predict(X_train)\n#   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n#   roc_auc = auc(false_positive_rate, true_positive_rate)\n#   train_results.append(roc_auc)\n#   y_pred = classifier.predict(X_test)\n#   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n#   roc_auc = auc(false_positive_rate, true_positive_rate)\n#   test_results.append(roc_auc)\n#\n#plt.plot(n_estimators, train_results, \"b\", label=\"Train AUC\")\n#plt.plot(n_estimators, test_results, \"r\", label=\"Test AUC\")\n#plt.legend()\n#plt.ylabel(\"AUC score\")\n#plt.xlabel(\"n_estimators\")\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de3038bc5e83f7e23e5b8cf39a569cf1bd36f7e7"},"cell_type":"markdown","source":"**If you're wondering how I found the n_estimators value, you can run the above code.\nThis process takes a very long time.**"},{"metadata":{"trusted":true,"_uuid":"9140c5ec9d980c1db6c0fd79cd80c6a6119ccf7e"},"cell_type":"code","source":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=67,criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rf= confusion_matrix(y_test, y_pred)\nrandom_forest=classifier.score(X_test,y_test)*100\nprint(\"Random Forest accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Random Forest\")\naccuracy.append(random_forest)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_rf,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Random Forest Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11eccdbc53d0381d57dbc245f1e5512fa6b3a26e"},"cell_type":"code","source":"#SVM\nfrom sklearn.svm import SVC\nclassifier=SVC(kernel=\"linear\",random_state=0)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm = confusion_matrix(y_test, y_pred)\nsvm=classifier.score(X_test,y_test)*100\nprint(\"SVM accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"SVM\")\naccuracy.append(svm)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_svm,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"SVM Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5b4cc6a5e797f59dd8ecb546d116a4ae9ddb986"},"cell_type":"code","source":"#Navie Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier=GaussianNB()\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb= confusion_matrix(y_test, y_pred)\nnb=classifier.score(X_test,y_test)*100\nprint(\"Navie Bayes accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Navie Bayes\")\naccuracy.append(nb)\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_nb,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Navie Bayes Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c799636a59ef3b2cefdba3d38774327e07c4e695"},"cell_type":"code","source":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lr = confusion_matrix(y_test, y_pred)\nlr=classifier.score(X_test,y_test)*100\nprint(\"Logistic Regression accuracy=\",classifier.score(X_test,y_test)*100)\nalgoritma.append(\"Logistic Regression\")\naccuracy.append(lr)\n\n\n#plot confusion matrix\nf,ax=plt.subplots(figsize=(4,4))\nsns.heatmap(cm_lr,annot=True,fmt=\".0f\",ax=ax,linewidths=2,linecolor=\"blue\")\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.xlabel(\"predicted Values\")\nplt.ylabel(\"true values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ae49c242988a345af04888cd1b51971167cd136"},"cell_type":"code","source":"plt.figure(figsize=(24,24))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\n#Logistic Regression Confusion Matrix\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Decision Tree Confusion Matrix\nplt.subplot(2,3,2)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dt,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#K Nearest Neighbors Confusion Matrix\nplt.subplot(2,3,3)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Naive Bayes Confusion Matrix\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Random Forest Confusion Matrix\nplt.subplot(2,3,5)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Support Vector Machine Confusion Matrix\nplt.subplot(2,3,6)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\"d\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"050f5a48cced88e66976492b2c52dcc0799d58ac"},"cell_type":"code","source":"f=plt.subplots(figsize=(20,20))\nplt.bar(algoritma,accuracy,color=\"orange\")\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"832b27c0d267d8c13b55dc93608d8dee1bde96ec"},"cell_type":"markdown","source":"<h2>CONCLUSION</h2>\n    \n**    According to this dataset SVM algorithm has found the best result.The SVM algorithm mis-classified only 19 data and showed 98.29% success.** \n\n**Waiting for your questions and suggestions.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}