{"cells":[{"metadata":{"_uuid":"58d0dff91eee1c60b508dfba6e4f1862d7a2a587"},"cell_type":"markdown","source":"# PASSNYC - Semi-supervised graph inference approach \nIn this Data science for good vent, PASSNYC challenges us to assess the needs of students by using publicly available data to quantify the challenges they face in taking the SHSAT. The ideal solution for this challenge will be able to tell PASSNYC about the schools where the chances of improvements are highest. PASSNYC wants to recognize underperforming schools and recognize right proxy measures so that PASSNYC and its partners can provide outreach services to the needful. They have asked for 3 qualities in the analysis - \n- **Performance** - How well the need of PASSNYC services are there for recognized schools - By changing the threshold for deciding if a school is underperforming, PASSNYC can detect schools which needed their immediate help.\n- **Influential** - PASSNYC wants the winning entry to be influential to convince their stakeholders - In this analysis, we present a semi-supervised graph inference approach that solves the problem of limited SHSAT registration data and uses D5 SHSAT registration data and extrapolates it with label propagation algorithm to detect underperforming schools from other districts. We also provide driving factor analysis, revised proxies for deciding if a school is underperforming. These model are fairly advanced and simple so they will be easy for stakeholders and partners to understand.\n- **Shareable** - The complete analysis should be simple and sharable - the models we use are explained using visualization and are very easy to understand."},{"metadata":{"_uuid":"cffe6926088ebdc45edcaa2f972073c30245c1a4"},"cell_type":"markdown","source":"In this Analysis, we are not going to do extensive EDA (as there are a bunch of great EDA already present), rather we will construct machine learning model - semi-supervised learning models - to extrapolate limited data available to assign if a school is underperforming or not. This kernel solves the problem of unavailability of other district's SHSAT registration and appears rate datasets and in very advanced and modular fashion learn from the district 5 data and detect underperforming school.\n"},{"metadata":{"_uuid":"85bf2f5dc3b996cf5714517f09bbb3ef9351f787"},"cell_type":"markdown","source":"### Can we make a model for detecting under performing Schools with only D5 SHSAT regestration data ?\n### YES, We can, using Semi-supervised learning approach. Lets start the analysis ..."},{"metadata":{"_uuid":"87c4929184038af14d20f72ae0617d6a3765ef6d"},"cell_type":"markdown","source":"# This analysis is a two kernel series -\n- **1. Kernel 1 (Label Propagation learning)** - In this kernel, we will construct a semi-supervised clustering model called label propagation and will use the limited data available (D5 SHSAT data) and assign if a school is underperforming or performing well. The school is underperforming or not is assigned on the basis SHSAT appearance rate and test registration rate. The threshold for assigning a school as underperforming is modular and PASSNYC can modify and re-run the model for a more or less aggressive targeting of underperforming schools. Apart from being very unique and first of its kind analysis on kaggle, this analysis runs very fast (less than a minute). At the end, a visualization on the map is made to show how the algorithm is spreading labels and showing underperforming schools on the map.\n\n\n\n- **2. Kernel 2 (Analyzing Driving Factors and Scope estimator)** - This is the second kernel for PASSNYC which effectively use sophisticated data analysis technique like Decision trees to get the revised proxy measures for the results we are getting using Label propagation results and Bayesian network and recognize the driving factor for SHSAT. It uses decision trees as trees are very easy to explain to stakeholders the rules behind assigning a school underperforming. It also uses provides a beautiful visualization of the complete tree. Link - https://www.kaggle.com/maheshdadhich/driving-factors-model-and-proxy-measures\n\n"},{"metadata":{"_uuid":"7751384cf2eaea6702d639f5af9b6c68ac689aa9"},"cell_type":"markdown","source":"# Why PASSNYC should use this analysis (8 Resasons why!!) - \n\nThere are many reasons why PASSNYC should use this engine - \n\n| S. No. | Reason      |   Comment (explanation)       | \n| :----------: | :-----------------------------------: |:--------------------------:|\n|1. |  **Performance on limited data (Only D5)**   | This kernel uses a **semi-supervised learning** technique which uses the limited data e.g. a limited number of labels (D5 SHSAT data) and assigns underperforming schools in complete NY. ||\n|2. | **Two models - Semi-supervised and Supervised** | It provides **two model for qualifying schools as underperforming** - one being a semi-supervised model to assign a school as a purely statistical method and other models (supervised learning) to explain the process that can be used a call a school underperforming. ||\n|3. | **Modular Codes** | These kernels uses modular codes and uses a threshold on SHSAT appear rate and registration rate and ** these thresholds can be easily changed to get the schools needing immediate support** to schools underperforming.  PASSNYC can change these thresholds and re-run the model to make different strategies||\n|4. | **Speed** | These kernels run super fast (**less than a minute**) so if changing parameters and re-running the model to make more aggressive strategies are very easy.   ||\n|5. | **Causal relations/ Driving factors** | This analysis finds **driving factors for each variables impacting the performance of schools** and establish causal relationships between variables. ||\n|6. | **Influential** | It provides driving factors and revised proxy measures, a decision tree diagram to explain the complete process which is much more likely to convince stakeholders ||\n|7. |**Revised proxy measures** | The decision trees analysis provides revised proxy measures used to assign a school as underperforming ||\n|8. | **Validation of results** | At the end validation is done using **Principal componant analysis** and schools are plotted along 3 principal componants, the plots itself validates the results, grouping low performing schools together in lower dimensions||\n\n\n"},{"metadata":{"_uuid":"1d5386c6de432f782ad41538f7b402936c33bce9"},"cell_type":"markdown","source":"# Kernel 1 - Label Propagation Learning \n![](https://lh3.googleusercontent.com/-aqfZxlJOmVM/W2i4EZ8agoI/AAAAAAAAv8c/accbDiECI_gan7r5jrbtDv62MI4cf6p_gCL0BGAYYCw/h6740/2018-08-06.png)"},{"metadata":{"trusted":true,"_uuid":"d0280d52e05e42384dac4ae9c3a016c08fd07af4","collapsed":true},"cell_type":"code","source":"# Loading Packages \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport time\nimport os \nimport folium \nimport scipy.stats\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial import distance\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom sklearn import preprocessing\nfrom plotly import tools\nimport plotly.plotly as py\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport sklearn.semi_supervised \nfrom sklearn import datasets\nfrom sklearn.semi_supervised import LabelPropagation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"70034764e7d97f460d1a868995de863ecbce0d8e"},"cell_type":"code","source":"# importing files \ndsfg_path = '../input/data-science-for-good/'\nsafety_path = '../input/ny-2010-2016-school-safety-report/'\nclass_size_path = '../input/ny-2010-2011-class-size-school-level-detail/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"318149f6b0147210a382d0bbd64f243792c1eb1c","collapsed":true},"cell_type":"code","source":"# Reading school_df \nschools_df = pd.read_csv(dsfg_path+'2016 School Explorer.csv')\n# Sanity check \nprint(\"Shape of Schools data is {}\".format(schools_df.shape))\nprint(\"Number of nulls in data are {}\".format(schools_df.isnull().sum().sum()))\nschools_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba5704048cfeaea4f61739dc261d675faddf6e7f","collapsed":true},"cell_type":"code","source":"# loading class size data\nclass_size = pd.read_csv(class_size_path + '2010-2011-class-size-school-level-detail.csv')\nclass_size.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e201baf89ab8e1de1106225968ecf5ba51da80a","collapsed":true},"cell_type":"code","source":"# Laoding district 5 SAT data\nSAT_df = pd.read_csv(dsfg_path+ 'D5 SHSAT Registrations and Testers.csv')\nprint(SAT_df.shape)\nSAT_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84177487e81585a387f58ef4bd892c9bb6bd314f"},"cell_type":"markdown","source":"## Important functions for creating Master data "},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true,"collapsed":true,"_uuid":"2eac7da909a38377774c6f9c0d64f3eccc47e8c5"},"cell_type":"code","source":"# Function for treating Outliers \n# 1 outlier treatment\ndef outlier_treatment(df,columns):\n    \"\"\"Function to cap outliers\"\"\"\n    temp=pd.DataFrame(columns=['variable','q75','q25'],index=range(len(columns)))\n    i=0    \n    for col in columns:\n        temp['variable'][i]=col\n        q75, q25 = np.nanpercentile(df[col], [75 ,25])\n        caps1,caps2=np.nanpercentile(df[col],[5,95])\n        h= (q75 - q25)*1.5\n        temp['q75'][i]=q75+h\n        temp['q25'][i]=q25-h\n        df[col]=np.where(df[col]>(q75+h),caps2,df[col])\n        df[col]=np.where(df[col]<(q25-h),caps1,df[col])\n        i=i+1\n    return df\n\n# 2 Function converting string % values to int\ndef percent_to_int(df_in):\n    \"\"\"Function to make % strings cols to float\n        credit - infocusp's script \"\"\"\n    for col in df_in.columns.values:\n        if col.startswith(\"Percent\") or col.endswith(\"%\") or col.endswith(\"Rate\"):\n            df_in[col] = df_in[col].astype(np.object).str.replace('%', '').astype(float)\n    return(df_in)\n\n# 3 Label for categories \ndef Label_for_cat_var(df, col):\n    \"\"\"Function to define labels for categorical columns\"\"\"\n    le = preprocessing.LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\n    del le\n    return(df)\n\n# 4 #Binning:\ndef binning(col, cut_points, labels=None):\n    \"\"\"Function to assign bins for a given variable\"\"\"\n    #Define min and max values\n    minval = col.min()\n    maxval = col.max()\n\n    #create list by adding min and max to cut_points\n    break_points =  [0.0]+ cut_points + [100.0]\n\n    #if no labels provided, use default labels 0 ... (n-1)\n    if not labels:\n        labels = range(len(cut_points)+1)\n\n    #Binning using cut function of pandas\n    colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n    return(colBin)\n\n# 5 Crime to perc \ndef perc_crime_transformation(df):\n    \"\"\"Function to convert crime to a percentage value\"\"\"\n    for col in tranform_list:\n        max_col = df[col].max()\n        min_col = df[col].min()\n        delta = max_col - min_col\n        for i in list(range(crime_affect.shape[0])):\n            df.loc[i, col] = (df.loc[i, col]- min_col)*100.0/(delta)\n    return(df)\n\n# 6. Deciding clusters for location, 32 district is much more, we don't need that much variations\ndef elbow_curve(df):\n    \"\"\"function to determine the number of cluster using elbow curve\"\"\"\n    columns = ['Latitude','Longitude']\n    df_new = df[columns]\n    Nc = range(1, 50)\n    kmeans = [KMeans(n_clusters=i) for i in Nc]\n    kmeans\n    score = [kmeans[i].fit(df_new).score(df_new) for i in range(len(kmeans))]\n    score\n    plt.plot(Nc,score)\n    plt.xlabel('Number of Clusters')\n    plt.ylabel('Score')\n    plt.title('Elbow Curve')\n    plt.show()\n    return(0)\n\n# 7. Assign clusters \ndef Assign_k_means_cluster(df, n):\n    \"\"\"function to assign k-means clusters \n        n = number of clusters\n    \"\"\"\n    columns = ['Latitude','Longitude']\n    df_new = df[columns]\n    kmeans = MiniBatchKMeans(n_clusters=n).fit(df_new)\n    df.loc[:, 'location_cluster'] = kmeans.predict(df[columns])\n    #test_meta.loc[:, 'k_means_cluster'] = kmeans.predict(test_meta[columns])\n    return(df)\n\n# 8. Function to convert a string to upper case\ndef to_upper(row):\n    \"\"\"Function to convert a string to upper case \"\"\"\n    return(row.upper())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d922244c5d11de31b833618612796f414092bbb"},"cell_type":"markdown","source":"## Creating master_df for Label propagator "},{"metadata":{"_uuid":"ca647fbb0309b3ddfbaa28832b70a1492879df75"},"cell_type":"markdown","source":"#### Schools_df preprocessing"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0560acc99b2c7609f1eac06db4cb668dfa141d55"},"cell_type":"code","source":"# creating master_df \n# converting all perc values in school_df to int values \nschools_df = percent_to_int(schools_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56226395794516ff49d90f325be05e70cb25feb8","collapsed":true},"cell_type":"code","source":"# Removing outliers from the dataframe \ncols_to_filter_outlier = []\nsubj= {1:'Math',\n       2:'ELA'}\ngrade_name = [3,4,5,6,7,8]\nfor grade in grade_name:\n    for p in [1,2]:\n        var = 'Grade '+ str(grade)  +' '+subj[p]+' 4s - All Students'\n        cols_to_filter_outlier.append(var)\n        \nprint(cols_to_filter_outlier)\nschools_df = outlier_treatment(schools_df,cols_to_filter_outlier)\nschools_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cd2a7c5df3b91272d4fd99be81d2d5996f051ad","collapsed":true},"cell_type":"code","source":"# finding optimum number of location clusters in schools_df\nelbow_curve(schools_df) # To check what is the optimum number of clusters ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3362662971518be46079fa48c87c71f58192f14","collapsed":true},"cell_type":"code","source":"# Assigning locations clusters in schools_df \n# I name it bayes df as same df will be used in bayesian analysis\nbayes_df = Assign_k_means_cluster(schools_df, 10)\nbayes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75cc65dd59da760dc1bdf34bbb9b1b75f8aafc56","collapsed":true},"cell_type":"code","source":"# Assigning perc of 4s in different classes \n# columns which are already given in percentage values \ncols_having_perc = []\nfor col in bayes_df.columns.values:\n    if col.startswith(\"Percent\") or col.endswith(\"%\") or col.endswith(\"Rate\"):\n        cols_having_perc.append(col)\ncols_having_perc # percentage of columns having % in them \n\n\n\n# Columns we create having perc\n# Converting all grades values to perc \ngrades_name = [6,7,8] # 3,4,5,\nbayes_df.rename(columns = {'Grade 3 Math - All Students tested':'Grade 3 Math - All Students Tested'}, inplace = True)\n\n\n\n#assigning a indicator class_size_zero = 0 or 1\nprint(\"size before \", bayes_df.shape)\n\n\n\n# It doesn't take it if all students in that class are zero \nfor grade in grades_name:\n    for subj in ['ELA', 'Math']:\n        All = 'Grade '+str(grade)+' '+subj+' - All Students Tested'\n        bayes_df = bayes_df.loc[bayes_df[All]!=0]\n\n\n        \n        \nprint(bayes_df.shape)\ncreated_perc_col = []\nfor grade in grades_name:\n    for subj in ['ELA', 'Math']:\n        col_name = subj+'_'+str(grade)+'_4s'\n        created_perc_col.append(col_name)\n        All_4s = 'Grade '+str(grade)+' '+subj+' 4s - All Students'\n        All = 'Grade '+str(grade)+' '+subj+' - All Students Tested'\n        bayes_df.loc[:,col_name] = (bayes_df[All_4s].values*100.0)/bayes_df[All].values\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cc3392afbe6eaa9203a5c210527b8b65e4d4ea0","collapsed":true},"cell_type":"code","source":"# Assiging label for non-numeric columns \nnon_numeric_cols = ['Rigorous Instruction Rating','Collaborative Teachers Rating','Supportive Environment Rating','Effective School Leadership Rating','Strong Family-Community Ties Rating','Trust Rating','Student Achievement Rating']\nbayes_df.dropna(axis =0, subset =non_numeric_cols,  inplace = True)\nprint(bayes_df.shape)\nfor col in non_numeric_cols:\n    bayes_df = Label_for_cat_var(bayes_df, col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9690ead11172b3d5cf78a3618e4049c0cb1b712"},"cell_type":"markdown","source":"#### SAT df preprocessing\n"},{"metadata":{"trusted":true,"_uuid":"4e4d79556de36505fa98c1a06eab40fa6c94eeac","collapsed":true},"cell_type":"code","source":"# Rolling up the time series data \nSAT_df.head()\naggregate_func = {'Enrollment on 10/31':np.mean,\n                  'Number of students who registered for the SHSAT':np.mean,\n                  'Number of students who took the SHSAT':np.mean}\n\nprint(\"Shape of SAT d5 data is {}\".format(SAT_df.shape))\nSAT_summary = pd.DataFrame(SAT_df.groupby(['DBN','School name']).agg(aggregate_func))\nprint(\"Shape of SAT summary is {}\".format(SAT_summary.shape))\nSAT_summary.reset_index(inplace = True)\nSAT_summary.head()\n\n\n\n# SAT_summary Designing features for getting under performance - performance indicators\nSAT_summary['Registration_rate'] = SAT_summary['Number of students who registered for the SHSAT']/(SAT_summary['Enrollment on 10/31'])\nSAT_summary['Appear_rate'] = SAT_summary['Number of students who took the SHSAT']/(SAT_summary['Number of students who registered for the SHSAT'])\n\n\n\n# Low performing school\n# Any Schools whose registration_rate and Apprear rate will be less 1 std below mean values are under performing\nmean_reg_rate = SAT_summary['Registration_rate'].mean()\nmean_App_rate = SAT_summary['Appear_rate'].mean()\nstd_reg_rate  = SAT_summary['Registration_rate'].std()\nstd_App_rate  = SAT_summary['Appear_rate'].std()\n\n\n\n# Threshold decision - \n# **Basically use complete data to get the mean and thresholds and not do it based on D5 data**\nn = 1.0  # anything which is one standard deviation below mean is underperforming\n\nthreshold_reg = mean_reg_rate - n*std_reg_rate\nthreshold_App = mean_App_rate - n*std_App_rate\n\ndef Assign_underperforming(row):\n    \"\"\"Function to Assign if a school is under performing or not\"\"\"\n    underperform = 0\n    if row['Registration_rate'] <= threshold_reg:\n        underperform = 1\n    if row['Appear_rate'] <= threshold_App:\n        underperform = 1\n    return(underperform)\n\nSAT_summary['Underperforming']  = SAT_summary.apply(lambda row:Assign_underperforming(row), axis= 1)\nSAT_summary.sample(5).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feb7d6568924475160cc64125713d7498372e5ba"},"cell_type":"markdown","source":"#### Class_size data \n"},{"metadata":{"trusted":true,"_uuid":"90b7f62fe7179d2abe515287c42f1fcbc149a638","collapsed":true},"cell_type":"code","source":"class_size = pd.read_csv(class_size_path + '2010-2011-class-size-school-level-detail.csv')\nclass_size.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"724aa7f6f6d4dc3c2acd4fb91b2427c610590b9c","collapsed":true},"cell_type":"code","source":"print(\"Class size with Nulls {}\".format(class_size.shape))\nclass_size = class_size.loc[class_size['SCHOOLWIDE PUPIL-TEACHER RATIO'].notnull()]\nprint(\"Class size without Nulls {}\".format(class_size.shape))\nclass_size = class_size.drop_duplicates(subset = ['SCHOOL NAME'])\nclass_size['SCHOOL NAME'] = list(map(to_upper, class_size['SCHOOL NAME']))\nclass_size['CSD'] = list(map(str, class_size['CSD']))\nclass_size.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c37b0b37a91237d5211904cd32a37353f6e1579","collapsed":true},"cell_type":"code","source":"def make_proper_school_code(row):\n    \"\"\"Function to a proper school code for class_size df\"\"\"\n    code = ''\n    if row['CSD'].__len__()==1:\n        code = '0'+row['CSD']\n    code = code + row['SCHOOL CODE']\n    return(code)\n\n\nclass_size['school_code'] = class_size.apply(lambda row: make_proper_school_code(row), axis =1)\n\n\n# Sanity check \ncode1 = class_size.school_code.unique()\ncode2 = bayes_df['Location Code'].unique()\nclass_size.head()   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b07404223fc72e1fb950d132ea751de1218c5299"},"cell_type":"markdown","source":"#### Preparing Safety data for schools"},{"metadata":{"trusted":true,"_uuid":"85cf35fbfde65b8d8ef3c94acd49455e73bdc6c6","collapsed":true},"cell_type":"code","source":"\nsafety_df = pd.read_csv(safety_path+ '2010-2016-school-safety-report.csv')\nsafety_df.head()\naggregate_crime_stats = {'Major N':np.mean,\n                         'Oth N':np.mean,\n                         'NoCrim N':np.mean,\n                         'Prop N':np.mean,\n                         'Vio N':np.mean}\ncrime_summary = pd.DataFrame(safety_df.groupby(['Geographical District Code']).agg(aggregate_crime_stats))\ncrime_summary.reset_index(inplace = True)\ncrime_summary['Geographical District Code'] = list(map(int, crime_summary['Geographical District Code']))\nprint(\"The shape of locatio wise crime data is {}\".format(crime_summary.shape))\ncrime_summary.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dd80ee4baa4a8b1e645887880188a44078e3254"},"cell_type":"markdown","source":"## Merging to make master_df for Label propagation \n"},{"metadata":{"trusted":true,"_uuid":"8f65381947a7b876cb1c3bc0ac18dd6eb3e4024f","collapsed":true},"cell_type":"code","source":"# Merge Other dataframes and make master_df\nmaster_df = bayes_df.merge(SAT_summary, left_on = 'Location Code', right_on = 'DBN', how = 'left')\nprint(master_df.shape)\nmaster_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32b94d36bc080f610790d0022b5cb6c656c95e3b","collapsed":true},"cell_type":"code","source":"# Merging class df for crime area wise summary \nmaster_df = master_df.merge(crime_summary, left_on = 'District', right_on = 'Geographical District Code', how = 'left')\nprint(\"Shape of master_df is {}\".format(master_df.shape[0]))\nmaster_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c1fd65f4c54869f27ae3046e6843fcb8bc31f626","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\nCols_lp = [#'Community School?',\n           'Economic Need Index', \n           'Percent ELL',\n           #'Percent Asian',\n           #'Percent Black',\n           #'Percent Hispanic',\n           #'Percent Black / Hispanic',\n           #'Percent White',\n           #'Student Attendance Rate',\n           'Percent of Students Chronically Absent',\n           #'Rigorous Instruction %',\n           #'Rigorous Instruction Rating', \n           #'Student Achievement Rating',\n           'Average ELA Proficiency',\n           'Average Math Proficiency', \n           'location_cluster',\n           'ELA_6_4s',\n           'Math_6_4s',\n           'ELA_7_4s',\n           'Math_7_4s',\n           'ELA_8_4s',\n           'Math_8_4s', \n           'Underperforming', \n           'Major N']\n\ncan_be_used =['Collaborative Teachers %',\n              'Collaborative Teachers Rating',\n              'Supportive Environment %',\n              'Supportive Environment Rating',\n              'Effective School Leadership %',\n              'Effective School Leadership Rating',\n              'Strong Family-Community Ties %',\n              'Strong Family-Community Ties Rating',\n              'Trust %',\n              'Trust Rating']\n\nCols_to_dummy =['Community School?',\n                'Rigorous Instruction Rating'\n               ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a9837483f45326e8e7563c89b4b9c38b76668740"},"cell_type":"code","source":"def Make_dummies(dataframe, col_for_dummies):\n    \"\"\" Function to make dummies for cat variables \"\"\"\n    col_for_svd = []\n    df = dataframe.copy()\n    for col in col_for_dummies:\n        temp = pd.get_dummies(temp[col], prefix = col)\n        col_for_svd = col_for_svd + temp.columns\n        df.drop(col, inplace = True)\n        df = df.concat([df, temp], axis = 0)\n        print(\"{} and dummies added shape is {}\".format(col, df.shape[1]))\n    return(df, col_for_svd)\n\n#LB_df, cols_for_svd = Make_dummies(bayes_df, col_for_dummies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f0c67db04db4b0acb206fb0658512bf392af854","collapsed":true},"cell_type":"code","source":"LB_df = master_df[Cols_lp].copy()\nLB_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3d966ed20a3aeefc54ee5361a738e3131bdd76d"},"cell_type":"markdown","source":"## Label propagator model "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9a517fe512c8a3851454b562ca7b7dd97435ffcc"},"cell_type":"code","source":"from sklearn.semi_supervised import LabelSpreading\ndef Label_propagator(df):\n    \"\"\"Function to use semi supervised label propagation to detect \"\"\"\n    df.fillna(-1, inplace = True)\n    label_prop_model = LabelPropagation()\n    label_prop_model = LabelSpreading(kernel = 'knn')\n    features = [x for x in df.columns if x != 'Underperforming']\n    label_prop_model.fit(df[features], df.Underperforming.values)\n    predicted_label = label_prop_model.predict(df[features])\n    df['Predicted_underperforming'] = predicted_label\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0603380e0e5cc48bddae8ab5c74ad2d609ac835a","collapsed":true},"cell_type":"code","source":"pred_df = Label_propagator(LB_df)\npred_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aed25697a07e5e55daae53978ed92d56ec05e4f3","collapsed":true},"cell_type":"code","source":"pred_df['Predicted_underperforming'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fcc18d16da81bb054104ac2e5c8c20643991278","collapsed":true},"cell_type":"code","source":"f, axes = plt.subplots(2,figsize=(8, 8), sharex=True, sharey = True)\naxes[0].hist(pred_df.Underperforming.values)\naxes[1].hist(pred_df.Predicted_underperforming.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f35021cdb9c2f7f3e4a808b929a12f384ed2718"},"cell_type":"markdown","source":"## Visualization on map : Before vs After label propagation"},{"metadata":{"trusted":true,"_uuid":"e65f59cf88484c68c42d4e03d716c9cdc915a885","collapsed":true},"cell_type":"code","source":"def plot_on_folium(df, status = 'before'):\n    \"\"\"function to generate map and add the pick up and drop coordinates\n    1. Path = 1 : Join pickup (blue) and drop(red) using a straight line\n    \"\"\"\n    circle=1\n    df1 = df.copy()\n    df1.reset_index(inplace = True)\n    new_df = df1[['Latitude','Longitude','Predicted_underperforming', 'Underperforming', 'School Name']]\n    #new_df.dropna(inplace = True)\n    print(new_df.shape)\n    m = folium.Map(location=[40.767937, -73.982155], zoom_start=10,tiles='Stamen Toner')\n    for i in list(range(new_df.shape[0])):\n        if status == 'before':\n            #print(new_df.loc[i]['Underperforming'])\n            if new_df.loc[i]['Underperforming']==-1.0:\n                color_ = '#0000FF'\n            elif new_df.loc[i]['Underperforming']==1.0:\n                color_ = '#FF0000'\n            else:\n                color_ = '#00FF00'\n        else:\n            if new_df.loc[i]['Predicted_underperforming']==1.0:\n                color_ = '#FF0000'\n                #print(color_)\n            else:\n                color_ = '#00FF00'\n        \n        try:\n            pick_long = new_df.loc[i]['Longitude']\n            pick_lat = new_df.loc[i]['Latitude']\n            pop = new_df.loc[i]['School Name']\n            #print(pop)\n            if circle == 1:\n                folium.CircleMarker(location=[pick_lat, pick_long],\n                        radius = 3,\n                        color=color_).add_to(m)\n            #folium.Marker([pick_lat, pick_long]).add_to(m)\n        except:\n            pass\n\n    return(m)\nmap_schools_df = pd.concat([master_df[[x for x in master_df.columns if x != 'Underperforming']], pred_df[['Predicted_underperforming', 'Underperforming']]], axis =1)\nprint(map_schools_df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d6b0e6832774e1e092057f500b7641a7c4b64ce"},"cell_type":"markdown","source":"## Before Label Propagation \n- **Blue** - Undefined Label\n- **Red** - Underperforming School\n- **Green** - Well performing School"},{"metadata":{"trusted":true,"_uuid":"8108a20d9e59cf1fc0273418a8d2393f06353798","collapsed":true},"cell_type":"code","source":"plot_on_folium(map_schools_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b55226cd53f2f63ca5b944167ed47e06b825177e"},"cell_type":"markdown","source":"# After running Label propagation\n- **Blue** - Undefined Label\n- **Red** - Underperforming School\n- **Green** - Well performing School"},{"metadata":{"trusted":true,"_uuid":"6b8fdb3271fe64bf4cb3d81b7f5f1400d0cda160","collapsed":true},"cell_type":"code","source":"plot_on_folium(map_schools_df,status = 'after')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6048f516a14225c96d305f1071be357a40c8c96e"},"cell_type":"markdown","source":"## Validation using principal componants analysis"},{"metadata":{"trusted":true,"_uuid":"bd4a162a0eacd5db09d61ece766960bb3e2c17e3","collapsed":true},"cell_type":"code","source":"# Visualizing low and well performing schools on Principal componants will validate the Algorithm\nfrom sklearn.decomposition import PCA\nX = LB_df.values\npca = PCA(n_components=3)\nPCA = pca.fit_transform(X)\nprint(pca.explained_variance_ratio_)\nprincipalDf = pd.DataFrame(data = PCA\n             , columns = ['PC1', 'PC2', 'PC3'])\n\nfinalDf = pd.concat([principalDf, map_schools_df[['Predicted_underperforming', 'School Name']]], axis = 1)\n\n# 3D scatter plot for Total Fats\ntrace1 = go.Scatter3d(\n    x=finalDf.PC1.values,\n    y=finalDf.PC2.values,\n    z=finalDf.PC3.values,\n    text=finalDf['School Name'].values,\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n#         sizeref=750,\n#         size= dailyValue['Cholesterol (% Daily Value)'].values,\n        color = finalDf['Predicted_underperforming'].values,\n        colorscale = [[0.0, 'rgb(0,255,0)'], [1.0, 'rgb(255,0,0)']],\n        colorbar = dict(title = 'Label propagation'),\n        line=dict(color='rgb(255, 255, 255)')\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='Underperforming schools visual')\nfig=dict(data=data, layout=layout)\niplot(fig, filename='3DBubble')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cccb425a1daff3d652d1c65b3aea72768e263bcb"},"cell_type":"markdown","source":"## The Above visual on 3 principal componants substantiate the Label propagation results\n** One can easily see that the schools performing low are very well seperated by well performing schools in lower dimensional space**"},{"metadata":{"trusted":true,"_uuid":"49d0c2d43ec7c0e118be54ee0d733ca348006c46","collapsed":true},"cell_type":"code","source":"# Saving the dataframes for further Analysis and as output\npath_to_save_dfs = ''\nbayes_df.to_csv(path_to_save_dfs + \"Bayes_df.csv\", index = False)\nSAT_summary.to_csv(path_to_save_dfs + \"SAT_summary.csv\", index = False)\nclass_size.to_csv(path_to_save_dfs + \"Class_size.csv\", index = False)\nsafety_df.to_csv(path_to_save_dfs + \"crime_df.csv\", index = False)\nmap_schools_df.to_csv(path_to_save_dfs +\"Labels.csv\", index = False) # Contains School name and if the school is underperforming ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73f3492e428065aae729eafbd9225a1b7981bdab"},"cell_type":"markdown","source":"### FInal results are provided in Performance_of_schools.csv file\n** Note**- When **predicted_underperfoming** variable takes the value 1, the school is underperforming else it's performing well."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6209c9f9f8b4bd4772274ddcb7846dde215fa77"},"cell_type":"code","source":"map_schools_df.to_csv(path_to_save_dfs +\"Performance_of_schools.csv\", index = False)\n# final list of well-performing and underperforming schools","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23684502c3363fa70747294e66cf436bfca2dcb0"},"cell_type":"markdown","source":"### Thanks for reading ...\n**Link to Next kernel** - https://www.kaggle.com/maheshdadhich/driving-factors-model-and-proxy-measures"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e42a0fbe789d966f2e41f4f107d97a8a02d9c56e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}