{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this notebook I tried to run a simple, normal regression and a Ridge regression; Ridge regressions can perform less well on training datasets but better on test datasets because they take into consideration that not all variables are as important as others in the prediction of the dependent variable. \nThe normal regression treates all parameters in an unbiased way."},{"metadata":{},"cell_type":"markdown","source":"Firstly, I'm importing what I need; the database I'll be using is Boston Housing prices, and can be directly loaded (versions I found here on Kaggle didn't have all the variables I has used before)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After importing I'm splitting the data in two:  1. my target variable, the price - df_labels, and 2. the rest of the independent variables, df."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# loading the Boston dataset\nfrom sklearn.datasets import load_boston\nhouse_price = load_boston()\ndf_labels = pd.DataFrame(house_price.target)\ndf = pd.DataFrame(house_price.data)\nprint(df_labels.head())\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All good, but they have no column names, must add them:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_labels.columns = ['PRICE']\ndf.columns = house_price.feature_names\nprint(df.shape)\nprint(df_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, I'm creating a new complete dataframe, just in case I'll need it.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total = df.merge(df_labels, left_index = True, right_index = True)\ndf_total.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I'm having a look at the variables to see if there are any missings to take care of, or categorial variables that should be encoded."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_total.describe()\ndf_total.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunately this is a nice database, no missings and to categorial variables :) "},{"metadata":{},"cell_type":"markdown","source":"Then I'm having a look at the distribution of my target variable, the price:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_labels['PRICE'], bins = 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good, almost perfect normal distribution, checking also skewness and kurtosis, that are not bad."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from scipy.stats import skew,kurtosis \nprint(skew(df_labels['PRICE']))\nprint(kurtosis(df_labels['PRICE'])) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then I'm getting to the point that I'm very interested in, the correlations; here a bit of research was needed to get an understanding of the variables, I'm going to mention some of the highly correlated ones:\nLSTAT is the % of the population with lower status, RM is the number of rooms, PTRATIO is the ration between pupils and teachers, TAX is the tax rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df_total.corr(method = 'pearson')\ncorr_matrix ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next step I'm scaling the data and then splitting it in train/test data to begin training my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardize and train/test split: standardize only data, not target\ndf = preprocessing.scale(df)\nX_train, X_test, y_train, y_test = train_test_split(\n    df, df_labels, test_size=0.3, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fiting the regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then cheking the RMSE of my model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#on train set\nfrom sklearn.metrics import mean_squared_error\ny_train_predicted = lin_reg.predict(X_train)\nlin_mse = mean_squared_error(y_train_predicted, y_train)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lin_reg.intercept_)\nprint(lin_reg.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'm applying the model on the test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#on test set\ny_test_predicted = lin_reg.predict(X_test)\nlin_mse = mean_squared_error(y_test_predicted, y_test)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And getting an RMSE a bit higher.\nNow, I want to compare the error I got with the variable itself (expressed in 1Ks): so there's an error of 5.41 on a variable with values between 5 and 50, and a mean of 22."},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see how rmse compares to the rest of the target var desciptives\ndf_labels['PRICE'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now moving on to the Ridge regression; now, in the first example the alpha is 0 so the RMSE is identical to the one above:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#do the same with ridge\nridge_reg = Ridge(alpha=0)\nridge_reg.fit(X_train, y_train)\ny_train_predicted = ridge_reg.predict(X_train)\nridge_mse = mean_squared_error(y_train_predicted, y_train)\nridge_rmse = np.sqrt(ridge_mse)\nridge_rmse ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I'm testing with a new alpha to see how the RMSE changes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_reg = Ridge(alpha=50)\nridge_reg.fit(X_train, y_train)\ny_train_predicted = ridge_reg.predict(X_train)\nridge_mse = mean_squared_error(y_train_predicted, y_train)\nridge_rmse = np.sqrt(ridge_mse)\nridge_rmse ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally, I'm using a Ridge cross-validation that will help tell me which alpha is the best from a list of alphas:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\nregr_cv = RidgeCV(alphas=[0.1,0.3, 0.5,0.7, 1.0, 10.0, 50.0])\nmodel = regr_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.alpha_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE when we're using the alpha we got after cross-validation:"},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_predicted = regr_cv.predict(X_train)\nridge_mse = mean_squared_error(y_train_predicted, y_train)\nridge_rmse = np.sqrt(ridge_mse)\nridge_rmse ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Or I can write it as a function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def function(i):\n    ridge_reg = Ridge(alpha = i)\n    ridge_reg.fit(X_train, y_train)\n    y_train_predicted = ridge_reg.predict(X_train)\n    ridge_mse = mean_squared_error(y_train_predicted, y_train)\n    ridge_rmse = np.sqrt(ridge_mse)\n    print(ridge_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"function(0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's see what RMSE I'll find on the test data with the value for alpha:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#on test set\ny_test_predicted = ridge_reg.predict(X_test)\nlin_mse = mean_squared_error(y_test_predicted, y_test)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}