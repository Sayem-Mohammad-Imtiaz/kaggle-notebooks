{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"0753e83a13268728748b4503c40fb8c8dc5b1a4c","_cell_guid":"602eb611-a96f-45bd-abf3-49fc22e90194"},"source":"Two solutions...\n---\nadaptations:\n\n*labellize\n*correlation with activity: weak at first sight, strongest link is with axis-z the least mover\n\nonly two linear models solve the problem\n---\n*Decision Tree\n*randomForest tree\n\nthe only problem i see is that the date has probably some 'data leak' properties... i don't understand why the date "},{"cell_type":"code","metadata":{"_uuid":"d142128d766fa8a6e27cbf68271ac86afac4ba9e","collapsed":true,"_cell_guid":"b14c48f5-fbf3-4d0a-a85c-e03cbf1a8659"},"execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nfrom sklearn.preprocessing import LabelEncoder\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ntrain = pd.read_csv(\"../input/ConfLongDemo_JSI.csv\")\n\ntrain.columns=['Sequence Name','tagID','Time stamp','Date','x coordinate','y coordinate','z coordinate','activity']\nprint(train.head())\n# process columns, apply object > numeric\n# process columns, apply LabelEncoder to categorical features\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder() \n        lbl.fit(list(train[c].values)) \n        train[c] = lbl.transform(list(train[c].values))","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"8bec62bed0b892ca07481144c5d56ded049bab74","collapsed":true,"_cell_guid":"449ddfeb-cc5f-46f4-a250-47a487662a20"},"execution_count":null,"source":"import seaborn as sns\nimport matplotlib.pyplot as plt # Visuals\nplt.style.use('ggplot') # Using ggplot2 style visuals \n\nf, ax = plt.subplots(figsize=(11, 15))\n\nax.set_axis_bgcolor('#fafafa')\nax.set(xlim=(-.05, 50))\nplt.ylabel('Dependent Variables')\nplt.title(\"Box Plot of Pre-Processed Data Set\")\nax = sns.boxplot(data = train, \n  orient = 'h', \n  palette = 'Set2')\n\n\n\n#sns.set(style=\"ticks\", color_codes=True)\n#g = sns.pairplot(train, hue='Dx')\n\n    \ndef plot_corr(df,size=10):\n    import matplotlib.pyplot as plt\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n\nplot_corr(train)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"new_col= train.groupby('activity').mean()\nprint(new_col.head().T)\ntrain=train.fillna(0)\n# Scatterplot Matrix\n# Variables chosen from Random Forest modeling.\ncols = ['x coordinate','y coordinate','z coordinate','activity']\n\nsns.pairplot(train[cols],\n             x_vars = cols,\n             y_vars = cols,\n             hue = 'activity', \n             )","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\n\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n\nfrom sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz \nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\nn_col=36\nX = train.drop(['activity'],axis=1) \nY=train['activity']\n#X=X.fillna(value=0)\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X=scaler.transform(X)\n#poly = PolynomialFeatures(2)\n#X=poly.fit_transform(X)\n\n\nnames = [\n         'DecisionTree',\n         'RandomForestClassifier',    \n         #'ElasticNet',\n         #'SVC',\n         #'kSVC',\n         'KNN',\n         #'GridSearchCV',\n         'HuberRegressor',\n         'Ridge',\n         'Lasso',\n         'LassoCV',\n         'Lars',\n         'BayesianRidge',\n         'SGDClassifier',\n         'RidgeClassifier',\n         'LogisticRegression',\n         'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators = 200),\n    #ElasticNetCV(cv=10, random_state=0),\n    #SVC(),\n    #SVC(kernel = 'rbf', random_state = 0),\n    KNeighborsClassifier(n_neighbors = 1),\n    #GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    Lasso(alpha=0.05),\n    LassoCV(),\n    Lars(n_nonzero_coefs=10),\n    BayesianRidge(),\n    SGDClassifier(),\n    RidgeClassifier(),\n    LogisticRegression(),\n    OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\nmodels=zip(names,classifiers,correction)\n   \nfor name, clf,correct in models:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n    \n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    conf=confusion_matrix(Y, np.round(regr.predict(X) ) ) \n    label=np.sort( Y.unique() )\n    sns.heatmap(conf, annot=True, xticklabels=label, yticklabels=label, cmap=\"YlGnBu\")\n    plt.show()\n    \n    print('--'*40)\n\n    # Classification Report\n    print(name,'Classification Report')\n    classif=classification_report(Y,np.round( regr.predict(X) ) )\n    print(classif)\n\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')\n    \n    if name=='DecisionTree':\n        label=train.columns\n        label=label[:-1].values\n        important=pd.DataFrame(clf.feature_importances_,index=label,columns=['imp'])\n        important.sort_values(by='imp').plot(kind='bar')\n        plt.show()\n\n ","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"}},"nbformat_minor":1}