{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Motivation\n[This video](https://www.youtube.com/watch?v=6RSQIHAVzuo&t=756s) inspired me to learn about Pipelining, since Andreas Muller, **core developer of the sklearn library** states that **'Everybody should be using Pipelines. If you are not using Pipelines, you are probably doing it wrong'** when using sklearn.\n\nPipelining is an efficient, clean, and highly modifiable way of building Machine Learning Models. You can simply stick together multiple steps and **evaluate each step and its parameters** during cross-validation. Not only can you do Preprocessing and Feature Engineering in these pipelines, but you can even switch between different model types. \n\nPipelining enforces us to treat each Data Cleaning and Feature Engineering step as a submodel of our final machine learning ensemble. According to [T. Scott Clendaniel](https://www.linkedin.com/in/tscottclendaniel/), understanding machine learning models this way is one key-mindset of Data Preprocessing, as mentioned in [this video](https://www.youtube.com/watch?v=vsKNxbP8R_8&t=1630s) about advanced Feature Engineering. It allows us to evaluate each Feature Engineering, Feature Selection and Model Selection step individually.\n\n\nTake a look at my [Comprehensive Tutorial: Feature Engineering](https://www.kaggle.com/milankalkenings/comprehensive-tutorial-feature-engineering), if you want to practice your pipelining skills."},{"metadata":{},"cell_type":"markdown","source":"***\n# Sections\n\n## [1. What is a Pipeline?](#sec0)\n\n## [2. Create Pipeline Helpers](#sec1)\n* [Slicer](#sec12)\n* [Heartbeat Identity Transformer](#sec12)\n* [Transformer Wrapper](#sec13)\n* [Estimator Wrapper](#sec14)\n\n\n## [3. Create the Pipeline](#sec2)\n* [Group Columns by type](#sec21)\n* [Create Pipelines for Preprocessing](#sec22)\n* [Combine all Preprocessing Pipelines](#sec23)\n* [Add Predictors to a Pipeline](#sec24)\n\n\n## [4. Pipeline validation with multiple Estimators](#sec3)\n## [5. Results](#sec4)\n***"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec0\"></a>\n***\n# 1. What is a Pipeline?\n\n\n\n\n**A Pipeline consists of two different sklearn objects:**\n### Transformers\nTransformers perform any transformation on any given data. \nThe correct usage of the methods **fit()** and **transform()** ensure, that the test data is transformed using the parameters, which were calculated on train. All transformers in our Pipeline have to implement both, **fit** and **transform**.\n* **fit()** calculates the parameters for the transformation and stores them. We only want to perform this on training data.\n* **transform()** transforms any given data using the parameters, calculated in **fit()**\n* **fit_transform()** performs the transform method after the fit method and thus should only be used on **training data**, since otherwise, **fit()** would train the parameters of the transformer on test.\n\n\n### Final Estimator\nThe last element of your Pipeline can be an **Estimator**, and thus only has to implement the **fit** method. E.g. we can use a random forest classifier. It wouldn't transform the data, but it would solely fit some parameters on the given data. The whole pipeline will act like the chosen estimator and thus it can call the respective estimator methods like **predict()**, **predict_proba()**, and so on.\n\n\n\n\nUsing Pipelines, we can easily switch between different Preprocessing, Feature Engineering methods and Machine Learning Models during **Gridsearch cross-validation**. The following sketch assumes, that we treat all numerical variables and all categorical variables equivalently. As we can see, the sketched Pipeline allows us to switch between two Encoding approaches for categorical data and it even allows us to switch between two different types of models."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('../input/images/pipes.PNG')\n\nfig = plt.figure(frameon=False)\nfig.set_size_inches(11,6)\nax = plt.Axes(fig, [0., 0., 1., 1.])\nax.set_axis_off()\nfig.add_axes(ax)\nax.imshow(img, aspect='auto')\nfig.savefig('pipes_out.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### First of all, we have to check for any **missing values** and we have to perform a **train/test split**..."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\n\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there any missing values in the dataset ?"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"split the data into a training set and a test set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(columns='target')\ny = df['target']\n\n# 80/20 split (based on the pareto principle)\n# due to the small size of the dataset, the random state has a huge impact on the results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n***\n# 2. Create Pipeline Helpers"},{"metadata":{},"cell_type":"markdown","source":"Custom Transformers have to inherit TransformerMixin and they need to implement the methods fit and transform for obvious reasons. Likewise, custom Estimators have to inherit BaseEstimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec11\"></a>\n## 2.1. Slicer\nThe first, and most important Helper-Transformer for Pipelines is the Slicer. We can use a Slicer to take a **subset of features** from our dataset and perform further transformations on it. (E.g., to One-Hot Encode Categorical Features with low cardinality).\n\nNote: Instead, you can use a column transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Slicer(TransformerMixin):\n    \"\"\"\n    Transformer to slice certain columns of a df.\n    \"\"\"\n    def __init__(self, col_names):\n        self.col_names = col_names\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        return X[self.col_names]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec12\"></a>\n## 2.2. Heartbeat Identity Transformer\nIt might be beneficial to have a simple tool for debuging inside a Pipeline. You can plug this Transformer into a Pipeline and activate it during cross validation if needed.\nIt is especially useful for more complex Pipelines, and you can use it to e.g. print the intermediate data state."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Heartbeat(TransformerMixin):\n    \"\"\"\n    Identity Transformer for Debugging. \n    Add any parameters or functionalities..\n    \"\"\"\n    def __init__(self, heartbeat_message='no errors yet', active=False):\n        self.heartbeat_message = heartbeat_message\n        self.active = active\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        if (self.active):\n            print(self.heartbeat_message)\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec13\"></a>\n## 2.3. Transformer Wrapper\nThis very simple wrapper allows us to decide whether we want a Transformer to be active or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ActivityWrapper(TransformerMixin):\n    \"\"\"\n    A Custom Wrapper that can activate/deactivate a Transformer.\n    \"\"\" \n    def __init__(self, transformer, active=False):\n        self.transformer = transformer\n        self.active = active\n\n    def fit(self, X, y=None, **kwargs):\n        if(self.active):\n            self.transformer.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        if(self.active):\n            X = self.transformer.transform(X)\n        return X\n    \n    def set_params(self, transformer=None, active=True):\n        self.transformer = transformer\n        self.active = active","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec14\"></a>\n## 2.4. Estimator Wrapper\nI found this [wrapper for estimators](https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers?noredirect=1&lq=1), which can be used to switch between different estimators during cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassifierWrapper(BaseEstimator):\n    \"\"\"\n    A Custom Wrapper that can switch between classifiers.\n    \"\"\" \n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n***\n# 3. Create the Pipeline"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec21\"></a>\n# 3.1. Group Columns by type\nEach List contains the names of features, which we want to **treat equally**."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_num = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ncols_cat_bin = ['sex', 'fbs', 'exang']\ncols_cat = ['cp', 'restecg', 'ca', 'thal', 'slope']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec22\"></a>\n# 3.2. Create Pipelines for Preprocessing \nStack the Transformers. \n\nTransformer $n$ performs its transformation after Transformer $n-1$. Each Transformer is represented by a tuple. The String in the tuple is the Identifier of the Transformer and we can use it to set parameters during Gridsearch cross-validation, as you will see later on.\n\nI will solely do some basic **preprocessing** in this example."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\npipe_num = Pipeline([\n    ('selector', Slicer(cols_num)),\n    ('scaler', MinMaxScaler())\n])\n\n# nothing to do\npipe_cat_bin = Pipeline([\n    ('selector', Slicer(cols_cat_bin))\n])\n\npipe_cat = Pipeline([\n    ('selector', Slicer(cols_cat)),\n    ('encoder_wrapped', ActivityWrapper(transformer=OneHotEncoder(handle_unknown='ignore', sparse=False)))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec23\"></a>\n# 3.3. Combine all Pre Processing Pipelines\nFeatureUnion combines several transformers and concatenates the outputs of each pipe. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\nfeatures = FeatureUnion([\n    ('num', pipe_num),\n    ('obj_bin', pipe_cat_bin),\n    ('cat', pipe_cat)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec24\"></a>\n# 3.4. Add Predictors to a Pipeline\nCreate a new pipe using the combined Preprocessing Transformers and an Estimator (or an Estimator Wrapper). Furthermore, I added an **inactive** Heartbeat-Transformer, just to show you that you can plug these transformers in whenever you want. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_full = Pipeline([\n    ('features', features),\n    ('heartbeat', Heartbeat()),\n    ('switchable', ClassifierWrapper())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you want to, you can already use such a pipeline for **predictions**, e.g. to compare it with the results of the cross validated pipe."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\npipe_logreg = Pipeline([\n    ('features', features),\n    ('logreg', LogisticRegression())\n])\n\npipe_logreg.fit(X_train, y_train)\ny_pred = pipe_logreg.predict(X_test)\n(y_pred == y_test).sum() / len(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n***\n# 4. Pipeline validation with multiple Estimators\nWe can set the parameters of each individual Element of our Pipeline using their identifiers, followed by two underscores and the parameter name. To reach a Classifier in our  ClassifierWrapper, we have to add two more underscores."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost\n\n# one dict for each estimator\nhyperparameters = [\n    {\n        'features__cat__encoder_wrapped__transformer': [OneHotEncoder(handle_unknown='ignore')],\n        'features__cat__encoder_wrapped__active': [True, False],\n        'switchable__estimator': [AdaBoostClassifier(base_estimator=LogisticRegression(), algorithm='SAMME.R')],\n        'switchable__estimator__n_estimators': [60, 70],\n        'switchable__estimator__learning_rate': [0.6, 0.7]\n    },\n    {\n        'features__cat__encoder_wrapped__transformer': [OneHotEncoder(handle_unknown='ignore', sparse=False)],\n        'features__cat__encoder_wrapped__active': [True, False],\n        'switchable__estimator': [RandomForestClassifier(random_state = 5)],\n        'switchable__estimator__n_estimators': [750],\n        'switchable__estimator__max_features': ['log2'],\n        'switchable__estimator__criterion': ['gini', 'entropy']\n    }\n]\n\n\nval = GridSearchCV(pipe_full, hyperparameters, cv=5, scoring='f1')\n# (by default,) GridSearchCV provides the best scoring pipe\nval.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec4\"></a>\n***\n# 5. Results"},{"metadata":{},"cell_type":"markdown","source":"Let's find out which parameters are the best."},{"metadata":{"trusted":true},"cell_type":"code","source":"val.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many data points are classified correctly? (accuracy)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = val.predict(X_test)\n(y_pred == y_test).sum() / len(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When inspecting the false predictions (to determine which circumstances could have led to a false prediction), I encountered a correlation with misclassification:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X_false_pred = X_test[y_pred != y_test]\nwith plt.style.context('dark_background'):\n    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(8, 10))\n    X_test['sex'].value_counts().plot.pie(ax=ax1, \n                                          title='Male patients in Testset', \n                                          colormap='viridis', \n                                          explode=[0.1,0], \n                                          wedgeprops={\"edgecolor\":\"black\"})\n    \n    X_false_pred['sex'].value_counts().plot.pie(ax=ax2, \n                                                title='Male patients in False Predictions', \n                                                colormap='viridis',\n                                                explode = [0.1,0], \n                                                wedgeprops={\"edgecolor\":\"black\"})\n    \nplt.savefig('errors_male.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like male patients are more likely to get false predictions, since they are slightly overrepresented in the set of incorrect predictions. \n\nI wouldn't overrate this result, since the number of false predictions (and the dataset in general) is pretty small."},{"metadata":{},"cell_type":"markdown","source":"# That's it. Thank you for reading this Notebook. I hope that I helped you in any way.\n\nTake a look at my [Comprehensive Tutorial: Feature Engineering](https://www.kaggle.com/milankalkenings/comprehensive-tutorial-feature-engineering), if you want to practice your pipelining skills with more elaborate Feature Engineering techniques.\n\n\n<br>\n<br>\n<br>\n\n## TODO:\n* find a better ensemble\n* add some EDA\n\nThis Notebook is based on [This Notebook by dbaghern](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines) but introduces new concepts and fixes. \n\nFeel free to **comment any suggestions for improvements**, because that's the best way of learning from each other."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}