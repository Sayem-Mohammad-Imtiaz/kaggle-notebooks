{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/williamroe/bi-lstm-with-crf-for-ner\n\nhttps://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/\n\nhttps://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n\nhttps://confusedcoders.com/data-science/deep-learning/how-to-build-deep-neural-network-for-custom-ner-with-keras\n\nhttps://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf","metadata":{}},{"cell_type":"code","source":"!python --version\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport csv \n\nimport numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom wordcloud import WordCloud\n\nsns.set(font_scale = 1)\n%matplotlib inline\nplt.style.use('ggplot')\n\nfrom IPython.core.pylabtools import figsize\n\nimport nltk, re, string, collections\nfrom collections import Counter\n\nfrom nltk import word_tokenize \nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nfrom math import nan\n\nfrom future.utils import iteritems\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n!pip install git+https://www.github.com/keras-team/keras-contrib.git\nfrom keras_contrib.layers import CRF\n    \nfrom keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nimport keras as k\n\nfrom keras.callbacks import ModelCheckpoint\n\n!pip install seqeval\n\nfrom seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n\n! pip install sklearn_crfsuite\n\nfrom  sklearn_crfsuite.metrics import flat_classification_report  \n\nfrom keras.models import load_model\nfrom keras_contrib.utils import save_load_utils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-24T11:44:55.358282Z","iopub.execute_input":"2021-05-24T11:44:55.358588Z","iopub.status.idle":"2021-05-24T11:45:30.845246Z","shell.execute_reply.started":"2021-05-24T11:44:55.358542Z","shell.execute_reply":"2021-05-24T11:45:30.844324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # reading dump file\n# ner_rows = []\n# sentence_id = 0\n\n# with open(\"../input/nercorpus/NER-corpus.DUMP\", \"r\") as ner_file:\n    \n#     for line in ner_file:\n#         label = line.split('\\t')[0]\n#         tags = line.split('\\t')[1].split()\n#         words = line.split('\\t')[2].split('\\n')[0].split()\n#         for tag, word in zip(tags, words):\n#             ner_rows.append([sentence_id, label, word.lower(), tag])\n        \n#         sentence_id+=1\n\n# # dump to csv for appropriate format\n# fields = ['sentence_id', 'label', 'word', 'tag']\n# filename = \"tbmm_ner.csv\"\n\n# with open(filename, 'w') as csvfile:\n#     csvwriter = csv.writer(csvfile)\n#     csvwriter.writerow(fields)\n#     csvwriter.writerows(ner_rows)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:34:07.053017Z","iopub.execute_input":"2021-05-24T09:34:07.055283Z","iopub.status.idle":"2021-05-24T09:34:07.061521Z","shell.execute_reply.started":"2021-05-24T09:34:07.055229Z","shell.execute_reply":"2021-05-24T09:34:07.060814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df = pd.read_csv(\"../input/tbmm-ner/tbmm_ner.csv\", encoding = \"utf-8\", error_bad_lines=False)\nner_df = ner_df.fillna(method=\"ffill\")\nner_df.info()\nner_df.head(10)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-05-24T11:45:30.848531Z","iopub.execute_input":"2021-05-24T11:45:30.848857Z","iopub.status.idle":"2021-05-24T11:45:41.120608Z","shell.execute_reply.started":"2021-05-24T11:45:30.848804Z","shell.execute_reply":"2021-05-24T11:45:41.119635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Data Observe**","metadata":{}},{"cell_type":"code","source":"ner_words = [i.lower() for i in ner_df['word'] if re.findall(\"^[a-zA-Z0-9ğüşöçİĞÜŞÖÇ]+$\", i) and len(i) > 1]\nner_words = [i for i in ner_words if i not in stopwords.words('turkish')]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:34:16.972591Z","iopub.execute_input":"2021-05-24T09:34:16.973095Z","iopub.status.idle":"2021-05-24T09:41:48.238974Z","shell.execute_reply.started":"2021-05-24T09:34:16.973047Z","shell.execute_reply":"2021-05-24T09:41:48.238042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = Counter(ner_words)\nc = list(c.most_common(500))\nmost_common = []\nfor i in range(len(c)):\n    most_common.append(c[i][0])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:41:48.242201Z","iopub.execute_input":"2021-05-24T09:41:48.242647Z","iopub.status.idle":"2021-05-24T09:41:49.606588Z","shell.execute_reply.started":"2021-05-24T09:41:48.242457Z","shell.execute_reply":"2021-05-24T09:41:49.605825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(background_color=\"white\",width=1000, height=600, max_font_size = 80).generate(' '.join(most_common))\nplt.figure(figsize=(40,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:41:49.609273Z","iopub.execute_input":"2021-05-24T09:41:49.609528Z","iopub.status.idle":"2021-05-24T09:41:50.743138Z","shell.execute_reply.started":"2021-05-24T09:41:49.609486Z","shell.execute_reply":"2021-05-24T09:41:50.74159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df['tag'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:41:50.74428Z","iopub.execute_input":"2021-05-24T09:41:50.744547Z","iopub.status.idle":"2021-05-24T09:41:52.221417Z","shell.execute_reply.started":"2021-05-24T09:41:50.744499Z","shell.execute_reply":"2021-05-24T09:41:52.220282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figsize(20, 10)\nsns.countplot(ner_df['tag'], palette=\"colorblind\");\nplt.xlabel('Tags'); ","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:41:52.222868Z","iopub.execute_input":"2021-05-24T09:41:52.22333Z","iopub.status.idle":"2021-05-24T09:41:54.734141Z","shell.execute_reply.started":"2021-05-24T09:41:52.223141Z","shell.execute_reply":"2021-05-24T09:41:54.733378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Data Formatting**","metadata":{}},{"cell_type":"code","source":"class SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(l, w, t) for l, w,t in zip(s[\"label\"].values.tolist(),\n                                                          s[\"word\"].values.tolist(),\n                                                          s[\"tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n           \ngetter = SentenceGetter(ner_df)\nsentences = getter.sentences","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:45:41.122085Z","iopub.execute_input":"2021-05-24T11:45:41.122534Z","iopub.status.idle":"2021-05-24T11:46:45.945235Z","shell.execute_reply.started":"2021-05-24T11:45:41.122348Z","shell.execute_reply":"2021-05-24T11:46:45.944478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = []\nfor label in set(ner_df[\"label\"].values):\n    if label is nan or isinstance(label, float):\n        labels.append('unk')\n    else:\n        labels.append(label)\nn_labels = len(labels)\n\nwords = list(set(ner_df[\"word\"].values))\nwords.append(\"unk\")\nn_words = len(words)\n\ntags = []\nfor tag in set(ner_df[\"tag\"].values):\n    if tag is nan or isinstance(tag, float):\n        tags.append('unk')\n    else:\n        tags.append(tag)\nn_tags = len(tags)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:46:45.946808Z","iopub.execute_input":"2021-05-24T11:46:45.947098Z","iopub.status.idle":"2021-05-24T11:46:47.553554Z","shell.execute_reply.started":"2021-05-24T11:46:45.947053Z","shell.execute_reply":"2021-05-24T11:46:47.552694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = max([len(s) for s in sentences])\nprint ('Maximum sentence length:', maxlen)\n\nprint ('The histogram of the lengths of sentences')\nplt.hist([len(s) for s in sentences], bins=50)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:46:47.555998Z","iopub.execute_input":"2021-05-24T11:46:47.556289Z","iopub.status.idle":"2021-05-24T11:46:48.157792Z","shell.execute_reply.started":"2021-05-24T11:46:47.556242Z","shell.execute_reply":"2021-05-24T11:46:48.156781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label2idx = {l: i for i, l in enumerate(labels)}\nword2idx = {w: i for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}\n\nidx2label = {v: k for k, v in iteritems(label2idx)}\nidx2word = {v: k for k, v in iteritems(word2idx)}\nidx2tag = {v: k for k, v in iteritems(tag2idx)}","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:46:48.159327Z","iopub.execute_input":"2021-05-24T11:46:48.159615Z","iopub.status.idle":"2021-05-24T11:46:48.468836Z","shell.execute_reply.started":"2021-05-24T11:46:48.159569Z","shell.execute_reply":"2021-05-24T11:46:48.467923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Data Splitting as Train and Test Sets**","metadata":{}},{"cell_type":"code","source":"X = [[word2idx[w[1]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\",value=n_words - 1)\n\ny = [[tag2idx[w[2]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\ny = [to_categorical(i, num_classes=n_tags) for i in y]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:46:48.47017Z","iopub.execute_input":"2021-05-24T11:46:48.470466Z","iopub.status.idle":"2021-05-24T11:47:12.436756Z","shell.execute_reply.started":"2021-05-24T11:46:48.470422Z","shell.execute_reply":"2021-05-24T11:47:12.435245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(X).shape\n# print(sentences[50])\n# print(y[50])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:42:21.734638Z","iopub.execute_input":"2021-05-24T11:42:21.734961Z","iopub.status.idle":"2021-05-24T11:42:21.775035Z","shell.execute_reply.started":"2021-05-24T11:42:21.734895Z","shell.execute_reply":"2021-05-24T11:42:21.774265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train\", X_train[2])\nprint(\"X_test\", X_test[2])\n\nprint(\"y_train\", list(y_train[2]))\nprint(\"y_test\", y_test[2])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:30:34.386317Z","iopub.status.idle":"2021-05-24T11:30:34.386998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **BI-LSTM and CRF Model & Traning of Model**","metadata":{}},{"cell_type":"code","source":"input = Input(shape=(50,))\nword_embedding_size = 150\n\n# Embedding Layer\nmodel = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=50)(input)\n\n# BI-LSTM Layer\nmodel = Bidirectional(LSTM(units=word_embedding_size, \n                           return_sequences=True, \n                           dropout=0.5, \n                           recurrent_dropout=0.5, \n                           kernel_initializer=k.initializers.he_normal()))(model)\nmodel = LSTM(units=word_embedding_size * 2, \n             return_sequences=True, \n             dropout=0.5, \n             recurrent_dropout=0.5, \n             kernel_initializer=k.initializers.he_normal())(model)\n\n# TimeDistributed Layer\nmodel = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  \n\n# CRF Layer\ncrf = CRF(n_tags)\nout = crf(model)\n\nmodel = Model(input, out)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:43:23.88791Z","iopub.execute_input":"2021-05-24T09:43:23.888579Z","iopub.status.idle":"2021-05-24T09:43:27.886887Z","shell.execute_reply.started":"2021-05-24T09:43:23.888508Z","shell.execute_reply":"2021-05-24T09:43:27.886041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimiser \nadam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n\n# Compile model\nmodel.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n\nmodel.summary()\n\n# Saving the best model only\nfilepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\n# Fit the best model\nhistory = model.fit(X_train, np.array(y_train), batch_size=256, epochs=20, validation_split=0.1, verbose=1, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T09:43:27.891546Z","iopub.execute_input":"2021-05-24T09:43:27.893668Z","iopub.status.idle":"2021-05-24T11:22:47.128632Z","shell.execute_reply.started":"2021-05-24T09:43:27.893609Z","shell.execute_reply":"2021-05-24T11:22:47.127722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Evaluation of the Results**","metadata":{}},{"cell_type":"code","source":"# Plot the graph \nplt.style.use('ggplot')\n\ndef plot_history(history):\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(accuracy) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, accuracy, 'b', label='Training acc')\n    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nplot_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:22:47.130241Z","iopub.execute_input":"2021-05-24T11:22:47.130526Z","iopub.status.idle":"2021-05-24T11:22:47.612148Z","shell.execute_reply.started":"2021-05-24T11:22:47.130477Z","shell.execute_reply":"2021-05-24T11:22:47.611164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            p_i = np.argmax(p)\n            out_i.append(idx2label[p_i])\n            out_i.append(idx2word[p_i])\n            out_i.append(idx2tag[p_i])\n            \n        out.append(out_i)\n    return out\n\ntest_pred = model.predict(X_test, verbose=1)   \npred_labels = pred2label(test_pred)\ntest_labels = pred2label(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:49:54.483147Z","iopub.execute_input":"2021-05-24T11:49:54.483506Z","iopub.status.idle":"2021-05-24T11:54:35.18784Z","shell.execute_reply.started":"2021-05-24T11:49:54.48345Z","shell.execute_reply":"2021-05-24T11:54:35.186972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:27:29.996244Z","iopub.execute_input":"2021-05-24T11:27:29.996733Z","iopub.status.idle":"2021-05-24T11:30:34.368172Z","shell.execute_reply.started":"2021-05-24T11:27:29.996681Z","shell.execute_reply":"2021-05-24T11:30:34.366695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = {}\nTN = {}\nFP = {}\nFN = {}\nfor tag in tag2idx.keys():\n    TP[tag] = 0\n    TN[tag] = 0    \n    FP[tag] = 0    \n    FN[tag] = 0    \n\ndef accumulate_score_by_tag(gt, pred):\n    if gt == pred:\n        TP[gt] += 1\n    elif gt != 'O' and pred == 'O':\n        FN[gt] +=1\n    elif gt == 'O' and pred != 'O':\n        FP[gt] += 1\n    else:\n        TN[gt] += 1","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:30:37.791595Z","iopub.execute_input":"2021-05-24T11:30:37.79194Z","iopub.status.idle":"2021-05-24T11:30:37.798938Z","shell.execute_reply.started":"2021-05-24T11:30:37.79189Z","shell.execute_reply":"2021-05-24T11:30:37.797907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tag in tag2idx.keys():\n    print(f'tag:{tag}')    \n    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))    ","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:30:40.108984Z","iopub.execute_input":"2021-05-24T11:30:40.109286Z","iopub.status.idle":"2021-05-24T11:30:40.118053Z","shell.execute_reply.started":"2021-05-24T11:30:40.109221Z","shell.execute_reply":"2021-05-24T11:30:40.115997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_labels, pred_labels))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:30:58.034572Z","iopub.execute_input":"2021-05-24T11:30:58.034885Z","iopub.status.idle":"2021-05-24T11:31:21.008681Z","shell.execute_reply.started":"2021-05-24T11:30:58.034837Z","shell.execute_reply":"2021-05-24T11:31:21.007434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 48\np = model.predict(np.array([X_test[i]]))\np = np.argmax(p, axis=-1)\ngt = np.argmax(y_test[i], axis=-1)\nprint(\"{:14}: {:5}: {}\".format(\"Word\\t\", \"True\\t\", \"Pred\\t\"))\nfor idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n    if words[w] != \"unk\":\n        print(\"{:14}: {:5}: \\t{}\".format(words[w],idx2tag[gt[idx]],tags[pred]))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:47:13.13433Z","iopub.status.idle":"2021-05-24T11:47:13.13491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **Demo Screen**","metadata":{}},{"cell_type":"code","source":"def readFile(fileName):\n    file = open(fileName, \"r\")\n    sentences = file.read()\n    file.close\n    \n    print(\"\\nYour file --> \\t\", sentences)\n    \n    words = []\n    \n    sentence_id = 0\n    \n    for s in sentences.split('.'):\n        for w in s.split():\n            words.append([sentence_id, w.lower()])\n        sentence_id+=1\n    \n    return words  ","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:54:35.189724Z","iopub.execute_input":"2021-05-24T11:54:35.190004Z","iopub.status.idle":"2021-05-24T11:54:35.196947Z","shell.execute_reply.started":"2021-05-24T11:54:35.18996Z","shell.execute_reply":"2021-05-24T11:54:35.196168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model():\n    input = Input(shape=(50,))\n    word_embedding_size = 150\n\n    # Embedding Layer\n    model = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=50)(input)\n\n    # BI-LSTM Layer\n    model = Bidirectional(LSTM(units=word_embedding_size, \n                               return_sequences=True, \n                               dropout=0.5, \n                               recurrent_dropout=0.5, \n                               kernel_initializer=k.initializers.he_normal()))(model)\n    model = LSTM(units=word_embedding_size * 2, \n                 return_sequences=True, \n                 dropout=0.5, \n                 recurrent_dropout=0.5, \n                 kernel_initializer=k.initializers.he_normal())(model)\n\n    # TimeDistributed Layer\n    model = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  \n\n    # CRF Layer\n    crf = CRF(n_tags)\n    out = crf(model)\n\n    model = Model(input, out)\n\n    model.load_weights('../input/ner-bi-lstm-crf-model/ner-bi-lstm-td-model-0.97.hdf5')\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:55:14.442173Z","iopub.execute_input":"2021-05-24T11:55:14.442508Z","iopub.status.idle":"2021-05-24T11:55:14.451463Z","shell.execute_reply.started":"2021-05-24T11:55:14.442459Z","shell.execute_reply":"2021-05-24T11:55:14.450702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading saved model\nmodel = load_model()\n\n# Read demo file\nwords = readFile(\"../input/tbmmcorpusdonem2027/NLP/donem20/yıl4/11.txt\")\nprint(\"\\n\\nAfter formating --> \\t\", words[0])\n\nmaxlen = max([len(w) for w in words])\nn_words = len(words)\n\nX = [word2idx[w[1]] for w in words]\nX = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\",value=n_words - 1)\n\nprint(\"\\n\\nAfter word to idx --> \\t\", X[0])\n\n# i = 48\n# p = model.predict(np.array([X[i]]))\n# p = np.argmax(p, axis=-1)\n# gt = np.argmax(y_test[i], axis=-1)\n# print(\"{:14}: {:5}: {}\".format(\"Word\\t\", \"True\\t\", \"Pred\\t\"))\n# for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n#     if words[w] != \"unk\":\n#         print(\"{:14}: {:5}: \\t{}\".format(words[w],idx2tag[gt[idx]],tags[pred]))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T11:56:29.730145Z","iopub.execute_input":"2021-05-24T11:56:29.730481Z","iopub.status.idle":"2021-05-24T11:56:34.807409Z","shell.execute_reply.started":"2021-05-24T11:56:29.73042Z","shell.execute_reply":"2021-05-24T11:56:34.805287Z"},"trusted":true},"execution_count":null,"outputs":[]}]}