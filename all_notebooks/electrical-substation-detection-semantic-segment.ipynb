{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Electrical Substation Detection\n\nThe task of this competition is to develop a Machine learning-based software using \nopen source tools. Further extract Electrical Substations from high resolution satellite \ndata,  and  submit  a  paper (PDF) describing  the  techniques  employed  in  solving  the \nproblem.  ","metadata":{"id":"JXTkH7miLdFb"}},{"cell_type":"code","source":"import numpy as np\nfrom skimage.io import imread\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport os\nfrom glob import glob\nimport random\nimport itertools\nfrom skimage import util\nfrom tqdm import tqdm_notebook\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"agOlt5D7LdFh","executionInfo":{"status":"ok","timestamp":1623242294814,"user_tz":-330,"elapsed":7863,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"outputId":"bb1d61b6-ef81-4c44-f339-203bcdabff12","execution":{"iopub.status.busy":"2021-06-10T10:40:00.006627Z","iopub.execute_input":"2021-06-10T10:40:00.006962Z","iopub.status.idle":"2021-06-10T10:40:00.011484Z","shell.execute_reply.started":"2021-06-10T10:40:00.006933Z","shell.execute_reply":"2021-06-10T10:40:00.010651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n\nThe  contestants will be provided with  the \nTraining Dataset \nof 100 Satellite  data \nchips of ~1m resolution for training. Each image chip will have one electrical substation \nfeature. Set of Points and Polygons AOI will also be provided for training the Machine \nlearning  network.  A \nTesting  Dataset \nof 20  satellite  data  chip’s  mosaic,  containing \nsubstations and other features will be provided for testing. The Tutorial\n Links \nwill be \nprovided for online learning of the API’s and libraries.","metadata":{"id":"B1E-1fcFLdFj"}},{"cell_type":"markdown","source":"## Observation\n\nThe bounding box or the observation file is not a square bounding box, it is a shape file and hence the case needs to be treated as an Image Segmentation Problem.\n\n\n## DataSet Preparation\n\nWe need to generate masks from shapefile to get a required label for each pixel.","metadata":{"id":"5CBFh0sPLdFo"}},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras\n!pip install segmentation_models\n\nimport segmentation_models as sm\n\n!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n\nimport albumentations as A\n\nimport cv2\n\nimport tensorflow.keras as keras\n\nimport tensorflow as tf\ntf.config.run_functions_eagerly(True)\n# from keras.utils import np_utils","metadata":{"id":"66Vz0uCynioy","executionInfo":{"status":"ok","timestamp":1623242365157,"user_tz":-330,"elapsed":16045,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"outputId":"9d29a5b4-97df-4763-a458-c9c830d06302","execution":{"iopub.status.busy":"2021-06-10T10:40:00.018318Z","iopub.execute_input":"2021-06-10T10:40:00.018619Z","iopub.status.idle":"2021-06-10T10:40:23.451162Z","shell.execute_reply.started":"2021-06-10T10:40:00.018587Z","shell.execute_reply":"2021-06-10T10:40:23.45022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_dir = '../input/electrical-substation-detection/train/image_chips'\ny_train_dir = '../input/electrical-substation-detection/train/labels'\n\nx_valid_dir = '../input/electrical-substation-detection/validation/image_chips'\ny_valid_dir = '../input/electrical-substation-detection/validation/labels'\n\n# x_test_dir = os.path.join(DATA_DIR, 'test')\n# y_test_dir = os.path.join(DATA_DIR, 'testannot')","metadata":{"id":"Idp45E5ZBbQr","executionInfo":{"status":"ok","timestamp":1623242365159,"user_tz":-330,"elapsed":18,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:40:23.453757Z","iopub.execute_input":"2021-06-10T10:40:23.45404Z","iopub.status.idle":"2021-06-10T10:40:23.463649Z","shell.execute_reply.started":"2021-06-10T10:40:23.454008Z","shell.execute_reply":"2021-06-10T10:40:23.462833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n    \n# helper function for data visualization    \ndef denormalize(x):\n    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n    x_max = np.percentile(x, 98)\n    x_min = np.percentile(x, 2)    \n    x = (x - x_min) / (x_max - x_min)\n    x = x.clip(0, 1)\n    return x\n    \n\n# classes for data loading and preprocessing\nclass Dataset:\n    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    CLASSES = ['nodetect', 'es']\n    \n    def __init__(\n            self, \n            images_dir, \n            masks_dir, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.ids = os.listdir(images_dir)\n        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n        \n        # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_fps[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.masks_fps[i], 0)\n        mask = mask/mask.max()\n        \n        # extract certain classes from mask (e.g. cars)\n        masks = [(mask == v) for v in self.class_values]\n        mask = np.stack(masks, axis=-1).astype('float')\n        \n        # add background if mask is not binary\n        if mask.shape[-1] != 1:\n            background = 1 - mask.sum(axis=-1, keepdims=True)\n            mask = np.concatenate((mask, background), axis=-1)\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        return len(self.ids)\n    \n    \nclass Dataloder(keras.utils.Sequence):\n    \"\"\"Load data from dataset and form batches\n    \n    Args:\n        dataset: instance of Dataset class for image loading and preprocessing.\n        batch_size: Integet number of images in batch.\n        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n    \"\"\"\n    \n    def __init__(self, dataset, batch_size=1, shuffle=False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataset))\n\n        self.on_epoch_end()\n\n    def __getitem__(self, i):\n        \n        # collect batch data\n        start = i * self.batch_size\n        stop = (i + 1) * self.batch_size\n        data = []\n        for j in range(start, stop):\n            data.append(self.dataset[j])\n        \n        # transpose list of lists\n        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n        \n        return batch[0], batch[1]\n    \n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\"\"\"\n        return len(self.indexes) // self.batch_size\n    \n    def on_epoch_end(self):\n        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n        if self.shuffle:\n            self.indexes = np.random.permutation(self.indexes)","metadata":{"id":"kZr8ao5o-lxb","executionInfo":{"status":"ok","timestamp":1623242365162,"user_tz":-330,"elapsed":18,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:40:23.466598Z","iopub.execute_input":"2021-06-10T10:40:23.466862Z","iopub.status.idle":"2021-06-10T10:40:23.48739Z","shell.execute_reply.started":"2021-06-10T10:40:23.466837Z","shell.execute_reply":"2021-06-10T10:40:23.486289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets look at data we have\ndataset = Dataset(x_train_dir, y_train_dir, classes=['NoDetect', 'ES'])\n\nimage, mask = dataset[10] # get some sample\nvisualize(\n    image=image, \n    substation_mask=mask[..., 1].squeeze(),\n)","metadata":{"id":"2V0VQifrDimE","executionInfo":{"status":"ok","timestamp":1623242367173,"user_tz":-330,"elapsed":2027,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"outputId":"1cde84f3-cd7a-488e-943c-5fb5e78630a4","execution":{"iopub.status.busy":"2021-06-10T10:40:23.489058Z","iopub.execute_input":"2021-06-10T10:40:23.489712Z","iopub.status.idle":"2021-06-10T10:40:23.754699Z","shell.execute_reply.started":"2021-06-10T10:40:23.489648Z","shell.execute_reply":"2021-06-10T10:40:23.753869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 512\n\n\ndef round_clip_0_1(x, **kwargs):\n    return x.round().clip(0, 1)\n\ndef normalize_albumenation(x, **kwargs):\n  return x\n\n# define heavy augmentations\ndef get_training_augmentation():\n    train_transform = [\n\n        A.HorizontalFlip(p=0.5),\n\n        A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=30, shift_limit=0.1, p=1, border_mode=0),\n\n        A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, always_apply=True, border_mode=0),\n        A.RandomCrop(height=IMG_SIZE, width=IMG_SIZE, always_apply=True),\n\n#         A.IAAAdditiveGaussianNoise(p=0.2),\n        # A.IAAPerspective(p=0.5),\n\n#         A.OneOf(\n#             [\n#                 A.CLAHE(p=1),\n#                 A.RandomBrightness(p=1),\n#                 A.RandomGamma(p=1),\n#             ],\n#             p=0.9,\n#         ),\n\n        # A.OneOf(\n        #     [\n        #         A.IAASharpen(p=1),\n        #         A.Blur(blur_limit=3, p=1),\n        #         A.MotionBlur(blur_limit=3, p=1),\n        #     ],\n        #     p=0.9,\n        # ),\n\n#         A.OneOf(\n#             [\n#                 A.RandomContrast(p=1),\n#                 A.HueSaturationValue(p=1),\n#             ],\n#             p=0.9,\n#         ),\n        A.Lambda(mask=round_clip_0_1)\n    ]\n    return A.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n    test_transform = [\n                      #  A.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n        A.RandomCrop(height=IMG_SIZE, width=IMG_SIZE, always_apply=True)\n        # A.PadIfNeeded(384, 480)\n    ]\n    return A.Compose(test_transform)\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        A.Lambda(image=preprocessing_fn),\n        A.Lambda(image=normalize_albumenation)\n    ]\n    return A.Compose(_transform)","metadata":{"id":"HaXNmERgJRDJ","executionInfo":{"status":"ok","timestamp":1623242367174,"user_tz":-330,"elapsed":10,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:40:23.755975Z","iopub.execute_input":"2021-06-10T10:40:23.756404Z","iopub.status.idle":"2021-06-10T10:40:23.768706Z","shell.execute_reply.started":"2021-06-10T10:40:23.756367Z","shell.execute_reply":"2021-06-10T10:40:23.767618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Lets look at augmented data we have\ndataset = Dataset(x_train_dir, y_train_dir, classes=['nodetect', 'es'], augmentation=get_training_augmentation())\n\nimage, mask = dataset[10] # get some sample\nvisualize(\n    image=image, \n    substation_mask=mask[..., 1].squeeze(),\n)","metadata":{"id":"iGIH4sfUJoZL","executionInfo":{"status":"ok","timestamp":1623242368892,"user_tz":-330,"elapsed":1726,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"outputId":"db256777-c501-4154-94be-1e2e8942a7e8","execution":{"iopub.status.busy":"2021-06-10T10:40:23.770096Z","iopub.execute_input":"2021-06-10T10:40:23.770644Z","iopub.status.idle":"2021-06-10T10:40:24.004389Z","shell.execute_reply.started":"2021-06-10T10:40:23.770553Z","shell.execute_reply":"2021-06-10T10:40:24.003412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BACKBONE = 'resnet34'\nBATCH_SIZE = 8\nCLASSES = ['es']\nLR = 0.0001\nEPOCHS = 150\nversion = 8\n\npreprocess_input = sm.get_preprocessing(BACKBONE)","metadata":{"id":"6fUsB3c-NkQU","executionInfo":{"status":"ok","timestamp":1623242368894,"user_tz":-330,"elapsed":18,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:40:24.005992Z","iopub.execute_input":"2021-06-10T10:40:24.006359Z","iopub.status.idle":"2021-06-10T10:40:24.012048Z","shell.execute_reply.started":"2021-06-10T10:40:24.00632Z","shell.execute_reply":"2021-06-10T10:40:24.010718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define network parameters\nn_classes = 1 if len(CLASSES) == 1 else (len(CLASSES) + 1)  # case for binary and multiclass segmentation\nactivation = 'sigmoid' if n_classes == 1 else 'softmax'\n\ntf.keras.backend.clear_session()\n#create mode\n# model = sm.PSPNet(BACKBONE, classes=n_classes, activation=activation)\n# model = sm.FPN(BACKBONE, classes=n_classes, activation=activation)\nmodel = sm.Unet(BACKBONE, classes=n_classes, activation=activation, encoder_weights='imagenet') #'imagenet')#, encoder_freeze=True)\n# model = sm.Unet( classes=n_classes, activation=activation)\n","metadata":{"id":"wNNmmsGPNrdl","executionInfo":{"status":"ok","timestamp":1623242369530,"user_tz":-330,"elapsed":652,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"outputId":"811f7924-54a7-4a82-e5b0-c7c7eff4fcc0","execution":{"iopub.status.busy":"2021-06-10T10:40:24.014819Z","iopub.execute_input":"2021-06-10T10:40:24.015356Z","iopub.status.idle":"2021-06-10T10:40:25.140206Z","shell.execute_reply.started":"2021-06-10T10:40:24.015319Z","shell.execute_reply":"2021-06-10T10:40:25.139235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define optomizer\noptim = keras.optimizers.Adam(LR)\n\n# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\ndice_loss = sm.losses.DiceLoss()\nfocal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\njacard_loss = sm.losses.JaccardLoss()\n\n# total_loss =dice_loss + (1*focal_loss) + (1*jacard_loss) + (1*bce_loss)\ntotal_loss = dice_loss+jacard_loss+focal_loss\n\n\n# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n\nmetrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n\n# compile keras model with defined optimozer, loss and metrics\nmodel.compile(optim, total_loss, metrics)","metadata":{"id":"ZqBG1ryTOoSk","executionInfo":{"status":"ok","timestamp":1623242369530,"user_tz":-330,"elapsed":6,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:40:25.141718Z","iopub.execute_input":"2021-06-10T10:40:25.142053Z","iopub.status.idle":"2021-06-10T10:40:25.160242Z","shell.execute_reply.started":"2021-06-10T10:40:25.142015Z","shell.execute_reply":"2021-06-10T10:40:25.158813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset for train images\ntrain_dataset = Dataset(\n    x_train_dir, \n    y_train_dir, \n    classes=CLASSES, \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocess_input),\n)\n\n# Dataset for validation images\nvalid_dataset = Dataset(\n    x_valid_dir, \n    y_valid_dir, \n    classes=CLASSES, \n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocess_input),\n)\n\ntrain_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n\n# check shapes for errors\nassert train_dataloader[0][0].shape == (BATCH_SIZE, IMG_SIZE, IMG_SIZE, 3)\nassert train_dataloader[0][1].shape == (BATCH_SIZE, IMG_SIZE, IMG_SIZE, n_classes)\n\n# define callbacks for learning rate scheduling and best checkpoints saving\ncallbacks = [\n    keras.callbacks.ModelCheckpoint('/kaggle/working/best_model_v%d.h5'%version, save_weights_only=True,save_best_only=True, mode='min', verbose=1),\n    keras.callbacks.ReduceLROnPlateau(factor=0.5, verbose=1),\n]\n\nwith open('/kaggle/working/ModelSummary_v%d.txt'%version, 'w') as f:\n  model.summary(print_fn=f.write)\n# model.summary()","metadata":{"id":"yh5JTAJcQQdB","executionInfo":{"status":"ok","timestamp":1623242375078,"user_tz":-330,"elapsed":5552,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:40:25.161965Z","iopub.execute_input":"2021-06-10T10:40:25.162512Z","iopub.status.idle":"2021-06-10T10:40:25.592173Z","shell.execute_reply.started":"2021-06-10T10:40:25.162443Z","shell.execute_reply":"2021-06-10T10:40:25.591341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nhistory = model.fit_generator(\n    train_dataloader, \n    steps_per_epoch=len(train_dataloader), \n    epochs=EPOCHS, \n    callbacks=callbacks, \n    validation_data=valid_dataloader, \n    validation_steps=len(valid_dataloader),\n)","metadata":{"id":"i0GNQU7zTMG5","executionInfo":{"status":"error","timestamp":1623242906093,"user_tz":-330,"elapsed":531021,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"outputId":"48cad781-d479-4dd3-9e17-0c69b97a7889","execution":{"iopub.status.busy":"2021-06-10T10:40:25.593338Z","iopub.execute_input":"2021-06-10T10:40:25.593699Z","iopub.status.idle":"2021-06-10T10:54:53.762617Z","shell.execute_reply.started":"2021-06-10T10:40:25.593662Z","shell.execute_reply":"2021-06-10T10:54:53.761826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation iou_score values\nplt.figure(figsize=(30, 10))\nplt.subplot(121)\nplt.plot(history.history['iou_score'])\nplt.plot(history.history['val_iou_score'])\nplt.title('Model iou_score')\nplt.ylabel('iou_score')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\n\n# Plot training & validation loss values\nplt.subplot(122)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig('/kaggle/working/TrainingSummary_v%d.jpg'%version)\nplt.show()\n","metadata":{"id":"zhoq10V4Zj46","executionInfo":{"status":"aborted","timestamp":1623242906087,"user_tz":-330,"elapsed":28,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:54:53.764175Z","iopub.execute_input":"2021-06-10T10:54:53.764528Z","iopub.status.idle":"2021-06-10T10:54:54.215681Z","shell.execute_reply.started":"2021-06-10T10:54:53.76449Z","shell.execute_reply":"2021-06-10T10:54:54.214694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img = imread('../input/electrical-substation-detection/test/mosaic_test.jpg')\nout_mask = np.zeros((test_img.shape[0], test_img.shape[1]))\n\nimg_x, img_y = 768, 768\n\n# test_model = keras.models.load_model('/kaggle/working/best_model_v%d.h5'%version)\nmodel.load_weights('/kaggle/working/best_model_v%d.h5'%version)\nfor i in range(5):\n    for j in range(5):\n        image = np.zeros((768,768,3))\n        image[:750, :750] = test_img[i*750:(i+1)*750, j*750:(j+1)*750]\n        image = get_preprocessing(preprocessing_fn=preprocess_input)(image=image)['image']\n        image = np.expand_dims(image, axis=0)\n        out_mask[i*750:(i+1)*750, j*750:(j+1)*750] = model.predict(image)[0,:750,:750,0]\n\n\nvisualize(\n    image=denormalize(test_img.squeeze()),\n    out_mask=out_mask,\n    out_mask_rounded=out_mask.round()\n)\n\nplt.imsave('/kaggle/working/OutMaskv%d.jpg'%version, out_mask.round(), cmap='gray')\nnp.save(\"/kaggle/working/out_mask_v%d.npy\"%version, np.array(out_mask))","metadata":{"id":"rDapgGU6j9V-","executionInfo":{"status":"aborted","timestamp":1623242906090,"user_tz":-330,"elapsed":26,"user":{"displayName":"samvram sahu","photoUrl":"","userId":"11397110940859969323"}},"execution":{"iopub.status.busy":"2021-06-10T10:54:54.216978Z","iopub.execute_input":"2021-06-10T10:54:54.217489Z","iopub.status.idle":"2021-06-10T10:55:04.185014Z","shell.execute_reply.started":"2021-06-10T10:54:54.217445Z","shell.execute_reply":"2021-06-10T10:55:04.183899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Individual Testing\n\nIf the original test image can be seen comprising of 5 x 5 tiles of 750 x 750 images, we can see individual results below by altering `i` for row index and `j` for column index of composing 5x5 = 25 images.","metadata":{}},{"cell_type":"code","source":"i=1\nj=3\n\nimage = np.zeros((768,768,3))\n\nimage[:750, :750] = test_img[i*750:(i+1)*750, j*750:(j+1)*750]\nimage = get_preprocessing(preprocessing_fn=preprocess_input)(image=image)['image']\nimage = np.expand_dims(image, axis=0)\nout_mask= model.predict(image)[0,:750,:750,0]\n\nvisualize(Input=test_img[i*750:(i+1)*750, j*750:(j+1)*750], Model_Results=out_mask, Predicted=out_mask.round())","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:55:04.186619Z","iopub.execute_input":"2021-06-10T10:55:04.187038Z","iopub.status.idle":"2021-06-10T10:55:04.595728Z","shell.execute_reply.started":"2021-06-10T10:55:04.186993Z","shell.execute_reply":"2021-06-10T10:55:04.594902Z"},"trusted":true},"execution_count":null,"outputs":[]}]}