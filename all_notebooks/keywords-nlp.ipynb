{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nauthors = pd.read_csv(\"../input/nips-2015-papers/Authors.csv\")\npaper_authors = pd.read_csv(\"../input/nips-2015-papers/PaperAuthors.csv\")\npapers = pd.read_csv(\"../input/nips-2015-papers/Papers.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sample view on each dataframes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"authors.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paper_authors.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HIGH LEVEL APPROACH**\n1. > Text PreProcessing\n       a.Noise Removal\n       b.Normalisation\n2. > Data Exploration\n       a.Word cloud to understand frequently used words\n       b.Top 20 unigrams,bi-grams,tri-grams\n3. > Convert text to a vector of word counts  \n4. > Covert text to a vector of term frequencies\n5. > Sort terms based on term frequencies in descending order to identify top N keywords"},{"metadata":{},"cell_type":"markdown","source":"> Before preprocessing,it is advisable to explore dataset in terms of word count,most common and uncommon words "},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.isnull().describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Dropping Event type column as it has no relevance at all."},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.drop(['EventType'],axis=1,inplace=True)\npapers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Words of each abstract are counted"},{"metadata":{"trusted":true},"cell_type":"code","source":"papers['word_count']=papers['Abstract'].apply(lambda x:len(str(x).split(\" \")))\npapers[['Abstract','word_count']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.word_count.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It indicate that average count for each word is 149 words per abstract.Min words per abstract is 58 and maximum words are 296 words.The word count is important to give us an indication of the size of the dataset that we are handling as well as the variation in word counts across the rows"},{"metadata":{},"cell_type":"markdown","source":"> **Frequently used words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(papers['Abstract']).split()).value_counts()[:20]\nfreq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Uncommon Words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"non_freq = pd.Series(' '.join(papers['Abstract']).split()).value_counts()[-20:]\nnon_freq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Text Pre-Processing **\n> Objectives\n1. Text Clean Up\n2. Shrinking the vocabulary to retain only relevant/important words\n3. Reduce sparsity"},{"metadata":{},"cell_type":"markdown","source":"> Noise Removal can be done by removing redundant text components like Punctuation,Tags,URLs\n> Normalization can be done by Lemmatization which works based on the root of the world and Stemming that removes suffixes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nstem = PorterStemmer()\nword = \"good\"\nprint(\"stemming:\",stem.stem(word))\nprint(\"lemmatization:\", lem.lemmatize(word, \"v\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The stemming and lemmatizaton of most of the nouns are same as that of words itself.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Stop words include the large number of prepositions, pronouns, conjunctions etc in sentences. These words need to be removed before we analyse the text, so that the frequently used words are mainly the words relevant to the context and not common words used in the text."},{"metadata":{},"cell_type":"markdown","source":"> There is a default list of stopwords in python nltk library. In addition, we might want to add context specific stopwords for which the “most common words” that we listed in the beginning will be helpful."},{"metadata":{"trusted":true},"cell_type":"code","source":"##Creating a list of stopwords and adding a custom list of stopwords\nstop_words=set(stopwords.words('english'))\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Creating a list of custom stopwords\nnew_words= [\"using\", \"show\", \"result\", \"large\", \"also\",\n            \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\"]\nstop_words=stop_words.union(new_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing Text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=[]\nfor i in range(0,403):\n    #remove punctuations\n    text=re.sub('^[a-zA-Z]',' ',papers.Abstract[i])\n    #convert to lower case\n    text=text.lower()\n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    #remove special characters and digits\n    text=re.sub('(\\\\d|\\\\W)',' ',text)\n    #Convert to list from string\n    list=text.split()\n    #Stemming\n    ps=PorterStemmer()\n    #Lemmatization\n    lem=WordNetLemmatizer()\n    temp=[lem.lemmatize(word) for word in list if word not in stop_words]\n    text=\" \".join(temp)\n    corpus.append(text)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualisation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nwordcloud = WordCloud(background_color='white',stopwords=stop_words,\n                                        max_words=100,\n                                        max_font_size=50, \n                                        random_state=42 #near to std_dev\n                                        ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Tokenisation is the process of converting the continuous text into a list of words. The list of words is then converted to a matrix of integers by the process of vectorisation. Vectorisation is also called feature extraction."},{"metadata":{},"cell_type":"markdown","source":"**CountVectoriser to tokenise the text and build a vocabulary of known words.**\n>     cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n    max_df — When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). This is to ensure that we only have words relevant to the context and not commonly used words.\n    max_features — determines the number of columns in the matrix.\n    n-gram range — we would want to look at a list of single words, two words (bi-grams) and three words (tri-gram) combinations."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Most frequently occuring words\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(corpus, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Most frequently occuring Bi-grams\ndef get_top_n2_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2),  \n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop2_words = get_top_n2_words(corpus, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Most frequently occuring Tri-grams\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), \n           max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting to a matrix of integers\n\nThe next step of refining the word counts is using the TF-IDF vectoriser. The deficiency of a mere word count obtained from the countVectoriser is that, large counts of certain common words may dilute the impact of more context specific words in the corpus. This is overcome by the TF-IDF vectoriser which penalizes words that appear several times across the document. TF-IDF are word frequency scores that highlight words that are more important to the context rather than those that appear frequently across documents.\n\nTF-IDF consists of 2 components:\n\n    TF — term frequency\n    IDF — Inverse document frequency\n    TF=Freq of terms/total no of terms\n    IDF=log(total documents)/number of documents with the term\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)# get feature names\nfeature_names=cv.get_feature_names()\n \n# fetch document for which keywords needs to be extracted\ndoc=corpus[123]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for sorting tf_idf in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,5)\n \n# now print the results\nprint(\"\\nAbstract:\")\nprint(doc)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ideally for the IDF calculation to be effective, it should be based on a large corpora and a good representative of the text for which the keywords need to be extracted. In our example, if we use the full article text instead of the abstracts, the IDF extraction would be much more effective. However, considering the size of the dataset, I have limited the corpora to just the abstracts for the purpose of demonstration."},{"metadata":{},"cell_type":"markdown","source":"This is a fairly simple approach to understand fundamental concepts of NLP and to provide a good hands-on practice with some python codes on a real-life use case. The same approach can be used to extract keywords from news feeds & social media feeds."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}