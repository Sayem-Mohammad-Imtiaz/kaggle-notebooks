{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sunspots/Sunspots.csv')\n\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time = df.iloc[:, 0]\nseries = df.iloc[:, 2]\n\nprint(time.shape)\nprint(series.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_series(time, series, fmt='-', start=0, end=None):\n    plt.plot(time[start:end], series[start:end], fmt)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.grid('on')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplot_series(time, series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time, series, start=1000, end=1300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data has a bit of <b>seasonality</b>. Each season is approximately 132 time steps (132 months = 11 years) long.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preparing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"split_time = 3000\ntime_train = time[:split_time]\nX_train = series[:split_time]\ntime_val = time[split_time:]\nX_val = series[split_time:]\n\nplt.figure(figsize=(15, 5))\nplot_series(time_train, X_train)\nplot_series(time_val, X_val)\nplt.legend(['Train', 'Validation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle=True, shuffle_buffer=None):\n    series = tf.expand_dims(series, axis=-1)\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)   # step once and slice series into (window_shape + 1) windows. [+1 for output]\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))  # convert them into tensors\n    if shuffle:\n        dataset = dataset.shuffle(shuffle_buffer)  # shuffling windows to get rid of 'sequence bias'\n    dataset = dataset.map(lambda window: (window[:-1], window[1:]))   # making (x, y) split\n    dataset = dataset.batch(batch_size).prefetch(1)   # batching (x, y) into batch_size sets\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_forecast(model, series, window_size, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size)) \n    dataset = dataset.batch(batch_size).prefetch(1)\n    forecast = model.predict(dataset)\n    return forecast","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models, Train, Predict","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model with SGD optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nwindow_size = 64\ntrain_batch_size = 256\nval_batch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntrain_set = windowed_dataset(X_train, window_size, train_batch_size, shuffle_buffer=len(X_train))\nval_set = windowed_dataset(X_val, window_size, val_batch_size, shuffle=False)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\noptimizer  =tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=['mae'])\nhistory = model.fit(train_set, \n                    epochs=100,\n                    validation_data=val_set,\n                    callbacks=[lr_scheduler])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training, Validation loss & mae\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation Loss')\n\nplt.subplot(122)\nplt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.xlabel('epochs')\nplt.ylabel('mae')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation MAE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting learning-rate vs loss\nplt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-8, 1e-3, 0, 80])\nplt.axvline(8e-6, color='orange', alpha=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Till lr = 8e-6, loss seems stable. So let's try with <b>lr = 8e-6</b>.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nwindow_size = 60\ntrain_batch_size = 100\nval_batch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntrain_set = windowed_dataset(X_train, window_size, train_batch_size, shuffle_buffer=len(X_train))\nval_set = windowed_dataset(X_val, window_size, val_batch_size, shuffle=False)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=8e-6, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=['mae'])\nhistory = model.fit(train_set, \n                    epochs=500,\n                    validation_data=val_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training, Validation loss & mae\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation Loss')\n\nplt.subplot(122)\nplt.plot(history.history['loss'][200:])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Zoomed Training Loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = model_forecast(model, series[:, np.newaxis], window_size, batch_size=32)\nforecast = forecast[split_time-window_size : -1, -1, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_val, X_val)\nplot_series(time_val, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(X_val, forecast).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model with Adam optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nwindow_size = 60\ntrain_batch_size = 100\nval_batch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntrain_set = windowed_dataset(X_train, window_size, train_batch_size, shuffle_buffer=len(X_train))\nval_set = windowed_dataset(X_val, window_size, val_batch_size, shuffle=False)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                 factor=0.5, \n                                                 patience=5, \n                                                 min_lr=1e-7)\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='val_mae', mode='min', patience=15)\noptimizer = tf.keras.optimizers.Adam()\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=['mae'])\nhistory = model.fit(train_set, \n                    epochs=500,\n                    validation_data=val_set, \n                    callbacks=[reduce_lr, earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training, Validation loss & mae\nplt.figure(figsize=(8, 5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation Loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = model_forecast(model, series[:, np.newaxis], window_size, batch_size=32)\nforecast = forecast[split_time-window_size : -1, -1, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_val, X_val)\nplot_series(time_val, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(X_val, forecast).numpy()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}