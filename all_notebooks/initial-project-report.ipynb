{"cells":[{"metadata":{},"cell_type":"markdown","source":"# GRAD-E1326: Python Programming for Data Scientists\n## Ph.D. Hannah Béchara\n### Ji Yoon Han & Mariana G. Carrillo \n\n**Initial Project report: Tweet Sentiment Analysis**\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Importing libraries for sentiment analysis \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk #Natural Language Processing Package \nimport os #functions for interacting with the operating system\nimport spacy #Models for NLP\nimport torch #also for NLP\nfrom tqdm.notebook import tqdm \nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\nimport transformers #contains pretrained models to perform tasks on texts\nfrom transformers import BertForSequenceClassification\nfrom wordcloud import WordCloud #For nice wordclouds\nimport tensorflow as tf #Package to develop train models \nfrom tensorflow.keras.preprocessing import text \nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem import WordNetLemmatizer\nimport time #for handling dates and times\nimport re \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.metrics import AUC\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom nltk.tokenize import TweetTokenizer\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading data"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#Loading and cleaning data\ntrain_data = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='latin-1')\ntest_data = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train data \n#preview\ntrain_data.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#descriptive statistics\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test data\n#preview\ntest_data.head(5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#descriptive statistics\ntest_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Create histogram --> Distribution TEST DATA\n#Can we also make this a function?\nplt.figure(figsize=(12,6)) #specifying the size of the figure\nsns.set_palette(\"Spectral\") #color palette\nsns.countplot(x='Sentiment', data=test_data, order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], )\nplt.xlabel('Sentiment(tag)')\nplt.ylabel('Count of tweets')\nplt.suptitle('Histogram of tweet distribution per sentiment classification (Test data)')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create histogram --> Distribution TRAIN DATA\nplt.figure(figsize=(12,6))\nsns.set_palette(\"Spectral\")\nsns.countplot(x='Sentiment', data=train_data, order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], )\nplt.xlabel('Sentiment (tag)')\nplt.ylabel('Count of tweets')\nplt.suptitle('Histogram of tweet distribution per sentiment classification (Train data)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of tweet counts --> TEST DATA\ntest_data.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='line', figsize=(12, 6))\nplt.title('Tweets on Coronavirus March 2020 (Test data)')\nplt.ylabel('Tweet Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of tweet counts --> TRAIN DATA \ntrain_data.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='line', figsize=(12, 6))\nplt.title('Tweets on Coronavirus, March 2020, (Train data)')\nplt.ylabel('Tweet Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import nltk / stopwords\nimport nltk\nnltk.download('stopwords')\n\n# Define stopwords \nstop_words = stopwords.words('english') #defining var to remove stopwords in the process_tweet function \n\n# Define function for cleaning tweets \ndef clean_tweet(tweet):\n    tweet = re.sub(r'http\\S+', ' ', tweet) #removing urls\n    tweet = re.sub(r'<.*?>', ' ', tweet)  # removing html tags    \n    tweet = re.sub(r'\\d+', ' ', tweet) #removing digits\n    tweet = re.sub(r'#\\w+', ' ', tweet)    #removing hashtags\n    tweet = re.sub(r'@\\w+', ' ', tweet) #removing mentions\n    tweet = tweet.split() #removing stop words\n    tweet = \" \".join([word for word in tweet if not word in stop_words])\n    return tweet\n\ntrain_data['CleanTweet'] = train_data['OriginalTweet'].apply(lambda x: clean_tweet(x))\ntrain_data.head(10)\n\n# Clean tweets from test data by creating a new column in the test_data df\ntest_data['CleanTweet'] = test_data['OriginalTweet'].apply(lambda x: clean_tweet(x))\ntest_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n#Attempt to create wordcloud\ndef wordcloud1(training_data):\n    stopwords = set(STOPWORDS)\n    stopwords.add(\"https\")\n    stopwords.add(\"00A0\")\n    stopwords.add(\"00BD\")\n    stopwords.add(\"00B8\")\n    stopwords.add(\"ed\")\n    wordcloud1 = WordCloud(background_color=\"white\",stopwords=stopwords).generate(\" \".join([i for i in train_data['OriginalTweet'].str.upper()]))\n    plt.imshow(wordcloud1)\n    plt.axis(\"off\")\n    plt.title(\"Most common words, training data\")\n    figsize=(12, 6)\n\nwordcloud1(train_data)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Attempt to create wordcloud - test data\ndef wordcloud2(test_data):\n    stopwords = set(STOPWORDS)\n    stopwords.add(\"https\")\n    stopwords.add(\"00A0\")\n    stopwords.add(\"00BD\")\n    stopwords.add(\"00B8\")\n    stopwords.add(\"ed\")\n    wordcloud2 = WordCloud(background_color=\"white\",stopwords=stopwords).generate(\" \".join([i for i in test_data['OriginalTweet'].str.upper()]))\n    plt.imshow(wordcloud2)\n    plt.axis(\"off\")\n    plt.title(\"Most common words, training data\")\n    figsize=(12, 6)\n\nwordcloud2(test_data)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"## tweet tokenizer \n\nimport nltk\nnltk.download('punkt')\n\nfrom nltk.tokenize import TweetTokenizer \n\ncompare_list = train_data['CleanTweet'].head(10)\n\n## need to add code to clean test_data\n\ntweet_tokenizer = TweetTokenizer()\n\ntweet_tokens = []\nfor sent in compare_list:\n    print(tweet_tokenizer.tokenize(sent))\n    tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer \nimport pandas as pd\n\n#instantiate CountVectorizer() \nvectoriser = CountVectorizer()\n\n#Generate vectors\n\nX_test = vectoriser.transform(test_data[\"CleanTweet\"])\ny_test = encoder.transform(test_data[\"Sentiment\"])\n\n \n# this steps generates word counts for the words in your docs \nXtest_wcount = cv.fit_transform(X_test)\nytest_wcount = cv.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TF-IDF Model - Ji Yoon**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import words\n\ntv = TfidfVectorizer(\n                    ngram_range = (1,3),\n                    sublinear_tf = True,\n                    max_features = 40000)\n\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_data(train_data, train_data['CleanTweet'], random_state=42, test_size=0.3)\n\ntrain_tv= tv.fit_transform(train_data['CleanTweet'])\ntest_tv= tv.fit_transform(test_data['CleanTweet'])\n\ntrain_tfidf = train_tv[:31962,:]\ntest_tfidf = test_tv[31962:,:]\n\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain)\n\nprediction = lreg.predict_proba(xvalid_tfidf)\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)\n\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### References \n* Matplotlib.org. 2020. Pyplot Tutorial — Matplotlib 3.3.2 Documentation. [online] Available at: <https://matplotlib.org/tutorials/introductory/pyplot.html> [Accessed 20 October 2020].\n* Kaggle.com. 2020. Sentiment Prediction. [online] Available at: <https://www.kaggle.com/shahraizanwar/covid19-tweets-sentiment-prediction-rnn-85-acc> [Accessed 18 October 2020].\n* "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}