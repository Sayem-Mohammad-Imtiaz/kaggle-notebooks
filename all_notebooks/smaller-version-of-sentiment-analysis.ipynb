{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport regex as re\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\nimport gc\nimport json\nfrom keras_preprocessing.text import tokenizer_from_json\nfrom keras.models import model_from_json\n\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tweet = pd.read_csv('/kaggle/input/first-gop-debate-twitter-sentiment/Sentiment.csv')[['sentiment' , 'text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tweet_sentiment(text):\n    if str(text) == 'Negative':\n        return 0\n    if str(text) == 'Neutral':\n        return 2\n    if str(text) =='Positive':\n        return 1\ntweet['sentiment'] = tweet.sentiment.apply(lambda x: tweet_sentiment(x))\ntweet = tweet[tweet.sentiment != 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tweet['text'] = tweet['text'].apply(lambda x: re.sub('@[a-zA-Z0-9]+:','',x))\n# tweet['text'] = tweet['text'].apply(lambda x: re.sub('@[a-zA-Z0-9]+','',x))\n# tweet['text'] = tweet['text'].apply(lambda x: re.sub('RT','',x))\n# tweet['text'] = tweet['text'].apply(lambda x: re.sub('[^a-zA-Z0-9\\s]','',x))\n# tweet['text'] = tweet['text'].apply(lambda x: x.lower())\ntweet['count'] = tweet['text'].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet = tweet.rename(columns = {'text': 'reviews' })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alexa = pd.read_csv('/kaggle/input/amazon-alexa-reviews/amazon_alexa.tsv' , delimiter = '\\t')[['verified_reviews','feedback']]\nalexa = alexa.rename(columns={'verified_reviews':'reviews' , 'feedback':'sentiment'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alexa['count'] = alexa['reviews'].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alexa.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([alexa , tweet], axis= 0 , sort = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['reviews' , 'sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\ndef preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\ndata.reviews = data.reviews.apply(lambda x: preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"documents = [_text.split() for _text in data.reviews] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W2V_SIZE = 30\nW2V_WINDOW = 7\nW2V_EPOCH = 16\nW2V_MIN_COUNT = 5\nw2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)\n\n\nw2v_model.build_vocab(documents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.most_similar(\"awesome\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(data.reviews)\nvocab_size = len(tokenizer.word_index)+1\nprint('Vocab Size is ',vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQUENCE_LENGTH = 30\nEPOCHS = 8\nBATCH_SIZE = 1024\nx_data = pad_sequences(tokenizer.texts_to_sequences(data.reviews) , maxlen = SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_data = data.sentiment\nprint(x_data.shape)\nprint(y_data.shape)\ny_data = y_data.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size , W2V_SIZE))\nfor word , i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding( vocab_size , W2V_SIZE , weights = [embedding_matrix] , input_length = SEQUENCE_LENGTH, trainable = False)\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100 , dropout = 0.2 , recurrent_dropout = 0.2 ))\nmodel.add(Dense(1 , activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\ncallbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_data , y_data , batch_size = BATCH_SIZE , epochs = EPOCHS , validation_split = 0.1  , verbose = 1 , callbacks = callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(text):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    # Predict\n    score = model.predict([x_test])[0]\n\n    return {\"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}  \n\nprint(predict('i am Happy'))\nprint(predict('i not feeling so great .Little Rest can help but you decide what should i do next '))\nprint(predict('i am sitting in library for 6 hours . i learned alot but i am tired'))\nprint(predict('i am tired'))\nprint(predict('good is not good'))\nprint(predict('bad is not good'))\nprint(predict('good is not bad'))\nprint(predict('how i can end up here'))\nprint(predict('i am sad'))\nprint(predict('i donot think this is working i tried everything but i have to think of some thing else'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('model_weights.h5')\nwith open('model_architecture.json', 'w') as f:\n    f.write(model.to_json())\n    \nmodel.save('entire_model.h5')\ntokenizer_json = tokenizer.to_json()\nwith open('tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}