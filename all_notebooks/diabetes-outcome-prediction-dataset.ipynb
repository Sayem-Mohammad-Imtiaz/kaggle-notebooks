{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font color='red'>\n<br>Content:\n    \n1. [>introduction](#1)    \n    * [preliminary information](#2)\n    * [dataset](#3)    \n1. [Normalization](#4)    \n    * [Normalization](#5)\n    * [Train/Test Split](#6)\n    * [Initialize and Sigmoid Func](#7)\n    * [Sigmoid Func](#8)\n    * [Updating(Learning)](#9)\n    * [Prediction](#10)\n    * [Logistic Regression](#11)\n    * [Sklearn with Logistic Regresion](#12)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# >introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# preliminary information","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. * Logistic Regression ( Lojistik Regresyon ) sınıflandırma işlemi yapmaya yarayan bir regresyon yöntemidir\n1. * For example,\n*     To predict whether an email is spam (1) or (0)\n*     Whether the tumor is malignant (1) or not (0)\n1. Consider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.\n1. From this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.\n\n\n<a ><img src=\"https://miro.medium.com/max/770/1*UgYbimgPXf6XXxMy2yqRLw.png\" alt=\"1\" border=\"0\">\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a ><img src=\"http://preview.ibb.co/cxP63H/5.jpg\" alt=\"1\" border=\"0\">\n    \n    \n* Let's try to explain how Logistic Regression works on the chart above.\n* We have visuals with 4096 pixels in size and these images also have 0 and 1 values ​​in different drawings.\n\n* Each pixel creates our input values, these go up to px1, px2… px4096.\n\n* We multiply each input value by the weights values.\n(px1w1 + px2w2 +… px4096 * w4096)\nWe add the value we get by bias and get a z value.\n\n* z = (px1w1 + px2 + w2 +… px4096w4096) + b\nWe subject the obtained z-value to the Sigmoid Function. The Sigmoid Function generates us values ​​between 0 and 1.\n\n* The Sigmoid process is now finished, we have a predict value by the model. We call it y_head.\n\n* If y_head is greater than 0.5, the result is 1, otherwise 0.\n\n* When we ask the model to recognize 0's and 1's, and return us false predictions, we get a loss function.\n\n* When we collect all the loss functions, we get the cost (cost) value.\n\n* We call the processes starting from the first step to the last step as Forward Propagation.\n\n* Ok, now we have created the model, but we had too many errors. Why? Because at first we assigned the first parameter (weights and bias) values ​​and the cost value turned out to be an absurd or high value.\n\n* We should minimize the cost value so that the model works with the best performance. What are we doing for this? Back Propagation.\n\n* With the back propagation algorithm, weights and bias are now determined according to the cost value. Then we do the same again. Until the cost value reaches the minimum value, we understand this when it starts to repeat itself, that is, when it approaches 0.\n\n* Weights: Weight or coefficient of each pixel,\n* Bias: Cut-off value.\n    \n> I hope these explanations were useful.\n \n* Let's dump this into code now ;)    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data.Outcome.values\n\nx_data=data.drop(\"Outcome\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# Train/Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n# Initialize and Sigmoid Func","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimension = 30\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n\n# w,b = initialize_weights_and_bias(30)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Sigmoid Func","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head\n# print(sigmoid(0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# Forward Backward Propagation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n# Updating(Learning)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n# Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 500)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**** Our 1st method correct estimate rate = 77.27272727272728%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n# Sklearn with Logistic Regresion ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**** Our 2st method correct estimate rate = 76.62337662337663%.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}