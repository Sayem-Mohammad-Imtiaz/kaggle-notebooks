{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing required libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_df = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_df['quality'].value_counts().plot(kind='pie', figsize = (10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data = wine_df, x='quality')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution plot of all the columns of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in wine_df.drop('quality', axis = 1):\n    sns.distplot(wine_df[j], kde = False)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier"},{"metadata":{},"cell_type":"markdown","source":"Standardising the values for easy evaluation by model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(wine_df.drop('quality', axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_feat = scaler.transform(wine_df.drop('quality', axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining new dataframe for scaled features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat = pd.DataFrame(scaler_feat, columns=wine_df.columns[:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX = df_feat\ny = wine_df['quality']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding best possible fit for K value"},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\n\nfor i in range(1,100):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i!=y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nplt.plot(range(1, 100), error_rate, color = 'green', linestyle = 'dashed', \n         marker = 'o', markerfacecolor = 'red', markersize = 10)\nplt.title('Error Rate vs K values')\nplt.xlabel('K values')\nplt.ylabel('Error Rate')\nfor i_x, i_y in zip(range(1,100), error_rate):\n    plt.text(i_x, i_y, '({},{})'.format(round(i_x,2), round(i_y,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_1 = KNeighborsClassifier(n_neighbors= 1)\nknn_1.fit(X_train, y_train)\npred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, r2_score, confusion_matrix\n\nprint(classification_report(y_test, pred))\nprint('\\n')\nprint(r2_score(y_test, pred))\nprint('\\n')\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy seems to be 57% using KNN Classifier"},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_1 = wine_df.drop('quality', axis=1)\ny_1 = wine_df['quality']\n\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size = 0.3, random_state = 101)\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\npara_grid = { 'C':[0.1, 1, 10, 100, 1000, 10000, 100000], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]}\ngrid = GridSearchCV(SVC(), para_grid, verbose = 3)\ngrid.fit(X_train_1, y_train_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_predictions = grid.predict(X_test_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_1, grid_predictions))\nprint('\\n')\nprint(r2_score(y_test_1, grid_predictions))\nprint('\\n')\nprint(confusion_matrix(y_test_1, grid_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy seems to be comparitively better than KNN Classifier"},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_2 = wine_df.drop('quality', axis= 1)\ny_2 = wine_df['quality']\n\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size = 0.3, random_state = 101)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators= 120)\nrfc.fit(X_train_2, y_train_2)\nrfc_pred = rfc.predict(X_test_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_2, rfc_pred))\nprint('\\n')\nprint(r2_score(y_test_2, rfc_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy of Random Forest seems to be better compared to both KNN and SVC"},{"metadata":{},"cell_type":"markdown","source":"**Hope this has helped you! :)\nAnd please do upvote the notebook.**"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}