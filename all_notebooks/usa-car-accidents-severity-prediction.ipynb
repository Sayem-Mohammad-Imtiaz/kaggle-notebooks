{"cells":[{"metadata":{"id":"gMuY_peKPqJx"},"cell_type":"markdown","source":"# USA Car Accidents Severity Prediction\n\nby Jinz Wang\n\nMay 29, 2020 (updated on Feb 22, 2021)\n\n"},{"metadata":{"id":"B6KKV32iPvag"},"cell_type":"markdown","source":"# 0 INTRODUCTION\n\n### Motivation\n\nThe economic and societal impact of traffic accidents cost U.S. citizens hundreds of billions of dollars every year. And a large part of losses is caused by a small number of serious accidents. Reducing traffic accidents, especially serious accidents, is nevertheless always an important challenge. The proactive approach, one of the two main approaches for dealing with traffic safety problems, focuses on preventing potential unsafe road conditions from occurring in the first place. For the effective implementation of this approach, accident prediction and severity prediciton are critical.\nIf we can identify the patterns of how these serious accidents happen and the key factors, we might be able to implement well-informed actions and better allocate financial and human resources. \n\n### Objectives\n\nThe first objective of this project is to recognize **key factors affecting the accident severity**. The second one is to develop a model that can **accurately predict accident severity**. To be specific, for a given accident, without any detailed information about itself, like driver attributes or vehicle type, this model is supposed to be able to predict the likelihood of this accident being a severe one. The accident could be the one that just happened and still lack of detailed information, or a potential one predicted by other models. Therefore, with the sophisticated real-time traffic accident prediction solution developed by the creators of the same dataset used in this project, this model might be able to further predict severe accidents in real-time.\n\n### Process\n\nData cleaning was first performed to detect and handle corrupt or missing records. EDA (Exploratory Data Analysis) and feature engineering were then done over most features. Finally, Logistic regression, Random Forest Classifier, and EasyEnsemble were used to develop the predictive model.   \n\nIt is worth noting that the severity in this project is \"**an indication of the effect the accident has on traffic**\", rather than the injury severity that has already been thoroughly studied by many articles. Another thing is that the final model is dependent on only **a small range of data attributes** that are **easily achievable** for all regions in the United States and before the accident really happened. \n\n### Key Findings\n* Country-wide accident severity can be accurately predicted with limited data attributes (location, time, weather, and POI).\n* **Minute(frequency-encoding)** is the most useful feature. An accident is more likely to be a serious one when accidents happen less frequently at this time.\n* Spatial patterns are also very important. For small areas like **street** and **zipcode**, severe accidents are more likely to happen at places having more accidents while for larger areas like **city** and **airport region**, at places having less accident.\n* **Pressure** is top fourth important feature in the random-forest model and there is negative correlation between pressure and severity.\n* If an accident happens on **Interstate Highway**, there is a 2% chance that it will be a serious one, which is about 2.3 times of average and higher than any other street type. \n* An accident is much less likely to be severe if it happens near **traffic signal** while more likely if near **junction**.\n\n### Dataset Overview\n\nUS-Accident dataset is a countrywide car accident dataset, which covers **49 states of the United States**. It contains more than **4 million cases** of traffic accidents that took place from **February 2016 to December 2020**. In this project, however, only the data of accidents that happened after **February 2019** and were reported by *MapQuest* was finally used in exploration analysis and modeling so that irrelevant factors can be eliminated to the greatest extent.\n\nLink for kaggle dataset: https://www.kaggle.com/sobhanmoosavi/us-accidents\n\n### Acknowledgements\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. “<a href=\"https://arxiv.org/abs/1906.05409\">A Countrywide Traffic Accident Dataset.</a>”, 2019.\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"<a href=\"https://arxiv.org/abs/1909.09638\">Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.</a>\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n\n### Refrences\n\nI found these notebooks really helpful:\n\n<a href=\"https://towardsdatascience.com/usa-accidents-data-analysis-d130843cde02\">USA Accidents Data Analysis</a>\n\nhttps://www.kaggle.com/sobhanmoosavi/us-accidents/discussion/113055\n\n<a href=\"https://www.kaggle.com/deepakdeepu8978/how-severity-the-accidents-is\">how Severity the Accidents is ?</a>\n\n<a href=\"https://www.kaggle.com/trivenisaraswathi/severity-prediction-in-sfo-bay-area\">Severity Prediction in SFO Bay Area</a> \n\n<a href=\"https://www.kaggle.com/phip2014/ml-to-predict-accident-severity-pa-mont\"> ML to Predict Accident Severity_PA_Mont</a>\n\n<a href=\"https://www.kaggle.com/suyash0010/severity-and-time-wasted-analysis\"> severity and hours wasted</a>\n\n<a href=\"https://www.kaggle.com/nikitagrec/usa-accidents-plotly-maps-text-classification\"> USA Accidents Plotly maps + text classification </a>\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Tabel of content\n1. [OVERVIEW & PREPROCESSING](#1) \n    \n    1.1 [Overview](#1.1) \n    \n    1.2 [Reporting Source](#1.2)\n    \n    1.3 [Useless Features](#1.3)\n    \n    1.4 [Clean Up Categorical Features](#1.4)\n    \n    1.5 [Fix Datetime Format](#1.5)\n    \n    <br>    \n2. [HANDLING MISSING DATA](#2)\n\n    2.1 [Drop Features](#2.1)\n    \n    2.2 [Separate Feature](#2.2)\n    \n    2.3 [Drop NaN](#2.3)\n    \n    2.4 [Value Imputation](#2.4)\n    \n    <br>    \n3. [EXPLORATION & ENGINEERING](#3)\n    \n    3.1 [Resampling](#3.1)\n    \n    3.2 [Time Features](#3.2)\n    \n    3.3 [Address Features](#3.3)\n    \n    3.4 [Weather Features](#3.4)\n    \n    3.5 [POI Features](#3.5)\n    \n    3.6 [Correlation](#3.6)\n    \n    3.7 [One-hot Encoding](#3.7)\n    \n    <br>\n4. [MODEL](#4)\n    \n    4.1 [Train Test Split](#4.1)\n    \n    4.2 [Logistic regression with balanced class weights](#4.2)\n    \n    4.3 [Random Forest](#4.3)\n    \n    4.4 [EasyEnsemble](#4.4)  \n    \n    4.5 [BalanceCascade](#4.5)\n    \n    <br>\n5. [FUTURE WORK](#5)"},{"metadata":{"id":"-FDro7QKP3Mi"},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# 1 OVERVIEW & PREPROCESSING"},{"metadata":{"id":"cGX0rWqJeGmx","outputId":"7edd7fb7-31e9-4c36-b595-62017c3a67fd","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport json\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom datetime import datetime\nimport glob\nimport seaborn as sns\nimport re\nimport os\nimport io\nfrom scipy.stats import boxcox","execution_count":null,"outputs":[]},{"metadata":{"id":"Nd9HHY7NodZg"},"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n## 1.1 Overview the dataset\nDetails about features in the dataset:\n\n**Traffic Attributes (12)**:\n\n* **ID**: This is a unique identifier of the accident record.\n\n* **Source**: Indicates source of the accident report (i.e. the API which reported the accident.).\n\n* **TMC**: A traffic accident may have a Traffic Message Channel (TMC) code which provides more detailed description of the event.\n\n* **Severity**: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\n\n* **Start_Time**: Shows start time of the accident in local time zone.\n\n* **End_Time**: Shows end time of the accident in local time zone.\n\n* **Start_Lat**: Shows latitude in GPS coordinate of the start point.\n\n* **Start_Lng**: Shows longitude in GPS coordinate of the start point.\n\n* **End_Lat**: Shows latitude in GPS coordinate of the end point.\n\n* **End_Lng**: Shows longitude in GPS coordinate of the end point.\n\n* **Distance(mi)**: The length of the road extent affected by the accident.\n\n* **Description**: Shows natural language description of the accident.\n\n**Address Attributes (9)**:\n\n* **Number**: Shows the street number in address field.\n\n* **Street**: Shows the street name in address field.\n\n* **Side**: Shows the relative side of the street (Right/Left) in address field.\n\n* **City**: Shows the city in address field.\n\n* **County**: Shows the county in address field.\n\n* **State**: Shows the state in address field.\n\n* **Zipcode**: Shows the zipcode in address field.\n\n* **Country**: Shows the country in address field.\n\n* **Timezone**: Shows timezone based on the location of the accident (eastern, central, etc.).\n\n**Weather Attributes (11)**:\n\n* **Airport_Code**: Denotes an airport-based weather station which is the closest one to location of the accident.\n\n* **Weather_Timestamp**: Shows the time-stamp of weather observation record (in local time).\n\n* **Temperature(F)**: Shows the temperature (in Fahrenheit).\n\n* **Wind_Chill(F)**: Shows the wind chill (in Fahrenheit).\n\n* **Humidity(%)**: Shows the humidity (in percentage).\n\n* **Pressure(in)**: Shows the air pressure (in inches).\n\n* **Visibility(mi)**: Shows visibility (in miles).\n\n* **Wind_Direction**: Shows wind direction.\n\n* **Wind_Speed(mph)**: Shows wind speed (in miles per hour).\n\n* **Precipitation(in)**: Shows precipitation amount in inches, if there is any.\n\n* **Weather_Condition**: Shows the weather condition (rain, snow, thunderstorm, fog, etc.).\n\n**POI Attributes (13)**:\n\n* **Amenity**: A Point-Of-Interest (POI) annotation which indicates presence of amenity in a nearby location.\n\n* **Bump**: A POI annotation which indicates presence of speed bump or hump in a nearby location.\n\n* **Crossing**: A POI annotation which indicates presence of crossing in a nearby location.\n\n* **Give_Way**: A POI annotation which indicates presence of give_way sign in a nearby location.\n\n* **Junction**: A POI annotation which indicates presence of junction in a nearby location.\n\n* **No_Exit**: A POI annotation which indicates presence of no_exit sign in a nearby location.\n\n* **Railway**: A POI annotation which indicates presence of railway in a nearby location.\n\n* **Roundabout**: A POI annotation which indicates presence of roundabout in a nearby location.\n\n* **Station**: A POI annotation which indicates presence of station (bus, train, etc.) in a nearby location.\n\n* **Stop**: A POI annotation which indicates presence of stop sign in a nearby location.\n\n* **Traffic_Calming**: A POI annotation which indicates presence of traffic_calming means in a nearby location.\n\n* **Traffic_Signal**: A POI annotation which indicates presence of traffic_signal in a nearby location.\n\n* **Turning_Loop**: A POI annotation which indicates presence of turning_loop in a nearby location.\n\n**Period-of-Day (4)**:\n\n* **Sunrise_Sunset**: Shows the period of day (i.e. day or night) based on sunrise/sunset.\n\n* **Civil_Twilight**: Shows the period of day (i.e. day or night) based on civil twilight.\n\n* **Nautical_Twilight**: Shows the period of day (i.e. day or night) based on nautical twilight.\n\n* **Astronomical_Twilight**: Shows the period of day (i.e. day or night) based on astronomical twilight."},{"metadata":{"id":"CE3iYqcWfee8","outputId":"023a665d-cc4c-4a98-8a06-03da57a406e3","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/us-accidents/US_Accidents_Dec20.csv')\nprint(\"The shape of data is:\",(df.shape))\ndisplay(df.head(3))","execution_count":null,"outputs":[]},{"metadata":{"id":"mkasxxejZR_f"},"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n## 1.2 Reporting Sources\n    \nThese data came from two sources, *MapQuest* and *Bing*, both of which report severity level but in a different way. Bing has 4 levels while MapQuest has 5. And according to dataset creator, there is no way to do a 1:1 mapping between them. Since severity is what we really care about in this project, I think it is crucial to figure out the difference."},{"metadata":{"id":"DhfBuyDJZlVH","outputId":"af0b5ccf-275f-4e9e-f70b-00640060a0ed","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"df_source = df.groupby(['Severity','Source']).size().reset_index().pivot(\\\n    columns='Severity', index='Source', values=0)\ndf_source.plot(kind='bar', stacked=True, title='Severity Count by Sources')","execution_count":null,"outputs":[]},{"metadata":{"id":"YDnD7CM92pP1"},"cell_type":"markdown","source":"The stacked bar chart shows that two data providers reported totally different proportions of accidents of each level. *MapQuest* reported so rare accidents with severity level 4 which can not even be seen in the plot, whereas *Bing* reported almost the same number of level 4 accidents as level 2. Meanwhile, *MapQuest* reported much more level 3 accidents than *Bing* in terms of proportion. These differences may be due to the different kinds of accidents they tend to collect or the different definitions of severity level, or the combination of them. If the latter is the case, I don't think we can use the data from both of them at the same time. To check it out, we can examine the distribution of accidents with different severity levels across two main measures, **Impacted Distance** and **Duration**.\n"},{"metadata":{"id":"1RW8ZmbJ9o1i","outputId":"d6794e52-62fd-42a8-f163-f757bbe8afba","trusted":true},"cell_type":"code","source":"# fix datetime type\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'])\ndf['End_Time'] = pd.to_datetime(df['End_Time'])\ndf['Weather_Timestamp'] = pd.to_datetime(df['Weather_Timestamp'])\n\n# calculate duration as the difference between end time and start time in minute\ndf['Duration'] = df.End_Time - df.Start_Time \ndf['Duration'] = df['Duration'].apply(lambda x:round(x.total_seconds() / 60) )\nprint(\"The overall mean duration is: \", (round(df['Duration'].mean(),3)), 'min')","execution_count":null,"outputs":[]},{"metadata":{"id":"eIYOiFXzCigC","outputId":"0b047680-9164-4b4b-83df-207c1fb080ee","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\nsns.boxplot(x=\"Severity\", y=\"Duration\",\n            data=df.loc[(df['Source']==\"MapQuest\") & (df['Duration']<400),], palette=\"Set2\", ax=axs[0])\naxs[0].set_title('MapQuest')\nfig.suptitle('Accidents Duration by Severity', fontsize=16)\nsns.boxplot(x=\"Severity\", y=\"Duration\",\n            data=df.loc[(df['Source']==\"Bing\") & (df['Duration']<400),], palette=\"Set2\", ax=axs[1])\naxs[1].set_title('Bing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"jimdbmOEIByk","outputId":"f0a190f0-e30a-493e-eff9-34faa9059b48","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\nsns.boxplot(x=\"Severity\", y=\"Distance(mi)\", \n            data=df.loc[(df['Source']==\"MapQuest\") & (df['Distance(mi)']<5),], palette=\"Set2\", ax=axs[0])\naxs[0].set_title('MapQuest')\nfig.suptitle('Impacted Distance by Severity', fontsize=16)\nsns.boxplot(x=\"Severity\", y=\"Distance(mi)\",\n            data=df.loc[(df['Source']==\"Bing\") & (df['Distance(mi)']<5),], palette=\"Set2\", ax=axs[1])\naxs[1].set_title('Bing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"MK0xcM50oQoX"},"cell_type":"markdown","source":"Two differences are obvious in the above plots. The first is that the overall duration and impacted distance of accidents reported by *Bing* are much longer than those by *MapQuest*. Second, same severity level holds different meanings for *MapQuest* and *Bing*. *MapQuest* seems to have a clear and strict threshold for severity level 4, cases of which nevertheless only account for a tiny part of the whole dataset. *Bing*, on the other hand, doesn't seem to have a clear-cut threshold, especially regards duration, but the data is more balanced. \n\nIt is hard to choose one and we definitely can't use both. I decided to select *MapQuest* because serious accidents are we really care about and the sparse data of such accidents is the reality we have to confront.\n\nFinally, drop data reported from *Bing* and 'Source' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[df['Source']==\"MapQuest\",]\ndf = df.drop(['Source'], axis=1)\nprint(\"The shape of data is:\",(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"id":"fg4ecO7DQO0K"},"cell_type":"markdown","source":"<a id=\"1.3\"></a>    \n## 1.3 Useless Features\nFeatures 'ID' doesn't provide any useful information about accidents themselves. 'TMC', 'Distance(mi)', 'End_Time' (we have start time), 'Duration', 'End_Lat', and 'End_Lng'(we have start location) can be collected only after the accident has already happened and hence cannot be predictors for serious accident prediction. For 'Description', the POI features have already been extracted from it by dataset creators. Let's get rid of these features first.\n\n"},{"metadata":{"id":"LIb_0DTxnWzt","trusted":true},"cell_type":"code","source":"df = df.drop(['ID','TMC','Description','Distance(mi)', 'End_Time', 'Duration', \n              'End_Lat', 'End_Lng'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"G6ZmltRz3tLP"},"cell_type":"markdown","source":"Check out some categorical features."},{"metadata":{"id":"gn8W2FkF2jd7","outputId":"42d78206-89a0-489e-c533-d4574b08813a","trusted":true},"cell_type":"code","source":"cat_names = ['Side', 'Country', 'Timezone', 'Amenity', 'Bump', 'Crossing', \n             'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', \n             'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', \n             'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\nprint(\"Unique count of categorical features:\")\nfor i in cat_names:\n  print(i,df[i].unique().size)","execution_count":null,"outputs":[]},{"metadata":{"id":"PM2Uekhh36uZ"},"cell_type":"markdown","source":"Drop 'Country' and 'Turning_Loop' for they have only one class."},{"metadata":{"id":"48IdU8YS4Nrz","trusted":true},"cell_type":"code","source":"df = df.drop(['Country','Turning_Loop'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ywyh4XCN44BC"},"cell_type":"markdown","source":"<a id=\"1.4\"></a>\n## 1.4 Clean Up Categorical Features\nIf we look at categorical features closely, we will find some chaos in 'Wind_Direction' and 'Weather_Condition'. It is necessary to clean them up first.\n\n### Wind Direction"},{"metadata":{"id":"j0QP6fPKQc7Y","outputId":"cafcd815-6e9a-454c-eca8-da251fac01c8","trusted":true},"cell_type":"code","source":"print(\"Wind Direction: \", df['Wind_Direction'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"2Rtv3KsmG_QE"},"cell_type":"markdown","source":"Simplify wind direction"},{"metadata":{"id":"-LItmME1Qz9P","outputId":"f8d91f10-0c2d-4792-a985-6939cb063ef8","trusted":true},"cell_type":"code","source":"df.loc[df['Wind_Direction']=='Calm','Wind_Direction'] = 'CALM'\ndf.loc[(df['Wind_Direction']=='West')|(df['Wind_Direction']=='WSW')|(df['Wind_Direction']=='WNW'),'Wind_Direction'] = 'W'\ndf.loc[(df['Wind_Direction']=='South')|(df['Wind_Direction']=='SSW')|(df['Wind_Direction']=='SSE'),'Wind_Direction'] = 'S'\ndf.loc[(df['Wind_Direction']=='North')|(df['Wind_Direction']=='NNW')|(df['Wind_Direction']=='NNE'),'Wind_Direction'] = 'N'\ndf.loc[(df['Wind_Direction']=='East')|(df['Wind_Direction']=='ESE')|(df['Wind_Direction']=='ENE'),'Wind_Direction'] = 'E'\ndf.loc[df['Wind_Direction']=='Variable','Wind_Direction'] = 'VAR'\nprint(\"Wind Direction after simplification: \", df['Wind_Direction'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"WS-3z6XjI3bx"},"cell_type":"markdown","source":"### Weather Condition\nWeather-related vehicle accidents kill more people annually than large-scale weather disasters(source: weather.com). According to Road Weather Management Program, most weather-related crashes happen on wet-pavement and during rainfall. Winter-condition and fog are another two main reasons for weather-related accidents. To extract these three weather conditions, we first look at what we have in 'Weather_Condition' Feature.\n"},{"metadata":{"id":"x3A6tH88CFiv","outputId":"fb822847-f6f0-4b6f-cd05-7f1d52c1d5ad","trusted":true},"cell_type":"code","source":"# show distinctive weather conditions \nweather ='!'.join(df['Weather_Condition'].dropna().unique().tolist())\nweather = np.unique(np.array(re.split(\n    \"!|\\s/\\s|\\sand\\s|\\swith\\s|Partly\\s|Mostly\\s|Blowing\\s|Freezing\\s\", weather))).tolist()\nprint(\"Weather Conditions: \", weather)","execution_count":null,"outputs":[]},{"metadata":{"id":"GOM2GlthCFiy"},"cell_type":"markdown","source":"Create features for some common weather conditions and drop 'Weather_Condition' then."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Clear'] = np.where(df['Weather_Condition'].str.contains('Clear', case=False, na = False), True, False)\ndf['Cloud'] = np.where(df['Weather_Condition'].str.contains('Cloud|Overcast', case=False, na = False), True, False)\ndf['Rain'] = np.where(df['Weather_Condition'].str.contains('Rain|storm', case=False, na = False), True, False)\ndf['Heavy_Rain'] = np.where(df['Weather_Condition'].str.contains('Heavy Rain|Rain Shower|Heavy T-Storm|Heavy Thunderstorms', case=False, na = False), True, False)\ndf['Snow'] = np.where(df['Weather_Condition'].str.contains('Snow|Sleet|Ice', case=False, na = False), True, False)\ndf['Heavy_Snow'] = np.where(df['Weather_Condition'].str.contains('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls', case=False, na = False), True, False)\ndf['Fog'] = np.where(df['Weather_Condition'].str.contains('Fog', case=False, na = False), True, False)","execution_count":null,"outputs":[]},{"metadata":{"id":"-5xg-h1oM0ap","trusted":true},"cell_type":"code","source":"# Assign NA to created weather features where 'Weather_Condition' is null.\nweather = ['Clear','Cloud','Rain','Heavy_Rain','Snow','Heavy_Snow','Fog']\nfor i in weather:\n    df.loc[df['Weather_Condition'].isnull(),i] = df.loc[df['Weather_Condition'].isnull(),'Weather_Condition']\n    df[i] = df[i].astype('bool')\n\ndf.loc[:,['Weather_Condition'] + weather]\n\ndf = df.drop(['Weather_Condition'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ByDeO5PHZqMx"},"cell_type":"markdown","source":"<a id=\"1.5\"></a>\n## 1.5 Fix Datetime Format"},{"metadata":{"id":"Kpxt_66DRAyM","outputId":"f0ecd317-a869-461c-de26-f4912efbebe4","trusted":true},"cell_type":"code","source":"# average difference between weather time and start time\nprint(\"Mean difference between 'Start_Time' and 'Weather_Timestamp': \", \n(df.Weather_Timestamp - df.Start_Time).mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"zBn975dDUv4h"},"cell_type":"markdown","source":"Since the 'Weather_Timestamp' is almost as same as 'Start_Time', we can just keep 'Start_Time'. Then map 'Start_Time' to 'Year', 'Month', 'Weekday', 'Day' (in a year), 'Hour', and 'Minute' (in a day)."},{"metadata":{"id":"0W4CubmvSobC","trusted":true},"cell_type":"code","source":"df = df.drop([\"Weather_Timestamp\"], axis=1)\n\ndf['Year'] = df['Start_Time'].dt.year\n\nnmonth = df['Start_Time'].dt.month\ndf['Month'] = nmonth\n\ndf['Weekday']= df['Start_Time'].dt.weekday\n\ndays_each_month = np.cumsum(np.array([0,31,28,31,30,31,30,31,31,30,31,30,31]))\nnday = [days_each_month[arg-1] for arg in nmonth.values]\nnday = nday + df[\"Start_Time\"].dt.day.values\ndf['Day'] = nday\n\ndf['Hour'] = df['Start_Time'].dt.hour\n\ndf['Minute']=df['Hour']*60.0+df[\"Start_Time\"].dt.minute\n\ndf.loc[:4,['Start_Time', 'Year', 'Month', 'Weekday', 'Day', 'Hour', 'Minute']]","execution_count":null,"outputs":[]},{"metadata":{"id":"Cte34NibRp6-"},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# 2 HANDLING MISSING DATA\n<a id=\"2.1\"></a>\n## 2.1 Drop Features\nAs seen from below, many columns have missing values."},{"metadata":{"id":"52X0bnvW3Vtz","outputId":"b2f26b4c-035c-4419-843e-7baa1eade1d4","trusted":true},"cell_type":"code","source":"missing = pd.DataFrame(df.isnull().sum()).reset_index()\nmissing.columns = ['Feature', 'Missing_Percent(%)']\nmissing['Missing_Percent(%)'] = missing['Missing_Percent(%)'].apply(lambda x: x / df.shape[0] * 100)\nmissing.loc[missing['Missing_Percent(%)']>0,:]","execution_count":null,"outputs":[]},{"metadata":{"id":"hjilXcSHRx9d"},"cell_type":"markdown","source":"More than 60% percent of 'Number', 'Wind_Chill(F)', and 'Precipitation(in)' is missing. Drop na and value imputation wouldn't work for these features. 'Number' and 'Wind_Chill(F)' will be dropped because they are not highly related to severity according to previous research, whereas 'Precipitation(in)' could be a useful predictor and hence can be handled by separating feature.\n\nDrop these features:\n\n 1. 'Number'\n\n 2. 'Wind_Chill(F)'\n"},{"metadata":{"id":"YuLuGBWgqfQ4","trusted":true},"cell_type":"code","source":"df = df.drop(['Number','Wind_Chill(F)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"6-3m73DOzwL6"},"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n## 2.2 Separate Featrue\nAdd a new feature for missing values in 'Precipitation(in)' and replace missing values with median.\n"},{"metadata":{"id":"abJWi0CH3lhS","outputId":"9f46085f-ea93-4c70-f167-ff8a026a6ff3","trusted":true},"cell_type":"code","source":"df['Precipitation_NA'] = 0\ndf.loc[df['Precipitation(in)'].isnull(),'Precipitation_NA'] = 1\ndf['Precipitation(in)'] = df['Precipitation(in)'].fillna(df['Precipitation(in)'].median())\ndf.loc[:5,['Precipitation(in)','Precipitation_NA']]","execution_count":null,"outputs":[]},{"metadata":{"id":"jqf6aFT5SBjl"},"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n## 2.3 Drop NaN\nThe counts of missing values in some features are much smaller compared to the total sample. It is convenient to drop rows with missing values in these columns.\n\nDrop NAs by these features:\n\n1. 'City'\n2. 'Zipcode'\n3. 'Airport_Code'\n4. 'Sunrise_Sunset'\n5. 'Civil_Twilight'\n6. 'Nautical_Twilight'\n7. 'Astronomical_Twilight'"},{"metadata":{"id":"NfCAW37CSTEN","trusted":true},"cell_type":"code","source":"df = df.dropna(subset=['City','Zipcode','Airport_Code',\n                       'Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'])","execution_count":null,"outputs":[]},{"metadata":{"id":"oLMGINInpBv-"},"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n## 2.4 Value Imputation\nMost of the rest columns only have small missing part that can be filled. (It is not absolutely necessary though, we can also just drop na)\n### Continuous Weather Data\nContinuous weather features with missing values:\n\n1. Temperature(F)\n\n2. Humidity(%)\n\n3. Pressure(in)\n\n4. Visibility(mi)\n\n5. Wind_Speed(mph)\n\nBefore imputation, weather features will be grouped by location and time first, to which weather is naturally related. 'Airport_Code' is selected as location feature because the sources of weather data are airport-based weather stations. Then the data will be grouped by 'Start_Month' rather than 'Start_Hour' because using the former is computationally cheaper and remains less missing values. Finally, missing values will be replaced by median value of each group. "},{"metadata":{"id":"VeIOMx8UajKg","outputId":"befb401d-af36-4dc3-9c7c-e3e070bb8726","trusted":true},"cell_type":"code","source":"# group data by 'Airport_Code' and 'Start_Month' then fill NAs with median value\nWeather_data=['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\nprint(\"The number of remaining missing values: \")\nfor i in Weather_data:\n  df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(x.median()))\n  print( i + \" : \" + df[i].isnull().sum().astype(str))","execution_count":null,"outputs":[]},{"metadata":{"id":"r0QWSev3WMsG"},"cell_type":"markdown","source":"There still are some missing values but much less. Just dropna by these features for the sake of simplicity."},{"metadata":{"id":"kpkoomnutwJG","trusted":true},"cell_type":"code","source":"df = df.dropna(subset=Weather_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"_jBsE-WDK5rN"},"cell_type":"markdown","source":"### Categorical Weather Features\nFor categorical weather features, majority rather than median will be used to replace missing values."},{"metadata":{"id":"U6fmpxdft1-F","outputId":"9ac9fb53-1152-42e7-a646-e27089eda7df","trusted":true},"cell_type":"code","source":"# group data by 'Airport_Code' and 'Start_Month' then fill NAs with majority value\nfrom collections import Counter\nweather_cat = ['Wind_Direction'] + weather\nprint(\"Count of missing values that will be dropped: \")\nfor i in weather_cat:\n  df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\n  print(i + \" : \" + df[i].isnull().sum().astype(str))\n\n# drop na\ndf = df.dropna(subset=weather_cat)","execution_count":null,"outputs":[]},{"metadata":{"id":"BjzYMh0Nd0fg"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# 3 EXPLORATION & ENGINEERING\n<a id=\"3.1\"></a>\n## 3.1 Resampling\nBased on the exploration we did in 1.2, the accidents with severity level 4 are much more serious than accidents of other levels, between which the division is far from clear-cut. Therefore, I decided to focus on level 4 accidents and regroup the levels of severity into level 4 versus other levels. "},{"metadata":{"id":"FTitVELQR67e","trusted":true},"cell_type":"code","source":"df['Severity4'] = 0\ndf.loc[df['Severity'] == 4, 'Severity4'] = 1\ndf = df.drop(['Severity'], axis = 1)\ndf.Severity4.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"W6hyPUqGWG0t"},"cell_type":"markdown","source":"As seen from above, the data is so unbalanced that we can hardly do exploratory analysis. To address this issue, the combination of over- and under-sampling will be used since the dataset is large enough. level 4 will be randomly oversampled to 50000 and other levels will be randomly undersampled to 50000."},{"metadata":{"trusted":true},"cell_type":"code","source":"def resample(dat, col, n):\n    return pd.concat([dat[dat[col]==1].sample(n, replace = True),\n                   dat[dat[col]==0].sample(n)], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"yVDkjncEldBb","outputId":"f441b06a-6cdc-40a9-fb0e-f5a2a0701786","trusted":true},"cell_type":"code","source":"df_bl = resample(df, 'Severity4', 50000)\nprint('resampled data:', df_bl.Severity4.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"id":"oQB8pBy5ZqS-"},"cell_type":"markdown","source":"Then we can do some exploratoty analysis on resampled data. "},{"metadata":{"id":"0h4xDOnf63Sb"},"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n## 3.2 Time Features\n### Year"},{"metadata":{"id":"B0lzrFtP6zCD","outputId":"b3c76675-9d79-4b38-97e3-770e54423c5f","trusted":true},"cell_type":"code","source":"df_bl.Year = df_bl.Year.astype(str)\nsns.countplot(x='Year', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Year (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ie-Cqv-ra9H2"},"cell_type":"markdown","source":"There must be something wrong. It is impossible that the number of accidents with severity level 4 after 2018 is 5 times more than the number before 2018 while the number of other levels accidents is less. Let's back to raw data to have a look.\n\nI created a heatmap of accidents with severity level 4 from 2016 to 2020, seeing how they actually distributed."},{"metadata":{"id":"5j1fgUuIFz0N","outputId":"849c1dd9-2ede-4de2-d3dc-7426254a3a39","trusted":true},"cell_type":"code","source":"# create a dataframe used to plot heatmap\ndf_date = df.loc[:,['Start_Time','Severity4']]         # create a new dateframe only containing time and severity\ndf_date['date'] = df_date['Start_Time'].dt.normalize() # keep only the date part of start time\ndf_date = df_date.drop(['Start_Time'], axis = 1)\ndf_date = df_date.groupby('date').sum()                # sum the number of accidents with severity level 4 by date\ndf_date = df_date.reset_index().drop_duplicates()\n\n# join the dataframe with full range of date from 2016 to 2020\nfull_date = pd.DataFrame(pd.date_range(start=\"2016-01-02\",end=\"2020-12-31\"))    \ndf_date = full_date.merge(df_date, how = 'left',left_on = 0, right_on = 'date')\ndf_date['date'] = df_date.iloc[:,0]\ndf_date = df_date.fillna(0)\ndf_date = df_date.iloc[:,1:].set_index('date')\n\n# group by date\ngroups = df_date['Severity4'].groupby(pd.Grouper(freq='A'))\nyears = pd.DataFrame()\nfor name, group in groups:\n    if name.year != 2020:\n        years[name.year] = np.append(group.values,0)\n    else:\n        years[name.year] = group.values\n  \n\n# plot\nyears = years.T\nplt.matshow(years, interpolation=None, aspect='auto')\nplt.title('Time Heatmap of Accident with Severity Level 4 (raw data)', y=1.2, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"RjeHj7add-H2"},"cell_type":"markdown","source":"The heatmap indicates that something changed after Feb 2019. Maybe it is the way that *MapQuest* defines severity or the way they collect data. Anyway, we have to narrow down our data again. Since the data after Feb 2019 is less imbalanced and the data in the future is more likely to look like this, dropping the data before Mar 2019 may be the best choice."},{"metadata":{"id":"6lYaeUl1g-TU","trusted":true},"cell_type":"code","source":"df = df.loc[df['Start_Time'] > \"2019-03-10\",:]\ndf = df.drop(['Year', 'Start_Time'], axis=1)\ndf['Severity4'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"n1JLhEaUu-bN"},"cell_type":"markdown","source":"\n### Month\nIt's quite interesting that the count of other levels accidents is mostly consistent from March to December, whereas the number of level 4 accidents rapidly increased from March to May and remained stable until September then increased again from October.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl = resample(df, 'Severity4', 20000)","execution_count":null,"outputs":[]},{"metadata":{"id":"wo3PE-eS7SI6","outputId":"99709021-e8cc-4f17-cded-bd44f2c27c27","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='Month', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Month (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"A2zJRXR-wS00"},"cell_type":"markdown","source":"### Weekday\nThe number of accidents was much less on weekends while the proportion of level 4 accidents was higher."},{"metadata":{"id":"kMd46kKr75Py","outputId":"1aac9d6a-8de2-4ec0-c537-3a7679456023","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='Weekday', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Weedday (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1BHaXShyQ0yA"},"cell_type":"markdown","source":"### Period-of-Day\nAccidents were less during the night but were more likely to be serious."},{"metadata":{"id":"DylnDZzkk2hW","outputId":"8bad28a8-7337-45ff-bc4d-d522fe6586f7","trusted":true},"cell_type":"code","source":"period_features = ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']\nfig, axs = plt.subplots(ncols=1, nrows=4, figsize=(13, 5))\n\nplt.subplots_adjust(wspace = 0.5)\nfor i, feature in enumerate(period_features, 1):    \n    plt.subplot(1, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in\\n{} Feature'.format(feature), size=13, y=1.05)\nfig.suptitle('Count of Accidents by Period-of-Day (resampled data)',y=1.08, fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"yKXFEdpQxMAE"},"cell_type":"markdown","source":"### Hour\nMost accidents happened during the daytime, especially AM peak and PM peak. When it comes to night, accidents were far less but more likely to be serious."},{"metadata":{"id":"-K20SSaw7me1","outputId":"92c1c0a7-c7cb-489a-ae96-5dfd7fdab898","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Hour', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Hour (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"GC8XVUM6GnNf"},"cell_type":"markdown","source":"### Frequence Encoding (Minute)\nAs seen in the plot of 'Hour', 'Minute' may also be an important predictor. But directly using it would produce an overabundance of dummy variables. Therefore, the frequency of 'Minute' was utilized as labels, rather than 'Minute' itself. To normalize the distribution, the frequency was also transformed by log."},{"metadata":{"id":"uKj0MpTW5bQz","outputId":"685c1d2d-226d-4c30-98d5-af318b3b74ea","trusted":true},"cell_type":"code","source":"# frequence encoding and log-transform\ndf['Minute_Freq'] = df.groupby(['Minute'])['Minute'].transform('count')\ndf['Minute_Freq'] = df['Minute_Freq']/df.shape[0]*24*60\ndf['Minute_Freq'] = df['Minute_Freq'].apply(lambda x: np.log(x+1))\n\n# resampling\ndf_bl = resample(df, 'Severity4', 20000)\n\n# plot\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nsns.violinplot(x='Minute_Freq', y=\"Severity4\", data=df_bl, palette=\"Set2\")    \nplt.xlabel('Minute_Fre', size=12, labelpad=3)\nplt.ylabel('Severity4', size=12, labelpad=3)    \nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.title('Minute Frequency by Severity (resampled data)', size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"enLdOZ3rYo-f"},"cell_type":"markdown","source":"The violin plot shows that the overall minute frequency of accidents with severity level 4 is less than other levels. In other words, an accident is more likely to be a serious one when accidents happen less frequently."},{"metadata":{"id":"MsGVra7C8pZS"},"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n## 3.3 Address Features\n### Timezone\nEastern time zone is the most dangeous one."},{"metadata":{"id":"AthEO9l5gex0","outputId":"c551b221-a6eb-4374-93fd-631ba1b96b4b","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,5))\nchart = sns.countplot(x='Timezone', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents by Timezone (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"WYAmIQhdRtz9"},"cell_type":"markdown","source":"### State\nFL, CA, and TX are the top 3 states with the most accidents."},{"metadata":{"id":"SRYW-Nmqz1Iz","outputId":"380343d1-bdc4-4544-b126-47e02fab8655","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nchart = sns.countplot(x='State', hue='Severity4', \n                      data=df_bl ,palette=\"Set2\", order=df_bl['State'].value_counts().index)\nplt.title(\"Count of Accidents in State\\nordered by accidents' count (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"cwwhKU2o33KP"},"cell_type":"markdown","source":"It is a different story if we order the plot by the count of accidents with severity of level 4. FL is still the top one but the next two are GA and VA."},{"metadata":{"id":"j9PICtgp3NRY","outputId":"cb072421-c03f-43e2-e8f1-a440893e78b0","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nchart = sns.countplot(x='State', hue='Severity4', data=df_bl ,palette=\"Set2\", order=df_bl[df_bl['Severity4']==1]['State'].value_counts().index)\nplt.title(\"Count of Accidents in State\\nordered by serious accidents' count (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"0giNeHInR8qd"},"cell_type":"markdown","source":"### County\nThere are too many counties that we cannot visualize them as we did for states. But we do can incorporate census data for them.\n\nSeveral basic variables, like total population, percent of commuters who drive, take transit or walk to work, and median household income, for all counties were downloaded from ACS 5-year estimates 2018. Then, counties' names were isolated. "},{"metadata":{"id":"ulvOGo0gWWSV","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install -q censusdata\nimport censusdata\n\n# download data\ncounty = censusdata.download('acs5', 2018, censusdata.censusgeo([('county', '*')]),\n                                   ['DP05_0001E',  'DP03_0019PE','DP03_0021PE','DP03_0022PE','DP03_0062E'],\n                                   tabletype='profile')\n# rename columns\ncounty.columns = ['Population_County','Drive_County','Transit_County','Walk_County','MedianHouseholdIncome_County']\ncounty = county.reset_index()\n# extract county name and state name\ncounty['County_y'] = county['index'].apply(lambda x : x.name.split(' County')[0].split(',')[0]).str.lower()\ncounty['State_y'] = county['index'].apply(lambda x : x.name.split(':')[0].split(', ')[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\ncounty['State_y'] = county['State_y'].replace(us_state_abbrev)","execution_count":null,"outputs":[]},{"metadata":{"id":"ulP40a2jhohn","outputId":"bca7d4a3-742b-438a-8cf0-9a24ff7b2bd0","trusted":true},"cell_type":"code","source":"county.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"tAL2tqbIjy-x"},"cell_type":"markdown","source":"Counties' names turned out to be very tricky. Converting all of them into lowercase is not enough. Some counties name in USA-accidents omit \"city\" or \"parish\", and hence can't be matched with names in census data. We need to manually put them back and rejoin them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert all county name to lowercase \ndf['County'] = df['County'].str.lower()\n\n# left join df with census data\ndf = df.merge(county, left_on = ['County','State'], right_on=['County_y','State_y'],how = 'left').drop(['County_y','State_y'], axis = 1)\njoin_var = county.columns.to_list()[:-2]\n\n# check how many miss match we got\nprint('Count of missing values before: ', df[join_var].isnull().sum())\n\n# add \"city\" and match again\ndf_city = df[df['Walk_County'].isnull()].drop(join_var, axis=1)\ndf_city['County_city'] = df_city['County'].apply(lambda x : x + ' city')\ndf_city = df_city.merge(county,left_on= ['County_city','State'],right_on = ['County_y','State_y'], how = 'left').drop(['County_city','County_y','State_y'], axis=1)\ndf = pd.concat((df[df['Walk_County'].isnull()==False], df_city), axis=0)\n\n# add \"parish\" and match again\ndf_parish = df[df['Walk_County'].isnull()].drop(join_var, axis=1)\ndf_parish['County_parish'] = df_parish['County'].apply(lambda x : x + ' parish')\ndf_parish = df_parish.merge(county,left_on= ['County_parish','State'],right_on = ['County_y','State_y'], how = 'left').drop(['County_parish','County_y','State_y'], axis=1)\ndf = pd.concat((df[df['Walk_County'].isnull()==False], df_parish), axis=0)\nprint('Count of missing values after: ', df[join_var].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"kjgOh3P0raIN"},"cell_type":"markdown","source":"Drop na and use Logit transformation on some variables having extremly skewed distribution."},{"metadata":{"id":"wQ2-sM4bmZnC","outputId":"8d15bf74-9ac2-490d-b3b6-d6d4b531d520","trusted":true},"cell_type":"code","source":"# drop na\ndf = df.drop('index', axis = 1).dropna()\n\n# log-transform\nfor i in ['Population_County','Transit_County','Walk_County']:\n    df[i + '_log'] = df[i].apply(lambda x: np.log(x+1))\ndf = df.drop(['Population_County','Transit_County','Walk_County'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = resample(df, 'Severity4', 20000)\n\n# plot\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\ncensus_features = ['Population_County_log','Drive_County','Transit_County_log','Walk_County_log','MedianHouseholdIncome_County']\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(census_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{}'.format(feature), size=16, y=1.05)\nfig.suptitle('Density of Accidents in Census Data (resampled data)', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"WxDxBNdzeIMK"},"cell_type":"markdown","source":"Percent of people taking transit to commute seems to related to severity. Level 4 accidents happened more frequently in those counties with a lower usage rate of transit."},{"metadata":{"id":"C6xGWgUb29TQ"},"cell_type":"markdown","source":"### Street\nThere are more and more studies found that higher speed limits were associated with an increased likelihood of crashes and deaths. (https://www.cga.ct.gov/2013/rpt/2013-R-0074.htm) And speed limits are highly related to street type. Street type hence can be a good predictor of serious accidents. There is no feature about street type in the original dataset though, we can extract it from the street name. \n\nThe top 40 most common words in street names were selected. This list contains not only street types but also some common words widely used in street names."},{"metadata":{"id":"OAB1njC3oLRg","outputId":"94a0f9fc-1dc9-44b0-d4ad-58b34eb474b0","trusted":true},"cell_type":"code","source":"# create a list of top 40 most common words in street name\nst_type =' '.join(df['Street'].unique().tolist()) # flat the array of street name\nst_type = re.split(\" |-\", st_type) # split the long string by space and hyphen\nst_type = [x[0] for x in Counter(st_type).most_common(40)] # select the 40 most common words\nprint('the 40 most common words')\nprint(*st_type, sep = \", \") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove some irrelevant words and add spaces and hyphen back"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove some irrelevant words and add spaces and hyphen back\nst_type= [' Rd', ' St', ' Dr', ' Ave', ' Blvd', ' Ln', ' Highway', ' Pkwy', ' Hwy', \n          ' Way', ' Ct', 'Pl', ' Road', 'US-', 'Creek', ' Cir',  'Route', \n          'I-', 'Trl', 'Pike', ' Fwy']\nprint(*st_type, sep = \", \")  ","execution_count":null,"outputs":[]},{"metadata":{"id":"3-V2Fl8KhJOX"},"cell_type":"markdown","source":"Create a dummy variable for each word in the list and plot the correlation between these key words and severity. "},{"metadata":{"id":"vwyKBYPBtxxC","outputId":"8bc728b9-3f87-4247-b487-5e31471bcb6c","trusted":true},"cell_type":"code","source":"# for each word create a boolean column\nfor i in st_type:\n  df[i.strip()] = np.where(df['Street'].str.contains(i, case=True, na = False), True, False)\ndf.loc[df['Road']==1,'Rd'] = True\ndf.loc[df['Highway']==1,'Hwy'] = True\n\n# resample again\ndf_bl = resample(df, 'Severity4', 20000)\n\n# plot correlation\ndf_bl['Severity4'] = df_bl['Severity4'].astype(int)\nstreet_corr  = df_bl.loc[:,['Severity4']+[x.strip() for x in st_type]].corr()\nplt.figure(figsize=(20,15))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(street_corr, annot=True, cmap=cmap, center=0).set_title(\"Correlation (resampled data)\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZX1pFj6w72RR"},"cell_type":"markdown","source":"Interstate Highway turns out to be the most dangerous street. Other roads like basic road, street, drive, and avenue are relatively safe. Let's just keep these five features.\n"},{"metadata":{"id":"ky0vP12s0vhN","trusted":true},"cell_type":"code","source":"drop_list = street_corr.index[street_corr['Severity4'].abs()<0.1].to_list()\ndf = df.drop(drop_list, axis=1)\n\n# resample again\ndf_bl = resample(df, 'Severity4', 20000)","execution_count":null,"outputs":[]},{"metadata":{"id":"E-g3oLbvi_xB"},"cell_type":"markdown","source":"### Side\nRight side of the line is much more dangerous than left side."},{"metadata":{"id":"D6M3mOEunmol","outputId":"65a7895d-403b-4e32-bed0-13e56fe44e44","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nchart = sns.countplot(x='Side', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents by Side (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"vGeS2-xohXyv"},"cell_type":"markdown","source":"### Latitude and Longitude"},{"metadata":{"outputId":"4bc9ff3e-b6c0-408f-b713-1aba84132cc3","id":"zJm_JjwNL2LD","trusted":true},"cell_type":"code","source":"df_bl['Severity4'] = df_bl['Severity4'].astype('category')\nnum_features = ['Start_Lat', 'Start_Lng']\nfig, axs = plt.subplots(ncols=1, nrows=2, figsize=(10, 5))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(1, 2, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Feature'.format(feature), size=14, y=1.05)\nfig.suptitle('Distribution of Accidents by Latitude and Longitude\\n(resampled data)', fontsize=18,y=1.08)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"P1uG9VEA40SG","outputId":"2b8c5556-f7c9-49fb-b91f-ef3562f9ed6a","trusted":true},"cell_type":"code","source":"df_4 = df[df['Severity4']==1]\n\nplt.figure(figsize=(15,10))\n\nplt.plot( 'Start_Lng', 'Start_Lat', data=df, linestyle='', marker='o', markersize=1.5, color=\"teal\", alpha=0.2, label='All Accidents')\nplt.plot( 'Start_Lng', 'Start_Lat', data=df_4, linestyle='', marker='o', markersize=3, color=\"coral\", alpha=0.5, label='Accidents with Serverity Level 4')\nplt.legend(markerscale=8)\nplt.xlabel('Longitude', size=12, labelpad=3)\nplt.ylabel('Latitude', size=12, labelpad=3)\nplt.title('Map of Accidents', size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"L2VeGRLpNKFj"},"cell_type":"markdown","source":"### Frequency Encoding\nSimilar to 'Minute', some location features like 'City' and 'Zipcode' that have too many unique values can be labeled by their frequency. \nFrequency encoding and log-transform:\n1. 'Street'\n2. 'City'\n3. 'County'\n4. 'Zipcode'\n5. 'Airport_Code'"},{"metadata":{"id":"KAGDyEtnNKFm","trusted":true},"cell_type":"code","source":"fre_list = ['Street', 'City', 'County', 'Zipcode', 'Airport_Code','State']\nfor i in fre_list:\n  newname = i + '_Freq'\n  df[newname] = df.groupby([i])[i].transform('count')\n  df[newname] = df[newname]/df.shape[0]*df[i].unique().size\n  df[newname] = df[newname].apply(lambda x: np.log(x+1))","execution_count":null,"outputs":[]},{"metadata":{"id":"PEgYHZ1hT-qS","outputId":"edf3b769-c577-4cd0-92e1-2212a0bf81b6","trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = resample(df, 'Severity4', 20000)\n\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(10, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfig.suptitle('Location Frequency by Severity (resampled data)', fontsize=16)\nfor i, feature in enumerate(fre_list, 1): \n    feature = feature + '_Freq'   \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity4', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{}'.format(feature), size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YOzbXDXoqA-X"},"cell_type":"markdown","source":"Two opposite patterns can be identified in these plots. For 'Street' and 'Zipcode', higher frequency means higher likelihood of being a serious accident. In contrast with these smaller regions, for 'City' and 'Airport_Code' instead, higher frequency means less likelihood of being a serious accident.\nGet rid of features we don't need anymore."},{"metadata":{"id":"lxN5CdzbeSOM","trusted":true},"cell_type":"code","source":"df = df.drop(fre_list, axis  = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Dj2qNgYChWeV"},"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n## 3.4 Weather Features\n### Continuous Weather Features\nNormalize features with extreamly skewed distribution first."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pressure_bc']= boxcox(df['Pressure(in)'].apply(lambda x: x+1),lmbda=6)\ndf['Visibility_bc']= boxcox(df['Visibility(mi)'].apply(lambda x: x+1),lmbda = 0.1)\ndf['Wind_Speed_bc']= boxcox(df['Wind_Speed(mph)'].apply(lambda x: x+1),lmbda=-0.2)\ndf = df.drop(['Pressure(in)','Visibility(mi)','Wind_Speed(mph)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"outputId":"d1275664-8b04-47a2-b8ce-c1ed1fb5673b","id":"1RZ5406FLsiO","trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = resample(df, 'Severity4', 20000)\n\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nnum_features = ['Temperature(F)', 'Humidity(%)', 'Pressure_bc', 'Visibility_bc', 'Wind_Speed_bc']\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Feature by Severity'.format(feature), size=14, y=1.05)\nfig.suptitle('Density of Accidents by Weather Features (resampled data)', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"6xLsmH1WkWGf"},"cell_type":"markdown","source":"### Weather Conditions"},{"metadata":{"id":"q_WCDw0Hhi5z","outputId":"b1f3f1d1-ddfb-40e4-b699-835779717791","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.6)\nfor i, feature in enumerate(weather, 1):    \n    plt.subplot(2, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in \\n {} Feature'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents by Weather Features (resampled data)', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"6vUvGy06ukEa"},"cell_type":"markdown","source":"As seen from above, accidents are little more likely to be serious during rain or snow while less likely on a cloudy day. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Heavy_Rain','Heavy_Snow','Fog'], axis  = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"DUjTCMoDkbYJ"},"cell_type":"markdown","source":"### Wind Direction"},{"metadata":{"id":"XBp-XXbCkLIx","outputId":"ecd8e703-6af5-4b50-815b-fef3b2bca1aa","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nchart = sns.countplot(x='Wind_Direction', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents in Wind Direction (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Wind_Direction'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Qx0FIRMQ8Ot6"},"cell_type":"markdown","source":"<a id=\"3.5\"></a>\n## 3.5 POI Features"},{"metadata":{"id":"0XaRPpf9uMd8","outputId":"23fb12cd-7340-41c2-a222-bcfb2f598773","trusted":true},"cell_type":"code","source":"POI_features = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal']\n\nfig, axs = plt.subplots(ncols=3, nrows=4, figsize=(15, 10))\n\nplt.subplots_adjust(hspace=0.5,wspace = 0.5)\nfor i, feature in enumerate(POI_features, 1):    \n    plt.subplot(3, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in {}'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents in POI Features (resampled data)',y=1.02, fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"nsgFVwAVxFwc"},"cell_type":"markdown","source":"Accidents near traffic signal and crossing are much less likely to be serious accidents while little more likely to be serious if they are near the junction. Maybe it is because people usually slow down in front of crossing and traffic signal but junction and severity are highly related to speed. Other POI features are so unbalanced that it is hard to tell their relation with severity from plots.\n\nDrop some features:\n\n1. 'Bump'\n2. 'Give_Way'\n3. 'No_Exit'\n4. 'Roundabout'\n5. 'Traffic_Calming'"},{"metadata":{"id":"NDbhqbFI3sF_","trusted":true},"cell_type":"code","source":"df= df.drop(['Amenity','Bump','Give_Way','No_Exit','Roundabout','Traffic_Calming'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"r5620A5QdxG3"},"cell_type":"markdown","source":"<a id=\"3.6\"></a>\n## 3.6 Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding\ndf[period_features] = df[period_features].astype('category')\ndf = pd.get_dummies(df, columns=period_features, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"FdqM-0STlezN","outputId":"bc4caac4-4d86-46fa-e37b-30b394e4133a","trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = resample(df, 'Severity4', 20000)\n\n# plot correlation\ndf_bl['Severity4'] = df_bl['Severity4'].astype(int)\nplt.figure(figsize=(25,25))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(df_bl.corr(), annot=True,cmap=cmap, center=0).set_title(\"Correlation Heatmap\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"A93f9cwgBRv-","trusted":true},"cell_type":"code","source":"df = df.drop(['Temperature(F)', 'Humidity(%)', 'Precipitation(in)', 'Precipitation_NA','Visibility_bc', 'Wind_Speed_bc',\n              'Clear','Cloud','Snow','Crossing','Junction','Railway','Month',\n              'Hour', 'Day','Minute','MedianHouseholdIncome_County', 'Transit_County_log', \n              'Walk_County_log','Drive_County', 'City_Freq','County_Freq','Airport_Code_Freq','Zipcode_Freq',\n              'Sunrise_Sunset_Night', 'Civil_Twilight_Night', 'Nautical_Twilight_Night'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = resample(df, 'Severity4', 20000)\n\n# plot correlation\ndf_bl['Severity4'] = df_bl['Severity4'].astype(int)\nplt.figure(figsize=(20,20))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(df_bl.corr(), annot=True,cmap=cmap, center=0).set_title(\"Correlation Heatmap\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"eqH6gMcPfQJ3"},"cell_type":"markdown","source":"<a id=\"3.7\"></a>\n## 3.7 One-hot Encoding \nOne-hot encode categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace([True, False], [1,0])\n\ncat = ['Side','Timezone','Weekday']\ndf[cat] = df[cat].astype('category')\ndf = pd.get_dummies(df, columns=cat, drop_first=True)\n\ndf_int = df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\ndf_float = df.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='float')\ndf = pd.concat([df.select_dtypes(include=['uint8']),df_int,df_float],axis=1)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"l2Fl3HBwZDSa"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# 4 Model\n\nImbalance ratio of this dataset is about 100, which is the key problem we need to deal with. There are several ways to handle it: \n1. **under-sampling** (I didn't use over-sampling because this dataset is large enough and over-sampling is very likely to casue overfitting)\n2. **modify the loss function**\n3. **ensemble methods**\n    * EasyEnsemble\n    * BalanceCascade\n\n\n**References**:\n\n*X. Y. Liu, J. Wu and Z. H. Zhou, \"Exploratory Undersampling forClass-Imbalance Learning,\" in IEEE Transactions on Systems, Man, andCybernetics, Part B (Cybernetics), vol. 39, no. 2, pp. 539-550,April 2009.*\n   \n[*Ajinkya More | Resampling techniques and other strategies*](https://www.youtube.com/watch?v=-Z1PaqYKC1w)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_predict\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n## 4.1 Train Test Split\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split X, y\nX = df.drop('Severity4', axis=1)\ny= df['Severity4']\n\n# split train, test\nX_train, X_test, y_train, y_test = train_test_split(\\\n  X, y, test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n## 4.2 Logistic regression with balanced class weights\nunder-sampling + modify the loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Randomly undersample majority class to about 10 times of minority class\nrus = RandomUnderSampler(sampling_strategy = 0.1, random_state=42)\nX_train_res, y_train_res = rus.fit_sample(X_train, y_train)\nprint (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\nprint (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_base = LogisticRegression()\ngrid = {'C': 10.0 ** np.arange(-2, 3),\n        'penalty': ['l1', 'l2'],\n        'class_weight': ['balanced']}\nclf_lr = GridSearchCV(clf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n\nclf_lr.fit(X_train_res, y_train_res)\n\ncoef = clf_lr.best_estimator_.coef_\nintercept = clf_lr.best_estimator_.intercept_\nprint (classification_report(y_test, clf_lr.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"4.3\"></a>\n ## 4.3 Random Forest\n under-sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_base = RandomForestClassifier()\ngrid = {'n_estimators': [10, 50, 100],\n        'max_features': ['auto','sqrt']}\nclf_rf = GridSearchCV(clf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n\nclf_rf.fit(X_train_res, y_train_res)\ny_pred = clf_rf.predict(X_test)\n\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nconf_matrix = pd.DataFrame(data=confmat,\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix \\n Random Forest\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try a different ratio."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Randomly undersample majority class to about 20 times of minority class\nrus = RandomUnderSampler(sampling_strategy = 0.05, random_state=42)\nX_train_res, y_train_res = rus.fit_sample(X_train, y_train)\nprint (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\nprint (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_base = RandomForestClassifier()\ngrid = {'n_estimators': [10, 50, 100],\n        'max_features': ['auto','sqrt']}\nclf_rf = GridSearchCV(clf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n\nclf_rf.fit(X_train_res, y_train_res)\ny_pred = clf_rf.predict(X_test)\n\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More data doesn't lead to better result."},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = pd.DataFrame(np.zeros((X_train_res.shape[1], 1)), columns=['importance'], index=df.drop('Severity4',axis=1).columns)\n\nimportances.iloc[:,0] = clf_rf.best_estimator_.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportances30 = importances.head(30)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='importance', y=importances30.index, data=importances30)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Random Forest Classifier Feature Importance', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature importance plot shows that high-resolution spatio-temporal patterns of accidents are the most useful features to predict severity. Apart from that, pressure, population, road type are also critical."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.4\"></a>\n## 4.4 EASYENSEMBLE\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# n folds random under-sampling\ndef multi_rus(X, y, n_folds, ratio):\n    X_res = [None] * n_folds\n    y_res = [None] * n_folds\n    rus = RandomUnderSampler(sampling_strategy = ratio, random_state=42)\n    for i in range(n_folds):\n        X_res[i], y_res[i] = rus.fit_sample(X, y)\n        \n    return X_res, y_res       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_res, y_train_res = multi_rus(X_train, y_train, 3, 0.1)\ny_pred_proba = np.zeros(len(y_test))\nfor i in range(len(y_train_res)):\n    clf = RandomForestClassifier(n_estimators=100, max_features='auto')\n    clf.fit(X_train_res[i], y_train_res[i])\n    y_pred_proba += clf.predict(X_test)\n    \ny_pred_proba = y_pred_proba/len(y_train_res)\ny_pred = (y_pred_proba > 0.5).astype(int)\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_res, y_train_res = multi_rus(X_train, y_train, 9, 0.2)\ny_pred_proba = np.zeros(len(y_test))\nfor i in range(len(y_train_res)):\n    clf = RandomForestClassifier(n_estimators=100, max_features='auto')\n    clf.fit(X_train_res[i], y_train_res[i])\n    y_pred_proba += clf.predict(X_test)\n    \ny_pred_proba = y_pred_proba/len(y_train_res)\ny_pred = (y_pred_proba > 0.5).astype(int)\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_res, y_train_res = multi_rus(X_train, y_train, 3, 0.1)\ny_pred_proba = np.zeros(len(y_test))\nfor i in range(len(y_train_res)):\n    clf_base = AdaBoostClassifier()\n    grid = {'n_estimators': [10, 50, 100]}\n    \n    clf = GridSearchCV(clf_base, grid, cv=3, n_jobs=8, scoring='f1_macro')\n    clf.fit(X_train_res[i], y_train_res[i])\n    y_pred_proba += clf.predict(X_test)\n    \ny_pred_proba = y_pred_proba/len(y_train_res)\ny_pred = (y_pred_proba > 0.5).astype(int)\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EasyEnsemble didn't improve the result very much. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4.5\"></a>\n## 4.5 BalanceCascade"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n#          Christos Aridas\n# License: MIT\n\ndef BalanceCascadeSample(X, \n                         y,\n                         estimator=AdaBoostClassifier(),\n                         random_state = 42,\n                         n_max_subset = 10\n                        ):\n    \"\"\"Resample the dataset.\n\n    Parameters\n    ----------\n    estimator : object, optional (default=AdaBoostClassifier())\n        An estimator inherited from :class:`sklearn.base.ClassifierMixin` and\n        having an attribute :func:`predict_proba`.\n        \n    X : ndarray, shape (n_samples, n_features)\n        Matrix containing the data which have to be sampled.\n\n    y : ndarray, shape (n_samples, )\n        Corresponding label for each sample in X.\n        \n    random_state : int, RandomState instance or None, optional (default=42)\n        If int, ``random_state`` is the seed used by the random number\n        generator; If ``RandomState`` instance, random_state is the random\n        number generator; If ``None``, the random number generator is the\n        ``RandomState`` instance used by ``np.random``.\n\n    n_max_subset : int or None, optional (default=10)\n        Maximum number of subsets to generate. By default, all data from\n        the training will be selected that could lead to a large number of\n        subsets. We can probably deduce this number empirically.\n        \n    Returns\n    -------\n    X_resampled : ndarray, shape (n_subset, n_samples_new, n_features)\n        The array containing the resampled data.\n\n    y_resampled : ndarray, shape (n_subset, n_samples_new)\n        The corresponding label of `X_resampled`\n\n    idx_under : ndarray, shape (n_subset, n_samples, )\n        If `return_indices` is `True`, a boolean array will be returned\n        containing the which samples have been selected.\n\n    \"\"\"\n    # array to know which samples are available to be taken\n    samples_mask = np.ones(y.shape, dtype=bool)\n\n    # where the different set will be stored\n    X_resampled = []\n    y_resampled = []\n    idx_under = []\n\n    n_subsets = 0\n    b_subset_search = True\n    while b_subset_search:\n        target_stats = Counter(y[samples_mask])\n        # build the data set to be classified\n        X_subset = np.empty((0, X.shape[1]), dtype=X.dtype)\n        y_subset = np.empty((0, ), dtype=y.dtype)\n        # store the index of the data to under-sample\n        index_under_sample = np.empty((0, ), dtype=y.dtype)\n        # value which will be picked at each round\n        X_constant = np.empty((0, X.shape[1]), dtype=X.dtype)\n        y_constant = np.empty((0, ), dtype=y.dtype)\n        index_constant = np.empty((0, ), dtype=y.dtype)\n        for target_class in target_stats.keys():            \n            X_constant = np.concatenate((X_constant,\n                                         X[y == target_class]),\n                                        axis=0)\n            y_constant = np.concatenate((y_constant,\n                                        y[y == target_class]),\n                                        axis=0)\n            index_constant = np.concatenate(\n                (index_constant,\n                 np.flatnonzero(y == target_class)),\n                axis=0)\n\n        # store the set created\n        n_subsets += 1\n        X_resampled.append(np.concatenate((X_subset, X_constant),\n                                          axis=0))\n        y_resampled.append(np.concatenate((y_subset, y_constant),\n                                          axis=0))\n        idx_under.append(np.concatenate((index_under_sample,\n                                         index_constant),\n                                        axis=0))\n\n        # fit and predict using cross validation\n        pred = cross_val_predict(estimator,\n                                 np.concatenate((X_subset, X_constant),\n                                                axis=0),\n                                 np.concatenate((y_subset, y_constant),\n                                                axis=0))\n        # extract the prediction about the targeted classes only\n        pred_target = pred[:y_subset.size]\n        index_classified = index_under_sample[pred_target == y_subset]\n        samples_mask[index_classified] = False\n\n        # check the stopping criterion\n        if n_subsets == n_max_subset:\n            b_subset_search = False\n\n    return np.array(X_resampled), np.array(y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rus = RandomUnderSampler(sampling_strategy = 0.1, random_state=42)\nX_train_res, y_train_res = rus.fit_sample(X_train, y_train)\nX_train_res, y_train_res = BalanceCascadeSample(X = X_train_res.to_numpy(), \n                                                y = y_train_res.to_numpy(),\n                                                estimator=RandomForestClassifier(n_estimators=100, max_features='auto'),\n                                                n_max_subset = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba = np.zeros(len(y_test))\nfor i in range(len(y_train_res)):\n    clf = RandomForestClassifier(n_estimators=100, max_features='auto')\n    clf.fit(X_train_res[i], y_train_res[i])\n    y_pred_proba += clf.predict(X_test)\n    \ny_pred_proba = y_pred_proba/len(y_train_res)\ny_pred = (y_pred_proba > 0.5).astype(int)\nprint (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar result as EasyEnsemble. "},{"metadata":{"trusted":true},"cell_type":"code","source":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nconf_matrix = pd.DataFrame(data=confmat,\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix \\n Random Forest\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"R-rC-g9Bwrp8"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# 5 Future Work\n1. Find a better way to handle class imbalance.\n2. Incorporate this model in a real-time accident risk prediction model or develop a new real-time severe accident risk prediction on grid cells.\n3. Detailed relations between some key factors and accident severity can be further studied.\n4. Policy implications of this project can be explored.  \n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}