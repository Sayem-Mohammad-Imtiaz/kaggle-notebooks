{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IPIP Scale** measures the big five five personality traits: extraversion, agreeableness, openness, conscientiousness, and neuroticism.\nEach trait represents a continuum. Individuals can fall anywhere on the continuum for that trait.\n<br> Scale contains 10 questions for each factor/trait. ","metadata":{}},{"cell_type":"code","source":"# reading the dataset\ndf = pd.read_csv('../input/big-five-personality-test/IPIP-FFM-data-8Nov2018/data-final.csv', sep='\\t')\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br> IPC: The number of records from the user's IP address in the dataset. \nFor maximum cleanliness, it is suggested using only the records where IPC value 1. ","metadata":{}},{"cell_type":"code","source":"df.IPC.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep the records where the IPC column equals to 1.\ndf = df.loc[df[\"IPC\"]==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to check whether it keeps correct number of records. \ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting only the answers of IPIP scale. So, we drop the other columns. ","metadata":{}},{"cell_type":"code","source":"df.drop(df.columns[50:], axis=1, inplace=True)\n# alternatively:\n# df = df.iloc[:, 0:50]\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing values","metadata":{}},{"cell_type":"code","source":"print(df.isnull().any().sum())\ndf.isnull().sum().sort_values(ascending = False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing data is under 1%. So, we delete the cases which has missing values.  ","metadata":{}},{"cell_type":"code","source":"# deleting the missing values\ndf.dropna(inplace = True)\nprint(df.shape)\nprint(df.isnull().sum())  # to check if we delete all cases with missing values.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reverse items[](http://)\nIPIP Scale contains reverse items, such as EXT2 \"I dont talk a lot\" in Extraversion scale. Giving high points of these question should be related with low level of extraversion. \n<br> Reverse items will be re-encoded. ","metadata":{}},{"cell_type":"code","source":"# to check whether the reverse items re-coded correctly - 1\ndf[[\"EXT2\",\"EXT4\", \"EST2\", \"AGR1\", \"CSN8\",\"OPN6\"]].head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#re-encoding reverse items\ndf.EXT2 = 6 - df.EXT2.values\ndf.EXT4 = 6 - df.EXT4.values\ndf.EXT6 = 6 - df.EXT6.values\ndf.EXT8 = 6 - df.EXT8.values\ndf.EXT10 = 6 - df.EXT10.values\ndf.EST2 = 6 - df.EST2.values\ndf.EST4 = 6 - df.EST4.values\ndf.AGR1 = 6 - df.AGR1.values\ndf.AGR3 = 6 - df.AGR3.values\ndf.AGR5 = 6 - df.AGR5.values\ndf.AGR7 = 6 - df.AGR7.values\ndf.CSN2 = 6 - df.CSN2.values\ndf.CSN4 = 6 - df.CSN4.values\ndf.CSN6 = 6 - df.CSN6.values\ndf.CSN8 = 6 - df.CSN8.values\ndf.OPN2 = 6 - df.OPN2.values\ndf.OPN4 = 6 - df.OPN4.values\ndf.OPN6 = 6 - df.OPN6.values\n\n# alternatively : \n#df['EXT2'] = df['EXT2'].map({1:5, 2:4, 3:3, 4:2, 5:1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to check whether the reverse items re-coded correctly - 2\ndf[[\"EXT2\",\"EXT4\", \"EST2\", \"AGR1\", \"CSN8\",\"OPN6\"]].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cluster analysis - K-means","metadata":{}},{"cell_type":"markdown","source":"### Standardization \nStandardization is the process of rescaling the values of the variables refers to the process of rescaling the values of the variables in your data set so they are on the same scale.In this scaling technique the values are centered around the mean with a unit standard deviation. \nIn cluster analysis, standardization may be crucial if the variables have a different unit or where the scales of each of variables are very different from one another (e.g., 0-1 vs 0-1000). In this dataset all questions are on the same scale. So, we dont make any standardization to the questions. \n","metadata":{}},{"cell_type":"markdown","source":"### Deciding the number of clusters\n\nThe elbow method is used to find the optimal value for clusters(k).\n\nFor more information about KElbowVisualizer:\nhttps://www.scikit-yb.org/en/latest/api/cluster/elbow.html#:~:text=The%20elbow%20method%20runs%20k,point%20to%20its%20assigned%20center.","metadata":{}},{"cell_type":"code","source":"df_sample = df[0:5000]\n\n# Visualize the elbow\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nkmeans = KMeans()\nvisualizer = KElbowVisualizer(kmeans, k=(2,15))\nvisualizer.fit(df_sample)\nvisualizer.poof()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up k-means\nk_means = KMeans(n_clusters = 5)\n\n#define 5 clusters and fit the model\nk_fit = k_means.fit(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting the Clusters\npd.options.display.max_columns = 10\npredictions = k_fit.labels_\ndf['Clusters'] = predictions\nprint(df.head())\ndf[\"Clusters\"].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T02:12:33.344219Z","iopub.execute_input":"2021-06-22T02:12:33.344789Z","iopub.status.idle":"2021-06-22T02:12:33.377043Z","shell.execute_reply.started":"2021-06-22T02:12:33.344754Z","shell.execute_reply":"2021-06-22T02:12:33.375497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Total scale scores","metadata":{}},{"cell_type":"code","source":"# calculating total scale score\n\ndf[\"extraversion\"] = 0\ndf[\"neuroticism\"] = 0\ndf[\"agreeableness\"] = 0\ndf[\"conscientiousness\"] = 0\ndf[\"openness\"] = 0\ndf[\"extraversion\"]= (df.EXT1 + df.EXT2 + df.EXT3 + df.EXT4 + df.EXT5 + df.EXT6 + df.EXT7 + df.EXT8 + df.EXT9 + df.EXT10)/10\ndf[\"neuroticism\"] = (df.EST1 + df.EST2 + df.EST3 + df.EST4 + df.EST5 + df.EST6 + df.EST7 + df.EST8 + df.EST9 + df.EST10)/10\ndf[\"agreeableness\"] = (df.AGR1 + df.AGR2 + df.AGR3 + df.AGR4 + df.AGR5 + df.AGR6 + df.AGR7 + df.AGR8 + df.AGR9 + df.AGR10)/10\ndf[\"conscientiousness\"] = (df.CSN1 + df.CSN2 + df.CSN3 + df.CSN4 + df.CSN5 + df.CSN6 + df.CSN7 + df.CSN8 + df.CSN9 + df.CSN10)/10\ndf[\"openness\"] = (df.OPN1 + df.OPN2 + df.OPN3 + df.OPN4 + df.OPN5 + df.OPN6 + df.OPN7 + df.OPN8 + df.OPN9 + df.OPN10)/10\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of the total scores\ndf[[\"extraversion\", \"neuroticism\", \"agreeableness\", \"conscientiousness\", \"openness\"]].describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of total scale scores within the clusters","metadata":{}},{"cell_type":"code","source":"table = df.groupby('Clusters')[\"extraversion\", \"neuroticism\", \"agreeableness\", \"conscientiousness\", \"openness\"].mean()\nprint(table)\n\ntable.plot(figsize=(14,9), kind=\"bar\", colormap='Paired')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Standardization of the total scores to increase the readability of the visualization","metadata":{}},{"cell_type":"code","source":"df_total_scores = df[[\"extraversion\", \"neuroticism\", \"agreeableness\", \"conscientiousness\", \"openness\"]]\nprint(df_total_scores.head())\nprint(df_total_scores.mean(axis=0))\ndf_total_scores = df_total_scores.apply(lambda x: (x-x.mean())/x.std(), axis = 0)\nprint(round(df_total_scores.std()))\nprint(round(df_total_scores.mean(axis=0)))\nprint(df_total_scores.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_total_scores[\"clusters\"] = df[\"Clusters\"]\ntable = df_total_scores.groupby('clusters')[\"extraversion\", \"neuroticism\", \"agreeableness\", \"conscientiousness\", \"openness\"].mean()\nprint(table)\n\ntable.plot(figsize=(14,9), kind=\"bar\", colormap='Paired')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://)Each total score has a mean of 0. \n+1 means that total score of that trait is one standard deviation above the mean, while -1 means one standard deviation below the mean.  ","metadata":{}}]}