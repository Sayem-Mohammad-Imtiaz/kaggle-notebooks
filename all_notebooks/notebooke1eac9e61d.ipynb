{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-22T15:42:09.167525Z","iopub.execute_input":"2021-08-22T15:42:09.168211Z","iopub.status.idle":"2021-08-22T15:42:09.188445Z","shell.execute_reply.started":"2021-08-22T15:42:09.168121Z","shell.execute_reply":"2021-08-22T15:42:09.187358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pywt\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy import stats \nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom keras.utils import np_utils","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:45:50.364397Z","iopub.execute_input":"2021-08-22T15:45:50.364846Z","iopub.status.idle":"2021-08-22T15:45:50.371098Z","shell.execute_reply.started":"2021-08-22T15:45:50.364809Z","shell.execute_reply":"2021-08-22T15:45:50.370271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler= StandardScaler()\ndef data_preprocess_train(X):\n    X_prep=scaler.fit_transform(X)\n    #do here your preprocessing\n    return X_prep\n\ndef feature_normalize(dataset):\n    mu = np.mean(dataset, axis=0)\n    sigma = np.std(dataset, axis=0)\n    return (dataset - mu)/sigma\n \ndef show_confusion_matrix(validations, predictions):\n\n    matrix = metrics.confusion_matrix(validations, predictions)\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(matrix,\n                cmap=\"coolwarm\",\n                linecolor='white',\n                linewidths=1,\n                xticklabels=LABELS,\n                yticklabels=LABELS,\n                annot=True,\n                fmt=\"d\")\n    plt.title(\"Confusion Matrix\")\n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.show()\n    return\n\ndef show_basic_dataframe_info(dataframe,\n                              preview_rows=20):\n\n    \"\"\"\n    This function shows basic information for the given dataframe\n\n    Args:\n        dataframe: A Pandas DataFrame expected to contain data\n        preview_rows: An integer value of how many rows to preview\n\n    Returns:\n        Nothing\n    \"\"\"\n\n    # Shape and how many rows and columns\n    print(\"Number of columns in the dataframe: %i\" % (dataframe.shape[1]))\n    print(\"Number of rows in the dataframe: %i\\n\" % (dataframe.shape[0]))\n    print(\"First 20 rows of the dataframe:\\n\")\n    # Show first 20 rows\n    print(dataframe.head(preview_rows))\n    print(\"\\nDescription of dataframe:\\n\")\n    # Describe dataset like mean, min, max, etc.\n    # print(dataframe.describe())\n\n\ndef read_data(file_path):\n\n    \"\"\"\n    This function reads the EEG data from a file\n\n    Args:\n        file_path: URL pointing to the CSV file\n\n    Returns:\n        A pandas dataframe\n    \"\"\"\n\n    column_names = ['code',\n                    'id-user',\n                    'time-stamp',\n                    'sample',\n                    'canal1',\n                    'canal2',\n                    'canal3',\n                    'canal4',\n                    'canal5',\n                    'canal6',\n                    'canal7',\n                    'canal8',\n                    'canal9',\n                    'canal10',\n                    'canal11',\n                    'canal12',\n                    'canal13',\n                    'canal14',\n                    'canal15',\n                    'canal16',\n                    'canal17',\n                    'canal18',\n                    'canal19',\n                    'canal20',\n                    'canal21',\n                    'canal22',  \n                    'canal23',\n                    'canal24',\n                    'canal25',\n                    'canal26',\n                    'canal27',\n                    'canal28',\n                    'canal29',\n                    'canal30',\n                    'canal31',\n                    'canal32',\n                    'canal33',\n                    'canal34',\n                    'canal35',\n                    'canal36',\n                    'canal37',\n                    'canal38',\n                    'canal39',\n                    'canal40',\n                    'canal41',\n                    'canal42'              \n                    ]\n    df = pd.read_csv(file_path,\n                     header=None,\n                     names=column_names)\n    return df\n\ndef shuffle_in_unison_scary(a, b):\n    rng_state = np.random.get_state()\n    np.random.shuffle(a)\n    np.random.set_state(rng_state)\n    np.random.shuffle(b)\ndef create_segments_and_labels(df, time_steps, step, label_name):\n\n    \"\"\"\n    This function receives a dataframe and returns the reshaped segments\n    of eeg canals as well as the corresponding labels\n\n    Args:\n        df: Dataframe in the expected format\n        time_steps: Integer value of the length of a segment that is created\n    Returns:\n        reshaped_segments\n        labels:\n    \"\"\"\n\n   \n    global N_CANALS\n    # Number of steps to advance in each iteration (for me, it should always\n    # be equal to the time_steps in order to have no overlap between segments)\n    # step = time_steps\n    \n    segments = []\n    labels = []\n    i=0\n    #for i in range(0, len(df) - time_steps, step):\n    while i<len(df) - time_steps:\n        oneSegment = []    \n        for j in range(1,N_CANALS+1):\n             \n            xx = df['canal'+str(j)].values[i: i + time_steps]\n            oneSegment.append(xx)\n           \n        arr=np.asarray(oneSegment,dtype= np.float32)\n        arr=data_preprocess_train(arr)\n        arr=feature_normalize(arr)\n        segments.append(arr)\n        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n        labels.append(label)\n        i=i+step\n     \n    shuffle_in_unison_scary(segments,labels)\n    reshaped_segments = np.asarray(segments).reshape(-1, time_steps, N_CANALS)\n    labels = np.asarray(labels)\n    return reshaped_segments, labels","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:45:58.82389Z","iopub.execute_input":"2021-08-22T15:45:58.824396Z","iopub.status.idle":"2021-08-22T15:45:58.846009Z","shell.execute_reply.started":"2021-08-22T15:45:58.824363Z","shell.execute_reply":"2021-08-22T15:45:58.8452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABELS = [\"1\",\n          \"2\",\n          \"3\",\n          \"4\",\n          \"5\"]\n\n\nN_CANALS = 42\n# The number of steps within one time segment\nTIME_PERIODS = 750 #3 seunds per 250 samples after stimulation\n\nSTEP_DISTANCE = 750","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:46:08.675476Z","iopub.execute_input":"2021-08-22T15:46:08.675975Z","iopub.status.idle":"2021-08-22T15:46:08.67993Z","shell.execute_reply.started":"2021-08-22T15:46:08.675945Z","shell.execute_reply":"2021-08-22T15:46:08.678885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = read_data('/kaggle/input/eeg-from-motor-cortex/dataKeras.csv')\n\n# Describe the data\nshow_basic_dataframe_info(df, N_CANALS)\n\n# Define column name of the label vector\n\n\n# %%\n\nprint(\"\\n--- Reshape the data into segments ---\\n\")\n\n# Differentiate between test set and training set\ndel df['time-stamp']\ndel df['sample']","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:46:13.007628Z","iopub.execute_input":"2021-08-22T15:46:13.008126Z","iopub.status.idle":"2021-08-22T15:46:16.110369Z","shell.execute_reply.started":"2021-08-22T15:46:13.008096Z","shell.execute_reply":"2021-08-22T15:46:16.109529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABEL = \"code\"\nle = preprocessing.LabelEncoder()\n# Add a new column to the existing DataFrame with the encoded values\ndf[LABEL] = le.fit_transform(df[\"code\"].values.ravel())\ndf_test = df[df['id-user'] > 7]\ndf_train = df[df['id-user'] <= 7]\n\nx_train, y_train = create_segments_and_labels(df_train,\n                                              TIME_PERIODS,\n                                              STEP_DISTANCE,\n                                              LABEL)\nprint('x_train shape: ', x_train.shape)\n\nprint(x_train.shape[0], 'training samples')\n\n\n# Inspect y data\nprint('y_train shape: ', y_train.shape)\n\nnum_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\nprint(\"num_sensors \"+str(num_sensors))\nnum_classes = le.classes_.size\nprint(\"The OpenVibe stimulationes code, from http://openvibe.inria.fr/stimulation-codes/\")\nprint(list(le.classes_))\n\ny_train = np_utils.to_categorical(y_train, num_classes)\nprint('New y_train shape: ', y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:47:00.261892Z","iopub.execute_input":"2021-08-22T15:47:00.262251Z","iopub.status.idle":"2021-08-22T15:47:01.289263Z","shell.execute_reply.started":"2021-08-22T15:47:00.26222Z","shell.execute_reply":"2021-08-22T15:47:01.28825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D,GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D,AveragePooling1D\nfrom keras.layers import  BatchNormalization,  Flatten, ReLU, LSTM \nfrom keras.utils import np_utils\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler\n\nseed = 3421\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\nmodel_m = Sequential()\n#model_m.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)))\nmodel_m.add(Conv1D(64,7, activation='relu',strides = 3, input_shape=(TIME_PERIODS, num_sensors)))\nmodel_m.add(Conv1D(64,7, activation='relu'))\n#model_m.add(MaxPooling1D(3))\n\nmodel_m.add(BatchNormalization())\nmodel_m.add(Dropout(0.2))\nmodel_m.add(Conv1D(128, 5,strides = 2, activation='relu'))\nmodel_m.add(Conv1D(128, 5, activation='relu'))\n#model_m.add(MaxPooling1D(3))\nmodel_m.add(BatchNormalization())\nmodel_m.add(Dropout(0.2))\nmodel_m.add(Conv1D(256, 3, activation='relu'))\nmodel_m.add(Conv1D(256, 3, activation='relu'))\n#model_m.add(GlobalMaxPooling1D())\n#model_m.add(GlobalAveragePooling1D())\n#model_m.add(MaxPooling1D(3))\n\nmodel_m.add(BatchNormalization())\nmodel_m.add(Dropout(0.2))\nmodel_m.add(Flatten())\nmodel_m.add(BatchNormalization())\n#model_m.add(Dropout(0.2))\n#model_m.add(Dense(42, activation = \"softmax\"))\n#model_m.add(BatchNormalization())\n#model_m.add(Dropout(0.2))\nmodel_m.add(Dense(num_classes, activation='softmax'))\n#model_m.add(Dense(num_classes, activation = \"sigmoid\"))\nmodel_m.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:48:50.310928Z","iopub.execute_input":"2021-08-22T15:48:50.311303Z","iopub.status.idle":"2021-08-22T15:48:50.763398Z","shell.execute_reply.started":"2021-08-22T15:48:50.311273Z","shell.execute_reply":"2021-08-22T15:48:50.762267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='best_model.{epoch:02d}-{loss:.2f}.h5',\n        monitor='loss', save_best_only=True),\n    keras.callbacks.EarlyStopping(monitor='acc', patience=10)\n]\nadam = Adam(learning_rate = 0.0001)\nmodel_m.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:48:57.237827Z","iopub.execute_input":"2021-08-22T15:48:57.23817Z","iopub.status.idle":"2021-08-22T15:48:57.255198Z","shell.execute_reply.started":"2021-08-22T15:48:57.238141Z","shell.execute_reply":"2021-08-22T15:48:57.254319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 100\nEPOCHS =50\n\nhistory = model_m.fit(x_train,\n                      y_train,\n                      batch_size=BATCH_SIZE,\n                      epochs=EPOCHS,\n                      callbacks=callbacks_list,\n                      validation_split=0.2,\n                      verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:49:37.757918Z","iopub.execute_input":"2021-08-22T15:49:37.758267Z","iopub.status.idle":"2021-08-22T15:51:35.216301Z","shell.execute_reply.started":"2021-08-22T15:49:37.758237Z","shell.execute_reply":"2021-08-22T15:51:35.215575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n--- Learning curve of model training ---\\n\")\n\n# summarize history for accuracy and loss\n\nplt.figure(figsize=(6, 4))\nplt.plot(history.history['acc'], \"g--\", label=\"Accuracy of training data\")\nplt.plot(history.history['val_acc'], \"g\", label=\"Accuracy of validation data\")\nplt.plot(history.history['loss'], \"r--\", label=\"Loss of training data\")\nplt.plot(history.history['val_loss'], \"r\", label=\"Loss of validation data\")\nplt.title('Model Accuracy and Loss')\nplt.ylabel('Accuracy and Loss')\nplt.xlabel('Training Epoch')\nplt.ylim(0)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:52:02.984098Z","iopub.execute_input":"2021-08-22T15:52:02.984502Z","iopub.status.idle":"2021-08-22T15:52:03.217896Z","shell.execute_reply.started":"2021-08-22T15:52:02.984442Z","shell.execute_reply":"2021-08-22T15:52:03.216869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\nfor i in range(1,N_CANALS+1):\n    df_test['canal'+str(i)] = feature_normalize(df_test['canal'+str(i)])\n\nx_test, y_test = create_segments_and_labels(df_test,\n                                            TIME_PERIODS,\n                                            STEP_DISTANCE,\n                                            LABEL)\n\nx_test = x_test.astype(\"float32\")\ny_test = y_test.astype(\"float32\")\n\ny_test = np_utils.to_categorical(y_test, num_classes)\n\nscore = model_m.evaluate(x_test, y_test, verbose=1)\n\nprint(\"\\nAccuracy on test data: %0.2f\" % score[1])\nprint(\"\\nLoss on test data: %0.2f\" % score[0])\n\n# %%\n\nprint(\"\\n--- Confusion matrix for test data ---\\n\")\n\ny_pred_test = model_m.predict(x_test)\n# Take the class with the highest probability from the test predictions\nmax_y_pred_test = np.argmax(y_pred_test, axis=1)\nmax_y_test = np.argmax(y_test, axis=1)\n\nshow_confusion_matrix(max_y_test, max_y_pred_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T15:52:45.306537Z","iopub.execute_input":"2021-08-22T15:52:45.306937Z","iopub.status.idle":"2021-08-22T15:52:46.530799Z","shell.execute_reply.started":"2021-08-22T15:52:45.306902Z","shell.execute_reply":"2021-08-22T15:52:46.529578Z"},"trusted":true},"execution_count":null,"outputs":[]}]}