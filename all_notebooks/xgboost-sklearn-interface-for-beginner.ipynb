{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Don't forget to upvote📈 if you like👍🏻 it","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"XGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. Note that a weak learner is one which is slightly better than random guessing. For example, a decision tree whose predictions are slightly better than 50%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n# Import Necessary Libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mlcourse/telecom_churn.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will simply number the States, and make the international plan (international roaming), Voice mail plan (voice mail) and target Churn attributes binary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"state_enc = LabelEncoder()\ndf['State'] = state_enc.fit_transform(df['State'])\ndf['International plan'] = (df['International plan'] == 'Yes').astype('int')\ndf['Voice mail plan'] = (df['Voice mail plan'] == 'Yes').astype('int')\ndf['Churn'] = (df['Churn']).astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Divide the data into training and test samples with a ratio of 7: 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Churn', axis=1), df['Churn'],test_size=0.3, stratify=df['Churn'], random_state=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Initializing parameters**\n\nbinary classification ('objective': 'binary:logistic')\nlimiting the depth of trees ('max_depth':3)\nwe don't want extra output ('silent':1)\nwe will perform 10 iterations of boosting\nthe gradient descent step is quite large ('eta':1) - the algorithm will learn quickly and \"aggressively\" (better results will be obtained if you reduce the eta and increase the number of iterations)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'binary:logistic',\n    'max_depth': 3,\n    'learning_rate': 1.0,\n    'n_estimators': 50\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training of the classifier**\n\nТут мы просто передаем слоавть параметров, данные и число итераций.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBClassifier(**params).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Forecasts for the test sample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_prob = xgb_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's calculate the percentage of correct algorithm responses in the test sample.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_labels = preds_prob > 0.5\nprint(\"Accuracy and F1 on the test set are: {:.2} and {:.2}\".format(\n    round(accuracy_score(y_test, predicted_labels), 3),\n    round(f1_score(y_test, predicted_labels), 3)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}