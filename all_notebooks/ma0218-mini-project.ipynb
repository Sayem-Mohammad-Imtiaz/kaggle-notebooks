{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MA0218 Mini Project \n---\n#### Selected Dataset 4: `Aviation Accident Database`\n#### Problem Statement: `Predicting the Probability of Fatality in Aviation Accident/Incident using classification modelling.`\n#### Designed By: `MA8 - Group AngKuKueh`\n> Ang Jun Jie U1822901A  \n> Lewis Lee U1820229F  \n> Ong Jun Yu U1920988L  \n> Tan AIk Lim Philip U1821641K  \n> Tay Song Heng Denzil U1823710F  ","metadata":{}},{"cell_type":"markdown","source":"---\n# Data Preparation","metadata":{}},{"cell_type":"code","source":"#Import the standard libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import csv file\naviationData = pd.read_csv(\"../input/dataset/AviationData.csv\", encoding = 'iso-8859-1')\n\npd.set_option('display.max_columns',31)\naviationData.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aviationData.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check for significance of the null values\n\nfor i in aviationData.columns:\n    data_missing = np.mean(aviationData[i].isnull())\n    print('{} - {}% , {}'.format(i, round(data_missing*100), aviationData[i].isna().sum()))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Considering the significance of null values and our initial judgement on the importance of each variable, we proceed to clean the data with the following variables of interest:\n`Event.Date`, `Latitude`, `Longtitude`, `Injury Severity`, `Aircraft Damage`, `Make`, `Amateur Built`, `Number of Engines`, `Engine Type`, `Purpose of Flight`, `Weather Condition`, `Broad Phase of Flight`, `Report Status`","metadata":{}},{"cell_type":"markdown","source":"---\n# Data Cleaning","metadata":{}},{"cell_type":"code","source":"#Understand the breakdown of the variables before cleaning\nvariables = ['Event.Date',\n            'Latitude',\n            'Longitude',\n            'Injury.Severity',\n            'Aircraft.Damage',\n            'Make',\n            'Amateur.Built',\n            'Number.of.Engines',\n            'Engine.Type',\n            'Purpose.of.Flight',\n            'Weather.Condition',\n            'Broad.Phase.of.Flight',\n            'Report.Status']\n\nfor x in variables:\n    print(x, ': ', aviationData[x].value_counts(), '\\n')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Event.date\n#Split the Event date dataset into month, day, year datasets\naviationData['Month'] = aviationData['Event.Date'].str.split('/', expand=True)[0]\naviationData['Day'] = aviationData['Event.Date'].str.split('/', expand=True)[1]\naviationData['Year'] = aviationData['Event.Date'].str.split('/', expand=True)[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Latitude and longtitude\n#Since around 64% of the data is missing, it does not make sense to fill\n#in the data. Filling in the null values with the mean or median of Latitude and longtitude\n#does not make sense. Hence we will be dropping the null values in the later analysis.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Injury.Severity\n#Categorise Injurity Severity dataset into Fatal and Non-Fatal\n#For incident and unavailable data, assumed as Non-Fatal\naviationData['Fatal'] = aviationData['Injury.Severity'].apply(lambda x: 'No' \n                                                              if x=='Non-Fatal' \n                                                              or x== 'Incident' \n                                                              or x=='Unavailable' \n                                                              else 'Yes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Aircraft.Damage\n#Fill the null values of Aircraft.Damage with the most common recurring value\naviationData['Aircraft.Damage'].fillna(aviationData['Aircraft.Damage'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make\n#Standardise Make to uppercase letters (to remove complications)\naviationData['Make'] = aviationData['Make'].str.upper()\n\n#Fill null values with 'Others'\naviationData['Make'].fillna('Others', inplace = True)\n\n#Group those Make with insignificant sample size(<1% of total) with 'Others'\nmake_others = aviationData[\"Make\"].value_counts()<850\naviationData[\"Make\"] = aviationData[\"Make\"].apply(lambda x: 'Others' if make_others.loc[x]==True else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Amateur.Built\n#Fill null values of Amateur.Built with the most common recurring data\naviationData['Amateur.Built'].fillna(aviationData['Amateur.Built'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number.of.Engines\n#Fill null values of Number.of.Engines with the most common recurring data\naviationData['Number.of.Engines'].fillna(aviationData['Number.of.Engines'].mode()[0], inplace=True)\n\n#Convert data type of Number.of.Engines to int64 for Exploratory Analysis\naviationData['Number.of.Engines'] = aviationData['Number.of.Engines'].astype('int64')\n\n#Simplify dataset by representing 3 or more engines as 3\naviationData['Number.of.Engines'] = aviationData['Number.of.Engines'].replace(4, 3)\naviationData['Number.of.Engines'] = aviationData['Number.of.Engines'].replace(8, 3)\naviationData['Number.of.Engines'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Engine.Type\n#Fill null values of Engine.Type with the most common recurring data\naviationData['Engine.Type'].fillna(aviationData['Engine.Type'].mode()[0], inplace=True)\n\n#Group those engine type with insignificant sample size(<1% of total) as 'Others'\ntype_others = aviationData['Engine.Type'].value_counts()<850\naviationData['Engine.Type'] = aviationData['Engine.Type'].apply(lambda x: 'Others' if type_others.loc[x]==True else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Purpose.of.Flight\n#Fill null values of Purpose.of.Flight as 'Unknown'\naviationData['Purpose.of.Flight'].fillna('Unknown', inplace=True)\n\n#Group purpose of flight with those of similar purpose\n#Public Aircraft\naviationData['Purpose.of.Flight'] = aviationData['Purpose.of.Flight'].replace('Public Aircraft - Local', 'Public Aircraft')\naviationData['Purpose.of.Flight'] = aviationData['Purpose.of.Flight'].replace('Public Aircraft - Federal', 'Public Aircraft')\naviationData['Purpose.of.Flight'] = aviationData['Purpose.of.Flight'].replace('Public Aircraft - State', 'Public Aircraft')\n\n#Work use\naviationData['Purpose.of.Flight'] = aviationData['Purpose.of.Flight'].replace('Executive/Corporate', 'Work Use')\naviationData['Purpose.of.Flight'] = aviationData['Purpose.of.Flight'].replace('Other Work Use', 'Work Use')\naviationData['Purpose.of.Flight'] = aviationData['Purpose.of.Flight'].replace('Business', 'Work Use')\n\n#Group those purpose of flight with insignificant sample size(<1% of total) as 'Others'\npurpose_others = aviationData['Purpose.of.Flight'].value_counts()<850\naviationData['Purpose.of.Flight'] = aviationData['Purpose.of.Flight'].apply(lambda x: 'Others' if purpose_others.loc[x]==True else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Weather.Condition\n#Fill null values of Weather.Condition with the most common recurring data\naviationData['Weather.Condition'].fillna(aviationData['Weather.Condition'].mode()[0], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Broad.Phase.of.Flight\n#Fill null values of Broad.Phase.of.Flight as 'UNKNOWN'\naviationData['Broad.Phase.of.Flight'].fillna('UNKNOWN', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Report.Status\n#No steps requred for cleaning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Breakdown of variables after cleaning\nfor x in variables:\n    print(x, ': ', aviationData[x].value_counts(), '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rearrange the order of columns\n#Exclude 'Day' as it is unnecessary in analysis\n#Exclude Latitude and Longtitude as they will be put in a separate dataframe/analysis\nclean_data = pd.DataFrame(aviationData[['Year',\n                                        'Month',\n                                        'Aircraft.Damage',\n                                        'Make',\n                                        'Amateur.Built',\n                                        'Number.of.Engines',\n                                        'Engine.Type',\n                                        'Purpose.of.Flight',\n                                        'Weather.Condition',\n                                        'Broad.Phase.of.Flight',\n                                        'Report.Status',\n                                        'Fatal']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Latitude and Longtitude dataframe\nLatLong_data = pd.DataFrame(aviationData[['Latitude', 'Longitude', 'Fatal']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check for null values(if any)\nfor i in clean_data.columns:\n    data_missing = np.mean(clean_data[i].isnull())\n    print('{} - {}% , {}'.format(i, round(data_missing*100), clean_data[i].isna().sum()))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_data.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### _Data Cleaning completed_ \n#### Datasets to be used for Data Exploration and Analysis:\n`clean_data` and `LatLong_Data` ","metadata":{}},{"cell_type":"markdown","source":"---\n# Data Exploration and Analysis","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF ACCIDENTS OVER THE YEARS","metadata":{}},{"cell_type":"code","source":"#Arrange by years(ascending)\nclean_data = clean_data.sort_values(by = 'Year', ascending = True)\n\n#Count plot of number of accidents every year\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nax = sb.countplot(x = 'Year', data = clean_data)\nax.axes.set_title(\"Total Accidents Each Year\",fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### YEARLY FATALITY COUNT","metadata":{}},{"cell_type":"code","source":"#Count plot of Fatal & Non-Fatal every year\nf, axes = plt.subplots(1, 1, figsize=(30, 10))\nax = sb.countplot(x = 'Year', hue = 'Fatal', data = clean_data, palette = 'Set1')\nax.axes.set_title(\"Fatal:Non Fatal Ratio Over the Years\",fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### YEARLY PERCENTAGE OF FATALITY COUNT","metadata":{}},{"cell_type":"code","source":"#Percentage of fatality over the years\ndataset = pd.DataFrame(clean_data.groupby('Year')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Year'] = dataset.index\nfatal_count = []\nfor yr in dataset['Year']:\n    data1 = clean_data.loc[clean_data['Year']== yr]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(45)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Year', data = dataset, orient = 'h')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot, we observe that the number of accidents reported has been decreasing over the years, however this is largely due to the number of non-fatal accidents decreasing as the number of fatal accidents still remain approximately the same. We can also observe that the fatality rate from 1948-1979 is very high, this is an anomaly as very few accidents were reported during that period as the aviation industry was still developing.","metadata":{}},{"cell_type":"markdown","source":"#### TOTAL NUMBER OF ACCIDENTS SORTED BY MONTH","metadata":{}},{"cell_type":"code","source":"#We will be diving deeper into the time series and look at possible pattern in a year\n#Arrange the months(ascending)\n\nclean_data['Month'] = clean_data['Month'].replace(['1'],'Jan')\nclean_data['Month'] = clean_data['Month'].replace(['2'],'Feb')\nclean_data['Month'] = clean_data['Month'].replace(['3'],'Mar')\nclean_data['Month'] = clean_data['Month'].replace(['4'],'Apr')\nclean_data['Month'] = clean_data['Month'].replace(['5'],'May')\nclean_data['Month'] = clean_data['Month'].replace(['6'],'Jun')\nclean_data['Month'] = clean_data['Month'].replace(['7'],'Jul')\nclean_data['Month'] = clean_data['Month'].replace(['8'],'Aug')\nclean_data['Month'] = clean_data['Month'].replace(['9'],'Sep')\nclean_data['Month'] = clean_data['Month'].replace(['10'],'Oct')\nclean_data['Month'] = clean_data['Month'].replace(['11'],'Nov')\nclean_data['Month'] = clean_data['Month'].replace(['12'],'Dec')\n\n#Count plot of accidents over the months\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nax = sb.countplot(x = 'Month', data = clean_data, order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                                                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nax.axes.set_title(\"Total Accidents Each Month\",fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### FATILITY RATES SORTED BY MONTH","metadata":{}},{"cell_type":"code","source":"#Count plot of accidents over the months\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nax = sb.countplot(x = 'Month', data = clean_data, hue = 'Fatal', order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                                                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], palette = 'Set1')\nax.axes.set_title(\"Fatal:Non Fatal Ratio Each Month\",fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### FATALITY PERCENTAGE SORTED BY MONTH","metadata":{}},{"cell_type":"code","source":"dataset = pd.DataFrame(clean_data.groupby('Month')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Month'] = dataset.index\nfatal_count = []\nfor mon in dataset['Month']:\n    data1 = clean_data.loc[clean_data['Month']== mon]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(12)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Month', data = dataset, order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                                                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is interesting to note that the majority of the accidents occur in the months of Jun/Jul, yet they have the lowest percentage of fatality.\n\nIt is also true for the months where the number of accidents are lower such as the first few months (Jan-June) and last few (July-Dec), yet they have a high percentage of fatality.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF AIRCRAFT DAMAGE","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(1, 1, figsize=(24, 10))\nax = sb.countplot(x = 'Aircraft.Damage', hue = 'Fatal', data = clean_data, palette = 'Set1')\nax.axes.set_title(\"Fatal:Non Fatal Ratio of Air Craft Damage\",fontsize=20)\n\n#Show Percentages of the Total Count\nAircraftDamage_data = clean_data['Aircraft.Damage']\ntotal = float(len(AircraftDamage_data))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2., height + 100, '{:1.2f}'.format(height*100/total),\n            ha=\"center\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot, we observe that most aircraft suffer substantial aircraft damage after an accident. In a case whereby the aircraft is destroyed, the rate of fatality is higher. Logically, a destroyed aircraft results in higher rate of death.\n\nHowever, there is also high fatal rate when aircraft suffer minor damage, yet a low fatal rate when aircraft suffer substantial damage. This is counter intuitive as we would expect higher rate of death in the case of higher damage to an aircraft. Nevertheless, this might be a biased statistic due to the significant difference in the count.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF MAKE","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(1, 1, figsize=(24, 10))\nax = sb.countplot(x = 'Make', hue = 'Fatal', data = clean_data, palette = 'Set1')\n\n#Show Percentages of the Total Count\nmake_data = clean_data['Make']\ntotal = float(len(make_data))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2., height + 100, '{:1.2f}'.format(height*100/total),\n            ha=\"center\")\n    \n#percentage fatality of Make\ndataset = pd.DataFrame(clean_data.groupby('Make')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Make'] = dataset.index\nfatal_count = []\nfor i in dataset['Make']:\n    data1 = clean_data.loc[clean_data['Make']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(11)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Make', data = dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we observe that `CESSNA` has the highest count of accident, with one of the lowest fatality percentage. `BOEING` has an exceptionally low count of fatality percentage while `BEECH`, `MOONEY` and `ROBINSON` has the highest fatality percentages. However, accident count for `BOEING`, `BEECH`, `MOONEY` and `ROBINSON` are the lowest and the statistic might be biased given the smaller sample count. It is still worthy to note that `BOEING` has the low accident and fatality rate.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF AMATEUR BUILT","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(1, 1, figsize=(24, 10))\nax = sb.countplot(x = 'Amateur.Built', hue = 'Fatal', data = clean_data, palette = 'Set1')\n\n#Percentage fatality of amateur built\ndataset = pd.DataFrame(clean_data.groupby('Amateur.Built')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Amateur.Built'] = dataset.index\nfatal_count = []\nfor i in dataset['Amateur.Built']:\n    data1 = clean_data.loc[clean_data['Amateur.Built']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(2)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Amateur.Built', data = dataset, palette = 'RdBu_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we observe that `Non-amateur built` aircraft have higher counts of accidents which may be due to a significantly higher number and frequency of `Non-amateur built` aircraft flown. `Amateur built` aircraft has a higher fatality rate which may be reasoned with their purpose of build and flight, yet can also be a biased statistic due to the significant difference in sample size. ","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF NUMBER OF ENGINES","metadata":{}},{"cell_type":"code","source":"#countplot of number of engines\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.countplot(x = 'Number.of.Engines', hue = 'Fatal', data = clean_data, palette = 'Set1')\n\n#Percentage fatality of Number of engines\ndataset = pd.DataFrame(clean_data.groupby('Number.of.Engines')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Number.of.Engines'] = dataset.index\nfatal_count = []\nfor i in dataset['Number.of.Engines']:\n    data1 = clean_data.loc[clean_data['Number.of.Engines']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(4)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Number.of.Engines', data = dataset, orient = 'h', palette = 'RdBu_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we observe that aircrafts with `1 engine` has the highest accident count. However, this may be because `1 engine` is the most common in aircrafts. Another observation is that as the `number of engines` increases from `0 to 2`, the fatality rate increases as well. However this observation may be biased due to significant difference in sample counts.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF ENGINE TYPE","metadata":{}},{"cell_type":"code","source":"#countplot of engine types\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.countplot(x = 'Engine.Type', hue = 'Fatal', data = clean_data, palette = 'Set1')\n\n#Percentage fatality of Engine type\ndataset = pd.DataFrame(clean_data.groupby('Engine.Type')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Engine.Type'] = dataset.index\nfatal_count = []\nfor i in dataset['Engine.Type']:\n    data1 = clean_data.loc[clean_data['Engine.Type']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(6)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Engine.Type', data = dataset, palette = 'RdBu_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we observe that aircrafts with `Reciprocating` engine type has the highest accident count. However, this may be because `Reciprocating` engine type is the most common in aircrafts. `Turbo Fan` has an exceptionally low fatality rate while `Turbo Prop` has the highest. Nevertheless, this might be biased statistic due to the small sample count for `Turbo Fan` and `Turbo Prop`.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF PURPOSE OF FLIGHT","metadata":{}},{"cell_type":"code","source":"#countplot of purpose of flight\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.countplot(x = 'Purpose.of.Flight', hue = 'Fatal', data = clean_data, palette = 'Set1')\n\n#Percentage fatality of Purpose of Flight\ndataset = pd.DataFrame(clean_data.groupby('Purpose.of.Flight')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Purpose.of.Flight'] = dataset.index\nfatal_count = []\nfor i in dataset['Purpose.of.Flight']:\n    data1 = clean_data.loc[clean_data['Purpose.of.Flight']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(8)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Purpose.of.Flight', data = dataset, palette = 'RdBu_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we observe that aircrafts for purpose of `personal` use has the highest accident count. However, this may be because `personal` use is the most common purpose of flight. `Aerial Application` and `Instructional` has exceptionally low fatality rate, which could be due to the nature of flight and experience of the pilots. `Work use` and `Others` has the highest fatality rates. Nevertheless, this might be biased statistic due to the small sample sizes.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF WEATHER CONDITION","metadata":{}},{"cell_type":"code","source":"#countplot of weather condition\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.countplot(x = 'Weather.Condition', hue = 'Fatal', data = clean_data, palette = 'Set1')\n\n#Percentage fatality of Weather condition\ndataset = pd.DataFrame(clean_data.groupby('Weather.Condition')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Weather.Condition'] = dataset.index\nfatal_count = []\nfor i in dataset['Weather.Condition']:\n    data1 = clean_data.loc[clean_data['Weather.Condition']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(3)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Weather.Condition', data = dataset, palette = 'RdBu_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### VMC - Visual Meteorological Conditions\n##### IMC - Instrument Meteorological Conditions\n##### UNK - Unknown\n\nFrom the above plots, we observe that flight in `VMC` has the highest accident count. However, this may be because `VMC` is the most common weather condition. `VMC` also has an exceptionally low fatality rate which is expectedly due to the ease of flight and management of adversity in a mild and safer weather condition. \n\nThe fatality rate is higher in `IMC` or `UNK` weather condition. Logically, flying and managing crisis in worse weather condition is likely to be a challenge and hence explains the higher rate of fatality. Rate of fatality can observed to be highly related to the weather condition. ","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF BROAD PHASE OF FLIGHT","metadata":{}},{"cell_type":"code","source":"#countplot of broad phase of flight\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.countplot(x = 'Broad.Phase.of.Flight', hue = 'Fatal', data = clean_data, palette = 'Set1')\n\n#Percentage fatality of Broad Phase of Flight\ndataset = pd.DataFrame(clean_data.groupby('Broad.Phase.of.Flight')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Broad.Phase.of.Flight'] = dataset.index\nfatal_count = []\nfor i in dataset['Broad.Phase.of.Flight']:\n    data1 = clean_data.loc[clean_data['Broad.Phase.of.Flight']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(12)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Broad.Phase.of.Flight', data = dataset, palette = 'RdBu_r')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we observe that `Landing` and `Taxi` phase has exceptionally low fatality rate. `Maneuvering` and `Unknown` phases of flight has the highest fatality rates.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF REPORT STATUS","metadata":{}},{"cell_type":"code","source":"#countplot of report status\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.countplot(x = 'Report.Status', hue = 'Fatal', data = clean_data, palette='Set1')\n\n#Percentage fatality of report status\ndataset = pd.DataFrame(clean_data.groupby('Report.Status')['Fatal'].count())\ndataset = dataset.rename(columns={'Fatal': \"Total\"})\ndataset['Report.Status'] = dataset.index\nfatal_count = []\nfor i in dataset['Report.Status']:\n    data1 = clean_data.loc[clean_data['Report.Status']== i]\n    data1 = len(data1.loc[data1['Fatal'] =='Yes'])\n    fatal_count.append(data1)\ndataset['Fatal_Count'] = fatal_count\ndataset['Percentage'] = dataset['Fatal_Count']/dataset['Total'] * 100\ndataset['Index'] = [x for x in range(4)]\ndataset = dataset.set_index('Index')\n\nf, axes = plt.subplots(1, 1, figsize=(24, 10))\nsb.barplot(x = 'Percentage', y = 'Report.Status', data = dataset, palette = 'RdBu_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we observe that `Probable Cause ` report status has highest accident count. However, this might be because a `Probable Cause` report status is most common. A `Factual` report status has the lowest fatality rate . A `Foreign` report status has the highest fatality rates. Nevertheless, this might be biased statistic due to the small sample sizes for `Foreign` and `Factual` reports.","metadata":{}},{"cell_type":"markdown","source":"#### ANALYSIS OF LATITUDE AND LONGITUDE","metadata":{}},{"cell_type":"code","source":"#Check for null values\nfor i in LatLong_data.columns:\n    latlong_missing = np.mean(LatLong_data[i].isnull())\n    print('{} - {}% , {}'.format(i, round(data_missing*100), LatLong_data[i].isna().sum()))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop the null values\nLatLong_data = LatLong_data.dropna()\n\n#check for null values (if any)\nfor i in LatLong_data.columns:\n    latlong_missing = np.mean(LatLong_data[i].isnull())\n    print('{} - {}% , {}'.format(i, round(data_missing*100), LatLong_data[i].isna().sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LatLong_data.describe()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Observe that max (Longitude) = 435.833334, value(s) that exist beyond the possible range of Longitude \n#Latitude: -90 to 90\n#Longtitude: -180 to 180 \n\n#Remove the anomaly\n\nlatrange = LatLong_data[(LatLong_data['Latitude'] >= 90) | (LatLong_data['Latitude'] <= -90)].index\nlongrange = LatLong_data[(LatLong_data['Longitude'] >= 180) | (LatLong_data['Longitude'] <= -180)].index\nLatLong_data.drop(latrange, inplace = True)\nLatLong_data.drop(longrange, inplace = True)\n\nLatLong_data.describe() #Check","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scatter plot to visualize the location of the accidents/incidents\n\nsb.set_style(\"darkgrid\")\nf, axes = plt.subplots(1, 1, figsize = (24, 14))\nsb.scatterplot(x = 'Longitude', y = 'Latitude', data = LatLong_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scatter plot to visualize the location of the accidents/incidents that are fatal and non-fatal\n\nf, axes = plt.subplots(1, 1, figsize = (24, 14))\nsb.scatterplot(x = 'Longitude', y = 'Latitude', hue = 'Fatal', data = LatLong_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plots above, we observe that the points shape the world map. Expected since the axis is latitude by longitude. Majority of accidents reported are in the area of the United States. Another observation is that majority of the accidents outside of the United States resulted in fatality. This is coherent with our observation from analysis of Report Status whereby a `Foreign` report on an accident is likely to be fatal.\n\n### Conclusion:  \n#### The possible factors that are likely to affect the fatality of an accident/incident: \n`Aircraft Damage`, `Purpose of Flight`, `Weather Conditions`, `Broad phase of flight` and `Report Status`.","metadata":{}},{"cell_type":"markdown","source":"---\n# Modelling and Predictions\n\nReference: <br>\nhttps://machinelearningmastery.com/feature-selection-with-categorical-data/","metadata":{}},{"cell_type":"markdown","source":"### Categorical Feature Selection\n\nUsing OriginalEncoder and LabelEncode to encode each variable to integers","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import mutual_info_classif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Allocating features / target variable\nX = pd.DataFrame(clean_data[['Month',              \n                             'Aircraft.Damage',     \n                             'Make',\n                             'Amateur.Built',\n                             'Number.of.Engines',\n                             'Engine.Type',\n                             'Purpose.of.Flight',\n                             'Weather.Condition',\n                             'Broad.Phase.of.Flight',\n                             'Report.Status',]])\n\ny = pd.DataFrame(clean_data[\"Fatal\"]) #Target Variable\n\n# prepare input data\ndef prepare_inputs(X_train, X_test):\n    oe = OrdinalEncoder()\n    oe.fit(X_train)\n    X_train_enc = oe.transform(X_train)\n    X_test_enc = oe.transform(X_test)\n    return X_train_enc, X_test_enc\n\n# prepare target\ndef prepare_targets(y_train, y_test):\n    le = LabelEncoder()\n    le.fit(y_train.values.ravel())\n    y_train_enc = le.transform(y_train)\n    y_test_enc = le.transform(y_test)\n    return y_train_enc, y_test_enc\n\n#Split randomly into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\n# prepare input data\nX_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n\n# prepare output data\ny_train_enc, y_test_enc = prepare_targets(y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Chi-Squared Feature Selection\nPearson’s chi-squared statistical hypothesis test is an example of a test for independence between categorical variables.","metadata":{}},{"cell_type":"code","source":"def select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=chi2, k='all')\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n    \n# plot the scores  \nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Chi2:** it can be seen that Features 1,3,6 and 7 are the four best variables to choose from.\n\nThese features are: `Aircraft.Damage`, `Amateur.Built`, `Purpose.of.Flight`, `Weather.Condition` respectively.","metadata":{}},{"cell_type":"markdown","source":"#### 2. Mutual Information Feature Selection\nMutual information from the field of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection.","metadata":{}},{"cell_type":"code","source":"def select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=mutual_info_classif, k='all')\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n    \n# plot the scores  \nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mutual_Info_classif:** it can be seen that Features 1,7,8 and 9 are the four best variables to choose from.\n\nThese features are: `Aircraft.Damage`, `Weather.Condition`, `Broad.Phase.of.Flight`, `Report.Status` respectively.","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression\n#### Modelling with ALL FEATURES","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the model\nmodel = LogisticRegression(solver='lbfgs', max_iter=1000)\nmodel.fit(X_train_enc, y_train_enc)\n\n# evaluate the model\ny_test_pred = model.predict(X_test_enc)\ny_train_pred = model.predict(X_train_enc)\n\n# evaluate predictions\naccuracy_test = accuracy_score(y_test_enc, y_test_pred)\naccuracy_train = accuracy_score(y_train_enc, y_train_pred)\nprint(\"Goodness of fit for model using ALL Features\")\nprint(\"Classification Accuracy (train dataset) :\\t %.2f\" %(accuracy_train*100))\nprint(\"Classification Accuracy (test dataset) :\\t %.2f\" %(accuracy_test*100))\n\n#Plotting a heatmap\nf, axes = plt.subplots(1, 2, figsize=(12, 4))\nsb.heatmap(confusion_matrix(y_train_enc, y_train_pred),\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsb.heatmap(confusion_matrix(y_test_enc, y_test_pred), \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n\naxes[0].set_title(\"Train\")\naxes[1].set_title(\"Test\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Built Using Chi-Squared Features\n\nWe can use the chi-squared test to score the features and select the four most relevant features.","metadata":{}},{"cell_type":"code","source":"def select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=chi2, k=4)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs\n\n# feature selection\nX_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n\n#fit the model\nmodel = LogisticRegression(solver='lbfgs', max_iter=1000)\nmodel.fit(X_train_fs, y_train_enc)\n\n# evaluate the model\ny_test_pred = model.predict(X_test_fs)\ny_train_pred = model.predict(X_train_fs)\n\n# evaluate predictions\naccuracy_test = accuracy_score(y_test_enc, y_test_pred)\naccuracy_train = accuracy_score(y_train_enc, y_train_pred)\nprint(\"Goodness of fit for model using Chi-Squared selected features\")\nprint(\"Classification Accuracy (train dataset) :\\t %.2f\" %(accuracy_train*100))\nprint(\"Classification Accuracy (test dataset) :\\t %.2f\" %(accuracy_test*100))\n\n#Plotting a heatmap\nf, axes = plt.subplots(1, 2, figsize=(12, 4))\nsb.heatmap(confusion_matrix(y_train_enc, y_train_pred),\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsb.heatmap(confusion_matrix(y_test_enc, y_test_pred), \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n\naxes[0].set_title(\"Train\")\naxes[1].set_title(\"Test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Built Using Mutual Information Features\nWe can repeat the experiment and select the top four features using a mutual information statistic.","metadata":{}},{"cell_type":"code","source":"def select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=mutual_info_classif, k=4)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs\n\n# feature selection\nX_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n\n# fit the model\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(X_train_fs, y_train_enc)\n\n# evaluate the model\ny_test_pred = model.predict(X_test_fs)\ny_train_pred = model.predict(X_train_fs)\n\n# evaluate predictions\naccuracy_test = accuracy_score(y_test_enc, y_test_pred)\naccuracy_train = accuracy_score(y_train_enc, y_train_pred)\nprint(\"Goodness of fit for model using Mutual Info selected features\")\nprint(\"Classification Accuracy (train dataset) :\\t %.2f\" %(accuracy_train*100))\nprint(\"Classification Accuracy (test dataset) :\\t %.2f\" %(accuracy_test*100))\n\n#Plotting a heatmap\nf, axes = plt.subplots(1, 2, figsize=(12, 4))\nsb.heatmap(confusion_matrix(y_train_enc, y_train_pred),\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsb.heatmap(confusion_matrix(y_test_enc, y_test_pred), \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n\naxes[0].set_title(\"Train\")\naxes[1].set_title(\"Test\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments:\n\nThe classification accuracy of the 3 different models above are shown to be ~87%. They are highly accurate for prediction of fatality. \n\nWe observe that the accuracy of prediction using selected features, either by `Chi-Squared` or `Mutual Info Classification`, is marginally higher then using `all features`. This might indicate that modelling using `all features` might impose a negative effect due to overfitting. ","metadata":{}},{"cell_type":"markdown","source":"### Natural Forest Regression\n\nReferences: <br>\nhttps://towardsdatascience.com/random-forest-in-python-24d0893d51c0","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_column',90)\n\nforest_data = pd.DataFrame(clean_data[['Month', \n                                       'Aircraft.Damage', \n                                       'Make', \n                                       'Amateur.Built', \n                                       'Number.of.Engines',\n                                       'Engine.Type', \n                                       'Purpose.of.Flight', \n                                       'Weather.Condition', \n                                       'Broad.Phase.of.Flight',\n                                       'Report.Status', \n                                       'Fatal']])\n\nfeatures = pd.get_dummies(forest_data)\nfeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_cols = [col for col in features if features[col].dtype.kind != 'O']\n# Positive is 2, Negative is 1 since dataframe +=1\n# Making it so no values will divide by 0 later\nfeatures[numeric_cols] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert Data to Arrays\n\nLabel - Data we want to Predict <br>\nFeatures - Variables to used for prediction <br>\nConvert to Numpy - in order for this algorithm to work","metadata":{}},{"cell_type":"code","source":"# Labels are the values we want to predict\nlabels = np.array(features['Fatal_Yes'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures = features.drop(['Fatal_No', 'Fatal_Yes'], axis = 1)\n\n# Saving feature names for later use\nfeature_list = list(features.columns)\n\n# Convert to numpy array\nfeatures = np.array(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training and Testing Sets\nWe expect the training features number of columns to match the testing feature number of columns and the number of rows to match for the respective training and testing features and the labels","metadata":{}},{"cell_type":"code","source":"# Split the data into training and testing sets \ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n                                                                            test_size = 0.3, random_state = 42)\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\n                                                        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating and training the model","metadata":{}},{"cell_type":"code","source":"# Instantiate model with 100 decision trees \nregressor = RandomForestRegressor(n_estimators=100, random_state= 42)\n\n# Train the model on training data\nregressor.fit(train_features, train_labels)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use the forest's predict method on the train data\nTo put our predictions in perspective, we can calculate an accuracy using the mean average percentage error subtracted from 100 %.","metadata":{}},{"cell_type":"code","source":"train_predictions = regressor.predict(train_features)\n\n# Calculate the absolute errors\nerrors = abs(train_predictions - train_labels)\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / train_labels)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Train Accuracy:', round(accuracy, 2), '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use the forest's predict method on the test data\nTo put our predictions in perspective, we can calculate an accuracy using the mean average percentage error subtracted from 100 %.","metadata":{}},{"cell_type":"code","source":"predictions = regressor.predict(test_features)\n\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / test_labels)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Test Accuracy:', round(accuracy, 2), '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Variable Importances\nThe importances returned in Skicit-learn represent how much including a particular variable improves the prediction\n","metadata":{}},{"cell_type":"code","source":"importances = list(regressor.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(features, round(importance, 4)) for features, \n                       importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In future implementations of the model, we can remove those variables that have no importance, and the performance will not suffer. Additionally, if we are using a different model(eg: a support vector machine) we could use the random forest feature importances as a feature selection method.","metadata":{}},{"cell_type":"markdown","source":"#### Visualization\n\nSimple bar plot of the feature importances to illustrate the disparities in the relative significance of the variables.","metadata":{}},{"cell_type":"code","source":"# list of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nf, axes = plt.subplots(1, 1, figsize = (24,12))\nplt.bar(x_values, importances, orientation = 'vertical')\n\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n\n# Axis labels and title\nplt.ylabel('Importance')\nplt.xlabel('Variable')\nplt.title('Variable Importances')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random forest with only the most important variables\n`Aircraft.Damage_Substantial` to see how the performance compares.","metadata":{}},{"cell_type":"code","source":"# New random forest with only the most important variables\nregressor_most_important = RandomForestRegressor(n_estimators= 100, random_state=42)\n\n# Extract the two most important features\nimportant_indices = [feature_list.index('Aircraft.Damage_Substantial')]\ntrain_important = train_features[:, important_indices]\ntest_important = test_features[:, important_indices]\n\n# Train the random forest\nregressor_most_important.fit(train_important, train_labels)\n\n# Make predictions and determine the error\npredictions = regressor_most_important.predict(test_important)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Most Important Variable to Predict - train data","metadata":{}},{"cell_type":"code","source":"errors = abs(train_predictions - train_labels)\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / train_labels)\naccuracy = 100 - np.mean(mape)\nprint('Train Accuracy:', round(accuracy, 2), '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Most Important Variable to Predict - test data","metadata":{}},{"cell_type":"code","source":"errors = abs(predictions - test_labels)\n\n# Display the performance metrics\nmape = np.mean(100 * (errors / test_labels))\naccuracy = 100 - mape\nprint('Accuracy:', round(accuracy, 2), '%.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comments:\n\nWith only 1 variable, we were able to achieve an accurate result - slightly more accurate if we were to use all the variables. \n\nThis means that if we were to operate with this model, using variables of importances are sufficient to achieve optimal performance. \n\nIn a production setting, we would need to weigh this effect on accuracy against the number of variables and time required to obtain them. \n\n---","metadata":{}},{"cell_type":"markdown","source":"# `THE` `END` `. `","metadata":{}}]}