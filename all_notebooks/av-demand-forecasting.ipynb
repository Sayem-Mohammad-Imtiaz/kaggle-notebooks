{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrandom_seed = 0\nrandom.seed(random_seed)\nnp.random.seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(actual, predicted):\n\n    predicted = np.array([np.log(np.abs(x+1.0)) for x in predicted])  # doing np.abs for handling neg values  \n    actual = np.array([np.log(np.abs(x+1.0)) for x in actual])\n    log_err = actual-predicted\n    \n    return 1000*np.sqrt(np.mean(log_err**2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/demand-forecasting/train_0irEZ2H.csv')\ntest = pd.read_csv('/kaggle/input/demand-forecasting/test_nfaJ3J5.csv')\nsubmit = pd.read_csv('/kaggle/input/demand-forecasting/sample_submission_pzljTaX.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape, submit.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, the data is on (store X sku) level, let's make a separate identifier to pick it later. Below, I have concatenated the store and sku id by making new column `store_sku`. <br>\nI have also checked if the number of such combinations is same across the train and test set. By making sure it is 0, we can rest assured that no cold-start needs to be done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['store_sku'] = (train['store_id'].astype('str') + \"_\" + train['sku_id'].astype('str'))\ntest['store_sku'] = (test['store_id'].astype('str') + \"_\" + test['sku_id'].astype('str'))\nlen(train['store_sku'].unique()) - len(test['store_sku'].unique()) # checking if the combination of store and sku is same across train and test.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(np.intersect1d(train['store_sku'].unique(), test['store_sku'].unique())) == len(test['store_sku'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that one entry for `total_price` is null in the train set. Lets replace it with the `base_price` for now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train[train['total_price'].isnull()]['base_price']\ntrain['total_price'] = train['total_price'].fillna(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appending train and test together for faster manipulation of data\ntest['units_sold'] = -1\ndata = train.append(test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Checking Data distribution for Train! \\n')\nfor col in train.columns:\n    print(f'Distinct entries in {col}: {train[col].nunique()}')\n    print(f'Common # of {col} entries in test and train: {len(np.intersect1d(train[col].unique(), test[col].unique()))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.units_sold.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train[train.units_sold <= 200].units_sold).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['units_sold'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that the target data is highly skewed. To make accurate predictions, we must normalise it before using.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.log1p(train['units_sold']).hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing for base and total price. Let's see if we can gain some insights about the target data from these two.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['base_price', 'total_price']].plot.box()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making price based new features\n\ntrain['diff'] = train['base_price'] - train['total_price']\n\ntrain['relative_diff_base'] = train['diff']/train['base_price']\ntrain['relative_diff_total'] = train['diff']/train['total_price']\n\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['diff'] = test['base_price'] - test['total_price']\ntest['relative_diff_base'] = test['diff']/test['base_price']\ntest['relative_diff_total'] = test['diff']/test['total_price']\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Studying correlation between features and the target. This will help us in regression later.\ncols = ['base_price', 'total_price', 'diff', 'relative_diff_base', 'relative_diff_total'\n        , 'is_featured_sku', 'is_display_sku', 'units_sold']\ntrain[cols].corr().loc['units_sold']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have good correlation of features with the target variable, hence doing a baseline regression won't be a bad start.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Baseline Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'current # of features in cols: {len(cols)}')\ncols.remove('units_sold')\nprint(f'current # of features to be used: {len(cols)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train[cols]\ny = np.log1p(train['units_sold']) # Transforming target into normal via logarithmic operation\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor()\nreg.fit(Xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = reg.predict(Xval)\nprint(f'The validation RMSLE error for baseline model is: {RMSLE(np.exp(yval), np.exp(preds))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_preds = reg.predict(test[cols])\nsubmit['units_sold'] = np.exp(sub_preds)\nsubmit.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('sub_baseline_v1.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Leaderboard scores:\n- Public: 731.0198\n- Private: ?\n\nNot a bad start!\nThe time series data that we have behaves differently for different sku and hence we should try fitting multiple models for each sku/store, i.e., different models for different store_sku combination. Let's try that out and see if that makes any improvement.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SKU level base model fitting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before fitting the model, we would like to encode our store and sku information. There are multiple ways of doing this:\n    1. One-hot encoding\n    2. Label encoding\n    3. Category encoding\nSince, the features here have high cardinality, we should go for Category encoding. I'll be using `MEstimateEncoder` for this purpose.\nYou can find its documentation at this link: http://contrib.scikit-learn.org/category_encoders/mestimate.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import TargetEncoder, MEstimateEncoder\nencoder = MEstimateEncoder()\nencoder.fit(train['store_id'], train['units_sold'])\ntrain['store_encoded'] = encoder.transform(train['store_id'], train['units_sold'])\ntest['store_encoded'] = encoder.transform(test['store_id'], test['units_sold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.fit(train['sku_id'], train['units_sold'])\ntrain['sku_encoded'] = encoder.transform(train['sku_id'], train['units_sold'])\ntest['sku_encoded'] = encoder.transform(test['sku_id'], test['units_sold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skus = train.sku_id.unique()\nprint(skus[:2])\n\ntest_preds = test.copy()\ntest_preds.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sku_model(sku, cols_to_use, reg):\n    X = train[train['sku_id'] == sku][cols_to_use]\n    y = train[train['sku_id'] == sku]['units_sold']\n    \n    Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\n    reg.fit(X,np.log1p(y))\n    \n    y_pred = reg.predict(Xval)\n    err = RMSLE(yval, np.exp(y_pred))\n    print(f'RMSLE for {sku} is: {err}')\n    \n    preds = reg.predict(test[test['sku_id'] == sku][cols_to_use])    \n    temp_df =  pd.DataFrame.from_dict({'record_ID': test_preds[test_preds['sku_id'] == sku]['record_ID'],\n                                       'units_sold':  np.exp(preds)})\n    return err, temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_use = cols + ['store_encoded', 'sku_encoded']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"err = dict() # for documenting error for each sku type\nsub = pd.DataFrame(None, columns = ['record_ID', 'units_sold'])\nreg = RandomForestRegressor(random_state = 2288)\n\nfor sku in skus:\n    err[sku], temp = sku_model(sku, cols_to_use, reg)\n    sub = sub.append(temp)\n\nprint(np.mean(list(err.values())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.sort_values(by = ['record_ID']).to_csv('sub_sku_RF_v2.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Till now we have done:\n- RF regressor for all data at once: Public LB score `731.0198`\n- RF regressor for data at sku level: Public LB score `481.0016`\n\nWow! That's a huge improvement. This proves that using data at sku level will certainly help the cause.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, we would also like to incorporate the `store_id` as one of the defining features of the multiple models that we are building. But as the problem statement defines, there are 76 different models for each one of them across each sku will be a herculean task. \nThen, how do we do it?\n\nWe will be using LightGBM's categorical features' input to our rescue. This can be used to provide categorical inputs to the model with a single line of code. Moreover, as the documentation says, it is 8 times faster than one-hot encoding. Find its link here.\nhttps://lightgbm.readthedocs.io/en/latest/Parameters.html#categorical_feature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# LightGBM Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_use += ['store_id', 'sku_id']\n# For defining categorical features to the model, we will build `cat_cols`\ncat_cols = ['is_featured_sku', 'is_display_sku', 'store_id', 'sku_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[cols_to_use]\ny = np.log1p(train['units_sold']) # Transforming target into normal via logarithmic operation\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest = test[cols_to_use]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runLGB(Xtrain, ytrain, Xval, yval, cat_cols, Xtest = None):\n    params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'l1',\n    #'max_depth': 9, \n    'learning_rate': 0.1\n    ,'verbose': 1\n    , \"min_data_in_leaf\" : 10\n    }\n\n    n_estimators = 800\n    early_stopping_rounds = 10\n\n    d_train = lgb.Dataset(Xtrain.copy(), label=ytrain.copy(), categorical_feature=cat_cols)\n    d_valid = lgb.Dataset(Xval.copy(), label=yval.copy(), categorical_feature=cat_cols)\n    watchlist = [d_train, d_valid]\n\n    model = lgb.train(params, d_train, n_estimators\n                      , valid_sets = [d_train, d_valid]\n                      , verbose_eval=n_estimators\n                      , early_stopping_rounds=early_stopping_rounds)\n\n    preds = model.predict(Xval, num_iteration=model.best_iteration)\n    err = RMSLE(yval, np.exp(preds))\n    \n    preds_test = model.predict(Xtest, num_iteration=model.best_iteration)\n    return  preds, err, np.exp(preds_test), model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val, err, pred_test,model = runLGB(Xtrain, ytrain, Xval, yval, cat_cols, Xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Public LB score: 419.7016\n- Private LB score: ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['units_sold'] = pred_test\nsubmit.to_csv('lgb_sub_store_sku_v3.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a =model.feature_importance(importance_type='split')\nfeature = pd.DataFrame(model.feature_name(), columns = ['feature'])\nfeature['importance'] = a\nfeature = feature.sort_values(by = ['importance'], ascending = False)\nfeature.head(11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without using any date and time features, we have scored `419.7016` on LB which is a huge improvement.\nLet's see if we can extract any useful information from the `week` feature.\nWe will try to build numeric features based on the week's start as well as end date. <br>\nI'll also be using 5-fold CV to strengthen the predictions later.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Extracting datetime features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ntrain['week'] = train['week'].astype('str')\ntrain['week'] = [datetime.strptime(x, '%d/%m/%y') for x in train['week']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['week'] = test['week'].astype('str')\ntest['week'] = [datetime.strptime(x, '%d/%m/%y') for x in test['week']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\ntrain['weekend_date'] = [x + datetime.timedelta(days=6) for x in train['week']]\ntest['weekend_date'] = [x + datetime.timedelta(days=6) for x in test['week']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_cols = list(train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime \ndef extract_time_features(df):\n    \n    start_date = datetime.datetime(2011,1, 17)\n    \n    print('starting basic feature extraction for week start date!')\n\n    df['year'] = df['week'].dt.year\n    df['date'] = [x.day for x in df['week']]\n    df['month'] = df['week'].dt.month\n    df['weekday'] = df['week'].dt.dayofweek\n    df['weeknum'] = df['week'].dt.weekofyear\n    \n    df['week_serial']  = [divmod((x-start_date).total_seconds(), 86400)[0]/7 for x in df['week']]\n    \n\n    '''\n    print('starting month end related feature extraction for week start date!')\n\n    df['quarter'] = [x.quarter for x in df['week']]\n    df['is_month_start'] = [x.is_month_start for x in df['week']]\n    df['is_month_end'] = [x.is_month_end for x in df['week']]\n    df['is_month_start'] = df['is_month_start'].astype(int)\n    df['is_month_end'] = df['is_month_end'].astype(int)\n    \n    df['start_week']= df.assign(start_week=pd.cut(df.date,[0,9,15,23,31],labels=[1,2,3,4]))['start_week']\n    df['start_week'] = df['start_week'].astype(int)\n    '''\n\n    print('Starting basic feature extraction for week end date!')\n    \n    df['end_year'] = df['weekend_date'].dt.year\n    df['end_date'] = [x.day for x in df['weekend_date']]\n    df['end_month'] = df['weekend_date'].dt.month\n    df['end_weekday'] = df['weekend_date'].dt.dayofweek\n    df['end_weeknum'] = df['weekend_date'].dt.weekofyear\n    df['end_week_serial']  = [divmod((x-start_date).total_seconds(), 86400)[0]/7 for x in df['weekend_date']]\n\n    '''\n    print('starting month end related feature extraction for week start date!')\n\n    df['end_quarter'] = [x.quarter for x in df['weekend_date']]\n    df['end_is_month_start'] = [x.is_month_start for x in df['weekend_date']]\n    df['end_is_month_end'] = [x.is_month_end for x in df['weekend_date']]\n    df['end_is_month_start'] = df['end_is_month_start'].astype(int)\n    df['end_is_month_end'] = df['end_is_month_end'].astype(int)\n    \n    df['end_week'] = df.assign(end_week=pd.cut(df.end_date,[0,9,15,23,31],labels=[1,2,3,4]))['end_week']\n    df['end_week'] = df['end_week'].astype(int)\n    '''\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = extract_time_features(train)\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = extract_time_features(test)\ntest.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Diff(li1, li2): \n    return list(set(li1) - set(li2))\n\ntotal_cols = list(test.columns)\nnew_feat = Diff(total_cols, current_cols)\nnew_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[new_feat].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training LGB Model with all the date/time features created","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'The number of features used before: {len(cols_to_use)}')\nprint(f'The number of categorical features used before: {len(cat_cols)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_use += new_feat\n#cat_cols += new_feat\n\nprint(f'The number of features to be used now: {len(cols_to_use)}')\nprint(f'The number of categorical features to be used now: {len(cat_cols)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtest = test[cols_to_use]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[cols_to_use]\ny = np.log1p(train['units_sold']) # Transforming target into normal via logarithmic operation\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val, err, pred_test,model = runLGB(Xtrain, ytrain, Xval, yval, cat_cols, Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['units_sold'] = pred_test\nsubmit.to_csv('lgb_time_store_sku_v4.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Public LB score: 377.3146\n- Private LB score: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a =model.feature_importance(importance_type='split')\nfeature = pd.DataFrame(model.feature_name(), columns = ['feature'])\nfeature['importance'] = a\nfeature = feature.sort_values(by = ['importance'], ascending = False)\nfeature.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making changes in the LGB model to improve the predictions (Tuning!)\ndef runLGB2(Xtrain, ytrain, Xval, yval, cat_cols, Xtest = None):\n    params = {\n    'boosting_type': 'dart', #dropout aided regressive trees (DART) # improves accuracy\n    'objective': 'regression',\n    'metric': 'l1', \n    #'max_depth': 10, \n    'learning_rate': 0.5\n    ,'verbose': 1\n    }\n    \n    #regularising for overfitting with inf depth\n    params[\"min_data_in_leaf\"] = 15 \n    params[\"bagging_fraction\"] = 0.7\n    params[\"feature_fraction\"] = 0.7\n    #params[\"bagging_freq\"] = 3\n    params[\"bagging_seed\"] = 50\n\n    n_estimators = 575\n    early_stopping_rounds = 30\n\n    d_train = lgb.Dataset(Xtrain.copy(), label=np.log1p(ytrain.copy()), categorical_feature=cat_cols)\n    d_valid = lgb.Dataset(Xval.copy(), label=np.log1p(yval.copy()), categorical_feature=cat_cols)\n    watchlist = [d_train, d_valid]\n\n    model = lgb.train(params, d_train, n_estimators\n                      , valid_sets = [d_train, d_valid]\n                      , verbose_eval=125\n                      , early_stopping_rounds=early_stopping_rounds)\n\n    preds = model.predict(Xval, num_iteration=model.best_iteration)\n    err = RMSLE(yval['units_sold'], np.exp(preds))\n    \n    preds_test = np.exp(model.predict(Xtest, num_iteration=model.best_iteration))\n    return  preds, err, preds_test, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.fit(train[new_feat], train['units_sold'])\ntrain[new_feat] = encoder.transform(train[new_feat], train['units_sold'])\ntest[new_feat] = encoder.transform(test[new_feat], test['units_sold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\npreds_buff = 0\nerr_buff = []\n\nX = train[cols_to_use]\ny = train[['units_sold']]\n\nn_splits = 10\nkf = StratifiedKFold(n_splits=n_splits, shuffle= True, random_state=22)\n\nfor dev_index, val_index in kf.split(X, y):\n    start = time.time()\n    Xtrain, Xval = X.iloc[dev_index], X.iloc[val_index]\n    ytrain, yval = y.iloc[dev_index], y.iloc[val_index]    \n    \n    pred_val, err, pred_test,model = runLGB2(Xtrain, ytrain, Xval, yval, cat_cols, Xtest)\n    preds_buff += pred_test\n    err_buff.append(err)\n    print(f'Mean Error: {np.mean(err_buff)}; Split error: {err}')\n    print(f'Total time in seconds for this fold: {time.time()-start}')\n    print('\\n')\n\npreds_buff /= n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(err_buff, np.mean(err_buff))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['units_sold'] = np.abs(preds_buff)\nsubmit.to_csv('lgb_time_10cv_v5.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Leaderboard scores:\n1. After adding basic time related features\n    - Public LB score: 363.6342\n    - Private LB score: ?\n2. After encoding time related features\n    - Public LB score: 365.7862\n    - Private LB score: ?\n3. After fine-tuning the hyperparameters with 10 fold CV\n    - Public LB score: 360.7086\n    - Private LB score: ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a =model.feature_importance(importance_type='split')\nfeature = pd.DataFrame(model.feature_name(), columns = ['feature'])\nfeature['importance'] = a\nfeature = feature.sort_values(by = ['importance'], ascending = False)\nfeature.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}