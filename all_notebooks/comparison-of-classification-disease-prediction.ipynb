{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Cardiovascular Disease Prediction\n**Benan AKCA** <a id=\"0\"></a>\n\n[![Digital-Heart-Health.png](https://i.postimg.cc/26p24TZ0/Digital-Heart-Health.png)](https://postimg.cc/qtLsrsQK)\n\n* [Introduction](#1)\n* [Import Modules](#2)\n* [Data Cleaning and EDA](#3)\n    * [Checking Duplication and Missed Values](#4)        \n    * [Visualization](#5)\n        * [Detecting Outliers](#6)\n* [Feature Engineering](#7)\n* [Model Selection](#8)\n    * [Training and Test Sets](#9)\n    * [Data Normalization](#10)\n    * [Model Comparison](#11)\n    * [K Fold Cross Validation](#12)\n    * [Grid Search](#13)\n* [Model Evaluation](#14)\n    * [Test Set Accuracy Score](#15)\n    * [Confusion Matrix](#16)\n    * [F1 Score Calculation](#17)\n* [ANN Aproach](#18)    \n* [Conclusion](#26)\n\n## INTRODUCTION <a id=\"1\"></a>\n<mark>[Return Contents](#0)\n<hr>\nWe have a nice data set that includes patient characteristics and a label about cardiovascular diseases.\n    \nThen what we have in our kernel;\n* Dropping irrevelant feature\n* Handling outliers of high blood pressure \n* Adding Body Mess Index as a new feature (Feature Engineering)\n* Comparison of Classification Models\n    * Logistic Regression\n    * Decision Tree\n    * Random Forest\n    * K Nearest Neighbors (KNN)\n    * Suppor Vector Machine (SVM)\n    * Naive Bayes\n* K Fold Cross Validation to prevent overfitting data\n* Grid Search algorithm for finding the best hyperparameters for the winning classification algorithm by \n* Calculating Precision, Recall and F1 scores for measuring the success with another evaluation metric\n* ANN Aproach\n\nand **more**."},{"metadata":{},"cell_type":"markdown","source":"### **Import Modules** <a id=\"2\"></a>\n<mark>[Return Contents](#0)\n<hr>\n\nImporting the necessary modules."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop,Nadam,Adadelta,Adam\nfrom tensorflow.keras.layers import BatchNormalization,LeakyReLU\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport seaborn as sns\nimport scipy.stats as stats\nimport sklearn\nimport os\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Data Cleaning and EDA** <a id=\"3\"></a>\n<mark>[Return Contents](#0)\n<hr>\nFirstly, let us explore what we have in our data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_raw = pd.read_csv(\"/kaggle/input/cardiovascular-disease-dataset/cardio_train.csv\", sep=\";\")\ndata_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw.drop(\"id\",axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Checking Duplication and Missing Values** <a id=\"4\"></a>\n<mark>[Return Contents](#0)\n<hr>\n    \nBefore visualization and outlier checks it is very important to handle duplicate and missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There is {} duplicated values in data frame\".format(data_raw.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can drop the duplicates because they have no any effect of training of model.\n* Firstly let us see the duplicated rows with eyes."},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated = data_raw[data_raw.duplicated(keep=False)]\nduplicated = duplicated.sort_values(by=['age', \"gender\", \"height\"], ascending= False) \n# I sorted the values to see duplication clearly\n\nduplicated.head(2) # Show us just 1 duplication of 24","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw.drop_duplicates(inplace=True)\nprint(\"There is {} duplicated values in data frame\".format(data_raw.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There is {} missing values in data frame\".format(data_raw.isnull().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is clean now."},{"metadata":{},"cell_type":"markdown","source":"### **Visualization** <a id=\"5\"></a>\n<mark>[Return Contents](#0)\n<hr>\n### Detecting Outliers <a id=\"6\"></a>\n\nDetecting outlier and handling them can increase our accuracy score."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_raw.copy(deep=True)\nx.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Columns of \"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\" may have outlier.\n* In order to compare them on same scale we need to standartize firstly."},{"metadata":{},"cell_type":"markdown","source":"<hr>\n#### My Standart Scalar Function\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"s_list = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\"]\ndef standartization(x):\n    x_std = x.copy(deep=True)\n    for column in s_list:\n        x_std[column] = (x_std[column]-x_std[column].mean())/x_std[column].std()\n    return x_std \nx_std=standartization(x)\nx_std.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In order to use the multi box graph plot we need to melt out data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_melted = pd.melt(frame=x_std, id_vars=\"cardio\", value_vars=s_list, var_name=\"features\", value_name=\"value\", col_level=None)\nx_melted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"cardio\", data=x_melted)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are some outliers in the dataset, but as seen above there is an unusual outlier in ap_hi and ap_lo features.\n* Let us calculate the low bound and hi bound of ap_lo and ap_hi features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ap_list = [\"ap_hi\", \"ap_lo\"]\nboundary = pd.DataFrame(index=[\"lower_bound\",\"upper_bound\"]) # We created an empty dataframe\nfor each in ap_list:\n    Q1 = x[each].quantile(0.25)\n    Q3 = x[each].quantile(0.75)\n    IQR = Q3 - Q1\n\n    lower_bound = Q1- 1.5*IQR\n    upper_bound = Q3 + 1.5*IQR\n    boundary[each] = [lower_bound, upper_bound ]\nboundary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can select the index of outlier data by using boundaries we calculated.\n* Normally we should analyze both upper outliers and below outliers but in this case, I consider to handle just uppers because of their extremely higher values."},{"metadata":{"trusted":true},"cell_type":"code","source":"ap_hi_filter = (x[\"ap_hi\"] > boundary[\"ap_hi\"][1])\nap_lo_filter = (x[\"ap_lo\"] > boundary[\"ap_lo\"][1])                                                           \noutlier_filter = (ap_hi_filter | ap_lo_filter)\nx_outliers = x[outlier_filter]\nx_outliers[\"cardio\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Cardiovascular disease is present in 83 percent of the  ap_hi and ap_lo outlier data,\n* Because of ap_hi and ap_lo symbolizes high blood pressure, the high rate of disease is consistent with real life. \n* For this reason, I decided to drop just medically impossible data from the dataset.\n* And I have dropped some of data because we have sufficent data (70000) otherwise I would try to handle them by assigning new values."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='cardio',data=x_outliers,linewidth=2,edgecolor=sns.color_palette(\"dark\", 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I am not a doctor but with the little help of google I've learned some information about blood pressure,\n* \"If oneâ€™s systolic pressure (*ap_hi*) exceeds *180* or diastolic pressure (*ap_lo*) crosses *120*, it is a stage that requires immediate medical attention.\"\n"},{"metadata":{},"cell_type":"markdown","source":"[![blood.jpg](https://i.postimg.cc/bwzm4sq5/blood.jpg)](https://postimg.cc/2VKdZjbd)"},{"metadata":{},"cell_type":"markdown","source":"* A study published by doctors in NCBI NLM recorded a maximum blood pressure of 370/360 mm Hg. This study was performed by recording blood pressure in 10 male athletes through radial artery catheterization.\n* Thus we can drop the ap_hi outlier values over 250 and ap_lo outlier values over 200, without fear of missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"out_filter = ((x[\"ap_hi\"]>250) | (x[\"ap_lo\"]>200) )\nprint(\"There is {} outlier\".format(x[out_filter][\"cardio\"].count()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x[~out_filter]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = x.corr()\nf, ax = plt.subplots(figsize = (15,15))\nsns.heatmap(corr, annot=True, fmt=\".3f\", linewidths=0.5, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see from correlation map easily; cholesterol, blood pressure (ap_hi and ap_low both) and age have a powerful relationship with cardiovascular diseases.\n* Glucogen and cholesterol have a strong relationship among them either."},{"metadata":{},"cell_type":"markdown","source":"### **Feature Engineering** <a id=\"7\"></a>\n<mark>[Return Contents](#0)\n\n<hr>\n### Body Mass Index Feature \n  \nHeight and weight seems uncorrelated with the cardio feature but Body Mass Index could be helpful to train our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bmi_calc(w, h):\n    return w/(h**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[\"bmi\"] = x[\"weight\"]/ (x[\"height\"]/100)**2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Detecting genders of patients"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = x[x[\"gender\"]==1][\"height\"].mean()\nb = x[x[\"gender\"]==2][\"height\"].mean()\nif a > b:\n    gender = \"male\"\n    gender2 = \"female\"\nelse:\n    gender = \"female\"\n    gender2 = \"male\"\nprint(\"Gender:1 is \"+ gender +\" & Gender:2 is \" + gender2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Women have many of the same risk factors with men for heart disease as men, such as smoking, high blood pressure, and high cholesterol especially after 65.\n* Thus we shouldn't categorize them into 1 and 2 because of 2 is always numerically bigger than 1, the model would take into account that and give a bigger ratio to men for having a disease\n* We did not change other categorical code to one hot encoding because they express really hierarchical size\n* An example from describtion of dataset : Cholesterol | 1: normal, 2: above normal, 3: well above normal\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x[\"gender\"] = x[\"gender\"] % 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nx_std = standartization(x)\n\ndata = pd.melt(x_std,id_vars=\"cardio\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"gender\", y=\"bmi\", hue=\"cardio\", data=x,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we interpret the violin plot, the median and quartiles of bmi distribution of patients is slightly higher than non-patients."},{"metadata":{},"cell_type":"markdown","source":"### **Model Selection** <a id=\"8\"></a>\n<mark>[Return Contents](#0)\n<hr>\n### Preparing the Training and Test Sets <a id=\"9\"></a>\n\nDetecting outlier and handling them can increase our accuracy score.\n\nTo create a model first of all we will split our data to training and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = x[\"cardio\"]\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.drop(\"cardio\", axis=1,inplace=True)\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Normalization <a id=\"10\"></a>\n<mark>[Return Contents](#0)\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\nx_train = normalize(x_train)\nx_test = normalize(x_test)\nx = normalize(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Comparison <a id=\"11\"></a>\n<mark>[Return Contents](#0)\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\ndec = DecisionTreeClassifier()\nran = RandomForestClassifier(n_estimators=100)\nknn = KNeighborsClassifier(n_neighbors=100)\nsvm = SVC(random_state=1)\nnaive = GaussianNB()\n\nmodels = {\"Decision tree\" : dec,\n          \"Random forest\" : ran,\n          \"KNN\" : knn,\n          \"SVM\" : svm,\n          \"Naive bayes\" : naive}\nscores= { }\n\nfor key, value in models.items():    \n    model = value\n    model.fit(x_train, y_train)\n    scores[key] = model.score(x_test, y_test)\n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_frame = pd.DataFrame(scores, index=[\"Accuracy Score\"]).T\nscores_frame.sort_values(by=[\"Accuracy Score\"], axis=0 ,ascending=False, inplace=True)\nscores_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.barplot(x=scores_frame.index,y=scores_frame[\"Accuracy Score\"])\nplt.xticks(rotation=45) # Rotation of Country names...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It seems that KNN and Random Forest algorithms are far ahead of the others.\n* So let's focus on these algorithms\n \n### K Fold Cross Validation <a id=\"12\"></a>\n<mark>[Return Contents](#0)\n<hr>\n    \n* With K-Fold cross-validation, we obtain \"K\" pieces of results from different mini-training sets from our main training set.\n* Then we choose the average of these results as the actual result.\n* After all, by taking the standard deviation of K result, we can examine whether the data is consistent."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\naccuracies_random_forest = cross_val_score(estimator=ran, X=x_train, y=y_train, cv=10)\naccuracies_knn = cross_val_score(estimator=knn, X=x_train, y=y_train, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest Average accuracy: \", accuracies_random_forest.mean())\nprint(\"Random Forest Standart Deviation: \", accuracies_random_forest.std())\nprint(\"KNN Average accuracy: \", accuracies_knn.mean())\nprint(\"KNN Standart Deviation: \", accuracies_knn.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standard deviation value shows that we achieve consistent results."},{"metadata":{},"cell_type":"markdown","source":"### Grid Search <a id=\"13\"></a>\n<mark>[Return Contents](#0)\n<hr>\n Let us use grid search algorithm, in order to find the best Random Forest Algorithm's \"n_estimators\" hyperparameter value for our model, l"},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_estimators\" : np.arange(10,150,10)}\n\nran_cv = GridSearchCV(ran, grid, cv=3) # GridSearchCV\nran_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter n_estimators: {}\".format(ran_cv.best_params_)) \nprint(\"Best score: {}\".format(ran_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(solver=\"liblinear\", max_iter=200)\ngrid = {\"penalty\" : [\"l1\", \"l2\"],\n         \"C\" : np.arange(60,80,2)} # (60,62,64 ... 78)\nlog_reg_cv = GridSearchCV(log_reg, grid, cv=3)\nlog_reg_cv.fit(x_train, y_train)\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter n_estimators: {}\".format(log_reg_cv.best_params_)) \nprint(\"Best score: {}\".format(log_reg_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This is the best of our results we obtained\n* Let us evaluate our best model"},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation <a id=\"14\"></a>\n<mark>[Return Contents](#0)\n<hr>\n### Test Set Accuracy Score <a id=\"15\"></a>    \n\n* Now we have selected our model with better hyper parameters than default ones.\n* It is time to evaluate model with our test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_best = LogisticRegression(C=74, penalty=\"l1\", solver=\"liblinear\")\nlogreg_best.fit(x_train, y_train)\nprint(\"Test accuracy: \",logreg_best.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ultimate success of our model in predicting the test set that has not been used in training is 0.72"},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix<a id=\"16\"></a>\n<mark>[Return Contents](#0)\n<hr>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_test\ny_pred = logreg_best.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.2, linecolor=\"purple\", ax=ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Grand Truth\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### F1 Score <a id=\"17\"></a>\n<mark>[Return Contents](#0)\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"TN = cm[0,0]\nTP = cm[1,1]\nFN = cm[1,0]\nFP = cm[0,1]\nPrecision = TP/(TP+FP)\nRecall = TP/(TP+FN)\nF1_Score = 2*(Recall * Precision) / (Recall + Precision)\npd.DataFrame([[Precision, Recall, F1_Score]],columns=[\"Precision\", \"Recall\", \"F1 Score\"], index=[\"Results\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. High precision relates to the low false positive rate\n1. High recall relates to the low false negative rate"},{"metadata":{},"cell_type":"markdown","source":"## ANN Aproach <a id=\"18\"></a>\n<mark>[Return Contents](#0)\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(6, input_dim=12, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = RMSprop(learning_rate=0.002)\nmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau( \n    monitor='val_loss',    # Quantity to be monitored.\n    factor=0.1,       # Factor by which the learning rate will be reduced. new_lr = lr * factor\n    patience=50,        # The number of epochs with no improvement after which learning rate will be reduced.\n    verbose=1,         # 0: quiet - 1: update messages.\n    mode=\"auto\",       # {auto, min, max}. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; \n                       # in the max mode it will be reduced when the quantity monitored has stopped increasing; \n                       # in auto mode, the direction is automatically inferred from the name of the monitored quantity.\n    min_delta=0.0001,  # threshold for measuring the new optimum, to only focus on significant changes.\n    cooldown=0,        # number of epochs to wait before resuming normal operation after learning rate (lr) has been reduced.\n    min_lr=0.00001     # lower bound on the learning rate.\n    )\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=400, restore_best_weights=True)\nhistory = model.fit(x=x_train, y=y_train.values,\n                    batch_size=1024, epochs=1500,\n                    verbose=0,validation_data=(x_test,y_test.values),\n                    callbacks=[learning_rate_reduction, es],\n                    shuffle=True)\n\nmodel.evaluate(x_test, y_test.values, verbose=2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion <a id=\"26\"></a>\n<mark>[Return Contents](#0)\n<hr>\n* If you like this, thank your for you upvotes.\n* If you have any comments or questions, I would be happy to hear them."},{"metadata":{},"cell_type":"markdown","source":"[![smile.jpg](https://i.postimg.cc/0jVh4z64/smile.jpg)](https://postimg.cc/fS0HtTf7)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}