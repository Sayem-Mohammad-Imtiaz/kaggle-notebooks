{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False,world_readable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KBO League(Baseball league) Match Prediction using statistical methods\nWe want to predidct 2020 KBO match result using 2015~2019 KBO data. "},{"metadata":{},"cell_type":"markdown","source":"### Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"../input/korea-baseball-datasetkbo-20152020\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2015 ~ 2019 data\nfileNames=['baseball_' + str(x) + '.csv' for x in list(range(2015,2020))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import data\ntaza = pd.DataFrame()\nfor fileName in tqdm(fileNames):\n    \n    temp = pd.read_csv(fileName)\n    taza = taza.append(temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data description\nThere are various match informations.<br>\nAnd we want to predict match result(win or lose) by using match informations(PA, AB, RUN, ...)<br>\nBut we can't use this data directly **because we can't get match informations(PA, AB, RUN, ...) before the match finish.**<br>\nSo I decide to use recent 50 games average informations of team.<br>\nBefore explain the preprocessing process, I'll explain **feature selection process.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"taza.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection using Correlation matrix of variables\nThere is correlation matrix. Predictor variable(y) is 'win' so I selected explanatory variables(X) that correlation coef from 'win' more than 0.3 ."},{"metadata":{"trusted":true},"cell_type":"code","source":"taza.corr().iplot(kind='heatmap',colorscale=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames = ['G_ID','GDAY_DS','T_ID','VS_T_ID','TB_SC'] + taza.corr()[taza.corr()['win']>0.3].index.tolist()\n\ntaza = taza[colnames]\ntaza['YEAR']=taza['G_ID'].str[:4]\nyears = [str(x) for x in range(2015,2020)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(colnames)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selected variable are 'T_ID' ~ 'OOO' (G_ID, GDAY_DS are information about match date, team)"},{"metadata":{},"cell_type":"markdown","source":"* ### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def weightedMean(arr):\n    return np.average(arr, weights=np.arange(0, len(arr), 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GetX():\n    def __init__(self,gi,N, taza):\n        self.gi = gi\n        self.N = N\n        self.taza =taza\n        self.date = gi[:8]\n        self.team1 = gi[8:10]\n        self.team2 = gi[10:12]\n    \n    def makeR(self,team):\n        '''\n        df1은 team1의 공격(타자)데이터\n        df2는 team1의 수비(실점)데이터\n        '''\n        df1 = self.taza[self.taza['T_ID']==team]\n        df1 = df1.reset_index(drop=True)\n        \n        df2 = self.taza[self.taza['VS_T_ID']==team]\n        df2 = df2.reset_index(drop=True)\n        \n        \n        \n        dateIdx = df1[df1['GDAY_DS']==int(self.date)].index[0]\n        if(dateIdx-self.N<0):\n            return []\n        else:\n            first = dateIdx-self.N\n            \n        df1 = df1[first:dateIdx]\n        sr1 = df1.iloc[:,5:-1].apply(lambda x:weightedMean(x),axis=0)\n        \n        df2 = df2[first:dateIdx]\n        sr2 = df2.iloc[:,5:-1].apply(lambda x:weightedMean(x),axis=0)\n        cN = sr2.index.values\n        sr2.index = [x+'_VS' for x in cN]\n        \n        if(len(sr1)<2):\n            return []\n        \n        temp = pd.Series({\"WIN_RATIO\":weightedMean(df1['win'])})\n        result = sr1.append(sr2)\n        result = result.append(temp)\n        \n        return result\n\n\n    def makeDf(self):\n        df1 = self.taza[self.taza['G_ID']==self.gi]\n\n        result1 = self.makeR(self.team1) #어웨이팀의 정보\n        result2 = self.makeR(self.team2) #홈(상대)팀의 정보\n\n        \n\n        if((len(result1)<=1)|(len(result2)<=1)):\n            return []\n        \n        cN = result2.index.values\n        result2.index = [x+'_ENE' for x in cN]\n        result = result1.append(result2)\n        \n        tempDf = self.taza[self.taza['GDAY_DS']<int(self.date)]\n        tempDf = tempDf[(tempDf['T_ID']==self.team1)&(tempDf['VS_T_ID']==self.team2)]\n        if(len(tempDf)==0):\n            result['RELATIVE_WIN'] = 0.5\n        else:\n\n            result['RELATIVE_WIN'] = tempDf['win'][-10:].mean()\n        result['G_ID']=self.gi\n        result['WIN'] = self.taza[self.taza['G_ID']==gi].iloc[0,-1]\n        \n\n        return result.to_dict()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gapLeftRight(df):\n    dfTemp = df.iloc[:,:-3].copy()\n\n    n = dfTemp.shape[1]//2\n\n    dfLeft = dfTemp.iloc[:,:n]\n    dfRight = dfTemp.iloc[:,n:]\n\n    dfColumns = dfLeft.columns\n\n    dfRight.columns = dfColumns\n\n    dfResult = dfLeft-dfRight\n\n    dfResult.columns = [x+'_GAP' for x in dfColumns]\n    dfResult = pd.concat([dfResult, df.iloc[:,-3:]], axis=1)\n    return dfResult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gapLeftRightRandom(df):\n    dfTemp = df.iloc[:,:-3].copy()\n\n    n = dfTemp.shape[1]//2\n\n    dfLeft = dfTemp.iloc[:,:n]\n    dfRight = dfTemp.iloc[:,n:]\n\n    dfColumns = dfLeft.columns\n\n    dfRight.columns = dfColumns\n\n    dfResult = dfLeft-dfRight\n\n    dfResult.columns = [x+'_GAP' for x in dfColumns]\n    dfResult = pd.concat([dfResult, df.iloc[:,-3:]], axis=1)\n    #Home, Base randomize\n    dfResult=dfResult.sample(len(dfResult))\n\n    nTemp = len(dfResult)//2\n\n    dfResult.iloc[:nTemp,:-3] = -1 * dfResult.iloc[:nTemp,:-3]\n\n    dfResult.iloc[:nTemp,[-3,-1]] = 1-dfResult.iloc[:nTemp,[-3,-1]]\n\n    dfResult = dfResult.sort_values(\"G_ID\")\n    return dfResult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"taza_ = taza[taza['YEAR']=='2015']\n\nX = []\nfor gi in taza_['G_ID'][::2]:\n    temp = GetX(gi,50, taza_.iloc[:,:-1])\n    temp = temp.makeDf()\n    if(type(temp)!=list):\n        X.append(temp)\n\ndf = pd.DataFrame(X)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look This table.\nFirst 17 columns are away team's information. And '~_VS' columns are about defend information. For example, in first row, 'RUN' means SS(SamSung) team's recent 50 games weighted mean of RUN score(=5.57) and 'RUN_VS' means SS team's recent 50 games weighted mean of loss RUN score. <br>\n\nAnd 18 ~ 34 columns('~_ENE') are home team's information. 'WIN' column is the match result of **'away team'**<br>\n'RELATIVE_WIN' is win ratio of away team vs home team(SS vs LG in first row) while playing 50 games.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And this table is 1 ~ 17 columns(away team's information) - 18 ~ 34 columns data(home team's information)<br>\nI'll use this data for modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"gapLeftRight(df).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make train, test data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#2015 ~ 2019(train set)\ndfX = pd.DataFrame()\nfor year in years:\n    taza_ = taza[taza['YEAR']==year]\n\n    X = []\n    for gi in taza_['G_ID'][::2]:\n        temp = GetX(gi,50, taza_.iloc[:,:-1])\n        temp = temp.makeDf()\n        if(type(temp)!=list):\n            X.append(temp)\n\n    df = pd.DataFrame(X)\n    dfX = dfX.append(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = gapLeftRight(dfX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2020(test set)\ntaza_ = pd.read_csv(\"baseball_2020.csv\")\ntaza_ = taza_[colnames]\ntaza_['YEAR']=taza_['G_ID'].str[:4]\n\nX = []\nfor gi in taza_['G_ID'][::2]:\n    temp = GetX(gi,50, taza_.iloc[:,:-1])\n    temp = temp.makeDf()\n    if(type(temp)!=list):\n        X.append(temp)\n\ndf = pd.DataFrame(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = gapLeftRight(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling\nI'll use logistic reg, decision tree, random forest, deep learning models for prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove draw cases\ntrain = train[train['WIN'] !=0.5].reset_index(drop=True)\ntest = test[test['WIN'] !=0.5].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logisticReg(XTrain, yTrain, XValid, yValid):\n    clf = LogisticRegression(random_state=0).fit(XTrain, yTrain)\n    pred = clf.predict(XValid)\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decisionT(XTrain, yTrain, XValid, yValid, mD, cT):\n    clf = tree.DecisionTreeClassifier(max_depth = mD, criterion=cT)\n    clf = clf.fit(XTrain, yTrain)\n    pred = clf.predict(XValid)\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def randomF(XTrain,yTrain, XValid, yValid, mD):\n    rf = RandomForestClassifier(n_estimators=mD)\n    rf.fit(XTrain, yTrain)\n    pred = rf.predict(XValid)\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def DNN(XTrain,yTrain, XValid, yValid, lr, bs):\n\n#     XTrain = pd.DataFrame(preprocessing.scale(XTrain))\n#     XValid = pd.DataFrame(preprocessing.scale(XValid))\n    \n    model = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(XValid.shape[1], )),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    optimizer = tf.keras.optimizers.Adagrad(lr=lr)\n\n    model.compile(optimizer=optimizer, loss='binary_crossentropy',\n                 metrics=['accuracy'])\n\n    model.fit(XTrain,yTrain, epochs=20,batch_size=bs, verbose=0)\n\n    \n    return model.predict(XValid).flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can validate model by cross validation.<br>\nAnd evaluaction function is **accuracy**."},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracyList=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" for trainIdx, validIdx in kf.split(train):\n        #shulffing\n        train_ = train.iloc[trainIdx].sample(len(trainIdx))\n        valid_ = train.iloc[validIdx].sample(len(validIdx))\n\n        XTrain = train_.iloc[:,:-2]\n        yTrain = train_.iloc[:,-1]\n\n        XTest = valid_.iloc[:,:-2]\n        yTest = valid_.iloc[:,-1]\n\n        lg = logisticReg(XTrain,yTrain,XTest,yTest)\n        accuracyList.append(np.mean(yTest==lg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean accuracy of logistic reg is about 55%"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(accuracyList)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision tree, random forest, dnn need hyper parameter tuning. So I'll tune these roughly"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtResult = {\"maxDepth\":[],\"accuracy\":[],\"criterion\":[]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for mD in tqdm(range(5,101,5)):\n    for cT in ['gini','entropy']:\n        dtResult['maxDepth'].append(mD)\n        dtResult['criterion'].append(cT)\n        \n        accuracyList = []\n        for trainIdx, validIdx in kf.split(train):\n            #shulffing\n            train_ = train.iloc[trainIdx].sample(len(trainIdx))\n            valid_ = train.iloc[validIdx].sample(len(validIdx))\n\n            XTrain = train_.iloc[:,:-2]\n            yTrain = train_.iloc[:,-1]\n\n            XTest = valid_.iloc[:,:-2]\n            yTest = valid_.iloc[:,-1]\n            \n            dt = decisionT(XTrain,yTrain,XTest,yTest, mD, cT)\n            accuracyList.append(np.mean(yTest==dt))\n        dtResult['accuracy'].append(np.mean(accuracyList))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtResult=pd.DataFrame(dtResult)\n\ndtResult.iplot(mode='lines',x='maxDepth', y='accuracy', categories='criterion',\n              xTitle='depth', yTitle='accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Max_depth = 5, criterion = entropy is best but accuracy score is lower than logistic reg"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfResult = {\"maxDepth\":[],\"accuracy\":[]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for mD in tqdm(range(5,101,5)):\n    \n    rfResult['maxDepth'].append(mD)\n    accuracyList = []\n    for trainIdx, validIdx in kf.split(train):\n        #shulffing\n        train_ = train.iloc[trainIdx].sample(len(trainIdx))\n        valid_ = train.iloc[validIdx].sample(len(validIdx))\n\n        XTrain = train_.iloc[:,:-2]\n        yTrain = train_.iloc[:,-1]\n\n        XTest = valid_.iloc[:,:-2]\n        yTest = valid_.iloc[:,-1]\n\n        rf = randomF(XTrain,yTrain,XTest,yTest, mD)\n        accuracyList.append(np.mean(yTest==rf))\n    rfResult['accuracy'].append(np.mean(accuracyList))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfResult=pd.DataFrame(rfResult)\n\nrfResult.iplot(mode='lines',x='maxDepth', y='accuracy',\n              xTitle='depth', yTitle='accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest is also bad result"},{"metadata":{"trusted":true},"cell_type":"code","source":"dnResult = {\"learningRate\":[],'batchSize':[],\"accuracy\":[]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for lr in tqdm([0.05, 0.01, 0.005, 0.001]):\n    for bS in [10,50,100,200]:\n        dnResult['learningRate'].append(lr)\n        dnResult['batchSize'].append(bS)\n        \n        accuracyList = []\n        for trainIdx, validIdx in kf.split(train):\n            #shulffing\n            train_ = train.iloc[trainIdx].sample(len(trainIdx))\n            valid_ = train.iloc[validIdx].sample(len(validIdx))\n\n            XTrain = train_.iloc[:,:-2]\n            yTrain = train_.iloc[:,-1]\n\n            XTest = valid_.iloc[:,:-2]\n            yTest = valid_.iloc[:,-1]\n            \n            dn = DNN(XTrain,yTrain,XTest,yTest, lr, bS)\n            accuracyList.append(np.mean(yTest ==(dn>0.5)*1))\n        dnResult['accuracy'].append(np.mean(accuracyList))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dnResult=pd.DataFrame(dnResult)\n\ndnResult.iplot(mode='lines',x='batchSize', y='accuracy', categories = 'learningRate',\n              xTitle='batchSize', yTitle='accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deep learning's result is best of 4 methods. But I afraid of overfitting.<br>\nlearning rate = 0.01, batchsize = 100 is best case."},{"metadata":{},"cell_type":"markdown","source":"### Predict using 2020 kbo data"},{"metadata":{"trusted":true},"cell_type":"code","source":"XTrain = train.iloc[:,:-2]\nyTrain = train.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XTest = test.iloc[:,:-2]\nyTest = test.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lR = LogisticRegression(random_state=0).fit(XTrain, yTrain)\nlogisticResult = np.mean(lR.predict(XTest) == yTest)\n\ndtR = tree.DecisionTreeClassifier(max_depth = 5, criterion='entropy').fit(XTrain, yTrain)\ndecisionResult = np.mean(dtR.predict(XTest)==yTest)\n\nrfR = RandomForestClassifier(n_estimators=45)\nrfR.fit(XTrain, yTrain)\nrandomResult = np.mean(rfR.predict(XTest)==yTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\ntf.keras.layers.Flatten(input_shape=(XTrain.shape[1], )),\ntf.keras.layers.Dense(256, activation='relu'),\ntf.keras.layers.Dense(200, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(128, activation='relu'),\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(32, activation='relu'),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(16, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\n\noptimizer = tf.keras.optimizers.Adagrad(lr=0.01)\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy',\n             metrics=['accuracy'])\n\nmodel.fit(XTrain,yTrain, epochs=20,batch_size=100, verbose=0)\ndnResult = np.mean((model.predict(XTest)>0.5).flatten()*1 == yTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"logistic regression : \",  logisticResult,\"\\n\",\n     \"decision tree : \", decisionResult, \"\\n\",\n     \"random forest : \", randomResult, \"\\n\",\n      \"deep learning : \", dnResult)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deep learning is best of all but terribly bad result i think... <br>\nIt is almost same result by predict all result as 0.<br>\nMore adequate preprocessing(such as PCA) or tuning parmeter is required.<br>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}