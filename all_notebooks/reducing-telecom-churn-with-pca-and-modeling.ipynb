{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## Telecom Churn: Logistic Regression with PCA\n\nWith 21 predictor variables, we need to predict whether a particular customer will switch to another telecom provider or not. In telecom terminology, customer attrition is referred to as 'churn'."},{"metadata":{},"cell_type":"markdown","source":"### Importing and Merging Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Pandas and NumPy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing all datasets\nchurn_data = pd.read_csv(\"../input/logit-churn-tele/churn_data.csv\")\ncustomer_data = pd.read_csv(\"../input/logit-churn-tele/customer_data.csv\")\ninternet_data = pd.read_csv(\"../input/logit-churn-tele/internet_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(churn_data))\nprint(len(customer_data))\nprint(len(internet_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging on 'customerID'\ndf_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final dataframe with all predictor variables\ntelecom = pd.merge(df_1, internet_data, how='inner', on='customerID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's understand the structure of our dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of our master dataset\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Yes to 1 and No to 0\ntelecom['PhoneService'] = telecom['PhoneService'].map({'Yes': 1, 'No': 0})\ntelecom['PaperlessBilling'] = telecom['PaperlessBilling'].map({'Yes': 1, 'No': 0})\ntelecom['Churn'] = telecom['Churn'].map({'Yes': 1, 'No': 0})\ntelecom['Partner'] = telecom['Partner'].map({'Yes': 1, 'No': 0})\ntelecom['Dependents'] = telecom['Dependents'].map({'Yes': 1, 'No': 0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy Variable Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dummy variable for the variable 'Contract' and dropping the first one.\ncont = pd.get_dummies(telecom['Contract'],prefix='Contract',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,cont],axis=1)\n\n# Creating a dummy variable for the variable 'PaymentMethod' and dropping the first one.\npm = pd.get_dummies(telecom['PaymentMethod'],prefix='PaymentMethod',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,pm],axis=1)\n\n# Creating a dummy variable for the variable 'gender' and dropping the first one.\ngen = pd.get_dummies(telecom['gender'],prefix='gender',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,gen],axis=1)\n\n# Creating a dummy variable for the variable 'MultipleLines' and dropping the first one.\nml = pd.get_dummies(telecom['MultipleLines'],prefix='MultipleLines')\n#  dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ml1],axis=1)\n\n# Creating a dummy variable for the variable 'InternetService' and dropping the first one.\niser = pd.get_dummies(telecom['InternetService'],prefix='InternetService',drop_first=True)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,iser],axis=1)\n\n# Creating a dummy variable for the variable 'OnlineSecurity'.\nos = pd.get_dummies(telecom['OnlineSecurity'],prefix='OnlineSecurity')\nos1= os.drop(['OnlineSecurity_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,os1],axis=1)\n\n# Creating a dummy variable for the variable 'OnlineBackup'.\nob =pd.get_dummies(telecom['OnlineBackup'],prefix='OnlineBackup')\nob1 =ob.drop(['OnlineBackup_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ob1],axis=1)\n\n# Creating a dummy variable for the variable 'DeviceProtection'. \ndp =pd.get_dummies(telecom['DeviceProtection'],prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,dp1],axis=1)\n\n# Creating a dummy variable for the variable 'TechSupport'. \nts =pd.get_dummies(telecom['TechSupport'],prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ts1],axis=1)\n\n# Creating a dummy variable for the variable 'StreamingTV'.\nst =pd.get_dummies(telecom['StreamingTV'],prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,st1],axis=1)\n\n# Creating a dummy variable for the variable 'StreamingMovies'. \nsm =pd.get_dummies(telecom['StreamingMovies'],prefix='StreamingMovies')\nsm1 = sm.drop(['StreamingMovies_No internet service'],1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,sm1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping the repeated variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have created dummies for the below variables, so we can drop them\ntelecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The varaible was imported as a string we need to convert it to float\ntelecom['TotalCharges'] =telecom['TotalCharges'].convert_objects(convert_numeric=True)\n#telecom['tenure'] = telecom['tenure'].astype(int).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see we have all variables as integer."},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Checking for Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for outliers in the continuous variables\nnum_telecom = telecom[['tenure','MonthlyCharges','SeniorCitizen','TotalCharges']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\nnum_telecom.describe(percentiles=[.25,.5,.75,.90,.95,.99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the distribution shown above, you can see that there no outliner in your data. The numbers are gradually increasing."},{"metadata":{},"cell_type":"markdown","source":"### Checking for Missing Values and Inputing Them"},{"metadata":{},"cell_type":"markdown","source":"It means that 11/7043 = 0.001561834 i.e 0.1%, best is to remove these observations from the analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the percentage of missing values\nround(100*(telecom.isnull().sum()/len(telecom.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing NaN TotalCharges rows\ntelecom = telecom[~np.isnan(telecom['TotalCharges'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking percentage of missing values after removing the missing values\nround(100*(telecom.isnull().sum()/len(telecom.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we don't have any missing values"},{"metadata":{},"cell_type":"markdown","source":"### Feature Standardisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalising continuous features\ndf = telecom[['tenure','MonthlyCharges','TotalCharges']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_df=(df-df.mean())/df.std()\ntelecom = telecom.drop(['tenure','MonthlyCharges','TotalCharges'], 1)\ntelecom = pd.concat([telecom,normalized_df],axis=1)\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the Churn Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"churn = (sum(telecom['Churn'])/len(telecom['Churn'].index))*100\nchurn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have almost 27% churn rate"},{"metadata":{},"cell_type":"markdown","source":"## Model Building\nLet's start by splitting our data into a training set and a test set."},{"metadata":{},"cell_type":"markdown","source":"### Splitting Data into Training and Test Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = telecom.drop(['Churn','customerID'],axis=1)\n\n# Putting response variable to y\ny = telecom['Churn']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running Your First Training Model"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(telecom.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping highly correlated variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test2 = X_test.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No'],1)\nX_train2 = X_train.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No'],1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the Correlation Matrix"},{"metadata":{},"cell_type":"markdown","source":"After dropping highly correlated variables now let's check the correlation matrix again."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))\nsns.heatmap(X_train2.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Re-Running the Model"},{"metadata":{},"cell_type":"markdown","source":"Now let's run our model again after dropping highly correlated variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"logm2 = sm.GLM(y_train,(sm.add_constant(X_train2)), family = sm.families.Binomial())\nlogm2.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection Using RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 13)             # running RFE with 13 variables as output\nrfe = rfe.fit(X,y)\nprint(rfe.support_)           # Printing the boolean results\nprint(rfe.ranking_)           # Printing the ranking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables selected by RFE \ncol = ['PhoneService', 'PaperlessBilling', 'Contract_One year', 'Contract_Two year',\n       'PaymentMethod_Electronic check','MultipleLines_No','InternetService_Fiber optic', 'InternetService_No',\n       'OnlineSecurity_Yes','TechSupport_Yes','StreamingMovies_No','tenure','TotalCharges']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's run the model using the selected variables\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogsk = LogisticRegression(C=1e9)\n#logsk.fit(X_train[col], y_train)\nlogsk.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparing the model with StatsModels\n#logm4 = sm.GLM(y_train,(sm.add_constant(X_train[col])), family = sm.families.Binomial())\nlogm4 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nmodres = logm4.fit()\nlogm4.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[col].shape\n#res = modres.predict(X_test[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted probabilities\ny_pred = logsk.predict_proba(X_test)\n# Converting y_pred to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_pred)\n# Converting to column dataframe\ny_pred_1 = y_pred_df.iloc[:,[1]]\n# Let's see the head\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df,y_pred_1],axis=1)\n# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 1 : 'Churn_Prob'})\n# Rearranging the columns\ny_pred_final = y_pred_final.reindex_axis(['CustID','Churn','Churn_Prob'], axis=1)\n# Let's see the head of y_pred_final\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\ny_pred_final['predicted'] = y_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.5 else 0)\n# Let's see the head\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix( y_pred_final.Churn, y_pred_final.predicted )\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted     Churn  not_churn  __all__\n# Actual\n# Churn            1359   169     1528\n# not_churn         256   326      582\n# __all__          1615   751     2110","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_pred_final.Churn, y_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#draw_roc(y_pred_final.Churn, y_pred_final.predicted)\n\"{:2.2f}\".format(metrics.roc_auc_score(y_pred_final.Churn, y_pred_final.Churn_Prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We see an overall AUC score of 0.83 looks like we did a decent job.\n- But we did spend a lot of effort on the features and their selection.\n- Can PCA help reduce our effort?"},{"metadata":{},"cell_type":"markdown","source":"### PCA on the data"},{"metadata":{},"cell_type":"markdown","source":"#### Note - \n- While computng the principal components, we must not include the entire dataset. Model building is all about doing well on the data we haven't seen yet!\n- So we'll calculate the PCs using the train data, and apply them later on the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape\n# We have 30 variables after creating our dummy variables for our categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Doing the PCA on the train data\npca.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's plot the principal components and try to make sense of them\n- We'll plot original features on the first 2 principal components as axes"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames = list(X_train.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\npcs_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the fist component is in the direction where the 'charges' variables are heavy\n - These 3 components also have the highest of the loadings"},{"metadata":{},"cell_type":"markdown","source":"#### Looking at the screeplot to assess the number of needed principal components"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looks like 16 components are enough to describe 95% of the variance in the dataset\n- We'll choose 16 components for our modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Basis transformation - getting the data onto our PCs"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train_pca = pca_final.fit_transform(X_train)\ndf_train_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating correlation matrix for the principal components - we expect little to no correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_train_pca.transpose())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (20,10))\nsns.heatmap(corrmat,annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1s -> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Indeed - there is no correlation between any two components! Good job, PCA!\n- We effectively have removed multicollinearity from our situation, and our models will be much more stable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying selected components to the test data - 16 components\ndf_test_pca = pca_final.transform(X_test)\ndf_test_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Applying a logistic regression on our Principal Components\n- We expect to get similar model performance with significantly lower features\n- If we can do so, we would have done effective dimensionality reduction without losing any import information"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model on the train data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlearner_pca = LogisticRegression()\nmodel_pca = learner_pca.fit(df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**\n\nNote that we are fitting the original variable y with the transformed variables (principal components). This is not a problem becuase the transformation done in PCA is *linear*, which implies that you've only changed the way the new x variables are represented, though the nature of relationship between X and Y is still linear. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making prediction on the test data\npred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Impressive! The same result, without all the hard work on feature selection!"},{"metadata":{},"cell_type":"markdown","source":"Why not take it a step further and get a little more 'unsupervised' in our approach?\nThis time, we'll let PCA select the number of components basen on a variance cutoff we provide"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pca_again = PCA(0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_pca2 = pca_again.fit_transform(X_train)\ndf_train_pca2.shape\n# we see that PCA selected 14 components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training the regression model\nlearner_pca2 = LogisticRegression()\nmodel_pca2 = learner_pca2.fit(df_train_pca2,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_pca2 = pca_again.transform(X_test)\ndf_test_pca2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making prediction on the test data\npred_probs_test2 = model_pca2.predict_proba(df_test_pca2)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So there it is - a very similar result, without all the hassles. We have not only achieved dimensionality reduction, but also saved a lot of effort on feature selection."},{"metadata":{},"cell_type":"markdown","source":"#### Before closing, let's also visualize the data to see if we can spot any patterns"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(df_train_pca[:,0], df_train_pca[:,1], c = y_train.map({0:'green',1:'red'}))\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there is a good amount of separation in 2D, but probably not enough\n\nLet's look at it in 3D, and we expect spread to be better (dimensions of variance, remember?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\n# ax = plt.axes(projection='3d')\nax.scatter(df_train_pca[:,2], df_train_pca[:,0], df_train_pca[:,1], c=y_train.map({0:'green',1:'red'}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So let's try building the model with just 3 principal components!"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_last = PCA(n_components=3)\ndf_train_pca3 = pca_last.fit_transform(X_train)\ndf_test_pca3 = pca_last.transform(X_test)\ndf_test_pca3.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#training the regression model\nlearner_pca3 = LogisticRegression()\nmodel_pca3 = learner_pca3.fit(df_train_pca3,y_train)\n#Making prediction on the test data\npred_probs_test3 = model_pca3.predict_proba(df_test_pca3)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 0.82! Isn't that just amazing!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}