{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\n#for sentiment analysis\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom wordcloud import WordCloud\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\n\nfrom imblearn.over_sampling import SMOTE,SVMSMOTE,ADASYN\n\nimport joblib\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\n\nfrom gensim.models import KeyedVectors\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/robinhood-app-reviews-on-google-play-store/Robinhood_GooglePlay_Reviews_enUS.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(subset=['content'], inplace = True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few quick things to note here:\n* **userName**: a lot of reviewers do not show their names and only appear as \"A Google User\"\n* **content** : not all review contents are unique, and from here we can speculate that many of them are probably one-(or few-) word reviews, thus the duplicates\n* **score**: there are a lot of 1s. In fact 50% of all scores are <= 1 (or just 1 in this case because there's no 0 score)\n* **replyContent**: replies from the company are large canned, which results in a rather low number of unique replies as we can see from the table","metadata":{}},{"cell_type":"markdown","source":"# Distributions of Scores","metadata":{}},{"cell_type":"code","source":"sns.set_context('paper')\nax = sns.displot(df['score'], kde = False, color = 'seagreen', bins = 5)\nax.fig.set_figwidth(15)\nax.fig.set_figheight(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the histogram, we can confirm that indeed most scores are 1. A score of 5 is the second most popular, and 3 (a neutral score) is the least likely. This means this app gets very polarizing reviews: it is either \"loved\" (score of 5) or \"hated\" (score of 1).","metadata":{}},{"cell_type":"markdown","source":"# Datetime Distribution of when 1 ☆ happened","metadata":{}},{"cell_type":"code","source":"df['datetime64'] = df['at'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')) #turn the string representation of datetime into datetime type","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_1 = df[df['score'] == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_1.groupby([score_1[\"datetime64\"].dt.year]).count().plot(kind = \"pie\",y = 'datetime64', figsize=(10,10), colormap='Spectral')\nplt.title(\"'When did 1☆ happen?'\", fontsize = 18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is immediately evident from this pie chart that most 1star reviews are from the year of 2021. We can continue to restrict the time range to specific months and days of 2021.","metadata":{}},{"cell_type":"code","source":"ax = score_1[(score_1['datetime64'] > pd.to_datetime('2021-01-01')) & (score_1['datetime64'] < pd.to_datetime('2021-04-30'))].groupby([score_1[\"datetime64\"].dt.month,score_1[\"datetime64\"].dt.day]).count().plot(kind = \"bar\", y = \"datetime64\",  figsize=(20,10), colormap = 'Spectral')\nplt.xlabel('Date', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.title(\"'When did 1☆ happen?'\", fontsize = 18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This agrees perfectly with the fact that Robinhood started [restricting GME transactions](https://www.bloomberg.com/news/articles/2021-01-28/robinhood-clients-report-trading-restrictions-on-gamestop-amc) on Jan 28th, which caused a major upset and prompted users to leave bad reviews and boycott the app. Most 1☆ reviews of 2021 come from the week of Jan 28th as we see almost no reviews on other dates. It is also noteworthy that the total counts of 1☆ reviews on Jan 28th and Jan 29th (\\~70k) make up more than half of the all-time total of 1☆ reviews (\\~135k, which can be confirmed using describe() on the **score_1** dataframe). ","metadata":{}},{"cell_type":"code","source":"score_1.describe(include = \"all\", datetime_is_numeric=True)['score']['count']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On another hand, if we explore the 5☆ reviews we see a very balanced distribution across the years (except for 2015 when the app first came out).","metadata":{}},{"cell_type":"code","source":"df[df['score'] == 5].groupby([df[\"datetime64\"].dt.year]).count().plot(kind = \"pie\",y = 'datetime64', figsize=(10,10), colormap='Spectral')\nplt.title(\"'When did 5☆ happen?'\", fontsize = 18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis: Building the dataset and quick look with Wordcloud","metadata":{}},{"cell_type":"markdown","source":"First let's assign labels for reviews based on the score:\n\n* Positive (label 1): a score of 4 or 5\n* Negative (label 0): a score of 1 or 2\n* Neutral (label 2): a score of 3","metadata":{}},{"cell_type":"code","source":"def assign_label(x):\n    if x >= 4:\n        return 1\n    elif x <= 2:\n        return 0\n    else:\n        return 2\n    \ndf['label'] = df['score'].apply(assign_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = [label for label in df['label']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can focus on the NLP part of the sentiment analysis. Below we will do multiple modifications to make the reviews more \"readable\" by machine learning: \n1. make everything lowercase\n2. remove bad symbols (unwanted characters)\n3. expand contractions (for example: don't -> do not)\n4. remove punctuations\n5. remove stopwords","metadata":{}},{"cell_type":"code","source":"contractions = pd.read_json('../input/english-contractions/contractions.json', typ='series') #getting the list of English contractions\ncontractions = contractions.to_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return contractions[match.group(0)]\n    return c_re.sub(replace, text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n\ndef clean_reviews(reviews):\n    cleaned_reviews = []\n    for review in reviews:\n        review = str(review)\n        review = review.lower()\n        review = BAD_SYMBOLS_RE.sub(' ', review)\n        \n        #expand contraction\n        review = expandContractions(review)\n\n        #remove punctuation\n        review = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", review).split())\n\n        #stop words\n        stop_words = set(stopwords.words('english'))\n        word_tokens = nltk.word_tokenize(review) \n        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n        review = ' '.join(filtered_sentence)\n        \n        cleaned_reviews.append(review)\n        \n    return cleaned_reviews","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = clean_reviews([review for review in df['content']]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = [x for x in X]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With WordCloud we can see what words/phrases come up the most for different sentiments:","metadata":{}},{"cell_type":"code","source":"textt = \" \".join(X[i] for i in range(len(X)) if Y[i] == 1) #only take into account positive reviews\nwordcloud = WordCloud(width = 512,height = 512, collocations=True, colormap=\"Greens\").generate(textt)\nplt.figure(figsize = (10, 10), facecolor = 'k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For positive reviews we see a lot of \"good\" words: great, user friendly, nice, easy, fun, etc. The most notable is \"easy use\", which imho is a true statement of how streamlined and simple the app is to trade. ","metadata":{}},{"cell_type":"code","source":"textt = \" \".join(X[i] for i in range(len(X)) if Y[i] == 0) #only take into account negative reviews\nwordcloud = WordCloud(width = 512,height = 512, collocations=True, colormap=\"Reds\").generate(textt)\nplt.figure(figsize = (10, 10), facecolor = 'k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bad reviews are filled with words such as: trash, criminal, lose money, poor, corrupt. But most significant are: **market manipulation** and **hedge fund**. These are very consistent with why users boycotted the app: because they believed it was controlled by hedge funds and manipulated the supposedly \"free\" market for their own good.","metadata":{}},{"cell_type":"code","source":"textt = \" \".join(X[i] for i in range(len(X)) if Y[i] == 2) #only take into account neutral reviews\nwordcloud = WordCloud(width = 512,height = 512, collocations=True, colormap=\"Oranges\").generate(textt)\nplt.figure(figsize = (10, 10), facecolor = 'k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally neutral reviews have mostly very generic, non-sentimental words. ","metadata":{}},{"cell_type":"markdown","source":"# Sentiment Analysis: Positive vs. Negative with Naive Bayes","metadata":{}},{"cell_type":"code","source":"MAX_SEQ_LENGTH = max(len(text.split()) for text in df['text']) #maximum length of a review\nMAX_FEATURES = 10000 #only consider the top 10000 most frequent words in the corpus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1 = df[df['score'] != 3] #disregard all neutral reviews","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenize and vectorize reviews\ncorpus = df_1['text'].values.astype('U') \ntfidf = TfidfVectorizer(max_features = MAX_FEATURES, ngram_range = (1, 2))   \ntdidf_tensor = tfidf.fit_transform(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that when we vectorize the corpus we consider both unigrams and bigrams (two consecutive words), and only consider the top 10000 most frequent words in the corpus. ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(tdidf_tensor, df_1['label'].values, test_size = 0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = nb.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Y_test, predictions, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using a very simple model we reach an accuracy of near 95% and acceptable F1 scores for both classes. Results for 0 (negative) is understandably better because we have twice as many samples compared to 1 (postive). The next challenge is to incorporate neutral reviews into the classification. We will have two challenges to overcome:  \n1. neutral reviews can be ambiguous and hard to recognize\n2. there are very few neutral reviews to train","metadata":{}},{"cell_type":"markdown","source":"# Sentiment Analysis: 3 classes - Baseline model with tf-idf and linear SVM","metadata":{}},{"cell_type":"code","source":"#tokenize and vectorize reviews\ncorpus = df['text'].values.astype('U') \ntfidf = TfidfVectorizer(max_features = MAX_FEATURES, ngram_range = (1, 2))  \ntdidf_tensor = tfidf.fit_transform(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(tdidf_tensor, df['label'].values, test_size = 0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model = SVC(kernel = 'linear', decision_function_shape = 'ovo') #for multi-class classification\nbaseline_model.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = baseline_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Y_test, predictions, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the first look, 89% accuracy sounds awesome for this baseline model. However, because it does not predict anything to be \"neutral\", we have reasons to suspect that this only reflects the underlying class distribution, i.e. because there is such a small number of neutral reviews for training, the model will just predict everything to be either \"positive\" or \"negative\", hence the high accuracy.\n\nHere we have a case of a significant unbalance in the training set leading to a classification model that heavily biases towards the more commons classes. ","metadata":{}},{"cell_type":"markdown","source":"# Baseline model using balanced class_weight parameter","metadata":{}},{"cell_type":"code","source":"baseline_model_2 = SVC(kernel = 'linear', class_weight = 'balanced', decision_function_shape = 'ovo') #setting the class weight to be balanced\nbaseline_model_2.fit(X_train, Y_train)\npredictions = baseline_model_2.predict(X_test)\nprint(classification_report(Y_test, predictions, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This classifier does recognize some neutral reviews, but at the cost of lowering F1 scores for the other two classes and the overall accuracy. It's easy to see that it misclassifies a lot of positive and negative reviews as neutral, resulting in a rather abysmal precision score of ~10% for the neutral class. This calls for a better way to treat the unbalance. ","metadata":{}},{"cell_type":"markdown","source":"# Oversampling with SMOTE","metadata":{}},{"cell_type":"code","source":"smote = SVMSMOTE()\nX_train_smote, Y_train_smote = smote.fit_resample(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_smote = SVC(kernel = 'linear', decision_function_shape = 'ovo')\nsvm_smote.fit(X_train_smote, Y_train_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = svm_smote.predict(X_test)\nprint(classification_report(Y_test, predictions, digits=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Oversampling with ADASYN","metadata":{}},{"cell_type":"code","source":"adasyn = ADASYN()\nX_train_adasyn, Y_train_adasyn = adasyn.fit_resample(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_adasyn = SVC(kernel = 'linear', decision_function_shape = 'ovo')\nsvm_adasyn.fit(X_train_adasyn, Y_train_adasyn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = svm_adasyn.predict(X_test)\nprint(classification_report(Y_test, predictions, digits=3))","metadata":{},"execution_count":null,"outputs":[]}]}