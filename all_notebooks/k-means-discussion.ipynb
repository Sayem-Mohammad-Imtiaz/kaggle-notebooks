{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# KMeans\n\nK-Means is the 'go-to' clustering algorithm for many simply because it is fast, easy to understand, and available everywhere (there's an implementation in almost any statistical or machine learning tool you care to use). K-Means has a few problems however. The first is that it isn't a clustering algorithm, it is a partitioning algorithm. That is to say K-means doesn't 'find clusters' it partitions your dataset into as many (assumed to be globular) chunks as you ask for by attempting to minimize intra-partition distances. That leads to the second problem: you need to specify exactly how many clusters you expect. If you know a lot about your data then that is something you might expect to know. If, on the other hand, you are simply exploring a new dataset then 'number of clusters' is a hard parameter to have any good intuition for. The usually proposed solution is to run K-Means for many different 'number of clusters' values and score each clustering with some 'cluster goodness' measure (usually a variation on intra-cluster vs inter-cluster distances) and attempt to find an 'elbow'. If you've ever done this in practice you know that finding said elbow is usually not so easy, nor does it necessarily correlate as well with the actual 'natural' number of clusters as you might like. Finally K-Means is also dependent upon initialization; give it multiple different random starts and you can get multiple different clusterings. This does not engender much confidence in any individual clustering that may result.\n\nSo, in summary, here's how K-Means seems to stack up against out desiderata:\n\n- **Don't be wrong!**: K-means is going to throw points into clusters whether they belong or not; it also assumes you clusters are globular. K-Means scores very poorly on this point.\n- **Intuitive parameters**: If you have a good intuition for how many clusters the dataset your exploring has then great, otherwise you might have a problem.\n- **Stability**: Hopefully the clustering is stable for your data. Best to have many runs and check though.\n- **Performance**: This is K-Means big win. It's a simple algorithm and with the right tricks and optimizations can be made exceptionally efficient. There are few algorithms that can compete with K-Means for performance. If you have truly huge data then K-Means might be your only option.\nBut enough opinion, how does K-Means perform on our test dataset? Let's have look. We'll be generous and use our knowledge that there are six natural clusters and give that to K-Means.\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nimport seaborn as sns\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.cluster import KMeans\n\nfrom scipy.stats import zscore\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the datset\n\ntech_supp_df = pd.read_csv(\"/kaggle/input/technical-customer-support-data/technical_support_data.csv\")\ntech_supp_df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The dataset contains one record for each unique problem type. It has metrics for each type like count, average calls to resolve, average resolution time etc.**","metadata":{}},{"cell_type":"code","source":"#Shape of the dataset\ntech_supp_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displaying the first five rows of dataset \ntech_supp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotiing the pairplot\ntechSuppAttr=tech_supp_df.iloc[:,1:]\ntechSuppScaled=techSuppAttr.apply(zscore)\nsns.pairplot(techSuppScaled,diag_kind='kde')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Group Data into similar clusters\n\nNow, we will use K-Means clustering to group data based on their attribute. First, we need to determine the optimal number of groups. For that we conduct the knee test to see where the knee happens.","metadata":{}},{"cell_type":"code","source":"#Finding optimal no. of clusters\nfrom scipy.spatial.distance import cdist\nclusters=range(1,10)\nmeanDistortions=[]\n\nfor k in clusters:\n    model=KMeans(n_clusters=k)\n    model.fit(techSuppScaled)\n    prediction=model.predict(techSuppScaled)\n    meanDistortions.append(sum(np.min(cdist(techSuppScaled, model.cluster_centers_, 'euclidean'), axis=1)) / techSuppScaled.shape[0])\n\n\nplt.plot(clusters, meanDistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average distortion')\nplt.title('Selecting k with the Elbow Method')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Though the bend is not coming out clearly as there are many bends, let us look at 3 clusters and 5 clusters","metadata":{}},{"cell_type":"code","source":"# Let us first start with K = 3\nfinal_model=KMeans(3)\nfinal_model.fit(techSuppScaled)\nprediction=final_model.predict(techSuppScaled)\n\n#Append the prediction \ntech_supp_df[\"GROUP\"] = prediction\ntechSuppScaled[\"GROUP\"] = prediction\nprint(\"Groups Assigned : \\n\")\ntech_supp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyze the distribution of the data among the two groups (K = 3). One of the most informative visual tool is boxplot.","metadata":{}},{"cell_type":"code","source":"techSuppClust = tech_supp_df.groupby(['GROUP'])\ntechSuppClust.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"techSuppScaled.boxplot(by='GROUP', layout = (2,4),figsize=(15,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Let us next try with K = 6, the next elbow point","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us first start with K = 6\nfinal_model=KMeans(6)\nfinal_model.fit(techSuppScaled)\nprediction=final_model.predict(techSuppScaled)\n\n#Append the prediction \ntech_supp_df[\"GROUP\"] = prediction\ntechSuppScaled[\"GROUP\"] = prediction\nprint(\"Groups Assigned : \\n\")\ntech_supp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"techSuppClust = tech_supp_df.groupby(['GROUP'])\ntechSuppClust.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"techSuppScaled.boxplot(by='GROUP', layout = (2,4),figsize=(15,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}