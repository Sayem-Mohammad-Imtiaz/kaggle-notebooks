{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 安装aix360\n!pip install git+https://github.com/Trusted-AI/AIX360.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入包\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nimport xgboost\nimport os\nimport warnings\nimport tensorflow as tf\nfrom keras.models import Sequential, Model, load_model, model_from_json\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint('x' in np.arange(5))   #returns False, without Warning\nfrom sklearn.model_selection import train_test_split\nfrom aix360.algorithms.contrastive import CEMExplainer, KerasClassifier\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入数据并区分X和y\nheloc = pd.read_csv('../input/home-equity-line-of-creditheloc/heloc_dataset_v1 (1).csv')\nX = heloc.drop(columns = 'RiskPerformance')\ny = heloc.RiskPerformance.replace(to_replace=['Bad', 'Good'], value=[1, 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 切分训练集及测试集并转换成array\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=99)\nX_train_array = np.array(X_train)\nX_test_array = np.array(X_test)\ny_train_array = np.array(y_train)\ny_test_array = np.array(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##########################################################################################################################","metadata":{}},{"cell_type":"code","source":"Data = heloc\nx_train = X_train_array\nx_test  = X_test_array\ny_train_b = y_train_array\ny_test_b  = y_test_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Z = np.vstack((x_train, x_test))\nZmax = np.max(Z, axis=0)\nZmin = np.min(Z, axis=0)\n\n#normalize an array of samples to range [-0.5, 0.5]\ndef normalize(V):\n    VN = (V - Zmin)/(Zmax - Zmin)\n    VN = VN - 0.5\n    return(VN)\n    \n# rescale a sample to recover original values for normalized values. \ndef rescale(X):\n    return(np.multiply ( X + 0.5, (Zmax - Zmin) ) + Zmin)\n\nN = normalize(Z)\nxn_train = N[0:x_train.shape[0], :]\nxn_test  = N[x_train.shape[0]:, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nn with no softmax\ndef nn_small():\n    model = Sequential()\n    model.add(Dense(10, input_dim=23, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(2, kernel_initializer='normal'))    \n    return model    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seeds for repeatability\nnp.random.seed(1) \ntf.set_random_seed(2) \n\nclass_names = ['Bad', 'Good']\n\n# loss function\ndef fn(correct, predicted):\n    return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=predicted)\n\n# compile and print model summary\nnn = nn_small()\nnn.compile(loss=fn, optimizer='adam', metrics=['accuracy'])\nnn.summary()\n\n\n# train model or load a trained model\nTRAIN_MODEL = False\n\nif (TRAIN_MODEL):             \n    nn.fit(xn_train, y_train_b, batch_size=128, epochs=500, verbose=1, shuffle=False)\n    nn.save_weights(\"../input/heloc-nnsmall/heloc_nnsmall (3).h5\")     \nelse:    \n    nn.load_weights(\"../input/heloc-nnsmall/heloc_nnsmall (3).h5\")\n        \n\n# evaluate model accuracy        \nscore = nn.evaluate(xn_train, y_train_b, verbose=0) #Compute training set accuracy\n#print('Train loss:', score[0])\nprint('Train accuracy:', score[1])\n\nscore = nn.evaluate(xn_test, y_test_b, verbose=0) #Compute test set accuracy\n#print('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PN ##","metadata":{}},{"cell_type":"code","source":"# Some interesting user samples to try: 2344 449 1168 1272\nidx = 1273\n\nX = xn_test[idx].reshape((1,) + xn_test[idx].shape)\nprint(\"Computing PN for Sample:\", idx)\nprint(\"Prediction made by the model:\", nn.predict_proba(X))\nprint(\"Prediction probabilities:\", class_names[np.argmax(nn.predict_proba(X))])\nprint(\"\")\n\nmymodel = KerasClassifier(nn)\nexplainer = CEMExplainer(mymodel)\n\narg_mode = 'PN' # Find pertinent negatives\narg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\narg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\narg_b = 9 # No. of updates to the coefficient of the main loss term\narg_kappa = 0.2 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\narg_beta = 1e-1 # Controls sparsity of the solution (L1 loss)\narg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\nmy_AE_model = None # Pointer to an auto-encoder\narg_alpha = 0.01 # Penalizes L2 norm of the solution\narg_threshold = 1. # Automatically turn off features <= arg_threshold if arg_threshold < 1\narg_offset = 0.5 # the model assumes classifier trained on data normalized\n                # in [-arg_offset, arg_offset] range, where arg_offset is 0 or 0.5\n# Find PN for applicant 1272\n(adv_pn, delta_pn, info_pn) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma,\n                                                            arg_alpha, arg_threshold, arg_offset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xpn = adv_pn\nclasses = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpn))], 'NIL' ]\n\nprint(\"Sample:\", idx)\nprint(\"prediction(X)\", nn.predict_proba(X), class_names[np.argmax(nn.predict_proba(X))])\nprint(\"prediction(Xpn)\", nn.predict_proba(Xpn), class_names[np.argmax(nn.predict_proba(Xpn))] )\n\n\nX_re = rescale(X) # Convert values back to original scale from normalized\nXpn_re = rescale(Xpn)\nXpn_re = np.around(Xpn_re.astype(np.double), 2)\n\ndelta_re = Xpn_re - X_re\ndelta_re = np.around(delta_re.astype(np.double), 2)\ndelta_re[np.absolute(delta_re) < 1e-4] = 0\n\nX3 = np.vstack((X_re, Xpn_re, delta_re))\n\ndfre = pd.DataFrame.from_records(X3) # Create dataframe to display original point, PN and difference (delta)\ndfre[23] = classes\n\ndf=pd.DataFrame(heloc)\ndfre.columns = df.columns\ndfre.rename(index={0:'X',1:'X_PN', 2:'(X_PN - X)'}, inplace=True)\ndfret = dfre.transpose()\n\n\ndef highlight_ce(s, col, ncols):\n    if (type(s[col]) != str):\n        if (s[col] > 0):\n            return(['background-color: yellow']*ncols)    \n    return(['background-color: white']*ncols)\n\ndfret.style.apply(highlight_ce, col='(X_PN - X)', ncols=3, axis=1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcdefaults()\nfi = abs((X-Xpn).astype('double'))/np.std(xn_train.astype('double'), axis=0) # Compute PN feature importance\nobjects = df.columns[-2::-1]\ny_pos = np.arange(len(objects))\nperformance = fi[0, -1::-1]\n\nplt.barh(y_pos, performance, align='center', alpha=0.5) # bar chart\nplt.yticks(y_pos, objects) # Display features on y-axis\nplt.xlabel('weight') # x-label\nplt.title('PN (feature importance)') # Heading\n\nplt.show() # Display PN feature importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PP ##","metadata":{}},{"cell_type":"code","source":"# Some interesting user samples to try: 9 11 24\nidx = 11\n\nX = xn_test[idx].reshape((1,) + xn_test[idx].shape)\nprint(\"Computing PP for Sample:\", idx)\nprint(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\nprint(\"Prediction probabilities:\", nn.predict_proba(X))\nprint(\"\")\n\n\nmymodel = KerasClassifier(nn)\nexplainer = CEMExplainer(mymodel)\n\narg_mode = 'PP' # Find pertinent positives\narg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\narg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\narg_b = 9 # No. of updates to the coefficient of the main loss term\narg_kappa = 0.2 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\narg_beta = 10.0 # Controls sparsity of the solution (L1 loss)\narg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\nmy_AE_model = None # Pointer to an auto-encoder\narg_alpha = 0.1 # Penalizes L2 norm of the solution\narg_threshold = 0.0 # Automatically turn off features <= arg_threshold if arg_threshold < 1\narg_offset = 0.5 # the model assumes classifier trained on data normalized\n                # in [-arg_offset, arg_offset] range, where arg_offset is 0 or 0.5\n(adv_pp, delta_pp, info_pp) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma,\n                                                            arg_alpha, arg_threshold, arg_offset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xpp = delta_pp\nclasses = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpp))]]\n\nprint(\"PP for Sample:\", idx)\nprint(\"Prediction(Xpp) :\", class_names[np.argmax(nn.predict_proba(Xpp))])\nprint(\"Prediction probabilities for Xpp:\", nn.predict_proba(Xpp))\nprint(\"\")\n\nX_re = rescale(X) # Convert values back to original scale from normalized\nadv_pp_re = rescale(adv_pp)\n# Xpp_re = X_re - adv_pp_re\nXpp_re = rescale(Xpp)\nXpp_re = np.around(Xpp_re.astype(np.double), 2)\nXpp_re[Xpp_re < 1e-4] = 0\n\nX2 = np.vstack((X_re, Xpp_re,))\n\ndf=pd.DataFrame(heloc)\ndfpp = pd.DataFrame.from_records(X2.astype('double')) # Showcase a dataframe for the original point and PP\ndfpp[23] = classes\ndfpp.columns = df.columns\ndfpp.rename(index={0:'X',1:'X_PP'}, inplace=True)\ndfppt = dfpp.transpose()\n\ndfppt.style.apply(highlight_ce, col='X_PP', ncols=2, axis=1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcdefaults()\nfi = abs(Xpp_re.astype('double'))/np.std(x_train.astype('double'), axis=0) # Compute PP feature importance\n    \nobjects = df.columns[-2::-1]\ny_pos = np.arange(len(objects)) # Get input feature names\nperformance = fi[0, -1::-1]\n\nplt.barh(y_pos, performance, align='center', alpha=0.5) # Bar chart\nplt.yticks(y_pos, objects) # Plot feature names on y-axis\nplt.xlabel('weight') #x-label\nplt.title('PP (feature importance)') # Figure heading\n\nplt.show()    # Display the feature importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}