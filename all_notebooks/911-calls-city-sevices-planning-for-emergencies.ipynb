{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ce98fb88-84a0-35f2-0900-267b555437d5"},"source":"Fields\n\n* lat : String variable, Latitude\n* lng: String variable, Longitude\n* desc: String variable, Description of the Emergency Call\n* zip: String variable, Zipcode\n* title: String variable, Title\n* timeStamp: String variable, YYYY-MM-DD HH:MM:SS\n* twp: String variable, Township\n* addr: String variable, Address\n* e: String variable, Dummy variable (always 1)\n\n# Aim:\n\nTo analyse the data to find patterns and trends. Use this data to create models to better predict the different unknown features. Follow this by finding solutions to real-life problems based on given data.\n\n# Abstract:\n\n**What is Data Science?**\nMaking Intelligent Conclusions from a large number of features.\n\nWelcome to 911 Calls database project, as one might know, 911 is the emergency call number in the United States. In case of any emergency be it traffic, ems or fire, the Emergency Services (911 Services) solve these problems. They also keep a record of the information regarding each call. At first sight it might be simply a list of all calls, but with the help of statistics, basic machine learning and data science we can come to many intelligent conclusions that may or may not even be directly connected to the data we have. That is the beauty of Data Science. This Notebook aims to present the Simple and the Complex conclusions we can come to by just using the data described above. "},{"cell_type":"markdown","metadata":{"_cell_guid":"4e70c7b3-b466-6a4c-f9c1-addd3b1ad671"},"source":"<h1 style='font-size:50px;text-align:center;color:#555'><br><br>Part I<br><br>Data Acquisition and Cleaning<br><br><br><br></h1>"},{"cell_type":"markdown","metadata":{"_cell_guid":"58ee45f5-a1d1-dde2-60e0-1e2f4c14c027"},"source":"# Library Imports"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5eff8adc-9676-8911-8275-abb8f19fe085"},"outputs":[],"source":"import numpy as np #for mathematical manipulation\nimport pandas as pd #for database manipulation\nimport matplotlib.pyplot as plt #for plotting\nimport seaborn as sns #better plotting library\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"d4138e81-c1b5-3c09-58b1-8a57f35758ee"},"source":"## Data "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43b0d391-cd76-3f54-3447-725f089a9a6a"},"outputs":[],"source":"data=pd.read_csv('../input/911.csv') #read data from csv"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b025981-d0bd-f1ff-97b5-9912f0f502e0"},"outputs":[],"source":"data.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac642125-32ee-4d71-0046-785fd5d87c38"},"source":"This is the Data we have at hand, as we can see we have a mostly text data with some very few numerical data. There are a total 99492 data points which we can use."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e64f391f-f72d-8a5d-319f-c151a74ade98"},"outputs":[],"source":"data.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f6ec37f4-e4b9-0b81-737f-021536ee60fb"},"source":"# Data Cleaning\n\nThe first step now will be to remove unnecessary columns, to be specific the dummy variable e"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"761231e0-79f0-d150-111a-3464bbb7d5b6"},"outputs":[],"source":"# Drop dummy variable e\ndata=data.drop('e',axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79501b83-0dd8-6f08-312c-c4c21145700b"},"outputs":[],"source":"data.head(2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a3d2e46-a7d9-334a-129f-ddef823c4ce2"},"source":"<h1 style='font-size:50px;text-align:center;color:#555'><br><br>Part II<br><br>Exploratory Data Analysis<br><br></h1>"},{"cell_type":"markdown","metadata":{"_cell_guid":"bbafac38-eb5e-1462-44f1-a7a1dbb051cf"},"source":"# Exploratory Data Analysis\n\nNow its time to make some simple conclusions from the data."},{"cell_type":"markdown","metadata":{"_cell_guid":"1dc41867-ad9b-8a14-2cc5-4d71853f5362"},"source":"### **Q1: What are the Top 10 Zipcodes for Emergency Calls?**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12a04b54-0a61-d558-2c2b-1ca69b50c615"},"outputs":[],"source":"top_10_zip=pd.DataFrame(data['zip'].value_counts().head(10))\ntop_10_zip.reset_index(inplace=True)\ntop_10_zip.columns=['ZIP','Count']\ntop_10_zip"},{"cell_type":"markdown","metadata":{"_cell_guid":"2fdeaeea-63d3-87bc-2e20-0b2da24571d5"},"source":"**Lets make a plot of the top 20 zip codes**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc532177-fd34-a2a5-a1b0-9ac8c71e4339"},"outputs":[],"source":"top_20_zip=pd.DataFrame(data['zip'].value_counts().head(20))\ntop_20_zip.reset_index(inplace=True)\ntop_20_zip.columns=['ZIP','Count']\nfig1=plt.figure(figsize=(12,6))\nsns.barplot(data=top_20_zip,x='ZIP',y='Count',palette=\"viridis\")\nfig1.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"77bedb0d-0526-fbba-5486-b5fd71652ded"},"source":"### **Q2: What are the Top 10 townships for 911 calls?**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc8f4ac8-33ee-2cba-44af-7ddbfb6f399c"},"outputs":[],"source":"top_10_twp=pd.DataFrame(data['twp'].value_counts().head(10))\ntop_10_twp.reset_index(inplace=True)\ntop_10_twp.columns=['Township','Count']\ntop_10_twp"},{"cell_type":"markdown","metadata":{"_cell_guid":"867323fb-7395-2456-e869-a574771cd548"},"source":"**Lets make a plot of the top 20 townships?**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9924e709-3f1f-a34e-484a-a722056780a8"},"outputs":[],"source":"top_20_twp=pd.DataFrame(data['twp'].value_counts().head(20))\ntop_20_twp.reset_index(inplace=True)\ntop_20_twp.columns=['Township','Count']\nfig2=plt.figure(figsize=(12,6))\ng=sns.barplot(data=top_20_twp,x='Township',y='Count',palette=\"viridis\")\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\nfig2.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e21342d-06e9-6a2f-facc-0b4439cd880b"},"source":"### **Q3: How many unique titles/reasons for emergency are there?**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8299b8a9-1326-48bf-d0db-3976da52560a"},"outputs":[],"source":"data['title'].nunique()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fbc839ec-7c4b-2bae-7d71-f6f8fba197c5"},"source":"This is an enormous amount of Data to process, Lets simplify the data into three main categories:  \n\n* EMS\n* Fire\n* Traffic\n\nFor this purpose we create a new column titled \"Reason\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae4d5007-5140-48ce-a300-25d869ada512"},"outputs":[],"source":"data['Reason']=data['title'].apply(lambda v:v.split(':')[0])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc0566ba-8133-3813-f208-442d5fe823dc"},"outputs":[],"source":"data['Reason'].nunique()"},{"cell_type":"markdown","metadata":{"_cell_guid":"50510757-7262-a273-4974-558c106f8e46"},"source":"**The title has now been simplified to three categories**  \n\nNow lets analyse this column  \n\n**The distribution of these Reasons are:**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7dba18c-17f0-3a34-d3ce-1e58afabc091"},"outputs":[],"source":"data['Reason'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"48851bd7-4a70-e912-db49-d15d4191d4cf"},"source":"**Now lets see the distribution of these values in the top 10 townships**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4414c3d6-e0c4-0b80-5853-ab257d3ade58"},"outputs":[],"source":"fig3=plt.figure(figsize=(12,6))\ng=sns.countplot(data=data[(data['twp'].isin(top_10_twp['Township']))],x='twp',hue='Reason',palette='viridis')\ng_x=g.set_xticklabels(g.get_xticklabels(),rotation=30)\nfig3.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c03fd2f-becc-a781-e997-fc7ca7a32cd4"},"source":"The desc columns has some more information we might be able to get. Lets extract some more information from it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"867f13ca-f7c1-f13e-7441-958b508ed85f"},"outputs":[],"source":"data['Station']=data['desc'].apply(lambda v:v.split(';')[2])"},{"cell_type":"markdown","metadata":{"_cell_guid":"3dceb65d-8e51-5a4e-9f78-a8317e582478"},"source":"## Analysing TimeStamps\n\nThe data initially is in str format, to make it usable, we have to convert it to a datetime object and then also obtaining each each hour, month and day"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90555a6b-d026-5cdb-71ba-c677edbfe2ad"},"outputs":[],"source":"data['timeStamp']=pd.to_datetime(data['timeStamp'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2eded145-2f46-b81a-7fb3-90e27aebabb5"},"outputs":[],"source":"data['Hour']=data['timeStamp'].apply(lambda v:v.hour)\ndata['DayOfWeek']=data['timeStamp'].apply(lambda v:v.dayofweek)\ndata['Month']=data['timeStamp'].apply(lambda v:v.month)\ndata['Date']=data['timeStamp'].apply(lambda v:v.date())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"587f91e8-e5f3-ee2e-1782-94e2b0db476c"},"outputs":[],"source":"# Map day values to proper strings\ndmap1 = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\ndata['DayOfWeek']=data['DayOfWeek'].map(dmap1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42109bdd-60b1-14f5-2bb2-931a5f3b2aec"},"outputs":[],"source":"data.head(2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"445c7ef9-3dac-b7c2-e05b-0febd70f1717"},"source":"Now we have a lot more information to analyse."},{"cell_type":"markdown","metadata":{"_cell_guid":"77ebc32b-9e1f-6b69-b965-cba0e8c518c7"},"source":"### **Q4: What is the Distribution of Emergency Calls by Day of the Week**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d4a419a-ac04-ef53-2959-9aba1a588a19"},"outputs":[],"source":"fig4=plt.figure(figsize=(12,8))\nsns.countplot(x='DayOfWeek',hue='Reason',palette='viridis',data=data)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0ba12794-7219-d814-9783-cf7c9257d3f4"},"source":"From the Data, it is pretty clear that there is a even variation in the the EMS calls, but a visible drop in Traffic Related Calls on Weekends, this is expected as there are fewer vehicles on the roads. Most other extremes are most likely a coincidence.\n\n### **Q5: Distribution of Emergency Calls by Day of the Week**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0f6a68e-923e-3935-4219-7dae7322393d"},"outputs":[],"source":"fig5=plt.figure(figsize=(12,8))\nsns.countplot(x='Month',hue='Reason',palette='viridis',data=data)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"},{"cell_type":"markdown","metadata":{"_cell_guid":"af8092a2-09b6-1ab5-8de8-eca6c0c1345b"},"source":"**The Data is missing some months!** So we need to make a more continuous distribution. For example a line plot."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"955a0fe6-b381-fa32-fea6-b8db1eec489f"},"outputs":[],"source":"databyMonth_EMS = data[data['Reason']=='EMS'].groupby('Month').count()\ndatabyMonth_Fire = data[data['Reason']=='Fire'].groupby('Month').count()\ndatabyMonth_Traffic = data[data['Reason']=='Traffic'].groupby('Month').count()\ndatabyMonth_Cumul = data.groupby('Month').count()\n\ndatabyMonth_EMS['twp'].plot(figsize=(12,8),label='EMS',lw=5,ls='--')\ndatabyMonth_Fire['twp'].plot(figsize=(12,8),label='Fire',lw=5,ls='--')\ndatabyMonth_Traffic['twp'].plot(figsize=(12,8),label='Traffic',lw=5,ls='--')\ndatabyMonth_Cumul['twp'].plot(figsize=(12,8),label='Total',lw=5)\n\nfig=plt.xticks(np.arange(1,13),['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\nplt.title(\"Emergency Call Rates vs Month\")\nplt.legend()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ce521d80-2b96-384b-4c70-f3a4c2d3b06c"},"source":"**Lets fit a regression line in this Data**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b1b5796-bece-a2aa-4242-5116010a9d55"},"outputs":[],"source":"sns.lmplot(data=databyMonth_Cumul.reset_index(),x='Month',y='twp')\nplt.title(\"Regression plot of Emergency calls vs Month\")\nplt.xlabel('Months')\nplt.ylabel('Counts')"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd100adc-5ed9-3272-b8d2-d612b8cf0dc1"},"source":"### **Now lets check the variation of Emergency Calls by Date**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"817137cf-a1c4-1c53-53d8-78dc7b94c079"},"outputs":[],"source":"data.groupby('Date').count()['twp'].plot(figsize=(15,3))\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8396943d-66eb-26af-abd3-6723268163f2"},"source":"This distribution is quite random, with certain spikes on specific dates.\n\n**Lets check the distribution of the different reasons**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e040a4aa-9595-f290-fa14-dd41ee56950d"},"outputs":[],"source":"data[data['Reason']=='EMS'].groupby('Date').count()['twp'].plot(figsize=(15,3),label='EMS')\ndata[data['Reason']=='Fire'].groupby('Date').count()['twp'].plot(figsize=(15,3),label='Fire')\ndata[data['Reason']=='Traffic'].groupby('Date').count()['twp'].plot(figsize=(15,3),label='Traffic')\nplt.tight_layout()\nplt.legend()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c97cd329-db3c-cdbe-aa77-64620e9e5a14"},"source":"Now we can see that there was a major spike in traffic around early february. This might be due to a variety of reasons. Lets Explore the data from January to February"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4398be99-5a9b-567c-2bb1-309737365c4b"},"outputs":[],"source":"strange_increase=data[(data['Reason']=='Traffic') & ( (data['timeStamp']>pd.to_datetime(\"2016-01-1\")) &  (data['timeStamp']<pd.to_datetime(\"2016-02-1\")))].reset_index().drop('index',axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fdbe58c-36ba-fa1c-e707-6f402728c60d"},"outputs":[],"source":"strange_increase['title'].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6e06c3db-eca3-5f33-1b2f-15f30f312369"},"source":"We can see that the number of disabled vehicles was unusually high but to put it in context we need to know the number in the other months. For this purpose we will compare it to the average of the next 6 months"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65c40387-2cc2-e067-f0fe-05810c1cf6e6"},"outputs":[],"source":"normal_counts=data[(data['Reason']=='Traffic') & ( (data['timeStamp']>pd.to_datetime(\"2016-02-1\")) &  (data['timeStamp']<pd.to_datetime(\"2016-08-1\")))].reset_index().drop('index',axis=1)\nnormal_counts['title'].value_counts()/6"},{"cell_type":"markdown","metadata":{"_cell_guid":"98eaca30-0d06-0a12-81a3-fc8c3107cea8"},"source":"Now we can clearly see that the number of disabled cars was ***UNUSUALLY HIGH*** in that month. By checking weather records of that month, we can find out that the temperature was lowest of the year in Montgomery County at that time. So the cold caused vehicle engines to freeze over and thats why the emergency calls rose"},{"cell_type":"markdown","metadata":{"_cell_guid":"e53c7e9f-d571-216e-2c70-da8092edb8c4"},"source":"## Lets move on to a Geographical Analysis\n\nFirst we look at a geographical (2D) plot of the emergency calls"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6d0a9a1-b703-0e3c-5985-8b53675c0c97"},"outputs":[],"source":"sns.jointplot(data=data,x='lng',y='lat',kind='scatter')"},{"cell_type":"markdown","metadata":{"_cell_guid":"273ce272-b884-c19c-9678-7de496ac04a7"},"source":"Thus the data a spread over a large region, but it is focused on a smaller region (in the upper-right corner). This area is most probably a city or a large settlement if we are to analyse this better geographically, we have to ignore the outliers\n\nFor this we take a error margin of +/- 4.5 * Standard Deviation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fda30f4-aa38-c3e1-63bb-a7cae1d52923"},"outputs":[],"source":"data_geog=data[(np.abs(data[\"lat\"]-data[\"lat\"].mean())<=(4.5*data[\"lat\"].std())) & (np.abs(data[\"lng\"]-data[\"lng\"].mean())<=(10*data[\"lng\"].std()))]\ndata_geog.reset_index().drop('index',axis=1,inplace=True)\nsns.jointplot(data=data_geog,x='lng',y='lat',kind='scatter')"},{"cell_type":"markdown","metadata":{"_cell_guid":"07e2cec9-20d0-ba46-4be6-cc1615e69fb5"},"source":"The Picture of the Township is now clearer, now lets do a Density Analysis, but before that we will straighten the township in accordance with standard grid system followed in city planning"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16fcbd5f-fc23-3296-7035-69e9e8bc4cb4"},"outputs":[],"source":"data_geog[['lat','lng']].head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dea94fa2-ab0f-56d0-7f62-e2e099ba30ae"},"outputs":[],"source":"pd.options.mode.chained_assignment = None #Remove Error Message\nx_mean=data_geog['lng'].mean()\ny_mean=data_geog['lat'].mean()\ndata_geog['x']=data_geog['lng'].map(lambda v:v-x_mean)\ndata_geog['y']=data_geog['lat'].map(lambda v:v-y_mean)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb73303f-9dab-d43f-a810-f01668252557"},"outputs":[],"source":"theta=np.pi/3\nrot_mat=np.array([np.cos(theta),-np.sin(theta),np.sin(theta),np.cos(theta)]).reshape(2,2)\ndata_geog[['x','y']]=data_geog[['x','y']].apply(lambda v:np.dot(v.as_matrix(),rot_mat),axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a949e82-4483-6658-be88-f5ded36db9de"},"outputs":[],"source":"sns.jointplot(data=data_geog,x='x',y='y',kind='scatter',xlim=(-0.3,0.3))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edd1ab76-d508-57ce-fe9d-02310e89baed"},"outputs":[],"source":"sns.jointplot(data=data_geog,x='x',y='y',kind='kde',xlim=(-0.3,0.3))"},{"cell_type":"markdown","metadata":{"_cell_guid":"47e382ca-9e21-c295-1bd8-12f1531ac3ce"},"source":"This gives us a very good picture of the high risk areas. Now lets see the output with respect to different Emergencies."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18778cf4-a9f3-d3e0-6a7d-81c4ea8ee246"},"outputs":[],"source":"sns.jointplot(data=data_geog[data_geog['Reason']=='EMS'],x='x',y='y',kind='kde',color='green',xlim=(-0.3,0.3))\nplt.title('EMS Distribution')\nplt.tight_layout()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1fe3633-8b5f-77a9-da25-995b418b0838"},"outputs":[],"source":"sns.jointplot(data=data_geog[data_geog['Reason']=='Fire'],x='x',y='y',kind='kde',color='red',xlim=(-0.3,0.3))\nplt.title('Fire Distribution')\nplt.tight_layout()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a9320d1-e446-2034-438b-be91fba8ae7d"},"outputs":[],"source":"sns.jointplot(data=data_geog[data_geog['Reason']=='Traffic'],x='x',y='y',kind='kde',color='purple',xlim=(-0.3,0.3))\nplt.title('Traffic Distribution')\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2db2e0e9-92f6-9211-df22-0637bc836c87"},"source":"**Lets observe the location of the different townships**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa2347d4-f3b0-2869-e37c-ee0f99c1051d"},"outputs":[],"source":"fig=plt.figure(figsize=(10,10))\ntwp_group=data_geog.groupby('twp')\nfor name, group in twp_group:\n    plt.plot(group.x, group.y, marker='o', linestyle='', label=name)\nplt.xlim(-0.3,0.3)\nplt.title(\"Townships\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"d4f627f9-88a9-ef92-7663-33c77e60f0ce"},"source":"Now we can visulize the townships projects in the the townships, but trying to manage the emergency services for each township individually is **inefficent and too unrealistic.** So a different grouping system arrangement is required. "},{"cell_type":"markdown","metadata":{"_cell_guid":"11f2d3a7-bb06-801e-972b-147aa54ff840"},"source":"<h1 style='font-size:50px;text-align:center;color:#555'><br><br>Part III<br><br>From Data to Models<br><br></h1>"},{"cell_type":"markdown","metadata":{"_cell_guid":"b128ead1-d42f-67f2-64ac-e251620cf387"},"source":"# City Geo-Clustering"},{"cell_type":"markdown","metadata":{"_cell_guid":"b25823bd-3d9a-f46b-92fb-fb5af6144e47"},"source":"***What do we mean by clustering?***  \nIt means the grouping of Data into groups with similar characters/features. The features we will consider will only be the geo-location: latitude and longitude. Since We have closely located data point with similar features this model will group high density areas together for the right values of number of clusters.\n\nNow lets try to divide the township into clusters using basic K-Means Clustering by selecting number of clusters over a range and select the most effective model.\n\nFirst lets use 10 as the number of clusters"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a33b7011-d44f-48d6-bd54-8288974b477f"},"outputs":[],"source":"from sklearn.cluster import KMeans"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3864519b-605f-d2b5-c6ad-18f80d70e122"},"outputs":[],"source":"X=data_geog[['x','y']].reset_index().drop('index',axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"587c41b2-19a1-c3fd-3cd5-eddb44af7bc8"},"outputs":[],"source":"kmeans=KMeans(n_clusters=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"858a13d6-9f01-44c8-91ff-8366b7415d7d"},"outputs":[],"source":"kmeans.fit(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fedc99f-2ce5-c2a6-f1fd-5fc90118ea59"},"outputs":[],"source":"fig=plt.figure(figsize=(7,7))\nplt.scatter(X['x'],X['y'],c=kmeans.labels_,cmap='rainbow')\nplt.xlim(-0.3,0.3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee29c2f5-4a9b-1a7e-84fc-a6fddb25aaa2"},"source":"Now we test out different values for the number of clusters"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9829e12f-cda9-30c3-8703-42c2a61b7108"},"outputs":[],"source":"fig=plt.figure(figsize=(12,12))\nfor i in range(3,12):\n    kmeans=KMeans(n_clusters=i)\n    kmeans.fit(X)\n    fig.add_subplot(3,3,i-2)\n    plt.scatter(X['x'],X['y'],c=kmeans.labels_,cmap='rainbow')\n    plt.title(\"Number of Clusters = {}\".format(i))\n    plt.xlim(-0.3,0.3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"29f66a94-812e-61d9-6708-1c4d377f2bb8"},"source":"Now we have a beautiful range of options of clustering our data. But to decide the final clustering model, we need to consider a few factors:  \n\n* Area of Township\n* Average Population in need of Emergency Services\n* Distribution of said emergencies\n\nLets calculate each of the above.\n\nFirstly for the area, We will use the formulae:  \n*A = 2.pi.R^2 |sin(lat1)-sin(lat2)| |lon1-lon2|/360*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30f707b1-11a1-1a48-dc9d-95fcbf7a7f32"},"outputs":[],"source":"latsin_dist=np.abs(np.sin(np.max(data_geog[\"lat\"])/180*np.pi)-np.sin(np.min(data_geog[\"lat\"])/180*np.pi))\nlng_dist=np.abs(np.max(data_geog[\"lng\"])-np.min(data_geog[\"lng\"]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0b153561-5db9-88b8-89bf-67baafbd3edb"},"source":"Now to calculate the value"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b364af95-8f63-6b99-650b-a4c3ddd7f107"},"outputs":[],"source":"def ll2area(latsin,lng):\n    return 2*np.pi*(6371**2)*latsin*lng/360\nA=ll2area(latsin_dist,lng_dist)\nprint(\"The Area of the Township is Appoximately {} sq. km\".format(A))"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e51c22d-a92c-defb-bc29-a48ac1654923"},"source":"Thus we now know the area of the township.\n\nFrom the lat-long we can find out the country and thus the approximate population density.\nHere it is USA so the avg. urban population density is 814 people per square mile = 314 people per square km  \n\nSo the Avg. Population can be calculated as follows:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac2942e0-d401-af0c-a427-ddc52c663900"},"outputs":[],"source":"pop=np.int(A*314)\nprint(\"The Avg Population of the Township is Appoximately {}\".format(pop))"},{"cell_type":"markdown","metadata":{"_cell_guid":"094d0862-95de-dd9b-a831-7a94926abea5"},"source":"We already know the distribution of emergencies from the earlier graphs so now we are ready to choose our cluster model  \n\n### Cluster Choice\n\nWe can see that southern area has very few emergencies so, that suggests a lower population or a safer area, so we do not need multiple cluster, a single larger cluster is enough (like n_clusters=6).  \n\nThe density is higher in the central part, which suggests the need of smaller clusters to accomodate a almost constant emergency services to population ratio, again here n_clusters=6 shines.\n\nSo at a first look, n_cluster=6, is the best choice.\n\nNow we have to figure out the specifics with n_clusters=6"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e943cc17-1e6e-d9c1-54af-c35fd33c690b"},"outputs":[],"source":"final_kmeans=KMeans(n_clusters=6)\nfinal_kmeans.fit(X)\nfig=plt.figure(figsize=(7,7))\nplt.scatter(X['x'],X['y'],c=final_kmeans.labels_,cmap='rainbow')\nplt.xlim(-0.3,0.3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"04572949-7e84-42d1-9506-f59d1346e0c3"},"source":"Now to make the new dataset with the cluster data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc0e4ee1-e08a-328c-e09d-abaab574eda5"},"outputs":[],"source":"data_clus=pd.concat([data_geog.reset_index().drop('index',axis=1),pd.DataFrame(final_kmeans.labels_,columns=['Cluster'])],axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7648ba77-26cd-29fd-8f0a-b255a8068a6e"},"outputs":[],"source":"data_clus.drop(['desc','title','timeStamp'],axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef9e12b5-bd81-2d4b-bf97-4a074a69b855"},"outputs":[],"source":"data_clus.tail(2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"441298cb-96ef-f84d-2ef8-13a2d1f95ff3"},"source":"Now we will create a model to approximate population of each cluster, for that we need to find the population densities per sqkm. Then we will use these values to find the model to best fit the data.\n\nThe steps to this process are as follows:\n\n1. **Group latitudes into 1 km blocks** and find the number of Data Points in each block now we can a use a **normally inefficient** kernel density to approximate the log(densities), which we convert to population densities by an appropriate function.\n\n2. Then we can find a **effective and efficient model** that fits the data. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42748e39-cd8d-850e-2dcf-275d5aab380f"},"outputs":[],"source":"data_block=data_clus.copy()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f310c7e-a88d-ae9f-fa18-d5f3fdcb555f"},"outputs":[],"source":"data_block['lat']=np.rint(data_block['lat']*100)/100 #reduce to 1 km block\ndata_block['lng']=np.rint(data_block['lng']*100)/100 #reduce to 1 km block"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c126a7d3-17c6-be4e-ff43-97de2fb79464"},"outputs":[],"source":"data_block.head(2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb188151-e95a-6f93-73ac-72f25f051c14"},"outputs":[],"source":"model_data=data_block.groupby(['lat','lng']).count().reset_index().drop(['zip','twp','Reason','Hour','DayOfWeek','Month','Date','Station','Cluster','x','y'],axis=1)\nprint(\"This is the form of our new data reduced to 1 sqkm blocks\")\nmodel_data.head(1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20122992-72d6-bf66-392b-fb4bf7649f7d"},"outputs":[],"source":"X2=model_data[['lat','lng']]\ny2=model_data.drop(['lat','lng'],axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"33e335a0-268d-29e6-b7ba-c5a35a91008f"},"source":"We now use Kernel Density Analysis to get the desities of emergencies throughout the data base and then use a simple proportion with mean kernel desnity vs mean population to get the population densities which we then tweak to meet our previous data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88a16bc9-0ff3-0c92-eb57-515963b98b8d"},"outputs":[],"source":"from sklearn.neighbors import KernelDensity\nkd=KernelDensity()\nkd.fit(X2,y2)\ny2=pd.DataFrame(np.exp(kd.score_samples(X2)))\nmean_kernel=np.exp(kd.score_samples(X2)).mean()\nmin_kernel=np.exp(kd.score_samples(X2)).min()\nmean_pop=0.95*pop/A\ndef kern2pop(ker):\n    return (((mean_kernel+np.sign(ker-mean_kernel)*(np.abs(ker-mean_kernel))**0.59)/mean_kernel))*mean_pop\ny2=pd.DataFrame(y2.apply(kern2pop))\nprint(\"Our data is now ready\")\npd.DataFrame(pd.concat([X2,y2],axis=1).head(5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"57732b74-15ca-0688-c6d9-369b35afb1e1"},"source":"#### Lets perform a K-Fold Cross Validation to get the best Regressor Model. The various possible models are:\n\n1. Linear Regression\n2. Quadratic Regression\n2. Decision Tree Regressor\n3. Random Forests Regressor\n4. Support Vector Regressor (Gaussian)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"981073ba-36d4-254a-f085-ba5099d19f10"},"outputs":[],"source":"from sklearn.model_selection import cross_val_score,cross_val_predict"},{"cell_type":"markdown","metadata":{"_cell_guid":"b98e546f-f725-d4d6-f831-1a38244f8d5e"},"source":"### Linear Regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8fac37ec-0c8d-46f0-f154-c1418ab6e42f"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f4becc0-4fc4-952e-390e-e511d1d0a487"},"outputs":[],"source":"lin_model=LinearRegression()\nprint(\"R2 Score: {} \".format(cross_val_score(lin_model,X2,y2,scoring='r2',cv=10).mean()))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(-cross_val_score(lin_model,X2,y2,scoring='neg_mean_squared_error',cv=10).mean())))\npredicted=cross_val_predict(lin_model,X2,y2,cv=10)\nplt.scatter(y2,predicted)\nplt.plot([y2.min(), y2.max()], [y2.min(), y2.max()], 'k--', lw=4)\nplt.title('Residual Error')\nplt.xlabel('Measured')\nplt.ylabel('Predicted')"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6753aba-2c2f-6f3b-2af9-1e2ee57c25bf"},"source":"This is a bad value of both R-squared and RMSE, thus Linear regression is a inaccurate predictor"},{"cell_type":"markdown","metadata":{"_cell_guid":"62a65887-34c1-ad95-ee40-9952daadaaf4"},"source":"### Quadratric Regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7443c12-d4ce-0919-0bd1-c89112b31d77"},"outputs":[],"source":"from sklearn.preprocessing import PolynomialFeatures"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f587915-e4b7-9dda-8460-3c552a8ff6eb"},"outputs":[],"source":"poly=PolynomialFeatures(2)\nX_quad=pd.DataFrame(poly.fit_transform(X2),columns=['1','lat','lng','lat^2','lat*lng','lng^2'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ba9f43f-576b-47a3-7fd3-5d11d16b6395"},"outputs":[],"source":"quad_model=LinearRegression()\nprint(\"R2 Score: {}\".format(cross_val_score(quad_model,X_quad,y2,scoring='r2',cv=10).mean()))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(-cross_val_score(quad_model,X_quad,y2,scoring='neg_mean_squared_error',cv=10).mean())))\npredicted=cross_val_predict(quad_model,X_quad,y2,cv=10)\nplt.scatter(y2,predicted)\nplt.plot([y2.min(), y2.max()], [y2.min(), y2.max()], 'k--', lw=4)\nplt.title('Residual Error')\nplt.xlabel('Measured')\nplt.ylabel('Predicted')"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e3caf6c-1c70-8df3-c25d-746874df24a4"},"source":"Comparitively the R squared value is much better, thus Quadratic regression is a much better model "},{"cell_type":"markdown","metadata":{"_cell_guid":"a7f0087d-11e4-160a-d6e0-2f706b96cf65"},"source":"### Decision Tree Regressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4333686-2d17-a48f-aef5-31442ab6e309"},"outputs":[],"source":"from sklearn.tree import DecisionTreeRegressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88cbfdf4-d473-194a-3dee-11ab26f37800"},"outputs":[],"source":"dtree_model=DecisionTreeRegressor()\nprint(\"R2 Score: {}\".format(cross_val_score(dtree_model,X2,y2,scoring='r2',cv=10).mean()))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(-cross_val_score(dtree_model,X2,y2,scoring='neg_mean_squared_error',cv=10).mean())))\npredicted=cross_val_predict(dtree_model,X2,y2,cv=10)\nplt.scatter(y2,predicted)\nplt.plot([y2.min(), y2.max()], [y2.min(), y2.max()], 'k--', lw=4)\nplt.title('Residual Error')\nplt.xlabel('Measured')\nplt.ylabel('Predicted')"},{"cell_type":"markdown","metadata":{"_cell_guid":"bf5a5e2c-7b47-4394-cdd1-bae6663ed833"},"source":"As apparent from the values, Decision Trees is also not the perfect model "},{"cell_type":"markdown","metadata":{"_cell_guid":"adb93e5d-c692-af96-24d5-cf5b9e6702fd"},"source":"### Random Forests"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3246444b-5f70-26f9-6400-29bd87cf0244"},"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddd07ded-2cd2-a9ce-373e-8d767cffbfdc"},"outputs":[],"source":"randf_model=RandomForestRegressor(n_jobs=-1)\nyx=y2.values.ravel()\nprint(\"R2 Score: {}\".format(cross_val_score(randf_model,X2,yx,scoring='r2',cv=10).mean()))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(-cross_val_score(randf_model,X2,yx,scoring='neg_mean_squared_error',cv=10).mean())))\npredicted=cross_val_predict(randf_model,X2,yx,cv=10)\nplt.scatter(yx,predicted)\nplt.plot([yx.min(), yx.max()], [yx.min(), yx.max()], 'k--', lw=4)\nplt.title('Residual Error')\nplt.xlabel('Measured')\nplt.ylabel('Predicted')"},{"cell_type":"markdown","metadata":{"_cell_guid":"726166c6-52a7-5ec8-2068-1835d92db1ea"},"source":"This result was expected as the number of features is less and there is nothing to decorrelate, so we cannot use Random Forest"},{"cell_type":"markdown","metadata":{"_cell_guid":"25042dd4-ce6d-f377-e616-7c1f0d5382e3"},"source":"### Support Vector Machine"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5104e821-db6c-f6c1-f5fe-ec9681499aad"},"outputs":[],"source":"from sklearn.svm import SVR "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"878f10ba-a2a1-8d1f-167a-09f2932db456"},"outputs":[],"source":"svm_model=SVR()\nprint(\"R2 Score: {}\".format(cross_val_score(svm_model,X2,yx,scoring='r2',cv=10).mean()))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(-cross_val_score(svm_model,X2,yx,scoring='neg_mean_squared_error',cv=10).mean())))\npredicted=cross_val_predict(svm_model,X2,yx,cv=10)\nplt.scatter(yx,predicted)\nplt.plot([yx.min(), yx.max()], [yx.min(), yx.max()], 'k--', lw=4)\nplt.title('Residual Error')\nplt.xlabel('Measured')\nplt.ylabel('Predicted')"},{"cell_type":"markdown","metadata":{"_cell_guid":"c67d6d4c-d719-741b-9d00-0b54d5a5f301"},"source":"Without the best params, this SVM result was quite expected. We now run an grid search cv to find the best params"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f413449-02e3-0ae2-6265-6f9f46ae3c2f"},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c96a080d-e04a-5a0b-a8d4-72509d3ceb77"},"outputs":[],"source":"param_grid={'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}\ngrid=GridSearchCV(SVR(),param_grid,verbose=0)\ngrid.fit(X2,yx)\nprint('Best Score is {} at {}'.format(grid.best_score_,grid.best_params_))"},{"cell_type":"markdown","metadata":{"_cell_guid":"47686df9-7c17-f149-27ab-80818cbebdf5"},"source":"Now for scoring the best SVC model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ce0715b-76fb-73ea-55d5-42373a8a4c40"},"outputs":[],"source":"gridsvm=SVR(C=1000,gamma=1)\nprint(\"R2 Score: {}\".format(cross_val_score(gridsvm,X2,yx,scoring='r2',cv=10).mean()))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(-cross_val_score(gridsvm,X2,yx,scoring='neg_mean_squared_error',cv=10).mean())))\npredicted=cross_val_predict(gridsvm,X2,yx,cv=10)\nplt.scatter(yx,predicted)\nplt.plot([yx.min(), yx.max()], [yx.min(), yx.max()], 'k--', lw=4)\nplt.title('Residual Error')\nplt.xlabel('Measured')\nplt.ylabel('Predicted')"},{"cell_type":"markdown","metadata":{"_cell_guid":"c61ac65d-887c-e2ff-e957-53492eebdc8c"},"source":"So here we can see the this is also a good model, but the results fall behind quadratic regression"},{"cell_type":"markdown","metadata":{"_cell_guid":"41bcecd4-7298-3f5f-f586-a0d714373cfa"},"source":"### Conclusion:  \nSince we can see both the SVM model and Quadratic regression are equally efficient, But since SVM are more computationally expensive , Quadratic regression is the model best suited to our purpose\n\n-----------\n### Cluster information\nThe next step is to find the area of each cluster, and the population density at the centriods"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"502a62d4-ec05-4db0-8a2f-ef8bba6da3ba"},"outputs":[],"source":"clus_info=pd.DataFrame(final_kmeans.cluster_centers_,columns=['x','y'])\nprint(\"Cluster Centers in local coordinate are:\")\nclus_info"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93c8fe8a-e92f-b949-4258-94d256faed09"},"outputs":[],"source":"fig=plt.figure(figsize=(7,7))\nplt.scatter(X['x'],X['y'],c=final_kmeans.labels_,cmap='summer')\nplt.xlim(-0.3,0.3)\nplt.scatter(clus_info['x'],clus_info['y'],marker='^',color='black')\nn=[' C1',' C2',' C3',' C4',' C5',' C6']\nfor i,txt in enumerate(n):\n    plt.annotate(txt,xy=(clus_info['x'][i],clus_info['y'][i]),color='black')"},{"cell_type":"markdown","metadata":{"_cell_guid":"c3e642f8-8065-ec33-53c4-98d1ff5be9ea"},"source":"Lets find the average population density of each of the clusters. For this we calculate the population density at each data point and then find average by cluster"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a4ab2bb-1b74-8acc-c542-202a8d392d30"},"outputs":[],"source":"poly=PolynomialFeatures(2)\nclus_quad=pd.DataFrame(poly.fit_transform(data_clus[['lat','lng']]),columns=['1','lat','lng','lat^2','lat*lng','lng^2'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e897bcd9-445d-31a2-8995-8942de2e8179"},"outputs":[],"source":"quad_model.fit(X_quad,y2)\npopdense=pd.DataFrame(quad_model.predict(clus_quad).ravel(),columns=[\"Pop. Density\"])\ndata_clus=pd.concat([data_clus,popdense],axis=1)\ndata_clus.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"edd92dff-0786-b5b5-141b-7a5c02e965a2"},"source":"Now we have the population density at each point in the dataset. So now we can find the cluster averages."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"971778ab-a142-9a88-b035-49798081c8b9"},"outputs":[],"source":"pope=data_clus.groupby('Cluster').mean()['Pop. Density'].as_matrix()\npope"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edbfd232-9c14-899b-b4d5-1c28c25a06c9"},"outputs":[],"source":"print(\"The predicted population densities are:\\n\")\n#popdense=quad_model.predict(clus_quad).ravel()\n#popdense=gridsvm.predict(data_clus[['lat','lng']]).ravel()\nprint(pope)\nprint(\"\\nSo we can see that {} has the maximum density and correspondingly a small cluster size.\\nAlso {} has least population density but much larger size. This generaly means that the populations have been managed equally\".format(n[pope.argmax()],n[pope.argmin()]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27966c0b-3e09-4794-ae3a-d5d2584b053d"},"outputs":[],"source":"areas=[]\nprint(\"The approximate cluster areas are:\\n\")\nfor i in range(0,6):\n    tempdata=data_clus[data_clus['Cluster']==i]\n    lats=np.abs(np.sin(np.max(tempdata[\"lat\"])/180*np.pi)-np.sin(np.min(tempdata[\"lat\"])/180*np.pi))\n    lngs=np.abs(np.max(tempdata[\"lng\"])-np.min(tempdata[\"lng\"]))\n    pops=(2/3)*ll2area(lats,lngs)\n    areas.append(pops)\n    print(\"Cluster {} : {:.2f} sq km\".format(i+1,pops))\nprint(\"\\nThe predicted cluster populations are:\")\nprint(areas*pope)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4b1b53d5-f4a1-f94e-e93c-3971ce34cd15"},"source":"<h1 style='font-size:50px;text-align:center;color:#555'><br><br>Part IV<br><br>Understanding the Situation<br><br></h1>"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a494a5d-6a90-a689-0fed-c6388be94637"},"source":"### Study of Existing Services"},{"cell_type":"markdown","metadata":{"_cell_guid":"ed3dd91e-087e-56b9-19de-579a6f5315a1"},"source":"First we clean the Station Column to get the Codes/Names of the Stations in the form of a Database"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4b6edea-667f-be10-58c5-b6dc6e401b92"},"outputs":[],"source":"def getname(v):\n    if len(v.split('Station'))>1:\n        if v.split('Station')[1][0]==':':\n            return v.split('Station')[1][1:]\n        else:\n            return v.split('Station')[1]\n    else:\n        return 0\ndata_geog['Station Name']=data_geog['Station'].apply(getname)\nstation_base=data_geog[data_geog['Station Name'] != 0].copy().drop(['timeStamp','title','desc'],axis=1)\nstation_base.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"770a6759-d4da-1676-f19b-3f7652fb299b"},"source":"How many Unique Stations are there?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d56e1f7-41c1-c550-33e6-bade06be32d0"},"outputs":[],"source":"station_base['Station Name'].nunique()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6118feed-1c40-3aa9-8ffa-920bebe9840e"},"source":"Lets interpolate the average location of these Station:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17409c59-477b-97e3-c227-1d60dfd1eee1"},"outputs":[],"source":"station_list =station_base.groupby('Station Name').mean().reset_index().drop(['zip','Hour','Month'],axis=1).drop(0)\nstation_list.head(2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8e8c9b80-42dd-9193-927b-069cb1eb1346"},"source":"Now to plot this data on the map"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9425048e-78bd-e26a-e4fc-7b6d0f2ba5fb"},"outputs":[],"source":"fig=plt.figure(figsize=(7,7))\nplt.scatter(X['x'],X['y'],c=final_kmeans.labels_,cmap='summer')\nplt.scatter(station_list['x'],station_list['y'],marker='^',color='black')\nplt.xlim(-0.3,0.3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"01d65244-0692-82b0-673e-f5c9e6f2fd89"},"source":"As apparent from the plot, the distribution of the services is uniform and ***doesnt take high risk areas into consideration***, but still we lack details as to which station handles which emergencies"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e971d832-f874-19b0-4ab1-af4a0cf85500"},"outputs":[],"source":"dummies=pd.get_dummies(station_base['Reason'])\ndummies.head(2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee0060b3-b34c-84ce-6917-49ff17d18007"},"source":"Notice how there is no Traffic Column, we have to assume that this means police stations responsible for managing traffic calls, are not properly labelled in the data so are of not much use to us"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d278af9-5445-a523-4af2-27167242da96"},"outputs":[],"source":"emergencies=pd.concat([station_base,dummies],axis=1).groupby('Station Name').sum().drop(['lat','lng','x','y','zip','Hour','Month'],axis=1).reset_index().drop(0)\nemergencies=pd.concat([emergencies,station_list[['lat','lng','x','y']]],axis=1)\npopdenseStation=pd.DataFrame(quad_model.predict(pd.DataFrame(poly.fit_transform(emergencies[['lat','lng']]),columns=['1','lat','lng','lat^2','lat*lng','lng^2'])).ravel(),columns=[\"Pop. Density\"])\nemergencies=pd.concat([emergencies.reset_index().drop('index',axis=1),popdenseStation],axis=1)\nemergencies.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fbe8edc-56df-0c4f-a4db-fe710912cda7"},"outputs":[],"source":"emergencies.tail(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a9c303b0-5279-08cc-1e25-e87449f8a900"},"source":"Interesting! We now know, Identification Codes with 'STA' handle fire emergencies (Fire Station) and the rest are Emergency Services"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"906a01a2-6e44-1a33-e24d-687b03f1cfa9"},"outputs":[],"source":"def makemap(ems,fire,lat,lng,size,alpha):\n    if fire>ems:\n        plt.scatter(lat,lng,marker='o',color='red',s=size,alpha=alpha)\n        plt.xlim(-0.3,0.3)\n    else:\n        ems=plt.scatter(lat,lng,marker='o',color='blue',s=size,alpha=alpha)\n        plt.xlim(-0.3,0.3)\nfig=plt.figure(figsize=(7,7))\nplt.scatter(X['x'],X['y'],c=final_kmeans.labels_,cmap='summer')\nplt.xlim(-0.3,0.3)\n\n\nfor index,row in emergencies.iterrows():\n    makemap(row['EMS'],row['Fire'],row['x'],row['y'],50,1)\n\nfire_station=emergencies[emergencies[\"Fire\"]>0].drop(['EMS','Fire'],axis=1)\nems_station=emergencies[emergencies[\"EMS\"]>0].drop(['EMS','Fire'],axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"899a1f0c-70e7-55b0-e7d1-1baddbc48b86"},"source":"Now we can see the distribution of the Services (RED for Fire Stations, Blue for EMS Service) but what is the actual zone of influence of each station, lets use an arbritary function  \n** f(Pop. Density)= 4 * (500 - Pop. Density) **"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c06f198-54ce-0943-4cfc-8b3b6579eee4"},"outputs":[],"source":"fig=plt.figure(figsize=(7,7))\nplt.scatter(X['x'],X['y'],c=final_kmeans.labels_,cmap='summer')\nplt.xlim(-0.3,0.3)\n\nfor index,row in emergencies.iterrows():\n    makemap(row['EMS'],row['Fire'],row['x'],row['y'],3* (550-row['Pop. Density']),0.4)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd62bd3e-8dd9-4007-80d7-72fc128d3c34"},"source":"As we can see already that the allocation of services is extremely inefficient with high density area where there is not much demand and sparsity of services in important areas"},{"cell_type":"markdown","metadata":{"_cell_guid":"45eeae54-7c33-323f-8346-28f1080706f4"},"source":"# Conclusion\n\nWe began with a simple detailed records of all emergency calls of a county. Firstly, we dealt with the features we did not need. Then we extracted whatever additional features we could from the given features. Then we used exploratory data analysis to find out some interesting aspects of the data including the emergencies per township. \n\nThis followed by an analysis of the number and type of emergencies over the year. We saw an sudden rise in disabled vehicles in february. From this we can suggest that the emergencies sevicies should expect as rise in such emergencies and be adequately prepared. \n\nThen we proceeded with the Geographical analysis of the data with the latitude and longitude aspects. We observed the maximum distribution of emergencies across a settlement and focused on it. The data suggests higher emergencies in certain areas over others, this suggests the presence of high risk populated zones which should be carefully managed.\n\nTo help in the above task we tried to find an efficient division of zones by clustering. We then evaluated our choice of number of clusters by evaluating the population densities calculated by the most accurate model.\n\nThis was followed by an analysis of the pre existing stations which not only proved the fact that the system in place is not extremely efficient adn has a lot of room for intelligent improvement"},{"cell_type":"markdown","metadata":{"_cell_guid":"bcf3080c-afc4-73c9-5e47-592cd5c5edf8"},"source":"# What Next?\n\nAfter doing a detailed analysis of the data at hand, we went forward with *clustering* the city (into something similar to districts). How is this more efficient?\n\n1. These districts are not arbitrary boundaries, these district boundaries have been computed to ensure that each of these have similar geographically controlled properties, specifically the demand for Public Services. So the Officials responsible for each such 'district' will be able to look after the needs more effectively.\n\n2. This entire process can be repeated *again* for each individual district to build more sub-clusters each with better service handling capabilities. For this purpose we will require more data, preferably over a time period greater than 10 years. That way the increase in needs can also be considered as a factor\n\n3. Thirdly, this entire process promotes a heirarchical organisation which is more efficient for both human and other intelligent systems\n\n## Thank You for Reading"},{"cell_type":"markdown","metadata":{"_cell_guid":"f9650c76-21f3-655a-645c-213494746e07"},"source":"#### About the Author: *Saptarshi Soham Mohanta*\n\nStudent of Physics and Mathematics. Interested in Data Science, Full-Stack Development, Machine Learning, and Statistical Analytics.\n\nKnown Languages: Java, Javascript (including jquery, D3, React.js, npm, Node.js), XML, HTML (including Bootstrap), CSS (including animate.css, SCSS), Python (including NumPy, Pandas, Seaborn, MatPlotLib, Plotly and Scikit-Learn libraries ) Learning Currently: C, C++, CUDA-C, PyCuda, Tensorflow Libraries"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}