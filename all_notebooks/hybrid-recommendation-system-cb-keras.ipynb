{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hybrid movie recommendation system","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading libraries and data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nimport nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the 'movies.csv' file and 'ratings.csv' file. We will get rid of release year in movie titles for future title analysis and of timestamps in ratings because we are not using them in this example.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = pd.read_csv('/kaggle/input/movie-lens-small-latest-dataset/movies.csv')\nmovies['title'] = movies['title'].str.strip().str[:-7]\nmovies['genres'] = movies['genres'].str.replace('|', ' ')\nuser_ratings = pd.read_csv('/kaggle/input/movie-lens-small-latest-dataset/ratings.csv')\nuser_ratings = user_ratings.drop(columns=['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the top 25 words used in movie titles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top_N = 25\n\ntxt = movies.title.str.lower().str.cat(sep=' ')\n\nwords = nltk.tokenize.word_tokenize(txt)\nword_dist = nltk.FreqDist(words)\n\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords = stopwords + [')', '(', ',', ':', \"'s\", '.', '!', '&', '?']\nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \n\nprint('All frequencies:')\nprint('=' * 60)\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word').head(10)\nprint(rslt)\nprint('=' * 60)\n\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n\nmatplotlib.style.use('ggplot')\nrslt.plot.bar(rot=0, figsize=(18, 10), fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It does not look representative. Numerical methods won't be really effective with this kind of data. The pre-trained word2vec model could help in this case but movie title should have a weak correlation with the movie itself and the user's preferences, so we will just ignore titles in this example.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the genres data. It contains strings with names of genres of the movies. There are only 20 unique that that appear in the string in alphabet order.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top_N = 25\n\ntxt = movies.genres.str.lower().str.cat(sep=' ')\nwords = nltk.tokenize.word_tokenize(txt)\nword_dist = nltk.FreqDist(words)\n\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords = stopwords + [')', '(', ',', ':', \"'s\", '.', '!', '&', '?']\nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \n\nprint('All frequencies:')\nprint('=' * 60)\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word').head(25)\nprint(rslt)\nprint('=' * 60)\n\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n\nmatplotlib.style.use('ggplot')\nrslt.plot.bar(rot=0, figsize=(16, 8), fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Content based recommendations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For content-based recommendations, we will be looking at movie genres and searching for similar movies. To find the similarity between the two movies we will be using the cosine similarity metric.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's look at our data again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To measure the similarity between movies, we need to present the genre line in a more formalized form. To do so, we will use a bag of words model. So for each movie, we will get a vector of 21 values indicating which genres it belongs to.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\nx = vectorizer.fit_transform(movies['genres'].values)\nfeature_names = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a dataframe for genre indicators and combining them in one list for each movie.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"genres_bow = pd.DataFrame(x.toarray(), columns=feature_names)\ngenres_bow['combined']= genres_bow.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing genres line with a bag of words representation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies['genres'] = genres_bow['combined']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below returns IDs for top N movies similar to the given one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cossim(movieid, top):\n    # Creating dataframe with only IDs and genres\n    movies_to_search = movies[['movieId', 'genres']]\n    # Remove the ID of the movie we are measuring distance to\n    movies_to_search = movies_to_search[movies_to_search.movieId != movieid]\n    # Saving distances to new column\n    movies_to_search['dist'] = movies_to_search['genres'].apply(lambda x: cosine_similarity(np.array(x).reshape(1, -1), np.array(movies.loc[movies['movieId'] == movieid]['genres'].values[0]).reshape(1, -1)))\n    # Remove the genres column\n    movies_to_search = movies_to_search.drop(columns=['genres'])\n    # Distance value is in the list inside of the list so we need to unpack it\n    movies_to_search = movies_to_search.explode('dist').explode('dist')\n    # Sort the data and return top values\n    return movies_to_search.sort_values(by=['dist'], ascending=False)['movieId'].head(top).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next function takes 20 top-rated movies by a selected user and returns 5 similar movies for each of those.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_similar(userid):\n    # Take all the movies watched by user\n    movies_watched_by_user = user_ratings[user_ratings.userId == user_id]\n    # Only 4.5 or higher rating filtered\n    movies_watched_by_user = movies_watched_by_user[movies_watched_by_user['rating'] > 4.5]\n    # Taking top 20 with highest ratings\n    top_movies_user = (movies_watched_by_user.sort_values(by=\"rating\", ascending=False).head(20))\n    top_movies_user['watched_movieId'] = top_movies_user['movieId']\n    top_movies_user = top_movies_user[['userId', 'watched_movieId']]\n    # Find 5 similar movies for each of the selected above\n    top_movies_user['similar'] = top_movies_user['watched_movieId'].apply(lambda x: (get_cossim(x, 5)))\n    # Remove movies that user have already watched from recommendations\n    result = [x for x in np.concatenate(top_movies_user['similar'].values, axis=0).tolist() if x not in top_movies_user.watched_movieId.values.tolist()]\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'get_top' function returns top N recommended movies sorted by mean user rating. (only movies with 10 or more ratings are used).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top(id, top):\n    # taking movies that user may like\n    smlr = get_similar(id)    \n    # Calculating mean rationg for every movie\n    movie_data = pd.merge(user_ratings, movies, on='movieId')\n    ratings_mean_count = pd.DataFrame(movie_data.groupby('movieId')['rating'].mean())\n    ratings_mean_count['rating_counts'] = pd.DataFrame(movie_data.groupby('movieId')['rating'].count())\n    # Sorting movies with 10 or more ratings by users\n    ratings_mean_count = ratings_mean_count[ratings_mean_count['rating_counts'] > 10]\n    # Returning top N movies sorted by rating\n    return ratings_mean_count[ratings_mean_count.index.isin(smlr)].sort_values(by=['rating'], ascending=False).head(top)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collaborative filtering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The code was retrieved from example from keras.io. The model was tuned a little bit by me.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = user_ratings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_ids = df[\"userId\"].unique().tolist()\n# Reassign user IDs\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuserencoded2user = {i: x for i, x in enumerate(user_ids)}\n\nmovie_ids = df[\"movieId\"].unique().tolist()\n# Reassign movie IDs\nmovie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\nmovie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n\ndf[\"user\"] = df[\"userId\"].map(user2user_encoded)\ndf[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n\nnum_users = len(user2user_encoded)\nnum_movies = len(movie_encoded2movie)\ndf[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n# min and max ratings will be used to normalize the ratings later\nmin_rating = min(df[\"rating\"])\nmax_rating = max(df[\"rating\"])\n\nprint(\n    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n        num_users, num_movies, min_rating, max_rating\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalizing ratings and splitting data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac=1, random_state=42)\nx = df[[\"user\", \"movie\"]].values\n# Normalize the targets between 0 and 1. Makes it easy to train.\ny = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n# Assuming training on 90% of the data and validating on 10%.\ntrain_indices = int(0.9 * df.shape[0])\nx_train, x_val, y_train, y_val = (\n    x[:train_indices],\n    x[train_indices:],\n    y[:train_indices],\n    y[train_indices:],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining model structure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_SIZE = 50\n\nclass RecommenderNet(keras.Model):\n    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n        super(RecommenderNet, self).__init__(**kwargs)\n        self.num_users = num_users\n        self.num_movies = num_movies\n        self.embedding_size = embedding_size\n        self.user_embedding = layers.Embedding(\n            num_users,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.user_bias = layers.Embedding(num_users, 1)\n        self.movie_embedding = layers.Embedding(\n            num_movies,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.movie_bias = layers.Embedding(num_movies, 1)\n\n    def call(self, inputs):\n        user_vector = self.user_embedding(inputs[:, 0])\n        user_bias = self.user_bias(inputs[:, 0])\n        movie_vector = self.movie_embedding(inputs[:, 1])\n        movie_bias = self.movie_bias(inputs[:, 1])\n        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n        # Add all the components (including bias)\n        x = dot_user_movie + user_bias + movie_bias\n        # The sigmoid activation forces the rating to between 0 and 1\n        return tf.nn.sigmoid(x)\n\nmodel = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\nmodel.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=0.0005)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the model in 15 epoches with batch size of 64 and learning rate of 0.0005.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=64,\n    epochs=15,\n    verbose=1,\n    validation_data=(x_val, y_val),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Set the ID of a user that we will be recommending movies to.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"user_id = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get top 20 recommendations based on previously watched movies.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top = get_top(user_id, 20)\ncontent_rec = top.index.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the top 20 recommendations based on similar users' preferences.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_df = pd.read_csv('/kaggle/input/movie-lens-small-latest-dataset/movies.csv') \n\n# Searching for movies that user already watched\nmovies_watched_by_user = df[df.userId == user_id]\n# Searching for movies that user haven't watched yet\nmovies_not_watched = movie_df[\n    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n][\"movieId\"]\n\nmovies_not_watched = list(\n    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n)\n\nmovies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\nuser_encoder = user2user_encoded.get(user_id)\nuser_movie_array = np.hstack(\n    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n)\n# Predicting ratings for movies\nratings = model.predict(user_movie_array).flatten()\n# Sorting predicted ratings and taking top 20\ntop_ratings_indices = ratings.argsort()[-20:][::-1]\n# Getting the actual IDs for movies\nrecommended_movie_ids = [\n    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mixing all recommendations in one list and randomly taking 10 of them to recommend.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Showing recommendations for user: {}\".format(user_id))\nprint(\"====\" * 10)\nprint(\"Movies with high ratings from user\")\nprint(\"----\" * 10)\ntop_movies_user = (\n    movies_watched_by_user.sort_values(by=\"rating\", ascending=False)\n    .head(10)\n    .movieId.values\n)\nmovie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)]\nfor row in movie_df_rows.itertuples():\n    print(row.title, \":\", row.genres)\n\nprint(\"----\" * 10)\nprint(\"Top 10 movie recommendations\")\nprint(\"----\" * 10)\n# Случайным образом выбираем 10 фильмов из рекомендаций\nto_recommend = random.sample((content_rec + recommended_movie_ids), 10)\nrecommended_movies = movie_df[movie_df[\"movieId\"].isin(to_recommend)]\nfor row in recommended_movies.itertuples():\n    print(row.title, \":\", row.genres)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some movies from content-based recommendations may repeat with each other and with collaborative filtering recommendations but it will only give them a higher chance of appearing in recommended movies, which is good in this example.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}