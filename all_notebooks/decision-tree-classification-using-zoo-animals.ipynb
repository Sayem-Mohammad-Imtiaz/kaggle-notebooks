{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd \ndf = pd.read_csv('../input/zoo.csv')\ndf.info()\nprint ('-------------------')\ndf.head()","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"398512e9d1d686dbedfea0bd1fc587e80dfcbd6f"},"cell_type":"code","source":"df2 = pd.read_csv('../input/class.csv')\ndf2.info()\nprint ('----------------')\ndf2.head()","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"455ad603fff11d879a07e1e1fb24f8c5ff91126c"},"cell_type":"code","source":"df3 = df.merge(df2,how='left',left_on='class_type',right_on='Class_Number')\ndf3.head()","execution_count":45,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"g = df3.groupby(by='Class_Type')['animal_name'].count()\ng / g.sum() * 100","execution_count":46,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fc4635be1d098f176ec304ad081d1d5e3b68e02"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(df3['Class_Type'],label=\"Count\",\n             order = df3['Class_Type'].value_counts().index) #sort bars\nplt.show()","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"fd1e60821db506a23aecb9841feb5788cc6d8094"},"cell_type":"markdown","source":"## Using the FacetGrid from Seaborn, we can look at the columns to help us understand what features may be more helpful than others in classification. [](http://)"},{"metadata":{"trusted":true,"_uuid":"4bd5c2e101c0440390c20049b5729599e6b69547"},"cell_type":"code","source":"feature_names = ['hair','feathers','eggs','milk','airborne','aquatic','predator','toothed',\n                 'backbone','breathes','venomous','fins','legs','tail','domestic']\n\ndf3['ct'] = 1\n\nfor f in feature_names:\n    g = sns.FacetGrid(df3, col=\"Class_Type\",  row=f, hue=\"Class_Type\")\n    g.map(plt.hist, \"ct\")\n    g.set(xticklabels=[])\n    plt.subplots_adjust(top=0.9)\n    g.fig.suptitle(f)","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"ba76922d89572f14ab099bac6a80ddf51189629c"},"cell_type":"markdown","source":"## Let's also look at the data using a heatmap."},{"metadata":{"trusted":true,"_uuid":"f01e30db74645ea359b5c2f24802f3fe48092a7e"},"cell_type":"code","source":"gr = df3.groupby(by='Class_Type').mean()\ncolumns = ['class_type','Class_Number','Number_Of_Animal_Species_In_Class','ct','legs'] #will handle legs separately since it's not binary\ngr.drop(columns, inplace=True, axis=1)\nplt.subplots(figsize=(10,4))\nsns.heatmap(gr, cmap=\"YlGnBu\")","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"3e3014cf29d6739cf62112943b4b3c4089f8219f"},"cell_type":"markdown","source":"## How about the legs column? How does it differ between class types?"},{"metadata":{"trusted":true,"_uuid":"8d6c9a4f0dfec171e30859894eeb480f94d3cb09"},"cell_type":"code","source":"sns.stripplot(x=df3[\"Class_Type\"],y=df3['legs'])","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"9abbe4a68dc3c8fe320d0ac8b5395c3b4e555b5c"},"cell_type":"markdown","source":"## Let's see how well a decision tree works if we use all of the features available to us and training with 20% of the data."},{"metadata":{"trusted":true,"_uuid":"43fffb8bc7117c8235a2730d9ddba909b2aa2ed8"},"cell_type":"code","source":"#specify the inputs (x = predictors, y = class)\nX = df[feature_names]\ny = df['class_type'] #there are multiple classes in this column\n\n#split the dataframe into train and test groups\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.2, test_size=.8)\n\n#specify the model to train with\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier().fit(X_train, y_train)\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning) #ignores warning that tells us dividing by zero equals zero\n\n#let's see how well it worked\npred = clf.predict(X_test)\nprint('Accuracy of classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))\nprint()\nprint(confusion_matrix(y_test, pred))\nprint()\nprint(classification_report(y_test, pred))","execution_count":51,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ace90072db4117be3b86ee92613bf1029985bc5e"},"cell_type":"code","source":"df3[['Class_Type','class_type']].drop_duplicates().sort_values(by='class_type') #this is the order of the labels in the confusion matrix above","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"583c304b451b49ab87aaf3ab40b0ebc4d6f87de9"},"cell_type":"markdown","source":"1. Accuracy <br/>\nCorrectly predicted observations overall <br/>\n<br/>\n1. Precison <br/>\n*How many animals that were labeled in class actually belong to that class? <br/>\nHigher score = less false positives <br/>*\n<br/>\n1. Recall <br/>\n*How many animals that belong to that class were assigned that class? <br/>\nGenerally a better metric for classes where you'd rather have a false positive than miss a true positive, like perhaps a healthcare screening or fraud flag <br/>*\n<br/>\n1. F1 score = weighted combination of precision and recall <br/>\n<br/>\n1. Support = the number of occurences in that class <br/>\n"},{"metadata":{"_uuid":"47d7d2c4f5500585ba8e622d379a90dfa732ac1d"},"cell_type":"markdown","source":"## What features were the most important in this model?"},{"metadata":{"trusted":true,"_uuid":"2903ef2fdb2ef986e4f4738593251cd56eec31b1"},"cell_type":"code","source":"imp = pd.DataFrame(clf.feature_importances_)\nft = pd.DataFrame(feature_names)\nft_imp = pd.concat([ft,imp],axis=1,join_axes=[ft.index])\nft_imp.columns = ['Feature', 'Importance']\nft_imp.sort_values(by='Importance',ascending=False)","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"fa48c523954b5f48c912a459efe4b02e9c12f16e"},"cell_type":"markdown","source":"## What if we reduced the training set size to 10%?"},{"metadata":{"trusted":true,"_uuid":"f3ac1a9e8676b8f7316d8dd04fc04fc71caad59e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.1, test_size=.9) \n\nclf2 = DecisionTreeClassifier().fit(X_train, y_train)\npred = clf2.predict(X_test)\nprint('Accuracy of classifier on test set: {:.2f}'\n     .format(clf2.score(X_test, y_test)))\nprint()\nprint(confusion_matrix(y_test, pred))\nprint()\nprint(classification_report(y_test, pred))","execution_count":54,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00b7af072721eaa2ad9cd0c596fd12afff5aab73"},"cell_type":"code","source":"imp2 = pd.DataFrame(clf2.feature_importances_)\nft_imp2 = pd.concat([ft,imp2],axis=1,join_axes=[ft.index])\nft_imp2.columns = ['Feature', 'Importance']\nft_imp2.sort_values(by='Importance',ascending=False)","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"bfc61d81eedc09a9c73720ed861dfc51a43b25fa"},"cell_type":"markdown","source":"## Let's go back to 20% in the training group and focus on visible features of the animals."},{"metadata":{"trusted":true,"_uuid":"7aec03ba95e80b4fc2b8fce0903c35df44d04a71"},"cell_type":"code","source":"visible_feature_names = ['hair','feathers','toothed','fins','legs','tail']\n\nX = df[visible_feature_names]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.2, test_size=.8)\n\nclf3= DecisionTreeClassifier().fit(X_train, y_train)\n\npred = clf3.predict(X_test)\nprint('Accuracy of classifier on test set: {:.2f}'\n     .format(clf3.score(X_test, y_test)))\nprint()\nprint(confusion_matrix(y_test, pred))\nprint()\nprint(classification_report(y_test, pred))","execution_count":56,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47908b06d7610232012ee1f753ff1b144c0183c8"},"cell_type":"code","source":"imp3= pd.DataFrame(clf3.feature_importances_)\nft = pd.DataFrame(visible_feature_names)\nft_imp3 = pd.concat([ft,imp3],axis=1,join_axes=[ft.index])\nft_imp3.columns = ['Feature', 'Importance']\nft_imp3.sort_values(by='Importance',ascending=False)","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"f2f559cc61e0a7b745ec91876dbe4b08ea37c7ec"},"cell_type":"markdown","source":"## If the dataset were larger, reducing the depth size of the tree would be useful to minimize memory required to perform the analysis. Below I've limited it to two still using the same train/test groups and visible features group as above."},{"metadata":{"trusted":true,"_uuid":"d1aaf1b07f3e68ba028194c439e5558498b73a89"},"cell_type":"code","source":"clf4= DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n\npred = clf4.predict(X_test)\nprint('Accuracy of classifier on test set: {:.2f}'\n     .format(clf4.score(X_test, y_test)))\nprint()\nprint(confusion_matrix(y_test, pred))\nprint()\nprint(classification_report(y_test, pred))","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87c77cc0e7ba521b8f408de797a9aa8229d48222"},"cell_type":"code","source":"imp4= pd.DataFrame(clf4.feature_importances_)\nft_imp4 = pd.concat([ft,imp3],axis=1,join_axes=[ft.index])\nft_imp4.columns = ['Feature', 'Importance']\nft_imp4.sort_values(by='Importance',ascending=False)","execution_count":59,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5632cd8a24c35455cb8b77e00d4857fe694bd53a"},"cell_type":"code","source":"columns = ['Model','Test %', 'Accuracy','Precision','Recall','F1','Train N']\ndf_ = pd.DataFrame(columns=columns)\n\ndf_.loc[len(df_)] = [\"Model 1\",20,.78,.80,.78,.77,81] #wrote the metrics down on paper and input into this dataframe\ndf_.loc[len(df_)] = [\"Model 2\",10,.68,.62,.68,.64,91]\ndf_.loc[len(df_)] = [\"Model 3\",20,.91,.93,.91,.91,81]\ndf_.loc[len(df_)] = [\"Model 4\",20,.57,.63,.57,.58,81]\nax=df_[['Accuracy','Precision','Recall','F1']].plot(kind='bar',cmap=\"YlGnBu\", figsize=(10,6))\nax.set_xticklabels(df_.Model)","execution_count":68,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}