{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Discussion Related with Outliers and Impact on Machine Learning ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Which Machine Learning Models are sensitive to outliers?","metadata":{}},{"cell_type":"markdown","source":"1. Naive Bayes Classifier----------------Not Sensitive to Outliers\n2. SVM----------------------------------Not Sensitive to Outliers\n3. Linear Regression---------------------Sensitive to Outliers\n4. Logistic Regression-------------------Sensitive to Outliers\n5. Decision Tree Regressor or Classifier-Not Sensitive to Outliers\n6. Ensemble(RF, XGBoost, GB)-------------Not Sensitive to Outliers\n7. KNN-----------------------------------Not Sensitive to Outliers\n8. Kmeans--------------------------------Sensitive to Outliers\n9. Hierarchical--------------------------Sensitive to Outliers\n10. PCA----------------------------------Sensitive to Outliers\n11. Neural Networks----------------------Sensitive to Outliers","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/titanic-machine-learning-from-disaster/train.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Age'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['Age'].dropna())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['Age'].fillna(100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gaussian Distributed","metadata":{}},{"cell_type":"code","source":"figure = df.Age.hist(bins=50)\nfigure.set_title('Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('No of passenger')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = df.boxplot(column='Age')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Age'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If the Data is normally Distributed we use this","metadata":{}},{"cell_type":"code","source":"# assuming Age follows a gaussian distribution we will calculate the boundaries which differentiates the outliers\nupper_boundary = df['Age'].mean() + 3*df['Age'].std()\nlower_boundary = df['Age'].mean() - 3*df['Age'].std()\nprint(lower_boundary), print(upper_boundary), print(df['Age'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If features are skewed we use the below technique ","metadata":{}},{"cell_type":"code","source":"figure = df.Fare.hist(bins=50)\nfigure.set_title('Fare')\nfigure.set_xlabel('Fare')\nfigure.set_ylabel('No of passenger')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(column='Fare')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Fare'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets compute the interquantile range to calculate the boundaries\nIQR = df.Fare.quantile(0.75)-df.Fare.quantile(0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower_bridge = df['Fare'].quantile(0.25)-(IQR*1.5)\nupper_bridge = df['Fare'].quantile(0.75)+(IQR*1.5)\nprint(lower_bridge), print(upper_bridge)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extreme outliers\nlower_bridge = df['Fare'].quantile(0.25)-(IQR*3)\nupper_bridge = df['Fare'].quantile(0.75)+(IQR*3)\nprint(lower_bridge), print(upper_bridge)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[data['Age']>=73,'Age']=73","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[data['Fare']>=100,'Fare']=100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure=data.Age.hist(bins=50)\nfigure.set_title('Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('No of passenger')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure=data.Fare.hist(bins=50)\nfigure.set_title('Fare')\nfigure.set_xlabel('Fare')\nfigure.set_ylabel('No of passenger')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(data[['Age','Fare']].fillna(0),data['Survived'],test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\ny_pred1=classifier.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nprint(\"Accuracy_score: {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"roc_auc_score: {}\".format(roc_auc_score(y_test,y_pred1[:,1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying Logistic Regression\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\ny_pred1=classifier.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nprint(\"Accuracy_score: {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"roc_auc_score: {}\".format(roc_auc_score(y_test,y_pred1[:,1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}