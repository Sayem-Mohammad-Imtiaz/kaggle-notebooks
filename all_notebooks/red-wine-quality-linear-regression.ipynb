{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Red wine quality - linear regression\nIn this kernel I will be applying different flavours of linear regressions to try to predict wine quality as a function of its physical properties.\n\nSome of the ideas for the EDA in this kernel have been inspired by https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Let's start with some imports and loading our dataset\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, learning_curve, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\ndf = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's meet the dataset\nOur target variable is `quality`. It looks like an ordered categorical variable, where a big value means a good wine. Let's explore how it looks like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='quality', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our wine scale ranges between 3 and 8. There are some really good wines (7-8), while most of them have an average quality between 5 and 6, and some of them are really poor (3-4).\n\nLet's now identify which physical properties are the ones that affect quality the most. We will use the correlation matrix, and will also take negative correlations into account, as some physical properties, like excessive acidity, may affect negatively the wine quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nidx = corr['quality'].abs().sort_values(ascending=False).index[:5]\nidx_features = idx.drop('quality')\nsns.heatmap(corr.loc[idx, idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the correlation matrix, the four more influential properties are:\n- Alcohol. Seems like a positive correlation: the more alcohol, the better the wine. Bibliography suggests there is an optimal value for alcohol around 13.6-14.0.\n- Volatile acidity. This represents the presence of certain volatile acids, like acetic acid. Too much acetic acid is considered a wine fault. We've got a negative correlation here, which makes sense.\n- Sulphates. Weak positive correlation.\n- Citric acid. May be added to wine to give a more 'fres' flavor. Weak positive correlation.\n\nLet's see what these variables look like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(2, 2, figsize=(20, 10))\nfor var, axis in zip(idx_features, ax.flatten()):\n    df[var].plot.hist(ax=axis)\n    axis.set_xlabel(var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost all wines have less than the optimal alcohol quality, which explains the strong positive correlation. All variables seem to have not very different ranges, so we may not need feature scaling. Let's confirm this for the rest of the variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no drastic differences in scale so we will not do feature scaling at all.\n\nLet's now visualize the five most relevant variables and their interactions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, vars=idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sulphates seem to have some positive correlation with citric acid. Volatile acidity presents negative correlation with citric acid. Citric acid is not a volatile acid, so this is not that surprising. Other than that, most variables appear to not have much correlation.\n\nAs quality is an ordered categorical variable, it is difficult to visualize relationships using scatter plots - let's do some box plots instead:"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(2, 2, figsize=(20, 10))\nfor i, var in enumerate(idx.drop('quality')):\n    sns.boxplot(x='quality', y=var, data=df, ax=ax.flatten()[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This confirms our suspicions, more or less. Note that the quality-alcohol relationship only appears to be lineal for values of quality between 5 and 8, so we may want to use a higher order polynomial to model it. There are also a lot of outliers in sulphates (TODO: we may want to treat these somehow?)\n\nLet's get into creating our model. We will try several different linear models, with and without regularization and polynomial features. We will also plot learning curves to visualize if we have bias or variance problems. Let's define some functions to do that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curves(X, y, model):\n    train_sizes, train_scores, cv_scores = learning_curve(model, X, y)\n    train_scores = np.mean(train_scores[1:], axis=1)\n    cv_scores = np.mean(cv_scores[1:], axis=1)\n    plt.figure(figsize=(10,10))\n    plt.plot(train_sizes[1:], train_scores, label='Train')\n    plt.plot(train_sizes[1:], cv_scores, label='CV')\n    plt.xlabel('Sample size')\n    plt.ylabel('R2')\n    plt.legend()\n    \ndef train(X, y, model, poly_degree=None):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n    if poly_degree is not None:\n        pol = PolynomialFeatures(poly_degree, include_bias=False)\n        X_train = pol.fit_transform(X_train)\n        X_test = pol.transform(X_test)\n    model.fit(X_train, y_train)\n    r2_train = model.score(X_train, y_train)\n    r2_test = model.score(X_test, y_test)\n    print('r2_train = {:.3f}, r2_test={:.3f}'.format(r2_train, r2_test))\n    plot_learning_curves(X, y, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Simple linear model\n\nKISS! Let's start with the simplest possible model: let's input all variables into a simple linera regression model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple linear regression\nfeatures = df.drop(columns='quality')\nX = features.copy()\ny = df['quality']\ntrain(X, y, LinearRegression())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not very optimal. Let's try now with polynomial features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try adding some polynomic features\ntrain(X, y, LinearRegression(), poly_degree=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like we have a little bit of overfitting around here. Let's add some regularization to the equation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train(X, y, Ridge(alpha=2.0), poly_degree=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We did a _little_ bit better than the simple linear model. How about selecting the top four features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_subset = df[idx].drop(columns='quality')\ntrain(feature_subset, y, Ridge(5.0), poly_degree=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That didn't help either. So far, the best model we have achieved is the regularized linear with degree two polynomial features - and it's not a great model.\n\nThis concludes my exploration of linear models for this dataset. In the next kernels, I will be approaching this problem differently - as a classification problem instead, trying to predict whether a wine has good quality (`quality` > threshold) or not.\n\nIf you have any feedback or suggestions to improve this notebook, please say! And if you found it useful, please leave an upvote :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}