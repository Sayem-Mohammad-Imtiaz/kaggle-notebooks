{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  SIGN-LANGUAGE\n\n![](https://assets.skyfilabs.com/images/blog/sign-language-translator.webp)\n\n#### Sign languages (also known as signed languages) are languages that utilize the visual-manual methodology to pass on importance. Language is communicated through the manual sign stream in the mix with non-manual components. The dataset called (Sign Language MNIST) is of the American Sign Language hand gestures representing letters. This is a multi-class classification problem with 24 classes of letters ( excluding J and Z which require motion).  ####\n\n##### This dataset is licensed under the  CC0 1.0 Universal Public Domain Dedication license.#####\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as ssn\nimport numpy as np\nimport keras\nfrom keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Dropout, MaxPool2D\nfrom keras.models import Sequential\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom pandas_profiling import ProfileReport\nimport time\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\nbase={0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',\n      14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X',24:'Y',25:'Z'}\n\n\n# Using tensorflow as backend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data= pd.read_csv('../input/sign-language-mnist/sign_mnist_train.csv')\ntest_data= pd.read_csv('../input/sign-language-mnist/sign_mnist_test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels=np.array(train_data['label'])\ndel train_data['label']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train= train_data.values\nx_train= x_train.reshape(-1,28,28,1)\n\n\nf, ax = plt.subplots(2,4)\n\nindex=0\n\nfor i in range(2):\n    for j in range(4):\n        ax[i,j].imshow(x_train[index].reshape(28,28),cmap='gray')\n        index+=1\n\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Images after normalising the inputs\n\n\nx_train=x_train/255.\n\nw, mx= plt.subplots(4,4)\n\nk=0\nfor i in range(4):\n    for j in range(4):\n        mx[i,j].imshow(x_train[k].reshape(28,28),cmap='binary')\n        mx[i,j].set_title(base[train_labels[k]])\n        \n        k+=1\n        \n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The total number of labels are {}\".format(len(set(train_labels))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_char=[]\nfor i in range(len(train_labels)):\n    train_labels_char.append(base[(train_labels[i])])\n\ntrain_labels_char=sorted(train_labels_char)\n\nssn.countplot(train_labels_char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### There is almost the similar count in all the labels. It means that our training data is varied properly among the labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels=test_data['label']\n\ny=test_data['label']\n\ndel test_data['label']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=test_data.values\ntest_data=test_data.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the uniformity of testing labels\n\nssn.countplot(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nx_test=test_data\nx_test=x_test/255.\nx_test=x_test.reshape(-1,28,28,1)\nprint(\"The number of variables in x_test is {}\".format(len(x_test)))\nplt.imshow(x_test[random.choice(range(1,len(x_test)))].reshape(28,28),cmap='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To overcome overfitting problem, I am trying to increase the training data by using ImageDataGenerator from the existing training data.\n\ndgen= ImageDataGenerator(featurewise_center=False,samplewise_center=False,featurewise_std_normalization=False,\n                        samplewise_std_normalization=False,zca_whitening=False, rotation_range=10, zoom_range=0.1,\n                        width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=False, vertical_flip=False)\ndgen.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Training the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel=Sequential()\n\n\n#Initial Conv2D layer with 64 filters with filter sizes of 3X3\nmodel.add(Conv2D(64,(3,3),strides=2,padding='same',activation='elu',input_shape=(28,28,1)))\n#Although RELU is common, I am experimenting with ELU. ELU is similar to RELU except negative inputs.\n\nmodel.add(MaxPool2D((2,2),strides=2,padding='same'))\n\n#Second Conv2D layer with 50 filters and 3X3 filter size\n\nmodel.add(Conv2D(50,(3,3),strides=1,padding='same',activation='elu'))\n\n#Third Conv2D layer with 128 filters and 3X3 filter size\n\nmodel.add(Conv2D(128,(3,3),strides=1,padding='same',activation='elu'))\n\nmodel.add(Flatten())\n\n#Output Layer\nmodel.add(Dense(units=24,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using TPU v3-8\n# x_train, train_labels, x_test, test_labels\nl_binarizer=LabelBinarizer()\ntrain_labels= l_binarizer.fit_transform(train_labels)\ntest_labels= l_binarizer.fit_transform(test_labels)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time=time.time()\nhistory=model.fit(dgen.flow(x_train,train_labels,batch_size=50),epochs=20, validation_data=(x_test,test_labels))\nend_time=time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mins= int(round(end_time-start_time,2)//60)\nseconds= round(round(end_time-start_time,2)%60)\nprint(\"The time taken to build the model is {} minutes {} seconds\".format(mins,seconds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The accuracy of the model is \",model.evaluate(x_test,test_labels)[1]*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_accuracy=history.history['accuracy']\ntraining_loss= history.history['loss']\nvalidation_accuracy=history.history['val_accuracy']\nvalidation_loss= history.history['val_loss']\nepochs=list(range(1,21))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy Part\n\nm, ax= plt.subplots(1,2)\nax[0].plot(epochs,training_accuracy,'yo--',label='Train Accuracy')\nax[0].plot(epochs, validation_accuracy,'bo--',label='Validation Accuracy')\nax[0].set_title('Accuracy')\nax[0].legend()\n\n\n#Loss Part\n\nax[1].plot(epochs, training_loss, 'yo-',label='Training Loss')\nax[1].plot(epochs, validation_loss,'bo-',label='Validation Loss')\nax[1].set_title(\"Loss\")\nax[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd= model.predict_classes(x_test)\nplt.figure(figsize=(16,16))\ncm= confusion_matrix(y,pd)\nssn.heatmap(cm, fmt=\"d\",cmap='PuRd',annot=True,linewidths=1,square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total=0\n\nf, mx= plt.subplots(4,2)\nk=300\nfor i in range(4):\n    for j in range(2):\n        mx[i,j].imshow(x_test[k].reshape(28,28),cmap='pink')\n        mx[i,j].set_title(\"Predicted: {} Actual: {}\".format(pd[k],y[k]))\n        if pd[k]==y[k]:\n            total+=1\n        k+=1\n        \n    plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total correctly classified results from random k is {} out of 8\".format(total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}