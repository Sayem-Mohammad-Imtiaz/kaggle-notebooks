{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import HTML\nHTML(\"\"\"\n<style>\nh1,h2,h3 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\tbox-shadow: \n\t\tinset 0 0 0 1px rgba(97,0,45, 1), \n\t\tinset 0 0 5px rgba(53,86,129, 1),\n\t\tinset -285px 0 35px #F2D8FF;\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n}\n</style>\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:06.926782Z","iopub.execute_input":"2021-08-10T09:54:06.927169Z","iopub.status.idle":"2021-08-10T09:54:06.93436Z","shell.execute_reply.started":"2021-08-10T09:54:06.927135Z","shell.execute_reply":"2021-08-10T09:54:06.933065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:#FF85A3;font-size:20px;color:#00033E;font-weight : bold\">âœ… Importing Required Libraries</h1>","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom prettytable import PrettyTable\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport math\nimport matplotlib.cm as cm\nfrom matplotlib import rcParams\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.model_selection import RandomizedSearchCV\nimport re\nimport time\nimport string\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Conv1D, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.tokenize import RegexpTokenizer\nfrom wordcloud import WordCloud\nimport glob\n\n%matplotlib inline","metadata":{"id":"-E4TvIVLMa73","scrolled":true,"execution":{"iopub.status.busy":"2021-08-10T09:54:12.058603Z","iopub.execute_input":"2021-08-10T09:54:12.058986Z","iopub.status.idle":"2021-08-10T09:54:12.077375Z","shell.execute_reply.started":"2021-08-10T09:54:12.05895Z","shell.execute_reply":"2021-08-10T09:54:12.07622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing plaintext data files","metadata":{}},{"cell_type":"code","source":"cols = ['text']\nplaintext_files=pd.DataFrame()\naddress = glob.glob('/kaggle/input/clinical-documents-on-syndromes-disease/plaintext_data/*.txt')\ncount=0\nfor i in address:\n    with open(i, \"rb\") as data_of_files:\n        data=pd.read_csv(data_of_files, sep='\\t', header=None, names=cols)\n        plaintext_files=pd.concat([plaintext_files,data['text']], axis=0)\n        count=count+1\n        if count==493:\n            break  \n\nplaintext_files=plaintext_files.rename(columns={0:'text'})          \nplaintext_files.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:12.716119Z","iopub.execute_input":"2021-08-10T09:54:12.716512Z","iopub.status.idle":"2021-08-10T09:54:14.345003Z","shell.execute_reply.started":"2021-08-10T09:54:12.716477Z","shell.execute_reply":"2021-08-10T09:54:14.3441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning and removing punctuations ","metadata":{}},{"cell_type":"code","source":"english_punctuations = string.punctuation\npunctuations_list = english_punctuations\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:14.346459Z","iopub.execute_input":"2021-08-10T09:54:14.346748Z","iopub.status.idle":"2021-08-10T09:54:14.351645Z","shell.execute_reply.started":"2021-08-10T09:54:14.34672Z","shell.execute_reply":"2021-08-10T09:54:14.350597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plaintext_files[\"text\"] = plaintext_files[\"text\"].apply(lambda x: cleaning_punctuations(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:14.35364Z","iopub.execute_input":"2021-08-10T09:54:14.354048Z","iopub.status.idle":"2021-08-10T09:54:14.419806Z","shell.execute_reply.started":"2021-08-10T09:54:14.353997Z","shell.execute_reply":"2021-08-10T09:54:14.418823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning special character ","metadata":{}},{"cell_type":"code","source":"def cleaning_characters(text): \n    \n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:14.421268Z","iopub.execute_input":"2021-08-10T09:54:14.421565Z","iopub.status.idle":"2021-08-10T09:54:14.425801Z","shell.execute_reply.started":"2021-08-10T09:54:14.421536Z","shell.execute_reply":"2021-08-10T09:54:14.424939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plaintext_files[\"text\"] = plaintext_files[\"text\"].apply(lambda x: cleaning_characters(x))\nplaintext_files","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:14.427209Z","iopub.execute_input":"2021-08-10T09:54:14.427513Z","iopub.status.idle":"2021-08-10T09:54:14.484764Z","shell.execute_reply.started":"2021-08-10T09:54:14.427486Z","shell.execute_reply":"2021-08-10T09:54:14.483836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Making the sentences of plain text sentences into words in each row","metadata":{}},{"cell_type":"code","source":"plaintext_files= plaintext_files.text.str.split(expand=True).stack()\nplaintext_files","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:14.48599Z","iopub.execute_input":"2021-08-10T09:54:14.486322Z","iopub.status.idle":"2021-08-10T09:54:16.362663Z","shell.execute_reply.started":"2021-08-10T09:54:14.48629Z","shell.execute_reply":"2021-08-10T09:54:16.361746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing ground truth annotation file","metadata":{}},{"cell_type":"code","source":"ground_truth=pd.read_csv('/kaggle/input/clinical-documents-on-syndromes-disease/ground_truth_annotation_file.csv')\nground_truth.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:16.364488Z","iopub.execute_input":"2021-08-10T09:54:16.36479Z","iopub.status.idle":"2021-08-10T09:54:16.393958Z","shell.execute_reply.started":"2021-08-10T09:54:16.364762Z","shell.execute_reply":"2021-08-10T09:54:16.393073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Taking the text in list","metadata":{}},{"cell_type":"code","source":"li=list(ground_truth['text'])\nli","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:16.395315Z","iopub.execute_input":"2021-08-10T09:54:16.395606Z","iopub.status.idle":"2021-08-10T09:54:16.422191Z","shell.execute_reply.started":"2021-08-10T09:54:16.395578Z","shell.execute_reply":"2021-08-10T09:54:16.421347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Matching the ground_truth with the plain text and creating the sentence, word and tag","metadata":{}},{"cell_type":"code","source":"Processed_data=pd.DataFrame({\n    'Sentence#': plaintext_files.index.get_level_values(0) + 1, \n    'Word': plaintext_files.values, \n    'Tag': plaintext_files.map(dict(zip(ground_truth.text, ground_truth['class']))).fillna(0).values\n})","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:16.423389Z","iopub.execute_input":"2021-08-10T09:54:16.423841Z","iopub.status.idle":"2021-08-10T09:54:16.492336Z","shell.execute_reply.started":"2021-08-10T09:54:16.423807Z","shell.execute_reply":"2021-08-10T09:54:16.491419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"#### Five top records of Processed_data","metadata":{}},{"cell_type":"code","source":"Processed_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:23.948896Z","iopub.execute_input":"2021-08-10T09:54:23.949464Z","iopub.status.idle":"2021-08-10T09:54:23.959596Z","shell.execute_reply.started":"2021-08-10T09:54:23.949416Z","shell.execute_reply":"2021-08-10T09:54:23.95874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Five last records of Processed_data","metadata":{}},{"cell_type":"code","source":"Processed_data.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:24.583808Z","iopub.execute_input":"2021-08-10T09:54:24.584347Z","iopub.status.idle":"2021-08-10T09:54:24.594245Z","shell.execute_reply.started":"2021-08-10T09:54:24.584304Z","shell.execute_reply":"2021-08-10T09:54:24.59337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Coloumns/features in Processed_data","metadata":{}},{"cell_type":"code","source":"Processed_data.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:25.161651Z","iopub.execute_input":"2021-08-10T09:54:25.161994Z","iopub.status.idle":"2021-08-10T09:54:25.170526Z","shell.execute_reply.started":"2021-08-10T09:54:25.161966Z","shell.execute_reply":"2021-08-10T09:54:25.169724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Length of Processed_data","metadata":{}},{"cell_type":"code","source":"print('lenght of Processed_data is', len(Processed_data))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:25.780248Z","iopub.execute_input":"2021-08-10T09:54:25.780803Z","iopub.status.idle":"2021-08-10T09:54:25.785756Z","shell.execute_reply.started":"2021-08-10T09:54:25.780758Z","shell.execute_reply":"2021-08-10T09:54:25.784768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shape of Processed_data","metadata":{}},{"cell_type":"code","source":"Processed_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:27.33279Z","iopub.execute_input":"2021-08-10T09:54:27.333369Z","iopub.status.idle":"2021-08-10T09:54:27.339317Z","shell.execute_reply.started":"2021-08-10T09:54:27.33332Z","shell.execute_reply":"2021-08-10T09:54:27.338249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Processed_data information","metadata":{}},{"cell_type":"code","source":"Processed_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:28.255005Z","iopub.execute_input":"2021-08-10T09:54:28.255409Z","iopub.status.idle":"2021-08-10T09:54:28.310211Z","shell.execute_reply.started":"2021-08-10T09:54:28.255376Z","shell.execute_reply":"2021-08-10T09:54:28.309153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Processed_data types of all coloumns","metadata":{}},{"cell_type":"code","source":"Processed_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:28.986842Z","iopub.execute_input":"2021-08-10T09:54:28.987229Z","iopub.status.idle":"2021-08-10T09:54:28.99504Z","shell.execute_reply.started":"2021-08-10T09:54:28.987192Z","shell.execute_reply":"2021-08-10T09:54:28.99385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Null values","metadata":{}},{"cell_type":"code","source":"np.sum(Processed_data.isnull().any(axis=1))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-10T09:54:29.440188Z","iopub.execute_input":"2021-08-10T09:54:29.440702Z","iopub.status.idle":"2021-08-10T09:54:29.489455Z","shell.execute_reply.started":"2021-08-10T09:54:29.440671Z","shell.execute_reply":"2021-08-10T09:54:29.488418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rows and columns in the Processed_dataset","metadata":{}},{"cell_type":"code","source":"print('Count of columns in the Processed_data is:  ', len(Processed_data.columns))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:29.907734Z","iopub.execute_input":"2021-08-10T09:54:29.90811Z","iopub.status.idle":"2021-08-10T09:54:29.913162Z","shell.execute_reply.started":"2021-08-10T09:54:29.908073Z","shell.execute_reply":"2021-08-10T09:54:29.911996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Count of rows in the Processed_data is:  ', len(Processed_data))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:30.345464Z","iopub.execute_input":"2021-08-10T09:54:30.345861Z","iopub.status.idle":"2021-08-10T09:54:30.351452Z","shell.execute_reply.started":"2021-08-10T09:54:30.345833Z","shell.execute_reply":"2021-08-10T09:54:30.35044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking duplicate Processed_data","metadata":{}},{"cell_type":"code","source":"current=len(Processed_data)\nprint('Rows of Processed_data before Delecting ', current)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:31.152743Z","iopub.execute_input":"2021-08-10T09:54:31.153287Z","iopub.status.idle":"2021-08-10T09:54:31.157645Z","shell.execute_reply.started":"2021-08-10T09:54:31.153255Z","shell.execute_reply":"2021-08-10T09:54:31.15693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"now=len(Processed_data)\nprint('Rows of Processed_data before Delecting ', now)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:31.359354Z","iopub.execute_input":"2021-08-10T09:54:31.3599Z","iopub.status.idle":"2021-08-10T09:54:31.365749Z","shell.execute_reply.started":"2021-08-10T09:54:31.359868Z","shell.execute_reply":"2021-08-10T09:54:31.364651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff=current-now\nprint('Duplicated rows are ', diff)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:31.617636Z","iopub.execute_input":"2021-08-10T09:54:31.617978Z","iopub.status.idle":"2021-08-10T09:54:31.623723Z","shell.execute_reply.started":"2021-08-10T09:54:31.617946Z","shell.execute_reply":"2021-08-10T09:54:31.622482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Count of the class","metadata":{}},{"cell_type":"code","source":"sns.countplot(data= Processed_data, x = \"Tag\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:31.910595Z","iopub.execute_input":"2021-08-10T09:54:31.911103Z","iopub.status.idle":"2021-08-10T09:54:32.262488Z","shell.execute_reply.started":"2021-08-10T09:54:31.91107Z","shell.execute_reply":"2021-08-10T09:54:32.26168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Processed_data[\"Tag\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:32.263611Z","iopub.execute_input":"2021-08-10T09:54:32.263984Z","iopub.status.idle":"2021-08-10T09:54:32.554829Z","shell.execute_reply.started":"2021-08-10T09:54:32.263956Z","shell.execute_reply":"2021-08-10T09:54:32.553688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Processed_data[\"Tag\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:32.556927Z","iopub.execute_input":"2021-08-10T09:54:32.55737Z","iopub.status.idle":"2021-08-10T09:54:32.623393Z","shell.execute_reply.started":"2021-08-10T09:54:32.557328Z","shell.execute_reply":"2021-08-10T09:54:32.621462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Words tagged as Others","metadata":{}},{"cell_type":"code","source":"Processed_data.loc[Processed_data['Tag'] == 'Others', 'Word'].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:34.859226Z","iopub.execute_input":"2021-08-10T09:54:34.859586Z","iopub.status.idle":"2021-08-10T09:54:34.898937Z","shell.execute_reply.started":"2021-08-10T09:54:34.859556Z","shell.execute_reply":"2021-08-10T09:54:34.897964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Words tagged as Disease_Syndrome      ","metadata":{}},{"cell_type":"code","source":"Processed_data.loc[Processed_data['Tag'] == 'Disease_Syndrome', 'Word'].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:35.305835Z","iopub.execute_input":"2021-08-10T09:54:35.306219Z","iopub.status.idle":"2021-08-10T09:54:35.344939Z","shell.execute_reply.started":"2021-08-10T09:54:35.306176Z","shell.execute_reply":"2021-08-10T09:54:35.343833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Words tagged as 0      ","metadata":{}},{"cell_type":"code","source":"Processed_data.loc[Processed_data['Tag'] == 0, 'Word'].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:35.691793Z","iopub.execute_input":"2021-08-10T09:54:35.692195Z","iopub.status.idle":"2021-08-10T09:54:35.731473Z","shell.execute_reply.started":"2021-08-10T09:54:35.692158Z","shell.execute_reply":"2021-08-10T09:54:35.730353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"### Making text text in lower case","metadata":{}},{"cell_type":"code","source":"Processed_data['Word']=Processed_data['Word'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:36.304171Z","iopub.execute_input":"2021-08-10T09:54:36.304523Z","iopub.status.idle":"2021-08-10T09:54:36.410919Z","shell.execute_reply.started":"2021-08-10T09:54:36.304495Z","shell.execute_reply":"2021-08-10T09:54:36.410192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Stemming","metadata":{}},{"cell_type":"code","source":"st = nltk.PorterStemmer()\ndef stemming_on_text(Processed_data):\n    text = [st.stem(word) for word in Processed_data]\n    return Processed_data\n\nProcessed_data[\"Word\"] = Processed_data[\"Word\"].apply(lambda x: stemming_on_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:36.736319Z","iopub.execute_input":"2021-08-10T09:54:36.736691Z","iopub.status.idle":"2021-08-10T09:54:37.871362Z","shell.execute_reply.started":"2021-08-10T09:54:36.736657Z","shell.execute_reply":"2021-08-10T09:54:37.870045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Lemmatizer\n- We used the lemmatization because it helps to achieve the roots of the words. Since we are working on the words basis so it is very important to know each word with its root not many words which give the same meaning. ","metadata":{}},{"cell_type":"code","source":"lm = nltk.WordNetLemmatizer()\ndef lemmatizer_on_text(Processed_data):\n    text = [lm.lemmatize(word) for word in Processed_data]\n    return Processed_data\n\nProcessed_data[\"Word\"] = Processed_data[\"Word\"].apply(lambda x: lemmatizer_on_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:37.87328Z","iopub.execute_input":"2021-08-10T09:54:37.873715Z","iopub.status.idle":"2021-08-10T09:54:43.310385Z","shell.execute_reply.started":"2021-08-10T09:54:37.873667Z","shell.execute_reply":"2021-08-10T09:54:43.309487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Taking only Others and Disease_Syndrome","metadata":{}},{"cell_type":"code","source":"aa=Processed_data[Processed_data['Tag']=='Others']\nbb=Processed_data[Processed_data['Tag']=='Disease_Syndrome']\nProcessed_data=pd.concat([aa,bb])\nProcessed_data","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:43.311777Z","iopub.execute_input":"2021-08-10T09:54:43.31218Z","iopub.status.idle":"2021-08-10T09:54:43.388346Z","shell.execute_reply.started":"2021-08-10T09:54:43.312145Z","shell.execute_reply":"2021-08-10T09:54:43.387607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Count of the class","metadata":{}},{"cell_type":"code","source":"sns.countplot(data= Processed_data, x = \"Tag\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:43.389524Z","iopub.execute_input":"2021-08-10T09:54:43.38992Z","iopub.status.idle":"2021-08-10T09:54:43.513997Z","shell.execute_reply.started":"2021-08-10T09:54:43.389893Z","shell.execute_reply":"2021-08-10T09:54:43.513282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Processed_data[\"Tag\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:43.514989Z","iopub.execute_input":"2021-08-10T09:54:43.51538Z","iopub.status.idle":"2021-08-10T09:54:43.70867Z","shell.execute_reply.started":"2021-08-10T09:54:43.515352Z","shell.execute_reply":"2021-08-10T09:54:43.707652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding the Others as 0 and Disease_Syndrome as 1","metadata":{}},{"cell_type":"code","source":"Processed_data['Tag']=Processed_data['Tag'].replace('Others',1)\nProcessed_data['Tag']=Processed_data['Tag'].replace('Disease_Syndrome',2)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:43.709946Z","iopub.execute_input":"2021-08-10T09:54:43.710245Z","iopub.status.idle":"2021-08-10T09:54:43.732531Z","shell.execute_reply.started":"2021-08-10T09:54:43.710217Z","shell.execute_reply":"2021-08-10T09:54:43.731499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Getting inpout and output features","metadata":{}},{"cell_type":"code","source":"X = Processed_data.Word\nY = Processed_data['Tag']\nle=LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:43.733772Z","iopub.execute_input":"2021-08-10T09:54:43.734064Z","iopub.status.idle":"2021-08-10T09:54:43.746944Z","shell.execute_reply.started":"2021-08-10T09:54:43.734036Z","shell.execute_reply":"2021-08-10T09:54:43.746067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Features Extraction using sequance extractor \n- Tensorflow sequence extractor transforms each word in text to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. By doing this, it use the words with top frequency as features. Since we used the tokenizor, so the words for sequence extractor taken into account which are known by the tokenizor. \n- We selected the 1500 maximum words window to extract from the data using tensorflow text to sequences method. The words are on one side as single feature vector and maximum length of word is 300. ","metadata":{}},{"cell_type":"code","source":"max_words = 1500\nmax_len = 300\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X)\nsequences = tok.texts_to_sequences(X)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:43.749325Z","iopub.execute_input":"2021-08-10T09:54:43.749646Z","iopub.status.idle":"2021-08-10T09:54:44.112057Z","shell.execute_reply.started":"2021-08-10T09:54:43.749617Z","shell.execute_reply":"2021-08-10T09:54:44.111002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences_matrix","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:44.113588Z","iopub.execute_input":"2021-08-10T09:54:44.113892Z","iopub.status.idle":"2021-08-10T09:54:44.120134Z","shell.execute_reply.started":"2021-08-10T09:54:44.113862Z","shell.execute_reply":"2021-08-10T09:54:44.11913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Split data into training and testing\n- 70% training data \n- 30% testing data\n","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(sequences_matrix, Y, test_size=0.30, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:44.121452Z","iopub.execute_input":"2021-08-10T09:54:44.121741Z","iopub.status.idle":"2021-08-10T09:54:44.142896Z","shell.execute_reply.started":"2021-08-10T09:54:44.121712Z","shell.execute_reply":"2021-08-10T09:54:44.142146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n    <h6>...</h6>\n    <h6>...</h6>\n    <h6>...</h6>\n    <h1><strong><center>RNN Model</center></strong></h1>\n    <h6>...</h6>\n    <h6>...</h6>\n    <h6>...</h6>\n    <i></i>\n</div>","metadata":{"papermill":{"duration":0.087869,"end_time":"2021-01-06T14:47:23.13766","exception":false,"start_time":"2021-01-06T14:47:23.049791","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Implementing RNN model\n- building the RNN with Hyper parameter\n- Tunning the hyper parameter accroding to the problem \n- We used 9 layers in the RNN model in which one is input layer and one is output layer. We used word embedding and LSTM layers to learn the patterns even better. We used RNN because it remembers the each and every information though the time. RNN model save the previous information and take the better decisions for classification. ","metadata":{}},{"cell_type":"code","source":"def RNN_model():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:44.143881Z","iopub.execute_input":"2021-08-10T09:54:44.144174Z","iopub.status.idle":"2021-08-10T09:54:44.151107Z","shell.execute_reply.started":"2021-08-10T09:54:44.144147Z","shell.execute_reply":"2021-08-10T09:54:44.149914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RNN_model()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:44.152509Z","iopub.execute_input":"2021-08-10T09:54:44.153041Z","iopub.status.idle":"2021-08-10T09:54:44.43694Z","shell.execute_reply.started":"2021-08-10T09:54:44.152976Z","shell.execute_reply":"2021-08-10T09:54:44.435859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training and validating \n\n- from 70% of the training data, we used 10% for validation","metadata":{}},{"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=500,epochs=20,\n          validation_split=0.1)\nprint('Training finished !!')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T09:54:44.438125Z","iopub.execute_input":"2021-08-10T09:54:44.438403Z","iopub.status.idle":"2021-08-10T10:00:20.640272Z","shell.execute_reply.started":"2021-08-10T09:54:44.438376Z","shell.execute_reply":"2021-08-10T10:00:20.639042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training and validation plot","metadata":{}},{"cell_type":"code","source":"def Training_and_validation_plot(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\nTraining_and_validation_plot(history)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:20.641818Z","iopub.execute_input":"2021-08-10T10:00:20.642321Z","iopub.status.idle":"2021-08-10T10:00:21.021706Z","shell.execute_reply.started":"2021-08-10T10:00:20.642276Z","shell.execute_reply":"2021-08-10T10:00:21.020707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Highest Validation accuracy point","metadata":{}},{"cell_type":"code","source":"val_acc = history.history['val_acc']\nacc = history.history['acc']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, val_acc, 'g', label = 'Validation Accuracy')\nplt.title('Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\ny_arrow = max(val_acc)\nx_arrow = val_acc.index(y_arrow) + 1\nplt.annotate(str(y_arrow)[:6],\n             (x_arrow, y_arrow),\n             xytext=(x_arrow + 5, y_arrow + .02),\n             arrowprops=dict(facecolor='green', shrink=0.05))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:21.022936Z","iopub.execute_input":"2021-08-10T10:00:21.023232Z","iopub.status.idle":"2021-08-10T10:00:21.307639Z","shell.execute_reply.started":"2021-08-10T10:00:21.023204Z","shell.execute_reply":"2021-08-10T10:00:21.306949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy","metadata":{}},{"cell_type":"code","source":"y_pred1 = model.predict(X_test)\ny_pred1=(y_pred1 > 0.5)\naccr = model.evaluate(X_test,y_test)\nrnn_acc=accr[1]\nprint('Test set\\n   Accuracy: {:0.3f}'.format(accr[1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:21.308737Z","iopub.execute_input":"2021-08-10T10:00:21.309006Z","iopub.status.idle":"2021-08-10T10:00:33.910444Z","shell.execute_reply.started":"2021-08-10T10:00:21.30898Z","shell.execute_reply":"2021-08-10T10:00:33.909614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Precision, Recall, F1","metadata":{}},{"cell_type":"code","source":"print('\\n')\nprint(\"Precision, Recall, F1\")\nprint('\\n')\nCR=classification_report(y_test, y_pred1)\nprint(CR)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:33.912813Z","iopub.execute_input":"2021-08-10T10:00:33.913103Z","iopub.status.idle":"2021-08-10T10:00:33.932372Z","shell.execute_reply.started":"2021-08-10T10:00:33.913075Z","shell.execute_reply":"2021-08-10T10:00:33.931639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"print('\\n')\nprint(\"confusion matrix\")\nprint('\\n')\nCR=confusion_matrix(y_test, y_pred1)\nprint(CR)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:33.933653Z","iopub.execute_input":"2021-08-10T10:00:33.934055Z","iopub.status.idle":"2021-08-10T10:00:33.962652Z","shell.execute_reply.started":"2021-08-10T10:00:33.934003Z","shell.execute_reply":"2021-08-10T10:00:33.961628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### AUC curve plot","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_pred1)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='AUC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-10T10:00:33.963742Z","iopub.execute_input":"2021-08-10T10:00:33.96401Z","iopub.status.idle":"2021-08-10T10:00:34.118969Z","shell.execute_reply.started":"2021-08-10T10:00:33.963983Z","shell.execute_reply":"2021-08-10T10:00:34.118067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n    <h6>...</h6>\n    <h6>...</h6>\n    <h6>...</h6>\n    <h1><strong><center>RF Model</center></strong></h1>\n    <h6>...</h6>\n    <h6>...</h6>\n    <h6>...</h6>\n    <i></i>\n</div>","metadata":{"papermill":{"duration":0.087869,"end_time":"2021-01-06T14:47:23.13766","exception":false,"start_time":"2021-01-06T14:47:23.049791","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Implementing Random Forest model\n- Random forest model works in the tree form. Various trees save the information of input features and decides the output class and then finally combine all the trees, calculate the weights and classify. RF model add adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model. ","metadata":{"id":"ok3YsIQSLG_M"}},{"cell_type":"markdown","source":"### Training the Algorithm with Random Search for Hyper parameter Optimization","metadata":{}},{"cell_type":"code","source":"RF = RandomForestClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:34.120288Z","iopub.execute_input":"2021-08-10T10:00:34.12058Z","iopub.status.idle":"2021-08-10T10:00:34.125066Z","shell.execute_reply.started":"2021-08-10T10:00:34.12055Z","shell.execute_reply":"2021-08-10T10:00:34.124118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:34.128417Z","iopub.execute_input":"2021-08-10T10:00:34.128738Z","iopub.status.idle":"2021-08-10T10:00:34.137838Z","shell.execute_reply.started":"2021-08-10T10:00:34.128707Z","shell.execute_reply":"2021-08-10T10:00:34.136789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Grid_RN = RandomizedSearchCV(estimator = RF, param_distributions = params)\nGrid_RN.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:00:34.139374Z","iopub.execute_input":"2021-08-10T10:00:34.140111Z","iopub.status.idle":"2021-08-10T10:05:06.560208Z","shell.execute_reply.started":"2021-08-10T10:00:34.14006Z","shell.execute_reply":"2021-08-10T10:05:06.559275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best parameters","metadata":{}},{"cell_type":"code","source":"Grid_RN.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:05:06.561288Z","iopub.execute_input":"2021-08-10T10:05:06.561578Z","iopub.status.idle":"2021-08-10T10:05:06.567386Z","shell.execute_reply.started":"2021-08-10T10:05:06.561549Z","shell.execute_reply":"2021-08-10T10:05:06.566485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing and calculating Accuracy","metadata":{}},{"cell_type":"markdown","source":"#### Accuracy","metadata":{}},{"cell_type":"code","source":"RF_R=Grid_RN.predict(X_test)\nRF_acc=Grid_RN.score(X_test, y_test)\nprint('Accuracy score= {:.2f}'.format(Grid_RN.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:05:06.568636Z","iopub.execute_input":"2021-08-10T10:05:06.568918Z","iopub.status.idle":"2021-08-10T10:05:06.884483Z","shell.execute_reply.started":"2021-08-10T10:05:06.568892Z","shell.execute_reply":"2021-08-10T10:05:06.883423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Precision, Recall, F1","metadata":{}},{"cell_type":"code","source":"print('\\n')\nprint(\"Precision, Recall, F1\")\nprint('\\n')\nCR=classification_report(y_test, RF_R)\nprint(CR)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:05:06.885674Z","iopub.execute_input":"2021-08-10T10:05:06.886011Z","iopub.status.idle":"2021-08-10T10:05:06.903807Z","shell.execute_reply.started":"2021-08-10T10:05:06.885983Z","shell.execute_reply":"2021-08-10T10:05:06.902834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"print('\\n')\nprint(\"confusion matrix\")\nprint('\\n')\nCR=confusion_matrix(y_test, RF_R)\nprint(CR)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:05:06.905119Z","iopub.execute_input":"2021-08-10T10:05:06.905417Z","iopub.status.idle":"2021-08-10T10:05:06.919317Z","shell.execute_reply.started":"2021-08-10T10:05:06.905388Z","shell.execute_reply":"2021-08-10T10:05:06.91821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### AUC curve plot","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, RF_R)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='AUC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-10T10:05:06.92059Z","iopub.execute_input":"2021-08-10T10:05:06.920891Z","iopub.status.idle":"2021-08-10T10:05:07.087746Z","shell.execute_reply.started":"2021-08-10T10:05:06.920862Z","shell.execute_reply":"2021-08-10T10:05:07.086981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison of RNN and Random Forest Results ","metadata":{}},{"cell_type":"code","source":"x = PrettyTable()\nprint('\\n')\nprint(\"Comparison of RNN and Random Forest Results\")\nx.field_names = [\"Model\", \"Accuracy\"]\n\nx.add_row([\"RNN model\",  round(rnn_acc,2)])\nx.add_row([\"Random Forest model\", round(RF_acc,3)])\n\nprint(x)\nprint('\\n')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-10T10:05:07.088693Z","iopub.execute_input":"2021-08-10T10:05:07.088959Z","iopub.status.idle":"2021-08-10T10:05:07.0962Z","shell.execute_reply.started":"2021-08-10T10:05:07.088932Z","shell.execute_reply":"2021-08-10T10:05:07.095484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We trained recurrent neural network RNN and random forest on this data. As we can see that Random Forest Model did not performed well but RNN outperformed with highets accuracy. Initially RNN was not giving good results but we tuned the parameters in order to get the good results. \n\nRNN model is not giving only high accuracy but also performing well on the other evaluation metrics which are confusion matrix, precision, recall, F1 measure and also can see the AUC curve which is showing high positive rate of classification. ","metadata":{}},{"cell_type":"code","source":"Acc = [['Results of RNN & RF', rnn_acc, RF_acc]]\nAcc = pd.DataFrame (Acc, columns = ['Comparison of RNN and RF models','RNN Model', 'RF Mocdel'])\nAcc.set_index(\"Comparison of RNN and RF models\",drop=True,inplace=True)\nAcc.plot(kind='bar',figsize=(15, 10)).legend(bbox_to_anchor=(1, 1))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T10:05:07.097442Z","iopub.execute_input":"2021-08-10T10:05:07.097713Z","iopub.status.idle":"2021-08-10T10:05:07.302652Z","shell.execute_reply.started":"2021-08-10T10:05:07.097686Z","shell.execute_reply":"2021-08-10T10:05:07.301525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"We applied different techniques on the data preparation and then features extraction. We used TF-IDF technique to extract the features from the text but the models was not giving good results. We changed the features extraction technique to Sequance extractor using tensorflow and we got the good results. Data preprocessing, data extraction and hyper parameter tuning helped to boost the perofrmance of RNN but Random forest did not performed well. \n\n\nWell about the hyper parameters tuning, we donâ€™t need to set different hyper parameters, train the model and show the results. \nTo do this what we need to do, we tune the hyper parameters according to problem, we train and test and know how the model giving results in deep learning. And in Random forest model, we can use GRID SEARCH approach that train the model on all the given hyper parameters and select the best parameters to train the final model.\n\nHyper parameters for RNN,  \nWe choose the different number of parameters but after tuning, we got best hyper parameters to get the good results. We used Sigmoid activation function as we need only one output. Sigmoid function helps to get one output for binary problem. We used binary cross entropy as our problem is to classify only two classes. RMSPROP optimizer helps to get the good results. We change and test different epochs and batch sizes but 20 epochs and 500 batch size is good to get the good results as well. \n\n\nHyper parameters for RF,  \nTo get the best parameters of random forest model, we used grid search which take different parameters every time and train the model. Finally grid search train the model on the best parameters. These are the best parameters for RF model:\n{'n_estimators': 200,\n 'max_features': 'sqrt',\n 'max_depth': 8,\n 'criterion': 'gini'}\n\nWe got good results on the RNN because it remember each and every information of previous times.","metadata":{}}]}