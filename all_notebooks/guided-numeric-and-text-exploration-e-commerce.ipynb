{"cells":[{"metadata":{"_uuid":"52f0edd3c837e28ecf895f1290ad2bfe77e36e88","_cell_guid":"0e0daf7f-680f-41b1-89c4-319087298d01"},"cell_type":"markdown","source":"# Women's Clothing E-Commerce with Natural Language Processing \n_by Nick Brooks, January 2018_\n\n***\n\n**Programming Language:** Python 3.5 in the Jupyter Notebook Environment\n\n**Textbook Resources Used:** <br>\n- Swamynathan, Manohar. Mastering Machine Learning with Python in Six Steps: a Practical\n- Implementation Guide to Predictive Data Analytics Using Python. Apress, 2017.\n- Bird, Steven. Natural Language Processing with Python. O&#39;Reilly Media, 2016.\n\n**Code Navigation:** <br>\nIn the code, text after hashtags (#) are supportive explanations, not executed as code.\nIndented line signifies code is part of larger function or loop. Not standalone. Furthermore,\nfunctions are used in order to facilitate the simplicity and exploratory process of the code.\nCode: Packages Used\n\n***"},{"metadata":{"_uuid":"33664f0f092ce27fa9557b8707aa87b17104110a"},"cell_type":"markdown","source":"# Tables of Content:\n\n**1. [Introduction](#Introduction)** <br>\n**2. [Univariate Distribution](#Univariate)** <br>\n**3. [Multivariate Distribution](#Multivariate)** <br>\n\t- 3.1 Categorical Variable by Categorical Variable\n\t- 3.2 Continuous Variable by Categorical Variable\n\t- 3.3 Continuous Variables  on Continuous Variables\n\t- 3.4 Percentage Standardize Distribution Plots\n    \n**4. [Multivariate Analysis](#Multianalysis)** <br>\n**5. [Working with Text](#Text)** <br>\n\t- 5.1 Text Pre-Processing\n\t- 5.2 Sentiment Analysis\n**5. [Sentiment Analysis](#Sentiment Analysis)** <br>\n**6. [Word Distribution and Word Cloud](#Word Distribution and Word Cloud)** <br>\n**7. [N Grams by Recommended Feature](#NGRAM)** <br>\n**8. [Supervised Learning](#Supervised Learning)** <br>\n\t- 8.1 Naive Bayes\n\n# **1. Introduction:** <a id=\"Introduction\"></a> <br>\nThis notebook is concerned with using the Python programming language and Natural Language Processing technology to explore trends in the customer reviews from an anonymized women’s clothing E-commerce platform, and extract actionable plans to improve its online e-commerce. The data is a collection of 22641 Rows and 10 column variables. Each row includes a written comment as well as additional customer information. This analysis will focus on using Natural Language techniques to find broad trends in the written thoughts of the customers. The total number of unique words in the dataset is 9810. \n\nMy goal is to get to understand what it is the customers appreciate and dislike about their purchases. To reach this goal, I conduct an observational study of this sizable dataset, first by understanding the characteristics of individual features, and ramping the complexity of the analysis once a proper target is envisioned. \n\n[Notebook Counterpart: In-Depth Simple Linear Regression](https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression)"},{"metadata":{"_kg_hide-input":false,"_uuid":"fc80252584ecff71f8facc69d234477ef64196ae","_cell_guid":"2f0c49d2-7743-47c6-9343-307e70f02197","trusted":true},"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport random\nimport os\nfrom os import path\nfrom PIL import Image\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Set Plot Theme\nsns.set_palette([\n    \"#30a2da\",\n    \"#fc4f30\",\n    \"#e5ae38\",\n    \"#6d904f\",\n    \"#8b8b8b\",\n])\n# Alternate # plt.style.use('fivethirtyeight')\n\n# Pre-Processing\nimport string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\n\n# Modeling\nimport statsmodels.api as sm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.util import ngrams\nfrom collections import Counter\nfrom gensim.models import word2vec\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fa2cf8f4c612b55961ca8dcbd1a28eee7bda725","_cell_guid":"c8f87a74-9f13-4bb3-8877-04ea4a7ec000"},"cell_type":"markdown","source":"**Code Explanation and Reasoning:** <br>\nThese packages are separated in four categories: *General, Visualization, Pre-Processing, and Modeling*.\n\nThe General category includes the basic data manipulation tools for scientific computation (`numpy`), dataframes (`pandas`), Natural Language Processing (`NLTK`), path directory manipulation (`os`), and image saving (`PIL`).\n\nThe Visualization section enables the creation of simple graphics (`matplotlib`, `seaborn`), as well as `wordcloud`'s text frequency visualization.\n\nThe Pre-Processing section extracts more specialized modules from the NLTK package such as tokenizers and stemmers to enable the preparation of text data for mathematical analysis.\n\nThe Modeling section includes `nltk`’s sentiment analysis module, which can determine the mood of text, NLTK’s N-grams, and `gensim.models`’s word2vec. It also includes `statsmodels.api` which offers an array of linear models."},{"metadata":{"_kg_hide-input":false,"_uuid":"fd77c886672d174285917b7d4a62d06e7665fe82","_cell_guid":"0d880fce-581d-4b80-a989-ad32ac77e156","trusted":true},"cell_type":"code","source":"# Read and Peak at Data\ndf = pd.read_csv(\"../input/Womens Clothing E-Commerce Reviews.csv\")\ndf.drop(df.columns[0],inplace=True, axis=1)\n\n# Delete missing observations for following variables\nfor x in [\"Division Name\",\"Department Name\",\"Class Name\",\"Review Text\"]:\n    df = df[df[x].notnull()]\n\n# Extracting Missing Count and Unique Count by Column\nunique_count = []\nfor x in df.columns:\n    unique_count.append([x,len(df[x].unique()),df[x].isnull().sum()])\n\n# Missing Values\nprint(\"Missing Values: {}\".format(df.isnull().sum().sum()))\n\n# Data Dimensions\nprint(\"Dataframe Dimension: {} Rows, {} Columns\".format(*df.shape))\n\n# Create New Variables: \n# Word Length\ndf[\"Word Count\"] = df['Review Text'].str.split().apply(len)\n# Character Length\ndf[\"Character Count\"] = df['Review Text'].apply(len)\n# Boolean for Positive and Negative Reviews\ndf[\"Label\"] = 0\ndf.loc[df.Rating >= 3,[\"Label\"]] = 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55e5e528713d86d7938a1e2776839170be2813f6","_cell_guid":"927ef19d-a6ab-4ebf-83c9-cbf1aa1d4d3d"},"cell_type":"markdown","source":"**Code Explanation and Reasoning**: <br>\nAfter reading in the data, a binary variable is created to better to generalize rating into \"good\" and \"bad\" reviews. It is important to note that rating is a personal level judgement. Perhaps the \"Recommended\" feature may provide more insight into the value of the product, since customers are putting their reputation on the line to state whether they think its worth wearing. This social dimension is paramount when it comes to this product. Indeed, I found that many reviews would declare the social response and experience they had while wearing the clothing item.\n\nNext, the dataset is purged of observation where the review text is unavailable. This was done because the review is the heart of this analysis. In fact, there are additional missing values, which I will leave for now, since they may provide usable extra information."},{"metadata":{"_uuid":"9c3ebc526c1d984c1a9b4bc084e67cdd9a106688","_cell_guid":"6efe31e6-d8e6-43c6-b34e-6e24ea55d585","trusted":true},"cell_type":"code","source":"df.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81a27c94291a97e65936376ce8adc8d7b280826a","_cell_guid":"5cd07f8e-29a3-42d9-8fa3-1d11387d5464"},"cell_type":"markdown","source":"\n\n***\n\n## 2. Univariate Distribution <a id=\"Univariate\"></a>\n\nTo start off my analysis, I will first take a look at the distribution of individual variables. This is a good way to see what I am up against, and understand the context of the subsequent multi-variate analysis."},{"metadata":{"_kg_hide-input":false,"_uuid":"a00a3a07b54d2ea8447da4e54eb5f0e4a0539c57","_cell_guid":"9f5b6a4b-b9fc-4a88-a22b-e454993eb33a","trusted":true},"cell_type":"code","source":"print(\"Dataframe Dimension: {} Rows, {} Columns\".format(*df.shape))\npd.DataFrame(unique_count, columns=[\"Column\",\"Unique\",\"Missing\"]).set_index(\"Column\").T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8eb05569612c7516d34aba1f3acb01c4de4d3e81","_cell_guid":"0f71cbf3-25e0-4765-91fa-0c5666167d2a"},"cell_type":"markdown","source":"**Interpretation** <br>\nThere are approximately 3000 missing values, which represents 1% of the dataset, but the dataset will not get trimmed further since the review text body is the only variable that must be complete.\n\nAmongst the categorical variables, the high unique count of Clothing ID and Class Names will require non-visual exploratory methods."},{"metadata":{"_kg_hide-input":false,"_uuid":"bf9ece7eca7f7a8c0d309640f815ec83a800a6a1","_cell_guid":"eda94704-4beb-4e81-9eeb-22e15414e081","trusted":true},"cell_type":"code","source":"df.describe().T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"0e3f766a098340fe143edc910038b7cfe8ccf22e","_cell_guid":"986daab5-f964-4e47-a438-8f97fc907bb8","trusted":true},"cell_type":"code","source":"df[[\"Title\", \"Division Name\",\"Department Name\",\"Class Name\"]].describe(include=[\"O\"]).T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"254bdac1a1313d83291cba14cdf4d693d7aa95d4","_cell_guid":"1a7af819-fae8-4ae6-a203-bc1dd749ee4d"},"cell_type":"markdown","source":"Just an overview. I want to explore these numbers using visualizations.\n\n***\n\n**Age and Positive Feedback Count Distributions:**"},{"metadata":{"_kg_hide-input":false,"_uuid":"22aef077013a9644c8df8ed1626f9e9d42205eb2","_cell_guid":"3e3d14b6-8b3a-4fcd-af51-5cd3c375aa0a","scrolled":true,"trusted":true},"cell_type":"code","source":"# Continous Distributions\nf, ax = plt.subplots(1,3,figsize=(12,4), sharey=False)\nsns.distplot(df.Age, ax=ax[0])\nax[0].set_title(\"Age Distribution\")\nax[0].set_ylabel(\"Density\")\nsns.distplot(df[\"Positive Feedback Count\"], ax=ax[1])\nax[1].set_title(\"Positive Feedback Count Distribution\")\nsns.distplot(np.log10((df[\"Positive Feedback Count\"][df[\"Positive Feedback Count\"].notnull()]+1)), ax=ax[2])\nax[2].set_title(\"Positive Feedback Count Distribution\\n[Log 10]\")\nax[2].set_xlabel(\"Log Positive Feedback Count\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db1fbd175882b8484f4dace14451b0f1c0dd49d5","_cell_guid":"5aca8a6c-a6d8-4e74-8835-6519cbf51104"},"cell_type":"markdown","source":"**Code Explanation:** <br>\nUsing seaborn, a simple variable frequency bar/density plot is created. In the log positive feedback count plot, I had to add 0.0001 to all values so that the logarithm of previously zero values can be taken. Matplotlib's subplots function is employed through assign each plot the **AX** argument.\n\n**Distribution of Age:** <br>\nMy a priori expectation was that the biggest group of reviewing customers would be young, tech savvy women between the age of 18 and 34. However, this plot would say otherwise, since it appears that not only is the 34 to 50 year old age most engage in reviewing products, they also appear to be the most positive reviewers, since they proportionately give higher more reviews of 5. Before making insight about these point, it would be wise to gather further data on the age distribution of shoppers. Nevertheless, this trend suggest that the core market segment for this clothing brand is women between 34 and 50. With its single peak and slight right tail, the distribution of age is more or less normal.\n\n**Distribution of Positive Feedback Count:** <br>\nThis kind of distribution is common for network effect phenomenon, where popularity has an exponential effect on response, and most individuals receive no attention. This phenomenon is also known as the *Cumulative-Advantage Effect / Matthew Effect* or the Pareto Principle.\n\n**Cumulative-Advantage Effect / Matthew Effect:** <br>\nCoined by Robert K. Merton in 1968, this [states that once a social agent gains a small advantage over other agents, that advantage will compound over time into an increasingly larger advantage.](http://www.thwink.org/sustain/glossary/CumulativeAdvantagePrinciple.htm) Here is the passage from the New Testament:\n\n>\"For to everyone who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away.\"\n> Matthew 25:29\n\nThis tendency effects any system with a positive feedback loop, which compounds. This effect turns out to be quite common among competing agents, and what we end up with, is a the Pareto Distribution.\n\n**Pareto Distribution:** <br>\nAlso known as the 80/20 rule. Often used to describe the distribution of wealth, 20% of the population hold 80% of the wealth. I wonder how accurate this rule of thumb applies to the distribution of Positive Feedback."},{"metadata":{"_kg_hide-input":false,"_uuid":"2cc1294a3d0495b640f553f535e3b7c46d7ba3f0","_cell_guid":"cde36ba1-40c6-4743-ad4f-932ee646e0d0","trusted":true},"cell_type":"code","source":"# Percentage Accumulation from \"Most Wealthy\"\ndef percentage_accumulation(series, percentage):\n    return (series.sort_values(ascending=False)\n            [:round(series.shape[0]*(percentage/100))]\n     .sum()/series\n     .sum()*100)\n\n# Gini Coefficient- Inequality Score\n# Source: https://planspace.org/2013/06/21/how-to-calculate-gini-coefficient-from-raw-data-in-python/\ndef gini(list_of_values):\n    sorted_list = sorted(list_of_values)\n    height, area = 0, 0\n    for value in sorted_list:\n        height += value\n        area += height - value / 2.\n    fair_area = height * len(list_of_values) / 2.\n    return (fair_area - area) / fair_area\n\n# Cumulative Percentage of Positive Feedback assigned Percent of Reviewers (from most wealthy)\ninequality = []\nfor x in list(range(100)):\n    inequality.append(percentage_accumulation(df[\"Positive Feedback Count\"], x))\n\n# Generic Matplotlib Plot\nplt.plot(inequality)\nplt.title(\"Percentage of Positive Feedback by Percentage of Reviews\")\nplt.xlabel(\"Review Percentile starting with Feedback\")\nplt.ylabel(\"Percent of Positive Feedback Received\")\nplt.axvline(x=20, c = \"r\")\nplt.axvline(x=53, c = \"g\")\nplt.axhline(y=78, c = \"y\")\nplt.axhline(y=100, c = \"b\", alpha=.3)\nplt.show()\n\n# 80-20 Rule Confirmation\nprint(\"{}% of Positive Feedback belongs to the top 20% of Reviews\".format(\n    round(percentage_accumulation(df[\"Positive Feedback Count\"], 20))))\n\n# Gini\nprint(\"\\nGini Coefficient: {}\".format(round(gini(df[\"Positive Feedback Count\"]),2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65c82a5e5a32ac7ed7dd082190511d39684d3fdf","_cell_guid":"7d2c8483-9d3e-4d40-a072-74d4400d3e3c"},"cell_type":"markdown","source":"**Interpretation:** <br>\nIn this case, the 80/20 rule applies pretty closely. Nevertheless do not take this rule as granted, since sometimes the proportion of inequality may be much higher! Since the Pareto Principle is a power law, it is fundamentally embedded in itself. However, notice the green vertical line, where 47% of reviews received *no* feedback at all.\n\nSince I am on the topic of inequaliy, I want to quickly touch the Gini Coefficient.\n\nNext, lets see what happens when we look at the top 20% of the top 20%..."},{"metadata":{"_kg_hide-input":false,"_uuid":"c63da785d449bcb8b3c72daa5110920fe03e468c","_cell_guid":"c6312a75-d17a-40b3-be23-47dd2afa4d3c","trusted":true},"cell_type":"code","source":"# Cumulative Percentage of Positive Feedback assigned Percent of Reviewers (from most wealthy)\ntop_20 = df[\"Positive Feedback Count\"].sort_values(ascending=False)[:round(df.shape[0]*(20/100))]\n\ninequality = []\nfor x in list(range(100)):\n    inequality.append(percentage_accumulation(top_20, x))\n\n# Generic Matplotlib Plot\nplt.plot(inequality)\nplt.title(\"Percentage of Positive Feedback by Percentage of Reviews\")\nplt.xlabel(\"Review Percentile starting with Feedback\")\nplt.ylabel(\"Percent of Positive Feedback Received\")\nplt.axvline(x=20, c = \"r\")\nplt.axhline(y=47, c = \"r\")\nplt.axhline(y=100, c = \"b\", alpha=.3)\n\nplt.show()\n\n# 80-20 Rule Confirmation\nprint(\"{}% of Positive Feedback belongs to the top 20% of Reviews\".format(\n    round(percentage_accumulation(top_20, 20))))\n\n# Gini\nprint(\"\\nGini Coefficient: {}\".format(round(gini(top_20),2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"399266c1bd24b9616f7f5bb0408bab08226a07bb","_cell_guid":"7fcf514e-b6aa-40ec-9e9e-cf4d5ec0a394"},"cell_type":"markdown","source":"**Interpretation:** <br>\nOh look it didn't hold up. Well think about, the compounding influence of the Network Effect as as whole accounts for the sizable portion of the population that receive nothing.\n\n***\n\n**Division Name and Department Name Distribution:**"},{"metadata":{"_kg_hide-input":false,"_uuid":"0341d3e73a9a5c07ed9238ea4574c86ce4a0e34e","_cell_guid":"8a9a94fd-6390-4847-b1f2-116a3119d3b5","trusted":true},"cell_type":"code","source":"row_plots = [\"Division Name\",\"Department Name\"]\nf, axes = plt.subplots(1,len(row_plots), figsize=(14,4), sharex=False)\n\nfor i,x in enumerate(row_plots):\n    sns.countplot(y=x, data=df,order=df[x].value_counts().index, ax=axes[i])\n    axes[i].set_title(\"Count of Categories in {}\".format(x))\n    axes[i].set_xlabel(\"\")\n    axes[i].set_xlabel(\"Frequency Count\")\naxes[0].set_ylabel(\"Category\")\naxes[1].set_ylabel(\"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"952d551767903e1d0da5abecc0d4062dbb9ccedd","_cell_guid":"576de23b-d160-4824-af77-104e46890496"},"cell_type":"markdown","source":"**Code Explanation:** <br>\nEnumerating the loop enables the loop iteration to coincide with the matplotlib subplot ax.\n\n**Distribution of Division Name** <br>\nThis high level feature describes had three categories: General, Petite, and Intimates. This offers some insight into the clothing sizes of the customers leaving reviews.\n\n**Distribution of Department Name** <br>\nIt is notable to observe that *Tops and Dresses* are the most commonly reviewed products. It would be interesting to investigate the motivation of leaving a review in the first place.\n\n***\n**Distribution of Clothing ID to Understand Product Popularity**"},{"metadata":{"_kg_hide-input":false,"_uuid":"41ce13cd7c9aafc99120322c9dabc0d2c4d744ee","_cell_guid":"ddec58fb-1cad-4cd3-a5ba-a009890e2c2c","trusted":true},"cell_type":"code","source":"# Clothing ID Category\nf, axes = plt.subplots(1,2, figsize=[14,7])\nnum = 30\nsns.countplot(y=\"Clothing ID\", data = df[df[\"Clothing ID\"].isin(df[\"Clothing ID\"].value_counts()[:num].index)],\n              order= df[\"Clothing ID\"].value_counts()[:num].index, ax=axes[0])\naxes[0].set_title(\"Frequency Count of Clothing ID\\nTop 30\")\naxes[0].set_xlabel(\"Count\")\n\nsns.countplot(y=\"Clothing ID\", data = df[df[\"Clothing ID\"].isin(df[\"Clothing ID\"].value_counts()[num:60].index)],\n              order= df[\"Clothing ID\"].value_counts()[num:60].index, ax=axes[1])\naxes[1].set_title(\"Frequency Count of Clothing ID\\nTop 30 to 60\")\naxes[1].set_ylabel(\"\")\naxes[1].set_xlabel(\"Count\")\nplt.show()\n\nprint(\"Dataframe Dimension: {} Rows\".format(df.shape[0]))\ndf[df[\"Clothing ID\"].isin([1078, 862,1094])].describe().T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5474f0e3484dcaa1dcf1fc356156559fb54cb372","_cell_guid":"378bb954-943b-4577-9c7d-906984ff5c11","trusted":true},"cell_type":"code","source":"df.loc[df[\"Clothing ID\"].isin([1078, 862,1094]),\n       [\"Title\", \"Division Name\",\"Department Name\",\"Class Name\"]].describe(include=[\"O\"]).T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f816c6bcab04b839facbc85aab8c0baa0816f6c9","_cell_guid":"a74dd4e5-1f79-4c41-be7f-d0e604419583"},"cell_type":"markdown","source":"**Code Explanation** <br>\nSince they are around one thousand unique *Clothing IDs*, I used boolean operators to only select the top 60 most popular cloth items, then optimizing notebook real estate by splitting them in two plot columns.\n\n**Interpretation** <br>\nIt appears like there are around three products that receive a small magnitude more reviews than others. I follow up on these findings by observing the descriptive statistics of the top three items. These items received an average rating of ~4.2, and an average recommendation rate of 81%. Furthermore, it appears that these products are predominately normal sized dresses.\n\nThese observations make me wonder about the nature of review popularity and rating performance. A question that could shed light on the customer's motivation to leave a review.\n\n***\n**Distribution of Class Name**"},{"metadata":{"_kg_hide-input":false,"_uuid":"7aebdf06831cbe7f9c0db12c188a352eea940d2a","_cell_guid":"d7a19fc2-8e5d-4506-ae3a-103dd3e38117","trusted":true},"cell_type":"code","source":"# Class Name\nplt.subplots(figsize=(9,5))\nsns.countplot(y=\"Class Name\", data=df,order=df[\"Class Name\"].value_counts().index)\nplt.title(\"Frequency Count of Class Name\")\nplt.xlabel(\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"043832155287b323efb817ed77e5dd626b5eeb4b","_cell_guid":"bd96c69d-74c5-4059-aa08-6ed16176432e"},"cell_type":"markdown","source":"**Interpretation:** <br>\nExploring the class variable suggests that the most popular clothing types are: Petite and Anthro, Dresses, Blouses, and Cut and Sew Knits. The distribution of reviews is fairly constant, suggesting that there are not negative nor positive outliers. This statement has been further verified by taking the mean of the label by class group. The results show that no class falls above .80, and the majority rest at .90. Casual bottoms and Chemises scored the highest in this criteria with a 100% positive review rate, however upon investigation this is because only 4 reviews were made in these categories.\n\n***\n**Distribution of Rating, Recommended IND, and Label:**"},{"metadata":{"_kg_hide-input":false,"_uuid":"5d79a0144b70893f360b6d1f6c9f6913c461f844","_cell_guid":"eb1e71d0-fbb0-4039-9e84-758d10e58ae6","scrolled":false,"trusted":true},"cell_type":"code","source":"#cat_dtypes = [x for x,y,z in unique_count if y < 10 and x not in [\"Division Name\",\"Department Name\"]]\ncat_dtypes = [\"Rating\",\"Recommended IND\",\"Label\"]\nincrement = 0\nf, axes = plt.subplots(1,len(cat_dtypes), figsize=(14,4), sharex=False)\n\nfor i in range(len(cat_dtypes)):\n    sns.countplot(x=cat_dtypes[increment], data=df,order=df[cat_dtypes[increment]].value_counts().index, ax=axes[i])\n    axes[i].set_title(\"Frequency Distribution for\\n{}\".format(cat_dtypes[increment]))\n    axes[i].set_ylabel(\"Occurrence\")\n    axes[i].set_xlabel(\"{}\".format(cat_dtypes[increment]))\n    increment += 1\naxes[1].set_ylabel(\"\")\naxes[2].set_ylabel(\"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25d677a8e4caa285284b6db097b33d8f626bc22c","_cell_guid":"3d35e24c-4985-43a4-a611-16c794ef2bdb"},"cell_type":"markdown","source":"**Code Explanation:**\nYet another way to iterate plots, where I both loop over the index position of cat_dtypes and subplot ax at the same time with range of the length of cat_dtypes.\n\n**Distribution of Rating:** <br>\nThe vast majority of reviews were highly positive, with a score of five out of five. This suggests that this retail store is performing fairly well, but then again, I am not familiar with the industry benchmark. Competitor reviews may be scraped and analyzed. It is important to note that these reviews are subjective, and some negative reviews may a outcome of a bad day, instead of constructive feedback. In the plot below, the Label plot is the binary classification of 1 = good, and 0= bad.\n\n**Distribution of Recommended IND:** <br>\nThis variable mirrors the positivity of the Rating distribution, but as mentioned earlier, I believe that it provides variation of positivity which is social, rather than personal.\n\n**Distribution of Label:** <br>\nI am surprised to see that products are rated 3 and over, than are recommended by the customer. I am eager to see the multivariate interaction between Rating and Recommended.\n\nI find these three variables especially promising in the quest of finding how customers express dislike. In the multivariate section, I shall explore the interplay between these variables.\n\n***\n**Word and Length:**"},{"metadata":{"_kg_hide-input":false,"_uuid":"bed30c62b6a02247cdbe4e7fb60b90527643a1a7","_cell_guid":"8d6fd507-fe6c-48ca-857d-edbfae1c7b32","trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2,4, figsize=(17,8), sharex=False)\nfor ii, xvar in enumerate(['Word Count', \"Character Count\"]):\n    for i,y in enumerate([\"Rating\",\"Department Name\",\"Recommended IND\"]):\n        for x in set(df[y][df[y].notnull()]):\n            sns.kdeplot(df[xvar][df[y]==x], label=x, shade=False, ax=axes[ii,i])\n        if ii is 0:\n            axes[ii,i].set_title('{} Distribution (X)\\nby {}'.format(xvar, y))\n        else:\n            axes[ii,i].set_title('For {} (X)'.format(xvar))\n    axes[ii,0].set_ylabel('Occurrence Density')\n    axes[ii,i].set_xlabel('')\n    # Plot 4\n    sns.kdeplot(df[xvar],shade=True,ax=axes[ii,3])\n    axes[ii,3].set_xlabel(\"\")\n    if ii is 0:\n        axes[ii,3].set_title('{} Distribution (X)\\n'.format(xvar))\n    else:\n        axes[ii,3].set_title('For {} (X)'.format(xvar))\n    axes[ii,3].legend_.remove()\nplt.show()\n\nprint(\"Correlation Coefficient of Word Cound and Character Count: {}\".format(\n    round(df[\"Word Count\"].corr(df[\"Character Count\"]), 2)))\n\nprint(\"\\nTotal Word Count is: {}\".format(df[\"Word Count\"].sum()))\nprint(\"Total Character Count is: {}\".format(df[\"Character Count\"].sum()))\ndf[[\"Word Count\",\"Character Count\"]].describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f390fb5a60961fde0694d49aa4f354cf787c1fd","_cell_guid":"94b30e70-902d-48fe-bdd3-59b8600630bf"},"cell_type":"markdown","source":"**Interpretation:** <br>\n- Review Character and Word Count are highly correlated.\n- I suspect that the retailer has a maximum word limit of 500, which caused the long tail to receive compression and spike. \n\n***\n\n## 3. Multivariate Distribution <a id=\"Multivariate\"></a>\n### 3.1 Categorical Variable by Categorical Variable\nIn this section, I utilize heatmaps to visualize the percentage occurrence pivot table. Note that I heavily utilized the technique of normalizing the proportion between variables classes by converting frequency into percentages. This technique is very fruitful because the relation upon which the percentage can be explored by aggregate, by index, and by column, each of which providing its own unique insight.\n\n**Division Name by Department Name:**"},{"metadata":{"_kg_hide-input":false,"_uuid":"9dc39083d7f91771f86bd464fbee704950427167","_cell_guid":"b4769440-693f-431c-964b-b34a4eb15761","trusted":true},"cell_type":"code","source":"# Heatmaps of Percentage Pivot Table\nf, ax = plt.subplots(1,2,figsize=(16, 4), sharey=True)\nsns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"]),\n            annot=True, linewidths=.5, ax = ax[0],fmt='g', cmap=\"Greens\",\n                cbar_kws={'label': 'Count'})\nax[0].set_title('Division Name Count by Department Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nsns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"], normalize=True).mul(100).round(0),\n            annot=True, linewidths=.5, ax=ax[1],fmt='g', cmap=\"Greens\",\n                cbar_kws={'label': 'Percentage %'})\nax[1].set_title('Division Name Count by Department Name - Crosstab\\nHeatmap Overall Percentage Distribution')\nax[1].set_ylabel('')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd658a59de1758f02380be383818cb4e2cbc4b33","_cell_guid":"da58e9ad-ac54-4161-9af6-6f2a46025596"},"cell_type":"markdown","source":"**How to Interpret:** <br>\nFor the second heatmap on the right, the percentages occurrence is in relation to the whole.\n\n**Interpretation:** <br>\nEvidently, the most common product is a normal sized top."},{"metadata":{"_kg_hide-input":false,"_uuid":"154806b91c08d07f58bec23ec6d854e0b76e6a64","_cell_guid":"fa9bca5b-a8cb-41c3-bfdd-fd32a640f2da","trusted":true},"cell_type":"code","source":"# Heatmaps of Percentage Pivot Table\nf, ax = plt.subplots(1,2,figsize=(16, 4), sharey=True)\nsns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"], normalize='columns').mul(100).round(0),\n            annot=True, linewidths=.5, ax=ax[0],fmt='g', cmap=\"Greens\",\n                cbar_kws={'label': 'Percentage %'})\nax[0].set_title('Division Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Columns')\n\nsns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"], normalize='index').mul(100).round(0),\n            annot=True, linewidths=.5, ax=ax[1],fmt='g', cmap=\"Greens\",\n                cbar_kws={'label': 'Percentage %'})\nax[1].set_title('Division Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Index')\nax[1].set_ylabel('')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0be6b978de2d54b6857aec2bc3d4d80f0476d87","_cell_guid":"26485f6a-2d2b-4be2-872d-6ef85ad7cd8a"},"cell_type":"markdown","source":"**How to Interpret:** <br>\nAlthough these two heatmaps use the same features, they different in the relation in which the percentage is taken. For the first plot on the left, the percentages add up to 100% by **column**, while the plot on the right has is **normalized into percentages by row**.\n\n**Interpretation:** <br>\nThe dominance of the *General* size is consistent across the various categories within **Department Name**. There a notable overall between *General Petite* and *Department Name*.\n\n***\n\n**Class Name by Department Name:**"},{"metadata":{"_kg_hide-input":false,"_uuid":"87a3601fe8987562dc83548b4a0543cbf5698104","_cell_guid":"af88a046-3292-4396-ac7c-37ae3253637b","trusted":true},"cell_type":"code","source":"# Heatmaps of Percentage Pivot Table\nf, ax = plt.subplots(1,2,figsize=(10, 7), sharey=True)\nfsize = 13\nsns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"]),\n            annot=True, linewidths=.5, ax = ax[0],fmt='g', cmap=\"Blues\",\n                cbar_kws={'label': 'Count'})\nax[0].set_title('Class Name Count by Department Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nsns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"], normalize=True).mul(100).round(0),\n            annot=True, linewidths=.5, ax=ax[1],fmt='g', cmap=\"Blues\",\n                cbar_kws={'label': 'Percentage %'})\nax[1].set_title('Class Name Count by Department Name - Crosstab\\nHeatmap Overall Percentage Distribution')\nax[1].set_ylabel('')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b73d2100132715d138adf6dbbf88e0cd949e2d74","_cell_guid":"9c29ddbf-a530-4430-88d5-066f1c42b5fd"},"cell_type":"markdown","source":"**Interpretation:** <br>\nHere we get a closer glimpse at the breakdown of specific clothing types. Up to now, the dominance of dress popularity has been evident, but not that of \"Knits\". This is a kind of thickly thread and colorful top item which I must confess I have not seen much of out in the real world."},{"metadata":{"_kg_hide-input":false,"_uuid":"b5d3ba1f05ed6a02c1fae0bdf351568880f1f4c9","_cell_guid":"3752c46e-9811-455a-b613-2eb88ed71d9a","trusted":true},"cell_type":"code","source":"# Heatmaps of Percentage Pivot Table\nf, ax = plt.subplots(1,2,figsize=(10, 7), sharey=True)\nfsize = 13\nsns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"], normalize = 'columns').mul(100).round(0)\n            ,annot=True, fmt=\"g\", linewidths=.5, ax=ax[0],cbar=False,cmap=\"Blues\")\nax[0].set_title('Class Name Count by Count - Crosstab\\nHeatmap % Distribution by Column', fontsize = fsize)\nax[1] = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"], normalize = 'index').mul(100).round(0)\n            ,annot=True, fmt=\"2g\", linewidths=.5, ax=ax[1],cmap=\"Blues\",\n                cbar_kws={'label': 'Percentage %'})\nax[1].set_title('Class Name Count by Count - Crosstab\\nHeatmap % Distribution by Index', fontsize = fsize)\nax[1].set_ylabel('')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c3ee696a0bda752d3425f22145f92461091f687","_cell_guid":"3f56d021-74d2-41c8-b687-70b54e4a0e25"},"cell_type":"markdown","source":"**Interpretation:**\nThis normalization of percentage by column and index explains how clothing types are distributed across departments. This provides a clear way to see which products are dominant within each category. Following up on knits, it appears that the runner up in the \"Tops\" category, \"Blouses\", is not that far behind.\n\n***\n**Division Name by Department Name:**"},{"metadata":{"_kg_hide-input":false,"_uuid":"a9bd660409d86825b131e0942f4cee700c35d5c2","_cell_guid":"2698ee04-085c-4306-b9b0-af1ac79a17cd","trusted":true},"cell_type":"code","source":"# Heatmaps of Percentage Pivot Table\nf, ax = plt.subplots(1,2,figsize=(10, 7), sharey=True)\nfsize = 13\nsns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"]),\n            annot=True, linewidths=.5, ax = ax[0],fmt='g', cmap=\"Reds\",\n                cbar_kws={'label': 'Count'})\nax[0].set_title('Class Name Count by Division Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nsns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"], normalize=True).mul(100).round(0),\n            annot=True, linewidths=.5, ax=ax[1],fmt='g', cmap=\"Reds\",\n                cbar_kws={'label': 'Percentage %'})\nax[1].set_title('Class Name Count by Division Name - Crosstab\\nHeatmap Overall Percentage Distribution')\nax[1].set_ylabel('')\nplt.tight_layout(pad=0)\nplt.show()\n\n# Heatmaps of Percentage Pivot Table\nf, ax = plt.subplots(1,2,figsize=(10, 7), sharey=True)\nfsize = 13\nsns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"], normalize = 'columns').mul(100).round(0)\n            ,annot=True, fmt=\"g\", linewidths=.5, ax=ax[0],cbar=False,cmap=\"Reds\")\nax[0].set_title('Class Name Count by Count - Crosstab\\nHeatmap % Distribution by Column', fontsize = fsize)\nax[1] = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"], normalize = 'index').mul(100).round(0)\n            ,annot=True, fmt=\"2g\", linewidths=.5, ax=ax[1],cmap=\"Reds\",\n                cbar_kws={'label': 'Percentage %'})\nax[1].set_title('Class Name Count by Count - Crosstab\\nHeatmap % Distribution by Index', fontsize = fsize)\nax[1].set_ylabel('')\nplt.tight_layout(pad=0)\n\n# MANUAL NORMALIZE with Applied Lambda on Pandas DataFrame\n# ctab = pd.crosstab(df['Class Name'], df[\"Rating\"]).apply(lambda r: r/r.sum(), axis=1).mul(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a2ca791d297cc022947475f5738a54e7985b089","_cell_guid":"f977f904-8dcc-40d7-9a2c-666b05fd195e"},"cell_type":"markdown","source":"**Interpretation:** <br>\nI think this plot wraps up the interplay between Blouses, Dresses, and Knits by showing that most reviews revolve around the normal sized version of the products. It is interesting to note that Dresses attract higher proportion of \"Petite\" sized customers.\n\n***\n### 3.2  Continuous Variable by Categorical Variable\n\nHere I want to look at the behavior of the continuous variables when sliced by various categorical variables. The general theme of this section is that there is no clear slicing of continuous on categorical variables that provide a clear, distinct pattern.\n\n**Positive Feedback Count Distribution by Rating, Department Name, Recommended IND, and Class Name**"},{"metadata":{"_kg_hide-input":false,"_uuid":"666889115139e1056c3641a822e372d46be89217","_cell_guid":"22ee8aef-73d1-40b3-8dfc-634e658e6e24","scrolled":false,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,4, figsize=(17,4), sharex=False)\nxvar = 'Positive Feedback Count'\nplotdf = np.log10(df['Positive Feedback Count'])\nfor i,y in enumerate([\"Rating\",\"Department Name\",\"Recommended IND\"]):\n    for x in set(df[y][df[y].notnull()]):\n        sns.kdeplot(plotdf[df[y]==x], label=x, shade=True, ax=axes[i])\n    axes[i].set_xlabel(\"{}\\nLog 10\".format(xvar))\n    axes[i].set_label('Occurrence Density')\n    axes[i].set_title('{} Distribution\\nby {}'.format(xvar, y))\naxes[0].set_ylabel('Occurrence Density')\n# Plot 4\nsns.kdeplot(plotdf,shade=True,ax=axes[3])\naxes[3].set_xlabel(\"{}\\nLog 10\".format(xvar))\naxes[3].set_title('{} Distribution\\n'.format(xvar))\naxes[3].legend_.remove()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0916f71edb2d40cf8ce22c67dd700a7f9867780","_cell_guid":"c1707889-4ffd-4dd9-b531-67f05cf2d806"},"cell_type":"markdown","source":"**Interpretation:** <br>\nSince Positive Feedback Count is in log form, the higher frequency of non-recommended [0] has a bigger effect than visually suggested. The more popular reviews are not recommended, which suggest that the content is in the form of constructive criticism."},{"metadata":{"_kg_hide-input":false,"_uuid":"4e6d685b9f9641eb89d32b1d90899a6834416661","_cell_guid":"643404c5-c65f-4f9f-a6b5-7b281b4b4ebd","trusted":true},"cell_type":"code","source":"# Checking inequality difference:\nfor rec in [0,1]:\n    temp = df[\"Positive Feedback Count\"][df[\"Recommended IND\"] == rec]\n\n    print(\"Recommended is {}\".format(rec))\n    # 80-20 Rule Confirmation\n    print(\"{}% of Positive Feedback belongs to the top 20% of Reviews with Recommeded = {}\".format(\n        round(percentage_accumulation(temp, 20)),rec))\n    # Gini\n    print(\"Gini Coefficient: {}\\n\".format(round(gini(temp),2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6968f061b4ce3893d1936dda68d13aa46ebb3a24","_cell_guid":"911cfc5d-2734-4251-9e0e-d451ee3dcfbb"},"cell_type":"markdown","source":"**Interpretation:** <br>\nThe difference is not huge, but nevertheless, a higher gini coefficient signigies higher inequality. This means that there is a bigger divergence between recommended reviews than there is between non-recommended reviews.\n\n***\n\n**Positive Feedback Count by Class Name:** <br>"},{"metadata":{"_kg_hide-input":false,"_uuid":"f1b4b4d09a7dd68792dc0a8f5c809d840d99908c","_cell_guid":"dbd0240f-cb64-45c7-b57d-81339a4a463a","trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,3, figsize=(18,4), sharex=False)\nfor x in set(df[\"Class Name\"][df[\"Class Name\"].notnull()]):\n    sns.kdeplot(df['Positive Feedback Count'][df[\"Class Name\"]==x]\n                ,label=x, shade=False, ax=axes[0])\n    \naxes[0].legend_.remove()\naxes[0].set_xlabel('{}'.format(xvar))\naxes[0].set_title('{} Distribution by {}\\n All Data'.format(xvar, \"Class Name\"))\n\nmin_value = 15\nfor x in set(df[\"Class Name\"][df[\"Class Name\"].notnull()]):\n    sns.kdeplot(df['Positive Feedback Count'][(df[\"Class Name\"]==x) &\n                                              (df[\"Positive Feedback Count\"] < min_value)]\n                ,label=x, shade=False, ax=axes[1])\n    \naxes[1].legend_.remove()\naxes[1].set_xlabel('{}'.format(xvar))\naxes[1].set_title('{} Distribution by {}\\n Values under {}'.format(xvar, \"Class Name\", min_value))\n\nfor x in set(df[\"Class Name\"][df[\"Class Name\"].notnull()]):\n    sns.kdeplot(np.log10(df['Positive Feedback Count']+1)[df[\"Class Name\"]==x]\n                ,label=x, shade=False, ax=axes[2])\n    \naxes[2].legend_.remove()\naxes[2].set_xlabel('Log 10 - {}'.format(xvar))\naxes[2].set_title('{} Distribution by {}\\nAll Data in Log10'.format(xvar, \"Class Name\"))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27b34733a6bb3a4a6ca5c4b6f873ce398e83bf86","_cell_guid":"dc79e6fb-1902-418a-b771-58f182fb8d9a"},"cell_type":"markdown","source":"Not much to say here. There are too many classes to include a legend.. A statistical test method would operate better at this dimensionality.\n\n***\n**Age Distribution by the Usual Suspects.. round them up**"},{"metadata":{"_kg_hide-input":false,"_uuid":"5268e9cd16816952d74027f97c07abe11d787bfd","_cell_guid":"71367009-67c9-492e-9945-4f9ef7b3ad93","trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,4, figsize=(16,4), sharex=False)\nxvar = \"Age\"\nplotdf = df[\"Age\"]\nfor i,y in enumerate([\"Rating\",\"Department Name\",\"Recommended IND\"]):\n    for x in set(df[y][df[y].notnull()]):\n        sns.kdeplot(plotdf[df[y]==x], label=x, shade=False, ax=axes[i])\n    axes[i].set_xlabel(\"{}\".format(xvar))\n    axes[i].set_label('Occurrence Density')\n    axes[i].set_title('{} Distribution by {}'.format(xvar, y))\n\nfor x in set(df[\"Class Name\"][df[\"Class Name\"].notnull()]):\n    sns.kdeplot(plotdf[df[\"Class Name\"]==x], label=x, shade=False, ax=axes[3])\n\naxes[3].legend_.remove()\naxes[3].set_xlabel('{}'.format(xvar))\naxes[0].set_ylabel('Occurrence Density')\naxes[3].set_title('{} Distribution by {}'.format(xvar, \"Class Name\"))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9771a2aac91b0bb39a28b15c34b021caae72cb1","_cell_guid":"c7a3d50a-bbf9-41ed-a91c-fbb4c384dd4d"},"cell_type":"markdown","source":"**Interpretation:** <br>\nUnlike Positive Feedback Count, Age has not been transformed into a logarithm. For these reasons, slight noise between the age distribution by these features are nothing to worry about. Age doesn't seem to receive influence on these dimensions.\n\n***\n\n### 3.3 Continuous Variables  on Continuous Variables\n\nTime for some scatter plots. with [Seaborn Joint Plot](https://tryolabs.com/blog/2017/03/16/pandas-seaborn-a-guide-to-handle-visualize-data-elegantly/)."},{"metadata":{"_kg_hide-input":false,"_uuid":"f1671b607334c33286e214f7c2cd286a5947c14b","_cell_guid":"fc58e277-22db-4d8e-a16e-e3a23f870d3e","trusted":true},"cell_type":"code","source":"# Normalization is futile here.. But here is a minmax standardization, and a z-score normalization function. \ndef minmaxscaler(df):\n    return (df-df.min())/(df.max()-df.min())\ndef zscorenomalize(df):\n    return (df - df.mean())/df.std()\n\ng = sns.jointplot(x= df[\"Positive Feedback Count\"], y=df[\"Age\"], kind='reg', color='g')\ng.fig.suptitle(\"Scatter Plot for Age and Positive Feedback Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"931a3cdb2372c3dc470fec75683b83581c943305","_cell_guid":"a2b64da0-e39f-45b6-9ded-ffa4b2461f2b"},"cell_type":"markdown","source":"**How to Interpret:** <br>\nDon't be deceived by the seemingly numerous amount of points over the Positive Feedback Count value of 0! The distribution plot up top clearly shows that most points reside at ZERO!\n\n**Interpretation:** <br>\nThere appears to be a slight correlation between age and positive feedback count received. It would be interesting to focus on the textual anatomy of high positive feedback reviews.\n***\n\n### 3.4 Percentage Standardize Distribution Plots\n\nSince many variables are severely unbalanced, I employ standardization by percentage to see if the proportion is consistent between categorical classes. This is the same idea used previously on heatmaps now applied to barcharts!\n\n[Percentage Standardize in Seaborn - Stackoverflow](https://stackoverflow.com/questions/34615854/seaborn-countplot-with-normalized-y-axis-per-group)"},{"metadata":{"_kg_hide-input":false,"_uuid":"a5236dc34f3151fca8020fa1b6e76e466b7a5388","_cell_guid":"81b87aac-a901-4a59-b798-16c212773e0b","trusted":true},"cell_type":"code","source":"def percentstandardize_barplot(x,y,hue, data, ax=None, order= None):\n    \"\"\"\n    Standardize by percentage the data using pandas functions, then plot using Seaborn.\n    Function arguments are and extention of Seaborns'.\n    \"\"\"\n    sns.barplot(x= x, y=y, hue=hue, ax=ax, order=order,\n    data=(data[[x, hue]]\n     .reset_index(drop=True)\n     .groupby([x])[hue]\n     .value_counts(normalize=True)\n     .rename('Percentage').mul(100)\n     .reset_index()\n     .sort_values(hue)))\n    plt.title(\"Percentage Frequency of {} by {}\".format(hue,x))\n    plt.ylabel(\"Percentage %\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd8102a707f2113b674a4049a4e2eb9dace3b2b8","_cell_guid":"78548b48-ca3e-4af6-820b-8f630fccb986"},"cell_type":"markdown","source":"**Code Explanation:** <br>\nMany transformation are conducted here.\n- Groupby([x])[hue]: Groups the data by the x variable, what will become the X axis of the barplot.\n- Value_counts(normalized=True): Then the hue variable, which is rowed by the x variable, is ordered by most frequent to least, and that value is converted to decimal percentage.\n- rename().mull(100): Then this is renamed to \"Percentage\", and the decimal value is multiplied by 100 to be in proper percentage units.\n\n***\n**Recommended IND by Department and Division**"},{"metadata":{"_kg_hide-input":false,"_uuid":"b82b64cd669816ef90a814071e66bd35bd8e2f9a","_cell_guid":"b226004a-1467-4ec6-bb1b-35afe7102d66","trusted":true},"cell_type":"code","source":"huevar = \"Recommended IND\"\nf, axes = plt.subplots(1,2,figsize=(12,5))\npercentstandardize_barplot(x=\"Department Name\",y=\"Percentage\", hue=huevar,data=df, ax=axes[0])\naxes[0].set_title(\"Percentage Frequency of {}\\nby Department Name\".format(huevar))\naxes[0].set_ylabel(\"Percentage %\")\npercentstandardize_barplot(x=\"Division Name\",y=\"Percentage\", hue=huevar,data=df, ax=axes[1])\naxes[1].set_title(\"Percentage Frequency of {}\\nby Division Name\".format(huevar))\naxes[1].set_ylabel(\"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a0767f6bc788ba8293a3afa15a4bcce209e2bbc","_cell_guid":"341b8983-2f23-4e1f-af3a-47615947b426"},"cell_type":"markdown","source":"**Interpretation:** <br>\nThe finding here is the same as the earlier heatmap. Nothing tremendous.\n\n***\n**Rating by Department and Divison Name**"},{"metadata":{"_kg_hide-input":false,"_uuid":"1de73ae67115b2b0cca02266abb2acaf1b022154","_cell_guid":"f1e2b150-452a-4f9a-a488-2558724d72c8","trusted":true},"cell_type":"code","source":"xvar = [\"Department Name\",\"Division Name\"]\nhuevar = \"Rating\"\nf, axes = plt.subplots(1,2,figsize=(12,5))\npercentstandardize_barplot(x=xvar[0],y=\"Percentage\", hue=huevar,data=df, ax=axes[0])\naxes[0].set_title(\"Percentage Frequency of {}\\nby {}\".format(huevar, xvar[0]))\naxes[0].set_ylabel(\"Percentage %\")\npercentstandardize_barplot(x=xvar[1],y=\"Percentage\", hue=\"Rating\",data=df, ax=axes[1])\naxes[1].set_title(\"Percentage Frequency of {}\\nby {}\".format(huevar, xvar[1]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7f75d28eff80b8ab2fe70d54b6899e7433d32b7","_cell_guid":"643ddef2-37c1-4b47-a509-d2c1e10d42c0"},"cell_type":"markdown","source":"**Interpretation:** <br>\nDepartment and Divison are consistent with the overall distribution of Rating.\n\n***\n**Positive Feedback Count over 40 by Recomended IND and Rating**"},{"metadata":{"_kg_hide-input":false,"_uuid":"cb245a70c7c7c603667a5cc77f2751ca8409f0a9","_cell_guid":"856e1b4b-258a-4c23-8009-cdd1bdd6943c","trusted":true},"cell_type":"code","source":"# Cuttoff Variable\ndf[\"Cutoff\"] = df[\"Positive Feedback Count\"] >= 40 # Temporary variable for facetgrid\n# Facet Grid Plot\ng = sns.FacetGrid(df, row = \"Cutoff\", col=\"Recommended IND\",\n                  hue=\"Rating\", size=4, aspect=1.1, sharey=False, sharex=False)\ng.map(sns.distplot, \"Positive Feedback Count\", hist=False)\ng.add_legend()\ng.axes[0,0].set_ylabel('Density')\ng.axes[1,0].set_ylabel('Density')\nplt.subplots_adjust(top=0.90)\ng.fig.suptitle('Positive Feedback Count by Recommended (Column) and Rating (Color) and Cutoff (Feedback >40 is True)')\n\n# Give cutoff line to each plot.\nfor x in [0,1]:\n    for y in [0,1]:\n        g.axes[x,y].axvline(x=40, c=\"r\")\n\nplt.show()\ndel df[\"Cutoff\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cedffdf5df26456f16bb0084a6089aa2d61f30a","_cell_guid":"167ea012-bdec-45b2-a049-6bde75dfc62b"},"cell_type":"markdown","source":"**Code Explanation:** <br>\nWhile I have mostly built my multi-plot visualization configuration from scratch, here is an facetplot example which is less complex, but nevertheless, requires careful planning of new variables/dimensions, such as my \"Cutoff\" variable.\n\nThe red vertical line corresponds to the cutoff rule. Note that KDE likes to smooth out its tails, even though the hard cutoff would contradict this. More realistic representation would appear with a barplot, but the clutter would be too in-intelligible.\n\n**Interpretation:** <br>\nAs a follow-up on the preview analysis on the dominant high positive feedback count rate of reviews recommended by the customer, this plot offers even more nuance.\n\nFirst finding is the bump in on the bottom left: Cutoff = True | Recommended IND = 0. Now, this plot explores un-hopeful criticism about certain products, which is why the light blue's (rating = 1) second bump dominates the ~110 positive feedback count range.\n\nThe second finding is the the bottom right plot. Here, these are popular reviews which are recommended. It it interesting to see the high spread of the yellow distribution, rating = 3. This indicates that hopeful reviews which offer constructive criticism are the most socially appreciated.\n\n***\n**Rating by Recommended IND**\n"},{"metadata":{"_kg_hide-input":false,"_uuid":"906d8dd4ce833c198ff81b02c98b7d060633d59f","_cell_guid":"f3d2e059-7d8a-4c8b-aea5-f546dafd1971","scrolled":false,"trusted":true},"cell_type":"code","source":"huevar = \"Rating\"\nf, axes = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(x=\"Rating\", hue=\"Recommended IND\",data=df, ax=axes[0])\naxes[0].set_title(\"Occurrence of {}\\nby {}\".format(huevar, \"Recommended IND\"))\naxes[0].set_ylabel(\"Count\")\npercentstandardize_barplot(x=\"Rating\",y=\"Percentage\", hue=\"Recommended IND\",data=df, ax=axes[1])\naxes[1].set_title(\"Percentage Normalized Occurrence of {}\\nby {}\".format(huevar, \"Recommended IND\"))\naxes[1].set_ylabel(\"% Percentage by Rating\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f08acaa1adb753a8ac46f54d06628395ae5bf57","_cell_guid":"b65c9b0b-6b0c-4406-8e00-2ab24bf14dbf"},"cell_type":"markdown","source":"**Interpretation:** <br>\nThis is a big one, which returns to my question: \"How do customers express their dislike for a Product\". There is a conflicting interest between the customers personal interaction with the product, such as the personal size fit, experience, and other personal synergies, and what the customer would invision for other customers.\n\nMy theory is that when customers give product a low rating, but nevertheless recommend the item, the customer is protesting about a personal complaint they have, such as a fit issue or customer service and product handling problem all the while still expressing admiration for the product, an approval of style worthy for the body of another.\n\nLooking at the data, it appears like five star ratings are void of non-recommendations, but low rated products are recommended a small amount of the time.\n\nThe more even occurrence between recommended and non-recommended on products with three rating is a phenomenon worth getting to the bottom of. Especially the recommended portion of the reviews, which might shed light on the biggest limitations of the retailers personal servicing, and the customers personal clothing experience.\n\n***\n\n## 4. Multivariate Analysis and Descriptive Statistics\n<a id=\"Multianalysis\"></a>\n\nIn this section, I will no longer look at merely observation count by feature, but also look at how averages and other descriptive statistics behave when cut up.\n\n**Rating by Recommended IND**"},{"metadata":{"_kg_hide-input":false,"_uuid":"056221b7042ed93b006021573910d3dd7e69f6c2","_cell_guid":"0984f298-c6eb-4bcd-84df-c9b562f6212c","trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,3,figsize=(12,5))\nrot = 30\ndf.pivot_table('Rating',\n               columns=['Recommended IND']).plot.bar(ax=axes[0],rot=rot)\naxes[0].set_title(\"Average Rating by\\nRecommended IND\")\ndf.pivot_table('Rating', index='Division Name',\n               columns=['Recommended IND']).plot.bar(ax=axes[1], rot=rot)\naxes[1].set_title(\"Average Rating by Divison Name\\nand Recommended IND\")\ndf.pivot_table('Rating', index='Department Name',\n               columns=['Recommended IND']).plot.bar(ax=axes[2], rot=rot)\naxes[0].set_ylabel(\"Rating\")\naxes[2].set_title(\"Average Rating by Department Name\\nand Recommended IND\")\nf.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9c3c9d787ef1e1cae022d66fc4c066bb7004598","_cell_guid":"0942a784-e265-4ec3-a62e-90c6774e8e7e"},"cell_type":"markdown","source":"**Interpretation:** <br>\nRating is just under max rating when recommended, and halfed when not recommended. Trend is consistent across Division and Department.\n\n***\n**Correlating Average Rating and Recommended IND by Clothing ID** <br>\nAnalysis of data grouped by Clothing ID."},{"metadata":{"_kg_hide-input":false,"_uuid":"c0842f7574c7de6a983e691c0a4f401c540d92d1","_cell_guid":"d230a677-35a0-4084-9522-b924580a1bf7","trusted":true},"cell_type":"code","source":"temp = (df.groupby('Clothing ID')[[\"Rating\",\"Recommended IND\", \"Age\"]]\n        .aggregate(['count','mean']))\ntemp.columns = [\"Count\",\"Rating Mean\",\"Recommended IND Count\",\n                \"Recommended Mean\",\"Age Count\",\"Age Mean\"]\ntemp.drop([\"Recommended IND Count\",\"Age Count\"], axis=1, inplace =True)\n\n# Plot Correlation Matrix\nf, ax = plt.subplots(figsize=[9,6])\nax = sns.heatmap(temp.corr()\n    , annot=True, fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'})\nax.set_title(\"Correlation Matrix for Mean and Count for\\nRating,Recommended, and Age\\nGrouped by Clothing ID\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8441ef4546c4dcbebeea1525d0dcf88770d44b5b","_cell_guid":"be24534f-ce44-4e3e-a796-42db7092681b"},"cell_type":"markdown","source":"**How to Interpret:** <br>\nI must stress the *Grouped By Clothing ID* aspect of this analysis. This aggregation investigates if there is trend between average rating and number of reviews by product. This is a different lense of analysis than merely running a correlation on *all customers reviews*.\n\n**Interpretation:** <br>\nThis correlation heatmap suggest that there is in fact no correlation between count and average value, which means that the popularity of the item does not lead to differential treatment when it comes to average scoring. The age variable behaves in this same as well.\n\nHowever, There is a strong positive correlation of .80 between rating and recommended IND mean."},{"metadata":{"_kg_hide-input":false,"_uuid":"1abd813f84d0f23b69518d170b6725c71ef41e94","_cell_guid":"2f08b13b-ad16-4fd0-9e8e-c410b594f4df","trusted":true},"cell_type":"code","source":"g = sns.jointplot(x= \"Recommended Mean\",y='Rating Mean',data=temp,\n                  kind='reg', color='b')\nplt.subplots_adjust(top=0.999)\ng.fig.suptitle(\"Rating Mean and Recommended Mean\\nGrouped by Clothing ID\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71c176e225887ae14e09f0b4e6fc74a632ceb9c3","_cell_guid":"f97db2e5-2a67-4197-b367-cb32175d9567"},"cell_type":"markdown","source":"**Interpretation:** <br>\nHere is a closer look at this correlation of interest. And look at that p-value! Someone call a publisher.\n\nJokes aside, perhaps the dots are the bottom left could be the products that unarguably need attention from the retailer, in the hope of preserving brand image."},{"metadata":{"_kg_hide-input":false,"_uuid":"8c3fa7e4532f916c580af8b3b8a59b7874e08a55","_cell_guid":"61adcef5-fbfe-4b19-9be9-06202945fd98","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplt.scatter(temp[\"Recommended Mean\"],temp[\"Rating Mean\"],\n            alpha = .8, c =temp[\"Count\"], cmap = 'seismic')\ncbar = plt.colorbar() # Color bar. Vive la France!\ncbar.set_label('Count', rotation=90)\nplt.xlabel(\"Average Recommended IND\")\nplt.ylabel(\"Average Rating\")\nplt.title(\"Clothing Piece Frequency (Color) on\\nRating and Recommended Mean Scatter\")\n\n# Vertical and Horizontal Lines\nl = plt.axhline(y=3.3)\nl = plt.axvline(x=.55)\n\n# Text\nplt.text(.15, 1, \"Lower\\nQuadrant\", ha='left',wrap=True,fontsize=17)\nplt.show()\n\n# Descriptives for LOW QUADRANT\ntemp[(temp[\"Rating Mean\"] < 3.3) | (temp[\"Recommended Mean\"] <= .55)].describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"26acb1a1bb2d08bdf918468ea50510a85dccba02","_cell_guid":"ff5ca64a-28ee-4c31-aa2c-c98995790652"},"cell_type":"markdown","source":"Follow-up on the previous correlation plot. This plot displays that these outliers are not very strongly represented. Indeed, the average count for the **LOW QUADRANT**, as labeled at the bottom left of the plot, is only 2.3. For these reasons, hyper negative reviews may be unrepresentative outliers, and not taken as the public's general opinion. \n\nA practise I could envision tackling this problem is to include the average rating of the product class, such as \"Dress\", in order to relieve customers who may be worried about product with low, hyper negative reviews.\n\n***\n**Correlating Average Rating and Recommended IND by Class Name** <br>\n- [Stackoverflow Annotating Outliers](https://stackoverflow.com/questions/43010462/annotate-outliers-on-seaborn-jointplot)"},{"metadata":{"_kg_hide-input":false,"_uuid":"9504163cd4fda31d225f99b5b519acfb4f378704","_cell_guid":"1b4b3cb2-3041-492d-9009-1de00e9386d8","scrolled":true,"trusted":true},"cell_type":"code","source":"key = \"Class Name\"\ntemp = (df.groupby(key)[[\"Rating\",\"Recommended IND\", \"Age\"]]\n        .aggregate(['count','mean']))\ntemp.columns = [\"Count\",\"Rating Mean\",\"Recommended Likelihood Count\",\n                \"Recommended Likelihood\",\"Age Count\",\"Age Mean\"]\ntemp.drop([\"Recommended Likelihood Count\",\"Age Count\"], axis=1, inplace =True)\n\n# Plot Correlation Matrix\nf, ax = plt.subplots(figsize=[9,6])\nax = sns.heatmap(temp.corr()\n    , annot=True, fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'})\nax.set_title(\"Correlation Coefficient for Mean and Count for\\nRating, Recommended Likelihood, and Age\\nGrouped by {}\".format(key))\nplt.show()\nprint(\"Class Categories:\\n\",df[\"Class Name\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1668d399b63ce3b3e1db03b53fa1637a9842c9d","_cell_guid":"33c2c75a-00fd-434d-a4ac-d57ef6d9e6d0"},"cell_type":"markdown","source":"**Interpretation:** <br>\nFor the various Class cateogries there a notable correlation between average age and recommendation likelihood. I shall investigate."},{"metadata":{"_kg_hide-input":false,"_uuid":"b73f64dc9e47e936d42b35438868261754552e6e","_cell_guid":"13781607-515e-48bf-9ad9-f328cafc8baa","trusted":true},"cell_type":"code","source":"# Simple Linear Regression Model\nmodel_fit = sm.OLS(temp[\"Recommended Likelihood\"],\n               sm.add_constant(temp[\"Age Mean\"])).fit() \ntemp['resid'] = model_fit.resid\n\n# Plot\ng = sns.jointplot(y=\"Recommended Likelihood\",x='Age Mean',data=temp,\n                  kind='reg', color='b')\nplt.subplots_adjust(top=0.999)\ng.fig.suptitle(\"Age Mean and Recommended Likelihood\\nGrouped by Clothing Class\")\nplt.ylim(.7, 1.01)\n\n# Annotate Outliers\nhead = temp.sort_values(by=['resid'], ascending=[False]).head(2)\ntail = temp.sort_values(by=['resid'], ascending=[False]).tail(2)\n\ndef ann(row):\n    ind = row[0]\n    r = row[1]\n    plt.gca().annotate(ind, xy=( r[\"Age Mean\"], r[\"Recommended Likelihood\"]), \n            xytext=(2,2) , textcoords =\"offset points\", )\n\nfor row in head.iterrows():\n    ann(row)\nfor row in tail.iterrows():\n    ann(row)\n\nplt.show()\ndel head, tail\n\ntemp[temp[\"Recommended Likelihood\"] > .95]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25186f42bf5002a728a88e2e6c176605991c33f2","_cell_guid":"b4c915cd-eb1b-46ca-9786-18218871138b"},"cell_type":"markdown","source":"**Interpretation:** <br>\nCheck out my [**Other Kernel: In-Depth Simple Linear Regression\n**](https://www.kaggle.com/nicapotato/in-depth-simple-linear-regression) for a deep dive into this regression.\n\n***\n\n## 5.  Working with Text <a id=\"Text\"></a>\n\nNow that a general understanding of the variables have been laid out, I will begin to analysis the customer reviews.\n\n### 5.1 Text Pe-Processing"},{"metadata":{"_kg_hide-input":false,"_uuid":"628d1f6222dc25f26e7887edc75d6cf650892765","_cell_guid":"f8b97ebb-085d-4388-9fc3-493b13a1bc46","trusted":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 500)\ndf[[\"Title\",\"Review Text\", \"Rating\"]].sample(7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a10e6d0a550a406ac713c774f3fa40050136691","_cell_guid":"3da605e0-a568-4db4-b43f-c5e77a82d571"},"cell_type":"markdown","source":"Evidently, the text data requires further processing ."},{"metadata":{"_kg_hide-input":true,"_uuid":"7956a4919260382fe89e5edf3581120461f40f92","_cell_guid":"71d898cd-7ff9-4a8f-a870-585abac562b3","trusted":true},"cell_type":"code","source":"from nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.porter import PorterStemmer\n#ps = LancasterStemmer()\nps = PorterStemmer()\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\n\ndef preprocessing(data):\n    txt = data.str.lower().str.cat(sep=' ') #1\n    words = tokenizer.tokenize(txt) #2\n    words = [w for w in words if not w in stop_words] #3\n    #words = [ps.stem(w) for w in words] #4\n    return words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"441dd7c4b06e161b4bcb5cb7a8cb771e8ebf8325","_cell_guid":"d58f392f-5de3-4989-bda6-41a3a792fbd1"},"cell_type":"markdown","source":"**Code Explanation:** <br>\nThis chunk of code creates a function that takes each review and combines them into one seamless text. It then applies lowercase, tokenizer, removes stopwords and punctuation, and finally uses the PorterStemmer.\n\n***\n\n**Interpretation:** <br>\nIn order to process the data set's centerpiece, the review body, I utilized the NLTK package to lowercase, tokenize, and remove stopwords and punctuation. Tokenizing treats each word as its own value, while the other steps gets rid of the noise and irrelevant symbols in the data, standardizing the reviews for analysis. Upon reviewing the performance of text analysis, I decided to implement the Porter Stemmer on the tokens in order to combine words with tense and plurality deviance. I contemplated exploring the use of sequential models, such as Long Short-term memory, which would benefit from stop words, but unfortunately I could only find predictive applications of it, no insight extracting aspects. \n\nThe last piece of data transformation conducted was to bin the continuous variable age into a categorical variable: age category.\n\n***\n\n### 5.2 Sentiment Analysis <a id=\"Sentiment Analysis\"></a>\n\nMy first attempt at understanding the customer reviews is to see how the textual sentiment relates to the rating scores. With this method, it will be possible to distinguish outright positive and negative comments from the constructive variant.\n\nI will also explore the interaction between sentiment score:\n- Raiting\n- Recommended\n- Positive Feedback Count"},{"metadata":{"_kg_hide-input":false,"_uuid":"511582fe0c6f4973577427487f495d0c4b98758f","_cell_guid":"b324b3b7-b5a1-4318-b865-8561dcef7eb4","trusted":true},"cell_type":"code","source":"# Pre-Processing\nSIA = SentimentIntensityAnalyzer()\ndf[\"Review Text\"]= df[\"Review Text\"].astype(str)\n\n# Applying Model, Variable Creation\ndf['Polarity Score']=df[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['compound'])\ndf['Neutral Score']=df[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['neu'])\ndf['Negative Score']=df[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['neg'])\ndf['Positive Score']=df[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['pos'])\n\n# Converting 0 to 1 Decimal Score to a Categorical Variable\ndf['Sentiment']=''\ndf.loc[df['Polarity Score']>0,'Sentiment']='Positive'\ndf.loc[df['Polarity Score']==0,'Sentiment']='Neutral'\ndf.loc[df['Polarity Score']<0,'Sentiment']='Negative'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84fd75b326cafb89719ad9c5a588358d982c7e57","_cell_guid":"87a42536-402f-489c-9a44-7ece3a2af8c2"},"cell_type":"markdown","source":"#### **Code Explanation:** <br>\n*Pre-processing* chunk loads the NLTK Sentiment Intensity Analyzer module, selects desired variables, and finally applies lowercasing to the column of reviews in the dataframe. The second paragraph of code *Applying Model and Variable Creation* classifies each review in the dataset on three dimensions: Positive, Neutral, and Negative. These results are stored in three respective columns. The overall sentiment is then determined and stored in the Sentiment column.\n\n- **Neutral/Negative/Positive Score:** Indicates the potency of these classes between 0 and 1. Onl\n- **Polarity Score:** Measures the difference between the Positive/Neutral/Negative values, where a positive numbers closer to 1 indicates overwhelming positivity, and a negative number closer to -1 indicates overwhelming negativity.\n\n***\n\n**Normalize Plots for Sentiment Distribution**"},{"metadata":{"_kg_hide-input":false,"_uuid":"0add25aa325ae7e50e2acbf0db1758947b22df8b","_cell_guid":"2d1ebd44-54c1-40de-80db-ef4a25474ec2","trusted":true},"cell_type":"code","source":"huevar = \"Recommended IND\"\nxvar = \"Sentiment\"\nf, axes = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(x=xvar, hue=huevar,data=df, ax=axes[0], order=[\"Negative\",\"Neutral\",\"Positive\"])\naxes[0].set_title(\"Occurence of {}\\nby {}\".format(xvar, huevar))\naxes[0].set_ylabel(\"Count\")\npercentstandardize_barplot(x=xvar,y=\"Percentage\", hue=huevar,data=df, ax=axes[1])\naxes[1].set_title(\"Percentage Normalized Occurence of {}\\nby {}\".format(xvar, huevar))\naxes[1].set_ylabel(\"% Percentage by {}\".format(huevar))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7facf79bd7bc7c24f23958a45fdd4239dd1334bc","_cell_guid":"f1936867-fcd9-4143-a0aa-7e6715856e0f"},"cell_type":"markdown","source":"**Interpretation:** <br>\nRecommended is a variable that clearly indicates positive sentiment in the review."},{"metadata":{"_kg_hide-input":false,"_uuid":"bb3262b3ae1411dfc8eb11834856f01a81627109","_cell_guid":"067a0768-f426-40cf-8dc1-2c9c83b74acd","scrolled":false,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2,2, figsize=[9,9])\nsns.countplot(x=\"Sentiment\", data=df, ax=axes[0,0], order=[\"Negative\",\"Neutral\",\"Positive\"])\naxes[0,0].set_xlabel(\"Sentiment\")\naxes[0,0].set_ylabel(\"Count\")\naxes[0,0].set_title(\"Overall Sentiment Occurrence\")\n\nsns.countplot(x=\"Rating\", data=df, ax=axes[0,1])\naxes[0,1].set_xlabel(\"Rating\")\naxes[0,1].set_ylabel(\"\")\naxes[0,1].set_title(\"Overall Raiting Occurrence\")\n\npercentstandardize_barplot(x=\"Rating\",y=\"Percentage\",hue=\"Sentiment\",data=df, ax=axes[1,0])\naxes[1,0].set_xlabel(\"Rating\")\naxes[1,0].set_ylabel(\"Percentage %\")\naxes[1,0].set_title(\"Standardized Percentage Raiting Frequency\\nby Sentiment\")\n\npercentstandardize_barplot(x=\"Sentiment\",y=\"Percentage\",hue=\"Rating\",data=df, ax=axes[1,1])\naxes[1,1].set_ylabel(\"Occurrence Frequency\")\naxes[1,1].set_title(\"Standardized Percentage Sentiment Frequency\\nby Raiting\")\naxes[1,1].set_xlabel(\"Sentiment\")\naxes[1,1].set_ylabel(\"\")\n\nf.suptitle(\"Distribution of Sentiment Score and Rating for Customer Reviews\", fontsize=14)\nf.tight_layout()\nf.subplots_adjust(top=0.92)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42b83061f6923657908c3f068ef305a60f504dd6","_cell_guid":"88c4eb88-0eb2-458a-a5ab-9e6f2bc7ff4d"},"cell_type":"markdown","source":"**Code Interpretation:** <br>\nThe last chunk, Visualization, plots the frequency of sentiments in a bar plot using matplotlib.\n\n**Interpretation:** <br>\nLike the distribution of rating, most reviews have a positive sentiment. Unlike the distribution of rating, there is a lower occurrence of neutral rating is lower in proportion to the occurrence of medium ranged ratings.\n\nThe plot on the bottom right tells and interesting story. The rating of positive sentiment reviews have an increasing occurrence as the rating gets higher. But, but negative and neutral sentiment reviews, the highest occurrence rating has 3 rating, further emphasizing that people's motivation of assigning a review score of three are multiple.\n\n***"},{"metadata":{"_kg_hide-input":false,"_uuid":"bdd1392854d456ea1dcf8d3989a5d444be5a33a1","_cell_guid":"3360fac4-1b46-45f4-904a-a5e8b88c05af","trusted":true},"cell_type":"code","source":"# Tweakable Variables (Note to Change Order Arguement if Xvar is changed)\nxvar = \"Sentiment\"\nhuevar = \"Department Name\"\nrowvar = \"Recommended IND\"\n\n# Plot\nf, axes = plt.subplots(2,2,figsize=(10,10), sharex=False,sharey=False)\nfor i,x in enumerate(set(df[rowvar][df[rowvar].notnull()])):\n    percentstandardize_barplot(x=xvar,y=\"Percentage\", hue=huevar,data=df[df[rowvar] == x],\n                 ax=axes[i,0], order=[\"Negative\",\"Neutral\",\"Positive\"])\n    percentstandardize_barplot(x=xvar,y=\"Percentage\", hue=\"Rating\",data=df[df[rowvar] == x],\n                 ax=axes[i,1], order=[\"Negative\",\"Neutral\",\"Positive\"])\n\n# Plot Aesthetics\naxes[1,0].legend_.remove()\naxes[1,1].legend_.remove()\naxes[0,1].set_ylabel(\"\")\naxes[1,1].set_ylabel(\"\")\naxes[0,0].set_xlabel(\"\")\naxes[0,1].set_xlabel(\"\")\naxes[0,0].set_ylabel(\"Recommended = FALSE\\nPercentage %\")\naxes[1,0].set_ylabel(\"Recommended = TRUE\\nPercentage %\")\naxes[1,1].set_title(\"\")\n\n# Common title and ylabel\nf.text(0.0, 0.5, 'Subplot Rows\\nSliced by Recommended', va='center', rotation='vertical', fontsize=12)\nf.suptitle(\"Review Sentiment by Department Name and Raiting\\nSubplot Rows Slice Data by Recommended\", fontsize=16)\nf.tight_layout()\nf.subplots_adjust(top=0.93)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b734cd468885f8244038f6f80cc7f7703e5ab537","_cell_guid":"5a8d59ee-970a-4391-933f-974e90cbbc15"},"cell_type":"markdown","source":"**How to Interpret:** <br>\nIn this plot, the upper and lower rows use the same variables, but the upper row is for non-recommended reviews, while the bottom row is for recommended reviews. This enables use to explore the nature of recommended reviews in terms of the mood of the writing, as well as the rating assigned by the customer.\n\n**Interpretation:** <br>\nWhile the distribution of departments does not seem to change depending on status of recommendation, rating is almost entirely inverted. My previous theory that recommended reviews hold more criticizing  weight does not hold up in this case since recommended reviews have a highly positive sentiment occurrence."},{"metadata":{"_kg_hide-input":false,"_uuid":"3372d2895c90787d25ad81b7f0ab58c3760a1d3c","_cell_guid":"4eb75615-acb0-4d07-afa9-c17b5826fc41","trusted":true},"cell_type":"code","source":"# Plot Correlation Matrix\nf, ax = plt.subplots(figsize=[9,6])\nax = sns.heatmap(df.corr(), annot=True,\n                 fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'})\nax.set_title(\"Correlation Matrix for All Variables\")\nplt.show()\n\n# Sentiment Positivity Score by Positive Feedback Count\nax = sns.jointplot(x= df[\"Positive Feedback Count\"], y=df[\"Positive Score\"], kind='reg', color='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d970046ba9738f4c8f48c0de30173329acb7966a","_cell_guid":"63cbf0f5-065b-40ba-823a-369a3152d1a2"},"cell_type":"markdown","source":"**Interpretation:** <br>\nInterestingly, there appears to be a substantial negative correlation between Positive Feedback Count and Positive Score, which suggests that the most acclaimed reviews on the platform are probably in the form on constructive criticism, rather than outright positivity.\n***\n\n## 6. Word Distribution and Word Cloud <a id=\"Word Distribution and Word Cloud\"></a> <br>\n\n** For this section, I deviated from the book and heavily relied upon the following online resources:** <br>\n- [Kaggle Longdoa: Word Cloud in Python](https://www.kaggle.com/longdoan/word-cloud-with-python)\n- [Word Cloud Package Forum](https://github.com/amueller/word_cloud/issues/134)\n- [Amueller Github](https://amueller.github.io/word_cloud/auto_examples/masked.html)"},{"metadata":{"_kg_hide-input":false,"_uuid":"847d72eb4ea3828149d3dbc7d8f1910be93742b6","_cell_guid":"ee137296-5f5a-4de1-95a6-d8397e907cf3","trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nsize = (10,7)\n\ndef cloud(text, title, stopwords=stopwords, size=size):\n    \"\"\"\n    Function to plot WordCloud\n    Includes: \n    \"\"\"\n    # Setting figure parameters\n    mpl.rcParams['figure.figsize']=(10.0,10.0)\n    mpl.rcParams['font.size']=12\n    mpl.rcParams['savefig.dpi']=100\n    mpl.rcParams['figure.subplot.bottom']=.1 \n    \n    # Processing Text\n    # Redundant when combined with my Preprocessing function\n    wordcloud = WordCloud(width=1600, height=800,\n                          background_color='black',\n                          stopwords=stopwords,\n                         ).generate(str(text))\n    \n    # Output Visualization\n    fig = plt.figure(figsize=size, dpi=80, facecolor='k',edgecolor='k')\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=50,color='y')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n# Frequency Calculation [One-Gram]\ndef wordfreqviz(text, x):\n    word_dist = nltk.FreqDist(text)\n    top_N = x\n    rslt = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n    matplotlib.style.use('ggplot')\n    rslt.plot.bar(rot=0)\n\ndef wordfreq(text, x):\n    word_dist = nltk.FreqDist(text)\n    top_N = x\n    rslt = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n    return rslt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f52b860c0d5134048f0c3989cadee75d110db5b0","_cell_guid":"e8b0fb52-8cdb-4b51-a2a0-dbc03e1ce00f"},"cell_type":"markdown","source":"#### **Code Explanation:** <br>\nThis code creates the word cloud visualization function. This function’s mathematical processes are hidden, since it does not explicitly state that it determines the frequency occurrence of each word in relation to the entire dictionary of words. Within the function, the Setting Function Parameter section creates the graphic structure using matplotlib. Then the text is formatted, and the word frequency is determined. Finally, the matplotlib structure is filled with words, where the larger the word size, the higher the word occurrence. "},{"metadata":{"_uuid":"ae0175190016da3bec88582756eccb6a898ef2bb","_cell_guid":"aeb26217-8251-4f91-bdc4-0e3b67618e09"},"cell_type":"markdown","source":"### Visualize Titles"},{"metadata":{"_kg_hide-input":false,"_uuid":"f98a173e2fea565b32022547503506e36190e502","_cell_guid":"0bfccdac-c106-4851-94d8-e07942261262","trusted":true},"cell_type":"code","source":"# Modify Stopwords to Exclude Class types, suchs as \"dress\"\nnew_stop = set(STOPWORDS)\nnew_stop.update([x.lower() for x in list(df[\"Class Name\"][df[\"Class Name\"].notnull()].unique())]\n                + [\"dress\", \"petite\"])\n\n# Cloud\ncloud(text= df.Title[df.Title.notnull()].astype(str).values,\n      title=\"Titles\",\n      stopwords= new_stop,\n      size = (7,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9851ae1b8462f3b121cc603f3868e71cb1d12c7","_cell_guid":"fa60043b-bb11-4be1-aad2-124c1ca14d18"},"cell_type":"markdown","source":"### Visualize Reviews"},{"metadata":{"_kg_hide-input":false,"_uuid":"ef185e0c44cba05e99951d6faaa6bbb0c9e472f1","_cell_guid":"5709dc2b-0b11-4dd7-b234-746c2d53a25f","trusted":true,"scrolled":false},"cell_type":"code","source":"# Highly Raited\ntitle =\"Highly Rated Comments\"\ntemp = df['Review Text'][df.Rating.astype(int) >= 3]\n\n# Modify Stopwords to Exclude Class types, suchs as \"dress\"\nnew_stop = set(STOPWORDS)\nnew_stop.update([x.lower() for x in list(df[\"Class Name\"][df[\"Class Name\"].notnull()].unique())]\n                + [\"dress\", \"petite\"])\n\n# Cloud\ncloud(text= temp.values, title=title,stopwords= new_stop)\n\n# Bar Chart\nwordfreq(preprocessing(temp),20).plot.bar(rot=45, legend=False,figsize=(15,5), color='g',\n                          title= title)\nplt.ylabel(\"Occurrence Count\")\nplt.xlabel(\"Most Frequent Words\")\nplt.show()\n\n# Low Raited\ntitle =\"Most Frequent Words in Low Rated Comments\"\ntemp = df['Review Text'][df.Rating.astype(int) < 3]\n\n# Modify Stopwords to Exclude Class types, suchs as \"dress\"\nnew_stop = set(STOPWORDS)\nnew_stop.update([x.lower() for x in list(df[\"Class Name\"][df[\"Class Name\"].notnull()].unique())]\n                + [\"dress\", \"petite\", \"skirt\",\"shirt\"])\n\n# Cloud\ncloud(temp.values, title= title, stopwords = new_stop)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccc62bf34c6e18463a97956d3e63ded4d5995d9e","_cell_guid":"f84c8533-ca56-41ac-a797-28beda67d252"},"cell_type":"markdown","source":"#### **Code Interpretation:** <br>\nThe central flaw of these word clouds is that they only show the distribution of individual words. This removes the context of the word, as well as disregard negative prefixes. In order to solve this problem I will utilize n-grams, which increases the size of observed values from one word to multiple words, enabling frequency counts to be conducted to word sequences. Although I would have prefered to visualize these findings through the use of Word Clouds, I was unable to program this in, thus leaving me with a simple table."},{"metadata":{"_uuid":"a709e4bb8347ce1bf858deb011ee7635a2895a3d","_cell_guid":"5858a2cb-06fb-46ae-83d1-4b5ed91c4c26"},"cell_type":"markdown","source":"**Taking a Different Lense: WordClouds by Department Name** <br>"},{"metadata":{"_kg_hide-input":false,"_uuid":"a0ae53600c6754580bed17ee96bb52a1fd0867b0","_cell_guid":"e8ccc16d-3f4d-4fce-bd3c-5f448de43181","trusted":true},"cell_type":"code","source":"department_set = df[\"Department Name\"][df[\"Department Name\"].notnull()].unique()\ndivision_set = df[\"Division Name\"][df[\"Division Name\"].notnull()].unique()\ndef cloud_by_category(data, category, subclass):\n    \"\"\"\n    Function to create a wordcloud by class and subclass\n    Category signifies the column variable\n    Subclass refers to the specific value within the categorical variable\n    \"\"\"\n    new_stop = set(STOPWORDS)\n    new_stop.update([x.lower() for x in list(data[\"Class Name\"][data[\"Class Name\"].notnull()].unique())]\n                   + [x.lower() for x in list(data[\"Department Name\"][data[\"Department Name\"].notnull()].unique())]\n                   + [\"dress\", \"petite\", \"jacket\",\"top\"])\n\n    # Cloud\n    cloud(text= data[\"Review Text\"][data[category]== subclass],\n          title=\"{}\".format(subclass),\n          stopwords= new_stop,\n          size = (10,6))\n    \n# Plot\ncloud_by_category(df, \"Division Name\", division_set[0])\ncloud_by_category(df, \"Division Name\", division_set[1])\ncloud_by_category(df, \"Division Name\", division_set[2])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"08c49f2d7146bc832dd5ecf409cd69d873f1b7d9","_cell_guid":"24b1f900-3e2d-4f12-b3cc-c307ba31b44a"},"cell_type":"markdown","source":"***\n## 7. N Grams by Recommended Feature\n<a id=\"NGRAM\"></a>\n\nAt this point, fit and product inconsistency strongly emerge as major topics in the reviews. From this information, I can infer that the dataset belongs to a online retailer, since brick and mortar stores have changing rooms to prevent this problem. The central themes in the product reviews brought to light by the n-grams are:\n- **Fit:** Whether the product’s advertised size actually corresponds to customer size and height.\n- **Love or Hate:** The customer's personal feelings towards the product.\n- **Complements:** The customer's social experience wearing the product.\n- **Product consistency:** Whether the product appears as advertised, lives up to quality expectations."},{"metadata":{"_kg_hide-input":false,"_uuid":"851ec99d523834284730798fc599a80a8e8d1d26","_cell_guid":"bf011d6a-764e-432b-bc38-6a6e5454ebec","trusted":true},"cell_type":"code","source":"## Helper Functions\nfrom nltk.util import ngrams\nfrom collections import Counter\ndef get_ngrams(text, n):\n    n_grams = ngrams((text), n)\n    return [ ' '.join(grams) for grams in n_grams]\n\ndef gramfreq(text,n,num):\n    # Extracting bigrams\n    result = get_ngrams(text,n)\n    # Counting bigrams\n    result_count = Counter(result)\n    # Converting to the result to a data frame\n    df = pd.DataFrame.from_dict(result_count, orient='index')\n    df = df.rename(columns={'index':'words', 0:'frequency'}) # Renaming index column name\n    return df.sort_values([\"frequency\"],ascending=[0])[:num]\n\ndef gram_table(data, gram, length):\n    out = pd.DataFrame(index=None)\n    for i in gram:\n        table = pd.DataFrame(gramfreq(preprocessing(data),i,length).reset_index())\n        table.columns = [\"{}-Gram\".format(i),\"Occurrence\"]\n        out = pd.concat([out, table], axis=1)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"c782606f4408e77d3d4770a41f420ae8b8e1d541","_cell_guid":"88db0b6d-b3b6-408e-a365-6d625118e19c","trusted":true},"cell_type":"code","source":"print(\"Non-Recommended Items\")\ngram_table(data= df['Review Text'][df[\"Recommended IND\"].astype(int) == 0], gram=[1,2,3,4,5], length=30)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"d23c1d5cffddc2d64ce930ae47494f3450611bd6","_cell_guid":"6035454c-1184-4f68-87fc-45f992f13b8f","trusted":true},"cell_type":"code","source":"print(\"Recommended Items\")\ngram_table(data= df['Review Text'][df[\"Recommended IND\"].astype(int) == 1], gram=[1,2,3,4,5], length=30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f046b276edd490fc370cff8e8da640105729cbf9","_cell_guid":"3e4fe5dd-14af-4dab-9757-e04a4b23a46f"},"cell_type":"markdown","source":"**Interpretation:** <br>\nIn the negative reviews, customers express their disappointment in the product, stating that they “really wanted to love” the item. This signifies that the product did not live up to the customers expectations. This occurred for multiple reasons. “Order wear size” and “Usual wear size” suggest that the fit did not suit their typical universal body size. Perhaps if better product dimension information could be provided, then the likelihood of this negative response could decrease. Furthermore, perhaps the product platform could track the user’s size through previous purchase in order to warn customer for potential size conflict.\nAnother form of negative review is in the disappointment in the product turnout. “Too much fabric” and “Looks nothing like” suggest inconsistency with online retail presentation and actual product. These reviews are especially destructive, since they damage the reputation of the store product quality, which is a online platforms biggest asset.\nOn the other hand, positive reviews are void of criticism, and are preoccupied with confirming fit and sharing social experience with the clothing. “True Size”, “Fit Perfectly”, “Fit like a glove”, on top of the multiple 2-grams with customer’s height suggest that a large part of positive reviews are employed to confirm product fit according to certain size. The high occurrence of this review suggest that height and size is usually a big issue, which this retail managed to consistently satisfy.\n“Received many compliments”, “Look forward to wearing”, “Everytime I wear”, “Looks great with jeans” are all comments which reflect the customer's experience wearing the product out in public. This not only express the relevance of trendy, jaw dropping fashion for customers in a social context, but also suggests that the product review are a highly social space, in which customers not only talk with the retailer, but with the other customers as well.\n\n***\n\n## 8. Intelligible Supervised Learning\n\nSupervised learning requires features (independent variable) and a label (dependent variable).  The Formatting section does just this by creating a tuple with the comment and customer rating label. Currently the independent variable is the entire comment. However, in order to the Naïve Bayes Algorithm to work, each word must be treated as a variable. Instead of utilizing sequential words, the model notes which words are present out of the entire dictionary of words available in the comments corpus. In order to reduce computational intensity, only the top 5000 most common words will be considered, instead of the 9000 unique words in the corpus. The find_features function does just this by checking the presence of words for a piece of text against word_features, a variable created earlier which includes the top 5000 most common words used by customers in this dataset. The Apply Function to Data section applies the find_features function to each individual customer review using a loop, while also retaining each review’s label."},{"metadata":{"_kg_hide-input":false,"_uuid":"efa39c38ec0de149b4b822b412721edf1b32a471","_cell_guid":"c68e8b80-c659-4cee-93ea-a1325447df87","scrolled":true,"trusted":true},"cell_type":"code","source":"df['tokenized'] = df[\"Review Text\"].astype(str).str.lower() # Turn into lower case text\ndf['tokenized'] = df.apply(lambda row: tokenizer.tokenize(row['tokenized']), axis=1) # Apply tokenize to each row\ndf['tokenized'] = df['tokenized'].apply(lambda x: [w for w in x if not w in stop_words]) # Remove stopwords from each row\ndf['tokenized'] = df['tokenized'].apply(lambda x: [ps.stem(w) for w in x]) # Apply stemming to each row\nall_words = nltk.FreqDist(preprocessing(df['Review Text'])) # Calculate word occurrence from whole block of text\n\nvocab_count = 200\nword_features= list(all_words.keys())[:vocab_count] # 2000 most recurring unique words\nprint(\"Number of words columns (One Hot Encoding): {}\".format(len(all_words)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67c6ef5169be6d32e8d5edd23158782f5da16ea3","_cell_guid":"6a0dc946-34f5-4fb8-8da8-c05873bc8b09"},"cell_type":"markdown","source":"\n**Converting Text to a Model-able format: One Hot Encoding**"},{"metadata":{"_kg_hide-input":false,"_uuid":"9464791b5d129af5c04738c6daac3673396c6fa5","_cell_guid":"ee1303c6-f185-4270-9e07-dcb292d0c874","trusted":true},"cell_type":"code","source":"# Tuple\nlabtext= list(zip(df.tokenized, (df[\"Recommended IND\"]))) \n\n# Function to create model features\n# for each review, records which unique words out of the whole text body are present\ndef find_features(document):\n    words = set(document)\n    features = {}\n    for w in word_features:\n        features[w] = (w in words)\n\n    return features\n# Apply function to data\nfeaturesets = [(find_features(text), LABEL) for (text, LABEL) in labtext]\nlen(featuresets)\n\n# Train/Test\ntraining_set = featuresets[:15000]\ntesting_set = featuresets[15000:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36eeddabebf452b29849a8321f7f89f5e435fa8d","_cell_guid":"301a1392-0817-4a99-9b0c-13f9c24952a1"},"cell_type":"markdown","source":"## 8.1 Naive Bayes\n\n- Independent Variable: Word choices in Reviews\n- Dependent Variable: Whether or not review was Recommended\n\nSupervised learning is typically employed to make predictions about the future. However, some simple models may also be opened up to offer some insight. Naive Bayes is a probabilistic model which depends on Bayes theorem to compute the probability of a word's category by looking at its occurrence over the different classes. Since this model looks at both good and bad reviews, it is able to extract the one-gram tokens which best polarize the categories. Using this model, I could potentially predict the positive or negative sentiment of unlabelled reviews."},{"metadata":{"_kg_hide-input":false,"_uuid":"37e3aecdff3a9d7a4bd56c4c6af851ef96b0fa0e","_cell_guid":"e1b13027-0d84-4024-b15b-d6517ea2b0eb","scrolled":true,"trusted":true},"cell_type":"code","source":"# Posterior = prior_occurrence * likelihood / evidence\nclassifier = nltk.NaiveBayesClassifier.train(training_set)\n\n# Output\nprint(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\nprint(classifier.show_most_informative_features(40))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a612bfa7d86b13e3e9d297d57c770692e475eb17","_cell_guid":"2fb4f406-be14-4fe5-95ab-978e987df5a8"},"cell_type":"markdown","source":"**How to Interpret the Table:** <br>\nThe first column displays the word, the second represents whether the word is not-recommended (0:1), or recommended (1:0). Lastly, the third column shows the ratio of occurrence. Looking at the first column, *“worst” *is a non-recommended word, whose presence indicates the review is 23 times more likely to be negative than positive. This model’s accuracy is 82%. Naive Bayes’ predictive power is limited compared to other, more complex models, but accuracy is not the goal for this analysis.\n***\n## Key Findings\n- Product recommendation and Product rating are used for different purposes.\n    - Recommended is a strong indicator for positive sentiment in the review.\n    - Rating is more convoluted, where rating around 3 are hopeful reviews with constructive criticism of the product."},{"metadata":{"trusted":true,"_uuid":"cd5d0351567a9e0c1b518e12d86ac98a4743040b"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport scikitplot as skplt\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09ab2d594ddecf555f50cfc1c3ba382b51760b8b"},"cell_type":"code","source":"vect = TfidfVectorizer()\nvect.fit(df[\"Review Text\"])\nX = vect.transform(df[\"Review Text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99ea5c098f6a83dc17fb7785d8199e0f24eeb47e"},"cell_type":"code","source":"y = df[\"Recommended IND\"].copy()\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.20, random_state=23, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5af9cc101c48f87c28051dd919d908abfa0913e4"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nprint(\"Train Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_train), y_train)))\nprint(\"Train Set ROC: {}\\n\".format(metrics.roc_auc_score(model.predict(X_train), y_train)))\n\nprint(\"Validation Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_valid), y_valid)))\nprint(\"Validation Set ROC: {}\".format(metrics.roc_auc_score(model.predict(X_valid), y_valid)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e97fac7b316a5c034314a3bd26b18eb5ab56a5ff"},"cell_type":"code","source":"print(metrics.classification_report(model.predict(X_valid), y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d906a3e08acc5246d662e6b278835b20fafd8c2d"},"cell_type":"code","source":"# Confusion Matrix\nskplt.metrics.plot_confusion_matrix(model.predict(X_valid), y_valid, normalize=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66fd45ba7a62a3d799cc6122d162a5a767b36ab5"},"cell_type":"markdown","source":"## ELI5"},{"metadata":{"trusted":true,"_uuid":"e29872c6674dd3ad7be42894064194b19c47b0ce"},"cell_type":"code","source":"target_names = [\"Not Recommended\",\"Recommended\"]\neli5.show_weights(model, vec=vect, top=100,\n                  target_names=target_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"2dee41a529c13bf2ec71ad3aeb676973011c8828"},"cell_type":"code","source":"for iteration in range(15):\n    samp = random.randint(1,df.shape[0])\n    print(\"Real Label: {}\".format(df[\"Recommended IND\"].iloc[samp]))\n    display(eli5.show_prediction(model,df[\"Review Text\"].iloc[samp], vec=vect,\n                         target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6da7a06e990ab7299eac6024105903a7f760b070"},"cell_type":"markdown","source":"## LGBM and SHAP"},{"metadata":{"trusted":true,"_uuid":"6a296ea8b6f4523c7363080b0016c6a68c39e78a"},"cell_type":"code","source":"import lightgbm as lgb\n\nprint(\"Light Gradient Boosting Classifier: \")\nlgbm_params = {\n        \"objective\": \"binary\",\n        'metric': {'auc'},\n        \"boosting_type\": \"gbdt\",\n        \"num_threads\": 4,\n        \"bagging_fraction\": 0.8,\n        \"feature_fraction\": 0.8,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"min_split_gain\": .1,\n        \"reg_alpha\": .1\n    }\n\nmodelstart= time.time()\n# LGBM Dataset Formatting \nlgtrain = lgb.Dataset(X_train, y_train,\n                feature_name=vect.get_feature_names())\nlgvalid = lgb.Dataset(X_valid, y_valid,\n                feature_name=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f891f8b402602f7733cbec225527f079e0cd670c"},"cell_type":"code","source":"# Go Go Go\nlgb_clf = lgb.train(\n    lgbm_params,\n    lgtrain,\n    num_boost_round=2000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=['train','valid'],\n    early_stopping_rounds=150,\n    verbose_eval=100\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b40cc3aa0f98a6189bb7d3fe675a8678c27e8c1e"},"cell_type":"code","source":"valid_pred = lgb_clf.predict(X_valid)\n_thresh = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    _thresh.append([thresh, metrics.f1_score(y_valid, (valid_pred>thresh).astype(int))])\n#     print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(y_valid, (valid_pred>thresh).astype(int))))\n\n_thresh = np.array(_thresh)\nbest_id = _thresh[:,1].argmax()\nbest_thresh = _thresh[best_id][0]\nprint(\"Best Threshold: {}\\nF1 Score: {}\".format(best_thresh, _thresh[best_id][1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ba91ed6a618518608b77c18e44018a79cb47f3"},"cell_type":"markdown","source":"## SHAP"},{"metadata":{"trusted":true,"_uuid":"3110b362362048cfb34b7e90dcc0301420266761"},"cell_type":"code","source":"import shap\nshap.initjs()\n\nnon_sparse = pd.DataFrame(vect.transform(df['Review Text']).toarray(), columns = vect.get_feature_names())\nprint(non_sparse.shape)\n\nexplainer = shap.TreeExplainer(lgb_clf)\nshap_values = explainer.shap_values(non_sparse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f93c26199ef83117f19f5c5f255dd5c51c6f26ba"},"cell_type":"code","source":"# summarize the effects of all the features\nshap.summary_plot(shap_values, non_sparse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e00525da90717d9d042329723ffd0a2ff2011c8"},"cell_type":"code","source":"# visualize the first prediction's explanation\nfor iteration in range(15):\n    samp = random.randint(1,df.shape[0])\n    print(\"Real Label: {}\".format(df[\"Recommended IND\"].iloc[samp]))\n    display(df[\"Review Text\"].iloc[samp])\n    display(shap.force_plot(explainer.expected_value, shap_values[samp,:], non_sparse.iloc[samp,:]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0fcf226c276df2ec2c3e06c6594f5c5df98a1af"},"cell_type":"markdown","source":"***\n**Reflection:** <br>\nI want to summarize my findings and experiment with some more statistical test. I personally have a hard time understanding the nuance and application of some statistical test, so I would like to see how their findings mirror my multivariate plots. I believe that visualizations are great for low-dimensionality data, but more scalable statistical methods must be understood in order to deal with messier situations.\n\n**To Do:**\n- I want to do some unsupervised learning, and make use of [**Clustering Dendrogram**](https://python-graph-gallery.com/404-dendrogram-with-heat-map/).\n- Word Vectorization and Clustering.\n\n*- Nick*"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}