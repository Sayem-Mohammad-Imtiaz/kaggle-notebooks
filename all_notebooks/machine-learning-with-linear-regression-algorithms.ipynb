{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n![](https://res.akamaized.net/domain/image/upload/t_web/c_fill,w_600/v1554864563/6_New_Jersey_Road_Five_Dock_NSW_Low_res_tq758d.jpg)\n\n## Context\n\nThis is a Sydney House Prices dataset.\n\nThis data set contains information on the houses sold in Sydney between 2000 and 2019. In this study, deficient value operations, outliers and eventually regression analysis will be applied.\n\n## Content\n1. [Load and Check Data](#0)\n1. [Dataset Description](#1)\n1. [Data Visualization](#2)\n1. [Missing Value Analysis](#3)\n    * [Defining and Visualizing Missing Values](#4)\n    * [Testing the Randomness of Missing Values](#5)\n    * [Operations on Missing Values](#6)\n1. [Variable Transformation](#7)\n1. [Outlier Value Analysis](#8)\n    * [Outlier Value Detection Using Boxplot](#9)\n    * [Outlier Value Analysis With IQR](#10) \n1. [Machine Learning With Regression Algorithms](#11)","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport missingno as msno\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.preprocessing import scale\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\n\nimport warnings\nwarnings.filterwarnings(\"ignore\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\"></a>\n# Load and Check Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Load file","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"shouse = pd.read_csv(\"../input/sydney-house-prices/SydneyHousePrices.csv\")\ndf = shouse.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* First 5 records in the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# Dataset Description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* With the info() function, we can see the total number of variables in the data set, the types of these variables and the number of observations in the variables.\n* Our Dataset consists of 199504 rows and 9 columns.\n* The most missing value is seen in the sellPrice column.\n* Variables and types:\n    - float64(1):bed,car\n    - int64(2):  Id, postalCode, sellPrice, bath\n    - object(8):Date,suburb, propType","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * Dataset variable names.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variable Description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" 1. **Id:**:  A variable with no name and a variable specifying indexes will be deleted below because it is unnecessary.\n 1. **Date**: Sales dates of houses.\n 1. **suburb**: Suburban names in Australia.  \n 1. **propType**: The type of house.\n 1. **sellPrice**:  Prices of house.\n 1. **car**: No idea.\n 1. **postalCode**: Postal code.\n 1. **bed**: Number of bed.\n 1. **bath**: Number of bathrooms.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Unnecessary variable deletion.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"Id\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Check, delete successful. Our new number of variables is 8.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Statistical information about the dataset.\n    * You can access information such as means, medians, standard deviations, minimum and maximum values of numerical variables with the describe() function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(df.describe(),2).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The correlation of numerical variables is examined in the dataset. It is determined that there is a moderate relationship between the variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Using two types of plots:\n\n* Univariate plots to better understand each attribute.\n* Multivariate plots to better understand the relationships between attributes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Univariate Plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Univariate plots â€“ plots of each individual variable.\n* Given that the input variables are numeric, we can create box and whisker plots of each.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# box and whisker plots \nplt.subplot(2,1,1)\ndf[\"propType\"].value_counts().plot(kind='pie', title='PropType', autopct='%.1f%%', figsize=[20,20]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Creating histogram of each input variable to get an idea of the distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(4,1,1)\ndf.bed.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"bed Variable Histogram Chart\");\n\n\nplt.subplot(4,1,2)\ndf.sellPrice.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"sellPrice Variable Histogram Chart\");\n\n\nplt.subplot(4,1,3)\ndf.bath.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"bath Variable Histogram Chart\");\n\n\nplt.subplot(4,1,4)\ndf.car.plot(kind='hist',color='pink',bins=50,figsize=(15,15))\nplt.title(\"car Variable Histogram Chart\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate Plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.heatmap(df.corr(),annot=True,linewidth=2.5,fmt='.3F',linecolor='black');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The relationship between numerical variables is examined. It cannot be said that there is a linear relationship between price and point variables when the data are examined.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue = \"propType\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Used when the values in the dataset are missing.\nMissing values are generally NA.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Is there any missing value in the data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n## Defining and Visualizing Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Total missing values in the variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Looking at the graph, the data at the top shows the missing data in the variables. On the left, it shows the percentages in the dataset. On the right, it shows the number of observations in the dataset. At the bottom of the graph, there are variable names.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(df,color = sns.color_palette('deep'));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n## Testing the Randomness of Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* When the graphic is examined, there are observation information in the data set on the left part, variable names in the upper part and missing observations on the right part.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df, color = (0.1, 0.2, 0.3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heat maps are used to learn the relationships between variables. The values in this graph range from -1 to 1. If the value is 1 there is a correct relationship between the two variables, if the value is -1 there is a inverse relationship between the two variables. If the value is 0, there is no relationship between the two variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.heatmap(df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Missing value numbers and percentages.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_value_table(df):\n    missing_value = df.isna().sum().sort_values(ascending=False)\n    missing_value_percent = 100 * df.isna().sum()//len(df)\n    missing_value_table = pd.concat([missing_value, missing_value_percent], axis=1)\n    missing_value_table_return = missing_value_table.rename(columns = {0 : 'Missing Values', 1 : '% Value'})\n    cm = sns.light_palette(\"darkred\", as_cmap=True)\n    missing_value_table_return = missing_value_table_return.style.background_gradient(cmap=cm)\n    return missing_value_table_return\n  \nmissing_value_table(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n## Operations on Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Filling in the Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Missing values in the dataset are filled with the average of the variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['car'] = df['car'].fillna(df['car'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bed'] = df['bed'].fillna(df['bed'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Variable Transformation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* One Hot Encoding means that categorical variables are represented as binary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"propType\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['propType'] = pd.Categorical(df['propType'])\ndfDummies = pd.get_dummies(df['propType'], prefix = 'propType')\ndfDummies  \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* One of the propType values converted to One Hot Encoding is added to the data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, dfDummies[\"propType_house\"]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Date.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  The date variable is divided by year and month.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Date_ = pd.to_datetime(df['Date'])\ndf['Year'] = Date_.dt.year\ndf['Months'] = Date_.dt.month\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Categorical variables are deleted and the data set consists only of numerical values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"Date\",\"suburb\",\"propType\"],axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n# Outlier Value Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n## Outlier Value Detection Using Boxplot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In statistics, an outlier is a data point that differs significantly from other observations.\n\n* Outlier is smaller than Q1-1.5(Q3-Q1) and higher than Q3+1.5(Q3-Q1) .\n\n    * (Q3-Q1) = IQR (INTER QUARTILE RANGE)\n    * Q3 = Third Quartile(%75)\n    * Q1 = First Quartile(%25)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = df.drop([\"postalCode\",\"propType_house\",\"Year\",\"Months\"],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Outlier observation analysis would be unnecessary for variables deleted above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, col in enumerate(dff.columns):\n    plt.figure(i)\n    sns.boxplot(x=col, data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n## Outlier Value Analysis With IQR","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Train-test separation process for outliers observation analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[[\"sellPrice\"]]\nX = df.drop(\"sellPrice\", axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = X_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del columns[\"postalCode\"]\ndel columns[\"propType_house\"]\ndel columns[\"Year\"]\ndel columns[\"Months\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_and_upper = {} # storage\nX_train_copy = X_train.copy() # train copy \n\nfor col in columns.columns: # outlier detect\n    q1 = X_train[col].describe()[4] # Q1 = Quartile 1 median 25 \n    q3 = X_train[col].describe()[6] # Q3 = Quartile 3 median 75 \n    iqr = q3-q1  #IQR Q3 -Q1\n    \n    lower_bound = q1-(1.5*iqr)\n    upper_bound = q3+(1.5*iqr)\n    \n    lower_and_upper[col] = (lower_bound, upper_bound)\n    X_train_copy.loc[(X_train_copy.loc[:,col]<lower_bound),col]=lower_bound*0.75\n    X_train_copy.loc[(X_train_copy.loc[:,col]>upper_bound),col]=upper_bound*1.25\n    \nlower_and_upper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_copy = X_test.copy() # test copy   \n\nfor col in columns.columns:\n    X_test_copy.loc[(X_test_copy.loc[:,col]<lower_and_upper[col][0]),col]=lower_and_upper[col][0]*0.75\n    X_test_copy.loc[(X_test_copy.loc[:,col]>lower_and_upper[col][1]),col]=lower_and_upper[col][1]*1.25","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Contrary observations of the train set were cleared.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, col in enumerate(X_train_copy.columns):\n    plt.figure(i)\n    sns.boxplot(x=col, data=X_train_copy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Contrary observations of the test set were cleared.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, col in enumerate(X_test_copy.columns):\n    plt.figure(i)\n    sns.boxplot(x=col, data=X_test_copy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* For Target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y_test);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_and_upper = {} # storage\ny_train_copy = y_train.copy() # train copy \n\nfor col in y_train.columns: # outlier detect\n    q1 = y_train[col].describe()[4] # Q1 = Quartile 1 median 25 \n    q3 = y_train[col].describe()[6] # Q3 = Quartile 3 median 75 \n    iqr = q3-q1  #IQR Q3 -Q1\n    \n    lower_bound = q1-(1.5*iqr)\n    upper_bound = q3+(1.5*iqr)\n    \n    lower_and_upper[col] = (lower_bound, upper_bound)\n    y_train_copy.loc[(y_train_copy.loc[:,col]<lower_bound),col]=lower_bound*0.75\n    y_train_copy.loc[(y_train_copy.loc[:,col]>upper_bound),col]=upper_bound*1.25\n    \nlower_and_upper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_copy = y_test.copy() # test copy   \n\nfor col in y_test.columns:\n    y_test_copy.loc[(y_test_copy.loc[:,col]<lower_and_upper[col][0]),col]=lower_and_upper[col][0]*0.75\n    y_test_copy.loc[(y_test_copy.loc[:,col]>lower_and_upper[col][1]),col]=lower_and_upper[col][1]*1.25","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Contrary observations of the train set were cleared.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y_train_copy);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Contrary observations of the test set were cleared.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y_test_copy);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n# Machine Learning With Regression Algorithms","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Multiple Linear Regression\n* Principal Component Regression (PCR)\n* Partial Least Squares Regression (PLS)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train:\",X_train_copy.shape)\nprint(\"y_train:\",y_train_copy.shape)\nprint(\"X_test:\",X_test_copy.shape)\nprint(\"y_test:\",y_test_copy.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model with Sklearn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_model(X_reduced_train, y_train, X_reduced_test, y_test, X_train, X_test):\n    \n    #Setting up final models with the best values\n    pcr_final_model = LinearRegression().fit(X_reduced_train[:,0:6],y_train)\n    pls_final_model = PLSRegression(n_components = 7).fit(X_train, y_train)\n    multi_linear_final_model = LinearRegression().fit(X_train, y_train)\n    \n    #Forecasting operations with final models.\n    y_pred_pcr = pcr_final_model.predict(X_reduced_test[:,0:6])\n    y_pred_pls = pls_final_model.predict(X_test)\n        \n    print(\"corrected bug of pcr model:\",np.sqrt(mean_squared_error(y_test, y_pred_pcr)))\n    print(\"corrected bug of multi linear regression model:\",np.sqrt(-cross_val_score(multi_linear_final_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean())\n    print(\"corrected bug of pls model:\",np.sqrt(mean_squared_error(y_test, y_pred_pls)))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_tuning(X_reduced_train, x_train, y_train):\n    cv_10 = model_selection.KFold(n_splits =10, shuffle = True, random_state = 1)\n    \n    lm = LinearRegression()\n    RMSE_pcr = []\n    RMSE_pls = []\n\n    #The best parameters are found with cross validation.\n    for i in np.arange(1, X_reduced_train.shape[1] + 1):    \n        score1 = np.sqrt(-1*cross_val_score(lm, X_reduced_train[:,:i], y_train.values.ravel(), cv = cv_10, scoring = \"neg_mean_squared_error\").mean())\n        RMSE_pcr.append(score1)\n    \n    #The best parameters are found with cross validation.\n    for i in np.arange(1, X_train.shape[1] + 1):\n        pls = PLSRegression(n_components=i)\n        score2 = np.sqrt(-1*cross_val_score(pls,  x_train, y_train, cv = cv_10, scoring = \"neg_mean_squared_error\").mean())\n        RMSE_pls.append(score2)\n    \n    \n    fig, axs = plt.subplots(2,figsize=(10,10))\n    fig.suptitle('PCR / PLS Model Tuning For Price Prediction Model')\n    axs[0].plot(RMSE_pcr, '-v')\n    axs[1].plot(np.arange(1, X_train.shape[1]+1), np.array(RMSE_pls), '-v', c = \"r\")\n    axs[0].set_xlabel('Number of components')\n    axs[0].set_ylabel('RMSE')\n    axs[1].set_xlabel('Number of components')\n    axs[1].set_ylabel('RMSE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_predict(x_train ,y_train, x_test, y_test):\n    pca = PCA()\n    X_reduced_train = pca.fit_transform(scale(x_train)) #Conversion processes for pcr model x_train.\n    X_reduced_test = pca.fit_transform(scale(x_test)) #Conversion processes for pcr model x_test.\n    \n    \n    #Building a models\n    pcr_model = LinearRegression().fit(X_reduced_train, y_train)\n    reg_model = LinearRegression().fit(x_train, y_train)\n    pls_model = PLSRegression(n_components = 2).fit(x_train, y_train)\n    \n    #Predicted operations from created models.\n    y1_pred = pcr_model.predict(X_reduced_test)\n    y2_pred = reg_model.predict(x_test)\n    y3_pred = pls_model.predict(x_test)\n    \n    print(\"primitive error of pcr model:\", np.sqrt(mean_squared_error(y_test, y1_pred)))\n    print(\"primitive error of multiple linear regression model:\", np.sqrt(mean_squared_error(y_test, y2_pred)))\n    print(\"primitive error of pls model:\", np.sqrt(mean_squared_error(y_test, y3_pred)))\n    print(\"----------------------------------------------------------------------------------\")\n    \n    model_tuning(X_reduced_train, x_train, y_train)\n    final_model(X_reduced_train, y_train, X_reduced_test, y_test, X_train, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_predict(X_train_copy, y_train_copy, X_test_copy, y_test_copy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Ridge Regression\n* Lasso Regression\n* ElasticNet Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Ridge Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The aim is to find the coefficients that minimize the mean square error by applying a penalty to these coefficients.\n\nIn the Ridge model, there is an extra term known as the term punishment. The Î» given here is actually specified by the alpha parameter in the ridge function. That's why we basically control the penalty term by changing alpha values. The higher the alpha values, the greater the penalty, and therefore the size of the coefficients decreases.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"-Important points:\n* Reduces the parameters, so it is mostly used to prevent multiple connections.\n* It is resistant to over learning.\n* Reduces model complexity by coefficient shrinkage.\n* L2 uses regularization technique.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Model/Estimation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# A high alpha value means a higher constraint on the coefficients. Here we will experiment with three alpha values.\n#alfa = [0.00005,0.5,10] \nalfa = 10**np.linspace(10,-2,100)*0.5 \nCoef_=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('---------------')\nridge_model=Ridge()\nfor gÃ¼ncelalfa in alfa:\n    ridge_model.set_params(alpha=gÃ¼ncelalfa)\n    ridge_model.fit(X_train_copy,y_train_copy)\n    y_pred = ridge_model.predict(X_test_copy)\n    Coef_.append(ridge_model.coef_)\n    mse = np.mean((y_pred - y_test_copy)**2)\n    print('AlfasÄ± ' + str(gÃ¼ncelalfa) + ' olan Ridge regresyon modelin Train skoru: ',ridge_model.score(X_train_copy,y_train_copy))\n    print('AlfasÄ± ' + str(gÃ¼ncelalfa) + ' olan Ridge regresyon modelin Test skoru: ',ridge_model.score(X_test_copy,y_test_copy))\n    print('KullanÄ±lan Ã¶znitelik sayÄ±sÄ±: ',np.sum(ridge_model.coef_!=0))\n    print('Test HatasÄ± MSE: ', mse)\n    print('\\n')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coef_\nprint(Coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing the Ridge_cv model to find the Optimal Lamp.\nalfa[0:5]\nridge_cv=RidgeCV(alphas=alfa,scoring=\"neg_mean_squared_error\",normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_cv.fit(X_train_copy,y_train_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the Optimal lambda.\nridge_cv.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up Ridge regression model with optimal lambda value\nridge_tuned=Ridge(alpha=ridge_cv.alpha_,normalize=True).fit(X_train_copy,y_train_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean squared error values\nmse= np.sqrt(mean_squared_error(y_test_copy,ridge_tuned.predict(X_test_copy)))\nmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n\n* It has been proposed against the disadvantage of Ridge regression leaving all relevant / unrelated variables in the model.\n* Lasso approximates the coefficients to zero.\n* L1 form resets some coefficients when lambda is big enough. Therefore, it eliminates variables.\n* Ridge and lasso methods are not superior to each other.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Model/Estimation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('---------------')\nlasso_model=Lasso()\nfor gÃ¼ncelalfa in alfa:\n    lasso_model.set_params(alpha=gÃ¼ncelalfa)\n    lasso_model.fit(X_train_copy,y_train_copy)\n    y_pred = lasso_model.predict(X_test_copy)\n    Coef_.append(lasso_model.coef_)\n    mse= np.sqrt(mean_squared_error(y_test_copy,y_pred))\n    print('AlfasÄ± ' + str(gÃ¼ncelalfa) + ' olan Lasso regresyon modelin Train skoru: ',lasso_model.score(X_train_copy,y_train_copy))\n    print('AlfasÄ± ' + str(gÃ¼ncelalfa) + ' olan Lasso regresyon modelin Test skoru: ',lasso_model.score(X_test_copy,y_test_copy))\n    print('KullanÄ±lan Ã¶znitelik sayÄ±sÄ±: ',np.sum(lasso_model.coef_!=0))\n    print('Test HatasÄ± MSE: ', mse)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coef_\nprint(Coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing the Lasso_cv model to find the Optimal Lamp.\nlasso_cv_model=LassoCV(alphas=None,cv=10,max_iter=10000,normalize=True).fit(X_train_copy,y_train_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the Optimal lambda.\nlasso_cv_model.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up Lasso regression model with optimal lambda value\nlasso_tuned=Lasso(alpha=lasso_cv_model.alpha_).fit(X_train_copy,y_train_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean squared error values\nmse= np.sqrt(mean_squared_error(y_test_copy,lasso_tuned.predict(X_test_copy)))\nmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ElasticNet Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The aim is to find the coefficients that minimize the sum of error squares by applying a penalty score to these coefficients. ElasticNet combines L1 and L2 approaches.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Model/Estimation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"    elastikNet_model=ElasticNet().fit(X_train_copy,y_train_copy)\n    y_pred = elastikNet_model.predict(X_test_copy)\n    mse= np.sqrt(mean_squared_error(y_test_copy,y_pred))\n    r2=r2_score(y_test_copy,y_pred)\n    print('AlfasÄ± ' + str(gÃ¼ncelalfa) + ' olan ElasticNet regresyon modelin Train skoru: ',elastikNet_model.score(X_train_copy,y_train_copy))\n    print('AlfasÄ± ' + str(gÃ¼ncelalfa) + ' olan ElasticNet regresyon modelin Test skoru: ',elastikNet_model.score(X_test_copy,y_test_copy))\n    print('KullanÄ±lan Ã¶znitelik sayÄ±sÄ±: ',np.sum(elastikNet_model.coef_!=0))\n    print('Test HatasÄ± MSE: ', mse)\n    print('Test HatasÄ± R2 score: ', r2)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coef_\nelastikNet_model.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing the ElasticNet_cv model to find the Optimal Lamp.\nelastikNet_cv_model=ElasticNetCV(cv=10,random_state=0).fit(X_train_copy,y_train_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the Optimal lambda.\nelastikNet_cv_model.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up Elastic Net regression model with optimal lambda value\nelastikNet_tuned=ElasticNet(alpha=elastikNet_cv_model.alpha_).fit(X_train_copy,y_train_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean squared error values\nmse= np.sqrt(mean_squared_error(y_test_copy,elastikNet_tuned.predict(X_test_copy)))\nmse","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}