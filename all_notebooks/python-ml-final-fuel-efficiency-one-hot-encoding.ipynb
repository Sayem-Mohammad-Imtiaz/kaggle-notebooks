{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Python ML final _ Fuel efficiency prediction with One Hot Encoding\n \n **By 國立陽明交通大學 醫學三 陳冠元 10701054**\n\n Inspired by https://www.freecodecamp.org/news/how-to-build-your-first-neural-network-to-predict-house-prices-with-keras-f8db83049159/\n \n **Why I chose to do this project:**\n> It would be interesting to be able to predict the fuel efficiency of a vehicle just by looking at the specs and the categorical data of the vehicle. This is actually not very difficult for individuals that knows about cars (yep, I am sort of a automobile fan), but I think it would be interesting to see if the same \"educated guess\" could be made using Python deep learning.\n\n>I have seen projects that tried to analyze this dataset with regression moodels, but not one hot encoding\n\n>The dataset I used contained **7385 car models** produced in the past decade cataloged by the Canadian government.\n\n>>**The dataset: https://www.kaggle.com/debajyotipodder/co2-emission-by-vehicles**\n\n>I put the sources that helped me along the way below:","metadata":{"papermill":{"duration":0.026284,"end_time":"2021-06-20T16:33:20.23091","exception":false,"start_time":"2021-06-20T16:33:20.204626","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Set our Fuel Efficiency target\n\nAccording to Environmental Protection Agency of US, \naverage fuel efficiency of typical passenger vehicle is **10.69L/100km, or 9.35 km/L**\n\nhttps://www.epa.gov/greenvehicles/greenhouse-gas-emissions-typical-passenger-vehicle","metadata":{"papermill":{"duration":0.027733,"end_time":"2021-06-20T16:33:20.881012","exception":false,"start_time":"2021-06-20T16:33:20.853279","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Please input the target fuel efficiency below:\n# Accuracy and AUC of the model would vary given the different fuel efficiency goals.\ntarget_fuel_efficiency = 9.35\n#Unit:km/L","metadata":{"execution":{"iopub.status.busy":"2021-06-23T05:09:39.866514Z","iopub.execute_input":"2021-06-23T05:09:39.86691Z","iopub.status.idle":"2021-06-23T05:09:39.871244Z","shell.execute_reply.started":"2021-06-23T05:09:39.866875Z","shell.execute_reply":"2021-06-23T05:09:39.870079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Data\nImport the csv file to have a look about the datasets.","metadata":{"papermill":{"duration":0.025419,"end_time":"2021-06-20T16:33:20.35988","exception":false,"start_time":"2021-06-20T16:33:20.334461","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# These are the things imported by Kaggle by default...\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.051924,"end_time":"2021-06-20T16:33:20.308393","exception":false,"start_time":"2021-06-20T16:33:20.256469","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:39.875013Z","iopub.execute_input":"2021-06-23T05:09:39.875523Z","iopub.status.idle":"2021-06-23T05:09:39.892031Z","shell.execute_reply.started":"2021-06-23T05:09:39.875476Z","shell.execute_reply":"2021-06-23T05:09:39.890993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the dataset so we can take a look about the dataset we are dealing with...\ndata = pd.read_csv('../input/co2-emission-by-vehicles/CO2 Emissions_Canada.csv')\n\n# Print out the dataset, a gigantic csv file.\ndata\n\n# Model\n#    4WD/4X4 = Four-wheel drive\n#    AWD = All-wheel drive\n#    FFV = Flexible-fuel vehicle\n#    SWB = Short wheelbase\n#    LWB = Long wheelbase\n#    EWB = Extended wheelbase\n    \n# Transmission\n#    A = automatic\n#    AM = automated manual\n#    AS = automatic with select shift\n#    AV = continuously variable\n#    M = manual\n#    3 - 10 = Number of gears\n    \n# Fuel type\n#    X = regular gasoline\n#    Z = premium gasoline\n#    D = diesel\n#    E = ethanol (E85)\n#    N = natural gas\n    \n# Fuel consumption: City and highway fuel consumption ratings are shown in litres per 100 kilometres (L/100 km)\n# - the combined rating (55% city, 45% hwy) is shown in L/100 km and in miles per imperial gallon (mpg)\n# CO2 emissions: the tailpipe emissions of carbon dioxide (in grams per kilometre) for combined city and highway driving\n","metadata":{"papermill":{"duration":0.096867,"end_time":"2021-06-20T16:33:20.482713","exception":false,"start_time":"2021-06-20T16:33:20.385846","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:39.893465Z","iopub.execute_input":"2021-06-23T05:09:39.893975Z","iopub.status.idle":"2021-06-23T05:09:39.937159Z","shell.execute_reply.started":"2021-06-23T05:09:39.893933Z","shell.execute_reply":"2021-06-23T05:09:39.936221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing by removing columns\nMake and model of the vehicle is removed since it could overwhelm the neural network.\n\nOther fuel consumption data is removed due to the algebraic relationship between fuel consuption and CO2 emission.","metadata":{"papermill":{"duration":0.026349,"end_time":"2021-06-20T16:33:20.535833","exception":false,"start_time":"2021-06-20T16:33:20.509484","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Cannot quantify Make and Model of an automobile.\ndata.pop('Make')\ndata.pop('Model')\n\n# These four values have tight algebraic relationship with the predicted values.\ndata.pop('Fuel Consumption City (L/100 km)')\ndata.pop('Fuel Consumption Hwy (L/100 km)')\ndata.pop('Fuel Consumption Comb (mpg)')\ndata.pop('CO2 Emissions(g/km)')\n\n#I thought taking these categorical data would increase model accuracy but it did not.\n#data.pop('Cylinders')\n#data.pop('Transmission')\n#data.pop('Fuel Type')\n\n# Save the modified csv file as a new file.\ndata.to_csv('preprocessed_fuel_efficiency.csv', index=False)\npd.read_csv('preprocessed_fuel_efficiency.csv')\n\n# Make sure the columns are properly removed.\ndata","metadata":{"papermill":{"duration":0.075317,"end_time":"2021-06-20T16:33:20.638106","exception":false,"start_time":"2021-06-20T16:33:20.562789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:39.938634Z","iopub.execute_input":"2021-06-23T05:09:39.938943Z","iopub.status.idle":"2021-06-23T05:09:39.993278Z","shell.execute_reply.started":"2021-06-23T05:09:39.938908Z","shell.execute_reply":"2021-06-23T05:09:39.992488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing with Microsoft Excel\n\nMicrosoft Excel function for averaging a column according to the condition of another column:\n\nhttps://www.excelforum.com/excel-general/541631-calculate-average-in-a-column-based-on-criteria-in-another-column.html","metadata":{"papermill":{"duration":0.02817,"end_time":"2021-06-20T16:33:20.693501","exception":false,"start_time":"2021-06-20T16:33:20.665331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Read in the csv file that contain the information about the means of different groups.\nmeans_of_groups = pd.read_csv('../input/means-of-groups/means of groups.csv')\n\nMOG = np.ndarray.tolist(means_of_groups.values)\n# Convert the csv file into a numpy array\n# And then convert it into a list\n# I really don't know why but lists behave better in loops and appendings compared to numpy arrays.\n\nmeans_of_groups\n# What I really did is use Microsoft Excel to calculate the average CO2 emission according to the categorical data below:\n# I know this could be done with a loop and some appending, but Excel is just so much faster...\n# This csv file is crucial for the ONE HOT ENCODONG process performed below.\n# This also showed that categorical data can also be a clue when predicting fuel efficiency of automobiles.\n# For example, COMPACT < MID - SIZE < FULL - SIZE when it comes to CO2 emissions of different vehicles.","metadata":{"papermill":{"duration":0.049976,"end_time":"2021-06-20T16:33:20.825412","exception":false,"start_time":"2021-06-20T16:33:20.775436","status":"completed"},"tags":[],"scrolled":true,"execution":{"iopub.status.busy":"2021-06-23T05:09:39.994472Z","iopub.execute_input":"2021-06-23T05:09:39.99491Z","iopub.status.idle":"2021-06-23T05:09:40.011843Z","shell.execute_reply.started":"2021-06-23T05:09:39.99488Z","shell.execute_reply":"2021-06-23T05:09:40.010871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The new csv dataset with several columns kicked out is opened and converted to numpy array.\nnew_data = pd.read_csv(\"preprocessed_fuel_efficiency.csv\")\ndataset = new_data.values\n\n# Print out the array and see if it is converted correctly.\ndataset","metadata":{"papermill":{"duration":0.044424,"end_time":"2021-06-20T16:33:21.070538","exception":false,"start_time":"2021-06-20T16:33:21.026114","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:40.013014Z","iopub.execute_input":"2021-06-23T05:09:40.01331Z","iopub.status.idle":"2021-06-23T05:09:40.02943Z","shell.execute_reply.started":"2021-06-23T05:09:40.013283Z","shell.execute_reply":"2021-06-23T05:09:40.028084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One Hot Encoding\n\nhttps://www.kite.com/python/answers/how-to-append-to-a-2d-list-in-python\n\nhttps://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html\n\nhttps://www.w3schools.com/python/python_lists_remove.asp\n\nhttps://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/","metadata":{}},{"cell_type":"code","source":"# This part took me quite a lot of time to think of and to compose the loop.\n# I had the loop misbehaving and the result turned out to be the numpy array.\n# Therefore, I coverted the array to a list.\n# To prevent confusion, dataset is called datalist after the conversion.\ndatalist = np.ndarray.tolist(dataset)\n\n# I came up with this by myself but I have no programming background...\n#I really wonder if there is a solution more elegant.\n\n# 2 variables are required to operate the loop.\na,b=0,0\nfor a in range(len(datalist)):\n    b=0\n    for b in range(len(datalist)):\n        if b<16:\n            if MOG[b][0]==datalist[a][0]:# The categorical data is compared by the order of \"means of groups(MOG)\" list.\n                datalist[a].append(1)# If the contents of the two list match, a 1 is appended to \"datalist\".\n                del datalist[a][0]# After 1 is appended, the categorical data in \"datalist\" is no lunger usful thus deleted.\n            else:\n                datalist[a].append(0)# If the contents don't match, then a 0 is appended.\n        elif b<43: # Since the categorical data of the first column of \"dataset\" ends in the 15th row...\n            if MOG[b][0]==datalist[a][2]: # We can skip to the next column to do the rest of the comprarisons.\n                datalist[a].append(1)\n                del datalist[a][2]\n            else:\n                datalist[a].append(0)\n        elif b<48:\n            if MOG[b][0]==datalist[a][2]:\n                datalist[a].append(1)\n                del datalist[a][2]\n            else:\n                datalist[a].append(0)\n        else:b=b+1\n    a=a+1\n    \n# The results are not all printed otherwise the page will contain something really long without abbreviating...\n# But the first, middle, last row is printed to check if everything is okay.\n# I think these are the common places loops go wrong...\nprint(datalist[0])\nprint(datalist[int(round(len(datalist)/2, 0))])\nprint(datalist[-1]) # [-1] means the last row.","metadata":{"execution":{"iopub.status.busy":"2021-06-23T05:09:40.050107Z","iopub.execute_input":"2021-06-23T05:09:40.050455Z","iopub.status.idle":"2021-06-23T05:09:51.428862Z","shell.execute_reply.started":"2021-06-23T05:09:40.050424Z","shell.execute_reply":"2021-06-23T05:09:51.427803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting the fuel efficiency outcomes to binary\n\nhttps://stackoverflow.com/questions/18716564/python-cant-assign-to-literal\n\nhttps://www.guru99.com/variables-in-python.html\n\nhttps://www.geeksforgeeks.org/python-output-formatting/","metadata":{"papermill":{"duration":0.028476,"end_time":"2021-06-20T16:33:21.12779","exception":false,"start_time":"2021-06-20T16:33:21.099314","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#This loop simplifies the fuel efficiency to 0 = below average; 1 = above average. \nx,y=0,0\nfor x in range(len(datalist)):\n    if datalist[x][2] < (100/target_fuel_efficiency): # The units have to be converted from km/L to L/100km\n        datalist[x].append(1)\n        y=y+1\n    else:\n        datalist[x].append(0)\n    del datalist[x][2]\n    x=x+1\nz=y/x # Just to see the amount of automobile models that meet the fuel efficiency target.\n\n#Just to examine the percentage of car models that perform above average in fuel efficiency.\nprint('Approximately {} % of vehicle models meet the fuel efficiency requested.'.format(round(z*100, 2)))\nprint()","metadata":{"papermill":{"duration":1.892107,"end_time":"2021-06-20T16:33:23.048325","exception":false,"start_time":"2021-06-20T16:33:21.156218","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:51.430509Z","iopub.execute_input":"2021-06-23T05:09:51.430926Z","iopub.status.idle":"2021-06-23T05:09:51.442035Z","shell.execute_reply.started":"2021-06-23T05:09:51.430884Z","shell.execute_reply":"2021-06-23T05:09:51.441239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#See how the attribution information and fuel efficiency data is modified.\nprint(datalist[0])\nprint(datalist[int(round(len(datalist)/2, 0))])\nprint(datalist[-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T05:09:51.443366Z","iopub.execute_input":"2021-06-23T05:09:51.443901Z","iopub.status.idle":"2021-06-23T05:09:51.459856Z","shell.execute_reply.started":"2021-06-23T05:09:51.443863Z","shell.execute_reply":"2021-06-23T05:09:51.458711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The reason why I have to save the array back to csv file is that lists got rejected by the deep learning model.\n# Since numpy arrays are accepted and kaggle display csv files and numpy arrays better, I convert the datalist into csv then to numpy array.\n\n#Heading will be lost therefore it had to be added back manually.\ndataset_csv = np.asarray(datalist)\nnp.savetxt(\"preprocessed_fuel_efficiency.csv\", dataset_csv, delimiter=\",\", \n           header='Engine Size(L),Cylinders,COMPACT,FULL-SIZE,MID-SIZE,MINICOMPACT,MINIVAN,PICKUP TRUCK - SMALL,PICKUP TRUCK - STANDARD,SPECIAL PURPOSE VEHICLE,STATION WAGON - MID-SIZE,STATION WAGON - SMALL,SUBCOMPACT,SUV - SMALL,SUV - STANDARD,TWO-SEATER,VAN - CARGO,VAN - PASSENGER,A4,A5,A6,A7,A8,A9,A10,AM5,AM6,AM7,AM8,AM9,AS4,AS5,AS6,AS7,AS8,AS9,AS10,AV,AV6,AV7,AV8,AV10,M5,M6,M,D,E,N,X,Z,Above average fuel efficiency')\nnew_data = pd.read_csv(\"preprocessed_fuel_efficiency.csv\")\n\n#This is the dataset AFTER PREPROCESSING.\n# Categorical data is turned into binary.\n# Fuel efficiency outcomes are saved as binary also.\nnew_data","metadata":{"papermill":{"duration":0.112518,"end_time":"2021-06-20T16:33:23.180966","exception":false,"start_time":"2021-06-20T16:33:23.068448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:51.461436Z","iopub.execute_input":"2021-06-23T05:09:51.461724Z","iopub.status.idle":"2021-06-23T05:09:51.849547Z","shell.execute_reply.started":"2021-06-23T05:09:51.461696Z","shell.execute_reply":"2021-06-23T05:09:51.848465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the fuel efficiency results into a separated list.","metadata":{"papermill":{"duration":0.030096,"end_time":"2021-06-20T16:33:23.240969","exception":false,"start_time":"2021-06-20T16:33:23.210873","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Converting the preprocessed csv file to array...again.\ndataset = new_data.values\n\n#Splitting the results\nX = dataset[:,0:50] # These are the inputs that will be dumped into the deen learning model.\nY = dataset[:,50] # These are the answers for validation and testing.\n\nprint(X)\nprint(Y)\n\n# This way there is a dot after the every element, since the elements are registered as floats.\n# Therefore the data can be accepted by the model.","metadata":{"papermill":{"duration":0.038707,"end_time":"2021-06-20T16:33:23.309909","exception":false,"start_time":"2021-06-20T16:33:23.271202","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:51.85101Z","iopub.execute_input":"2021-06-23T05:09:51.851373Z","iopub.status.idle":"2021-06-23T05:09:51.859697Z","shell.execute_reply.started":"2021-06-23T05:09:51.851334Z","shell.execute_reply":"2021-06-23T05:09:51.858523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make the values of every column between 0 and 1 so the deep learning model can process it.\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)\n\n# See if the data is scaled properly\nX_scale","metadata":{"papermill":{"duration":1.035131,"end_time":"2021-06-20T16:33:24.375844","exception":false,"start_time":"2021-06-20T16:33:23.340713","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:51.860883Z","iopub.execute_input":"2021-06-23T05:09:51.861202Z","iopub.status.idle":"2021-06-23T05:09:51.878466Z","shell.execute_reply.started":"2021-06-23T05:09:51.861171Z","shell.execute_reply":"2021-06-23T05:09:51.877549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since N=7385, I think 10% for testing and validation is quite enough.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.1)\nX_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n\nprint(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)","metadata":{"papermill":{"duration":0.102706,"end_time":"2021-06-20T16:33:24.509338","exception":false,"start_time":"2021-06-20T16:33:24.406632","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:51.879923Z","iopub.execute_input":"2021-06-23T05:09:51.880203Z","iopub.status.idle":"2021-06-23T05:09:51.892677Z","shell.execute_reply.started":"2021-06-23T05:09:51.880175Z","shell.execute_reply":"2021-06-23T05:09:51.891518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Model\n\nhttps://keras.io/api/layers/activations/","metadata":{"papermill":{"duration":0.020624,"end_time":"2021-06-20T16:33:24.550885","exception":false,"start_time":"2021-06-20T16:33:24.530261","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense","metadata":{"papermill":{"duration":5.656401,"end_time":"2021-06-20T16:33:30.228137","exception":false,"start_time":"2021-06-20T16:33:24.571736","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:51.895378Z","iopub.execute_input":"2021-06-23T05:09:51.895773Z","iopub.status.idle":"2021-06-23T05:09:51.899874Z","shell.execute_reply.started":"2021-06-23T05:09:51.895737Z","shell.execute_reply":"2021-06-23T05:09:51.89891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5 inputs with 4 hidden neuron layers and 1 output.\nmodel = Sequential([\n    Dense(50, activation='relu', input_shape=(50,)),\n    Dense(64, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(10, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\n\n# Have a general idea of what the model is like.\nmodel.summary()","metadata":{"papermill":{"duration":0.155677,"end_time":"2021-06-20T16:33:30.415452","exception":false,"start_time":"2021-06-20T16:33:30.259775","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:11:42.093439Z","iopub.execute_input":"2021-06-23T05:11:42.093808Z","iopub.status.idle":"2021-06-23T05:11:42.142272Z","shell.execute_reply.started":"2021-06-23T05:11:42.093779Z","shell.execute_reply":"2021-06-23T05:11:42.140975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://keras.io/api/metrics/\n\nhttps://keras.io/api/losses/probabilistic_losses/\n\nhttps://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/","metadata":{}},{"cell_type":"code","source":"#adam optimizer works somehow works better than sgd.\nmodel.compile(\n    optimizer='adam',\n    loss='mse', # Works better than binary_crossentropy somehow...\n    metrics=['AUC','accuracy'])","metadata":{"papermill":{"duration":0.048724,"end_time":"2021-06-20T16:33:30.495746","exception":false,"start_time":"2021-06-20T16:33:30.447022","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:09:51.951755Z","iopub.execute_input":"2021-06-23T05:09:51.952041Z","iopub.status.idle":"2021-06-23T05:09:51.962863Z","shell.execute_reply.started":"2021-06-23T05:09:51.952007Z","shell.execute_reply":"2021-06-23T05:09:51.962023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I find 32 per batch is optimal and takes at least 100 epochs.\nhist = model.fit(X_train, Y_train,\n          batch_size=32, epochs=100,\n          validation_data=(X_val, Y_val))","metadata":{"papermill":{"duration":79.45177,"end_time":"2021-06-20T16:34:49.968414","exception":false,"start_time":"2021-06-20T16:33:30.516644","status":"completed"},"tags":[],"scrolled":true,"execution":{"iopub.status.busy":"2021-06-23T05:09:51.964368Z","iopub.execute_input":"2021-06-23T05:09:51.964673Z","iopub.status.idle":"2021-06-23T05:10:27.276914Z","shell.execute_reply.started":"2021-06-23T05:09:51.964623Z","shell.execute_reply":"2021-06-23T05:10:27.276191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\nThe model has an above 90% accuracy, I find it satsfactory since the categorical data is overwhelming.","metadata":{"papermill":{"duration":0.646927,"end_time":"2021-06-20T16:34:51.263536","exception":false,"start_time":"2021-06-20T16:34:50.616609","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.evaluate(X_test, Y_test)[1]\nprint(X_test)\nprint(Y_test)","metadata":{"papermill":{"duration":0.732472,"end_time":"2021-06-20T16:34:52.6433","exception":false,"start_time":"2021-06-20T16:34:51.910828","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:10:27.278345Z","iopub.execute_input":"2021-06-23T05:10:27.27881Z","iopub.status.idle":"2021-06-23T05:10:27.342556Z","shell.execute_reply.started":"2021-06-23T05:10:27.278779Z","shell.execute_reply":"2021-06-23T05:10:27.341487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Error report\n\nThe vehicles that the model did not make the correct prediction is displayed and saved as a csv file.\n\nhttps://www.geeksforgeeks.org/python-save-list-to-csv/","metadata":{}},{"cell_type":"code","source":"# First, convery the numpy array into a list\nprediction = np.ndarray.tolist(np.round(model.predict(X_test),0))\nX_test_list = np.ndarray.tolist(X_test)\nconverted=[]\n\n# This loop with 4 variables has two functions:\n## 1.Rebuild the categorical data based on the one hot encoding results.\n## 2.Check if the accuracy reported from the trained model is correct.\ni,j,k,l=0,0,0,0\nfor i in range(len(Y_test)):\n    if Y_test[i]==prediction[i][0]:\n        j=j+1\n    else:\n        converted.append([int(i)])\n        for l in range(len(MOG)):\n            if l in range(16):\n                if int(X_test[i][l+2])==1:\n                    converted[k].append(str(MOG[l][0]))\n                    converted[k].append(X_test_list[i][0])\n                    converted[k].append(X_test_list[i][1])\n            elif l in range(43):\n                if int(X_test[i][l+2])==1:\n                    converted[k].append(str(MOG[l][0]))\n            elif l in range(48):\n                if int(X_test[i][l+2])==1:\n                    converted[k].append(str(MOG[l][0]))\n                    converted[k].append(int(Y_test[i]))\n                    converted[k].append(int(np.round(model.predict(X_test),0)[i]))\n            l=l+1\n        k=k+1\n    i=i+1\n\n\n# This tool is handy when it comes to saving list to csv file.\nimport csv\n  \nfields = ['X_test index', 'Vehicle class', 'Engine size', 'Cylinders', 'Transmission', 'Fuel type', 'Y_test', 'MODEL PREDICTION'] \n    \nrows = converted # data rows of csv file \n  \nwith open('error_report.csv', 'w') as f:\n      \n    write = csv.writer(f)# using csv.writer method from CSV package\n      \n    write.writerow(fields)\n    write.writerows(rows)\n\nerror_report_csv = pd.read_csv(\"error_report.csv\")\n\n# Make sure the accuracy of the model is properly displayed\nprint('confirmed accuracy:',round(j/i, 4))\nprint()\nerror_report_csv","metadata":{"execution":{"iopub.status.busy":"2021-06-23T05:10:27.343753Z","iopub.execute_input":"2021-06-23T05:10:27.344065Z","iopub.status.idle":"2021-06-23T05:10:28.490646Z","shell.execute_reply.started":"2021-06-23T05:10:27.344037Z","shell.execute_reply":"2021-06-23T05:10:28.48978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Over estimation and Under estimation of the vehicle's fuel efficiency\n\nhttps://www.geeksforgeeks.org/python-ways-to-remove-duplicates-from-list/\n\nhttps://www.geeksforgeeks.org/python-list-sort/","metadata":{}},{"cell_type":"code","source":"# The original dataset is reloaded.\ndata2 = pd.read_csv('../input/co2-emission-by-vehicles/CO2 Emissions_Canada.csv')\ndatalist2 = np.ndarray.tolist(data2.values)\n\nunderestimate_efficiency = []\noverestimate_efficiency = []\n\n# If all the elements match, the index from the original dataset will be appended.\np,q=0,0\nfor p in range(len(converted)):\n    for q in range(len(datalist2)):\n        if converted[p][1]==datalist2[q][2] and converted[p][2]==datalist2[q][3] and (converted)[p][3]==datalist2[q][4] and converted[p][4]==datalist2[q][5] and converted[p][5]==datalist2[q][6]: \n            if (converted[p][7]==0 and datalist2[q][9]<(100/target_fuel_efficiency)): \n                underestimate_efficiency.append(q)\n            elif (converted[p][7]==1 and datalist2[q][9]>(100/target_fuel_efficiency)):\n                overestimate_efficiency.append(q)\n        q=q+1\n    p=p+1\n\nunderestimate_efficiency2 = []\nfor i in underestimate_efficiency:\n    if i not in underestimate_efficiency2:\n        underestimate_efficiency2.append(i)\n        \noverestimate_efficiency2 = []\nfor i in overestimate_efficiency:\n    if i not in overestimate_efficiency2:\n        overestimate_efficiency2.append(i)\n\n# The results are sorted in ascending order.        \nunderestimate_efficiency2.sort()\noverestimate_efficiency2.sort()\n        \nprint('Models that had its fuel efficiency underestimated: ')\nprint(underestimate_efficiency2)\nprint()\nprint('Models that had its fuel efficiency overestimated: ') \nprint(overestimate_efficiency2)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T05:10:28.492136Z","iopub.execute_input":"2021-06-23T05:10:28.492516Z","iopub.status.idle":"2021-06-23T05:10:28.564821Z","shell.execute_reply.started":"2021-06-23T05:10:28.492476Z","shell.execute_reply":"2021-06-23T05:10:28.563901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fuel efficiency underestimated","metadata":{}},{"cell_type":"code","source":"underestimate_error = []\n\na=0\nfor a in range(len(underestimate_efficiency2)):\n    A= underestimate_efficiency2[a]\n    underestimate_error.append(datalist2[A])\n    a=a+1\n\nimport csv\n  \nfields = ['Make','Model','Vehicle Class','Engine Size(L)','Cylinders','Transmission','Fuel Type','Fuel Consumption City (L/100 km)','Fuel Consumption Hwy (L/100 km)','Fuel Consumption Comb (L/100 km)','Fuel Consumption Comb (mpg)','CO2 Emissions(g/km)'] \nrows = underestimate_error \n  \nwith open('underestimate_error.csv', 'w') as f:  \n    write = csv.writer(f) \n    write.writerow(fields)\n    write.writerows(rows)\n\nunderestimate_error_csv = pd.read_csv(\"underestimate_error.csv\")\nunderestimate_error_csv","metadata":{"execution":{"iopub.status.busy":"2021-06-23T05:10:28.566042Z","iopub.execute_input":"2021-06-23T05:10:28.566399Z","iopub.status.idle":"2021-06-23T05:10:28.598766Z","shell.execute_reply.started":"2021-06-23T05:10:28.56636Z","shell.execute_reply":"2021-06-23T05:10:28.597783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fuel efficiency overestimated","metadata":{}},{"cell_type":"code","source":"overestimate_error = []\n\nb=0\nfor b in range(len(overestimate_efficiency2)):\n    B=overestimate_efficiency2[b]\n    overestimate_error.append(datalist2[B])\n    b=b+1\n\nimport csv\n   \nfields = ['Make','Model','Vehicle Class','Engine Size(L)','Cylinders','Transmission','Fuel Type','Fuel Consumption City (L/100 km)','Fuel Consumption Hwy (L/100 km)','Fuel Consumption Comb (L/100 km)','Fuel Consumption Comb (mpg)','CO2 Emissions(g/km)']    \nrows = overestimate_error \n  \nwith open('overestimate_error.csv', 'w') as f:\n    write = csv.writer(f)\n    write.writerow(fields)\n    write.writerows(rows)\n\noverestimate_error_csv = pd.read_csv(\"overestimate_error.csv\")\noverestimate_error_csv","metadata":{"execution":{"iopub.status.busy":"2021-06-23T05:10:28.599979Z","iopub.execute_input":"2021-06-23T05:10:28.600248Z","iopub.status.idle":"2021-06-23T05:10:28.642797Z","shell.execute_reply.started":"2021-06-23T05:10:28.600222Z","shell.execute_reply":"2021-06-23T05:10:28.642087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of the model","metadata":{"papermill":{"duration":0.430444,"end_time":"2021-06-20T16:34:53.506833","exception":false,"start_time":"2021-06-20T16:34:53.076389","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#just plot the stuff above out.\nimport matplotlib.pyplot as plt\nplt.plot(hist.history['val_loss'])\nplt.plot(hist.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Val', 'Train'], loc='upper right')\nplt.show()","metadata":{"papermill":{"duration":0.60131,"end_time":"2021-06-20T16:34:54.533842","exception":false,"start_time":"2021-06-20T16:34:53.932532","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:10:28.643807Z","iopub.execute_input":"2021-06-23T05:10:28.644201Z","iopub.status.idle":"2021-06-23T05:10:28.774582Z","shell.execute_reply.started":"2021-06-23T05:10:28.644162Z","shell.execute_reply":"2021-06-23T05:10:28.773546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist.history['val_auc'])\nplt.plot(hist.history['auc'])\nplt.plot(hist.history['val_accuracy'])\nplt.plot(hist.history['accuracy'])\nplt.title('Model AUC and Accuracy')\nplt.ylabel('AUC and Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Val AUC', 'Train AUC','Val Accuracy', 'Train Accuracy'], loc='lower right')\nplt.show()","metadata":{"papermill":{"duration":0.604126,"end_time":"2021-06-20T16:34:55.558049","exception":false,"start_time":"2021-06-20T16:34:54.953923","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-23T05:10:28.775957Z","iopub.execute_input":"2021-06-23T05:10:28.776231Z","iopub.status.idle":"2021-06-23T05:10:28.941544Z","shell.execute_reply.started":"2021-06-23T05:10:28.776205Z","shell.execute_reply":"2021-06-23T05:10:28.94045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# My previous attempt to deal with categorical data:\n\n**Link to my previous project:** https://www.kaggle.com/galenchen/python-ml-final-fuel-efficiency-prediction\n\n>Basically, I **took the average of the CO2 emissions of different categories**. Then **replacing the categorical data with the means** I calculated.\n\nAs a result: With similar deep learning model,\n\n>With **one hot encoding**, the accuracy is about **93%** (loss = **mean_square_error** works better)\n\n>With **means of each category**, the accuracy is about **87%** (loss = **binary_crossentropy** works pooply)\n\n**In conclusion, one hot encoding is more accurate when it comes to predicting the fuel efficiency of automobiles.**","metadata":{}}]}