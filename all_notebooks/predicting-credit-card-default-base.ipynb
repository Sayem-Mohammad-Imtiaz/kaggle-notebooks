{"cells":[{"metadata":{},"cell_type":"markdown","source":"- The notebook has been created as a solution to a handson training session, introduction to machine learning\n- There are other notebooks with parts of this notebook converted to fill-in-the-blanks that trainees can use during the session"},{"metadata":{},"cell_type":"markdown","source":"# Business Problem and Solution Framework"},{"metadata":{},"cell_type":"markdown","source":"1. **Business Problem** To predict which credit card accounts are expected to default in their payments next month\n2. **Available Data**\n    1. Geography: Taiwan \n    2. Duration: April 2005 - September 2005\n    3. Source: UCI Machine Learning Repostiory https://archive.ics.uci.edu/ml/\n    4. Contents of Data: Information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients\n4. **Success Criteria** \n    1. Accuracy of prediction"},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{},"cell_type":"markdown","source":"### Kaggle Data Location"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Library Imports"},{"metadata":{},"cell_type":"markdown","source":"Python has a suite of libraries that are aimed at data science. Few commonly used libraries of python are:\n* pandas : meant for tabular data manipulation and analysis library\n* numpy : meant for linear algebra operations\n* scikit-learn : meant for machine learning and statistial models\n* matplotlib and seaborn : meant for visualizations and graphs\n* nltk : meant for natural language processing\n\n**Import the necessary libraries for our notebook**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the tabular data manipulation and analysis library: pandas\nimport pandas as pd \n\n# import thel linear algebra library: numpy\nimport numpy as np\n\n# import machine learning model library: sklearn\nimport sklearn as sk\n\n# import visualization libraries: matplotlib, seaborn\nfrom matplotlib import pyplot as plt \nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Import and Understanding"},{"metadata":{},"cell_type":"markdown","source":"- Before we begin the model development exercise we need to understand the data available, its quantity and its quality\n- Load the data and try answering the following questions that form the first impression of the data\n    1. Understand how big the data is, number of rows, number of columns, size in MBs/GBs/TBs\n    1. What is the target variable ?\n    1. What attributes are available in the data to predict the target ?"},{"metadata":{},"cell_type":"markdown","source":"### Load the data"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"- We will use pandas and it's functions to load and maintain data in the notebook\n- Read the data into a pandas dataframe using read_csv() function of pandas\n    - pd.read_csv(*file_to_read*)\n    - location of file: */kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv*\n    - Returns: Pandas dataframe holding the data\n- You can view top n-rows of the data in a DataFrame using the function head(n)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the variable into a pandas DataFrame\nfulldata=pd.read_csv('/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a peek at top-10 rows of data using the head function of dataframe\nfulldata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- pandas has a limit on number of columns and rows it shows without truncation. We can alter the limit by setting the variables:\n    - pd.options.display.max_columns\n    - pd.options.display.max_rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns=50 # increase the number of columns displayed without truncation\npd.options.display.max_rows=999 # increase the number of rows displayed without truncation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a peek at top-10 rows of data using the head function of the dataframe\nfulldata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Understanding"},{"metadata":{},"cell_type":"markdown","source":"- Q. What the target variable in the data ?\n    - A. default.payment.next.month\n- Q. What are the attribute variables in the data ?\n    - A. Everything else except the column ID\n- Q. How can one describe a row in the above dataset ?\n    - A. A row represents an account along with its demographic attributes, payment history, default status in the next month"},{"metadata":{},"cell_type":"markdown","source":"**P1. What is the size of the data ?**\n- Input: Raw Data\n- Output:\n    - Number of rows\n    - Number of columns\n    - *Number of files*\n    - *Size of files*\n\nshape attribute of DataFrame holds the number of rows and columns in a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use shape attribute of DataFrame object to obtain size\nfulldata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**P2. What information is available in the data ?**\n- Input: Raw Data\n- Output: Detailed understanding of features and rows of the data"},{"metadata":{},"cell_type":"markdown","source":"**Model development requires careful selection of attributes**\n1. Adding irrelevant attributes act as noise and requires an effort from the model to learn to ignore. They also cost computation.\n2. Dropping relevant attributes make it difficult for model to learn to make the correct prediction\n3. Further, incorrectly chosen attributes, sometimes (in case of *Target Leakage*), can give false impression of excellent model performance\n\nThough automated techniques exist for feature selection they are not robust against all the issues above. Hence, the first step is always gaining an understanding of attributes available and the values they take.This requires close collaboration with domain experts.\n\nSince we are using a public dataset we have a well defined data dictionary for the dataset"},{"metadata":{},"cell_type":"markdown","source":"**Attribute Descripition**\n- **ID**\n    - ID: ID of each client\n- **Numeric Variables**\n    - LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n    - BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n    - BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n    - BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n    - BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n    - BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n    - BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n    - PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n    - PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n    - PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n    - PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n    - PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n    - PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n- **Ordinal Variables**\n    - PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n    - PAY_2: Repayment status in August, 2005 (scale same as above)\n    - PAY_3: Repayment status in July, 2005 (scale same as above)\n    - PAY_4: Repayment status in June, 2005 (scale same as above)\n    - PAY_5: Repayment status in May, 2005 (scale same as above)\n    - PAY_6: Repayment status in April, 2005 (scale same as above)\n- **Categorical Variables**\n    - SEX: Gender (1=male, 2=female)\n    - EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n    - MARRIAGE: Marital status (1=married, 2=single, 3=others)\n    - AGE: Age in years\n- **Target / Label**\n    - default.payment.next.month: Default payment (1=yes, 0=no)"},{"metadata":{},"cell_type":"markdown","source":"**Todo**\n- Education has 2 values that are unknown => merge the two values\n- Categorical attributes have been defined using numeric values => ensure pandas treats categorical attributes as categorical\n- ID column is not a useful attribute and should be used as the index for rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"fulldata.loc[fulldata.EDUCATION==6,'EDUCATION']=5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**P3. Ensure correctness the data type for each attribute**\n\nGiven some attributes look like numbers but are categories we need to ensure that pandas treats them as categorical attributes and not numbers"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Check the current dtype\nfulldata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correct the dtypes for categorical variables\nfulldata['ID']=fulldata['ID'].astype(object)\nfulldata['SEX']=fulldata['SEX'].astype(object)\nfulldata['EDUCATION']=fulldata['EDUCATION'].astype(object)\nfulldata['MARRIAGE']=fulldata['MARRIAGE'].astype(object)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Re-check the current dtype\nfulldata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert ID as the index for the dataframe\nfulldata=fulldata.set_index('ID')\nfulldata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Detailed Attribute Analysis"},{"metadata":{},"cell_type":"markdown","source":"Detailed attribute analysis allows one to get a detailed picture of dataset. It helps discover:\n1. Extent of missing values in each columns\n1. Range of categorical values each attribute takes along with frequency of each\n1. Average values of numeric attributes\n1. Anomalous/extreme/outlier values present in data\n1. Deviations, if any, from the data dictionary\n1. Irrelevant attributes(e.g. attributes that take only one value)\n1. Distribution of target labels"},{"metadata":{},"cell_type":"markdown","source":"**P4. Study attribute value distribution**\n\n***Numerical Attributes***"},{"metadata":{},"cell_type":"markdown","source":"describe() function of pandas DataFrame gets a summary of all numeric attributes. It can be pushed to get summary for non-numeric attributes too but that part is not comprehensive so we don't use it that often.\n\nUse describe() function below to get profile summary of numeric attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use describe() function of pandas DataFrame to get a summary of all numeric attributes. Use a .T at the end of the function call to make the output more readable\nfulldata.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**\n* No. of columns having one or more missing value is 0\n* % of cases having default is 22%\n* median age in the dataset is 34 while minimum is 21"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Use KDE PLOT to get detailed distribution for each attribute\nfor aCol in fulldata.columns:\n    if fulldata[aCol].dtype==object:\n        continue\n    print('Column:',aCol)\n    sns.kdeplot(fulldata[aCol],shade=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Categorical Attribute***"},{"metadata":{},"cell_type":"markdown","source":"value_counts() function of pandas DataFrame column(Pandas Series) allows to get frequency distribution for categorical variables. Check it out in action below"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Use _value_counts() to plot values using histogram\nfor aCol in fulldata.columns:\n    if fulldata[aCol].dtype==object:\n        if aCol=='ID':\n            continue\n        print(aCol)\n        print('----------------------------')\n#         plt.figure(figsize=(15,5))\n        sns.barplot(fulldata[aCol].value_counts().index,fulldata[aCol].value_counts())\n        plt.show()\n        print(fulldata[aCol].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n- What is odd about attribute values for marriage and education, compared to data dictionaries\n    - Unexpected value 0 occurring for marriage and education. It is not present in the dataset\n    - The 0 value can be assumed to represent *unknown*"},{"metadata":{},"cell_type":"markdown","source":"# Data Split: Training data and Test data"},{"metadata":{},"cell_type":"markdown","source":"**How do we measure model performance**\n- During training we want the model to learn just the right level of rules for classification\n- If model learns more detailed rules than necessary, it will make correct predictions on existing data but incorrect predictions on new data\n- If model learns less detailed rules than necessary, it will make incorrect predicitons on existing data as well as new data\n\n**Held Out Data Sample Setup**\n- To evaluate if the model has learnt the right level of rules:\n    - we hide part of labeled data, called test data, from training process\n    - train the model on remaining available data called training data\n    - evaluate model performance on training data\n    - evaluate model performance on test data\n    - ensure that performance is as high as possible on test data while being similar to performance on training data\n\n\nTypical Ratio of training data and test data size: 8:2"},{"metadata":{},"cell_type":"markdown","source":"**Split the read data into training and test data points in the ratio 8:2**\n- Function train_test_split in sklearn.model_selection package provides a functionality to split training and test data sets\n- Typically different runs of training and test data split give different results due to randomization\n- To ensure that we get same results with each randomization, we provide a specific random_state value to the train_test_split function"},{"metadata":{"trusted":true},"cell_type":"code","source":"rseed=11 # ensures reproducibility of results, detailed later","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split # helps split the data into multiple components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into X and y\nfullX=fulldata.iloc[:,:-1]\nfully=fulldata.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use train validation test split using command from sklearn to split into Train, Validation and Test\ntrainX,testX,trainy,testy=train_test_split(fullX,fully,random_state=rseed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Creation"},{"metadata":{},"cell_type":"markdown","source":"- Since algorithms expect everything to be numeric, we need to modify the categorical variables to numeric columns before feeding them to the model\n- One-hot encoding is a way to convert categorical variables to numeric columns\n\n**Converting all categorical variables to numeric variables using an object of OneHotEncoder**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# convert categorical to one hot\ncatCols=[]\ni=-1\nfor aCol in trainX.columns:\n    i+=1\n    if trainX[aCol].dtype != object:\n        continue\n    catCols.append(i)\n    print(aCol)\nprint('Categorical Features:',catCols)\nohe=sk.preprocessing.OneHotEncoder(categorical_features=catCols)\nohe=ohe.fit(trainX)\ntrainX2=pd.DataFrame(ohe.transform(trainX).toarray())\ntestX2=pd.DataFrame(ohe.transform(testX).toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking what trainX2 looks like\ntrainX2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# values identified\nohe.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing with initial dataframe\ntrainX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With all attribute columns being numeric we can now proceed towards training a classification model."},{"metadata":{},"cell_type":"markdown","source":"# Train Model"},{"metadata":{},"cell_type":"markdown","source":"- There are many models that can be used for classification, like, Logistic Regression, Decision Trees and Neural Networks\n- Various models differ in what types of rules they can create to do classification and how they discovered those rules from the data\n- We will try Decision Trees for classifying our data in this exercise\n- Decison trees create a rules in form of a tree like flowchart\n- Decision trees are available in sklearn.tree library in form of class DecisionTreeClassifier\n\n**Create a decision tree classifier and train it using training data prepared above**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import decision tree module from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\n\n# create a DecisiopnTreeClassifier() object\nmodel=DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Once we have the classifier object, sklearn allows us to train the classifier by using the .fit() function\n- .fit() function takes in training data features, X, and training data labels, y as parameters\n\n**Use .fit() function of the model to train classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the fit function on the model to train the model using training data\nmodel.fit(trainX2,trainy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- once trained .predict() function of the classifier can be used to make predictions\n\n**Use .predict() function to get predictions on training and test data sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the predict function on training and test data to come up with training data predictions\ntrainp=model.predict(trainX2)\ntestp=model.predict(testX2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Performance Evaluation"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"- Once predictions are available, model performance can be evaluated using functions like accuracy_score() from sklearn.metrics module\n\n### Metric: Accuracy\n\n**Estimate the accuracy score for training and test datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('training dataset accuracy:',sk.metrics.accuracy_score(trainy,trainp))\nprint('test dataset accuracy:',sk.metrics.accuracy_score(testy,testp))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metric: Confusion Matrix\n\n**Plot the confusion matrix for training and test datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TRAINING DATA')\nplt.figure(figsize=(4,4))\nsns.heatmap(sk.metrics.confusion_matrix(trainy,trainp),annot=True,fmt='d',linewidths=0.5,annot_kws={'size':20})\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TESTING DATA')\nplt.figure(figsize=(4,4))\nsns.heatmap(sk.metrics.confusion_matrix(testy,testp),annot=True,fmt='d',linewidths=0.5,annot_kws={'size':20})\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}