{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Attack Prediction using Deep Learning\n","metadata":{}},{"cell_type":"markdown","source":"## Importing necessary Libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import (layers, optimizers,losses)\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import regularizers\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler,MaxAbsScaler\nfrom tensorflow.keras import callbacks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### Reading CSV file into pandas Dataframe","metadata":{}},{"cell_type":"code","source":"heart=pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Overview of Dataframe","metadata":{}},{"cell_type":"code","source":"heart.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checkinng for any null values","metadata":{}},{"cell_type":"code","source":"heart.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There is no null values","metadata":{}},{"cell_type":"markdown","source":"## Creating X,Y feature sets for classification","metadata":{}},{"cell_type":"code","source":"x = heart.drop(\"output\",axis=1)\ny = heart[\"output\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the dataset into Train and Test sets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x,y, train_size=0.8, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Glance at the head of Train sets","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Normalizing the values using Maximum Absolute Scaler","metadata":{}},{"cell_type":"code","source":"scaler = MaxAbsScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Neural Network for classification","metadata":{}},{"cell_type":"markdown","source":"## Model Arcitechture\n\n   \n   ### In this task we used a neural network composing of these following layers, activation functions and optmizer and loss function:\n\n* Input Layer : Takes input  \n* Dense Layer : Dense layer is the regular deeply connected neural network layer.\n* BatchNormalization Layer : Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n* The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged\n* Activation function: An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.\n\n* Regularization :  Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the model's performance on the unseen data as well.\n\n\n\n### Activation Function : \n  \n * ReLU :The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero,\n * Sigmoid : Sigmoid activation function, sigmoid(x) = 1 / (1 + exp(-x)).\n    Applies the sigmoid activation function. For small values (<-5), sigmoid returns a value close to zero, and for large values (>5) the result of the function gets close to 1.\n \n### Regularization :\n * L1 regulazier : In L1 norm we shrink the parameters to zero.\n \n### Adam Optimizer : \n * Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n \n### SGD Optimizer : \n  * Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression.\n \n### Binary Crossentropy Loss: \n * Cross-entropy is the default loss function to use for binary classification problems.\n   It is intended for use with binary classification where the target values are in the set {0, 1}.","metadata":{}},{"cell_type":"code","source":"inputs = layers.Input(shape = (13,), name = 'Input_layer')\ndense_1 = layers.Dense(16, kernel_regularizer = regularizers.l1(1e-2),activation = 'relu', name = 'Dense1')(inputs)\nbatch_norm1 = layers.BatchNormalization( name = 'Batch_norm1')(dense_1)\ndropout_1 = layers.Dropout(0.2 , name = 'Dropout1')(batch_norm1)\n\ndense_2 = layers.Dense(32, kernel_regularizer = regularizers.l1(1e-2),activation = 'relu', name = 'Dense2')(dropout_1)\nbatch_norm2 = layers.BatchNormalization(name = 'Batch_norm2')(dense_2)\ndropout_2 = layers.Dropout(0.2, name = 'Dropout2')(batch_norm2)\n\ndense_3 = layers.Dense(64, kernel_regularizer = regularizers.l1(1e-2),activation = 'relu', name = 'Dense3')(dropout_2)\nbatch_norm3 = layers.BatchNormalization(name = 'Batch_norm3')(dense_3)\ndropout_3 = layers.Dropout(0.2, name = 'Dropout3')(batch_norm3)\n\ndense_3 = layers.Dense(128, kernel_regularizer = regularizers.l1(1e-3),activation = 'relu', name = 'Dense4')(dropout_3)\noutputs = layers.Dense(1, activation = 'sigmoid', name = 'Output_layer')(dense_3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Selecting optimizer, loss function, metrics and callbacks","metadata":{}},{"cell_type":"code","source":"optimizer = optimizers.Adam(lr = 0.01)\nloss = losses.BinaryCrossentropy()\nmetrics = 'accuracy'\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,patience=6, min_lr=1e-4, verbose = True)\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=True,mode='auto')\ncallbacks = [reduce_lr]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Making the NN Model using Tensorflow's functional API","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Model(inputs = inputs, outputs = outputs, name='heart_attack_prediction')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Compiling the model","metadata":{}},{"cell_type":"code","source":"model.compile(loss = loss, optimizer = optimizer, metrics = metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Overview of the Model (including total parameters)","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training the model with fixed number of epochs","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train,y_train, shuffle = True, epochs = 300, validation_data=(X_test,y_test),verbose =2, callbacks = callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting Accuracy vs Epochs and Loss vs Epochs Curve","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20, 6))\nax = ax.ravel()\n\nfor i, met in enumerate([ \"accuracy\", \"loss\"]):\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history['val_'+met])\n    ax[i].set_title(\"Model {}\".format(met))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(met)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\n\n\n  ### We used metrics from Scikit learn to evaluate the performance of our model\n  \n  ### Performance Criteria:\n  \n  * Balanced Accuracy Score\n  * F1 Score\n  * Precision Score\n  * Recall Score\n \n\n### We plot the Confusion Metrics, AUC Curve","metadata":{}},{"cell_type":"code","source":"def get_metrics(train_ds, train_y):\n\n    train_y = np.asarray(train_y)\n    a = model.predict(train_ds)\n    prediction = []\n    for i in range(len(a)):\n        if a[i] >= 0.5:\n            prediction.append(1)\n        else:\n            prediction.append(0)\n    train_y = train_y.flatten()\n    \n    \n    bal_acc = sklearn.metrics.balanced_accuracy_score(train_y, prediction)\n    f1_score = sklearn.metrics.f1_score(train_y,prediction,average='macro')\n    pre = sklearn.metrics.precision_score(train_y,prediction,average='macro')\n    rec = recall_score(train_y,prediction,average='macro')\n    confusion = confusion_matrix(train_y, prediction)\n    class_rep = classification_report(train_y,prediction)\n    \n    \n    \n    print('\\n')\n    print('Balanced accuracy =',bal_acc)\n    print('F1 score = ',f1_score)\n    print('Precision =',pre)\n    print('Recall =',rec)\n    print('Classification Report =\\n',class_rep)\n    print('Confusion Matrix =\\n',confusion)\n    print('\\n')\n    \n    fig, ax = plt.subplots(figsize=(12, 12))\n    \n    display_con = sklearn.metrics.ConfusionMatrixDisplay(confusion)\n    \n    display_con.plot(ax=ax)\n    ax.set_title('Confusion Matrix')\n    \n    \n    fpr, tpr, thresholds = sklearn.metrics.roc_curve(train_y,prediction)\n    roc_auc = sklearn.metrics.auc(fpr, tpr)\n    display_roc = sklearn.metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n    fig, ax = plt.subplots(figsize=(10, 6))\n    display_roc.plot(ax=ax)\n    ax.set_title('AUC Curve')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation on Train Dataset","metadata":{}},{"cell_type":"code","source":"get_metrics(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation on Test Dataset","metadata":{}},{"cell_type":"code","source":"get_metrics(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"### **Here we tried to use Neural Network for binary classification.  Neural Networks work best when there is a lot of data. In this scenerio, the dataset was relatively small. But we acheived a quite a good model with some Hyperparameter tunings like using L1 regularization, using several optimizers(AdamW, RMSprop, SGD) with various learning rate. We used callbacks to stop overfitting our model. We also used some normalization technique like Batch normalization, Dropout to prevent overfitting as Neural Nets are very much prone to overfit when the dataset is quite small.**","metadata":{}}]}