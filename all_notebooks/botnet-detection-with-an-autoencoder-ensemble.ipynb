{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Botnet Detection with an Autoencoder Ensemble\n21 June 2021  \nThis notebook was created for a course at Istanbul Technical University.\n\nIt is a continuation of our [preliminary experiments](https://www.kaggle.com/happyemoji/botnet-detection-with-an-autoencoder).\n- We implement (a simplified version of) the autoencoder-ensemble-based anomaly detection described in the Kitsune paper [1].","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nfrom scipy.cluster.hierarchy import dendrogram, to_tree\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras import layers, losses, Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:45:08.886278Z","iopub.execute_input":"2021-06-28T16:45:08.88684Z","iopub.status.idle":"2021-06-28T16:45:14.917533Z","shell.execute_reply.started":"2021-06-28T16:45:08.886747Z","shell.execute_reply":"2021-06-28T16:45:14.916633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n1. Dataset\n2. Autoencoder Ensemble\n3. Conclusion  \n4. References\n---\n# 1. Dataset\nN-BaIoT [2]  \nhttps://archive.ics.uci.edu/ml/datasets/detection_of_IoT_botnet_attacks_N_BaIoT\n- Nine IoT devices were used.\n- The devices were 2 smart doorbells, 1 smart thermostat, 1 smart babymonitor, 4 security cameras and 1 webcam.\n- Traffic was captured when the devices were in normal execution and after infection with malware.\n- Mirai and BashLite (aka gafgyt) malware were used.\n- From the network traffic, 115 features were extracted as described in [1].","metadata":{}},{"cell_type":"code","source":"def load_nbaiot(filename):\n    return np.genfromtxt(\n        os.path.join(\"/kaggle/input/nbaiot-dataset\", filename),\n        delimiter=\",\",\n        skip_header=1\n    )\n\nbenign = load_nbaiot(\"1.benign.csv\")\nX_train = benign[:40000]\nX_test0 = benign[40000:]\nX_test1 = load_nbaiot(\"1.mirai.scan.csv\")\nX_test2 = load_nbaiot(\"1.mirai.ack.csv\")\nX_test3 = load_nbaiot(\"1.mirai.syn.csv\")\nX_test4 = load_nbaiot(\"1.mirai.udp.csv\")\nX_test5 = load_nbaiot(\"1.mirai.udpplain.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:45:14.919231Z","iopub.execute_input":"2021-06-28T16:45:14.919631Z","iopub.status.idle":"2021-06-28T16:46:56.071148Z","shell.execute_reply.started":"2021-06-28T16:45:14.919588Z","shell.execute_reply":"2021-06-28T16:46:56.07029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, X_test0.shape, X_test1.shape, X_test2.shape,\n      X_test3.shape, X_test4.shape, X_test5.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:46:56.072466Z","iopub.execute_input":"2021-06-28T16:46:56.072683Z","iopub.status.idle":"2021-06-28T16:46:56.07829Z","shell.execute_reply.started":"2021-06-28T16:46:56.072661Z","shell.execute_reply":"2021-06-28T16:46:56.077269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# 2. Autoencoder Ensemble\nThe paper [1] describes Kitsune, an ensemble of autoencoders for network intrusion detection. Using an ensemble instead of one big autoencoder -- as was presented in [5] -- helps against the curse of dimensionality. So training and execution are computationally more efficient. Source code for a Python implementation is provided at https://github.com/ymirsky/Kitsune-py.\n\nAt the start, a number of samples are analyzed to group the features into disjoint subsets. This is done by calculating the pairwise correlation distances between all features of these samples. The subsets are selected with agglomerative hierarchical clustering. So each subset is a group of features that are highly inter-correlated. The maximum size of a subset is a parameter of the algorithm.\n\nAfter that, an autoencoder is created for each subset of features. These autoencoders from the ensemble layer. Each autoencoder sees only its subset of features of each sample. The reconstruction errors of all ensemble members form the input for the final autoencoder. That is, it learns how well the ensemble can reconstruct the training data. The final autoencoder's own reconstruction error is used for the output, i.e., surpassing a given threshold is reported as an anomaly.\n\nWe reimplement a basic version of this in the sequel.\n\n## Clustering the Features\nFor the clustering phase, a part of the training data is split off.\n\nWe use `scikit-learn`'s Agglomerative Clustering. Building clusters with a given maximum size requires re-creating the linking matrix \"manually\".","metadata":{}},{"cell_type":"code","source":"def agglomerative_clustering(data):\n    # sqrt makes this a proper distance metric\n    correlation_distance = np.sqrt(1-np.corrcoef(data.T))\n    ac = AgglomerativeClustering(\n        n_clusters=None,\n        affinity=\"precomputed\",\n        linkage=\"single\",\n        distance_threshold=0\n    )\n    ac.fit(correlation_distance)\n    return ac\n\nfeature_mapping_phase = 7777\nac = agglomerative_clustering(X_train[:feature_mapping_phase])","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:46:56.079493Z","iopub.execute_input":"2021-06-28T16:46:56.079729Z","iopub.status.idle":"2021-06-28T16:46:56.107273Z","shell.execute_reply.started":"2021-06-28T16:46:56.079705Z","shell.execute_reply":"2021-06-28T16:46:56.106084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, to create the linkage matrix, we follow the tutorial at https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html.","metadata":{}},{"cell_type":"code","source":"def linkage_matrix(model):\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    return np.column_stack([model.children_, model.distances_, counts]).astype(float)\n\nlm = linkage_matrix(ac)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:46:56.108713Z","iopub.execute_input":"2021-06-28T16:46:56.109089Z","iopub.status.idle":"2021-06-28T16:46:56.117758Z","shell.execute_reply.started":"2021-06-28T16:46:56.10905Z","shell.execute_reply":"2021-06-28T16:46:56.116725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndendrogram(lm)\nplt.close(\"all\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:46:56.119462Z","iopub.execute_input":"2021-06-28T16:46:56.120016Z","iopub.status.idle":"2021-06-28T16:46:56.35578Z","shell.execute_reply.started":"2021-06-28T16:46:56.119972Z","shell.execute_reply":"2021-06-28T16:46:56.35485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the algorithm described in the Kitsune paper, we split the dendrogram until no cluster is larger than a given size.","metadata":{}},{"cell_type":"code","source":"def find_subsets(tree, max_cluster_size=10):\n    if tree.count <= max_cluster_size:\n        return [np.array(tree.pre_order())]\n    recursion1 = find_subsets(tree.get_left(), max_cluster_size)\n    recursion2 = find_subsets(tree.get_right(), max_cluster_size)\n    return recursion1+recursion2\n    \nsubsets = find_subsets(to_tree(lm))\n\nsubsets","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:46:56.356888Z","iopub.execute_input":"2021-06-28T16:46:56.357143Z","iopub.status.idle":"2021-06-28T16:46:56.369236Z","shell.execute_reply.started":"2021-06-28T16:46:56.357119Z","shell.execute_reply":"2021-06-28T16:46:56.368173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Autoencoder Ensemble for Anomaly Detection\nHere we implement the ensemble as outlined above. Details are on pages 8 and 9 in the Kitsune paper [1].","metadata":{}},{"cell_type":"code","source":"class Autoencoder(Model):\n    def __init__(self, n):\n        super(Autoencoder, self).__init__()\n        self.encoder = Sequential([\n            layers.Dense(n, activation=\"relu\"),\n            layers.Dense(int(0.75*n), activation=\"relu\"),\n        ])\n        self.decoder = layers.Dense(n, activation=\"relu\")\n    \n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\ndef compile_and_train(ae, x):\n    ae.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n    ae.fit(\n        x=x,\n        y=x,\n        # in reality, it is supposed to be an online algorithm, so\n        # we make only 1 pass over the training data\n        epochs=1\n    )\n\nclass Ensemble:\n    def __init__(self, feature_subsets):\n        self.map = feature_subsets\n        self.scaler_ensemble = MinMaxScaler()\n        self.scaler_output = MinMaxScaler()\n        self.ensemble_layer = []\n        for subset in feature_subsets:\n            ae = Autoencoder(len(subset))\n            self.ensemble_layer += [ae]\n        self.output_layer = Autoencoder(len(feature_subsets))\n        \n    def train(self, data):\n        scaled = self.scaler_ensemble.fit_transform(data)\n        loss_ensemble = []\n        \n        for i, (features, ae) in enumerate(zip(self.map, self.ensemble_layer)):\n            x = scaled[:, features]\n            print(f\"##**~~__ Autoencoder {i+1}/{len(self.map)} for {len(features)} dimensions\")\n            compile_and_train(ae, x)\n            loss_ensemble += [losses.mse(x, ae(x))]\n            \n        # Because of the above loop, loss_ensemble now has shape\n        # (n_autoencoders, n_samples). But for the output layer, the previous\n        # layer outputs are actually treated as features. Therefore transpose\n        loss_ensemble = self.scaler_output.fit_transform(np.array(loss_ensemble).T)\n        print(f\"##**~~__ Output Autoencoder for {loss_ensemble.shape[1]} dimensions\")\n        compile_and_train(self.output_layer, loss_ensemble)\n        loss_out = losses.mse(loss_ensemble, self.output_layer(loss_ensemble))\n        self.threshold = np.mean(loss_out)+np.std(loss_out)\n    \n    def predict(self, data):\n        scaled = self.scaler_ensemble.transform(data)\n        loss_ensemble = []\n        \n        for features, ae in zip(self.map, self.ensemble_layer):\n            x = scaled[:, features]\n            loss_ensemble += [losses.mse(x, ae(x))]\n            \n        loss_ensemble = self.scaler_output.transform(np.array(loss_ensemble).T)\n        loss_out = losses.mse(loss_ensemble, self.output_layer(loss_ensemble))\n        return loss_out > self.threshold","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:46:56.371021Z","iopub.execute_input":"2021-06-28T16:46:56.371273Z","iopub.status.idle":"2021-06-28T16:46:56.385186Z","shell.execute_reply.started":"2021-06-28T16:46:56.371247Z","shell.execute_reply":"2021-06-28T16:46:56.384229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble = Ensemble(subsets)\nensemble.train(X_train[feature_mapping_phase:])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-28T16:46:56.388161Z","iopub.execute_input":"2021-06-28T16:46:56.388427Z","iopub.status.idle":"2021-06-28T16:47:28.059044Z","shell.execute_reply.started":"2021-06-28T16:46:56.388401Z","shell.execute_reply":"2021-06-28T16:47:28.058096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = [X_test0, X_test1, X_test2, X_test3, X_test4, X_test5]\n\nfor i, x in enumerate(test_data):\n    print(i)\n    print(f\"Shape of data: {x.shape}\")\n    outcome = ensemble.predict(x)\n    print(f\"Detected anomalies: {np.mean(outcome)*100}%\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T16:47:28.060094Z","iopub.execute_input":"2021-06-28T16:47:28.060334Z","iopub.status.idle":"2021-06-28T16:47:31.091858Z","shell.execute_reply.started":"2021-06-28T16:47:28.06031Z","shell.execute_reply":"2021-06-28T16:47:31.090912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# 3. Conclusion\n\nIndeed, the results are almost exactly the same as they are with one big autoencoder, see [our example](https://www.kaggle.com/happyemoji/botnet-detection-with-an-autoencoder).\n\nThe following will be our next steps.\n\n### Reduced Feature Set\nIn [2], it is suggested to use a subset of 23 features instead of all 115. However, different -- supervised -- algorithms were used. Are these 23 features enough also for the unsupervised anomaly detection system?\n\n- Compare performance of the full and the reduced feature set using\n    - a shallow autoencoder,\n    - a deep autoencoder and\n    - an ensemble of autoencoders.\n\n### Datasets for Comparison\nMedBIoT [2]  \nhttps://cs.taltech.ee/research/data/medbiot/\n- Three real and 80 simulated IoT devices were used.\n- The real devices were 2 smart switches and 1 smart light bulb.\n- Traffic was captured when the devices were in normal execution and after infection with malware.\n- Mirai, BashLite, and Torii malware were used.\n- There is an unprocessed version of the dataset (pcap files) and one where 115 features were extracted as described in [1].\n\nIoT-23 [3]  \nhttps://www.stratosphereips.org/datasets-iot23\n- Three real IoT devices and a Raspberry Pi were used.\n- The real devices were 1 smart light bulb, 1 smart doorbell and 1 smart speaker / virtual assistant.\n- Traffic was captured when the IoT devices were in normal execution and when the Raspberry Pi was infected with malware.\n- Eleven malware families -- including Mirai, Torii and Gagfyt (aka BashLite) -- were used across 20 different captures.\n\n`pcap` files are provided, so the feature extraction has to be performed before comparing with the other datasets.\n\n### Algorithmic Improvements\nHave there been any advances on anomaly detection with autoencoders which can be incorporated? For example, can we use Variational Autoencoders for better performance?\n\n# References\n[1] Mirsky, Yisroel et al. \"Kitsune: An Ensemble of Autoencoders for Online Network Intrusion Detection\", NDSS (2018). https://arxiv.org/abs/1802.09089v2  \n[2] Alhowaide, Alaa, et al. \"Towards the design of real-time autonomous IoT NIDS.\" Cluster Computing (2021): 1-14. https://doi.org/10.1007/s10586-021-03231-5  \n[3] Guerra-Manzanares, Alejandro, et al. \"MedBIoT: Generation of an IoT Botnet Dataset in a Medium-sized IoT Network.\" ICISSP 1 (2020): 207-218. https://doi.org/10.5220/0009187802070218  \n[4] Meidan, Yair, et al. \"N-BaIoT—Network-based Detection of IoT Botnet Attacks Using Deep Autoencoders.\" IEEE Pervasive Computing 17.3 (2018): 12-22. https://arxiv.org/abs/1805.03409  \n[5] Garcia, Sebastian et al. \"IoT-23: A labeled dataset with malicious and benign IoT network traffic\" (2020). (Version 1.0.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.4743746","metadata":{}}]}