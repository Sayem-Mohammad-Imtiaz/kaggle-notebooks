{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nfrom scipy.stats import reciprocal\nfrom sklearn import preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.ensemble import StackingClassifier\n\ndef data_process(data):\n    data = data.drop(\"Cabin\", 1)\n    data = data.drop(\"Embarked\", 1)\n    data = data.drop(\"Ticket\",1)\n    data = data.drop(\"Name\", 1)\n    data = data.drop(\"PassengerId\", 1)\n    data[\"Sex\"] = LabelEncoder().fit_transform(data[\"Sex\"])\n    \n    numerical_attr = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"]\n    \n    for attr in numerical_attr:\n        data[attr].fillna(round(data[attr].mean(), 0), inplace=True)\n    return data\ntrain_data = data_process(train_data)\n\nsss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n\ntraining_set = []\nvalidation_set = []\n\nfor train_index, test_index in sss1.split(train_data, train_data[\"Survived\"]):\n    training_set = train_data.loc[train_index]\n    validation_set = train_data.loc[test_index]\n\ntraining_features = training_set.drop(\"Survived\", 1).to_numpy()\ntraining_labels = training_set.drop([\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"], 1).to_numpy().ravel()\n\nvalidation_features = validation_set.drop(\"Survived\", 1).to_numpy()\nvalidation_labels = validation_set.drop([\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"], 1).to_numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression Modeling \nlogistic_reg_baseline = LogisticRegression(solver = 'liblinear', random_state = 1)\nlogistic_reg_baseline.fit(training_features, training_labels)\nlog_reg_baseline_train_score = logistic_reg_baseline.score(training_features, training_labels)\nprint(\"Training Score for Baseline Model:\", log_reg_baseline_train_score)\n\nlog_reg_baseline_validation_score = logistic_reg_baseline.score(validation_features, validation_labels)\nprint(\"Validation Score for Baseline Model:\", log_reg_baseline_validation_score)\n\n# Using GridSearchCV to fine-tune the Logistic Regression model\nmax_iter_logistic_reg = [i*10 for i in range(10, 31)]\nc_logistic_reg = [i/100 for i in range (1,20)]\nparameters_logistic_reg = {'C' : c_logistic_reg, 'max_iter' : max_iter_logistic_reg, 'penalty' : ['l1', 'l2']}\n\nlogistic_reg_best = GridSearchCV(logistic_reg_baseline, parameters_logistic_reg)\nlogistic_reg_best.fit(training_features, training_labels)\nlogistic_reg_finetune = logistic_reg_best.best_estimator_\nlogistic_reg_finetune.fit(training_features, training_labels)\nlog_reg_finetune_train_score = logistic_reg_finetune.score(training_features, training_labels)\nprint(\"Training Score for Finetuned Model:\", log_reg_finetune_train_score)\n\nlog_reg_finetune_validation_score = logistic_reg_finetune.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", log_reg_finetune_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Modeling \nrandomforest_baseline = RandomForestClassifier(random_state = 1)\nrandomforest_baseline.fit(training_features, training_labels)\nrandomforest_baseline_train_score = randomforest_baseline.score(training_features, training_labels)\nprint(\"Training Score for Baseline Model:\", randomforest_baseline_train_score)\n\nrandomforest_baseline_validation_score = randomforest_baseline.score(validation_features, validation_labels)\nprint(\"Validation Score for Baseline Model:\", randomforest_baseline_validation_score)\n\n# Using GridSearchCV to fine-tune the Random Forest model\nn_estimators_randomforest = [i*10 for i in range(10,21)]\nmax_features = ['sqrt', 'log2']\nmax_depth_randomforest = range(2, 6)\nparameters_randomforest = {'n_estimators' : n_estimators_randomforest, 'max_features' : max_features, 'max_depth' : max_depth_randomforest}\n\nrandomforest_best = GridSearchCV(randomforest_baseline, parameters_randomforest)\nrandomforest_best.fit(training_features, training_labels)\nrandomforest_finetune = randomforest_best.best_estimator_\n\nrandomforest_finetune.fit(training_features, training_labels)\nrandomforest_finetune_train_score = randomforest_finetune.score(training_features, training_labels)\nprint(\"Training Score for Finetuned Model:\", randomforest_finetune_train_score)\n\nrandomforest_finetune_validation_score = randomforest_finetune.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", randomforest_finetune_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting Classifier Modeling \ngradboosting_baseline = GradientBoostingClassifier(random_state = 1)\ngradboosting_baseline.fit(training_features, training_labels)\ngradboosting_baseline_train_score = gradboosting_baseline.score(training_features, training_labels)\nprint(\"Training Score for Baseline Model:\", gradboosting_baseline_train_score)\n\ngradboosting_baseline_validation_score = gradboosting_baseline.score(validation_features, validation_labels)\nprint(\"Validation Score for Baseline Model:\", gradboosting_baseline_validation_score)\n\n# # Using GridSearchCV to fine-tune the Gradient Boosting model\nlearning_rate_gradboosting = [i/100 for i in range(5, 21)]\nn_estimators_gradboosting = [i*10 for i in range(10,21)]\nmax_depth_gradboosting = range(3,6)\nparameters_gradboosting = {'learning_rate' : learning_rate_gradboosting, 'n_estimators' : n_estimators_gradboosting, 'max_depth' : max_depth_gradboosting}\n\ngradboosting_best = GridSearchCV(gradboosting_baseline, parameters_gradboosting)\ngradboosting_best.fit(training_features, training_labels)\ngradboosting_finetune = gradboosting_best.best_estimator_\ngradboosting_finetune.fit(training_features, training_labels)\ngradboosting_finetune_train_score = gradboosting_finetune.score(training_features, training_labels)\nprint(\"Training Score for Finetuned Model:\", gradboosting_finetune_train_score)\n\ngradboosting_finetune_validation_score = gradboosting_finetune.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", gradboosting_finetune_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM Classifier Modeling \nsvm_baseline = SVC(random_state = 1)\nsvm_baseline.fit(training_features, training_labels)\nsvm_baseline_train_score = svm_baseline.score(training_features, training_labels)\nprint(\"Training Score for Baseline Model:\", svm_baseline_train_score)\n\nsvm_baseline_validation_score = svm_baseline.score(validation_features, validation_labels)\nprint(\"Validation Score for Baseline Model:\", svm_baseline_validation_score)\n\n# Using GridSearchCV to fine-tune the SVC model\nkernel_svm = ['poly', 'rbf', 'sigmoid']\ndegree_svm = range(3,10)\nparameters_svm = {'kernel' : kernel_svm, 'degree' : degree_svm}\nsvm_best = GridSearchCV(svm_baseline, parameters_svm)\nsvm_best.fit(training_features, training_labels)\nsvm_finetune = svm_best.best_estimator_\nsvm_finetune.fit(training_features, training_labels)\nsvm_finetune_train_score = svm_finetune.score(training_features, training_labels)\nprint(\"Training Score for Finetuned Model:\", svm_finetune_train_score)\n\nsvm_finetune_validation_score = svm_finetune.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", svm_finetune_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoostClassifier Modeling\nadaboost_baseline = AdaBoostClassifier(random_state = 1)\nadaboost_baseline.fit(training_features, training_labels)\nadaboost_baseline_train_score = adaboost_baseline.score(training_features, training_labels)\nprint(\"Training Score for Baseline Model :\", adaboost_baseline_train_score)\n\nadaboost_baseline_validation_score = adaboost_baseline.score(validation_features, validation_labels)\nprint(\"Validation Score for Baseline Model :\", adaboost_baseline_validation_score)\n\n# Using GridSearchCV to fine-tune the AdaBoost model\nlearning_rate_adaboost = [i/10 for i in range(1, 11)]\nn_estimators_adaboost = range(50, 101)\nparameters_adaboost = {'n_estimators' : n_estimators_adaboost, 'learning_rate' : learning_rate_adaboost}\n\nadaboost_best = GridSearchCV(adaboost_baseline, parameters_adaboost)\nadaboost_best.fit(training_features, training_labels)\nadaboost_finetune = adaboost_best.best_estimator_\nadaboost_finetune.fit(training_features, training_labels)\nadaboost_finetune_train_score = adaboost_finetune.score(training_features, training_labels)\nprint(\"Training Score for Finetuned Model:\", adaboost_finetune_train_score)\n\nadaboost_finetune_validation_score = adaboost_finetune.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", adaboost_finetune_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extratree_baseline = ExtraTreeClassifier(random_state = 1)\nextratree_baseline.fit(training_features, training_labels)\nextratree_baseline_train_score = extratree_baseline.score(training_features, training_labels)\nprint(\"Training Score for Baseline Model :\", extratree_baseline_train_score)\n\nextratree_baseline_validation_score = extratree_baseline.score(validation_features, validation_labels)\nprint(\"Validation Score for the Baseline Model :\", extratree_baseline_validation_score)\n\n# Using GridSearchCV to fine-tune the ExtraTreeClassifier model\ncriterion_extratree = ['gini', 'entropy']\nsplitter_extratree = ['random']\nmax_depth_extratree = [None, 3, 5, 7, 11]\nparameters_extratree = {'criterion' : criterion_extratree, 'splitter' : splitter_extratree, 'max_depth' : max_depth_extratree}\nextratree_best = GridSearchCV(extratree_baseline, parameters_extratree)\nextratree_best.fit(training_features, training_labels)\nextratree_finetune = extratree_best.best_estimator_\nextratree_finetune.fit(training_features, training_labels)\nextratree_finetune_train_score = extratree_finetune.score(training_features, training_labels)\nprint(\"Training Score for Finetuned Model:\", extratree_finetune_train_score)\n\nextratree_finetune_validation_score = extratree_finetune.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", extratree_finetune_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Voting Classifier attempt\nvoting_clf = VotingClassifier(estimators = [('rf', randomforest_finetune), ('gb', gradboosting_finetune),('lg', logistic_reg_finetune), ('ab', adaboost_finetune), ('xtree', extratree_finetune)], voting = 'hard')\nvoting_clf.fit(training_features, training_labels)\nvoting_clf_training_score = voting_clf.score(training_features, training_labels)\nprint(\"Training Score for the Voting Classifier :\", voting_clf_training_score)\nvoting_clf_validation_score = voting_clf.score(validation_features, validation_labels)\nprint(\"Validation Score for the Voting Classifier :\", voting_clf_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic Stacking Classifier attempt\nestimators_stacking = [('rf', randomforest_finetune), ('gb', gradboosting_finetune),('lg', logistic_reg_finetune), ('ab', adaboost_finetune), ('xtree', extratree_finetune)]\nfinal_estimator = [LogisticRegression(random_state = 1)]\nparameters_stacking = {'final_estimator' : final_estimator}\nstacking_clf = StackingClassifier(estimators_stacking, LogisticRegression(random_state = 1))\nstacking_best = GridSearchCV(stacking_clf, parameters_stacking)\nstacking_best.fit(training_features, training_labels)\nstacking_finetune = stacking_best.best_estimator_\n\nstacking_finetune.fit(training_features, training_labels)\nstacking_finetune_train_score = stacking_finetune.score(training_features, training_labels)\nprint(\"Training Score for Finetuned Model:\", stacking_finetune_train_score)\n\nstacking_finetune_validation_score = stacking_finetune.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", stacking_finetune_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multilayer Stacking Classifier attempt\nlevel1_train_set = []\nlevel2_train_set = []\n\nsss2 = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state=0)\n\nfor index1, index2 in sss2.split(training_set, training_set[\"Survived\"]):\n    level1_train_set = train_data.loc[index1]\n    level2_train_set = train_data.loc[index2]\n\nlevel1_training_features = level1_train_set.drop(\"Survived\", 1).to_numpy()\nlevel1_training_labels = level1_train_set.drop([\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"], 1).to_numpy().ravel()\n\nlevel2_training_features = level2_train_set.drop(\"Survived\", 1).to_numpy()\nlevel2_training_labels = level2_train_set.drop([\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"], 1).to_numpy().ravel()\n\nlevel1_estimator1 = StackingClassifier(estimators_stacking, LogisticRegression(random_state = 1))\nlevel1_estimator2 = StackingClassifier(estimators_stacking, AdaBoostClassifier(random_state = 1))\nlevel1_estimator3 = StackingClassifier(estimators_stacking, RandomForestClassifier(random_state = 1))\nlevel1_estimator4 = StackingClassifier(estimators_stacking, DecisionTreeClassifier(random_state = 1))\n\nlevel1_estimator1.fit(level1_training_features, level1_training_labels)\nlevel1_estimator2.fit(level1_training_features, level1_training_labels)\nlevel1_estimator3.fit(level1_training_features, level1_training_labels)\nlevel1_estimator4.fit(level1_training_features, level1_training_labels)\n\nlevel2_estimator = StackingClassifier([('l1',level1_estimator1), ('l2', level1_estimator2), ('l3', level1_estimator3), ('l4', level1_estimator4)], LogisticRegression(random_state = 1))\nlevel2_estimator.fit(level2_training_features, level2_training_labels)\nlevel2_estimator_train_score = level2_estimator.score(level2_training_features, level2_training_labels)\nprint(\"Training Score for Finetuned Model:\", level2_estimator_train_score)\n\nlevel2_estimator_validation_score = level2_estimator.score(validation_features, validation_labels)\nprint(\"Validation Score for Finetuned Model:\", level2_estimator_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nbasic_dnn_model = keras.models.Sequential([\n    keras.layers.Flatten(input_shape = [6]),\n    keras.layers.Dense(300, activation = keras.activations.relu\n, kernel_initializer = 'he_normal'),\n    keras.layers.Dense(200, activation = keras.activations.relu\n, kernel_initializer = 'he_normal'),\n    keras.layers.Dense(100, activation = keras.activations.relu\n, kernel_initializer = 'he_normal'),\n    keras.layers.Dense(50, activation = keras.activations.relu\n, kernel_initializer = 'he_normal'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n])\nbasic_dnn_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nbasic_dnn_model.fit(training_features, training_labels, epochs = 100, verbose = 0, batch_size = 32)\n\nbasic_dnn_model_training_score = basic_dnn_model.evaluate(training_features, training_labels)\nprint(\"Basic Deep Neural Network Training Score :\", basic_dnn_model_training_score[1])\n\nbasic_dnn_model_validation_score = basic_dnn_model.evaluate(validation_features, validation_labels)\nprint(\"Basic Deep Neural Network Validation Score :\", basic_dnn_model_validation_score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(2)\ndef build_model1(activation_function = keras.activations.relu, n_hidden = 1, n_neurons = 100, learning_rate = 1e-3, input_shape = [6]):\n    model = keras.models.Sequential()\n    model.add(keras.layers.InputLayer(input_shape = input_shape))\n    for layer in range(n_hidden):\n        model.add(keras.layers.Dense(n_neurons, activation = activation_function))\n    model.add(keras.layers.Dense(1, activation = tf.keras.activations.sigmoid))\n    optimizer = keras.optimizers.SGD(learning_rate = learning_rate)\n    model.compile(loss=\"binary_crossentropy\", optimizer = optimizer, metrics = ['accuracy'])\n    return model\n\nkeras_classifier1 = keras.wrappers.scikit_learn.KerasClassifier(build_model1, verbose = 0)\nkeras_classifier1.fit(training_features, training_labels, epochs = 100, callbacks=[keras.callbacks.EarlyStopping(patience=10)], verbose = 0)\nkerass_model1_training_score = keras_classifier1.score(training_features, training_labels)\nprint(\"Training Score for the First Keras Classifier :\", kerass_model1_training_score)\nkeras_model1_validation_score = keras_classifier1.score(validation_features, validation_labels)\nprint(\"Validation Score for the First Keras Classifier :\", keras_model1_validation_score)\n\nactivation_function_keras_classifier1 = [keras.activations.relu, keras.activations.selu, keras.activations.elu]\nn_hidden_keras_classifier1 = [1, 2, 3, 5, 7]\nn_neurons_keras_classifier1 = [10, 20, 40, 60, 80, 120, 160]\nlearning_rate_keras_classifier1 = [1e-4, 1e-3, 1e-2, 2e-4, 2e-3, 2e-2, 3e-4, 3e-3, 3e-2]\nparameters_keras_classifier1 = {'activation_function' : activation_function_keras_classifier1, 'n_hidden' : n_hidden_keras_classifier1, 'n_neurons' : n_neurons_keras_classifier1,'learning_rate' : learning_rate_keras_classifier1}\n\nrnd_search_keras_classifier1 = RandomizedSearchCV(keras_classifier1, parameters_keras_classifier1, n_iter = 10, cv = 3, verbose = 0)\nrnd_search_keras_classifier1.fit(training_features, training_labels, epochs = 100, callbacks=[keras.callbacks.EarlyStopping(patience=10)], verbose = 0)\nprint(\"Best Parameters for the First Keras Classifier\", rnd_search_keras_classifier1.best_params_)\nprint(\"Best Cross Validation Score for the First Keras Classifier :\", rnd_search_keras_classifier1.best_score_)\n\nrnd_search_keras_classifier1_best = rnd_search_keras_classifier1.best_estimator_\nrnd_search_keras_classifier1_best.fit(training_features, training_labels)\nrnd_search_keras_classifier1_best_validation_score = rnd_search_keras_classifier1_best.score(validation_features, validation_labels)\nprint(\"Best Parameter First Keras Classifier Validation Score :\", rnd_search_keras_classifier1_best_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Furthering our attempt using a Neural Network with Batch Normalization\ndef build_model2(input_shape = 6, activation_function = keras.activations.elu,  optimization_function = keras.optimizers.Adam(), weight_initializer = 'he_normal'):\n    def input_less_build_model2():\n        keras_model = keras.models.Sequential([\n            keras.layers.InputLayer(input_shape = [input_shape]),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(400, activation = activation_function, kernel_initializer = weight_initializer),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(200, activation = activation_function, kernel_initializer = weight_initializer),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(100, activation = activation_function, kernel_initializer = weight_initializer),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(50, activation = activation_function, kernel_initializer = weight_initializer),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(1, activation = keras.activations.sigmoid)\n        ])\n        keras_model.compile(loss = 'binary_crossentropy', optimizer = optimization_function, metrics = ['accuracy'])\n        return keras_model\n    return input_less_build_model2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Experimenting with varying optimizers, activation functions, weight initializers, batch size, and learning schedules\ntf.random.set_seed(2)\nkeras_model1 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(),verbose = 0)\n\nkeras_model1_cross_val_score = cross_val_score(keras_model1, training_features, training_labels, verbose = 0)\nprint(\"Base Keras Model Cross Validation Score :\", keras_model1_cross_val_score.mean())\n\nkeras_model1.fit(training_features, training_labels, batch_size = 32, epochs = 100, verbose = 0)\n\nkeras_model1_training_score = keras_model1.score(training_features, training_labels)\nprint(\"Base Keras Model Training Score :\", keras_model1_training_score)\n\nkeras_model1_validation_score = keras_model1.score(validation_features, validation_labels)\nprint(\"Base Keras Model Validation Score :\", keras_model1_validation_score)\nkeras_model1.model.save(\"keras_model1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Working with SELU activation and LeCun Normal Initialization\ntf.random.set_seed(2)\nkeras_model2 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(activation_function = keras.activations.selu, weight_initializer = keras.initializers.lecun_normal()), verbose = 0)\n\nkeras_model2_cross_val_score = cross_val_score(keras_model2, training_features, training_labels, verbose = 0)\nprint(\"Keras Model with SELU activation and LeCun Normal Initialization Cross Validation Score :\", keras_model2_cross_val_score.mean())\n\nkeras_model2.fit(training_features, training_labels, batch_size = 32, epochs = 100, verbose = 0)\n\nkeras_model2_training_score = keras_model2.score(training_features, training_labels)\nprint(\"Keras Model with SELU activation and LeCun Normal Initialization Training Score :\", keras_model2_training_score)\n\nkeras_model2_validation_score = keras_model2.score(validation_features, validation_labels)\nprint(\"Keras Model with SELU activation and LeCun Normal Initialization Validation Score :\", keras_model2_validation_score)\nkeras_model2.model.save(\"keras_model2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(2)\nmomentum_optimizer = keras.optimizers.SGD(lr = 0.001, momentum = 0.9)\nkeras_model3 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = momentum_optimizer),verbose = 0)\n\nkeras_model3_cross_val_score = cross_val_score(keras_model3, training_features, training_labels, verbose = 0)\nprint(\"Keras Model with Momentum Optimization Cross Validation Score :\", keras_model3_cross_val_score.mean())\n\nkeras_model3.fit(training_features, training_labels, batch_size = 32, epochs = 100, verbose = 0)\n\nkeras_model3_training_score = keras_model3.score(training_features, training_labels)\nprint(\"Keras Model with Momentum Optimization Training Score :\", keras_model3_training_score)\n\nkeras_model3_validation_score = keras_model3.score(validation_features, validation_labels)\nprint(\"Keras Model with Momentum Optimization Validation Score :\", keras_model3_validation_score)\nkeras_model3.model.save(\"keras_model3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(2)\nnesterov_optimizer = keras.optimizers.SGD(lr = 0.001, momentum = 0.9, nesterov = True)\nkeras_model4 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = nesterov_optimizer), verbose = 0)\n\nkeras_model4_cross_val_score = cross_val_score(keras_model4, training_features, training_labels, verbose = 0)\nprint(\"Keras Model with Nesterov Accelerated Gradient Optimization Cross Validation Score :\", keras_model4_cross_val_score.mean())\n\nkeras_model4.fit(training_features, training_labels, batch_size = 32, epochs = 100, verbose = 0)\n\nkeras_model4_training_score = keras_model4.score(training_features, training_labels)\nprint(\"Keras Model with Nesterov Accelerated Gradient Optimization Training Score :\", keras_model4_training_score)\n\nkeras_model4_validation_score = keras_model4.score(validation_features, validation_labels)\nprint(\"Keras Model with Nesterov Accelerated Gradient Optimization Validation Score :\", keras_model4_validation_score)\nkeras_model4.model.save(\"keras_model4\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(2)\nkeras_model5 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = keras.optimizers.Adagrad()), verbose = 0)\n\nkeras_model5_cross_val_score = cross_val_score(keras_model5, training_features, training_labels, verbose = 0)\nprint(\"Keras Model with AdaGrad Optimization Cross Validation Score :\", keras_model5_cross_val_score.mean())\n\nkeras_model5.fit(training_features, training_labels, batch_size = 32, epochs = 100, verbose = 0)\n\nkeras_model5_training_score = keras_model5.score(training_features, training_labels)\nprint(\"Keras Model with AdaGrad Optimization Training Score :\", keras_model5_training_score)\n\nkeras_model5_validation_score = keras_model5.score(validation_features, validation_labels)\nprint(\"Keras Model with AdaGrad Optimization Validation Score :\", keras_model5_validation_score)\nkeras_model5.model.save(\"keras_model5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(2)\nrmsprop_optimizer = keras.optimizers.RMSprop(lr = 0.001, rho = 0.9)\nkeras_model6 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = rmsprop_optimizer), verbose = 0)\n\nkeras_model6_cross_val_score = cross_val_score(keras_model6, training_features, training_labels, verbose = 0)\nprint(\"Keras Model with RMSProp Optimization Cross Validation Score :\", keras_model6_cross_val_score.mean())\n\nkeras_model6.fit(training_features, training_labels, batch_size = 32, epochs = 100, verbose = 0)\n\nkeras_model6_training_score = keras_model6.score(training_features, training_labels)\nprint(\"Keras Model with RMSProp Optimization Training Score :\", keras_model6_training_score)\n\nkeras_model6_validation_score = keras_model6.score(validation_features, validation_labels)\nprint(\"Keras Model with RMSProp Optimization Validation Score :\", keras_model6_validation_score)\nkeras_model6.model.save(\"keras_model6\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(2)\nadam_nadam_optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\nkeras_model7 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = adam_nadam_optimizer), verbose = 0)\n\nkeras_model7_cross_val_score = cross_val_score(keras_model7, training_features, training_labels, verbose = 0)\nprint(\"Keras Model with Adam and Nadam Optimization Cross Validation Score :\", keras_model7_cross_val_score.mean())\n\nkeras_model7.fit(training_features, training_labels, batch_size = 32, epochs = 100, verbose = 0)\n\nkeras_model7_training_score = keras_model7.score(training_features, training_labels)\nprint(\"Keras Model with Adam and Nadam Optimization Training Score :\", keras_model7_training_score)\n\nkeras_model7_validation_score = keras_model7.score(validation_features, validation_labels)\nprint(\"Keras Model with Adam and Nadam Optimization Validation Score :\", keras_model7_validation_score)\nkeras_model7.model.save(\"keras_model7\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom Stacking for Deep Neural Networks\ntf.random.set_seed(2)\nclass single_layer_stacking_dnn:\n    def __init__(self, level0_estimators = None, final_estimator = RandomForestClassifier(random_state = 1)):\n        self.level0_estimators = level0_estimators\n        self.final_estimator = final_estimator\n    \n    def fit(self, train_X, train_y):\n        level0_train_set = []\n        level1_train_set = []\n\n        sss3 = StratifiedShuffleSplit(n_splits = 1, test_size = 0.1, random_state = 2)\n\n        for index1, index2 in sss3.split(training_set, training_set[\"Survived\"]):\n            level0_train_set = train_data.loc[index1]\n            level1_train_set = train_data.loc[index2]\n        level0_training_features = level0_train_set.drop(\"Survived\", 1).to_numpy()\n        level0_training_labels = level0_train_set.drop([\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"], 1).to_numpy().ravel()\n        \n        level1_training_features = level1_train_set.drop(\"Survived\", 1).to_numpy()\n        level1_training_labels = level1_train_set.drop([\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"], 1).to_numpy().ravel()\n        \n        for level0_estimator in self.level0_estimators:\n            level0_estimator.fit(level0_training_features, level0_training_labels, batch_size = 32, epochs = 100, verbose = 0)\n        \n        level0_outputs = [level0_estimator.predict(level1_training_features) for level0_estimator in self.level0_estimators]\n        level0_outputs_merge = np.concatenate(level0_outputs, axis = 1)\n        \n        self.final_estimator.fit(level0_outputs_merge, level1_training_labels, batch_size = 32, epochs = 100, verbose = 0)\n        \n    def predict(self, test_X):\n        level0_outputs = [level0_estimator.predict(test_X) for level0_estimator in self.level0_estimators]\n        level0_outputs_merge = np.concatenate(level0_outputs, axis = 1)\n        \n        final_predictions = self.final_estimator.predict(level0_outputs_merge).reshape(-1, 1)\n        return final_predictions\n    \n    def score(self, validation_X, validation_y):\n        level0_outputs = [level0_estimator.predict(validation_X) for level0_estimator in self.level0_estimators]\n        level0_outputs_merge = np.concatenate(level0_outputs, axis = 1)\n        return self.final_estimator.score(level0_outputs_merge, validation_y)\ntest_model1 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(),verbose = 0)\ntest_model2 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(activation_function = keras.activations.selu, weight_initializer = keras.initializers.lecun_normal()), verbose = 0)\ntest_model3 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = momentum_optimizer),verbose = 0)\ntest_model4 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = nesterov_optimizer), verbose = 0)\ntest_model5 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = keras.optimizers.Adagrad()), verbose = 0)\ntest_model6 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = rmsprop_optimizer), verbose = 0)\ntest_model7 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = adam_nadam_optimizer), verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Custom Stacking with a Deep Neural Network as the final estimator\ntf.random.set_seed(3)\nfinal_estimator_model = keras.wrappers.scikit_learn.KerasClassifier(build_model2(input_shape = 5), verbose = 0)\nstacking_dnn = single_layer_stacking_dnn([test_model3, test_model4, test_model5, test_model6, test_model7], final_estimator_model)\nstacking_dnn.fit(training_features, training_labels)\n\nstacking_dnn_training_score = stacking_dnn.score(training_features, training_labels)\nprint(\"Stacking Deep Neural Networks Training Score :\", stacking_dnn_training_score)\n\nstacking_dnn_validation_score = stacking_dnn.score(validation_features, validation_labels)\nprint(\"Stacking Deep Neural Networks Validation Score :\", stacking_dnn_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(3)\nfinal_test_model1 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(),verbose = 0)\nfinal_test_model2 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(activation_function = keras.activations.selu, weight_initializer = keras.initializers.lecun_normal()), verbose = 0)\nfinal_test_model3 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = momentum_optimizer),verbose = 0)\nfinal_test_model4 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = nesterov_optimizer), verbose = 0)\nfinal_test_model5 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = keras.optimizers.Adagrad()), verbose = 0)\nfinal_test_model6 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = rmsprop_optimizer), verbose = 0)\nfinal_test_model7 = keras.wrappers.scikit_learn.KerasClassifier(build_model2(optimization_function = adam_nadam_optimizer), verbose = 0)\nfinal_stacking_dnn_estimator = keras.wrappers.scikit_learn.KerasClassifier(build_model2(input_shape = 5), verbose = 0)\nfinal_stacking_dnn = single_layer_stacking_dnn([final_test_model3, final_test_model4, final_test_model5, final_test_model6, final_test_model7], final_stacking_dnn_estimator)\n\nfull_train_data = train_data\nfull_train_features = train_data.drop(\"Survived\", 1).to_numpy()\nfull_train_labels = training_set.drop([\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"], 1).to_numpy().ravel()\nfinal_stacking_dnn.fit(full_train_features, full_train_labels)\n\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nprocessed_test_data = data_process(test_data)\ntest_features = processed_test_data.to_numpy()\npredictions = final_stacking_dnn.predict(test_features)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": predictions.ravel()\n    })\nsubmission.to_csv('titanic_submission1.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}