{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Classifying the silhouette of Vehicles\n\n#### Overview:\nThe data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars.\n\n\n#### Objective:\nThe objective is to classify a given silhouette as one of three types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles.\n\n","metadata":{}},{"cell_type":"markdown","source":"<!-- ![alt text](vehicle1.jpg) -->\n\n### Silhouette of vehicles","metadata":{}},{"cell_type":"markdown","source":"<!-- <img src = \"vehicle1.jpg\" style = \"width:600px\"/> -->","metadata":{}},{"cell_type":"markdown","source":"## 1. Data pre-processing – Perform all the necessary preprocessing on the data ready to be fed to an Unsupervised algorithm","metadata":{}},{"cell_type":"code","source":"# Importing all the necessary libraries\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(color_codes=True)\n\nimport warnings # to ignore warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVC      \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:20.344308Z","iopub.execute_input":"2021-07-18T07:15:20.344876Z","iopub.status.idle":"2021-07-18T07:15:21.599087Z","shell.execute_reply.started":"2021-07-18T07:15:20.344729Z","shell.execute_reply":"2021-07-18T07:15:21.598037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### || Importing the dataset to Data Frame ||","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/vehicle2/vehicle-2.csv')\n\ndf.head(10)  # High level observation of dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.600457Z","iopub.execute_input":"2021-07-18T07:15:21.600728Z","iopub.status.idle":"2021-07-18T07:15:21.661153Z","shell.execute_reply.started":"2021-07-18T07:15:21.600701Z","shell.execute_reply":"2021-07-18T07:15:21.660107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The dataframe has {} rows and {} columns\".format(df.shape[0],df.shape[1]),'\\n\\n')\n\ndf.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-18T07:15:21.662726Z","iopub.execute_input":"2021-07-18T07:15:21.663013Z","iopub.status.idle":"2021-07-18T07:15:21.684126Z","shell.execute_reply.started":"2021-07-18T07:15:21.662987Z","shell.execute_reply":"2021-07-18T07:15:21.683241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are 846 rows with 19 columns.\n### The categorical column 'class' represents the category of vehicles.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum() # Null value check","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.685552Z","iopub.execute_input":"2021-07-18T07:15:21.685819Z","iopub.status.idle":"2021-07-18T07:15:21.694292Z","shell.execute_reply.started":"2021-07-18T07:15:21.685787Z","shell.execute_reply":"2021-07-18T07:15:21.693344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From above we can see that few null values are there in the columns of the dataset. \n### So we have two options either we will drop those null values or we will impute those null values. Dropping null values is not a good way because we will lose some information. Hence going forward we will impute those null values with the median value of the respective columns","metadata":{}},{"cell_type":"code","source":"# Five point summary of the numerical attributes\ndf.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.69543Z","iopub.execute_input":"2021-07-18T07:15:21.695707Z","iopub.status.idle":"2021-07-18T07:15:21.759713Z","shell.execute_reply.started":"2021-07-18T07:15:21.69568Z","shell.execute_reply":"2021-07-18T07:15:21.758577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of target column\nprint(df['class'].value_counts(),'\\n')\nsns.countplot(df['class']);","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.76101Z","iopub.execute_input":"2021-07-18T07:15:21.761389Z","iopub.status.idle":"2021-07-18T07:15:21.923415Z","shell.execute_reply.started":"2021-07-18T07:15:21.761347Z","shell.execute_reply":"2021-07-18T07:15:21.922399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From above we can see that cars are most followed by bus and then vans.\n### The data is not distributed equally for the 3 vehicle classes. The 50% of the data belongs to car class.\n### radius_ratio, axis_aspect_ration, length_aspect_ratio & scaled_radius_of_gyration.1 columns have very long right tail, all the columns mean is greater than the median\n### The scale of the columns are very different so would need normalization.","metadata":{}},{"cell_type":"markdown","source":"### || Data Pre-processing ||","metadata":{}},{"cell_type":"code","source":"#instead of dropping the rows, lets replace the missing values with median value. \ndf.median()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.924917Z","iopub.execute_input":"2021-07-18T07:15:21.925359Z","iopub.status.idle":"2021-07-18T07:15:21.936041Z","shell.execute_reply.started":"2021-07-18T07:15:21.925326Z","shell.execute_reply":"2021-07-18T07:15:21.934933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace the missing values with median value.\n# we do not need to specify the column names below\n# every column's missing value is replaced with that column's median respectively\ndf = df.fillna(df.median())    # The fillna() function is used to fill NaN values using the specified method","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.93851Z","iopub.execute_input":"2021-07-18T07:15:21.938796Z","iopub.status.idle":"2021-07-18T07:15:21.952818Z","shell.execute_reply.started":"2021-07-18T07:15:21.938757Z","shell.execute_reply":"2021-07-18T07:15:21.951724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().apply(pd.value_counts)   # checking null values now","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.954793Z","iopub.execute_input":"2021-07-18T07:15:21.955091Z","iopub.status.idle":"2021-07-18T07:15:21.984758Z","shell.execute_reply.started":"2021-07-18T07:15:21.955054Z","shell.execute_reply":"2021-07-18T07:15:21.983873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Null values are treated","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.986249Z","iopub.execute_input":"2021-07-18T07:15:21.986651Z","iopub.status.idle":"2021-07-18T07:15:21.991263Z","shell.execute_reply.started":"2021-07-18T07:15:21.986607Z","shell.execute_reply":"2021-07-18T07:15:21.990105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum()  # no duplicate values found in the dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:21.992971Z","iopub.execute_input":"2021-07-18T07:15:21.9934Z","iopub.status.idle":"2021-07-18T07:15:22.01186Z","shell.execute_reply.started":"2021-07-18T07:15:21.993326Z","shell.execute_reply":"2021-07-18T07:15:22.010854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optional, we can implement without replacing the classes with numerical values also\nreplace_dependent= {'class' : {'car': 0, 'bus': 1, 'van': 2} }\n\ndf = df.replace(replace_dependent)\ndf['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:22.01343Z","iopub.execute_input":"2021-07-18T07:15:22.013832Z","iopub.status.idle":"2021-07-18T07:15:22.026167Z","shell.execute_reply.started":"2021-07-18T07:15:22.013789Z","shell.execute_reply":"2021-07-18T07:15:22.025485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:22.027244Z","iopub.execute_input":"2021-07-18T07:15:22.027678Z","iopub.status.idle":"2021-07-18T07:15:22.049326Z","shell.execute_reply.started":"2021-07-18T07:15:22.027635Z","shell.execute_reply":"2021-07-18T07:15:22.048281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Understanding the attributes - Find relationship between different attributes (Independent variables) and choose carefully which all attributes have to be a part of the analysis and why","metadata":{}},{"cell_type":"code","source":"k=1\nplt.figure(figsize=(20,30))\n\n# using for loop to iterate over all the columns in the dataframe and plot the histogram of those \n\nfor col in df.columns[0:18]: # iterating columns except dependent column\n    plt.subplot(5,4,k)\n    sns.distplot(df[col],color='black')\n    k=k+1","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:22.050594Z","iopub.execute_input":"2021-07-18T07:15:22.050901Z","iopub.status.idle":"2021-07-18T07:15:26.836375Z","shell.execute_reply.started":"2021-07-18T07:15:22.050869Z","shell.execute_reply":"2021-07-18T07:15:26.835404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k=1\nplt.figure(figsize=(20,30))\n\n# using for loop to iterate over all the columns in the dataframe and plot the boxplot of those \n# as we can observe outliers easily in boxplot\n\nfor col in df.columns[0:18]: # iterating columns except dependent column\n    plt.subplot(5,4,k)\n    sns.boxplot(y=df[col],color='black')\n    k=k+1","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:26.837663Z","iopub.execute_input":"2021-07-18T07:15:26.838168Z","iopub.status.idle":"2021-07-18T07:15:28.920624Z","shell.execute_reply.started":"2021-07-18T07:15:26.838127Z","shell.execute_reply":"2021-07-18T07:15:28.919571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we can see few columns are having outliers \n### * There are no outliers in compactness and circularity columns and they looks like normally distributed.\n### * There are no outliers in the distance_circularity column but we can see that there is right skewness as the tail is towards the right side(mean>median)\n### * we can see that there are outliers in radius_ratio and max_length_aspect_ratio columns and there is positive skewness because the long tail is on the right side(mean>median)\n### * We can analyze that in the columns scaled_cariance, scaled_variance.1, scaled_radius_of_gyration.1, skewness_about and skewness_about.1 are having outliers and right skewed\n### * Columns except previous point even though seems not having outliers but they are highly skewed to the right, there are more than one peaks in the distribution\n\n\n### After seeing that there are outliers present in the columns, i assume that the presence of outliers in these columns are natural.\n### As we know that models are affected by outliers, we can treat the outliers with some metric, we will see down th line\n","metadata":{}},{"cell_type":"code","source":"# kde plots to show the distribution of the all the variables with respect to dependent variable\nk=1\nplt.figure(figsize=(20,30))\nfor col in df.columns[0:18]:\n    plt.subplot(5,4,k)\n    sns.kdeplot(df[df['class']==0][col],color='red',label='car',shade=True)\n    sns.kdeplot(df[df['class']==1][col],color='blue',label='bus',shade=True)\n    sns.kdeplot(df[df['class']==2][col],color='yellow',label='van',shade=True)\n    plt.title(col)\n    k=k+1","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:28.921994Z","iopub.execute_input":"2021-07-18T07:15:28.922518Z","iopub.status.idle":"2021-07-18T07:15:33.364968Z","shell.execute_reply.started":"2021-07-18T07:15:28.922478Z","shell.execute_reply":"2021-07-18T07:15:33.364193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  **Observations** :\n### Spread of compactness is least for van. mean compactness is highest for car. For Bus compactness is right skewed indicating that less number of buses have high compactness.\n### Mean circularity is higher for cars\n### Mean distance_circularity is also higher for cars\n### pr.axis_aspect_ratio is has almost same distribution for car, van and buses\n### 'scaled_radius_of_gyration', 'scaled_radius_of_gyration.1', 'skewness_about', 'skewness_about.1', 'skewness_about.2', have almost similar distribution for cars, buses and vans.\n","metadata":{}},{"cell_type":"code","source":"# sns.pairplot(df,diag_kind='kde')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:33.365874Z","iopub.execute_input":"2021-07-18T07:15:33.366239Z","iopub.status.idle":"2021-07-18T07:15:33.369519Z","shell.execute_reply.started":"2021-07-18T07:15:33.366211Z","shell.execute_reply":"2021-07-18T07:15:33.368441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From above pair plots we can see that many columns are correlated among them, this leads to multi-collinearity and many columns have long tail so that is the indication of outliers.we will see down the line with the help of correlation matrix what's the strength of correlation using heatmap","metadata":{}},{"cell_type":"code","source":"fig= plt.subplots(figsize=(20, 10))\nsns.heatmap(df.corr(),annot=True, linewidth = 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:33.370858Z","iopub.execute_input":"2021-07-18T07:15:33.371118Z","iopub.status.idle":"2021-07-18T07:15:35.596784Z","shell.execute_reply.started":"2021-07-18T07:15:33.371092Z","shell.execute_reply":"2021-07-18T07:15:35.595871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From above correlation matrix we can see that there are many features which are highly correlated. if we see carefully then Scatter ratio & Scaled_variance1 has almost perfect positive linear relationship and many other features also there which having more than 0.9(positive and negative) correlation and it is difficult to determine which dimensions to drop manually.\n\n### So our objective is to recognize whether an object is a van or bus or car based on some input features. so our main assumption is there is little or no multicollinearity between the features. All the features should be independent with one another, if two features is highly correlated then there is no use in using both features.\n\n###  As we have seen in the distribution plots, we have outliers in some columns and we can remove those outliers or we will apply pca and let pca to be decided how it will explain above data which is in high dimension with smaller number of variables and luckily it removes multicollinearity as well.\n\n### We can drop the variables which are highly correlated with each other by setting threshold and also hypothesis test can be implemented on all these variables by checking p-value of those and variables can be removed which are having p-value more than 0.05\n\n### Since we are going to implement PCA technique, it will take care multicollinearity and gives us the the variables which are independent to each other","metadata":{}},{"cell_type":"code","source":"correlation_values=df.corr()['class']\npd.DataFrame(correlation_values.sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:35.597872Z","iopub.execute_input":"2021-07-18T07:15:35.598134Z","iopub.status.idle":"2021-07-18T07:15:35.612241Z","shell.execute_reply.started":"2021-07-18T07:15:35.598109Z","shell.execute_reply":"2021-07-18T07:15:35.61103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With above correlation dataframe we can observe that only top 4 columns has positive correlation with the target variable and rest are having negative correlation,","metadata":{}},{"cell_type":"markdown","source":"#","metadata":{}},{"cell_type":"markdown","source":"## 3. Split the data into train and test (Suggestion: specify “random state” if you are using train_test_split from Sklearn)","metadata":{}},{"cell_type":"code","source":"x = df.iloc[:,0:18]  # independent variables\ny = df['class']      # target variable\n\n# Scaling the data using zscore technique as the predictor values has different scale\nfrom scipy.stats import zscore\nXScaled=x.apply(zscore)\nXScaled.head()\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(XScaled, y, test_size=0.2, random_state = 56)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:35.613559Z","iopub.execute_input":"2021-07-18T07:15:35.613895Z","iopub.status.idle":"2021-07-18T07:15:35.635655Z","shell.execute_reply.started":"2021-07-18T07:15:35.613851Z","shell.execute_reply":"2021-07-18T07:15:35.63454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig= plt.subplots(figsize=(20, 10))\nsns.heatmap(XScaled.corr(),annot=True, linewidth = 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:35.637043Z","iopub.execute_input":"2021-07-18T07:15:35.637318Z","iopub.status.idle":"2021-07-18T07:15:37.860833Z","shell.execute_reply.started":"2021-07-18T07:15:35.637288Z","shell.execute_reply":"2021-07-18T07:15:37.860129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Train a Support vector machine using the train set and get the accuracy on the test set","metadata":{}},{"cell_type":"code","source":"# instantiating the model\nsvm_model = SVC()\n\n# fitting the model\nsvm_model.fit(x_train, y_train)\n\n# score on unseen data\naccuracy = svm_model.score(x_test,y_test)\nprint(accuracy*100)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:37.861731Z","iopub.execute_input":"2021-07-18T07:15:37.86209Z","iopub.status.idle":"2021-07-18T07:15:37.889312Z","shell.execute_reply.started":"2021-07-18T07:15:37.862062Z","shell.execute_reply":"2021-07-18T07:15:37.888158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_df = pd.DataFrame({ 'Model' : 'SVM',  'Accuracy' : [accuracy*100] })\nscores_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:37.893045Z","iopub.execute_input":"2021-07-18T07:15:37.893365Z","iopub.status.idle":"2021-07-18T07:15:37.904721Z","shell.execute_reply.started":"2021-07-18T07:15:37.893335Z","shell.execute_reply":"2021-07-18T07:15:37.903491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction using test data\ny_pred = svm_model.predict(x_test)\n\n# generating classification report of actual and predicted values\nprint(metrics.classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:37.90966Z","iopub.execute_input":"2021-07-18T07:15:37.910022Z","iopub.status.idle":"2021-07-18T07:15:37.925348Z","shell.execute_reply.started":"2021-07-18T07:15:37.909992Z","shell.execute_reply":"2021-07-18T07:15:37.924294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix of actual and predicted values\nmetrics.confusion_matrix(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:37.926469Z","iopub.execute_input":"2021-07-18T07:15:37.926722Z","iopub.status.idle":"2021-07-18T07:15:37.935148Z","shell.execute_reply.started":"2021-07-18T07:15:37.926698Z","shell.execute_reply":"2021-07-18T07:15:37.934317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Perform K-fold cross validation and get the cross validation score of the model","metadata":{}},{"cell_type":"code","source":"# number of splits (25)\nnum_folds = 2\n\n# initialising kfold object\nkfold = KFold(n_splits = num_folds, random_state = 56, shuffle=True)\n\n# specifying the model to perform cross validation\nmodel = SVC()\n\nfrom sklearn.model_selection import cross_val_predict\n\n\n# noting accuracy scores of all the 25 split runs\nfor i in [num_folds]:\n    scores = cross_val_score(model, XScaled, y, cv = kfold)\n#     y_pred = cross_val_predict(model, XScaled, y, cv=kfold)\n#     y_pred = model.predict(y_test)\n#     print(metrics.confusion_matrix(y,y_pred))\n#     print(metrics.classification_report(y,y_pred))\n\n# printing all the 25 scores\nprint(scores)\nprint('')\n\n# # here we are getting average accuracy with standard deviation for range estimate\n# print('Overall Accuracy : {:.2f}% ({:.2f}%)'.format(scores.mean()*100.0, scores.std()*100.0))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:37.9364Z","iopub.execute_input":"2021-07-18T07:15:37.936685Z","iopub.status.idle":"2021-07-18T07:15:37.987825Z","shell.execute_reply.started":"2021-07-18T07:15:37.936659Z","shell.execute_reply":"2021-07-18T07:15:37.986386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we are getting higher accuracy. By seeing average of all the 25 cross validation scores we can say that our model would perform betweeen 91% and 100% of accuracy with 95% of the confidence interval","metadata":{}},{"cell_type":"code","source":"scores_df1 = pd.DataFrame({'Model': ['SVM(cross_val)'], 'Accuracy' : [scores.mean()*100]})\nscores_df = pd.concat([scores_df,scores_df1]).drop_duplicates()\nscores_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:37.989278Z","iopub.execute_input":"2021-07-18T07:15:37.989596Z","iopub.status.idle":"2021-07-18T07:15:38.004838Z","shell.execute_reply.started":"2021-07-18T07:15:37.989563Z","shell.execute_reply":"2021-07-18T07:15:38.00367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Use PCA from Scikit learn, extract Principal Components that capture about 95% of the variance in the data","metadata":{}},{"cell_type":"markdown","source":"#### Principal Component Analysis is an unsupervised machine learning technique which finds insights of data without having prior knowledge. It reduces data by projecting onto lower dimensional basis know as principal components\n#### It uses the matrix operations from linear algebra and statistics to calculate a projection of the original data into the same number or fewer dimensions\n#### Main idea of using PCA is to seek most accurate data representation in lower dimension space and it removes the noise and explore the hidden pattern within the data\n\n#### Below we will implement the PCA with step by step","metadata":{}},{"cell_type":"markdown","source":"### Step 1. Normalize / Scale the data  \n--- As the data is already scaled using z-score when splitting the data into independent and dependent columns hence, we are not going to perform scaling here","metadata":{}},{"cell_type":"markdown","source":"### Step 2. Second step is to Generate the covariance matrix / correlation matrix for all the dimensions","metadata":{}},{"cell_type":"code","source":"covMatrix = np.cov(XScaled,rowvar=False)\npd.DataFrame(covMatrix)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.006328Z","iopub.execute_input":"2021-07-18T07:15:38.00676Z","iopub.status.idle":"2021-07-18T07:15:38.055259Z","shell.execute_reply.started":"2021-07-18T07:15:38.006719Z","shell.execute_reply":"2021-07-18T07:15:38.054229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Covariance is the direction of the linear relationship between variables.","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.056691Z","iopub.execute_input":"2021-07-18T07:15:38.057093Z","iopub.status.idle":"2021-07-18T07:15:38.061757Z","shell.execute_reply.started":"2021-07-18T07:15:38.057049Z","shell.execute_reply":"2021-07-18T07:15:38.060482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3. Perform eigen decomposition, that is, compute eigen vectors which are the principal components and the corresponding eigen values which are the magnitudes of variance captured","metadata":{}},{"cell_type":"markdown","source":"#### Here we will generate the eigen values and their corresponding eigen vectors.\n#### The eigenvectors are nothing but the principal components of our above covariance matrix and represent the axis of new feature space whereas eigenvalues are the magnitude of those vectors","metadata":{}},{"cell_type":"code","source":"pca = PCA()\npca.fit(XScaled)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.063404Z","iopub.execute_input":"2021-07-18T07:15:38.064042Z","iopub.status.idle":"2021-07-18T07:15:38.139081Z","shell.execute_reply.started":"2021-07-18T07:15:38.063991Z","shell.execute_reply":"2021-07-18T07:15:38.137884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eignen values\npca.explained_variance_","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.140397Z","iopub.execute_input":"2021-07-18T07:15:38.140805Z","iopub.status.idle":"2021-07-18T07:15:38.145821Z","shell.execute_reply.started":"2021-07-18T07:15:38.140752Z","shell.execute_reply":"2021-07-18T07:15:38.145168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eigen Vectors\npd.DataFrame(pca.components_)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.146819Z","iopub.execute_input":"2021-07-18T07:15:38.147264Z","iopub.status.idle":"2021-07-18T07:15:38.186112Z","shell.execute_reply.started":"2021-07-18T07:15:38.147225Z","shell.execute_reply":"2021-07-18T07:15:38.185064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here in the below code printing the percentage of variance explained by each principle component from 1 to 18 (as there are 18 independent columns in our dataset)\n","metadata":{}},{"cell_type":"code","source":"k = 1\ntotal = []\nfor i in pca.explained_variance_ratio_*100:\n    print('Variance explained by Principle Component',k,'is : {:.2f}%'.format(i))\n    k+=1\n    total.append(i)\nprint('\\nTotal variance explained by all the principle components:',sum(total),'%')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.187802Z","iopub.execute_input":"2021-07-18T07:15:38.188236Z","iopub.status.idle":"2021-07-18T07:15:38.203126Z","shell.execute_reply.started":"2021-07-18T07:15:38.188187Z","shell.execute_reply":"2021-07-18T07:15:38.202034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we can notice that the First principal component is a linear combination of original predictor variables which captures the maximum variance i.e. 52% in the data set followed by two,three and so on... It determines the direction of highest variability in the data.\n\n### Will visualize the plot of above principle components","metadata":{}},{"cell_type":"code","source":"# Implementing scree plot \nplt.figure(figsize=(16 , 5))\nplt.bar(range(1, 19), pca.explained_variance_ratio_, label = 'Individual explained variance',color='lightblue',edgecolor='black')\nplt.step(range(1, 19), np.cumsum(pca.explained_variance_ratio_),where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.204586Z","iopub.execute_input":"2021-07-18T07:15:38.204938Z","iopub.status.idle":"2021-07-18T07:15:38.463452Z","shell.execute_reply.started":"2021-07-18T07:15:38.204905Z","shell.execute_reply":"2021-07-18T07:15:38.462386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visually we can observe that there is a steep drop in variance explained with increase in number of PC's.\n### We will proceed with 7 components here as it explains 95% of the variance\n### With only 7 variables/PC's we can explain over 95% of the variation in the original data!","metadata":{}},{"cell_type":"code","source":"# NOTE - we are generating only 7 PCA dimensions (dimensionality reduction from 18 to 7)\n\npca2 = PCA(n_components=7)  # here you can notice we are specifying 7 PC components in the parameter called n_components\npca2.fit(XScaled)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.464824Z","iopub.execute_input":"2021-07-18T07:15:38.465147Z","iopub.status.idle":"2021-07-18T07:15:38.480928Z","shell.execute_reply.started":"2021-07-18T07:15:38.465116Z","shell.execute_reply":"2021-07-18T07:15:38.48001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eigen Vectors (transformed into 7 dimensions)\npd.DataFrame(pca2.components_).T","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.482184Z","iopub.execute_input":"2021-07-18T07:15:38.48247Z","iopub.status.idle":"2021-07-18T07:15:38.501857Z","shell.execute_reply.started":"2021-07-18T07:15:38.482443Z","shell.execute_reply":"2021-07-18T07:15:38.500887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eigen Values (only 7)\npca2.explained_variance_","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.50314Z","iopub.execute_input":"2021-07-18T07:15:38.503452Z","iopub.status.idle":"2021-07-18T07:15:38.510154Z","shell.execute_reply.started":"2021-07-18T07:15:38.503412Z","shell.execute_reply":"2021-07-18T07:15:38.509084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percentage of variance explained by 7 PC components\nsum(pca2.explained_variance_ratio_*100)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.511558Z","iopub.execute_input":"2021-07-18T07:15:38.511885Z","iopub.status.idle":"2021-07-18T07:15:38.520319Z","shell.execute_reply.started":"2021-07-18T07:15:38.511854Z","shell.execute_reply":"2021-07-18T07:15:38.519369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now Implementing scree plot on 8 variables\nplt.figure(figsize=(16 , 5))  # size of the plot\n\n# bar plot\nplt.bar(range(1, 8), pca2.explained_variance_ratio_, label = 'Individual explained variance',color='lightblue',edgecolor='black')\n\n# step plot on bars which is a cummulative sum of the variance explained by 7 pc components\nplt.step(range(1, 8), np.cumsum(pca2.explained_variance_ratio_),where='mid', label = 'Cumulative explained variance',color = 'black')\n\nplt.ylabel('Explained Variance Ratio')  # x axis label\nplt.xlabel('Principal Components')      # y axis label\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.521905Z","iopub.execute_input":"2021-07-18T07:15:38.522268Z","iopub.status.idle":"2021-07-18T07:15:38.792061Z","shell.execute_reply.started":"2021-07-18T07:15:38.522239Z","shell.execute_reply":"2021-07-18T07:15:38.791026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #transforming the impute raw data which is in 18 dimension into 7 new dimension with pca\npca_transformed =  pca2.transform(XScaled)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.793247Z","iopub.execute_input":"2021-07-18T07:15:38.793535Z","iopub.status.idle":"2021-07-18T07:15:38.800579Z","shell.execute_reply.started":"2021-07-18T07:15:38.793504Z","shell.execute_reply":"2021-07-18T07:15:38.799507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the shape of pca_transformed data\npca_transformed.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.801996Z","iopub.execute_input":"2021-07-18T07:15:38.802338Z","iopub.status.idle":"2021-07-18T07:15:38.814413Z","shell.execute_reply.started":"2021-07-18T07:15:38.802305Z","shell.execute_reply":"2021-07-18T07:15:38.813444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising PCA dimensions with pair panel\nsns.pairplot(pd.DataFrame(pca_transformed),diag_kind = 'kde')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:38.815816Z","iopub.execute_input":"2021-07-18T07:15:38.816179Z","iopub.status.idle":"2021-07-18T07:15:46.926211Z","shell.execute_reply.started":"2021-07-18T07:15:38.816147Z","shell.execute_reply":"2021-07-18T07:15:46.925038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Above pairplot clearly shows there is no Collinearity between the variables, hence correlation is close to zero. So we could able to decrease the correlation between independent variables\n\n### Going forward only with 7 columns we will be able to build the model which should give almost same accuracy as the model with 18 dimensions, will see below","metadata":{}},{"cell_type":"code","source":"# will see the shape of original train and test dataset\n\nprint('original data shape')\nprint('shape of x_train',x_train.shape)\nprint('shape of y_train',y_train.shape)\nprint('shape of x_test',x_test.shape)\nprint('shape of y_test',y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:46.927506Z","iopub.execute_input":"2021-07-18T07:15:46.927821Z","iopub.status.idle":"2021-07-18T07:15:46.934585Z","shell.execute_reply.started":"2021-07-18T07:15:46.927766Z","shell.execute_reply":"2021-07-18T07:15:46.933661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Repeat steps 3,4 and 5 but this time, use Principal Components instead of the original data. And the accuracy score should be on the same rows of test data that were used earlier. ","metadata":{}},{"cell_type":"code","source":"# split the transformed pca data\npca_x_train, pca_x_test, pca_y_train, pca_y_test = train_test_split(pca_transformed, y, test_size = 0.2, random_state = 56)\n\n\n# Shape of new train and test data\nprint('*** Transformed data using pca ***')\nprint('   shape of pca_x_train:',pca_x_train.shape)\nprint('   shape of pca_y_train:',pca_y_train.shape)\nprint('   shape of pca_x_test:',pca_x_test.shape)\nprint('   shape of pca_y_test:',pca_y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:46.935841Z","iopub.execute_input":"2021-07-18T07:15:46.936123Z","iopub.status.idle":"2021-07-18T07:15:46.949235Z","shell.execute_reply.started":"2021-07-18T07:15:46.936095Z","shell.execute_reply":"2021-07-18T07:15:46.948052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can observe that dimensions are reduced to 7, this is called the dimensionality reduction technique. Now we will build the SVM model again and predict the score and also will compare with original data","metadata":{}},{"cell_type":"code","source":"# creating a dataframe with the new dataset\n### pca_transformed is the new dataset with 7 principle components\npca_transformed = pd.DataFrame(pca_transformed,columns = df.columns[0:7])\n\n# shape of the dataframe\nprint(pca_transformed.shape)  \n\n# displaying head of the dataframe\npca_transformed.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:46.950501Z","iopub.execute_input":"2021-07-18T07:15:46.950821Z","iopub.status.idle":"2021-07-18T07:15:46.973244Z","shell.execute_reply.started":"2021-07-18T07:15:46.950766Z","shell.execute_reply":"2021-07-18T07:15:46.972227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is our new transformed data using PCA including 7 columns and 846 data points\n\n### We will build the classification model again with this new data","metadata":{}},{"cell_type":"code","source":"# instanstiating the object of SVM model / building the svm model using principle components instead of original data\nsvm_pca = SVC()\n\n# fitting the model on new data\nsvm_pca.fit(pca_x_train, pca_y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:46.974453Z","iopub.execute_input":"2021-07-18T07:15:46.974733Z","iopub.status.idle":"2021-07-18T07:15:46.994643Z","shell.execute_reply.started":"2021-07-18T07:15:46.974708Z","shell.execute_reply":"2021-07-18T07:15:46.993578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score of test data\nprint('Accuracy score of SVM model after reducing dimensions :',svm_pca.score(pca_x_test,pca_y_test),'\\n\\n')\n\n# prediction using pca test data\nsvm_pca_pred = svm_pca.predict(pca_x_test)\n\n# generating classification report of actual and predicted values\nprint(metrics.classification_report(pca_y_test, svm_pca_pred))\n\n# confusion matrix of actual and predicted values\nprint('\\n Confusion matrix:\\n',metrics.confusion_matrix(pca_y_test, svm_pca_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:46.995919Z","iopub.execute_input":"2021-07-18T07:15:46.996199Z","iopub.status.idle":"2021-07-18T07:15:47.013175Z","shell.execute_reply.started":"2021-07-18T07:15:46.996171Z","shell.execute_reply":"2021-07-18T07:15:47.011924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_df1 = pd.DataFrame({'Model': ['SVM with PCA'], 'Accuracy' : [svm_pca.score(pca_x_test,pca_y_test)*100]})\nscores_df = pd.concat([scores_df,scores_df1]).drop_duplicates()\nscores_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:47.014519Z","iopub.execute_input":"2021-07-18T07:15:47.014907Z","iopub.status.idle":"2021-07-18T07:15:47.032221Z","shell.execute_reply.started":"2021-07-18T07:15:47.014871Z","shell.execute_reply":"2021-07-18T07:15:47.031054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see that getting better accuracy even with the 7 variables/principle components. it's close to the accuracy which we got with raw data","metadata":{}},{"cell_type":"code","source":"# number of splits\nnum_folds = 25\n\n# initialising kfold object\nkfold = KFold(n_splits = num_folds, random_state = 56, shuffle=True)\n\n# specifying the model to perform cross validation\nmodel = SVC()\n\n# noting accuracy scores of all the 25 split runs\nscores = cross_val_score(model, pca_transformed, y, cv = kfold)\n\n# printing all the 25 scores\nprint(scores)\nprint('')\n\n# here we are getting average accuracy with standard deviation for range estimate\nprint('Overall Accuracy : {:.2f}% ({:.2f}%)'.format(scores.mean()*100.0, scores.std()*100.0))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:47.033277Z","iopub.execute_input":"2021-07-18T07:15:47.033524Z","iopub.status.idle":"2021-07-18T07:15:47.545998Z","shell.execute_reply.started":"2021-07-18T07:15:47.033501Z","shell.execute_reply":"2021-07-18T07:15:47.544608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### By seeing average of all the 25 cross validation scores we can say that our model would perform betweeen 84% and 100% of accuracy with 95%(two std) of confidence interval","metadata":{}},{"cell_type":"code","source":"scores_df1 = pd.DataFrame({'Model': ['SVM(cross_val) with PCA'], 'Accuracy' : [scores.mean()*100]})\nscores_df = pd.concat([scores_df,scores_df1]).drop_duplicates()\nscores_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:47.547433Z","iopub.execute_input":"2021-07-18T07:15:47.547859Z","iopub.status.idle":"2021-07-18T07:15:47.562946Z","shell.execute_reply.started":"2021-07-18T07:15:47.547815Z","shell.execute_reply":"2021-07-18T07:15:47.561891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights drawn :-","metadata":{}},{"cell_type":"markdown","source":"### Here, we have tried to study the data pattern and correlation among all the features with the help of various kind of plots like univariate, bivariate and multivariate plots. To check the skewness and spread of the data points for each features. From boxplots, presence of outliers were witnessed and to nullify the effect of outliers scaling has been done with the help of Z-score. To check the correlation between the feature columns pair plot, correlation matrix and correlation heat map also been drawn. Those above three plots/matrix clearly indicated a strong correlation or the multicollinearity between many features.\n\n### To calculate classification results SVM model has been implemented and other technique which is part of feature enginering such as KFold cross validation.\n\n### And our Big boy in part of this project, PCA has been implemented where we have extracted features by reducing dimensions from 18 to 7\n\n### By reducing the dimension of  feature space, we have fewer relationships between variables to consider and are less likely to overfit our model and as an added benefit, each of the “new” variables after PCA are all independent of one another, we saw this in pairplot of transformed variables with PCA\n\n### The classification results were computed for respective models created with some metrics such as accuracy scores, confusion matrix with respect to actual vs predicted and classification report of actual vs prdicted results\n\n","metadata":{}},{"cell_type":"markdown","source":"__________________________________________________________________________________________________________________________","metadata":{}},{"cell_type":"markdown","source":"## 8. Compare the accuracy scores and cross validation scores of Support vector machines – one trained using raw data and the other using Principal Components, and mention your findings (5 points)","metadata":{}},{"cell_type":"code","source":"scores_df","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:47.564312Z","iopub.execute_input":"2021-07-18T07:15:47.564785Z","iopub.status.idle":"2021-07-18T07:15:47.583135Z","shell.execute_reply.started":"2021-07-18T07:15:47.564736Z","shell.execute_reply":"2021-07-18T07:15:47.581838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Above dataframe is the accuracy scores on test set of all the techniques we have implemented so far","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(9,5))\nplt.title('Accuracy values for various techniques')\n\nax = sns.barplot(scores_df['Model'],scores_df['Accuracy'],color='lightblue');\n\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(p.get_height()), \n                    (p.get_x() + p.get_width() / 2, p.get_height()),\n                    ha = 'center', va = 'center', \n                    xytext = (0, -12), \n                    textcoords = 'offset points') # used annotation to show the percentage of accuracy","metadata":{"execution":{"iopub.status.busy":"2021-07-18T07:15:47.584272Z","iopub.execute_input":"2021-07-18T07:15:47.584579Z","iopub.status.idle":"2021-07-18T07:15:47.789387Z","shell.execute_reply.started":"2021-07-18T07:15:47.584539Z","shell.execute_reply":"2021-07-18T07:15:47.788212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From the above bar plot, below interpretation were drawn:\n\n### Here we can observe PCA is doing very good job. The accuracies of all the four techniques we have implemented almost similar with above 90%. So we could able to achieve with only 7 variables \n\n### Accuracy with PCA is approximately 93% and with the raw data we are getting 95% of the accuracy and the cross validation seems performing well with the raw data but note that pca 93% accuracy is with only 70 dimensions where as rawdata has 18 dimensions. But every thing has two sides, disadvantage of PCA is we cannot do interpretation with the model much, it's blackbox.\n\n### In feature extraction(PCA), we create 18 new independent variables, where each new independent variable is a linear combination of each of the 18 old independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.\n\n### We keep as many of the new independent variables as we want, but we drop the least important ones, here we reduced them to seven while still retaining the most valuable parts of all of the variables! so only we got better accuracy.\n\n### Performance of the models can be improved by doing hyper parameters tuning using Cross validation and GridSearchCV and select the best parameters for SVM, and also we can select more number of principle components\n\n### By implementing PCA technique we overcome the issue of model complexity with more number of variables but less interpretability\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}