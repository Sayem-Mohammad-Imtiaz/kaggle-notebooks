{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 소개  \n이제 특성 추출을 위해 합성곱 레이어가 사용하는 레이어를 살펴보았으니, 스스로 신경망을 만들어보자!"},{"metadata":{},"cell_type":"markdown","source":"# 간단하게부터 세련되게까지 \n마지막 세 개의 레슨에서는, 합성곱 신경망이 필터링, 감지, 압축 세 과정을 통해 특성 추출을 하는 과정을 살펴보았다. 특성 추출을 한 번 하면 선의 형태와 대비 등 간단한 특징만 잡아낸다. 이는 대부분의 분류 문제를 해결하기에는 너무 간단하다. 대신, 합성곱 신경망은 특성 추출을 계속 반복하여 신경망이 깊게 학습할수록 더욱 '복잡하고 세련된' 특성을 감지하도록 한다.  \n<figure>\n<img src=\"https://i.imgur.com/VqmC1rm.png\" alt=\"Features extracted from an image of a car, from simple to refined.\" width=800>\n</figure>  "},{"metadata":{},"cell_type":"markdown","source":"# 합성곱 블록\n이 방법은 특성 추출을 하는 '합성곱 블록' 여러 개를 통과시키게 하여 구현 가능하다.  \n<figure>\n<img src=\"https://i.imgur.com/pr8VwCZ.png\" width=\"400\" alt=\"Extraction as a sequence of blocks.\">\n</figure>  \n  \n  이런 합성곱 블록은 'Conv2D'와 'MaxPool2D' 레이어의 스택(stack)이다.  \n<figure>\n<!-- <img src=\"./images/2-block-crp.png\" width=\"400\" alt=\"A kind of extraction block: convolution, ReLU, pooling.\"> -->\n<img src=\"https://i.imgur.com/8D6IhEw.png\" width=\"400\" alt=\"A kind of extraction block: convolution, ReLU, pooling.\">\n</figure>  \n  \n  각 블록은 하나의 특성 추출 과정을 의미하고, 합성곱 신경망에서 이런 블록을 조합하면서 추출되는 특성을 분류 문제를 해결하기에 더욱 적합한 특성으로 재출력시킬 수 있다. 이렇듯 현대 합성곱 신경망의 깊은 구조가 압도적인 성능의 이유가 된다."},{"metadata":{},"cell_type":"markdown","source":"# 예시 - 합성곱 신경망 만들기  \n복잡한 특성을 추출할 수 있는 깊은 합성곱 신경망을 구현하는 과정을 살펴보자 이 예시에서는, 케라스의 Sequential 모델을 만들어 Cars 데이터셋에 훈련시킬 것이다.  \n## Step 1 - 데이터 불러오기  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# 시드 생성기\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# matplotlib 사전 설정\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # 에러 메시지를 띄우지 않게 하여 출력창을 정리함\n\n# 훈련, 검증 데이터셋 불러오기\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels = 'inferred',\n    label_mode = 'binary',\n    image_size = [128, 128],\n    interpolation='nearest',\n    batch_size = 64,\n    shuffle = True\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False\n)\n\n# 데이터 파이프라인\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (ds_train_.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE))\nds_valid = (ds_valid_.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2 - 모델 정의하기\n아래는 우리가 사용할 모델의 모습니다.  \n<figure>\n<!-- <img src=\"./images/2-convmodel-1.png\" width=\"200\" alt=\"Diagram of a convolutional model.\"> -->\n<img src=\"https://i.imgur.com/U1VdoDJ.png\" width=\"250\" alt=\"Diagram of a convolutional model.\">\n</figure>  \n우리 모델은 베이스로 Conv2D와 MaxPool2D 레이어의 블럭이 3개, 헤드인 Dense 레이어들로 이루어진다. 이 구조를 파라미터만 잘 설정한다면 케라스의 Sequential 모델로 적을 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    \n    # 첫 번째 합성곱 레이어\n    layers.Conv2D(filters = 32, kernel_size=3, activation='relu', padding='same',\n                 input_shape = [128,128,3]), # [높이, 너비, 컬러 채널(RGB)]로 입력값의 차원 정의\n    layers.MaxPool2D(),\n    \n    # 두 번째 합성곱 레이어\n    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n    \n    # 세 번째 합성곱 레이어\n    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n    \n    # 분류자 헤드\n    layers.Flatten(),\n    layers.Dense(units=6, activation='relu'),\n    layers.Dense(units=1, activation='sigmoid')\n])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"블럭마다 필터의 크기는 증가한다 : 64, 128, 256. 이는 흔한 패턴이다. MaxPool2D 레이어가 특성 맵의 사이즈를 감소시키니, 그 개수를 증가시키는 것이다."},{"metadata":{},"cell_type":"markdown","source":"## Step 3 - 훈련  \n이제 모델을 비용에 대해 최적화하여 이진 분류 문제에 걸맞게 훈련시키자."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer = tf.keras.optimizers.Adam(epsilon = 0.01),\n    loss = 'binary_crossentropy',\n    metrics = ['binary_accuracy']\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data = ds_valid,\n    epochs = 40\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이 모델은 첫 번째 레슨의 VGG16 모델보다 작다. 16개의 합성곱 레이어를 가진 VGG16에 비해 3개 밖에 없다. 그러나 데이터 셋을 어느 정도 잘 반영한다. 합성곱 레이어를 추가한다면 더욱 효과적일 것이다."},{"metadata":{},"cell_type":"markdown","source":"# 결론  \n이 튜토리얼에서는 어떻게 여러 개의 합성곱 블록으로 이루어진 합성곱 신경망을 만드는지를 살펴보았다."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}