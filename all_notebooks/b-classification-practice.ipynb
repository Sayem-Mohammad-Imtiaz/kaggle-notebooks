{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nIn this code I tried to show you classification types with using sklearn.\n\n1. [Entreing and Normalizing Data](#1)\n1. [Train Test Split](#3)\n1. [Classification Types](#2)\n    * [Logistic Regression Classification](#4)\n    * [KNN](#5)\n    * [SVM](#6)\n    * [Naive Bayes](#7)\n    * [Decision Tree Classification](#8)\n    * [Random Forest Classification](#9)\n    * [Confusion Matrix](#10)\n1. [Conclusion](#11)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv),\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a><br>\n# Entering and Normalizing Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/lower-back-pain-symptoms-datasetlabelled/Dataset_spine.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Class_att=[1 if each ==\"Abnormal\" else 0 for each in data.Class_att]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data.Class_att.values\nx_data=data.drop([\"Class_att\"],axis=1)\n\nx=(x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a><br>\n# Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a><br>\n# Classification Types"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a><br>\n## Logistic Regression Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\n\nprint(\"Logistic Regression score:\",lr.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a><br>\n## KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\n\nprint(\"{} nn score: {}\".format(3,knn.score(x_test,y_test)))\n\nscore_list=[]\n\nfor each in range(1,15):\n    knn=KNeighborsClassifier(n_neighbors=each)\n    knn.fit(x_train,y_train)\n    score_list.append(knn.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k\")\nplt.ylabel(\"Scores\")\nplt.show()\n\nknn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train,y_train)\n\nprint(\"according to the graph the max score value is at k = 7 and score is :\",knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a><br>\n## SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm=SVC(random_state=42)\nsvm.fit(x_train,y_train)\n\nprint(\"svm score:\",svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a><br>\n## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"nb score:\",nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a><br>\n## Decision Tree Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndf=DecisionTreeClassifier(random_state=42)\ndf.fit(x_train,y_train)\n\nprint(\"Desicion Tree Classification score:\",df.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a><br>\n## Random Forest Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(x_train,y_train)\n\nprint(\"Random Forest Classification score:\",rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a><br>\n## Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=rf.predict(x_test)\ny_true=y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a><br>\n# Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_method=[lr,knn,svm,nb,df,rf]\nmethod_acc=[]\nmethod_name=[\"lr\",\"knn\",\"svm\",\"nb\",\"df\",\"rf\"]\n\nfor each in class_method:\n    method_acc.append(each.score(x_test,y_test)*100)\n    \nplt.plot(method_name,method_acc)\nplt.xlabel(\"Methods of Classifiaction\")\nplt.ylabel(\"Accuracies of Methods (%)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In conclusion, we can say that random forest classification is has the greatest accuracy ans svm has the least accuracy. \nIf you want to get more information about classification you can visit: https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}