{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns #for plotting\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#load data\ndata_file = \"../input/StudentsPerformance.csv\"\ndata = pd.read_csv(data_file)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a59ba51cb826d0943d71fa7b26dbc20b359772a"},"cell_type":"markdown","source":"The first thing we look at is the relation between scores(math score, reading score and writing score) over all the categorical dimensions given. From the plots we will try to derive some conclusion."},{"metadata":{"trusted":true,"_uuid":"b0174906e2d109bfcc3fd9fb51603d055aa739b7"},"cell_type":"code","source":"#relationship between scores\nf, axes = plt.subplots(1, 3, figsize=(20, 5), sharex=True)\nsns.scatterplot(x=\"math score\", y=\"reading score\", data=data, ax=axes[0])\nsns.scatterplot(x=\"reading score\", y=\"writing score\", data=data, ax=axes[1])\nsns.scatterplot(x=\"writing score\", y=\"math score\", data=data, ax=axes[2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"519db7c8838f67450f3f9c10e70b91649c669d62"},"cell_type":"markdown","source":"Here we can conlude that the scores are linearly related. However, reading and writing scores show a much higher linear coorealtion than math vs reading and math vs writing.\nLet's analyse the above charts with some dimensions."},{"metadata":{"trusted":true,"_uuid":"e52125a40be9a49d8ec7be17af1c27a8c0e975b7"},"cell_type":"code","source":"#relationship between scores (gender analysis)\nf, axes = plt.subplots(1, 3, figsize=(20, 5), sharex=True)\nsns.scatterplot(x=\"math score\", y=\"reading score\", data=data, ax=axes[0], hue=\"gender\")\nsns.scatterplot(x=\"reading score\", y=\"writing score\", data=data, ax=axes[1], hue=\"gender\")\nsns.scatterplot(x=\"writing score\", y=\"math score\", data=data, ax=axes[2], hue=\"gender\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547e268b870e13284c0c3cb2adfbb0ac403e0bf8"},"cell_type":"markdown","source":"The above charts give a great understanding of the gender vs the scores. Let's try to build a model to predict the gender given the three scores(math, reading and writing).\nWe will build SVM classifier."},{"metadata":{"trusted":true,"_uuid":"f7339a34d18dbed1eaf76f96420956f3571b5cc3"},"cell_type":"code","source":"#prepare X and y\nX = data[['math score', 'writing score', 'reading score']]\nscaler = MinMaxScaler()  # Default behavior is to scale to [0,1]\nX = scaler.fit_transform(X)\ny = data[['gender']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51301ffc14fd3370534a9281e234703c79002934"},"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"626b6f2bd250bb55a1b95b2e39dd4484bbb8f72c"},"cell_type":"markdown","source":"**Linear Kernel**"},{"metadata":{"trusted":true,"_uuid":"7a9ca701c2ef6fbba7a20f5a906a37e01933c8f8"},"cell_type":"code","source":"#training\nfrom sklearn.svm import SVC  \nsvclassifier = SVC(kernel='linear')  \nsvclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c88ae01c92bb95aed857ee9e54530d8dd20d028"},"cell_type":"code","source":"#making predictions\ny_pred = svclassifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"350e56742f0807ede31c4d3f790e5b665c3b95a2"},"cell_type":"code","source":"#evaluating the algorithm\nfrom sklearn.metrics import classification_report, confusion_matrix  \nprint(confusion_matrix(y_test,y_pred))  \nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ebe1eb4997adbe43c29bea8fb0dd5a7e492dbb6"},"cell_type":"markdown","source":"We have good accuracy here. But can we improve the prediction accuracy. Let's use kernel SVM.\n\n**Polynomial Kernel with degrees=2**"},{"metadata":{"trusted":true,"_uuid":"42ad6c0e8ff742989d635fb5d85e0db643895d06"},"cell_type":"code","source":"#training with degree=2\nfrom sklearn.svm import SVC  \nsvclassifier = SVC(kernel='poly', degree=2)  \nsvclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08ef10a54570f6ea3cd6835bd6d6d539630f7636"},"cell_type":"code","source":"#prediction\ny_pred = svclassifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d92bad27e46d69456daadcb66a55aacfb4d18e6"},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d84627d21da30810a7afb9c6bb9a2b8ba71b6f2"},"cell_type":"markdown","source":"**Gausian Kernel**"},{"metadata":{"trusted":true,"_uuid":"9e649b80bb9491107676380a3ab0376f7849171d"},"cell_type":"code","source":"svclassifier = SVC(kernel='rbf')  \nsvclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f1173705ff5cbccafe22c0ac019ba2679490f0f"},"cell_type":"code","source":"y_pred = svclassifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8350aeaf63c6b93abb780aae578a62cfffcfb0ef"},"cell_type":"markdown","source":"**Sigmoid Kernel**"},{"metadata":{"trusted":true,"_uuid":"b51c02e90c208ff8dab086001b5b898c27ca20d1"},"cell_type":"code","source":"svclassifier = SVC(kernel='sigmoid')  \nsvclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9556fd90712cd46b3f50875af02f7987ce86c30"},"cell_type":"code","source":"y_pred = svclassifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00457c9944917573a537851db4119ba99773ac43"},"cell_type":"markdown","source":"**Comparison of the kernels**\n\nWe can see above that the best results are achieved with linear SVM as from the visulations we can infer that the features are linearnly seperable."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}