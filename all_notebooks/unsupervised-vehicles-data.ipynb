{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as pi\nimport seaborn as sn\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import zscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the display layout for the coding environment\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:90% !important; }</style>\"))\n\nsn.set(font_scale=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nprint('Listing files in the folder')\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Set Aanalysis for the Vehicle Data\n- Loading the data set and checking out the dimensions\n- Analysing various elements of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_data = pd.read_csv(\"/kaggle/input/vehicle2/vehicle-2.csv\")\n\nprint(\"\\nDimensions of the data\")\n\nprint(\"Shape of the data :{0}\".format(vehicle_data.shape))\nprint(\"Size of the data :{0}\".format(vehicle_data.size))\nprint(\"nDim of the data :{0}\".format(vehicle_data.ndim))\nprint(\"Shape x * y :{0}\".format(vehicle_data.shape[0]*vehicle_data.shape[1]))\n\nprint(\"\\nData types of the Data\")\nprint(vehicle_data.info())\n\nprint(\"\\nData elements from file\")\nvehicle_data.head(20).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for the missing or the null values\n\n- The numeric columns does not have invalid characters only the NaN "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the types and the missing values of the data\n\nprint(\"\\nList of Null values in each column\")\nprint(vehicle_data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace the null value with the median\n\nvehicle_modified_data = vehicle_data.drop(['class'],axis=1)\n\nvehicle_modified_data=vehicle_modified_data.apply(lambda x: x.fillna(x.median()),axis=0)\n\nvehicle_modified_data['type'] = vehicle_data['class']\n\nprint(\"\\nVerify all the null values are replaced\")\nprint(vehicle_modified_data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Dataset Analysis\n\n- Large number of attributes in the dataset, total 19 attributes\n- All the attributes are linear in nature\n- Data types for all attributes looks fine\n- All the attribute are relevant, no need to drop any of the attributes\n- **'cartype'** is the Target variable, will do detailed analysis on that attribute later\n\n#### Complexties in the data set\n\n- Values of the attributes are in different units, hence needs to be converted to the common type by preprocessors\n- Some of the attributes are seem to be related or dependent, identifying the column might pose a challenge\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define functions to identify the number of outliers\n\ndef Identify_Outliers(data_column):\n    \n    dataFrame = pd.DataFrame(data_column)\n    \n    Quar1 = dataFrame.quantile(0.25)  \n    Quar3 = dataFrame.quantile(0.75)  \n    \n    IQR = Quar3 - Quar1\n    \n    return ((dataFrame < (Quar1-1.5*IQR)) | (dataFrame> (Quar3+1.5*IQR))).sum()\n    \n\nprint(\"\\n Number of outliers in each attribute \\n\")\nfor (columnName, columnData) in vehicle_modified_data.iteritems(): \n    print(Identify_Outliers(vehicle_modified_data[columnName]))\n\ndata_columns = vehicle_modified_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the columns with higher amount of outliers \n\nfig, axis = plt.subplots(2, 2, figsize=(25, 12), sharex=False)\n\naxis[0,0].set_title(\"pr.axis_aspect_ratio\")\nsn.boxplot(vehicle_modified_data[\"pr.axis_aspect_ratio\"],color='green',orient='h',ax=axis[0,0]);\n\naxis[0,1].set_title(\"max.length_aspect_ratio\")\nsn.boxplot(vehicle_modified_data[\"max.length_aspect_ratio\"],color='green',orient='h',ax=axis[0,1])\n\naxis[1,0].set_title(\"scaled_radius_of_gyration.1\")\nsn.boxplot(vehicle_modified_data[\"scaled_radius_of_gyration.1\"],color='orange',orient='h',ax=axis[1,0])\n\naxis[1,1].set_title(\"skewness_about\")\nsn.boxplot(vehicle_modified_data[\"skewness_about\"],color='orange',orient='h',ax=axis[1,1])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Outliers Analysis\n\n- The oultiers on these datasets are nominal as compared to the number of data\n- SVM is sensitivie to outlier, but with the number of outliers it should not have more impact on the analysis\n- Not correcting the outliers as these values have less unique values, modifiying the outliers will impact the data quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"#col_median =vehicle_modified_data['max.length_aspect_ratio'].median()\n#vehicle_modified_data['max.length_aspect_ratio']=pi.where(vehicle_modified_data['max.length_aspect_ratio']>13 ,13,vehicle_modified_data['max.length_aspect_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear info for the data\nprint('\\nFive point summary for the attributes')\nvehicle_modified_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Data: ', vehicle_modified_data.shape)\n\nprint('\\nMedian for the data')\nprint(vehicle_modified_data.median())\n\nprint('\\nMode for the data')\nprint(vehicle_modified_data.mode())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_columns =  vehicle_modified_data.columns\n\nprint('\\nSkewing of the data\\n')\n\nfor col_name in vehicle_columns:\n    \n    if(col_name == 'type'):\n        continue\n    print('{0} Parameter is Right Skewed:   {1}'.format(col_name,vehicle_modified_data[col_name].mean() > vehicle_modified_data[col_name].median()))\n    \nprint('\\n{0} Parameter is Left Skewed: {1}'.format('elongatedness',vehicle_modified_data['elongatedness'].mean() < vehicle_modified_data['elongatedness'].median()))\nprint('{0} Parameter is Left Skewed: {1}'.format('hollows_ratio',vehicle_modified_data['hollows_ratio'].mean() < vehicle_modified_data['hollows_ratio'].median()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Five point analysis\n\n- The data seemed to be skewed mostly on the right, the visualisation will give clue on the distribution\n- Only the elongatedness and hollows_ratio is skewed to the left, it could be because of the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Display_BoxPlot(col,axis_rad,color):\n    axis_rad.set_title(col)\n    sn.distplot(vehicle_modified_data[col],color=color,ax=axis_rad);\n\nsn.set(font_scale=1.5)\n\nfig, axis = plt.subplots(4, 4, figsize=(30, 35), sharex=False)\n\nDisplay_BoxPlot(vehicle_columns[0],axis[0,0],'green')\nDisplay_BoxPlot(vehicle_columns[1],axis[0,1],'green')\nDisplay_BoxPlot(vehicle_columns[2],axis[0,2],'green')\nDisplay_BoxPlot(vehicle_columns[3],axis[0,3],'green')\n\nDisplay_BoxPlot(vehicle_columns[4],axis[1,0],'orange')\nDisplay_BoxPlot(vehicle_columns[5],axis[1,1],'orange')\nDisplay_BoxPlot(vehicle_columns[6],axis[1,2],'orange')\nDisplay_BoxPlot(vehicle_columns[7],axis[1,3],'orange')\n\nDisplay_BoxPlot(vehicle_columns[8],axis[2,0],'red')\nDisplay_BoxPlot(vehicle_columns[9],axis[2,1],'red')\nDisplay_BoxPlot(vehicle_columns[10],axis[2,2],'red')\nDisplay_BoxPlot(vehicle_columns[11],axis[2,3],'red')\n\nDisplay_BoxPlot(vehicle_columns[12],axis[3,0],'blue')\nDisplay_BoxPlot(vehicle_columns[13],axis[3,1],'blue')\nDisplay_BoxPlot(vehicle_columns[14],axis[3,2],'blue')\nDisplay_BoxPlot(vehicle_columns[15],axis[3,3],'blue')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Univariate analysis\n\n- The distribution looks good for the compactness, radius_ratio, skweness_about.1, scaled_radius_of_gyration\n- Scatter_ratio and the pr.axis_rectangularity shows a double distribution, non-normal"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(1, 2, figsize=(20, 7), sharex=False)\n\nDisplay_BoxPlot(vehicle_columns[16],axis[0],'green')\nDisplay_BoxPlot(vehicle_columns[17],axis[1],'green')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nNumber of Unique values in each attribute\")\nprint(vehicle_modified_data.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nTotal values in categorical variables\" )\nprint(vehicle_modified_data['type'].value_counts())\nprint(vehicle_modified_data['pr.axis_rectangularity'].value_counts())\n\nfig = plt.subplots(figsize=(8, 5), sharex=False)\n\nchart=sn.countplot(y='type',data=vehicle_modified_data);\n\nplt.show()\n\n\nfig = plt.subplots(figsize=(8, 5), sharex=False)\n\nchart=sn.countplot(y='pr.axis_rectangularity',data=vehicle_modified_data);\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the Correlation between the variables\n\nfig, axis = plt.subplots(2, 1, figsize=(30, 30), sharex=False)\n\nsn.set(font_scale=1.2)\n\nsn.heatmap(vehicle_modified_data.corr(), mask=pi.triu(vehicle_modified_data.corr()),\n           annot_kws={\"size\": 14}, annot=True,fmt='.3f',ax=axis[0],cmap='BrBG');\n\ncorr_thresold=0.4\nvehicle_corr_threshold=vehicle_modified_data.corr()>corr_thresold\n\nsn.heatmap(vehicle_corr_threshold,mask=pi.triu(vehicle_modified_data.corr()), annot_kws={\"size\": 16},annot=True,fmt='d',ax=axis[1]);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.set(font_scale=1.5)\n\n\n#not_null_columns = ['circularity','distance_circularity','radius_ratio','pr.axis_aspect_ratio','scatter_ratio',\n#                   'elongatedness','pr.axis_rectangularity','scaled_variance','scaled_variance.1','scaled_radius_of_gyration',\n#                    'scaled_radius_of_gyration.1','skewness_about','skewness_about.1','skewness_about.2']\n\nplot_vars=['compactness','circularity','distance_circularity','radius_ratio','scatter_ratio','pr.axis_rectangularity',\n           'max.length_rectangularity','scaled_variance','scaled_variance.1','scaled_radius_of_gyration']\n\ngraph=sn.pairplot(vehicle_modified_data,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='type',diag_kind='kde');\n\ngraph.fig.set_size_inches(35,35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary of analysis for the correlated  variables\n- scaled_variance is strongly related to scatter_ratio which show very positive dependency\n- radius ratio seems to be strongly clouded toghether except few outlier, potentialy not to be included part of the feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_vars=['pr.axis_aspect_ratio','max.length_aspect_ratio','elongatedness','scaled_radius_of_gyration.1','skewness_about','skewness_about.1',\n           'skewness_about.2','hollows_ratio']\n\ngraph=sn.pairplot(vehicle_modified_data,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='type',diag_kind='kde');\n\ngraph.fig.set_size_inches(35,35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary of the less correlated variables\n\n- elongatedness and the max_length_aspect_ratio seems to have the similarities of higher the corresponding values lower the values\n- The scaled_radius_of_gyration.1 is showing negative correlation for the hollows_ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(2, 3, figsize=(20, 12), sharex=False)\n\n\nsn.swarmplot(data=vehicle_modified_data,y='scatter_ratio',x='type',hue='type',ax=axis[0,0]);\nsn.swarmplot(data=vehicle_modified_data,y='max.length_rectangularity',x='type',hue='type',ax=axis[0,1]);\nsn.swarmplot(data=vehicle_modified_data,y='scaled_radius_of_gyration',x='type',hue='type',ax=axis[0,2]);\n\n\nsn.violinplot(data=vehicle_modified_data,y='distance_circularity',x='type',hue='type',ax=axis[1,0]);\nsn.swarmplot(data=vehicle_modified_data,y='radius_ratio',x='type',hue='type',ax=axis[1,1]);\nsn.violinplot(data=vehicle_modified_data,y='circularity',x='type',hue='type',ax=axis[1,2]);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data for the analysis\n\n- Making the values to the common units\n- Not removing any of the attributes as of now for the model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"axis_x = vehicle_modified_data.drop(['type'],axis=1)\naxis_y = vehicle_modified_data['type']\n\naxis_x_scaled=axis_x.apply(zscore)\n\nprint(f\"\\nScales Axis Shape: {axis_x_scaled.shape}\")\naxis_x_scaled.head(10).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cluster Group for identying catergories\n- Kmeans cluser for identifying different categories in the data, just a sampling exercise"},{"metadata":{"trusted":true},"cell_type":"code","source":"axis_x_copy=axis_x_scaled.copy()\n\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans \n\nclusters=range(1,19)\nmeanDistortions=[]\n\nfor k in clusters:\n    model=KMeans(n_clusters=k)\n    model.fit(axis_x_copy)\n    prediction=model.predict(axis_x_copy)\n    meanDistortions.append(sum(pi.min(cdist(axis_x_copy, model.cluster_centers_, 'euclidean'), axis=1)) / axis_x_copy.shape[0])\n\n\nplt.plot(clusters, meanDistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average distortion')\nplt.title('Selecting k with the Elbow Method')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us first start with K = 5\nkmeans_model=KMeans(5)\nkmeans_model.fit(axis_x_copy)\naxis_x_predicted=kmeans_model.predict(axis_x_copy)\n\naxis_x_copy['Group'] = axis_x_predicted\naxis_x_copy.groupby(['Group']).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identifying the features for the model through the coeff\n- the pairplot visualisation helped in analysing the features\n- In order to identify the feature importance, using the Coeff function and filtering features based on the values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm \n\n#spliting the data into 70/30\nx_train_fearure,x_test_feature,y_train_feature,y_test_feature = train_test_split(axis_x_scaled,axis_y,test_size=0.3,random_state=80)\n\nsvm_model = svm.SVC(kernel='linear')\nsvm_model.fit(x_train_fearure,y_train_feature)\n\nceof_data=pd.concat([pd.Series(svm_model.coef_[0]),pd.Series(x_train_fearure.columns)],axis=1)\n\nprint(\"\\n Feature coeff\\n\")\nprint(ceof_data.sort_values(by=[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the Feature selection\n\n- Removing the features which are less than -1.0, this method helps to remove some of un related features"},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data to training and testing\n\n- Normal approach to split it into 70:30, the same will be following"},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping the variable while are not identified independent\naxis_x_drop_scaled = axis_x_scaled.drop(['radius_ratio','hollows_ratio','elongatedness'],axis=1)\n\n#spliting the data into 70/30\nx_train_scale,x_test_scale,y_train_scale,y_test_scale = train_test_split(axis_x_drop_scaled,axis_y,test_size=0.3,random_state=120)\n\nprint(f\"\\nShape of the final Data: {axis_x_drop_scaled.shape}\\n\")\naxis_x_drop_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nSummary of the training and test data \\n')\nveh_car = len(vehicle_modified_data.loc[vehicle_modified_data['type'] == 'car'])\nveh_van = len(vehicle_modified_data.loc[vehicle_modified_data['type'] == 'van'])\nveh_bus = len(vehicle_modified_data.loc[vehicle_modified_data['type'] == 'bus'])\n\nprint (f\"{len(x_train_scale)/len(axis_x_drop_scaled)*100} % data in the Training\")\nprint (f\"{len(x_test_scale)/len(axis_x_drop_scaled)*100} % data in the Testing\")\n\nprint(\"\\nPercent of the Vehicle types in Total\")\nprint (f\"Car: {veh_car} in total {len(axis_y)} {veh_car/len(axis_y)*100}%\")\nprint (f\"Van: {veh_van} in total {len(axis_y)} {veh_van/len(axis_y)*100}%\")\nprint (f\"Bus: {veh_bus} in total {len(axis_y)} {veh_bus/len(axis_y)*100}%\")\n\nprint(\"\\nPercent of the Vehicle types in Training data\")\nprint (f\"Car: {len(y_train_scale.loc[y_train_scale[:]=='car'])} in total {len(y_train_scale)} {len(y_train_scale.loc[y_train_scale[:]=='car'])/len(y_train_scale)*100}%\")\nprint (f\"Van: {len(y_train_scale.loc[y_train_scale[:]=='van'])} in total {len(y_train_scale)} {len(y_train_scale.loc[y_train_scale[:]=='van'])/len(y_train_scale)*100}%\")\nprint (f\"Bus: {len(y_train_scale.loc[y_train_scale[:]=='bus'])} in total {len(y_train_scale)} {len(y_train_scale.loc[y_train_scale[:]=='bus'])/len(y_train_scale)*100}%\")\n\nprint(\"\\nPercent of the Vehicle types in Test data\")\nprint (f\"Car: {len(y_test_scale.loc[y_test_scale[:]=='car'])} in total {len(y_test_scale)} {len(y_test_scale.loc[y_test_scale[:]=='car'])/len(y_test_scale)*100}%\")\nprint (f\"Van: {len(y_test_scale.loc[y_test_scale[:]=='van'])} in total {len(y_test_scale)} {len(y_test_scale.loc[y_test_scale[:]=='van'])/len(y_test_scale)*100}%\")\nprint (f\"Bus: {len(y_test_scale.loc[y_test_scale[:]=='bus'])} in total {len(y_test_scale)} {len(y_test_scale.loc[y_test_scale[:]=='bus'])/len(y_test_scale)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support vector machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm_model = svm.SVC(gamma=.2,C=0.8)\nsvm_model.fit(x_train_scale,y_train_scale)\n\nsvm_predict_train = svm_model.predict(x_train_scale)\nsvm_predict_test = svm_model.predict(x_test_scale)\n\nsvm_train_accuracy_normal=metrics.accuracy_score(y_train_scale,svm_predict_train);\nsvm_test_accuracy_normal=metrics.accuracy_score(y_test_scale,svm_predict_test);\n\nprint(\"\\nAccuracy of the SVM Model\\n\")\nprint(f\"Train Accuracy: {svm_train_accuracy_normal}\")\nprint(f\"Test  Accuracy: {svm_test_accuracy_normal}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nkfoldcross = KFold(n_splits=12, random_state=130,shuffle=True)\nscore = cross_val_score(svm_model, axis_x_scaled, axis_y, cv=kfoldcross)\n\nkfold_score_normal=score.mean()\nprint(\"Kfold Crossvalidation score\")\nprint(f\"Accuracy: {kfold_score_normal}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA - Analysis for the attributes\n- Preparing the PCA with all the attributes\n- Based on the variance in the data choosing the right principal components for the analysis\n- Using the original data and scaling with the standard scaler for the PCA analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\ncolumn_names = axis_x.columns\n\n# pre-processing the split data using the preprocesser technique, standard processor\nscaler_model = preprocessing.StandardScaler()\nscaled_data= scaler_model.fit_transform(axis_x)\n\naxis_x_standardscaled = pd.DataFrame(scaled_data,columns=column_names)\naxis_x_standardscaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the PCA for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ncomponents_selected=18\npca = PCA(n_components=components_selected)\npca.fit(axis_x_standardscaled)\n\nprint(f\"\\n The explained variance based on the {components_selected} components\\n\")\nprint(pca.explained_variance_ratio_)\n\n\ncumm_var_ratio = pd.Series(pi.cumsum(pca.explained_variance_ratio_))\nprint(f\"\\nThe cummulative variance ratio base on the {components_selected} components\\n\")\nprint(cumm_var_ratio)\n\n\nplt.bar(list(range(1,19)),pca.explained_variance_ratio_,alpha=1, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()\n\nplt.step(list(range(1,19)),pi.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of the PCA analysis (Dimensionality Reduction)\n\n- Evaluation based on the 95% cum variance explained, the eigen value is 7.\n- The 7 dimensions is very reasonable to explain the 95% over the original data"},{"metadata":{"trusted":true},"cell_type":"code","source":"components_selected=7\npca = PCA(n_components=components_selected)\npca.fit(axis_x_standardscaled)\n\nprint(f\"\\n The explained variance based on the {components_selected} components\\n\")\nprint(pca.explained_variance_ratio_)\n\n\ncumm_var_ratio = pd.Series(pi.cumsum(pca.explained_variance_ratio_))\nprint(f\"\\nThe cummulative variance ratio base on the {components_selected} components\\n\")\nprint(cumm_var_ratio)\n\n\naxis_x_transformed = pd.DataFrame(pca.transform(axis_x_standardscaled))\naxis_x_transformed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph=sn.pairplot(axis_x_transformed,kind='scatter',diag_kind='kde');\n\ngraph.fig.set_size_inches(35,35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of the the Pair plot\n- The PCA component helped to identify 7 columns for the model training\n- The column 0,1,3,4,5 has almost no correlation with the other column, which seems to be good selection for the model training\n- The columnt although it seems to have less coorelation, the x value is close to 0 for all the value of y. With some values is those are some kind of outliers possibly\n- The column 0 shows a double peak possible 2 clusters of the data overlapping, culter analysis will reveal some picture. Not part of scope now"},{"metadata":{},"cell_type":"markdown","source":"## Model Tuning\n- Based on the the data dervied by the PCA, training the models again and check the accuracy\n- The scaled data we have used for transform needs to be split"},{"metadata":{"trusted":true},"cell_type":"code","source":"#spliting the data transformed using the PCA\n\n\nx_train_trans,x_test_trans,y_train_trans,y_test_trans = train_test_split(axis_x_transformed,axis_y,test_size=0.3,random_state=160)\n\nprint('\\nSummary of the training and test data \\n')\nveh_car = len(vehicle_modified_data.loc[vehicle_modified_data['type'] == 'car'])\nveh_van = len(vehicle_modified_data.loc[vehicle_modified_data['type'] == 'van'])\nveh_bus = len(vehicle_modified_data.loc[vehicle_modified_data['type'] == 'bus'])\n\nprint (f\"{len(x_train_trans)/len(axis_x_transformed)*100} % data in the Training\")\nprint (f\"{len(x_test_trans)/len(axis_x_transformed)*100} % data in the Testing\")\n\nprint(\"\\nPercent of the Vehicle types in Training data\")\nprint (f\"Car: {len(y_train_trans.loc[y_train_trans[:]=='car'])} in total {len(y_train_trans)} {len(y_train_trans.loc[y_train_trans[:]=='car'])/len(y_train_trans)*100}%\")\nprint (f\"Van: {len(y_train_trans.loc[y_train_trans[:]=='van'])} in total {len(y_train_trans)} {len(y_train_trans.loc[y_train_trans[:]=='van'])/len(y_train_trans)*100}%\")\nprint (f\"Bus: {len(y_train_trans.loc[y_train_trans[:]=='bus'])} in total {len(y_train_trans)} {len(y_train_trans.loc[y_train_trans[:]=='bus'])/len(y_train_trans)*100}%\")\n\nprint(\"\\nPercent of the Vehicle types in Test data\")\nprint (f\"Car: {len(y_test_trans.loc[y_test_trans[:]=='car'])} in total {len(y_test_trans)} {len(y_test_trans.loc[y_test_trans[:]=='car'])/len(y_test_trans)*100}%\")\nprint (f\"Van: {len(y_test_trans.loc[y_test_trans[:]=='van'])} in total {len(y_test_trans)} {len(y_test_trans.loc[y_test_trans[:]=='van'])/len(y_test_trans)*100}%\")\nprint (f\"Bus: {len(y_test_trans.loc[y_test_trans[:]=='bus'])} in total {len(y_test_trans)} {len(y_test_trans.loc[y_test_trans[:]=='bus'])/len(y_test_trans)*100}%\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm_model = svm.SVC(gamma=.2,C=1)\nsvm_model.fit(x_train_trans,y_train_trans)\n\nsvm_predict_train_pca = svm_model.predict(x_train_trans)\nsvm_predict_test_pca = svm_model.predict(x_test_trans)\n\nsvm_train_accuracy_pca = metrics.accuracy_score(y_train_trans,svm_predict_train_pca)\nsvm_test_accuracy_pca = metrics.accuracy_score(y_test_trans,svm_predict_test_pca)\n\nprint(\"\\nAccuracy of the SVM Model\\n\")\nprint(f\"Train Accuracy: {svm_train_accuracy_pca}\")\nprint(f\"Test  Accuracy: {svm_test_accuracy_pca}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nkfoldcross = KFold(n_splits=12, random_state=190,shuffle=True)\nscore = cross_val_score(svm_model, axis_x_transformed, axis_y, cv=kfoldcross)\n\nkfold_score_pca=score.mean()\nprint(\"Kfold Crossvalidation score\")\nprint(f\"Accuracy: {kfold_score_pca}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Display_Barplot (title,x_axis, y_axis, data,palette, axis):\n    \n    chart=sn.barplot(y=y_axis,x=x_axis,data=data,palette=palette,ax=axis)\n    for patch in chart.patches:\n        chart.annotate(format(patch.get_height(), '.5f'), \n                       (patch.get_x() + patch.get_width() / 2., patch.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, 9), textcoords = 'offset points')\n    chart.set_title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_plot = pd.DataFrame((['SVM',svm_train_accuracy_normal,svm_test_accuracy_normal,kfold_score_normal],\n                                 ['SVM After PCA',svm_train_accuracy_pca,svm_test_accuracy_pca,kfold_score_pca]),\n                         columns=['Model','train_accuracy','test_accuracy','kfold_score'])\n\n\nsn.set(font_scale=1.5)\nfig, axis = plt.subplots(1, 3, figsize=(30, 10), sharex=False)\n\n\nDisplay_Barplot('Train Accuracy Comparison','Model','train_accuracy',data_plot,'Spectral',axis[0])\nDisplay_Barplot('Test Accuracy Comparison','Model','test_accuracy',data_plot,'Spectral',axis[1])\nDisplay_Barplot('Train Accuracy Comparison','Model','kfold_score',data_plot,'icefire',axis[2])\n\nplt.show()\n\ndata_plot.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary of the Analysis\n\n- The above plot shows the variation in the training and the test data, the model seems to be more fit on the training data than test data\n- The feature selected before PCA was 15 and after PCA was 7. but the accuracy score has a very less difference. This results in less computation can acheive a good accuracy\n- The feature selection after the PCA also has the same difference in the test data.\n- Kfold fold variance also showed a decrease in score after the PCA\n- The decrease is accuracy postively mean the model is getting better, no overfit as well the number of features we have used has greatly reduced which mean much lesser computation\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}