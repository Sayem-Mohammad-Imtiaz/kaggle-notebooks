{"cells":[{"metadata":{"_uuid":"0adc7fca81c2396e9e578d6fdb3cd3866bd01bb5"},"cell_type":"markdown","source":"# H1-B VISA "},{"metadata":{"_uuid":"c8479b4e24fd8097742db1dd3508ecece751dc63"},"cell_type":"markdown","source":"A COMPREHENSIVE ANALYSIS ON H1-B VISAS - Saiteja Nakka\n- To know about the data visit https://storage.googleapis.com/kaggle-datasets/11361/15737/h1b-2017-metadata.pdf?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1539822929&Signature=sd7i4%2BNuZyrh5E19BShsF8wGJB08CNYsf%2BvX4%2FAufvXe57w9dK7uVIV0uwqZI6ZKRNp56amXAAZsKciXmABlbuZ4rmRanmNjhM6JP3Iuv4UyIa7IvKJKnVaSq%2F8ppYYZMVveVivH3yVJn3eWfNhvhrBdShRC%2BcG4j8KuvCdpO70wzUX7bNvfA2McsSp5zMIbYA9mtu0Sqk6gtAuSmgfY9Q7e5U1wRSnKKEJzINXIVUtShZPZ3WBJyf8Yypxoj7QfXNzXMUXtUSMSTY45UYIvVtoRTs4jucgpMptXA5pvlGmIDShct5qgw%2Fn83%2Bv3Gm8J%2BgXNxMkv5AQw6kavgmYQmA%3D%3D"},{"metadata":{"trusted":false,"_uuid":"8267208522732cb8ff69d48575bea092bbe15e74"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\ninit_notebook_mode(connected=True)\nfrom plotly import tools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7d5c16c00efc2ccecf9bcb8a17c170704711dc27"},"cell_type":"code","source":"df = pd.read_csv(\"../input/H-1B_Disclosure_Data_FY17.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ce503adde233d00b615e24607af014b3de2dc55d"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef7d76b41c0af1d43411d6481ac79afb67a457af"},"cell_type":"markdown","source":"## We focus on H1-B visa and Employers from USA only"},{"metadata":{"trusted":false,"_uuid":"677463213dd3061d28285cf7a24d48c2e1a6ab5d"},"cell_type":"code","source":"df.VISA_CLASS.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a29c382e1bf7202ceb2ee417c8f2cbb783bab8a4"},"cell_type":"code","source":"df.EMPLOYER_COUNTRY.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"96df63d40de7f8877dc1e8b8eae11dd173300f98"},"cell_type":"code","source":"df = df[df.VISA_CLASS == 'H-1B']\ndf= df[df.EMPLOYER_COUNTRY == 'UNITED STATES OF AMERICA']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b891da3f2d68e7dd8241b7d5d3f5a8df3601f91"},"cell_type":"markdown","source":"#### Number of unique values each column has"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"ddb0a353586fc12561feda6d9fa3d53dce696acf"},"cell_type":"code","source":"df.apply(lambda x:len(x.unique()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b434de3a6d7974eb793ea99ce5a0160e71198d9a"},"cell_type":"markdown","source":"#### Columns which have missing values in it"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"fb422f4cb6a70466860b0e62c55f64d54b80394e"},"cell_type":"code","source":"df.isnull().sum()[df.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68d4c3bdce7ff762d1a3898193c1e569801ebb4b"},"cell_type":"markdown","source":"### DROP the useless columns.\n- By examining the above two columns we can remove all the useless columns.\n- Since we have a lot of those columns to drop, I instead selected the ones I need"},{"metadata":{"trusted":false,"_uuid":"27a5362848eea3d25859475347219375bc50abfa"},"cell_type":"code","source":"to_select = ['CASE_STATUS', 'EMPLOYMENT_START_DATE','EMPLOYER_NAME', 'EMPLOYER_STATE','JOB_TITLE', 'SOC_NAME','FULL_TIME_POSITION',\n            'PREVAILING_WAGE','PW_UNIT_OF_PAY','WORKSITE_STATE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"130d61e85942a78cd93370d5629ef4643c8fc21a"},"cell_type":"code","source":"df = df[to_select]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5f8974c24d41776cc01bcb83830a4fea7e19ffbb"},"cell_type":"code","source":"df.isnull().sum()[df.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edccd6dde4fb87cd96c04e73098678c53f8981e9"},"cell_type":"markdown","source":"## Dealing with missing values\n- The following method is same as dropna."},{"metadata":{"trusted":false,"_uuid":"b7c3fe0ec689fd8edf66494e8364c9d6ed85113b"},"cell_type":"code","source":"df = df[df['EMPLOYMENT_START_DATE'].notnull()]\ndf = df[df['JOB_TITLE'].notnull()]\ndf = df[df['SOC_NAME'].notnull()]\ndf = df[df['FULL_TIME_POSITION'].notnull()]\ndf = df[df['PW_UNIT_OF_PAY'].notnull()]\ndf = df[df['WORKSITE_STATE'].notnull()]\ndf = df[df['EMPLOYER_NAME'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87df6b7483c4d9fd26203bcd4c584af3bbb66df"},"cell_type":"markdown","source":"###### We got rid of null values"},{"metadata":{"trusted":false,"_uuid":"b8397027026c22a5ba5ab6f3fee14107e4794316"},"cell_type":"code","source":"df.isnull().sum()[df.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"44a200d6f3eae6c8f4cf968f2c5da45b9405de56"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b9794ebae3615b328d4448d8d8a00b8322dc97"},"cell_type":"markdown","source":"### Convert the EMPLOYMENT_START_DATE to pandas date time format"},{"metadata":{"trusted":false,"_uuid":"ed6a471ea6f6d5e6e42ca20e89d22d8bd3f5c5dc"},"cell_type":"code","source":"df['EMPLOYMENT_START_DATE'] = pd.to_datetime(df['EMPLOYMENT_START_DATE'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb0c83bc5e3873f4ba6a3dd894388d5cdc9357bf"},"cell_type":"markdown","source":"#### The following cell shows us how the wage is varied.\n- You might notice some abornmal values like the max hourly pay as 2017,143166 etc etc.\n- These are called the outliers and our data has a lot of them"},{"metadata":{"trusted":false,"_uuid":"f51930fd760dbfa5ff04280f45803b7dc20fa3d3"},"cell_type":"code","source":"df.groupby(['FULL_TIME_POSITION','PW_UNIT_OF_PAY']).describe()['PREVAILING_WAGE']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e0fe5bc3116907f4abd1fe63d859b99be2b82f9"},"cell_type":"markdown","source":"### To make our analysis easy lets first convert the Monthly, Weekly and Bi-weekly pay to Annual pay\n- The hourly pay conversion takes a lot of time. So, i have done in following stages after outliers were removed.\n- Montly pay is multiplied by 12. ( As we have 12 months in a year)\n- Weekly pay is multiplied by 48. ( Even though we have 52 weeks per year, I felt this might be better)\n- Bi-Weekly pay is multiplied by 24. ( As we have two bi-weeks in a month)"},{"metadata":{"trusted":false,"_uuid":"f55dc839964a1c93dff20c9b940c1ce358876bef"},"cell_type":"code","source":"for i in df.index:   \n        if df.loc[i,'PW_UNIT_OF_PAY'] == 'Month':\n            df.loc[i,'PREVAILING_WAGE'] = df.loc[i,'PREVAILING_WAGE'] * 12\n        if df.loc[i,'PW_UNIT_OF_PAY'] == 'Week':\n            df.loc[i,'PREVAILING_WAGE'] = df.loc[i,'PREVAILING_WAGE'] * 48\n        if df.loc[i,'PW_UNIT_OF_PAY'] == 'Bi-Weekly':\n            df.loc[i,'PREVAILING_WAGE'] = df.loc[i,'PREVAILING_WAGE'] * 24","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef03fa4a16b9c9620b2a628b1a57ecccb55ebced"},"cell_type":"markdown","source":"#### Replace the names bi-weekly, month and week by year."},{"metadata":{"trusted":false,"_uuid":"5c971536413abd2fc418cf003a9873589eb6e75f"},"cell_type":"code","source":"df.PW_UNIT_OF_PAY.replace(['Bi-Weekly','Month','Week'],['Year','Year','Year'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b90a90c9659e2c8f6ba634bb2fe2aeb10586ff8"},"cell_type":"markdown","source":"#### Checking again.\n- Now you that we have unit of pay in hour and year only."},{"metadata":{"trusted":false,"_uuid":"b256cee62f007d45c10d4e702bacc8d96f19713d"},"cell_type":"code","source":"df.groupby(['FULL_TIME_POSITION','PW_UNIT_OF_PAY']).describe()['PREVAILING_WAGE']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15f7eccae67b773b1b1b5d9f3a96c658364937c6"},"cell_type":"markdown","source":"#### Creating a new dummy column\n- As, we are going to deal with a lot of groupby methods below, it will be easy for us if we have a count column."},{"metadata":{"trusted":false,"_uuid":"352dc58fff6075dd5b5d9711c0276bf1f67fd1de"},"cell_type":"code","source":"df['countvar'] = 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e44be894bb13947fe45130987e8985e5bb370d0a"},"cell_type":"markdown","source":"### Top Employers sponsoring H1-B's\n- The plots are made using plotly which are interactive. So, you can hover over the plot to know more details"},{"metadata":{"trusted":false,"_uuid":"db51c52abff043c8cfbffd5352d27bd62c3cce3c"},"cell_type":"code","source":"dftop = df.groupby('EMPLOYER_NAME',as_index=False).count()\ndftop = dftop.sort_values('countvar',ascending= False)[['EMPLOYER_NAME','countvar']][0:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e844a739de0e355709860cc20414f617dd8430c3"},"cell_type":"code","source":"t1 = go.Bar(x=dftop.EMPLOYER_NAME.values,y=dftop.countvar.values,name='top30')\nlayout = go.Layout(dict(title= \"TOP EMPLOYERS SPONSORING\",yaxis=dict(title=\"Num of applications\")))\ndata = [t1]\nfig =go.Figure(data,layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddfdfb5b5d4e9b7e1b3ffac7e1bf1f16da58d8ea"},"cell_type":"markdown","source":"### Top Employers and its Case status bar."},{"metadata":{"trusted":false,"_uuid":"f2a84bc733a061ce00f42b7914b38a0eac51e319"},"cell_type":"code","source":"dftop1 = df.groupby(['EMPLOYER_NAME','CASE_STATUS'],as_index=False).count()\ndftop1=dftop1[dftop1.EMPLOYER_NAME.isin(dftop.EMPLOYER_NAME)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d453ff650e6fde57269ee90c84709659ccb115a6"},"cell_type":"code","source":"t1 = go.Bar(x=dftop1[dftop1.CASE_STATUS == 'CERTIFIED'].sort_values('countvar',ascending= False)['EMPLOYER_NAME'].values,y=dftop1[dftop1.CASE_STATUS == 'CERTIFIED'].sort_values('countvar',ascending= False)['countvar'].values,name='CERTIFIED')\nt2 = go.Bar(x=dftop1[dftop1.CASE_STATUS == 'CERTIFIED-WITHDRAWN'].sort_values('countvar',ascending= False)['EMPLOYER_NAME'].values,y=dftop1[dftop1.CASE_STATUS == 'CERTIFIED-WITHDRAWN'].sort_values('countvar',ascending= False)['countvar'].values,name='CERTIFIED-WITHDRAWN')\nt3 = go.Bar(x=dftop1[dftop1.CASE_STATUS == 'DENIED'].sort_values('countvar',ascending= False)['EMPLOYER_NAME'].values,y=dftop1[dftop1.CASE_STATUS == 'DENIED'].sort_values('countvar',ascending= False)['countvar'].values,name='DENIED')\nt4 = go.Bar(x=dftop1[dftop1.CASE_STATUS == 'WITHDRAWN'].sort_values('countvar',ascending= False)['EMPLOYER_NAME'].values,y=dftop1[dftop1.CASE_STATUS == 'WITHDRAWN'].sort_values('countvar',ascending= False)['countvar'].values,name='WITHDRAWN')\n\ndata = [t1,t2,t3,t4]\nlayout = go.Layout(\n    barmode='stack'\n)\n\nfig =go.Figure(data,layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e9bfa49b0fd6d2cc4060648eeddb64a26fb54cc"},"cell_type":"markdown","source":"# Number of Applications per State.\n- Barplot and Choropleth graph"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"d8db21bc87068ac62104eef3cb0236c32f953167"},"cell_type":"code","source":"dfempst = df.groupby('EMPLOYER_STATE',as_index=False).count()[['EMPLOYER_STATE','countvar']].sort_values('countvar',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"27c364acf7593d205a81f3271dba8a68cc06025c"},"cell_type":"code","source":"t1 = go.Bar(x=dfempst.EMPLOYER_STATE.values,y=dfempst.countvar.values,name='Employerstate')\nlayout = go.Layout(dict(title= \"NUMBER OF APPLICATIONS PER STATE\",xaxis=dict(title=\"STATES\"),yaxis=dict(title=\"Num of applications\")))\ndata = [t1]\nfig =go.Figure(data,layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c970a4501c71fa1e7d5574ba323cb75466df094b"},"cell_type":"code","source":"data=[dict(\n    type='choropleth',\n    locations = dfempst.EMPLOYER_STATE,\n    z = dfempst.countvar,\n    locationmode = 'USA-states',marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Number of applications\")\n)]\nlayout= dict(title=\"2011-2018 H1B VISA APPLICATIONS ( EMPLOYER STATE)\",geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\nfig = dict( data=data, layout=layout )\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1269044c59af29cedbba8810b6ca43e56c567332"},"cell_type":"markdown","source":"## Top 20 Job titles"},{"metadata":{"trusted":false,"_uuid":"68d9890a09c4c84a410208dcb3187fba02fdfdcf"},"cell_type":"code","source":"dfjob = df.groupby('JOB_TITLE',as_index=False).count()[['JOB_TITLE','countvar']].sort_values('countvar',ascending=False)[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20736687432f4db6e7515c425c232fe10d595726"},"cell_type":"code","source":"t1 = go.Bar(x=dfjob.JOB_TITLE.values,y=dfjob.countvar.values,name='jobtitle')\nlayout = go.Layout(dict(title= \"TOP 20 JOBS\",yaxis=dict(title=\"Num of applications\")))\ndata = [t1]\nfig =go.Figure(data,layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a911cc2d7d59f98fb74794e662471b155c1d121"},"cell_type":"markdown","source":"### Extracting the YEAR from the EMPLOYMENT_START_DATE."},{"metadata":{"trusted":false,"_uuid":"074e1f4cf58a1ccae7ef095e7839f36ac9011d42"},"cell_type":"code","source":"df['year'] = df.EMPLOYMENT_START_DATE.apply(lambda x: x.year)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3015698e0c85961e3c16cb562f38aaab7718301"},"cell_type":"markdown","source":"#### Number of applications per year\n- As this the data upto 2017, the employment_start_date has only very few 2018 dates.\n- And of those few, some are removed during the process of dealing with missing values"},{"metadata":{"trusted":false,"_uuid":"05d4e325c4f5f28d954e695ed95f08b433e7f38a"},"cell_type":"code","source":"dfyear = df.groupby('year',as_index=False).count()[['year','countvar']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d0ca777215cf29e2844bd619a23e3cb86a9bbf46"},"cell_type":"code","source":"t1 = go.Scatter(\n    x=dfyear.year,\n    y=dfyear.countvar\n)\nlayout = go.Layout(dict(title= \" NUMBER OF APPLICATIONS PER YEAR\",xaxis=dict(title=\"YEARS\"),yaxis=dict(title=\"Num of applications\")))\ndata = [t1]\nfig = go.Figure(data=data,layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e93e3d01e3b93469a87cfc969314ec1d94d12c4"},"cell_type":"markdown","source":"### Distribution of Case_Status column\n- A lot of them were certified. (Hope's alive :) )"},{"metadata":{"trusted":false,"_uuid":"de4c088e271166789f699d2d3f843ab4139b5247"},"cell_type":"code","source":"t1 = go.Bar(x=df.groupby('CASE_STATUS').count().index,y=df.groupby('CASE_STATUS').count()['countvar'],name='CASESTATUSWISE')\ndata = [t1]\niplot(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f88c7e3769af1f3aed30c1f6ff60c185c11112e8"},"cell_type":"markdown","source":"### Distribution of Case_Status column/ Full_Time position"},{"metadata":{"trusted":false,"_uuid":"e8051948de5c4260eecdff9c7e976276b75b5961"},"cell_type":"code","source":"t1 = go.Bar(x=df[df.FULL_TIME_POSITION == 'Y'].groupby('CASE_STATUS').count().index,y=df[df.FULL_TIME_POSITION == 'Y'].groupby('CASE_STATUS').count()['countvar'],name='FULL-TIME ')\nt2 = go.Bar(x=df[df.FULL_TIME_POSITION == 'N'].groupby('CASE_STATUS').count().index,y=df[df.FULL_TIME_POSITION == 'N'].groupby('CASE_STATUS').count()['countvar'],name='PART-TIME ')\ndata = [t1,t2]\nlayout = go.Layout(barmode='stack')\nfig = go.Figure(data =data,layout =layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d3d6c9e45c628ff75b18fa2abe1e400bf5bb031"},"cell_type":"markdown","source":"## Dealing with outliers in Pay scale.\n- In the below code, see the difference between the 75th percentile and the max value, that huge difference clearly indicates the presence of outliers.\n- The min has value of 0, which is obviously false. ( No one works for free)"},{"metadata":{"trusted":false,"_uuid":"99e7985fdb23626f67b496aa2e8263eae7b4a8d5"},"cell_type":"code","source":"df.PREVAILING_WAGE.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e24822ffdd58bc2b231c248a785090f707b36730"},"cell_type":"code","source":"df.PW_UNIT_OF_PAY.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be7fc34590e046bbbcfd2299ac30d702c4a411ae"},"cell_type":"markdown","source":"### My knowledge about H1B\n- The full time H1B employees have a minimum salary of 65k now, as we have data fromm 2011 I'm setting the minimum salary to be 40K.\n- There is no limit for maximum salary. But, any package above 200K is skeptical. But, considering the Doctors in mind. I will chose the cutoff for 270K."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"5e2d28bde450291374e6cbdfa27a0d41262e736c"},"cell_type":"code","source":"dum = df[(df.FULL_TIME_POSITION == 'Y') & (df.PW_UNIT_OF_PAY == 'Year')]\nind1 = dum[(dum.PREVAILING_WAGE > 270000) | (dum.PREVAILING_WAGE < 40000)].index\ndf = df.drop(ind1,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6de328ee19f2c3633f7fc09248bd5133fb5f9619"},"cell_type":"markdown","source":"- The partime employees might have a minimum salary of 30K-32K(in 2011) and maximum couldnt be more than 150K"},{"metadata":{"trusted":false,"_uuid":"2c80e054174022f907a3b84008c405325b159077"},"cell_type":"code","source":"dum = df[(df.FULL_TIME_POSITION == 'N') & (df.PW_UNIT_OF_PAY == 'Year')]\nind1 = dum[(dum.PREVAILING_WAGE > 150000) | (dum.PREVAILING_WAGE < 32000)].index\ndf = df.drop(ind1,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56ad79a60975891058ccf40e1b1b1402b4bcc03c"},"cell_type":"markdown","source":"#### Hourly Pay\n- The minimum hourly salary should atleast 15. I cant imagine less than 15 cuz gas stations pay 10-12/hour\n- the maximum salary may be around 110/hour.(Purely my guess)"},{"metadata":{"trusted":false,"_uuid":"781e0898a55112b96a78575ca70dde35d43cdd2d"},"cell_type":"code","source":"dum = df[(df.PW_UNIT_OF_PAY == 'Hour')]\nind1 = dum[(dum.PREVAILING_WAGE > 110) | (dum.PREVAILING_WAGE < 15)].index\ndf = df.drop(ind1,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea1f424f0e94f26ef2b8365efc7d727a8a0dc004"},"cell_type":"markdown","source":"### Converting the hourly pay to annual pay.\n- FULL_TIMERS: As they work 40hours a week and we have 48 weeks in a year, we multiply with 40*48\n- PART_TIMERS: As they work something around 25-35 hours a week, let me take the average of 30 hours a week,So we multiply with 30*48."},{"metadata":{"trusted":false,"_uuid":"cb399912bed5e82cf33766fadeb846f61eb8daa4"},"cell_type":"code","source":"k = df[(df.PW_UNIT_OF_PAY == 'Hour') & (df.FULL_TIME_POSITION == 'Y')].index\ndf.loc[k,'PREVAILING_WAGE'] = df.loc[k,'PREVAILING_WAGE'] * 1920","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"33f9f67fdab328b4af82f4f7fe0fde124d2f1d8a"},"cell_type":"code","source":"k = df[(df.PW_UNIT_OF_PAY == 'Hour') & (df.FULL_TIME_POSITION == 'N')].index\ndf.loc[k,'PREVAILING_WAGE'] = df.loc[k,'PREVAILING_WAGE'] * 1440","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96fc60e0d289993c0369e2aa7d6fc016ca9d7b5e"},"cell_type":"markdown","source":"#### As, we now have all data in year pay scale. We go ahead and remove PW_UNIT_PAY"},{"metadata":{"trusted":false,"_uuid":"09c17e9be2fc5e70e195e3146d6972634afc7e4a"},"cell_type":"code","source":"df=df.drop(['PW_UNIT_OF_PAY'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f1596fef3c30ffa5d5068c6152673973ebfd1a6"},"cell_type":"markdown","source":"### Average Annual pay of each year from 2011-2018"},{"metadata":{"trusted":false,"_uuid":"0d56db932e1821d9b655e6238e2c4ae646ef4243"},"cell_type":"code","source":"t1 = go.Scatter(\n    x=df.groupby('year').mean().index,\n    y=df.groupby('year').mean().PREVAILING_WAGE\n)\n\nlayout = go.Layout(dict(title= \" AVERAGE ANNUAL PAY vs YEAR\",xaxis=dict(title=\"YEARS\"),yaxis=dict(title=\"AVERAGE ANNUAL PAY\")))\ndata = [t1]\nfig = go.Figure(data=data,layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9e149bf8ec4ee4fbbc4e845ea80d02287c00d08"},"cell_type":"markdown","source":"### HOTTEST JOB IN EVERY STATE\n#### - In the plot below hover around the states in map to know more."},{"metadata":{"trusted":false,"_uuid":"99aaa532b7a728b692f25bcb08662ce331f8f0cc"},"cell_type":"code","source":"dum = df[[\"EMPLOYER_STATE\",\"JOB_TITLE\"]]\ndum = dum.groupby([\"EMPLOYER_STATE\",\"JOB_TITLE\"]).size().reset_index()\ndum.columns = ['EMPLOYER_STATE', 'JOB_TITLE', \"COUNT\"]\ndum = dum.groupby(['EMPLOYER_STATE', 'JOB_TITLE']).agg({'COUNT':sum})\ndum = dum['COUNT'].groupby(level=0, group_keys=False)\ndum = dum.apply(lambda x: x.sort_values(ascending=False).head(1))\ndum = pd.DataFrame(dum).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d84f1b92ba1265431dad791156e60e18420fbdce"},"cell_type":"code","source":"data=[dict(\n    type='choropleth',\n    locations = dum.EMPLOYER_STATE,\n    z = dum.COUNT,\n    locationmode = 'USA-states',\n    text = dum.JOB_TITLE,\n    marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Number of application\")\n)]\nlayout= dict(title=\"Top job title in the state\",geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\nfig = dict( data=data, layout=layout )\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c13ff9026a075478e9f4b6ee683c9405a2eff89"},"cell_type":"markdown","source":"### Average salary of H1B employee in each state.\n- As expected, california pays more"},{"metadata":{"trusted":false,"_uuid":"ae91e8bc611f6535e92b4a68b96c263eeab07ff6"},"cell_type":"code","source":"dum = df.groupby('EMPLOYER_STATE',as_index=False).mean()[['EMPLOYER_STATE','PREVAILING_WAGE']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"608197ee5cd56b69114864fd10894556fa875c8e"},"cell_type":"code","source":"data=[dict(\n    type='choropleth',\n    locations = dum.EMPLOYER_STATE,\n    z = dum.PREVAILING_WAGE,\n    locationmode = 'USA-states',\n    marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Avg salary in USD\")\n)]\nlayout= dict(title=\"Average salaries per state\",geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\nfig = dict( data=data, layout=layout )\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"082436576f173d420fd6c1465cb9eeb4e8b8dfda"},"cell_type":"markdown","source":"## IT and TECH Analysis.\n- The following was taken from @DhrumilVora.(Kaggle)\n- It creates a new column occupation based on the key words from the SOC_NAME column."},{"metadata":{"trusted":false,"_uuid":"0cee5c2e77d2f3620bf7fb37b692d40109d41ba0"},"cell_type":"code","source":"df['OCCUPATION'] = np.nan\ndf['SOC_NAME'] = df['SOC_NAME'].str.lower()\ndf.OCCUPATION[df['SOC_NAME'].str.contains('computer','programmer')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('data scientist','data analyst')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('data engineer','data base')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('machine learning','artifical intelligence')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('spark','apache')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('hadoop','big data')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('sql','cyber')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('developer','full stack')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('fullstack','etl')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('data','network')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('software tester','cloud')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('information','informatica')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('jira','programmer')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('software','web developer')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('database')] = 'Computer Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('math','statistic')] = 'Mathematical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('predictive model','stats')] = 'Mathematical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('teacher','linguist')] = 'Education Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('professor','Teach')] = 'Education Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('school principal')] = 'Education Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('medical','doctor')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('physician','dentist')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('Health','Physical Therapists')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('surgeon','nurse')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('psychiatr')] = 'Medical Occupations'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('chemist','physicist')] = 'Advance Sciences'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('biology','scientist')] = 'Advance Sciences'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('biologi','clinical research')] = 'Advance Sciences'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('public relation','manage')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('management','operation')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('chief','plan')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('executive')] = 'Management Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('advertis','marketing')] = 'Marketing Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('promotion','market research')] = 'Marketing Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('business','business analyst')] = 'Business Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('business systems analyst')] = 'Business Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('accountant','finance')] = 'Financial Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('financial')] = 'Financial Occupation'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('engineer','architect')] = 'Architecture & Engineering'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('surveyor','carto')] = 'Architecture & Engineering'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('technician','drafter')] = 'Architecture & Engineering'\ndf.OCCUPATION[df['SOC_NAME'].str.contains('information security','information tech')] = 'Architecture & Engineering'\ndf['OCCUPATION']= df.OCCUPATION.replace(np.nan, 'Others', regex=True)\ndf['SOC_NAME'] = df['SOC_NAME'].str.upper()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f2e2940f93f80c906535495ef35bf886a76fdfee"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"610b0922996a9a72920bf83028f42691c882bff9"},"cell_type":"markdown","source":"## The newly created column contents"},{"metadata":{"trusted":false,"_uuid":"6c54353964027a0abc662756fe6e78504285e168"},"cell_type":"code","source":"df.OCCUPATION.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c951fff620b3814355f8921e182fbd6ffef5700f"},"cell_type":"markdown","source":"### The Average annual salaries of the newly created departments."},{"metadata":{"trusted":false,"_uuid":"6ed8c1d0171597c75049b2ad978b84f056636860"},"cell_type":"code","source":"dum = df.groupby('OCCUPATION',as_index = False).mean()[['OCCUPATION','PREVAILING_WAGE']]\nt1 =go.Bar(x=dum.OCCUPATION,y=dum.PREVAILING_WAGE,name='wageperoccuaption')\nlayout = go.Layout(dict(title= \" AVERAGE ANNUAL PAY vs OCCUPATION\",yaxis=dict(title=\"AVERAGE ANNUAL PAY\")))\ndata = [t1]\nfig = go.Figure(data=data,layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b35df62c544f54a724ab568993bdea30266e2303"},"cell_type":"markdown","source":"### Working only on the TECH/IT data."},{"metadata":{"trusted":false,"_uuid":"95e0d68d979d4b3b406b443937fd3f7e523a1750"},"cell_type":"code","source":"dfcomp = df[df.OCCUPATION == 'Computer Occupations']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c5ccfc45f6dbcaaa4744d82610a84960c63fea8"},"cell_type":"markdown","source":"### Average salary of IT H1B employee in each state.\n- As expected, california pays more"},{"metadata":{"trusted":false,"_uuid":"df68eb0d882ae531612acab5d6282951633fc250"},"cell_type":"code","source":"dum = dfcomp.groupby('EMPLOYER_STATE',as_index=False).mean()[['EMPLOYER_STATE','PREVAILING_WAGE']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b0d8305994b21765ab11e1121a90e8fcb551b2a4"},"cell_type":"code","source":"data=[dict(\n    type='choropleth',\n    locations = dum.EMPLOYER_STATE,\n    z = dum.PREVAILING_WAGE,\n    locationmode = 'USA-states',\n    marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Avg salary in USD\")\n)]\nlayout= dict(title=\"Average salaries of TECH(IT) per state\",geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\nfig = dict( data=data, layout=layout )\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2f6de6044bfe17b500e9fb41cc2d6594753d29f"},"cell_type":"markdown","source":"## TOP 20 MOST PAID FIELDS.\n- Medical field domination "},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"03e99c8dc8317db795dddc0882a5fe735c300532"},"cell_type":"code","source":"dum = df.groupby('SOC_NAME',as_index=False).mean()[['SOC_NAME','PREVAILING_WAGE']]\ndum.sort_values('PREVAILING_WAGE',ascending= False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44c33fa1f59be31d0c09350f03a7f63b8dc0ba29"},"cell_type":"markdown","source":"## TOP 20 MOST PAID TECH FIELDS."},{"metadata":{"trusted":false,"_uuid":"829ea9e38c353064dc4ac831ed83da47f42fcbd7"},"cell_type":"code","source":"dum = dfcomp.groupby('SOC_NAME',as_index=False).mean()[['SOC_NAME','PREVAILING_WAGE']]\ndum.sort_values('PREVAILING_WAGE',ascending= False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5bb0c6f60bc520fc28cddbff67f6cd8876be4c6"},"cell_type":"markdown","source":"## DATA SCIENCE: \n- I'm going to do some analysis on our domain.\n- Created a new column DS as shown below. "},{"metadata":{"trusted":false,"_uuid":"f56df3d4f75875707fd4280a5d9c4925849c3ab1"},"cell_type":"code","source":"df['DS'] = np.nan\ndf.DS[df['JOB_TITLE'].str.contains('DATA SCIENTIST')] = 'DATA SCIENTIST'\ndf.DS[df['JOB_TITLE'].str.contains('DATA ANALYST')] = 'DATA ANALYST'\ndf.DS[df['JOB_TITLE'].str.contains('MACHINE LEARNING')] = 'MACHINE LEARNING'\ndf.DS[df['JOB_TITLE'].str.contains('BUSINESS ANALYST')] = 'BUSINESS ANALYST'\ndf.DS[df['JOB_TITLE'].str.contains('DEEP LEARNING')] = 'DEEP LEARNING'\ndf.DS[df['JOB_TITLE'].str.contains('ARTIFICIAL INTELLIGENCE')] = 'ARTIFICIAL INTELLIGENCE'\ndf.DS[df['JOB_TITLE'].str.contains('BIG DATA')] = 'BIG DATA'\ndf.DS[df['JOB_TITLE'].str.contains('HADOOP')] = 'HADOOP'\ndf.DS[df['JOB_TITLE'].str.contains('DATA ENGINEER')] = 'DATA ENGINEER'\ndf['DS']= df.DS.replace(np.nan, 'Others', regex=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36a614b4b878924c1718fce829d420da28e14c64"},"cell_type":"markdown","source":"#### Examine the new column we created."},{"metadata":{"trusted":false,"_uuid":"17323700f5a11e0befbff98e6ceee56fab90848f"},"cell_type":"code","source":"df.DS.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e7336a8470cac2a062a59295829c509471f41ab5"},"cell_type":"code","source":"dum = df.groupby('DS',as_index=False).mean()[['DS','PREVAILING_WAGE']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"615ac0ef7c21df7cdc12aeea5cd807dd9bf065d4"},"cell_type":"code","source":"t1 =go.Bar(x=dum.DS,y=dum.PREVAILING_WAGE,name='DataScience')\ndata = [t1]\niplot(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cca29c6f9e2e16e03e54d2127e086c284de978d"},"cell_type":"markdown","source":"### GROWTH IN DATA SCIENCE FROM YEARS"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"d26e884ab80bd93f7cf685c02d290b4e53bdcc13"},"cell_type":"code","source":"dum = df.groupby(['year','DS']).count().reset_index()[['year','DS','countvar']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"48763facecbebe30281fdeabeb4efadb769cf575"},"cell_type":"code","source":"data = []\nfor i in dum.DS.unique():\n    if i != 'Others':\n        data.append(go.Scatter(x = dum[dum.DS == i].year,y= dum[dum.DS == i].countvar,name=i))\n\nlayout = go.Layout(dict(title= \"GROWTH IN DATA SCIENCE\",xaxis=dict(title=\"YEARS\"),yaxis=dict(title=\"Number of applications\")))\n        \nfig = go.Figure(data,layout)    \niplot(fig)    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"474e9755feeaccf73efe11de1f0c475590bbf92b"},"cell_type":"markdown","source":"### DATA SCIENCE JOB AND THE STATE WHICH IT TOPS\n- Means, the respective data science job and the state which has the most number of respective job.\n- To explain, the business analysts are more in New Jersey while the data analysts are more in california."},{"metadata":{"trusted":false,"_uuid":"1b562f36a6008c7d935f8ebac6bfe02d72c70d13"},"cell_type":"code","source":"dum = df[[\"DS\",\"EMPLOYER_STATE\"]]\ndum = dum.groupby([\"DS\",\"EMPLOYER_STATE\"]).size().reset_index()\ndum.columns = [\"DS\",\"EMPLOYER_STATE\", \"COUNT\"]\ndum = dum.groupby([\"DS\",\"EMPLOYER_STATE\"]).agg({'COUNT':sum})\ndum = dum['COUNT'].groupby(level=0, group_keys=False)\ndum = dum.apply(lambda x: x.sort_values(ascending=False).head(1))\ndum = pd.DataFrame(dum).reset_index()\ndum[0:-1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c3042c77071156ea83c2c89a210cf47f7734b6c"},"cell_type":"markdown","source":"### DATA SCIENCE JOB AND THE EMPLOYER WHICH IT TOPS\n- Means, the respective data science job and the Employer which has the most number of respective job."},{"metadata":{"trusted":false,"_uuid":"db019f29eedc152a23131e3fe6ccdc2e3abd5283"},"cell_type":"code","source":"dum = df[[\"DS\",\"EMPLOYER_NAME\"]]\ndum = dum.groupby([\"DS\",\"EMPLOYER_NAME\"]).size().reset_index()\ndum.columns = [\"DS\",\"EMPLOYER_NAME\", \"COUNT\"]\ndum = dum.groupby([\"DS\",\"EMPLOYER_NAME\"]).agg({'COUNT':sum})\ndum = dum['COUNT'].groupby(level=0, group_keys=False)\ndum = dum.apply(lambda x: x.sort_values(ascending=False).head(1))\ndum = pd.DataFrame(dum).reset_index()\ndum[0:-1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16ad164e4a63271c09b97bdd9e250c1ec543e020"},"cell_type":"markdown","source":"### VIRGINIA and DC"},{"metadata":{"trusted":false,"_uuid":"d9e32f625b8115edfb99b14f1e5a01f41958dac5"},"cell_type":"code","source":"dfvadc = df[(df.EMPLOYER_STATE == 'VA') | (df.EMPLOYER_STATE == 'DC')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1de4cafc287f3ec40a2ed588f29d559f8dc714c5"},"cell_type":"code","source":"dfvadc = dfvadc[dfvadc.DS != 'Others']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8133a010fcd10990d6d1e5095f19d0705bb1e4c9"},"cell_type":"code","source":"dum = dfvadc[[\"DS\",\"EMPLOYER_NAME\"]]\ndum = dum.groupby([\"DS\",\"EMPLOYER_NAME\"]).size().reset_index()\ndum.columns = [\"DS\",\"EMPLOYER_NAME\", \"COUNT\"]\ndum = dum.groupby([\"DS\",\"EMPLOYER_NAME\"]).agg({'COUNT':sum})\ndum = dum['COUNT'].groupby(level=0, group_keys=False)\nnewdum = dum.apply(lambda x: x.sort_values(ascending=False).head(1))\nnewdum = pd.DataFrame(newdum).reset_index()\nnewdum[0:-1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85c1c2714f52fe32de6520b9e285b7e0a4f66693"},"cell_type":"markdown","source":"## Companies to FOCUS (for me)"},{"metadata":{"trusted":false,"_uuid":"01500aa205ee9c329e6bc71bafbdcc106cb491e5"},"cell_type":"code","source":"pd.DataFrame(dum.apply(lambda x: x.sort_values(ascending=False).head(15))).reset_index()['EMPLOYER_NAME']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0562132ece1b34df1c4b7d519be774609235b6f"},"cell_type":"markdown","source":"## END\n- SAITEJA NAKKA"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}