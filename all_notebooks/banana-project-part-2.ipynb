{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"buisness = pd.read_csv('../input/yelp-csv/yelp_academic_dataset_business.csv')\nreview = pd.read_csv('../input/yelp-csv/yelp_academic_dataset_review.csv')","execution_count":2,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (2,5,8,15,18,21,27,30,43,49,52,62,64,66,70,84,89,92,100) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"buisness_n = buisness[buisness['categories'].str.contains('Restaurant') == True]","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"buisness_n = buisness_n.fillna(0)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_n = review[review.business_id.isin(buisness_n['business_id']) == True]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_new = review_n.sample(n = 350000, random_state = 42)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = review_new['text']","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts_n = []\nfor i in text:\n    i = i.replace(\"\\n\", \" \")\n    texts_n.append(i)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\ntoken_texts = []\nfor i in range (len(texts_n)):\n    r = word_tokenize(texts_n[i])\n    token_texts.append(r)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\ntexts_new = [\" \".join([word for word in text if word not in punctuation and not word.isnumeric() \\\n                      and len(word) > 1]) for text in token_texts]\ntexts_new[1]","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"\"we went here last night with group of friends and got beers at the bar while waiting for table the beer selection is n't the greatest but it 's pretty solid we enjoyed some potosi cave ale and were seated in about minutes not too bad for group of on saturday night especially considering there was huge line of people waiting had the bad breath burger which truly lived up to its name the beef was quality and all the toppings on it were garlicky cheesy awesome goodness the bf had the reuben and said it was pretty tasty too think it 's safe to say we 'll be coming back\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\nlem_texts = []\ntoken_texts2 = []\nfor i in range (len(texts_new)):\n    r = word_tokenize(texts_new[i])\n    token_texts2.append(r)\nfor text in token_texts2:\n    l = ' '.join([lem.lemmatize(w) for w in text])\n    lem_texts.append(l)\nlem_texts[0]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"\"mean who doe n't love walking into Fred 's and eating lunch in the middle of Barneys ... 've been twice and honestly had great experience amazing service and delicious food Once for brunch had the prix fix brunch that includes salad brunch entree drink dessert AND coffee ... for buck not bad deal ... second time came wa little different strolled in with friend and we had boozy weekday lunch he had the burger and had the chicken salad ... he ordered manhattan which he exclaimed wa the best one he 's ever had and had delicious pinot grigio micah MISSSSS you but the wait staff wa the best we made friend with them and chatted them up for almost two hour ... they kept coming over and gave u cute place to go too ... they were really sweet Too bad it INSANELY overpriced like the Proenza Schoulers\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"review = review.drop(['text'], axis=1)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review['text'] = lem_texts","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review = review_new[['text','stars']]","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = review[0:280000]\ntest = review[280000:]","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train, columns = ['stars'])","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.get_dummies(test, columns = ['stars'])","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sample(frac = 0.1, random_state = 42)\ntest = test.sample(frac = 0.1, random_state = 42)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['stars_1', 'stars_2', 'stars_3', 'stars_4', 'stars_5']\ny_train = train[class_names].values","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt'\n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nw2v_index = dict(get_coefs(*o.strip().split()) for o in open(w2v_model))","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(list(train['text'].values))\n","execution_count":30,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = tokenizer.texts_to_sequences(train['text'].values)\nX_test = tokenizer.texts_to_sequences(test['text'].values)\nX_train = pad_sequences(X_train, maxlen = 200)\nX_test = pad_sequences(X_test, maxlen = 200)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_words = min(20000, len(word_index))\nw2v_matrix = np.zeros((nb_words, 200))\nmissed = []\nfor word, i in word_index.items():\n    if i >= 20000: break\n    w2v_vector = w2v_index.get(word)\n    if w2v_vector is not None:\n        w2v_matrix[i] = w2v_vector\n    else:\n        missed.append(word)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, GRU\nfrom keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nembed_size = 200 \nmax_features = 20000\nmaxlen = 200\n\ninp = Input(shape = (maxlen,))\nx = Embedding(max_features, embed_size, weights = [w2v_matrix], trainable = True)(inp)\nx = SpatialDropout1D(0.5)(x)\nx = Bidirectional(LSTM(40, return_sequences=True))(x)\nx = Bidirectional(GRU(40, return_sequences=True))(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nconc = concatenate([avg_pool, max_pool])\noutp = Dense(5, activation = 'sigmoid')(conc)","execution_count":35,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nmodel = Model(inputs = inp, outputs = outp)\n# patience is how many epochs to wait to see if val_loss will improve again.\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3)\ncheckpoint = ModelCheckpoint(monitor = 'val_loss', save_best_only = True, filepath = 'yelp_lstm_gru_weights.hdf5')\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, batch_size = 512, epochs = 20, validation_split = .1, callbacks=[earlystop, checkpoint])","execution_count":37,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nTrain on 25200 samples, validate on 2800 samples\nEpoch 1/20\n25200/25200 [==============================] - 62s 2ms/step - loss: 0.4961 - acc: 0.7833 - val_loss: 0.4639 - val_acc: 0.8001\nEpoch 2/20\n25200/25200 [==============================] - 59s 2ms/step - loss: 0.4430 - acc: 0.8048 - val_loss: 0.4167 - val_acc: 0.8104\nEpoch 3/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.4002 - acc: 0.8153 - val_loss: 0.3841 - val_acc: 0.8175\nEpoch 4/20\n25200/25200 [==============================] - 57s 2ms/step - loss: 0.3769 - acc: 0.8222 - val_loss: 0.3614 - val_acc: 0.8288\nEpoch 5/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.3577 - acc: 0.8294 - val_loss: 0.3486 - val_acc: 0.8345\nEpoch 6/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.3465 - acc: 0.8347 - val_loss: 0.3444 - val_acc: 0.8338\nEpoch 7/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.3336 - acc: 0.8401 - val_loss: 0.3343 - val_acc: 0.8368\nEpoch 8/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.3250 - acc: 0.8443 - val_loss: 0.3298 - val_acc: 0.8393\nEpoch 9/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.3157 - acc: 0.8478 - val_loss: 0.3284 - val_acc: 0.8377\nEpoch 10/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.3104 - acc: 0.8513 - val_loss: 0.3292 - val_acc: 0.8405\nEpoch 11/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.3006 - acc: 0.8569 - val_loss: 0.3281 - val_acc: 0.8400\nEpoch 12/20\n25200/25200 [==============================] - 57s 2ms/step - loss: 0.2935 - acc: 0.8603 - val_loss: 0.3332 - val_acc: 0.8397\nEpoch 13/20\n25200/25200 [==============================] - 59s 2ms/step - loss: 0.2873 - acc: 0.8649 - val_loss: 0.3264 - val_acc: 0.8429\nEpoch 14/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.2817 - acc: 0.8676 - val_loss: 0.3297 - val_acc: 0.8404\nEpoch 15/20\n25200/25200 [==============================] - 57s 2ms/step - loss: 0.2754 - acc: 0.8708 - val_loss: 0.3295 - val_acc: 0.8417\nEpoch 16/20\n25200/25200 [==============================] - 58s 2ms/step - loss: 0.2677 - acc: 0.8752 - val_loss: 0.3348 - val_acc: 0.8413\n","name":"stdout"},{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"<keras.callbacks.History at 0x7f42225179e8>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict([X_test], batch_size=1024, verbose = 1)","execution_count":38,"outputs":[{"output_type":"stream","text":"7000/7000 [==============================] - 3s 498us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, test[class_names].values, verbose = 1, batch_size=1024)","execution_count":39,"outputs":[{"output_type":"stream","text":"7000/7000 [==============================] - 3s 411us/step\n","name":"stdout"},{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"[0.3293969285828727, 0.841542843750545]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_new.stars.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}