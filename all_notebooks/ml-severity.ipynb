{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/us-accidents/US_Accidents_June20.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (23,9))\nsns.heatmap(df.corr(), annot = True )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Temperature and Wind_Chill(F) have correlation of 0.99 (should probably just get rid of one cause their basically the same thing). Bump and Traffic_calming have correlation of 0.66 so fairly high. Crossing and Traffic_signal have a correlation of 0.45, no surprise there. Start_Lat has a 100% correlation with End_Lat so I'll get rid of one, same thing with Start_Lng and End_Lng","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Wind_Chill(F)', 'End_Lat', 'End_Lng'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count()/3513617  #Lets see the percentage of non-null values for each column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It seems like Number is just missing too many values, TMC is also missing a lot but we may be able to feature_engineer it along with other\ndf.drop(['Number', 'ID'], axis = 1, inplace = True) #ID is also useless to us","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TMC'].value_counts() #TMC doesn't really correlate with anything and is also a classification meaning we can't really replace any values for it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(subset = ['TMC'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For Temperature, Humidity, Pressure, Visibility, Wind_speed, and Precipitation we can just get their means\nvalues = {'Temperature(F)': df['Temperature(F)'].mean(), 'Humidity(%)': df['Humidity(%)'].mean(), 'Pressure(in)': df['Pressure(in)'].mean(), 'Visibility(mi)': df['Visibility(mi)'].mean(), 'Wind_Speed(mph)' : df['Wind_Speed(mph)'].mean(), 'Precipitation(in)': df['Precipitation(in)'].mean() }\ndf.fillna(value = values, inplace = True)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Okay I think we can just drop everything else now\ndf.dropna(inplace = True)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#All the twilights seem to be pretty much the same thing so I'll drop them\ndf.drop(['Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() #Okay so lets start making dummy variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source = pd.get_dummies(df['Source'])\ndf = pd.concat([df.drop('Source', axis = 1), source], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we make changes with the Time. Thank you to Deepak Deepu for helping me out with this one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors = 'coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'], errors = 'coerce')\n\ndf['Year'] = df['Start_Time'].dt.year\ndf['Month'] = df['Start_Time'].dt.month\ndf['Day'] = df['Start_Time'].dt.day\ndf['Hour'] = df['Start_Time'].dt.hour\n\ndf['Duration'] = round((df['End_Time']- df['Start_Time'])/np.timedelta64(1,'m'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_outliers=df['Duration']<=0\n\ndf[neg_outliers] = np.nan\n\ndf.dropna(subset=['Duration'],axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Start_Time', 'End_Time'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Country is useless because this is only happening in the U.S. Also County and City are just too specific for me to use and have way too many categories\ndf.drop(['Country', 'County', 'City'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Zipcode, Timezone, Airport_Code, Weather_Timestamp are also pretty useless to me\ndf.drop(['Zipcode', 'Timezone', 'Airport_Code', 'Weather_Timestamp'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I'm going to use street just so I can see if they were on a highway or not\n\ndef location(street):\n    if 'I-' in street:\n        return 1\n    else:\n        return 0\n\ndf['highway'] = df['Street'].apply(location)\ndf.drop('Street', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['highway'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#State is just too broad to affect the severity of the accident and the description just has the information in the other variables\ndf.drop(['State', 'Description'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Side'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There seems to be one random value in side so lets get rid of it and then create dummy variables for it\nvalue = df[(df['Side'] != 'R') & (df['Side'] != 'L')].index\ndf.drop(value, inplace = True)\ndf['Side'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sides = pd.get_dummies(df['Side'], drop_first = True)\nsides = sides.rename({'R' : 'Side'}, axis = 1)\ndf = pd.concat([df.drop('Side', axis = 1), sides], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Wind_Direction'].value_counts() #Way too many directions, lets just split it up into Calm, North, South, East, West, and Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Wind_Direction'] = df['Wind_Direction'].apply(lambda dire: dire[0])\ndf['Wind_Direction'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wind = pd.get_dummies(df['Wind_Direction'], drop_first = True)\ndf = pd.concat([df.drop('Wind_Direction', axis = 1), wind], axis = 1)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Weather_Condition'].value_counts().head(30) #Rain (and drizzle), Snow, Thunder (and storm), Cloud (and Overcast), Clear (and Fair), haze (and Smoke and fog) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weather(kind):\n    if 'Rain' in kind or 'Snow' in kind or 'Storm' in kind or 'Thunder' in kind or 'Drizzle' in kind:\n        return 'Slippery'\n    elif 'Fog' in kind or 'Smoke' in kind or 'Haze' in kind or 'Mist'in kind:\n        return 'Vis_obstruct'\n    else:\n        return 'Fair'\n    \nweather = df['Weather_Condition'].apply(weather)\nweather.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_type = pd.get_dummies(weather, drop_first = True)\nweather_type.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df.drop('Weather_Condition', axis = 1) , weather_type], axis = 1)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Sunrise_Sunset'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sky = pd.get_dummies(df['Sunrise_Sunset'], drop_first = True)\nsky.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df.drop('Sunrise_Sunset', axis = 1), sky], axis = 1)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets see if we can reduce any of the columns by seeing how correlated they are with each other\nplt.figure(figsize = (26,16))\nsns.heatmap(df.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Turning_Loop'].value_counts() #Turning_Loop is just all zeroes so it's useless","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It also seems like Visibility and Slippery are correlated also MapQuest and MapQuest_Bing\ndf.drop(['Turning_Loop', 'Slippery', 'MapQuest-Bing'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop('Severity', axis = 1)\ny_rfc = df['Severity']\ny_nn = df['Severity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X,y_rfc, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Because this is Multi-Classification, lets start with Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators= 100)\nrfc.fit(X_train_r, y_train_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\ny_rfc_pred = rfc.predict(X_test_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test_r, y_rfc_pred))\nprint('\\n')\nprint(classification_report(y_test_r, y_rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(y_nn)\ny_nn = encoder.transform(y_nn)\ny_nn = to_categorical(y_nn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y_nn, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets try Neural Network\nfrom sklearn.preprocessing import MinMaxScaler\nscaler  = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(39, activation = 'relu', input_dim = len(df.columns) - 1))\nmodel.add(Dropout(rate = 0.4))\n\nmodel.add(Dense(20 , activation = 'relu'))\nmodel.add(Dropout(rate = 0.4))\n\nmodel.add(Dense(10 , activation = 'relu'))\nmodel.add(Dropout(rate = 0.4))\n\nmodel.add(Dense(4, activation = 'softmax'))\n\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs = 30, callbacks = [early_stop],batch_size = 256, validation_data = (X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(X_test, y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}