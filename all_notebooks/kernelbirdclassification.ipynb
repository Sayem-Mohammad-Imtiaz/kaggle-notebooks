{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Overview/Goal of this Model:**\n\nWe will be building a convolution neural network using transfer learning and try to apply optimization techniques for better model performance. This shall be done for the kaggle dataset available from https://www.kaggle.com/gpiosenka/100-bird-species on birdspecies. We plan to use the available 25812 (number changes as new versions are published) training images to train a visualnet model while using validation data (5 per species available) so that we can accurately predict the species of the images in test dataset. As of May 2nd 2020 this dataset had 190 bird species which can get updated over time."},{"metadata":{},"cell_type":"markdown","source":"**Description of data:\n**\nSome important considerations when working with the data are as follows:\n\n1) Size of dataset:\n\nAll images provided in the dataset are of size 224 X 224 X 3. Hence these are color images in the jpg format. We have a total of 25812 training images, 950 test images(5 per species) and 950 validation images(5 per species)\n\n2) Organization of data:\n\nImages for each species are contained in a separate sub directory for each of the train, test and validate datasets. We also have a \"consolidated\" image set available that combines all the available images and could have allowed to do our own train, validate, test split. For the purpose of this model I have not done that.\n\n3) Distribution of images by gender: Male to Female species images are in the ratio 4:1 in the dataset. Given that the physical characteristics of the males in birds can be very different from female in color (males generally brigher), size and other physical characteristics the classifier will not perform well with female species images. However given this information is not available on the images it is not possible to test how the model is performing specifically on females species images in terms of prediction.\n\n4) In the images the birds are sitting on the branches and are centered mostly"},{"metadata":{},"cell_type":"markdown","source":"\n**Summary of methods**\n\nTried different architectures: (added dense and removed the last few layers of the models)\n* InceptionV3\n* vgg16\n* MobileNet\n\nOptimizers:\nAdam\nSGD\nrmsprop\n\nTried to adjust the:\n*decay_rate*: We used this option with optimizers to stabilize learning rate. \n*learning rate*: We have used a learning rate of 0.0001 which seems to have worked well with earlier models\n\nSteps to avoid overfitting by using regularization techniques:\n\nUsed dropout with new dense layers added \nweight decay(L2 regularization)\n\nThe goal of the model is to generalize well so that it can perform well on the never seen before test data. Data augmentation helps to generating more training data  from existing training samples, by augmenting the samples via a number of random transformations as follows:\n\n* *rotation_range* is a value in degrees (0–180), a range within which to randomly rotate pictures: We tried rotation of 20-40% in different attempts\n* *width_shift* and *height_shift* are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally:\n  We tried in the models to changes this by 20% horizontally and vertically\n* *shear_range* is for randomly applying shearing transformations. We tried 20 % shear_range\n* *zoom_range* is for randomly zooming inside pictures. Again here we tried 20 % zoom_range\n* *horizontal_flip* is for randomly flipping half the images horizontally—relevant when there are no assumptions of horizontal asymmetry (for example,real-world pictures. Since in the images all the birds seem to be sitting vertically I did not try this option.\n* *fill_mode* is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift. We have used nearest here"},{"metadata":{"_uuid":"18d8a607-c773-48b8-8a2c-a6ee1376a43e","_cell_guid":"771477c6-9f01-4308-889c-273a59a2d9eb","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Used to make data more uniform across screen.\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:95% !important; }</style>\"))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Load the REQUIRED libraries\n# This magic function not found\n#%tensorflow_version 2.x\n#from tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import backend, models, layers, regularizers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing import image\n# Libraries needed to build model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten,Dropout,AveragePooling2D,DepthwiseConv2D\n# library to read the kaggle authentication fike\nimport json\nfrom IPython.display import display # Library to help view images\nfrom PIL import Image # Library to help view images\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # Library for data augmentation\n\nimport os, shutil # Library for navigating files\n\n# These are functions that are needed in Google Collab\n#from google.colab import drive # Library to mount google drives\n# Colab library to upload files to notebook\n#from google.colab import files\n\nimport time\n\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\n\n# display images\nfrom IPython.display import Image\n\nfrom tensorflow.keras.utils import plot_model # This will print model architecture.\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport seaborn as sns\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Move files to /kaggle/working directory\n#!cp -r /kaggle/input/100-bird-species/ /kaggle/working/190-bird-species\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nlist_image=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        list_image.append(os.path.join(dirname, filename))\n        \n# View a few of the images\nfor i in range(14, 39):\n    plt.subplot(5, 5, + 1 + i-14)\n    image = mpimg.imread(list_image[i-14])\n    plt.imshow(image)\n    #imgplot = plt.imshow(img)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary of model:**\n1. The model below used pre-trained network MobileNet to transfer weights to models and use data augmentation. I have kept the weights of all layers and added a few more dense layers to train with the new dataset\n1. We add a dense layer with dropout to this architecture before predicting the multiple classes.\n1. We use adam optimizer with learning rate of 1e-5 and decay rate 1e-6\n1. Used EarlyStopping based on accuracy monitoring with patient of 5 and with restore_best_weights=True"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use the MobileNet CNN that was trained on ImageNet data which gives better performance than others like Inception_V3 and VGG16\nmobile = MobileNet(include_top=False,input_shape=(224,224,3),pooling='avg', weights='imagenet',alpha=1, depth_multiplier=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mobile.summary()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(mobile, show_layer_names=True,show_shapes=True ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify the traning, validation, and test directories.  \nbase_dir='/kaggle/input/100-bird-species'\ntrain_dir = os.path.join(base_dir,'train')\nvalidation_dir = os.path.join(base_dir,'valid')\ntest_dir = os.path.join(base_dir,'test')\nconsolidated_dir=os.path.join(base_dir,'consolidated')\n\n# Given the number of bird species that need to be predicted can change we will make this a parameter\nclass_num=len(os.listdir(train_dir))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef round_down(n, decimals=0):\n    multiplier = 10 ** decimals\n    return math.floor(n * multiplier) / multiplier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_batch=128\n# Data Augmentation on training set\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   #horizontal_flip=True, # Flip image horizontally \n                                   samplewise_center=True,\n                                   samplewise_std_normalization=True,\n                                   fill_mode='nearest')\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Since the file images are in a dirrectory we need to move them from the directory into the model.  \n# Keras as a function that makes this easy. Documentaion is here: https://keras.io/preprocessing/image/\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir, # The directory where the train data is located\n    target_size=(224, 224), # Reshape the image to 150 by 150 pixels. This is important because it makes sure all images are the same size.\n    batch_size=train_batch, # We will take images in batches of 200.\n    class_mode='categorical') # The classification is multiple categories.\n\nvalidation_generator = train_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(224, 224),\n    class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    class_mode='categorical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    class_mode='categorical',\n    shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_steps_per_epoch=round_down(len(train_generator.filenames)/train_batch)\nvalidation_batch=32\nvalid_steps=round_down(len(validation_generator.filenames)/validation_batch)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"backend.clear_session()\n# We will use the MobileNet CNN that was trained on ImageNet data which gives better performance than others like Inception_V3 and VGG16\nmobile = MobileNet(include_top=False,input_shape=(224,224,3),weights='imagenet')\nmodel_name='MobileNet'\nmobile.trainable = False\nmobileMod= models.Sequential()\nmobileMod.add(mobile)\nmobileMod.add(layers.Flatten())\nmobileMod.add(layers.Dense(256, activation = 'relu',kernel_regularizer = regularizers.l1(0.00001)))\nmobileMod.add(layers.Dropout(0.4))\nmobileMod.add(layers.Dense(class_num, activation = 'softmax'))\n\n# We will still use the same generators with data augmentation defined above\nepoch=40\nstart_3 = time.perf_counter()\n\nmobileMod.compile(optimizer = 'adam',\n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy'])\n\nhistory= mobileMod.fit_generator(\n    train_generator,\n    steps_per_epoch=train_steps_per_epoch,\n    epochs=epoch,\n    validation_data=validation_generator,\n    validation_steps=valid_steps,\n    verbose = 2,\n    callbacks = [EarlyStopping(monitor='accuracy', patience = 5, restore_best_weights=True)])\nend_3 = time.perf_counter()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code to plot performance in difference epochs. Here I am not seeing the graphs because only epoch was run above. But google collab should give the graph\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, len(history_dict['accuracy']) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the performance of the model on test dataset. Low here because of the above issue of only epoch run\ntest_loss, test_acc = mobileMod.evaluate_generator(test_generator, steps = valid_steps)\nprint('test_acc for {} is {}'.format(model_name,test_acc))\nprint('Loaded {} feature extractor in {:.2f}sec'.format(model_name, end_3-start_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will now look at the images that were incorrectly classfied by model\n# reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order or use shuffle=false\n\nY_pred = mobileMod.predict_generator(prediction_generator,verbose=1)\npredicted_classes = np.argmax(np.round(Y_pred), axis=1)\npredicted_class_indices=np.argmax(Y_pred,axis=1)\n\n#labels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\ntest_labels=[labels[k] for k in prediction_generator.classes]\nfilenames=test_generator.filenames\nresults=pd.DataFrame({\"Filename\":filenames,\"labels\":test_labels,\"Predictions\":predictions}) #\n# Look at the images that have been misclassified\nrslt_df = results[results['labels'] != results['Predictions']] \n#print(rslt_df)\n\n#Add full path to the Filename\nrslt_df.loc[rslt_df.index, 'Filename'] = '/kaggle/input/100-bird-species/test/' + rslt_df['Filename'].astype(str)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To display misclassified images in pandas dataframe we will use the following function which have adopted from https://stackoverflow.com/questions/46107348/how-to-display-image-stored-in-pandas-dataframe\nimport glob\nimport random\nimport base64\nimport pandas as pd\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom IPython.display import HTML\nimport io\n\npd.set_option('display.max_colwidth', -1)\n\n\ndef get_thumbnail(path):\n    #path = \"\\\\\\\\?\\\\\"+path # This \"\\\\\\\\?\\\\\" is used to prevent problems with long Windows paths\n    path = path # This \"\\\\\\\\?\\\\\" is used to prevent problems with long Windows paths\n    i = Image.open(path)    \n    return i\n\ndef image_base64(im):\n    if isinstance(im, str):\n        im = get_thumbnail(im)\n    with BytesIO() as buffer:\n        im.save(buffer, 'jpeg')\n        return base64.b64encode(buffer.getvalue()).decode()\n\ndef image_formatter(im):\n    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can pass our local image path to get_thumbnail(path) with following:\n\nrslt_df['FilenamePill'] = rslt_df.Filename.map(lambda f: get_thumbnail(f))\n\n#view pandas dataframe with resized images by call image_formatter function in IPython.display HTML function:\nHTML(rslt_df.to_html(formatters={'FilenamePill': image_formatter}, escape=False))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the misclassified image above that the model had trouble classifying Albatross image when in different settings. Since the background based on location and flying or sitting position can change we should use data augmentation to enhance the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class_indices=np.argmax(Y_pred,axis=1)\npredictions = [labels[k] for k in predicted_class_indices]\ntest_labels=[labels[k] for k in test_generator.classes]\n\nplt.subplots(figsize=(20,15))\n#sns.heatmap(confusion_matrix(test_generator.classes, y_pred))\nsns.heatmap(confusion_matrix(test_labels, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification Report')\nprint(classification_report(test_generator.classes, predicted_class_indices, target_names=list(test_generator.class_indices.keys())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here is a summary of the results of other runs:**\n\n1) test_acc for InceptionV3 with weights not frozen in last 4 layers is 0.6369612812995911\noptimizer used =Adam(lr=0.0001)\nNo data augmentation used\nTime taken for model to fit = 591.04sec\nThis was run for just 10 epochs and could have performed better if run for a higher number of epochs. \n\n\n2) test_acc for MobileNet without Data Augmentation and Adam classifier is 0.5638841390609741\nHere we used the default settings on adam optimizer. \non fit_generator we used :\ncallbacks=[ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 5, verbose = 1)\nLoaded MobileNet with Data Augmentation and Adam classifier feature extractor in 2203.69sec (0.61 hr)\nEven though the sepeed was better the performance was not as good\n\n3) test_acc for MobileNet with Data Augmentation and Adam classifier was  0.8897005319595337\noptimizerA=Adam(lr=0.0001)\nLoaded MobileNet with Data Augmentation feature extractor in 1510.89sec which was relatively fast. \nWe however had trained this model on just 20 epochs. It could have done better with higher number of epochs. It is also possible to play with batch size. Here I had used 200 with steps_per_epoch of 125. We can reduce the batch size and increase steps_per_epoch. \n\n4) test_acc for MobileNet with Data Augmentation and SGD Optimizer and controlled batch size is 0.9264943599700928\nHere we have used \nSGD(lr=0.0001, momentum=0.9, nesterov=True)\nHere we used the callbacks :\n[EarlyStopping(monitor = 'val_accuracy', patience = 5,restore_best_weights = True),\nReduceLROnPlateau(monitor = 'val_loss', factor = 0.7,patience = 5, verbose = 1)]) \nWe had specified 50 epochs and did have early stopping hence we could have run the model for a higher number of epochs\n\n5) Loaded MobileNet with Data Augmentation and SGD Optimizer and controlled batch size feature extractor in 13745.94sec (3.82 hrs). The time taken with a low learning rate is high.\n\n\ntest_acc for MobileNet with rmsprop optimizer with default setting was  0.8850405216217041\nHere during model fitting we used callbacks=[LearningRateScheduler(lr_schedule)]\nwhere the function was defined as follows\n\ndef lr_schedule(epoch) :\n    lrate = 0.0001\n    if epoch > 15:\n        lrate = 0.00005\n    if epoch > 20:\n        lrate = 0.00001\n    return lrate \nWe added 2 dense layes in this architecture with 40 % drop off and l1 regularizers. "},{"metadata":{},"cell_type":"markdown","source":"ROC curves are typically used in binary classification to study the output of a classifier hence I have not used it here. \n\n**Analysis of Results**:\n1. When using TransferLearning MobileNet gave better performance than Inception_V3, VGG16. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks.\n1. Adjusting the learning rate on the optimizer helps in better convergence of the model. Both adam, sgd did well with a learning rate around 0.0001\n1. Data Augmentation did not apparently provide any benefit on model performance"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}