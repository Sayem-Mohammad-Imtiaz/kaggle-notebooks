{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"771236dfba7bf693b1e0dfeab1786a4a5df6ebe4","_cell_guid":"6c41e85d-7885-4157-82b4-ee5ed14540a1"},"execution_count":null,"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# import \nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load dataset from CSV File using Pandas"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"data = pd.read_csv('../input/faults.csv')","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop Redundant type of steel feature (since they are in one-hot encoding and only 2, one of them can be dropped without any loss)\n  #Split data in features and labels and convert to numpy array"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"data.drop('TypeOfSteel_A400', axis = 1)\nfeatures = data.values\nlabels = features[:,27:34]\nfeatures = features[:,0:27]","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for co-relation between features"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"df = pd.DataFrame(features)\ndf.corr()\n#no correlation found.","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for Null or missing values in data"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"df.isnull().values.any()\n# No values or NaN or Null","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert One Hot Encoding to class labels"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"labels = [np.argmax(row) for row in labels]","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train test split with test size of 30%"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"features_train, features_test, labels_train, labels_test = train_test_split(features, labels,test_size=0.30,random_state=42)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Random Forest Classifier\nOther classifier were also tested, but Random Forest Provided the most accuracy for this task"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"rf_clf = RandomForestClassifier(random_state=17, min_samples_split=2, n_estimators=55)\n# Parameters tuned using GridCV function below. Avoided running here for brevity and saving time\nrf_clf.fit(features_train,labels_train)\nrf_pred = rf_clf.predict(features_test)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy Score and Classification Report "},{"metadata":{},"execution_count":null,"cell_type":"code","source":"print(accuracy_score(rf_pred, labels_test))\nprint(classification_report(rf_pred,labels_test))","outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# We are getting accuracy of arounf 78% and a good F1 score for each of the labels, except the \"0\"th."},{"metadata":{},"cell_type":"markdown","source":"# Functions for tuning the Random Forest Classifier"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"# these functions will be time consuming, depending on the number of parameters\n# input features and class labels\n# return tuned classifier and the best score\ndef TuneRandomForest(features, labels):\n    min_samples_split = np.arange(2,100)\n    n_estimators = np.arange(10,100)\n    parameters = {'n_estimators' : n_estimators,'min_samples_split':min_samples_split}\n    clf = RandomForestClassifier()\n    return gridCVTune(clf,parameters)\n\n# inputs are the paramaters you wish to tune for and model and features and class labels\n# output is tuned model and best score.\ndef gridCVTune(clf,parameters, features, labels):\n    gridCV= GridSearchCV(clf,parameters)\n    gridCV.fit(features,labels)\n    return gridCV.best_estimator_, gridCV.best_score_","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"","outputs":[]}],"nbformat_minor":1}