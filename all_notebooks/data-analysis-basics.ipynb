{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****DATA ANALYSÄ°S****","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head() # this code helps us to read first 5 informations about data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail() #this code helps us to read last 5 informations about data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data)) #length of data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() # basic informations about data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.columns) # clolumn's names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr() #corrolations between some values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By analyzing this figure we can think about some didactions. For example, there are some connection between popularity and revenue, budget and revenue, vote count and popularity etc. By the helping of these corrolations we can say that if budget increase, revenue would increased accordingly. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Line Plot\n# color = color, label = label, linewidth = width of line, alpha = opacity, grid = grid, linestyle = sytle of line\ndata.budget.plot(kind = 'line', color = 'red',label = 'Budget',linewidth=1,alpha = 1,grid = True,linestyle = ':')\ndata.revenue.plot(color = 'black',label = 'Revenue',linewidth=1, alpha = 0.5,grid = True,linestyle = '-.')\nplt.legend(loc='upper right')     # legend = puts label into plot\nplt.xlabel('x axis')              # label = name of label\nplt.ylabel('y axis')\nplt.title('Line Plot')            # title = title of plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plot \n# x = budget, y = revenue\ndata.plot(kind='scatter', x='budget', y='revenue',alpha = 0.5,color = 'purple')\nplt.xlabel('Budget')              # label = name of label\nplt.ylabel('Revenue')\nplt.title('Budget Revenue Scatter Plot')            # title = title of plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram\n# bins = number of bar in figure\ndata.budget.plot(kind = 'hist',bins = 50,figsize = (12,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filtering\n\nx = data[\"budget\"] > 200000000\ndata[x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data[\"revenue\"] > 200000000\ndata[x]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These codes affirm my first theory. Becouse if we anlyze budget bigger than 200000000 we can only achieve 98 films. However, we can achieve 4496 films their revenue bigger than 200000000.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,value in data[['budget']][0:2].iterrows():\n    print(index,\" : \",value)\n    \n    \nfor index,value in data[['revenue']][0:2].iterrows():\n    print(index,\" : \",value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column = \"popularity\", by = \"budget\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleaning Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape # Rows and columns of the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['genres'].value_counts(dropna =False)) # Frequency of genres","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, There are 370 drama films and 282 comedy films ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we seen above min popularity is 0 otherwise, max is 875.581305","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column='revenue',by = 'budget') # shows us basics statistics like outliers, quentiles.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****Tidy Data****\n\nWe use melt() method. Easily, you can choose some features and rename it to make it more understandable for you. In this way, you can create very little data(table).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = data.head() # First lets create new data to make it easy\ndata_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets melt\n# id_vars = what we do not wish to melt\n# value_vars = what we want to melt\nmelted = pd.melt(frame=data_new,id_vars = 'Name', value_vars= ['budget','revenue'])\nmelted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****Pivoting Data****\n\nReverse of melting data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Index is name\n# I want to make that columns are variable\n# Finally values in columns are value\nmp = melted.pivot(index = 'Name', columns = 'variable', values = 'value')\nmp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****Contcatinating****\nBy using this method we can sort two datas.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data.head()\ndata2= data.tail()\nconcat_hor = pd.concat([data1,data2],axis =0,ignore_index =True) # axis = 0 : adds dataframes in row (in short, this is about vertically  or horizantily.)\nconcat_hor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data['budget'].head()\ndata2= data['revenue'].head()\nconcat_ver = pd.concat([data1,data2],axis =1) # axis = 1 : adds dataframes in columns.\nconcat_ver","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes # Shows us what kind of datatypes used in data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets convert object(str) to categorical and int to float.\ndata['production_companies'] = data['production_companies'].astype('category')\ndata['vote_count'] = data['vote_count'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above; vote count is converted to float, production_companies is converted to category.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# ****Missing Data and Testing it With Assert****\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In data there are 4803 entries but if we analyse tagline; there are 3959 non-null objects. Its mean there are 844 null objects.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets chech tagline\ndata[\"tagline\"].value_counts(dropna =False)\n# As you can see, there are 844 NAN value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets drop nan values\ndata1 = data   # also we will use data to fill missing value so I assign it to data1 variable\ndata1[\"tagline\"].dropna(inplace = True)  #dropna means drop null objects, #inplace true means save informations to data.\n# So does it work ?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"We will check it by using assert method. On other words, putting something forward.\nAs an illustration:\nassert 2+2 = 4 --> will return nothing becouse calculation is true,\nSo if we make a mistake:\nassert 2+2 = 5 --> will return error becouse calculation is not true.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert  data1['tagline'].notnull().all() # returns nothing because we drop nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1[\"tagline\"].fillna('empty',inplace = True) #The fillna() function is used to fill NA/NaN values using the specified method.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert  data1['tagline'].notnull().all() # returns nothing because we do not have nan values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # With assert statement we can check a lot of thing. For example\n# assert data.columns[1] == 'Name'\n# assert data.Speed.dtypes == np.int","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For example lest make a mistake:\nassert data.columns[1] == \"budget\" #will return error becouse index of budget is 0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****Pandas Foundation****","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will build data frames by using dictionary and zip methods.\nzip() method: This function returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"films = [\"The Shawshank Redemption\", \"The Godfather\", \"The Dark Knight\"]\nimdb = [\"9,3\", \"9,2\", \"9.0\"]\nlist_label = [\"Name\", \"Imdb\"]\nlist_col = [films, imdb]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped)\ndf = pd.DataFrame(data_dict)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding new columns\ndf[\"Years\"] = [\"1994\", \"1972\", \"2008\"]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Broadcasting : adding a column but each of these columns have same value.\ndf[\"Voted from\"] = \"Us Box Office\" # But this is a assumption.\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****VISUAL EXPLORATORY DATA ANALYSIS****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting\ndata1 = data.loc[:,[\"budget\",\"revenue\",\"popularity\"]]\ndata1.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subplots\ndata1.plot(subplots = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot  \ndata1.plot(kind = \"scatter\",x=\"budget\",y = \"revenue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hist plot  \ndata1.plot(kind = \"hist\",y = \"popularity\",bins = 50,range= (0,250)) # range shows gap of row\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\ndata1.plot(kind = \"hist\",y = \"popularity\",bins = 50,range= (0,250),ax = axes[0])\ndata1.plot(kind = \"hist\",y = \"popularity\",bins = 50,range= (0,250),ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****STATISTICAL EXPLORATORY DATA ANALYSIS****\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****INDEXING PANDAS TIME SERIES****\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in 12.index there are relase dates\nrl_date = data.release_date\nprint(type(rl_date))\ndatetime_object = pd.to_datetime(rl_date) #converting to datetime data type\nprint(type(datetime_object))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# In order to practice lets take head of our data and add it a time list\ndata2 = data.head()\ndate_list = rl_date\ndatetime_object = pd.to_datetime(date_list)\ndata2[\"date\"] = datetime_object\n# lets make date as index\ndata2= data2.set_index(\"date\")\ndata2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can select according to our date index\nprint(data2.loc[\"2009-12-10\"])\nprint(\"---------------------------------------------------------------------------\")\nprint(data2.loc[\"2009-03-10\":\"2015-01-16\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****RESAMPLING PANDAS TIME SERIES****\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Resampling: statistical method over different time intervals\nNeeds string to specify frequency like \"M\" = month or \"A\" = year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use data2 that we create at previous part\ndata2.resample(\"A\").mean() #resemple is a method","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets resample with month\ndata2.resample(\"M\").mean()\n# As you can see there are a lot of nan because data2 does not include all months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In real life (data is real. Not created from us like data2) we can solve this problem with interpolate\n# We can interpolete from first value\ndata2.resample(\"M\").first().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can interpolate with mean()\ndata2.resample(\"M\").mean().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****MANIPULATING DATA FRAMES WITH PANDAS****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\n#data= data.set_index(\"#\") # make # index of data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexing using square brackets\ndata[\"original_title\"][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.original_title[1] #alternatively","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using loc accessor\ndata.loc[1,[\"original_title\"]] # loc --> Access a group of rows and columns by label(s) or a boolean array.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting only some columns\ndata[[\"original_title\",\"vote_average\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****SLICING DATA FRAME****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slicing and indexing series\ndata.loc[1:10,\"original_title\":\"vote_average\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reverse slicing \ndata.loc[10:1:-1,\"original_title\":\"vote_average\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From something to end\ndata.loc[1:10, \"tagline\": ] # features after tagline ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****FILTERING DATA FRAMES****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating boolean series\nboolean = data.vote_average > 8.5\ndata[boolean]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining filters\nfirst_filter = data.vote_average > 5.0\nsecond_filter = data.budget > 150000000\ndata[first_filter & second_filter]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering column based others\ndata.title[data.revenue>1000000000] # nested features, title that film's revenues bigger than 1000000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining column using other columns\ndata[\"profit\"] = data.revenue - data.budget\ndata.head() # at the end you can see","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****INDEX OBJECTS AND LABELED DATA****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# our index name is this:\nprint(data.index.name)\n# lets change it, it was nothing\ndata.index.name = \"index_name\"\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overwrite index\n# if we want to modify index we need to change all of them.\ndata.head()\n# first copy of our data to data3 then change index \ndata3 = data.copy() # coppying for not break down our main data\n# lets make index start from 100. It is not remarkable change but it is just example\ndata3.index = range(100,4903,1)\ndata3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****HIERARCHICAL INDEXING****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\ndata.head()\n# As you can see there is index. However we want to set one or more column to be index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting index : original_title is outer release_date  is inner index\ndata1 = data.set_index([\"original_title\",\"release_date\"]) \ndata1.head(50)\n# data1.loc[\"Avatar\",\"2009-12-10\"] # how to use indexes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****PIVOTING DATA FRAMES****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {\"species\":[\"cat\",\"cat\",\"dog\",\"dog\"],\"gender\":[\"F\",\"M\",\"F\",\"M\"],\"agression_%\":[20,35,42,53],\"age\":[1,4,3,7]}\ndf = pd.DataFrame(dic)\ndf #our new data frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivoting\ndf.pivot(index=\"species\",columns = \"gender\",values=\"agression_%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****STACKING and UNSTACKING DATAFRAME****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.set_index([\"species\",\"gender\"])\ndf1\n# lets unstack it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# level determines indexes\ndf1.unstack(level=0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.unstack(level=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change inner and outer level index position\ndf2 = df1.swaplevel(0,1)\ndf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****MELTING DATA FRAMES****\nMelting is reverse of pivoting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.pivot(index=\"species\",columns = \"gender\",values=\"agression_%\")\npd.melt(df,id_vars=\"species\",value_vars=[\"age\",\"agression_%\"]) --> we melted gender","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****CATEGORICALS AND GROUPBY****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"species\").mean() # --> make group means belongs to species.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also you can use it for min max methods etc.\ndf.groupby(\"species\").std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can only choose one of the feature\ndf.groupby(\"agression_%\").age.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can choose multiple features\ndf.groupby(\"species\")[[\"age\",\"agression_%\"]].min() # min age and min agression belongs to specific species.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank you.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}