{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A Simple Implementation of Gradient Descent for Linear Regression","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression Class","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class LinearRegress:\n    \n    \"\"\"\n    Linear Regression with Gradient Descent.\n    \"\"\"\n    def __init__(self,bias=True, learning_rate = 0.01, max_iteration=1000, keep_hist = True):\n        \"\"\"\n        Linear Regression with Gradient Descent.\n        \n        params\n        ------\n        bias: bool. Default=True. Whether to add bias variable or not.\n        \n        learning_rate: float. learning rate for gradient descent. \n        \n        max_iteration: scalar. Maximum iteration for Gradient Descent.\n        \n        keep_hist: bool. If true, stores J and W for each iteration.\n        \n        \"\"\"\n        self.bias = bias\n        self.w = None\n        self.error = None\n        self.alpha = learning_rate\n        self.max_iter = max_iteration\n        self.n_samples = None\n        self.n_features = None\n        self.keep_hist = keep_hist\n        self.history = []\n        \n    def gradient_descent(self,X,y):\n        \"\"\"\n        Simple SGD. \n\n        params\n        ------\n        X: Predictors. Shape(n_samples, n_features)\n\n        y: Target. Shape(n_samples)\n\n        \"\"\"\n        n_samples = len(y)\n        \n        # inserts column of 1s for bias.\n        X = np.insert(X,0,1,axis=1) \n        \n        # Weight Matrix. Shape(n_features,1)\n        w = np.random.rand(X.shape[1])\n        \n        for itr in range(self.max_iter):\n            # forward pass\n            yhat = np.dot(X,w) # (n_samples, 1)\n            # objective\n            error = yhat - y\n            J = (1/(2 * n_samples)) * np.dot(error,error)\n            \n            # backward pass\n            w = w - (self.alpha / n_samples) * np.dot(X.T,error)\n            \n            if self.keep_hist is True:\n                self.history.append((J,w))\n            if self.verbose and (itr % 100 == 0 or itr == self.max_iter - 1):\n                print(\"Iteration: %d J: %.2f\" % (itr,J))\n        self.w = w\n\n    def fit(self,X,y,verbose=True):\n        \"\"\"\n        Train linear regression model.\n        \"\"\"\n        self.verbose = verbose\n        self.gradient_descent(X,y)\n\n    def score(self,X,y):\n        \"\"\"\n        Return the coefficient of determination R^2 of the prediction.\n        \n        params\n        ------\n        X: array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead, shape = (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.\n            \n        y: array-like of shape (n_samples,)\n            True values for X.\n        \n        returns\n        -------\n        score: float.\n            R^2 score.\n        \"\"\"\n        yhat = self.predict(X)\n        return r2_score(y,yhat)\n    \n    def predict(self,X):\n        \"\"\"\n        Predicts using linear model.\n        \n        params\n        ------\n        X: array-like of shape (n_samples, n_features)\n            Test samples.\n        returns\n        -------\n        y: array-like of shape(n_samples,)\n            Output of the model.\n        \"\"\"\n        X = np.insert(X,0,1,axis=1)\n        yhat = np.dot(X,self.w)\n        return yhat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing\nOn UCI Heart Dataset. This is a ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# heart data\ndf = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\nX = df.iloc[:,:-1].copy()\ny = df.iloc[:,-1].copy()\nX_train,X_test,y_train,y_test = train_test_split(X.values,y.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegress(learning_rate=0.000001,max_iteration=1000)\nmodel.fit(X_train,y_train,verbose=True)\nmodel.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the history of cost and weights over iteration period","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(facecolor='white')\ncosts = [tpl[0] for tpl in model.history]\ncoefs = [tpl[1] for tpl in model.history]\nax.plot(range(len(costs)),costs,color='green')\nax.set_xlabel('Iteration')\nax.set_ylabel('Objective')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing with SKLEARN's SGDRegressor.\n\n*It is obvious that R^2 score is not the only thing to compare in two models. SGDRegressor is better and fits more complex linear data*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sk = SGDRegressor()\nsk.fit(X_train,y_train)\nsk.score(X_test,y_test) > model.score(X_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}