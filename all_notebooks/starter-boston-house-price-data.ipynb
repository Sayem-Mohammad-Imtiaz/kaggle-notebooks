{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomScaler(BaseEstimator):\n    def __init__(self, columns ):\n        self.scaler = StandardScaler()\n        self.columns = columns\n        self.mean_ = None\n        self.std_ = None\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.std_ = np.std(X[self.columns])\n        return self\n    \n    def transform(self, X, y=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns,index=X.index)\n        X_not_scaled = X.loc[:, ~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n#     if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n#         columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/J4QlqZu.jpg\"/>\n\n\n## Introduction\nGreetings starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Analysis"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # plotting\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 1 csv file in the current version of the dataset:\n"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/Boston-house-price-data.csv', delimiter=',')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:**\n* here we observe that all columns are of numeric datatype\n* we can also observe that all of non-null column values have 506, which is fortunately equals to total rows in dataframe\n* i.e., we dont have any null values in this dataframe"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('total number of null values : {0}'.format(df.isna().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**\n* here if we observe standard-deviation is much larger than mean for few of the columns which we need to normalize\n* we need to check the distribution by ploting the data, and do the required normalizations"},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Analysis"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,9))\ncorr = df.corr().round(2)\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# # Want diagonal elements as well\n# mask[np.diag_indices_from(mask)] = False\n\nsns.heatmap(data=corr, annot=True,cmap='coolwarm',mask=mask)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**\n* columns RAD & TAX are highly positively correlated \n    * **inference**: as the accessibility to radial highways increases so does the porperty TAX\n* columns DIS is highly negatively correlated with INDUS, NOX, AGE\n    * **inference**: as the distances to five boston employment centres increases\n    * proportion of non-retail business acres per town decreases\n    * nitric oxides concentration (parts per 10 million) decreases\n    * proportion of owner-occupied units built prior to 1940 decreases\n* column LSTAT & MEDV are highly negatively correlated\n    * **inference**: as the % lower status of the population increases \n    * Median value of owner-occupied homes decreases"},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for col in df.columns:\n    fig,ax = plt.subplots(1,2,figsize=(15,1.5))\n    if len(np.unique(df[col]))<10:\n        sns.countplot(df[col],ax=ax[0])\n    else:\n        sns.distplot(df[col],bins=50 if len(np.unique(df[col]))>50 else None,ax=ax[0])\n        \n    sns.boxplot(df[col],ax=ax[1])\n    plt.suptitle(col,fontsize=20,y=1.2)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization"},{"metadata":{},"cell_type":"markdown","source":"### Now lets select few columns from do some normalization techniques"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"columns = [col for col in df.columns if len(np.unique(df[col]))>50]\ncolumns.remove('MEDV')\ncolumns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for col in columns:\n    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,1.5))\n    \n    sns.distplot(df[col],bins=50,ax=ax[0])\n    ax[0].set_title('original')\n    \n    quantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal',n_quantiles=int(len(df)/20), random_state=0)\n    X_trans = quantile_transformer.fit_transform(df[col].values.reshape((len(df),1)))\n    sns.distplot(X_trans,bins=50,ax=ax[1])\n    ax[1].set_title('normalized')\n    \n    plt.suptitle(col,fontsize=20,y=1.2)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Operations on dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"c = columns.copy()\nc.append('MEDV')\nX = df[c]\nX_train = X.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clipping outliers from train data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for k, v in X_train.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 / np.shape(X_train)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Q1 = X_train.quantile(0.25)\nQ3 = X_train.quantile(0.75)\nIQR = Q3 - Q1\n\nX_train = X_train[~((X_train < (Q1 - 1.5 * IQR)) |(X_train > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Variable MEDV"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.distplot(X_train['MEDV']);plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Independent variables (INPUTS)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols = 3\nrows = int(len(X_train.drop('MEDV',axis=1).columns)/cols)\n\nplt.figure(figsize=(15,10))\nfor i,col in enumerate(X_train.drop('MEDV',axis=1).columns):\n    ax = plt.subplot(rows, cols, i+1)\n    sns.distplot(X_train[col],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train, y_train = X_train.drop('MEDV',axis=1), X_train['MEDV']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOG Transform target variable for better results"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.log(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as the data is small lets not split for the validation data instead go for cross validation"},{"metadata":{},"cell_type":"markdown","source":"lets do normalization on the train and transform test data with it"},{"metadata":{},"cell_type":"markdown","source":"# Normalization"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"quantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal',n_quantiles=int(len(X_trans)/20), random_state=0)\nX_train.loc[:,columns] = quantile_transformer.fit_transform(X_train[columns].values.reshape((len(X_train),len(columns))))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols = 3\nrows = int(len(X_train.columns)/cols)\n\nplt.figure(figsize=(15,10))\nfor i,col in enumerate(X_train.columns):\n    ax = plt.subplot(rows, cols, i+1)\n    sns.distplot(X_train[col],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardization"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scaler = CustomScaler(columns)#check at the start of the book to find the CustomScaler\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now the data has been scaled"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols = 3\nrows = int(len(X_train.columns)/cols)\n\nplt.figure(figsize=(15,10))\nfor i,col in enumerate(X_train.columns):\n    ax = plt.subplot(rows, cols, i+1)\n    sns.distplot(X_train[col],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cutting outliers again after normalization"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X = X_train.copy()\nX.loc[:,'MEDV']=y_train\nQ1 = X.quantile(0.25)\nQ3 = X.quantile(0.75)\nIQR = Q3 - Q1\n\nX = X[~((X < (Q1 - 1.5 * IQR)) |(X > (Q3 + 1.5 * IQR))).any(axis=1)]\ny_train = X['MEDV']\nX_train = X.drop('MEDV',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for k, v in X_train.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 / np.shape(X_train)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scores_map={}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nLR_model = LinearRegression()\nscores = cross_val_score(LR_model,X_train,y_train,cv=10,n_jobs=-1,scoring='neg_mean_squared_error')\nscores_map['LR']=scores\nprint('Logistic Regression negative RMSE {:.3f} (+/- {:.3f})'.format(scores.mean(),scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine Regressor (SVR)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\n\n\nsvr_rbf = SVR(kernel='rbf')\ngrid = GridSearchCV(svr_rbf, cv=10, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best parameters :\", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_rbf = SVR(kernel='rbf',C=10,gamma=0.01)\n\nscores = cross_val_score(svr_rbf,X_train,y_train,cv=10,n_jobs=-1,scoring='neg_mean_squared_error')\nscores_map['SVR']=scores\nprint('SVR negative RMSE {:.3f} (+/- {:.3f})'.format(scores.mean(),scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(random_state=0)\ngrid = GridSearchCV(tree, cv=10, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best parameters : \", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor(max_depth=7)\nscores = cross_val_score(tree, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\nscores_map['DTree'] = scores\nprint(\"D.Tree negative RMSE {:.3f} (+/- {:.3f})\".format(scores.mean(),scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K Nearest Neighbours Regression (KNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor()\n\ngrid = GridSearchCV(knn, cv=10, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best parameters :\", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsRegressor(n_neighbors=4)\nscores = cross_val_score(knn, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\nscores_map['KNN'] = scores\nprint(\"KNN negative RMSE {:.3f} (+/- {:.3f})\".format(scores.mean(),scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(random_state=0)\nparam_grid={'n_estimators':[50,100,150, 200], 'learning_rate': [0.5,0.1,0.05,0.02,0.001]\n            , 'max_depth':[2, 3,4,5,6,7,8], 'min_samples_leaf':[3,5,9,11,14,16]\n            ,'min_samples_split':[2,4,6,8,10], 'alpha':[0.05,0.1,0.3,0.5]}\n# grid = GridSearchCV(gbr, cv=10, param_grid=param_grid, scoring='neg_mean_squared_error')\ngrid = RandomizedSearchCV(gbr, cv=10, param_distributions=param_grid, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best params :\", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(n_estimators=200,min_samples_split=2,min_samples_leaf=3,max_depth=8,learning_rate=0.02,alpha=0.05,   random_state=0)\nscores = cross_val_score(gbr, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\nscores_map['GBR'] = scores\nprint(\"GBR negative RMSE {:.3f} (+/- {:.3f})\".format(scores.mean(),scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance Comparisions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nscores_map = pd.DataFrame(scores_map)\nsns.boxplot(data=scores_map)\nplt.xticks(fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**\n* almost all regressors are performing sholder to sholder"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}