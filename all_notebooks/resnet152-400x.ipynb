{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install Pillow==5.3.0\n!pip install image\n!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import PIL\nprint(PIL.PILLOW_VERSION)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport time\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nimport torchvision\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom PIL import Image\nfrom torch.optim import lr_scheduler\nimport copy\nimport json\nimport os\nfrom os.path import exists\n# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Organizing the dataset\ndata_dir = '../input/breakhis-400x/BreaKHis 400X'\ntrain_dir = data_dir + '/train'\nvalid_dir = data_dir + '/test'\nbatch_size = 32\nuse_gpu = torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nwith open('../input/catname/cat_to_name (1).json', 'r') as f:\n    cat_to_name = json.load(f)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Définissez vos transformations pour les ensembles de formation et de validation\n# Augmentation et normalisation des données pour la formation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomRotation(30),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n#Charger les jeux de données avec ImageFolder\ndata_dir = '../input/breakhis-400x/BreaKHis 400X'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])for x in ['train', 'test']}\n#print(image_datasets)\n# À l'aide des jeux de données d'images et des trains, définissez les chargeurs de données\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'test']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\nprint(dataset_sizes)\n\nclass_names = image_datasets['test'].classes\n\nprint(class_names)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.resnet152(pretrained=True)\n# Geler les paramètres\nfor param in model.parameters():\n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\n\n# Remplacement du classificateur de modèle pré-formé par notre classificateur\n\nclassifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(2048, 512)),\n                          ('relu', nn.ReLU()),\n                          ('dropout1', nn.Dropout(p=0.5)),\n                          ('fc2', nn.Linear(512, 2)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\nmodel.fc = classifier\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\n\n\n# Remplacement du classificateur de modèle pré-formé par notre classificateur\n\nmodel.classifier [6] = nn.Sequential ( \n                      nn.Linear (4096, 256), \n                      nn.ReLU (), \n                      nn.Dropout (0.5), \n                      nn.Linear (256, 2),                    \n                      nn.LogSoftmax ( dim = 1))\nprint(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc_v = 0.0\n    best_acc_T = 0.0\n    best_loss_v= 1.0\n    best_loss_T= 1.0\n    loss_dict = {'train': [], 'test': []}\n    acc_dict = {'train': [], 'test': []}\n\n    for epoch in range(1, num_epochs+1):\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs, labels = inputs.to(device), labels.to(device)\n              \n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward() #L'appel de .backward()plusieurs fois accumule le gradient (par addition) pour chaque paramètre. C'est pourquoi vous devez appeler optimizer.zero_grad()après chaque .step()appel.\n                        optimizer.step()#est effectue une mise à jour des paramètres basée sur le gradient actuel SGD\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            if phase == 'train':\n                scheduler.step()\n                #print(labels)\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            loss_dict[phase].append(epoch_loss)\n            acc_dict[phase].append(epoch_acc)\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n\n           # copier en profondeur le modèle\n            if phase == 'test' :\n                if epoch_acc > best_acc_v  :\n                   best_acc_v = epoch_acc\n                   best_model_wts = copy.deepcopy(model.state_dict())\n                if best_loss_v > epoch_loss:\n                   best_loss_v = epoch_loss    \n            if phase == 'train' :\n                if epoch_acc > best_acc_T:\n                   best_acc_T = epoch_acc\n                if best_loss_T > epoch_loss:\n                   best_loss_T = epoch_loss    \n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best valid accuracy: {:4f}'.format(best_acc_v))\n    #print('Best train  accuracy: {:4f}'.format(best_acc_T))\n    #print('valid losss: {:4f}'.format(best_loss_v))\n    #print('train losss: {:4f}'.format(best_loss_T))\n\n\n    #   charger les meilleurs poids de modèle\n    model.load_state_dict(best_model_wts)\n    return model,loss_dict, acc_dict\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ress_loss = {'train': [], 'test': []}\nress_acc = {'train': [], 'test': []}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a model with a pre-trained network\nres_loss = {'train': [], 'test': []}\nres_acc = {'train': [], 'test': []}\nif use_gpu:\n    print (\"Using GPU: \"+ str(use_gpu))\n    model = model.cuda()\n# NLLLoss because our output is LogSoftmax\ncriterion = nn.NLLLoss()\n# Adam optimizer with a learning rate\noptimizer = optim.SGD(model.fc.parameters(), lr=0.006, momentum=0.9)\n# Decay LR by a factor of 0.1 every 5 epochs 15\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n\nmodel_ft,loss_dict, acc_dict = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200)\nres_loss = loss_dict\nres_acc = acc_dict\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ress_loss['train'].extend(loss_dict['train'])\nress_loss['test'].extend(loss_dict['test'])\nress_acc['train'].extend(acc_dict['train'])\nress_acc['test'].extend(acc_dict['test'])\nprint(ress_loss)\nprint(ress_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom torch.utils.data import DataLoader\nplt.rcParams[\"figure.figsize\"] = (13,13)\n\nres_loss = loss_dict\nres_acc = acc_dict\nplt.title(\"Loss\")\n\nplt.plot(ress_loss['train'],label='Training Loss')  \nplt.plot(ress_loss['test'],label='Validation Loss')  \nplt.savefig('loss_resnet152_400X.png')\nplt.legend()  \nplt.show()  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nplt.rcParams[\"figure.figsize\"] = (11,11)\n\nres_loss = loss_dict\nres_acc = acc_dict\nplt.title(\"Accuracy\")\n\nplt.plot(ress_acc['train'],label='Training acc')  \nplt.plot(ress_acc['test'],label='Validation acc')\nplt.savefig('ACC_resnet152_400X.png')\n\nplt.legend()  \nplt.show()  \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nplt.rcParams[\"figure.figsize\"] = (11,11)\n\nres_loss = loss_dict\nres_acc = acc_dict\nplt.title(\"Accuracy and loss\")\n\nplt.plot(ress_acc['train'],label='Training acc')  \nplt.plot(ress_acc['test'],label='Validation acc')\nplt.plot(ress_loss['test'],label='Validation Loss') \nplt.savefig('2_resnet152_400X.png')\n\nplt.legend()  \nplt.show()  \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the checkpoint \nnum_epochs=200\nmodel.class_to_idx = dataloaders['train'].dataset.class_to_idx\nmodel.epochs = num_epochs\ncheckpoint = {'input_size': [3, 224, 224],\n                 'batch_size': dataloaders['train'].batch_size,\n                  'output_size': 2,\n                  'state_dict': model.state_dict(),\n                  'data_transforms': data_transforms,\n                  'optimizer_dict':optimizer.state_dict(),\n                  'class_to_idx': model.class_to_idx,\n                  'epoch': model.epochs,\n                  'ress_loss': ress_loss,\n                  'ress_acc': ress_acc \n             }\ntorch.save(checkpoint, 'resnet152.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = models.resnet152()\n    \n    # Our input_size matches the in_features of pretrained model\n    input_size = 2048\n    output_size = 2\n    \n    classifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(2048, 512)),\n                          ('relu', nn.ReLU()),\n                          #('dropout1', nn.Dropout(p=0.2)),\n                          ('fc2', nn.Linear(512, 2)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n\n# Replacing the pretrained model classifier with our classifier\n    model.fc = classifier\n    \n    \n    model.load_state_dict(checkpoint['state_dict'])\n    \n    return model, checkpoint['class_to_idx']\n# Get index to class mapping\nloaded_model, class_to_idx = load_checkpoint('resnet152.pth')\n\nidx_to_class = { v : k for k,v in class_to_idx.items()}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef process_image(image):\n    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n        returns an Numpy array\n    '''\n    \n    # Process a PIL image for use in a PyTorch model\n\n    size = 256, 256\n    image.thumbnail(size, Image.ANTIALIAS)\n    image = image.crop((128 - 112, 128 - 112, 128 + 112, 128 + 112))\n    npImage = np.array(image)\n    npImage = npImage/255.\n        \n    imgA = npImage[:,:,0]\n    imgB = npImage[:,:,1]\n    imgC = npImage[:,:,2]\n    \n    imgA = (imgA - 0.485)/(0.229) \n    imgB = (imgB - 0.456)/(0.224)\n    imgC = (imgC - 0.406)/(0.225)\n        \n    npImage[:,:,0] = imgA\n    npImage[:,:,1] = imgB\n    npImage[:,:,2] = imgC\n    \n    npImage = np.transpose(npImage, (2,0,1))\n    \n    return npImage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(image, ax=None, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    \n    # PyTorch tensors assume the color channel is the first dimension\n    # but matplotlib assumes is the third dimension\n    image = image.numpy().transpose((1, 2, 0))\n    \n    # Undo preprocessing\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = std * image + mean\n    \n    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n    image = np.clip(image, 0, 1)\n    \n    ax.imshow(image)\n    \n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(image_path, model, topk=2):\n    ''' Predict the class (or classes) of an image using a trained deep learning model.\n    '''\n    \n    # Implement the code to predict the class from an image file\n    \n    image = torch.FloatTensor([process_image(Image.open(image_path))])\n    model.eval()\n    output = model.forward(Variable(image))\n    pobabilities = torch.exp(output).data.numpy()[0]\n    \n\n    top_idx = np.argsort(pobabilities)[-topk:][::-1] \n    top_class = [idx_to_class[x] for x in top_idx]\n    top_probability = pobabilities[top_idx]\n\n    return top_probability, top_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display an image along with the top 2 classes\ndef view_classify(img, probabilities, classes, mapper):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    img_filename = img.split('/')[-2]\n    img = Image.open(img)\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,10), ncols=1, nrows=2)\n    cancer_type = mapper[img_filename]\n    \n    ax1.set_title(cancer_type)\n    ax1.imshow(img)\n    ax1.axis('off')\n    \n    y_pos = np.arange(len(probabilities))\n    ax2.barh(y_pos, probabilities)\n    ax2.set_yticks(y_pos)\n    ax2.set_yticklabels([mapper[x] for x in classes])\n    ax2.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_A-14-29960CD-400-017.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_A-14-22549G-400-027.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign1.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_F-14-14134-400-020.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign2.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_F-14-14134E-400-009.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign3.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_F-14-9133-400-007.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign4.png')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# benign"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_TA-14-16184CD-400-011.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign5.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_A-14-29960CD-400-017.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign6.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_F-14-14134-400-024.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign7.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_F-14-14134-400-030.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign8.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_A-14-22549AB-400-005.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('benign9.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/malignant/SOB_M_DC-14-10926-400-004.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('malignant1.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/malignant/SOB_M_DC-14-11031-400-003.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('malignant2.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/malignant/SOB_M_DC-14-11031-400-008.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('malignant3.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/breakhis-400x/BreaKHis 400X/test/malignant/SOB_M_DC-14-11520-400-020.png'\np, c = predict(img, loaded_model)\nprint(p)\nview_classify(img, p, c, cat_to_name)\nplt.savefig('malignant5.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_img_paths = [\"../input/breakhis-400x/BreaKHis 400X/test/malignant/SOB_M_DC-14-12312-400-004.png\",\n                        \"../input/breakhis-400x/BreaKHis 400X/test/malignant/SOB_M_DC-14-11031-400-003.png\",\n                        \"../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_A-14-22549AB-400-001.png\",\n                        \"../input/breakhis-400x/BreaKHis 400X/test/benign/SOB_B_A-14-22549AB-400-007.png\"]\nimg_list = [Image.open(img_path) for img_path in validation_img_paths]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvalidation_batch = torch.stack([data_transforms['test'](img).to(device)\n                                for img in img_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_logits_tensor = model(validation_batch)\npred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, len(img_list), figsize=(20, 5))\nfor i, img in enumerate(img_list):\n    ax = axs[i]\n    ax.axis('off')\n    ax.set_title(\"{:.0f}% BENIGN, {:.0f}% MALIGNANT\".format(100*pred_probs[i,0],\n                                                          100*pred_probs[i,1]))\n    ax.imshow(img)\n    plt.savefig('tous.png')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}