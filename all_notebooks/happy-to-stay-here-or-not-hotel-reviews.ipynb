{"cells":[{"metadata":{"_uuid":"d12f520a017611e1d1d71954d65bef5cde87b565","_cell_guid":"e4a23f25-7cb6-460a-b41e-3e4048300ef1"},"cell_type":"markdown","source":"**Happy to stay here or not? – Hotel reviews**"},{"metadata":{"_uuid":"f4db780b8c88ddadf15f33c2b41c5bdf22f71a18","_cell_guid":"e5e1544d-2986-4752-b214-344b9443ca78"},"cell_type":"markdown","source":"**Introduction**\nHere I will use the data published by Anurag Sharma about hotel reviews that were given by costumers.  \nThe data is given in two files, a train and test. \n* *train.csv* – is the training data, containing unique **User_ID** for each entry with the review entered by a costumer and the browser and device used. The target variable is **Is_Response**, a variable that stats whether the costumes was **happy** or **not_happy** while staying in the hotel.  This type of variable makes the project to a classification problem. \n* *test.csv* – is the testing data, contains similar headings as the train data, without the target variable. \n"},{"metadata":{"_uuid":"cafe84417eedd74772afcd4fd2ff700ae0372f1f","_cell_guid":"4e46ff64-cc18-4037-90a9-943180667b79"},"cell_type":"markdown","source":"**Helper functions and libraries**"},{"metadata":{"_uuid":"6a8bd764b1d09deb61df05398fc5f36db44cbcf3","_cell_guid":"4b113dd1-b917-451c-8dd3-072a90f00172","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npal = sns.color_palette()\nfrom wordcloud import WordCloud, STOPWORDS\n\n#text preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\nimport string\nimport re\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom scipy.sparse import hstack, csr_matrix\n\n#ML model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, make_scorer\nfrom sklearn.model_selection import KFold, cross_val_score\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00cfc7281ccec740f66ca7d82a59602a1d29576b","_cell_guid":"b843feae-8efd-448d-9337-2558dd94d963"},"cell_type":"markdown","source":"**Load data**"},{"metadata":{"_uuid":"872d2446f9d6c06a32aff602ca2125ee87e00dae","collapsed":true,"_cell_guid":"242b8a7e-c1b4-4289-b0ae-42b5b1bdbe3a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05a3e35a20b3bcc18e84b146a27626946a548736","_cell_guid":"c64d9b03-7a31-4fdd-bb58-24bf847cf603"},"cell_type":"markdown","source":"**Overview of train data**"},{"metadata":{"_uuid":"a2eb351d31fd38919309e8f841ec1e221e1598b6","_cell_guid":"0dd92afc-79d3-4878-9472-7e410114bc5e","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d856ac6b872dea94343fbab79c7cfcf427485808","_cell_guid":"2d85f850-9aa7-4a75-b0ce-d6d97a678cb1"},"cell_type":"markdown","source":"**Overview of test data**"},{"metadata":{"_uuid":"b38a4326e3622e4012715721ff00507c8637f9b7","_cell_guid":"3b051e83-0beb-4062-87d7-c095030fd359","trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71a998bcca175c10212234f03fd44a099317f884","_cell_guid":"18aae27f-8109-4970-a127-faed9337c02f","trusted":true},"cell_type":"code","source":"print('Total number of reviews for training: {}'.format(len(df_train)))\nprint('Total number of reviews for testing: {}'.format(len(df_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02139ded850e09a6975b94fa02e9c8b671860fa9","_cell_guid":"b3378b65-08f5-489e-ada5-5fae7665c84c"},"cell_type":"markdown","source":"**Check for missing values in test and train**"},{"metadata":{"_uuid":"3dbf644e433e335687ee8789869e9c6b30377877","_cell_guid":"733f3b39-288a-4554-871a-4c0cb5cb1575","trusted":true},"cell_type":"code","source":"df_train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bd8aa8476c8ec011600823016ee265e00cd8852","_cell_guid":"936984d9-bc5e-4ec6-8eaf-39cc65ed0998","trusted":true},"cell_type":"code","source":"df_test.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e50bf3277da9570882271a02c1046f9742fcfd2f","_cell_guid":"37478503-26fb-45c5-b095-13bf968f9fa5"},"cell_type":"markdown","source":"**Preprocessing the train and test sets**"},{"metadata":{"_uuid":"03cd6dfcb2beecd9a21dcf8400c78e43bc0a8b7f","collapsed":true,"_cell_guid":"5aebd54d-93e5-4830-885a-56e493352d08","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf_train[\"Is_Response\"] = labelencoder.fit_transform(df_train[\"Is_Response\"])\n#1 not happy, 0 happy\n\ndf_train[\"Device_Used\"] = labelencoder.fit_transform(df_train[\"Device_Used\"])\ndf_test[\"Device_Used\"] = labelencoder.transform(df_test[\"Device_Used\"])\n\ndf_train[\"Browser_Used\"] = labelencoder.fit_transform(df_train[\"Browser_Used\"])\ndf_test[\"Browser_Used\"] = labelencoder.transform(df_test[\"Browser_Used\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0140db68fa161689f81b23add8e0da45181da2e7","_cell_guid":"49529069-de62-42c0-8f32-4d1642cddda3"},"cell_type":"markdown","source":"**Overview after preprocessing**"},{"metadata":{"_uuid":"f130b493b1d8ebe70f6916cfc5f8bd064c591424","_cell_guid":"cc90afed-cd9c-4542-91a0-c748c055a07c","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80f963835e9881d0ff3cbcc06a9ad5a61a187132","_cell_guid":"b1d8eded-1152-45a3-bd4c-0e0b74a04843","trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20fa04232c5eab921959cdba0a0b61daaa555466","_cell_guid":"04cf8e8a-7b5b-4cbb-bad6-ed8b957793f9"},"cell_type":"markdown","source":"**The target feature**\n\nIs the target feature balanced?"},{"metadata":{"_uuid":"e94ce5fecb30dfafd8fd7a940e54bcb2a26d843f","_cell_guid":"8bbda9aa-b7be-47f5-9237-d1c2eae2982b","trusted":true},"cell_type":"code","source":"ax = df_train['Is_Response'].value_counts().plot(kind='bar')\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_height())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    ax.text(i.get_x()+0.1, i.get_height(), \\\n            str(round((i.get_height()/total)*100, 1))+'%', fontsize = 13,\n                color = 'black')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94af04b837d7eecb4b9cb9094ddf715303cc5401","_cell_guid":"0759acdd-eb32-4940-86da-38aba56b0dd2"},"cell_type":"markdown","source":"The data is clearly imbalanced. 68% of the reviews are happy costumers and approximately 32% are not happy. The imbalance of the target variable requires a careful consideration in the prediction stage in this project. "},{"metadata":{"_uuid":"c2bc83c25433410ea69717d010bacf3b6da2313d","_cell_guid":"12a5fe46-dcc9-4a0e-a92d-e2e43e59341a"},"cell_type":"markdown","source":"**Text preprocessing**"},{"metadata":{"_uuid":"621c150cefe9c48ba78653121a946dc59b99f186","_cell_guid":"66be1cfc-b7d0-4d93-8b69-0ff11d60754f"},"cell_type":"markdown","source":"Some of the text in the description column is contracted so expansion of the text in needed. Here I will use the function *decontracted* in order to expand the text. "},{"metadata":{"_uuid":"5912bba2701dc84457cd1c4a6a96567d954b5b6c","_cell_guid":"1c918ebd-a80b-4f62-a2bc-508e886d97ef","trusted":true},"cell_type":"code","source":"import re\ndef decontracted(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'cause\", \" because\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub(r\"\\'em\", \" them\", phrase)\n    phrase = re.sub(r\"\\'t've\", \" not have\", phrase)\n    phrase = re.sub(r\"\\'d've\", \" would have\", phrase)\n    phrase = re.sub(r\"\\'clock\", \"f the clock\", phrase)\n    return phrase\n\nprint(\"finished  decontracted\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7901d819079be4479752a1da6706d2ba76fc039","_cell_guid":"9d936888-a567-432f-81a1-21bf848cdfd3"},"cell_type":"markdown","source":"Example for the function *decontracted* :"},{"metadata":{"_uuid":"778d1151c13292dee9747c32ab10dd953aa638fb","collapsed":true,"_cell_guid":"74ad1c0a-3389-4670-a4a2-16df66f8aa14","trusted":true},"cell_type":"code","source":"text = \"very good hotel in the midst of it all.best:you can't starve:carnegie-deli next doordel frisco's and ruths chris some blocks awaygordon ramsay with - michelin-stars downstairs. park-view from vista-suites looking north\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f6fab1cc1f3acb833353799960ce008abaf8f5c","_cell_guid":"f010a556-143e-42d8-930f-9c6bd08e4bc0","trusted":true},"cell_type":"code","source":"decontracted(text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a6659259d2b08e901c3dd34061b4798042d50ed","_cell_guid":"fd026839-c32d-4817-bed1-be5dd00aeede"},"cell_type":"markdown","source":"Let's apply the fuction *decontracted* on the Description column in test and train:"},{"metadata":{"_uuid":"ea7b7e6c1b7b1aa125657487f568b220ba9dfa6e","_cell_guid":"8d829a2f-6fe7-4db8-805d-e27c08fa20f1","trusted":true},"cell_type":"code","source":"df_train[\"Description\"] = df_train[\"Description\"].apply(decontracted)\ndf_test[\"Description\"] = df_test[\"Description\"].apply(decontracted)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64c2f6a2fff1425ace32eb4910458b58afb18f3b","_cell_guid":"450e1d2a-ac8c-4832-a7e9-21f4d311303f"},"cell_type":"markdown","source":"**Most frequent Description words**"},{"metadata":{"_uuid":"78351ec8ac0a6585a8949499160c85cb418032c5","_cell_guid":"20dabe57-b6ac-4bc3-98df-1956a4af8440","trusted":true},"cell_type":"code","source":"train_desc = pd.Series(df_train['Description'].tolist()).astype(str)\ncloud = WordCloud(width=1440, height=1080,stopwords=STOPWORDS).generate(\" \".join(train_desc.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.title(\"Most frequent words in the Description column\")\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaddfc24cdfb64771377a442655a2cf0a3c8bc31","_cell_guid":"510637b5-260a-4d39-8824-57b73648bd48"},"cell_type":"markdown","source":"Oh WOW! The most frequent words in the reviews of the hotels is \"front desk\"! This is very intresting because my first thught here was that the most frequent word will be something like \"comfortable bed\" or \"breakfast\". This means that people that write positive/negative reviews about hotels refers to the front desk as a main property in their review."},{"metadata":{"_uuid":"dca4169fa07f6c04dff3e01ec16e3f353cd4e856","_cell_guid":"50c90b6c-d0cf-43ad-903b-a2a18b538966"},"cell_type":"markdown","source":"Now let's divide the *Description* column to 2, for happy review and not happy review, and see which words appears the most in the text."},{"metadata":{"_uuid":"973d82158e20d88968ac2553bd54759f8ed2c51e","collapsed":true,"_cell_guid":"45bfd3f3-8e9e-4034-806d-2cf498dc219b","trusted":true},"cell_type":"code","source":"happy=df_train[df_train[\"Is_Response\"]==0]\nnot_happy=df_train[df_train[\"Is_Response\"]==1]\n\ntrain_happy = pd.Series(happy['Description'].tolist()).astype(str)\ntrain_not_happy = pd.Series(not_happy['Description'].tolist()).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f914d1dacd542e675daea4e08a8cfd43126146ec","_cell_guid":"0fb34473-b709-416f-ac22-faaae29ca134","trusted":true},"cell_type":"code","source":"cloud_happy = WordCloud(background_color=\"white\",max_words=50,width=300, height=300,stopwords=STOPWORDS).generate(\" \".join(train_happy.astype(str)))\ncloud_not_happy = WordCloud(background_color=\"white\",max_words=50,width=300, height=300,stopwords=STOPWORDS).generate(\" \".join(train_not_happy.astype(str)))\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 5))\nax = axes[0]\nax.imshow(cloud_happy)\nax.set_title(\"Happy\")\nax.axis('off')\n\nax = axes[1]\nax.imshow(cloud_not_happy)\nax.set_title(\"Not Happy\")\nax.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d47dff0e113c977367ae38b70b57524bdd03495","_cell_guid":"2b38add7-a88b-494d-a7fc-559b54e8d5e7"},"cell_type":"markdown","source":"Top 5 words for **happy** reviews:\n1. *hotel*  \n2. *one*\n3. *front desk*\n4. *room* \n5. *even*\n\nTop 5 words for **not happy** reviews:\n1. *room*  \n2. *hotel*\n3. *one*\n4. *front desk*\n5. *stay*\n\nThe top 4 words for happy and not happy reviews are similar. The word \"front desk\" pops again as one of the most common words that appear in a review.  "},{"metadata":{"_uuid":"ca70220395cfd7cb26232b28a8aa930b28ebbc49","_cell_guid":"bd777854-aca2-4ad8-85b3-c78c7baa0f6a"},"cell_type":"markdown","source":"**Length of a review** "},{"metadata":{"_uuid":"9860a65132897260e8682938072f63da386c6488"},"cell_type":"markdown","source":"Let's look at the length of each hotel review by its characters and words in the text: "},{"metadata":{"_uuid":"8c592027469bc4bc0753f16f0423b30c4bc25622","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(10,5))\n\ndist_happy_char = train_happy.apply(len)\ndist_not_happy_char = train_not_happy.apply(len)\n\nax[0].hist(dist_happy_char, bins=100, range=[0, 15000], color=pal[1], normed=True, label='happy')\nax[0].hist(dist_not_happy_char, bins=100, range=[0, 15000], color=pal[2], normed=True, alpha=0.5, label='not_happy')\nax[0].set_title('Normalised histogram of '+ r\"$\\bf{\" + 'character' + \"}$\"+ ' \\n count in review description')\nax[0].legend()\nax[0].set_xlabel('Number of characters')\nax[0].set_ylabel('Probability')\n\nprint(' for number of charcter: \\n mean-happy {:.2f} std-happy {:.2f} \\n mean-not_happy {:.2f} std-not_happy {:.2f} \\n max-happy {:.2f} \\n max-not_happy {:.2f}'.format(\n        dist_happy_char.mean(), dist_happy_char.std(), dist_not_happy_char.mean(), dist_not_happy_char.std(), dist_happy_char.max(), dist_not_happy_char.max()))\n\ndist_happy_word = train_happy.apply(lambda x: len(x.split(' ')))\ndist_not_happy_word = train_not_happy.apply(lambda x: len(x.split(' ')))\n\nax[1].hist(dist_happy_word, bins=100, range=[0, 2400], color=pal[1], normed=True, label='happy')\nax[1].hist(dist_not_happy_word, bins=100, range=[0, 2400], color=pal[2], normed=True, alpha=0.5, label='not_happy')\nax[1].set_title('Normalised histogram of '+ r\"$\\bf{\" + 'word' + \"}$\"+ ' \\n count in review description')\nax[1].legend()\nax[1].set_xlabel('Number of words')\nax[1].set_ylabel('Probability')\n\nprint('for number of words: \\n mean-happy {:.2f} std-happy {:.2f} \\n mean-not_happy {:.2f} std-not_happy {:.2f} \\n max-happy {:.2f} \\n max-not_happy {:.2f}'.format(dist_happy_word.mean(), \n                          dist_happy_word.std(), dist_not_happy_word.mean(), dist_not_happy_word.std(), dist_happy_word.max(), dist_not_happy_word.max()))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1465ab55b3fa83c156a7b4b1f7326216a56cbb13"},"cell_type":"markdown","source":"Both graphs look very similar but a closer look reveals that *happy* reviews are most likely to be short and *not_happy* reviews are long. This might help the classification model so let's add this as features to the data."},{"metadata":{"_uuid":"f609bf8a7e0ccdc5514ba78995c78197b7ba4f25","trusted":true},"cell_type":"code","source":"#for words\ndf_train[\"num_words\"] = df_train[\"Description\"].apply(lambda x: len(str(x).split()))\ndf_test[\"num_words\"] = df_test[\"Description\"].apply(lambda x: len(str(x).split()))\n#for chars\ndf_train[\"num_chars\"] = df_train[\"Description\"].apply(lambda x: len(str(x)))\ndf_test[\"num_chars\"] = df_test[\"Description\"].apply(lambda x: len(str(x)))\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a4ba6bb31666f90374b729c540c7187683c3876"},"cell_type":"code","source":"'''\n#extracting more features from the text\n\nimport string\ndef unique_word_fraction(row):\n    \"\"\"function to calculate the fraction of unique words on total words of the text\"\"\"\n    text = row['Description']\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    unique_count = list(set(text_splited)).__len__()\n    return (unique_count/word_count)\n\n\neng_stopwords = set(stopwords.words(\"english\"))\ndef stopwords_count(row):\n    \"\"\" Number of stopwords fraction in a text\"\"\"\n    text = row['Description'].lower()\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    stopwords_count = len([w for w in text_splited if w in eng_stopwords])\n    return (stopwords_count/word_count)\n\n\ndef punctuations_fraction(row):\n    \"\"\"functiopn to claculate the fraction of punctuations over total number of characters for a given text \"\"\"\n    text = row['Description']\n    char_count = len(text)\n    punctuation_count = len([c for c in text if c in string.punctuation])\n    return (punctuation_count/char_count)\n\n\ndef fraction_noun(row):\n    \"\"\"function to give us fraction of noun over total words \"\"\"\n    text = row['Description']\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = nltk.pos_tag(text_splited)\n    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n    return (noun_count/word_count)\n\ndef fraction_adj(row):\n    \"\"\"function to give us fraction of adjectives over total words in given text\"\"\"\n    text = row['Description']\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = nltk.pos_tag(text_splited)\n    adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n    return (adj_count/word_count)\n\ndef fraction_verbs(row):\n    \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n    text = row['Description']\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = nltk.pos_tag(text_splited)\n    verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n    return (verbs_count/word_count)\n\n\ndf_train['unique_word_fraction'] = df_train.apply(lambda row: unique_word_fraction(row), axis =1)\ndf_train['stopwords_count'] = df_train.apply(lambda row: stopwords_count(row), axis =1)\ndf_train['punctuations_fraction'] = df_train.apply(lambda row: punctuations_fraction(row), axis =1)\ndf_train['fraction_noun'] = df_train.apply(lambda row: fraction_noun(row), axis =1)\ndf_train['fraction_adj'] = df_train.apply(lambda row: fraction_adj(row), axis =1)\ndf_train['fraction_verbs'] = df_train.apply(lambda row: fraction_verbs(row), axis =1)\ndf_train.head()\n\n#did not improved the classifier result\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b57b6a05240f992276aec234f343f046681b8a27"},"cell_type":"markdown","source":"**Sentiment analysis**"},{"metadata":{"_uuid":"583a095d0e10708e22fb9efae242ab1647924eb0","trusted":true,"collapsed":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsia = SentimentIntensityAnalyzer()\ndef sentiment_nltk(text):\n    res = sia.polarity_scores(text)\n    return res['compound']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2190aa4b259f0046e07f80f2e7fd541d6c92752a","trusted":true,"collapsed":true},"cell_type":"code","source":"df_train[\"sentiment\"] = df_train[\"Description\"].apply(sentiment_nltk)\n#df_test[\"sentiment\"] = df_test[\"Description\"].apply(sentiment_nltk)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6eff5db242862377a1e0b15d3748cba77118e7c3","trusted":true},"cell_type":"code","source":"happy_sent=df_train[df_train[\"Is_Response\"]==0]\nnot_happy_sent=df_train[df_train[\"Is_Response\"]==1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7fa3c5f9beb78b5a379cf68735af102ff644454","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.hist(happy_sent['sentiment'], bins=100, range=[-1, 1], color=pal[1], normed=True, label='happy')\nplt.hist(not_happy_sent['sentiment'], bins=100, range=[-1, 1], color=pal[2], normed=True, alpha=0.5, label='not_happy')\nplt.title('Normalised histogram from sentiment analysis')\nplt.legend()\nplt.xlabel('Sentiment analysis polarity score')\nplt.ylabel('Probability')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20529dfceb42dfe52ec54a21f9093f58964e7d6f"},"cell_type":"markdown","source":"Preparing for feature extraction from text"},{"metadata":{"_uuid":"43b12d413be53364bf5fb0de75af5bdd88282b16","collapsed":true,"_cell_guid":"299fe93e-46df-4c5e-bd42-388496d7a3d3","trusted":true},"cell_type":"code","source":"X_description = df_train['Description']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8da189dc1a87c53310e159934f97c72229a9339c"},"cell_type":"markdown","source":"Preparing the data for classification"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9a2c015484a9b6237e5f3f8146f67ec78ec0df74"},"cell_type":"code","source":"X = df_train.drop(['User_ID','Description','Is_Response'], axis=1)\nY = df_train['Is_Response']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67073762a18239d761a12d9f87267db69ac5e570"},"cell_type":"markdown","source":"Preparing CountVectorizer for classification"},{"metadata":{"_uuid":"28e48bf2a9ffade5b774015c2a2af24ce11f6f3f","collapsed":true,"_cell_guid":"0e807f46-3b5d-46d6-bcd4-b77fa40a598a","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english', max_features = 400)\n\nX_ctv =  ctv.fit_transform(X_description)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bbc8aa7501e73c88bfbbb3b1c84ff1f97a746b5"},"cell_type":"code","source":"from scipy.sparse import hstack, csr_matrix\n\nfeat_train = csr_matrix(X.values)\n\nX_train_stack_ctv = hstack([feat_train, X_ctv[0:feat_train.shape[0]]])\n\nprint('Train shape: ', X_train_stack_ctv.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2f4d6ebd0be6ec55c20ebd9b4e539c6c054ecaa"},"cell_type":"markdown","source":"Preparing TfidfVectorizer for classification"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0b27f162dac5c5064f0a1355e132c2d8adfd709e"},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,\n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english', max_features=400)\n\nX_tfv =  tfv.fit_transform(X_description) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd0310526887e0ae69fc0d87461a9dfee790eebb"},"cell_type":"code","source":"X_train_stack_tfidf = hstack([feat_train, X_tfv[0:feat_train.shape[0]]])\n\nprint('Train shape: ', X_train_stack_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d530e34deda31cc39d9aaed5955b36c7537c0b1c"},"cell_type":"markdown","source":"Some useful code"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"438fc835bba167903ea9d2232c37776ea6a610fb"},"cell_type":"code","source":"from sklearn.metrics import precision_score\ndef classification_report_with_precision_score(y_true, y_pred):\n    originalclass.extend(y_true)\n    predictedclass.extend(y_pred)\n    return precision_score(y_true, y_pred) # return accuracy score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cac4fc0853b647bc7104098773d896a92208ce5"},"cell_type":"code","source":"def model_cv(model,X,Y):\n    outer_cv = KFold(n_splits=10, shuffle=True)\n    clf = model\n    nested_score = cross_val_score(clf, X=X, y=Y, cv=outer_cv, scoring = make_scorer(classification_report_with_precision_score))\n    print(classification_report(originalclass, predictedclass)) \n    print (\"mean precision score: \" + str(model)+ \": %0.3f std: (%0.3f)\" % (np.mean(nested_score),np.std(nested_score)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e04ca02fca6ce741b0ed45a1fbc888cf364a403f"},"cell_type":"markdown","source":"**Predictions with CountVectorizer**"},{"metadata":{"_uuid":"96ddfef4585aa61deccff548176f9d69a31d2a00","_cell_guid":"6650b7b8-b051-4ea6-a10f-77808087f196","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\noriginalclass = []\npredictedclass = []\nmodel_cv(LogisticRegression(),X_train_stack_ctv,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"495408a4888bb0ec76c6e7121d5bfd4bd3f8ba81"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\noriginalclass = []\npredictedclass = []\nmodel_cv(DecisionTreeClassifier(),X_train_stack_ctv,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"037a4f1a76e913fa6c09a28580b46cdf36c90bc1"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\noriginalclass = []\npredictedclass = []\nmodel_cv(RandomForestClassifier(n_estimators = 40),X_train_stack_ctv,Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2513b833e94b111ed1b67c5aaa61ca7e2aef6782"},"cell_type":"markdown","source":"**Predictions with TfidfVectorizer**"},{"metadata":{"_uuid":"0acead3869358bc067c53872415127f1bfc1433a","_cell_guid":"7fa8d19b-e4bf-487e-9082-7c64d1c6c8ec","trusted":true},"cell_type":"code","source":"originalclass = []\npredictedclass = []\nmodel_cv(LogisticRegression(),X_train_stack_tfidf,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46a54766380e65ced54833785d6c2f08899bbcc8"},"cell_type":"code","source":"originalclass = []\npredictedclass = []\nmodel_cv(DecisionTreeClassifier(),X_train_stack_tfidf,Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a69bea1b2a05e0bac1aa746432ed9b43f3ee52f","_cell_guid":"44fd63f3-ec7c-462e-8f41-90064ceb17dc","trusted":true},"cell_type":"code","source":"originalclass = []\npredictedclass = []\nmodel_cv(RandomForestClassifier(n_estimators = 40),X_train_stack_tfidf,Y)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}