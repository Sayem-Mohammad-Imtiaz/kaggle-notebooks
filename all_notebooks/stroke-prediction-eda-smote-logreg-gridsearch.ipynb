{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.info())\nprint(df.describe())\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One Hot Encoding  ever_married, smoking_status, Residence_type, work_type, gender\ndf  = pd.get_dummies(df, columns=[\"ever_married\", \"smoking_status\", \"Residence_type\", \"work_type\", \"gender\" ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()\n#only bmi values missing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deal with missing bmis \n# what indicates/influences the bmi?\ncorr = df.corr(\"pearson\")\nplt.figure(figsize=(20,20))\nsns.heatmap(corr ,annot=True,cmap=\"RdYlGn\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.pairplot(df, hue=\"bmi\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = list()\n\nfor index,element in corr[\"bmi\"].items(): \n    if element>0.2 or element < -0.2 :\n        names.append(index)\n\nnames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(df, x=\"age\", bins=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df[\"age\"].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create age_class to fill missing bmis more accuratly \nfor i,e in df[\"age\"].items(): \n    if e <= 16.4:\n        df.at[i, \"age_class\"] = 1\n    if e > 16.4 and e <= 32.8: \n        df.at[i, \"age_class\"] = 2\n    if e > 32.8 and e <= 49.2: \n         df.at[i, \"age_class\"] = 3\n    if e > 49.2 and e <= 65.6:\n         df.at[i, \"age_class\"] = 4\n    if e > 65: \n         df.at[i, \"age_class\"] = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocessing\n#test train split\ny = df[\"stroke\"]\nX = df.drop([\"stroke\"], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=46)\nscaler = StandardScaler()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change class balance by oversampling\n#val train spilt\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n#                                                 test_size = 0.2,random_state=22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create median df after splitting in train/test to prevent leakage of information \nmedian_df_train = X_train.groupby([\"age_class\", \"ever_married_No\", \"smoking_status_Unknown\", \"work_type_Private\",\"work_type_children\"]).median()\nmedian_df_train = median_df_train.reset_index()\n\nmedian_df_test = X_test.groupby([\"age_class\", \"ever_married_No\", \"smoking_status_Unknown\", \"work_type_Private\",\"work_type_children\"]).median()\nmedian_df_test = median_df_test.reset_index()\n\n#median_df_val = X_val.groupby([\"age_class\", \"ever_married_No\", \"smoking_status_Unknown\", \"work_type_Private\",\"work_type_children\"]).median()\n#median_df_val  = median_df_val.reset_index()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_value(row, df_median): \n    # return mean of cells that meet the rows conditions\n    condition = ((median_df_train[\"age_class\"] == row[\"age_class\"]) &\n                (median_df_train[\"ever_married_No\"] == row[\"ever_married_No\"]) &\n                (median_df_train[\"smoking_status_Unknown\"] == row[\"smoking_status_Unknown\"]) & \n                (median_df_train[\"work_type_Private\"] == row[\"work_type_Private\"]) &\n                (median_df_train[\"work_type_children\"] == row[\"work_type_children\"]))\n    return median_df_train[condition]['bmi'].values[0]\n\ndef fill_bmi(df, df_median): \n    bmis = list()\n    for index, row in df.iterrows():\n        if np.isnan(row[\"bmi\"]) : \n            row[\"bmi\"] = get_value(row, df_median)\n        bmis.append(row[\"bmi\"])\n    return bmis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fill bmi in test/train with corresponding medians\nX_train = X_train.copy()\nX_test = X_test.copy()\nX_train.loc[:,\"bmi\"] = fill_bmi(X_train, median_df_train )\nX_test.loc[:,\"bmi\"] = fill_bmi(X_test, median_df_test)\n#X_val.loc[:,\"bmi\"] = fill_bmi(X_val, median_df_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop age_class \nX_train = X_train.drop(\"age_class\", axis = 1)\nX_test = X_test.drop(\"age_class\", axis = 1)\n#X_val = X_val.drop(\"age_class\", axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for mulitcollinearity\nvif_data = pd.DataFrame()\nX_temp = sm.add_constant(X_train)\nvif_data[\"feature\"] = X_temp.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X_temp.values, i)\n                          for i in range(len(X_temp.columns))]\n\nprint(vif_data)\n#-> low mulitcollinearity -> try Logistic Regression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.loc[df[\"stroke\"] ==1, \"id\"].count())\nprint(df.loc[df[\"stroke\"] ==0, \"id\"].count())   \n#imbalanced target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardize features to improve performance \nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use SMOTE to oversample class 1\nsm = SMOTE(random_state=42)\nx_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression with oversampled class 1 \n# 1 (positive) is stroke, 0 (negative) no stroke \nlog_model_smote = LogisticRegression(max_iter=1000, solver = 'liblinear', random_state = 44)\nlog_model_smote.fit(x_train_smote, y_train_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train results \nlog_smote_pred_train = log_model_smote.predict(x_train_smote)\nprint(classification_report(y_train_smote,log_smote_pred_train))\n\n\n# test results \nlog_smote_pred_test = log_model_smote.predict(X_test)\nprint(classification_report(y_test,log_smote_pred_test))\nprint(confusion_matrix(y_test, log_smote_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, log_smote_pred_test)\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Logistic Regression with Class_weigth \"balanced\"\nlog_model = LogisticRegression(max_iter = 10, class_weight = \"balanced\", solver = 'liblinear', random_state = 44)\nlog_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train results \nlog_pred_train = log_model.predict(X_train)\nprint(classification_report(y_train,log_pred_train))\n\n\n# test results \nlog_pred_test = log_model.predict(X_test)\nprint(classification_report(y_test,log_pred_test))\nprint(confusion_matrix(y_test, log_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, log_pred_test)\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# knn \nknn_model = KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(x_train_smote, y_train_smote)\n\n# train results \nknn_pred_train = knn_model.predict(x_train_smote)\nprint(classification_report(y_train_smote,knn_pred_train))\n\n\n# test results \nknn_pred_test = knn_model.predict(X_test)\nprint(classification_report(y_test,knn_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, knn_pred_test)\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#random forest \nrndf_model = RandomForestClassifier(max_depth = 5, random_state = 45, class_weight='balanced_subsample')\nrndf_model.fit(X_train, y_train)\n\n# train results \nrndf_pred_train = rndf_model.predict(X_train)\nprint(classification_report(y_train,rndf_pred_train))\n\n\n# test results \nrndf_pred_test = rndf_model.predict(X_test)\nprint(classification_report(y_test,rndf_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, rndf_pred_test)\nprint(confusion_matrix(y_test, rndf_pred_test))\nprint(y_test.value_counts())\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Pipe for Tuning of LogReg and RandomForest \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logReg = LogisticRegression(random_state = 42)\n#rndf = RandomForestClassifier()\n#pipe = Pipeline([('logReg', logReg),('rndf', RandomForestClassifier())])\nparam_grid = [\n    {'penalty' : ['l1', 'l2'],\n    'class_weight' : ['balanced', {0:0.1, 1:0.8}, {0:0.1, 1:0.9}, {0:0.1, 1:0.3}],\n    'solver' : ['liblinear'],\n    'max_iter' : list(range(100,200))}]\n    #'rndf__n_estimators' : list(range(10,101,10)),\n    #'rndf__max_features' : list(range(6,10,5))}]\n\ngrid_search_log = GridSearchCV(logReg, param_grid=param_grid, cv = 5, verbose=True, scoring = 'recall')\nbest = grid_search_log.fit(X_train, y_train)\nprint(best)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_logReg_train = grid_search_log.predict(X_train)\nbest_logReg_test = grid_search_log.predict(X_test)\n\nprint(classification_report(y_train , best_logReg_train))\nprint(classification_report(y_test,best_logReg_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, best_logReg_test)\nprint(confusion_matrix(y_test, best_logReg_test))\nprint(y_test.value_counts())\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Feel free to leave comments! Every input is greatly appreciated!**","metadata":{}}]}