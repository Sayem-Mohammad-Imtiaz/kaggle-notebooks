{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Learning from great feature engineering\n\nThis notebook forks [How I made top 0.3% on a Kaggle competition](https://www.kaggle.com/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition/notebook), big thanks to the author for great material! \n\nIn this notebook, I added the following improvements:\n\n+ further inspections on how each feature engineering tasks help to improve prediction\n+ fix minor bugs in creating boolean features such as `HasWoodDeck`\n+ add a section to measure __association__ between categorical features and target\n+ replace standard onehot encoding by target encoding, this helps a lot. Actually, this trick helps individual models perform on-par with the heavyweight stacked model.\n\n\n\nIf you like this kernel, please give it an upvote. Thank you!","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:17.67828Z","start_time":"2020-07-04T16:34:16.682576Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"!pip install mlxtend","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:17.692246Z","start_time":"2020-07-04T16:34:17.680275Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T20:04:26.582573Z","start_time":"2020-07-04T20:04:26.526677Z"},"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read in the dataset as a dataframe\n\nfolder = '/kaggle/input/house-prices-advanced-regression-techniques/'\n\ntrain = pd.read_csv(os.path.join(folder, 'train.csv'))\ntest = pd.read_csv(os.path.join(folder, 'test.csv'))\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## The Goal\n\n- Each row in the dataset describes the characteristics of a house.\n- Our goal is to predict the SalePrice, given these features.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:17.80594Z","start_time":"2020-07-04T16:34:17.748093Z"},"trusted":true},"cell_type":"code","source":"# Preview the data we're working with\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SalePrice: the variable we're trying to predict","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:18.100156Z","start_time":"2020-07-04T16:34:17.806936Z"},"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:18.108143Z","start_time":"2020-07-04T16:34:18.102147Z"},"trusted":true},"cell_type":"code","source":"# Skew and kurt\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Features: a deep dive","execution_count":null},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Let's visualize some of the features in the dataset","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:27.953051Z","start_time":"2020-07-04T16:34:18.110127Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train.columns:\n    if train[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"and plot how the features are correlated to each other, and to SalePrice","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:28.824745Z","start_time":"2020-07-04T16:34:27.956038Z"},"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"corr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Let's plot how SalePrice relates to some of the features in the dataset","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:29.247501Z","start_time":"2020-07-04T16:34:28.826727Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:33.087222Z","start_time":"2020-07-04T16:34:29.249494Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:33.425984Z","start_time":"2020-07-04T16:34:33.088216Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000));","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:33.734229Z","start_time":"2020-07-04T16:34:33.426981Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['LotArea']], axis=1)\ndata.plot.scatter(x='LotArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:34.051412Z","start_time":"2020-07-04T16:34:33.736224Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:34.065372Z","start_time":"2020-07-04T16:34:34.053404Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Transform target \nI use a log transform on target to make it approx normally distributed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the distribution of the SalePrice.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:34.409953Z","start_time":"2020-07-04T16:34:34.066879Z"},"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:34.416934Z","start_time":"2020-07-04T16:34:34.411951Z"},"trusted":true},"cell_type":"code","source":"# log(1+x) transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the SalePrice again.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.058498Z","start_time":"2020-07-04T16:34:34.419001Z"},"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SalePrice is now normally distributed, excellent!","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.072685Z","start_time":"2020-07-04T16:34:35.060433Z"},"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.114091Z","start_time":"2020-07-04T16:34:35.073675Z"},"trusted":true},"cell_type":"code","source":"# Split features and labels\ntrain_labels = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:27:52.423498Z","start_time":"2020-07-04T17:27:52.419498Z"},"trusted":true},"cell_type":"code","source":"target = 'SalePrice'","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Fill missing values","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.162976Z","start_time":"2020-07-04T16:34:35.116085Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"# determine the threshold for missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.544494Z","start_time":"2020-07-04T16:34:35.164952Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"# Visualize missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"We can now move through each of the features above and impute the missing values for each of them.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.558951Z","start_time":"2020-07-04T16:34:35.546474Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.714071Z","start_time":"2020-07-04T16:34:35.562453Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\nall_features = handle_missing(all_features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:34:35.768902Z","start_time":"2020-07-04T16:34:35.716042Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"# Let's make sure we handled all the missing values\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"There are no missing values anymore!","execution_count":null},{"metadata":{"hidden":true},"cell_type":"markdown","source":"### Improvement ","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:39:07.921206Z","start_time":"2020-07-04T16:39:07.916227Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"def list_numeric_columns(data):\n    return list(data.columns[np.where(data.dtypes != 'object')])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:45:31.592365Z","start_time":"2020-07-04T16:45:31.585384Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"train_labels[:3]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:47:21.436526Z","start_time":"2020-07-04T16:47:21.430452Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"num_cols = list_numeric_columns(all_features)\nX_train, y_train = all_features.iloc[:len(train_labels)][num_cols], train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:47:22.498114Z","start_time":"2020-07-04T16:47:22.475176Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nround(lr.score(X_train, y_train), 4)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"__Obs:__\n\nThis is already much better than my linear model; thanks to the __log transform__ which make target approx. a normal distribution.","execution_count":null},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Fix skewed features","execution_count":null},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0],"hidden":true,"trusted":true},"cell_type":"code","source":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0],"hidden":true,"trusted":true},"cell_type":"code","source":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"We use the scipy function boxcox1p which computes the Box-Cox transformation. The goal is to find a simple transformation that lets us normalize data.","execution_count":null},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# Normalize skewed features\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"All the features look fairly normally distributed now.","execution_count":null},{"metadata":{"hidden":true},"cell_type":"markdown","source":"### Improvement ","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:53:15.396225Z","start_time":"2020-07-04T16:53:15.389625Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"num_cols = list_numeric_columns(all_features)\nX_train, y_train = all_features.iloc[:len(train_labels)][num_cols], train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T16:53:19.348544Z","start_time":"2020-07-04T16:53:19.334069Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nround(lr.score(X_train, y_train), 4)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"__Obs:__ seems no improvement.","execution_count":null},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Create interesting features","execution_count":null},{"metadata":{"hidden":true},"cell_type":"markdown","source":"ML models have trouble recognizing more complex patterns (and we're staying away from neural nets for this competition), so let's help our models out by creating a few features based on our intuition about the dataset, e.g. total area of floors, bathrooms and porch area of each house.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:03:27.539493Z","start_time":"2020-07-04T17:03:27.481121Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"all_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['HasWoodDeck'] = (all_features['WoodDeckSF'] >= 0) * 1\nall_features['HasOpenPorch'] = (all_features['OpenPorchSF'] >= 0) * 1\nall_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] >= 0) * 1\nall_features['Has3SsnPorch'] = (all_features['3SsnPorch'] >= 0) * 1\nall_features['HasScreenPorch'] = (all_features['ScreenPorch'] >= 0) * 1\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\nall_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nall_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\nall_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"hidden":true},"cell_type":"markdown","source":"### Improvement","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:05:53.583127Z","start_time":"2020-07-04T17:05:53.574157Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"num_cols = list_numeric_columns(all_features)\nX_train, y_train = all_features.iloc[:len(train_labels)][num_cols], train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:05:55.190923Z","start_time":"2020-07-04T17:05:55.175931Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nround(lr.score(X_train, y_train), 4)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"__Obs:__ some improvement.","execution_count":null},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Feature transformations\nLet's create more features by calculating the log and square transformations of our numerical features. We do this manually, because ML models won't be able to reliably tell if log(feature) or feature^2 is a predictor of the SalePrice.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:06:53.7829Z","start_time":"2020-07-04T17:06:53.692049Z"},"code_folding":[0],"hidden":true,"trusted":true},"cell_type":"code","source":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\nall_features = logs(all_features, log_features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:06:54.789023Z","start_time":"2020-07-04T17:06:54.756054Z"},"code_folding":[0],"hidden":true,"trusted":true},"cell_type":"code","source":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_features = squares(all_features, squared_features)","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"hidden":true},"cell_type":"markdown","source":"### Improvement","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:07:33.03046Z","start_time":"2020-07-04T17:07:33.02348Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"num_cols = list_numeric_columns(all_features)\nX_train, y_train = all_features.iloc[:len(train_labels)][num_cols], train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:07:35.095408Z","start_time":"2020-07-04T17:07:35.072968Z"},"hidden":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nround(lr.score(X_train, y_train), 4)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"__More improvement!__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Association between categorical variables and target\n\nA metrics similar to correlation between two numeric variables. It is called Correlation Ratio (sometimes marked by the greek letter Eta) and has value in range [0, 1]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install dython","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sadly, I cannot install `dython` here, so I cannot plot the heat map showing correlation between categorical variables and target. However, I can still compute the correlation via a helper method (sourced from [dython](https://github.com/shakedzy/dython)). ","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T19:49:29.004952Z","start_time":"2020-07-04T19:49:28.994435Z"},"trusted":true},"cell_type":"code","source":"# from dython.nominal import correlation_ratio, associations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_ratio(categories, measurements):\n#     categories = convert(categories, 'array')\n#     measurements = convert(measurements, 'array')\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat) + 1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0, cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) / np.sum(n_array)\n    numerator = np.sum(\n        np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg),\n                                      2)))\n    denominator = np.sum(np.power(np.subtract(measurements, y_total_avg), 2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator / denominator)\n    return eta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def identify_nominal_columns(data: pd.DataFrame, include=['object', 'category']):\n\n    nominal_columns = list(data.select_dtypes(include=include).columns)\n    return nominal_columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T19:50:26.896954Z","start_time":"2020-07-04T19:50:26.893933Z"},"trusted":true},"cell_type":"code","source":"cat_feats = identify_nominal_columns(train)\nprint(cat_feats)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T20:04:41.046054Z","start_time":"2020-07-04T20:04:40.967248Z"},"trusted":true},"cell_type":"code","source":"eta_corrs = [correlation_ratio(train[cf], train[target]) for cf in cat_feats]\ncorr_target_cat_feats = pd.DataFrame({'cat_feat': cat_feats, 'corr_with_target': eta_corrs\n                                     }).sort_values('corr_with_target', ascending=False)\ncorr_target_cat_feats","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T19:59:54.997913Z","start_time":"2020-07-04T19:59:54.988957Z"},"trusted":true},"cell_type":"code","source":"corr_target_cat_feats.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, neighborhood has a very high association with target. The four next-in-line are features for quality of exterior, basement, kitchen and whether garage is finish.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T20:07:57.581608Z","start_time":"2020-07-04T20:07:56.613153Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"# top10_cat_feats = list(corr_target_cat_feats['cat_feat'].values[:10])\n# associations(train[top10_cat_feats + [target]], figsize=(11, 11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode categorical features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Use target encoding.","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:12:00.749052Z","start_time":"2020-07-04T17:12:00.629646Z"},"trusted":true},"cell_type":"code","source":"import category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:15:59.435899Z","start_time":"2020-07-04T17:15:59.428919Z"},"code_folding":[0],"trusted":true},"cell_type":"code","source":"def identify_nominal_columns(data: pd.DataFrame, include=['object', 'category']):\n    \"\"\"Given a dataset, identify categorical columns.\n\n    Parameters:\n    -----------\n    dataset : a pandas dataframe\n    include : which column types to filter by; default: ['object', 'category'])\n\n    Returns:\n    --------\n    categorical_columns : a list of categorical columns\n\n    Example:\n    --------\n    >> df = pd.DataFrame({'col1': ['a', 'b', 'c', 'a'], 'col2': [3, 4, 2, 1]})\n    >> identify_nominal_columns(df)\n    ['col1']\n\n    \"\"\"\n\n    nominal_columns = list(data.select_dtypes(include=include).columns)\n    return nominal_columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:35:04.009943Z","start_time":"2020-07-04T17:35:03.999808Z"},"trusted":true},"cell_type":"code","source":"def encode_by_target(cat_feats, data, train_idx, target):\n    target_enc = ce.TargetEncoder()\n    # fit on train set\n    train = data.iloc[train_idx]\n    target_enc.fit(train[cat_feats], train[target])\n    \n    # Transform the features on both train and test,\n    # rename the columns with _target suffix, and join to dataframe\n    # also remove old categ vars\n    encoded_data = data.join(target_enc.transform(data[cat_feats]).add_suffix('_target')).drop(columns=cat_feats)\n    return encoded_data","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:19:21.049254Z","start_time":"2020-07-04T17:19:21.033566Z"},"trusted":true},"cell_type":"code","source":"cat_feats = identify_nominal_columns(all_features)\ncat_feats = list(np.setdiff1d(cat_feats, ['MoSold', 'YrSold']))\nprint(cat_feats)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:39:00.313525Z","start_time":"2020-07-04T17:39:00.307429Z"},"trusted":true},"cell_type":"code","source":"type(train.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:42:14.889985Z","start_time":"2020-07-04T17:42:14.882425Z"},"trusted":true},"cell_type":"code","source":"train_idx = range(len(train_labels))\ndata = all_features.copy()\ndata[target] = list(train.SalePrice.values) + [0]*len(test)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:42:49.387312Z","start_time":"2020-07-04T17:42:49.38132Z"},"trusted":true},"cell_type":"code","source":"len(data) == len(all_features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:42:59.240356Z","start_time":"2020-07-04T17:42:58.524808Z"},"trusted":true},"cell_type":"code","source":"encoded_data = encode_by_target(cat_feats, data, train_idx, target)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:43:53.170653Z","start_time":"2020-07-04T17:43:53.164523Z"},"trusted":true},"cell_type":"code","source":"# Remove any duplicated column names\nencoded_data = encoded_data.loc[:,~encoded_data.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:18:45.028572Z","start_time":"2020-07-04T18:18:45.022574Z"},"trusted":true},"cell_type":"code","source":"encoded_data.MoSold = encoded_data.MoSold.astype(int)\nencoded_data.YrSold = encoded_data.YrSold.astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Improvement","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:51:54.160785Z","start_time":"2020-07-04T18:51:54.148775Z"},"trusted":true},"cell_type":"code","source":"train, test = encoded_data.iloc[:len(train_labels)], encoded_data[len(train_labels):]\n\nX_train, y_train = train.drop(columns=[target]), train[target]\nX_test = test.drop(columns=[target])\nX_train.shape, y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:18:59.724757Z","start_time":"2020-07-04T18:18:59.702796Z"},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nround(lr.score(X_train, y_train), 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Big improvement!!!__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Train a model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Key features of the model training process:\n- **Cross Validation:** Using 10-fold cross-validation\n- **Models:** On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n- **Stacking:** I will stack the models via stacking technique [here](https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Setup cross validation and define error metrics","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T17:56:50.039573Z","start_time":"2020-07-04T17:56:50.036238Z"},"trusted":true},"cell_type":"code","source":"# Setup cross validation folds and for out-of-fold predictions for stacking\nNFOLDS = 10\nkf = KFold(n_splits=NFOLDS, random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:21:29.52836Z","start_time":"2020-07-04T18:21:29.522748Z"},"trusted":true},"cell_type":"code","source":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X_train, y_train):\n#     use all workers\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kf, verbose=1, n_jobs=-1))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup models","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:14:07.715612Z","start_time":"2020-07-04T18:14:07.704647Z"},"code_folding":[1,16,38,48,57],"trusted":true},"cell_type":"code","source":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\n# stack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n#                                 meta_regressor=xgboost,\n#                                 use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Get cross validation scores for each model","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:20:34.611203Z","start_time":"2020-07-04T18:19:47.759125Z"},"trusted":true},"cell_type":"code","source":"scores = {}\n\nscore = cv_rmse(lightgbm, X_train, y_train)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:30:41.340054Z","start_time":"2020-07-04T18:27:36.086672Z"},"trusted":true},"cell_type":"code","source":"score = cv_rmse(xgboost, X_train, y_train)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:30:43.31306Z","start_time":"2020-07-04T18:30:41.342031Z"},"trusted":true},"cell_type":"code","source":"score = cv_rmse(svr, X_train, y_train)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:30:49.333989Z","start_time":"2020-07-04T18:30:43.316053Z"},"trusted":true},"cell_type":"code","source":"score = cv_rmse(ridge, X_train, y_train)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:32:16.838947Z","start_time":"2020-07-04T18:30:49.335941Z"},"trusted":true},"cell_type":"code","source":"score = cv_rmse(rf, X_train, y_train)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:33:55.598148Z","start_time":"2020-07-04T18:32:16.842907Z"},"trusted":true},"cell_type":"code","source":"score = cv_rmse(gbr, X_train, y_train)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the models","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:44:57.614059Z","start_time":"2020-07-04T18:35:27.107131Z"},"trusted":true},"cell_type":"code","source":"# print('stack_gen')\n# stack_gen_model = stack_gen.fit(np.array(X_train), np.array(y_train))\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X_train, y_train)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X_train, y_train)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X_train, y_train)\n\nprint('RandomForest')\nrf_model_full_data = rf.fit(X_train, y_train)\n\n# print('GradientBoosting')\n# gbr_model_full_data = gbr.fit(X_train, y_train)\n\n# print('xgboost')\n# xgb_model_full_data = xgboost.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = len(X_train)\nntest = len(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = X_train.index\nsplits = kf.split(X_train)\nfor i, (train_index, test_index) in enumerate(splits):\n    x_tr = X_train.iloc[train_index]\n    break\n    #     print(np.setdiff1d(train_index, idx))\n    ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:45:18.694536Z","start_time":"2020-07-04T18:45:18.689549Z"},"trusted":true},"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train.iloc[train_index]\n        y_tr = y_train.iloc[train_index]\n        x_te = x_train.iloc[test_index]\n\n        clf.fit(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Output of the First level Predictions** \n\nWe now feed the training and test data into our base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_oof_train, svr_oof_test = get_oof(svr, X_train, y_train, X_test)\nlgb_oof_train, lgb_oof_test = get_oof(lightgbm, X_train, y_train, X_test)\nridge_oof_train, ridge_oof_test = get_oof(ridge, X_train, y_train, X_test)\nrf_oof_train, rf_oof_test = get_oof(rf, X_train, y_train, X_test)\nprint('Train & Create 1st level predictions: Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stack 1st level predictions as new feats\nbase_predictions_train = pd.DataFrame( {\n     'SVR': svr_oof_train.ravel(),\n     'LGB': lgb_oof_train.ravel(),\n     'Ridge': ridge_oof_train.ravel(),\n      'RandomForest': rf_oof_train.ravel(),\n    })\nbase_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stack 1st level predictions as new feats\nbase_predictions_test = pd.DataFrame( {\n     'SVR': svr_oof_test.ravel(),\n     'LGB': lgb_oof_test.ravel(),\n     'Ridge': ridge_oof_test.ravel(),\n      'RandomForest': rf_oof_test.ravel(),\n    })\nbase_predictions_test.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:45:57.001983Z","start_time":"2020-07-04T18:45:55.098841Z"},"trusted":true},"cell_type":"code","source":"stacked_model = LGBMRegressor(objective='regression', \n                           num_leaves=6,\n                           learning_rate=0.01, \n                           n_estimators=1000,\n                           max_bin=200, \n                           bagging_fraction=0.8,\n                           bagging_freq=4, \n                           bagging_seed=8,\n                           feature_fraction=0.2,\n                           feature_fraction_seed=8,\n                           min_sum_hessian_in_leaf = 11,\n                           verbose=-1,\n                           random_state=42,\n                            n_jobs=-1\n                           )\n# stacked_model.fit(base_predictions_train, y_train)\n# final_score = stacked_model.score(base_predictions_train, y_train)\n# print(final_score)\n\nscore = cv_rmse(stacked_model, base_predictions_train, y_train)\nprint('Stacked model score: {} ()'.format(score.mean(), score.std()))\nscores['stacked_model'] = (score.mean(), score.std())\n\n# Get final precitions from the stacked model\ny_pred = stacked_model.predict(base_predictions_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identify the best performing model","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:46:42.199346Z","start_time":"2020-07-04T18:46:41.89314Z"},"code_folding":[0],"scrolled":true,"trusted":true},"cell_type":"code","source":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from the graph above that the blended model far outperforms the other models, with an RMSLE of 0.075. This is the model I'll use for making the final predictions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:47:21.530716Z","start_time":"2020-07-04T18:47:21.514197Z"},"trusted":true},"cell_type":"code","source":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(os.path.join(folder, \"sample_submission.csv\"))\nsubmission.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By individual models","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:54:37.010903Z","start_time":"2020-07-04T18:54:36.809223Z"},"trusted":true},"cell_type":"code","source":"# LGB\nsubmission.iloc[:,1] = np.expm1(lgb_model_full_data.predict(X_test))\n# without fixing outliers\nsubmission.to_csv(\"/kaggle/working/lgb_pred0.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"/kaggle/working//lgb_pred1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By stacked models","execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T19:00:55.892267Z","start_time":"2020-07-04T19:00:53.64911Z"},"trusted":true},"cell_type":"code","source":"# Append predictions from blended models\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test)))\nsubmission.to_csv(\"/kaggle/working//blended_0.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T19:04:06.586107Z","start_time":"2020-07-04T19:04:06.55918Z"},"trusted":true},"cell_type":"code","source":"# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"/kaggle/working//blended_fix_outlier.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-07-04T18:04:06.503334Z","start_time":"2020-07-04T17:56:39.844Z"},"trusted":true},"cell_type":"code","source":"# Scale predictions\nsubmission['SalePrice'] *= 1.001619 # why this magic number?\nsubmission.to_csv(\"/kaggle/working//submission_regression2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Somehow, the blended model is worse than LGB.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}