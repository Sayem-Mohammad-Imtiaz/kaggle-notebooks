{"cells":[{"metadata":{"_cell_guid":"7713a91e-9537-4cc6-96ad-0bf5c6b8b161","_uuid":"08d1ac326dfb6d6254fd7545104b05cb0cd9ea4c"},"cell_type":"markdown","source":"# Logistic Regression Optimization"},{"metadata":{"_cell_guid":"97a00775-11d7-437b-85bd-e000e987a597","_uuid":"ffbf19f6b39e9f20418acd6beb7de68ccf84a606","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom patsy import dmatrices\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9538c34-91e8-4d32-80de-e31025055255","_uuid":"4acbeb132ae9d695044e5a88aabe4a3072a51c46","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/HR_comma_sep.csv\")\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5c2f8a08-7492-4132-89ba-ca4a1f578c8a","_uuid":"d953ac085f6188eacd880d01d8bee34d48d1d589","trusted":false},"cell_type":"code","source":"data.left = data.left.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a6389de0-6be1-4077-8777-23b9113eb583","_uuid":"1a6e3da43c5c43b6eca8d4e660479487050443b4","trusted":false},"cell_type":"code","source":"y, X = dmatrices('left~satisfaction_level+last_evaluation+number_project+average_montly_hours+time_spend_company+Work_accident+promotion_last_5years+C(sales)+C(salary)', data, return_type='dataframe')\nX = np.asmatrix(X)\ny = np.ravel(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84ee455e8322d0d4f8d8f0d2b5dc758a3ef6734c","trusted":false},"cell_type":"code","source":"X[:,4]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c398896b2723fee68dc36ec3a84be949c12ad97","trusted":false},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c144091-01eb-415c-bd42-706fcded6b03","_uuid":"13e6b3c21be9fc3567b189844e1acb1bb0e0c1ee"},"cell_type":"markdown","source":"将所有列的值归一化到[0,1]区间。\n\n目前数据标准化方法有多种，归结起来可以分为直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。不同的标准化方法，对系统的评价结果会产生不同的影响，然而不幸的是，在数据标准化方法的选择上，还没有通用的法则可以遵循。"},{"metadata":{"collapsed":true,"_cell_guid":"04b08df1-6e64-4a2c-aaf3-a4f5b08c3187","_uuid":"9d10376b43ec57b849555affcb992ac6e68411dd","trusted":false},"cell_type":"code","source":"for i in range(1, X.shape[1]):\n    xmin = X[:,i].min()\n    xmax = X[:,i].max()\n    X[:, i] = (X[:, i] - xmin) / (xmax - xmin)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b130d1c3399bfdadabf1351d3e49ea231dee490","trusted":false},"cell_type":"code","source":"np.random.seed(1)\nalpha = 1  # learning rate\nbeta = np.random.randn(X.shape[1]) # 随机初始化参数beta,19个\nbeta","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9abce712-649c-4582-85e3-274bf0d9174d","_uuid":"c66039121f042a9056e7cc55347fe90846b26429","trusted":false},"cell_type":"code","source":"for T in range(500):\n    prob = np.array(1. / (1 + np.exp(-np.matmul(X, beta)))).ravel()  # 根据当前beta预测离职的概率\n    prob_y = list(zip(prob, y))\n    loss = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y]) / len(y) # 计算损失函数的值\n    error_rate = 0\n    for i in range(len(y)):\n        if ((prob[i] > 0.5 and y[i] == 0) or (prob[i] <= 0.5 and y[i] == 1)):\n            error_rate += 1;\n    error_rate /= len(y)\n    if T % 5 ==0 :\n        print('T=' + str(T) + ' loss=' + str(loss) + ' error=' + str(error_rate))\n    # 计算损失函数关于beta每个分量的导数\n    deriv = np.zeros(X.shape[1])\n    for i in range(len(y)):\n        deriv += np.asarray(X[i,:]).ravel() * (prob[i] - y[i])\n    deriv /= len(y)\n    # 沿导数相反方向修改beta\n    beta -= alpha * deriv","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a1c2370d-19de-4643-89af-cb80b6b568c9","_uuid":"f0279ada0d6c3a4209d9132e0d291a18cd726bb9"},"cell_type":"markdown","source":"用检查梯度的方法检验运算是否正确"},{"metadata":{"_uuid":"092d3ff1442f34c7b55a482656ead5224af47e7f","trusted":false},"cell_type":"code","source":"#dF/dbeta0\nprob = np.array(1. / (1 + np.exp(-np.matmul(X, beta)))).ravel()  # 根据当前beta预测离职的概率\nprob_y = list(zip(prob, y))\nloss = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y]) / len(y) # 计算损失函数的值\nprint(loss)\nderiv = np.zeros(X.shape[1])\nfor i in range(len(y)):\n    deriv += np.asarray(X[i,:]).ravel() * (prob[i] - y[i])\nderiv /= len(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ddac99d2d1c3476b152ebb1259bee5a230c2a8","trusted":false},"cell_type":"code","source":"deriv[0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_kg_hide-output":true,"_uuid":"24c8acebbd4c729ac385be6b3c19ec0bada5cecb","trusted":false},"cell_type":"code","source":"# add a little delta\ndelta = 0.0001\nbeta [0] += delta","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eef575b39e61488e4ed8371df7f4524347b232f","trusted":false},"cell_type":"code","source":"# loss calculation\nprob = np.array(1. / (1 + np.exp(-np.matmul(X, beta)))).ravel()  # 根据当前beta预测离职的概率\nprob_y = list(zip(prob, y))\nloss2 = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y]) / len(y)\nprint(loss2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b66bd9eddc25c029b231fffe4a7371775643c9ca","trusted":false},"cell_type":"code","source":"a1 = (loss2 - loss) / delta  # (F(b0+delta, b1, b2,..., bn) - F(b0,b1,...bn)) / delta\na2 = deriv[0]  # 用公式 算出来的导数\nprint(a1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc23e8b86bbb068573fa787389055b07b8af3044","trusted":false},"cell_type":"code","source":"# err deviration =  e.g. 0.0001\nabs(a1-a2)/abs(a2)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f35fdec6dc5492724c41e8496cf0f8ff4b1bb272","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}