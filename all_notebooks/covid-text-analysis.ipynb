{"cells":[{"metadata":{},"cell_type":"markdown","source":" # Project description"},{"metadata":{},"cell_type":"markdown","source":"In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 200,000 scholarly articles, including over 100,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up."},{"metadata":{},"cell_type":"markdown","source":"## Importing and Installing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n#File processing\nimport zipfile\nimport numpy as np\nimport pandas as pd\nimport glob\nimport json\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scispacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install en_core_sci_md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_sci_md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download en","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading and transforming the files into a data frame"},{"metadata":{},"cell_type":"markdown","source":"Reference for some of the code in this section: [COVID EDA Initial Exploration Tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool)"},{"metadata":{},"cell_type":"markdown","source":"### Loading Metadata\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/CORD-19-research-challenge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_features = {'paper_id': [], 'title': [],\n                   'abstract': [], 'text': []}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading Json Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path='../input/CORD-19-research-challenge/document_parses/pdf_json'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df = pd.DataFrame.from_dict(corona_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"json_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)\nlen(json_filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(json_filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(json_filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_corona_df(json_filenames, df):\n  for file_name in json_filenames:\n    row = {'paper_id': None, 'title': None,\n           'abstract': None, 'text': None}\n    \n    with open(file_name) as json_data:\n     # if file_name == './sample_data/anscombe.json':\n      #  continue\n\n      data = json.load(json_data)\n\n      row['paper_id'] = data['paper_id'].strip() # ' 345 ' -> '345'\n      row['title'] = data['metadata']['title'].strip()\n\n      abstract_list = [abstract['text'] for abstract in data['abstract']]\n      abstract = '\\n '.join(abstract_list)\n      row['abstract'] = abstract.strip()\n\n      text_list = [text['text'] for text in data['body_text']]\n      text = '\\n '.join(text_list)\n      row['text'] = text.strip()\n\n      df = df.append(row, ignore_index = True)\n  return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df = return_corona_df(json_filenames, corona_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Text Analysis\nimport nltk\nfrom IPython.core.display import HTML\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting a text sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df['text'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning the file"},{"metadata":{},"cell_type":"markdown","source":"#### Checking for null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corona_df[corona_df['paper_id'] == ''])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corona_df[corona_df['title'] == ''])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corona_df[corona_df['abstract'] == ''])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corona_df[corona_df['text'] == ''])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df = corona_df[corona_df['title'] != '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df = corona_df[corona_df['abstract'] != '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.drop_duplicates(['abstract', 'text', 'title'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_text = corona_df['text'][5000]\nsample_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Taking a sample to analyse quickly"},{"metadata":{"trusted":true},"cell_type":"code","source":"#corona_df = corona_df.sample(n = 500, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#corona_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NLP Pre Processing the information"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = en_core_sci_md.load(disable=['tagger', 'parser', 'ner'])\nnlp.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(spacy.lang.en.stop_words.STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(spacy.lang.en.stop_words.STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_stop_words = ['et', 'al', 'doi', 'copyright', 'http', 'https', 'fig', 'table', 'result', 'show']\nfor word in new_stop_words:\n  nlp.vocab[word].is_stop = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extracting word radicals with lemming  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def spacy_tokenizer(sentence):\n  sentence = sentence.lower()\n  list = []\n  list = [word.lemma_ for word in nlp(sentence) if not (word.is_stop or\n                                                        word.like_num or\n                                                        word.is_punct or\n                                                        word.is_space or\n                                                        len(word) == 1)]\n  list = ' '.join([str(element) for element in list])\n\n  return list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_text = corona_df['text'][5000]\nsample_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removing numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = '1 ' + sample_text\ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removing punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = spacy_tokenizer(test)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df['text'] = corona_df['text'].apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sample_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corona_df['text'][5000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corona_df['text'][5000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treatment of Frequent Terms"},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in corona_df.iterrows():\n  print(row['paper_id'], row['title'])\n  text_file = open('corpus/' + row['paper_id'] + '.txt', 'w')\n  n = text_file.write(row['text'])\n  text_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import PlaintextCorpusReader\ncorpus = PlaintextCorpusReader('corpus', '.*')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = corpus.fileids()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.raw('00b88130d2a7c8489e209742494303b6731d7544.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = corpus.words()\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequency = nltk.FreqDist(words)\nmost_common = frequency.most_common(100)\nmost_common","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing with wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\ncolor_map = ListedColormap(['purple', 'goldenrod', 'red', 'royalblue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\ncloud = WordCloud(background_color = 'dark', max_words=100, colormap=color_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud = cloud.generate(corona_df['text'].str.cat(sep='\\n'))\nplt.figure(figsize=(15,15))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.to_csv('corona_df.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extraction of entities"},{"metadata":{},"cell_type":"markdown","source":"#### Testing the function"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting specific text\ntext = str(corona_df['text'][10644])\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_ent = spacy.load('en')\nnlp_ent.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp_ent(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for entity in doc.ents:\n  if entity.label_ == 'NORP' or entity.label_ == 'GPE':\n    print(entity.text, entity.label_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy import displacy\ndisplacy.render(doc, style = 'ent', jupyter = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Counting entities"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Which are the most cited countries in this dataset?\ngpe = []\nfor index, row in corona_df.iterrows():\n  text = row['text']\n  doc = nlp_ent(text)\n  for entity in doc.ents:\n    if entity.label_ == 'GPE':\n      gpe.append(str(entity.text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gpe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values_gpe, counts_gpe = np.unique(np.array(gpe), return_counts = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpe_df = pd.DataFrame({'value': values_gpe, 'counts': counts_gpe})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpe_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpe_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpe_df_filtered = gpe_df[gpe_df.counts > 50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpe_df_filtered.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpe_df_filtered.head(16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize': (15,8)})\nsns.barplot(x = 'value', y = 'counts', hue='value', data=gpe_df_filtered);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text for Search"},{"metadata":{},"cell_type":"markdown","source":"### Searching with NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = nltk.Text(corpus.words())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if any results were found for the word 'pulmonary':"},{"metadata":{"trusted":true},"cell_type":"code","source":"match = text.concordance('pulmonary', width = 150, lines=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Searching with NLTK has a lot of restrictions. The code above will only find the exact result, ignoring similar words. This is a very simple way of solving the challenge. "},{"metadata":{},"cell_type":"markdown","source":"### Searching with 'find'"},{"metadata":{},"cell_type":"markdown","source":"#### 'Find' method"},{"metadata":{"trusted":true},"cell_type":"code","source":"string = 'spread wuhan city china infect traveller cause sporadic secondary transmission city secondary city epidemic'\nsearch_string = 'city'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(string.find(search_string))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string[13]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string[13:13+10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string[13-10:13]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string[13:13+10000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Código baseado em: https://www.journaldev.com/23666/python-string-find"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_texts(input_str, search_str, number_of_words):\n  l = []\n  index = 0\n  number_of_words = number_of_words\n  while index < len(input_str):\n    i = input_str.find(search_str, index)\n    if i == -1:\n      return l\n    \n    if input_str[i-number_of_words:i] == '':\n      start = 0\n    else:\n      start = i - number_of_words\n\n    l.append(input_str[start:i] + input_str[i:i+number_of_words])\n    index = i + 1\n  return l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = find_texts(string, search_string, 50)\ntexts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a visualization with html:"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(f'<h1>{search_string.upper()}</h1>'))\ndisplay(HTML(f\"\"\"<p><strong>Number of matches:</strong> {len(texts)}</p>\"\"\"))\nfor i in texts:\n  #print(i)\n  marked_text = str(i.replace(search_string, f\"<mark>{search_string}</mark>\"))\n  #print(marked_text)\n  display(HTML(f\"\"\"<blockquote>... {marked_text} ...</blockquote>\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Implementing on the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string = 'pulmonary disease'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string = spacy_tokenizer(search_string)\nsearch_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_all_texts(input_str, search_str, number_of_words):\n  text_list = []\n  index = 0\n  number_of_words = number_of_words\n  while index < len(input_str):\n    i = input_str.find(search_str, index)\n    if i == -1:\n      return text_list\n    \n    if input_str[i-number_of_words:i] == '':\n      start = 0\n    else:\n      start = i - number_of_words\n\n    text_list.append(input_str[start:i] + input_str[i:i+number_of_words])\n    index = i + 1\n  return text_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"documents = []\nfor index, row in corona_df.iterrows():\n  documents.append(find_all_texts(row['text'], search_string, 40))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(documents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for doc in documents:\n  if doc != []:\n    print(doc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in corona_df.iterrows():\n  texts = find_all_texts(row['text'], search_string, 400)\n  if texts == []:\n    continue\n  \n  paper_id = row['paper_id']\n  title = row['title']\n  display(HTML(f'<h1>{search_string.upper()}</h1>'))\n  display(HTML(f\"\"\"<p>\n                      <strong>Title:</strong> {title}</br>\n                      <strong>ID:</strong> {paper_id}</br>\n                      <strong>Number of matches:</strong> {len(texts)}\n                   </p>\"\"\"))\n  for i in texts:\n    marked_text = str(i.replace(search_string, f\"<mark>{search_string}</mark>\"))\n    display(HTML(f\"\"\"<blockquote>... {marked_text} ...</blockquote>\"\"\"))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vantagens\n\nRápido\nFácil implementação\nMuito útil para pesquisas simples com uma palavra\nBom para palavras-chave\nDesvantagens\n\nSomente uma palavra-chave\nNão possui ordenação de importância\nConsidera somente a palavra \"completa\""},{"metadata":{},"cell_type":"markdown","source":"### Searching with Spacy"},{"metadata":{},"cell_type":"markdown","source":"#### Testing spacy"},{"metadata":{"trusted":true},"cell_type":"code","source":"string = 'spread wuhan city china infect traveller cause sporadic secondary transmission city secondary city epidemic'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_strings = ['city', 'traveller']\ntokens_list = [nlp(item) for item in search_strings]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nlp.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add('SEARCH', None, *tokens_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(string)\nmatches = matcher(doc)\nmatches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc[2:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc[10-5:10+5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matches[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matches[0][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matches[0][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc[matches[0][1]:matches[0][2]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Implementing spacy on the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_strings = ['smoking', 'pulmonary disease']\ntokens_list = [nlp(spacy_tokenizer(item)) for item in search_strings]\ntokens_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add('SEARCH', None, *tokens_list)\nnumber_of_words = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_strings_html = ' '.join([str(element) for element in search_strings])\nsearch_strings_html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in corona_df.iterrows():\n  marked_text = ''\n  doc = nlp(row['text'])\n  paper_id = row['paper_id']\n  title = row['title']\n  matches = matcher(doc)\n  if matches == []:\n    continue\n\n  display(HTML(f'<h1>{search_strings_html.upper()}</h1>'))\n  display(HTML(f\"\"\"<p>\n                      <strong>Title:</strong> {title}</br>\n                      <strong>ID:</strong> {paper_id}</br>\n                      <strong>Number of matches:</strong> {len(matches)}\n                   </p>\"\"\"))\n  for i in matches:\n    start = i[1] - number_of_words\n    if start < 0:\n      start = 0\n    for j in range(len(tokens_list)):\n      if doc[i[1]:i[2]].similarity(tokens_list[j]) == 1.0:\n        search_text = str(tokens_list[j])\n        marked_text += str(doc[start:i[2] + number_of_words]).replace(search_text, f\"<mark>{search_text}</mark>\")\n        marked_text += \"<br /><br />\"\n  display(HTML(f\"\"\"<blockquote>... {marked_text} ...</blockquote>\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vantagens\n- Fácil implementação\n- Muito útil para pesquisas simples com mais palavras\n- Bom para palavras-chave\n- Usa uma biblioteca própria para processamento de linguagem natural\n\nDesvantagens\n- Não possui ordenação de importância"},{"metadata":{},"cell_type":"markdown","source":"### Fuzzywuzzy for strings similarity\nDistância Levenshtein: https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_Levenshtein"},{"metadata":{},"cell_type":"markdown","source":"#### Tests with fuzzywuzzy"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fuzzywuzzy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install python-Levenshtein","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import fuzz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similaridade da string em ordem\nfuzz.ratio('Apple Inc.', 'Apple')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.partial_ratio('Apple Inc.', 'Apple')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignora a ordem das palavras\nfuzz.token_sort_ratio('Lakers x Chigaco Bulls', 'Chigago Bulls x Lakers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignora palavras duplicadas\nfuzz.token_set_ratio('Today we have a great game: Lakers x Chigago Bulls', 'Chicago Bulls x Lakers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparison with the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string = 'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying lemmatizer: "},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string = spacy_tokenizer(search_string)\nprint(search_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratio = []\npartial_ratio = []\nsort_ratio = []\nset_ratio = []\nfor index, row in corona_df.iterrows():\n  ratio.append(fuzz.ratio(row['text'], search_string))\n  partial_ratio.append(fuzz.partial_ratio(row['text'], search_string))\n  sort_ratio.append(fuzz.token_sort_ratio(row['text'], search_string))\n  set_ratio.append(fuzz.token_set_ratio(row['text'], search_string))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(partial_ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(sort_ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(set_ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparison with abstract"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df['abstract'] = corona_df['abstract'].apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratio = []\npartial_ratio = []\nsort_ratio = []\nset_ratio = []\nfor index, row in corona_df.iterrows():\n  ratio.append(fuzz.ratio(row['abstract'], search_string))\n  partial_ratio.append(fuzz.partial_ratio(row['abstract'], search_string))\n  sort_ratio.append(fuzz.token_sort_ratio(row['abstract'], search_string))\n  set_ratio.append(fuzz.token_set_ratio(row['abstract'], search_string))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(partial_ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(sort_ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(set_ratio).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting the most similar papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {}\nfor index, row in corona_df.iterrows():\n  scores[row['paper_id']] = fuzz.token_set_ratio(row['text'], search_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores['532f2c636fca1caae1f23885b9dc0e3302a0afd5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\nsorted_scores = sorted(scores.items(), key=operator.itemgetter(1), reverse = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_scores[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.loc[corona_df['paper_id'] == '68a7101a90454172c91785d8c352f776a82df5d4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\ndisplay(HTML(f'<h4>{search_string.upper()}</h4>'))\nfor i in sorted_scores[:10]:\n  df = corona_df.loc[corona_df['paper_id'] == i[0]]\n  display(HTML(f\"\"\"<p>\n                      <strong>Title:</strong> {df['title']}</br>\n                      <strong>ID:</strong> {i[0]}</br>\n                      <strong>Score:</strong> {i[1]}</br>\n                      <strong>Abstract:</strong> {str(df['abstract'])[0:700]}\n                   </p>\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vantagens\n\nBiblioteca com várias funções\nÚtil tanto para pesquisas por palavras-chave quanto para textos maiores\nNotas de acordo com a importância\nDesvantagens\n\nModelo de cálculo matemático não é muito adequado"},{"metadata":{},"cell_type":"markdown","source":"### Similar papers with TF-IDF and cosine similarity"},{"metadata":{},"cell_type":"markdown","source":"#### TD-IDF"},{"metadata":{},"cell_type":"markdown","source":"TF-IDF (Term frequency - inverse document frequency)\nRedimensionar a frequência das palavras pela frequência com que aparecem em todos os documentos\n\nTerm frequency (TF): frequência da palavra no documento atual - TF = (número de vezes que o termo t aparece no documento) / (número de termos no documento)\n\nInverse document frequency (IDF): quão rara é a palavra nos documentos - IDF = log(N/n), N é o número de documentos e n é o número de documentos que o termo t apareceu\n\nTF-IDF: importância de uma palavra para um documento em uma coleção ou corpus\n\nConsiderando um documento com 100 palavras no qual a palavra cachorro aparece 5 vezes\n\nTF = 5 / 100 = 0.05\n\nTemos 100 documentos no total (N) e a palavra cachorro aparece aparece em 20 desses documentos (n)\n\nIDF = log(100 / 20) = 0.69\n\nTF-IDF = 0.05 * 0.69 = 0.034\n\nQuanto maior o valor do peso, mais raro é o termo. Quanto menor o peso, mais comum é o termo"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = corona_df['text'][:3].tolist()\ntexts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts.append(texts[0])\ntexts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\nvectorized = tfidf.fit_transform(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf.vocabulary_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf.idf_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized.todense().shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cos similarity"},{"metadata":{},"cell_type":"markdown","source":"Link: https://en.wikipedia.org/wiki/Cosine_similarity\nCálculos passo a passo: https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\nTemos duas matrizes: TF e IDF\nCosine Similarity (d1, d2) = Dot product(d1, d2) / ||d1|| * ||d2||"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized[0].todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_similarity(vectorized[0], vectorized[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_similarity(vectorized[0], vectorized[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarity = cosine_similarity(vectorized[0], vectorized)\nsimilarity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### TD-IDF and cosine similarity in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = corona_df['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\nvectorized = tfidf.fit_transform(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string = 'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string = spacy_tokenizer(search_string)\nprint(search_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string_vectorized = tfidf.transform([search_string])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_string_vectorized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarity = cosine_similarity(search_string_vectorized, vectorized)\nsimilarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(similarity[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_dict = {}\nfor i in range(len(similarity[0])):\n  scores_dict[i] = similarity[0][i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\nsorted_scores = sorted(scores_dict.items(), key=operator.itemgetter(1), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sorted_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(f'<h4>{search_string.upper()}</h4>'))\nfor i in sorted_scores[:10]:\n  df = corona_df.iloc[i[0]]\n\n  display(HTML(f\"\"\"<p>\n                      <strong>Title:</strong> {df['title']}</br>\n                      <strong>ID:</strong> {df['paper_id']}</br>\n                      <strong>Score:</strong> {i[1]}</br>\n                      <strong>Abstract:</strong> {str(df['abstract'][0:700])}\n                   </p></br>\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grouping the Papers"},{"metadata":{},"cell_type":"markdown","source":"#### Treating the database"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_completo = pd.read_csv('/content/gdrive/My Drive/corona_df_completo.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_completo.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_completo = corona_df_completo.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_completo.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', 100)\ncorona_df_completo.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndataset_texts = corona_df_completo['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dataset_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=2**12)\nvectorized = tfidf.fit_transform(dataset_texts)\nvectorized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reduction of dimensionality"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_pca = pca.fit_transform(vectorized.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This resulted in a 2 columns matrix with all the 29k articles. "},{"metadata":{"trusted":true},"cell_type":"code","source":"components = pca.explained_variance_ratio_\ncomponents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize': (10,8)})\nsns.scatterplot(X_pca[:,0], X_pca[:, 1])\nplt.title('Covid-19 Papers');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Defining number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss = []\nfor i in range(1, 21):\n  kmeans = MiniBatchKMeans(n_clusters = i, random_state = 0)\n  kmeans.fit(vectorized)\n  wcss.append(kmeans.inertia_)\nplt.plot(range(1, 21), wcss)\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grouping with k-means"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 5\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"palette = sns.color_palette('bright', len(set(y_pred)))\nsns.scatterplot(X_pca[:,0], X_pca[:, 1], hue=y_pred, legend='full', palette=palette)\nplt.title('Clustered Covid-19 Papers');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on: https://www.kaggle.com/maksimeren/covid-19-literature-clustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_notebook()\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_pca[:,0], \n    y= X_pca[:,1],\n    x_backup = X_pca[:,0],\n    y_backup = X_pca[:,1],\n    desc= y_labels, \n    titles= corona_df_completo['title'],\n    abstract = corona_df_completo['abstract'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"Covid-19 Papers\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# add callback to control \ncallback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var radio_value = cb_obj.active;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            labels = data['desc'];\n            \n            if (radio_value == '20') {\n                for (i = 0; i < x.length; i++) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                }\n            }\n            else {\n                for (i = 0; i < x.length; i++) {\n                    if(labels[i] == radio_value) {\n                        x[i] = x_backup[i];\n                        y[i] = y_backup[i];\n                    } else {\n                        x[i] = undefined;\n                        y[i] = undefined;\n                    }\n                }\n            }\n\n\n        source.change.emit();\n        \"\"\")\n\n# callback for searchbar\nkeyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var text_value = cb_obj.value;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            abstract = data['abstract'];\n            titles = data['titles'];\n            \n            for (i = 0; i < x.length; i++) {\n                if(abstract[i].includes(text_value) || \n                   titles[i].includes(text_value)  {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                } else {\n                    x[i] = undefined;\n                    y[i] = undefined;\n                }\n            }\n        source.change.emit();\n        \"\"\")\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-4\", \"C-5\",\n                                  \"C-6\", \"C-7\", \"C-8\",\n                                  \"C-9\", \"C-10\", \"C-11\",\n                                  \"C-12\", \"C-13\", \"C-14\",\n                                  \"C-15\", \"C-16\", \"C-17\",\n                                  \"C-18\", \"C-19\", \"All\"], \n                          active=20, callback=callback)\n\n# search box\nkeyword = TextInput(title=\"Search:\", callback=keyword_callback)\n\n#header\nheader = Div(text=\"\"\"<h1>Covid-19 Papers</h1>\"\"\")\n\n# show\nshow(column(header, widgetbox(option, keyword),p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summarizing the papers"},{"metadata":{},"cell_type":"markdown","source":"#### Test with the function"},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_original = pd.read_csv('/content/gdrive/My Drive/corona_df_original.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_original.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df_original.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-extractive-summarizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from summarizer import Summarizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = str(corona_df_original['text'][0])\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Summarizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = ''.join(result)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}