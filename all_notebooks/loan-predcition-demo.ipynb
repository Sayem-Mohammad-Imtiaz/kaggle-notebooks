{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> <b> Demo on typical ML tasks for Loan Prediction <center> <b>\n\n### Introduction \n\n<font size=3> \nThe problem statement is to identify eligibility of customers for a home related loan product. \n\nAs explained in the presentation, building a ML model requires data to be understood properly. A model developer may start with some hypothesis and validate it using the data. Hypothesis generation is based on common understanding about the problem statement. Busines understanding of the data and offcourse problem statement is essential to make a good model. \n  \nTypical tasks that are undertaken in building a ML model are as follows:\n*     Data processing\n*     Data exploration & Visualization\n*     Model building & Evaluation\n   \nNot all data is in a form which can be directly be consumed by a model or can be explored. For e.g. sometimes data comes with various missing values and it becomes essential to understand what exactly led to missing values and what can be done about them before using them in the model."},{"metadata":{},"cell_type":"markdown","source":"Dont worry about the coding too much now. The process of solving the problem should be focus rather than coding.\nLet's deep dive..!! "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## Importing Kaggle directory for getting input dataset so that we can use the path to read the data. \n##The below code prints the path where the loan data set is hosted. \n## We will use that path to read data set using python code\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Importing Packages**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lot of tasks associated with building a model or data processing comes in form of libraries/packages which are essentially\n## curated set of codes. Rather than building them from scratch, We use a lot of these to do tasks related to Model dev, data processing etc.\n## Below we import some of these packages","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np  # for mathematical operations\nimport pandas as pd  # for data operations and processing\nimport seaborn as sns # for visualization\nfrom matplotlib import pyplot as plt  #for visualization\nfrom pylab import plot, show, subplot, specgram, imshow, savefig #for visualization\nfrom matplotlib.pyplot import rcParams\nrcParams['figure.figsize'] = 10,6\n\n#Sklearn is the one of the most important library having lot of modules for data processing, ML models development and validation\n#Below we import some of these\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,f1_score, confusion_matrix , recall_score , precision_score\nfrom sklearn import tree\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the input dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we are reading the data set from the location we printed earlier. We are using pandas library (imported in the earlier cell)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/loan-data-set/loan_data_set.csv\") # To read csv using pandas. df is the variable in which we are storing the contents of the file\n\ndf.sample(5) # To see a sample of 5 records from the data set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we are deriving some data statistics to get a sense of loan data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What does the data consist of- (No. of rows, No. of columns)\ndf.shape\n# So we have 614 data points and 13 columns in the loan data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the data types? Whether a content in a column is string or object or integer or float etc.\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get descriptive stats like count, No. of unique values, mean etc\ndf.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Majority of the times, values in the data set are missing and understanding those missing values really becomes important. \n# For building a model we have to really be sure how to treat them, whether to replace them or whether to completely remove the rows\n# or columns where quite a lot of missing values are there","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Missing Values\n\n# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \"columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \"columns that have missing values\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are very less %age of values missing from every column. So we can use some of the popluar to techniques to handle them. \n# While there is a rigorous academic material to analyse the missing values and treating them, here we are using simple imputation of \n# missing values by mean values if the column is numeric and ignore or remove from the ones where column is categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Missing values\n* Replace numerical values with mean\n* Ignore/remove categorical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the data points where loan amount term is missing\ndf[df['Loan_Amount_Term'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The below two lines of code replace the Loan Amount term and Loan Amount with their respective column means\ndf.LoanAmount.fillna(df.LoanAmount.mean(),inplace=True)\ndf.Loan_Amount_Term.fillna(df.Loan_Amount_Term.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If we check the Loan Amount term column for Null values again- we see there are no rows having null values\ndf[df['Loan_Amount_Term'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the Rows where any of the columns are still null\ndf.dropna(how=\"any\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"Loan_ID\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking top 5 rows of data\ndf.head(5)\n# See the Columns which had categorical values have been mapped to numeric values. We can always derive what value were mapped \n# to what numeric values using inverse_transform(y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = [\"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\",\"Credit_History\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numerical_cols].hist(figsize=(30,15), bins=20)\nplt.suptitle(\"Histograms of numerical values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"as_fig = sns.FacetGrid(df,hue='Loan_Status',aspect=5)\n\nas_fig.map(sns.kdeplot,'ApplicantIncome',shade=True)\n\noldest = df['ApplicantIncome'].max()\n\nas_fig.set(xlim=(0,oldest))\n\nas_fig.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"as_fig = sns.FacetGrid(df,hue='Loan_Status',aspect=5)\n\nas_fig.map(sns.kdeplot,'CoapplicantIncome',shade=True)\n\noldest = df['CoapplicantIncome'].max()\n\nas_fig.set(xlim=(0,oldest))\n\nas_fig.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"as_fig = sns.FacetGrid(df,hue='Loan_Status',aspect=5)\n\nas_fig.map(sns.kdeplot,'LoanAmount',shade=True)\n\noldest = df['LoanAmount'].max()\n\nas_fig.set(xlim=(0,oldest))\n\nas_fig.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Gender',kind='count',data=df,hue='Loan_Status')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Dependents',kind='count',data=df,hue='Loan_Status')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Education',kind='count',data=df,hue='Loan_Status')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Self_Employed',kind='count',data=df,hue='Loan_Status')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsns.factorplot('Credit_History',kind='count',data=df,hue='Loan_Status')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding the categorical values (which are non numeric) into corresonding numeric values\n\ndf[\"Gender\"] = df[\"Gender\"].map({'Male':1, 'Female':0})\ndf[\"Married\"] = df[\"Married\"].map({'Yes':1, 'No':0})\ndf[\"Dependents\"] = df[\"Dependents\"].map({'0':0, '1':0,'2':2,'3+':3})\ndf[\"Self_Employed\"] = df[\"Self_Employed\"].map({'Yes':1, 'No':0})\ndf[\"Education\"] = df[\"Education\"].map({'Graduate':1, 'Not Graduate':0})\ndf[\"Property_Area\"] = df[\"Property_Area\"].map({'Semiurban':0, 'Rural':0,'Urban':2})\ndf[\"Loan_Status\"] = df[\"Loan_Status\"].map({'Y':1, 'N':0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cateforical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(4,5))\nsns.countplot(x = \"Education\", data=df, order = df[\"Education\"].value_counts().index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"ApplicantIncome\", y=\"LoanAmount\", data=df, col=\"Gender\",color=\"Blue\",alpha=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.relplot(x=\"Loan_Amount_Term\", y=\"LoanAmount\", data=df,kind=\"line\",hue=\"Education\",ci=None)\ng.fig.set_size_inches(15,7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(15,8))\nsns.heatmap(data=df.corr().round(2),annot=True,linewidths=0.5,cmap=\"Blues\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Importance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(importance,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier().fit(df.drop(\"Loan_Status\",axis=1),df[\"Loan_Status\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance(rf_model.feature_importances_,df.drop(\"Loan_Status\",axis=1).columns,'RANDOM FOREST')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc_model = GradientBoostingClassifier().fit(df.drop(\"Loan_Status\",axis=1),df[\"Loan_Status\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance(gbc_model.feature_importances_,df.drop(\"Loan_Status\",axis=1).columns,'GRADIENT BOOSTING')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Loan_Status',axis=1)\ny = df['Loan_Status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=DecisionTreeClassifier()\nXtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.25,random_state=0)\nmodel.fit(Xtrain,ytrain)\nplt.figure(figsize=(20,20))\ntree.plot_tree(model.fit(Xtrain,ytrain))\ndt_predict = model.predict(Xtest)\nprint ('Accuracy:', accuracy_score(ytest, dt_predict)*100,\"%\")\nprint ('Precision:', precision_score(ytest, dt_predict,average='weighted')*100,\"%\")\nprint ('Recall:', recall_score(ytest, dt_predict,average='weighted')*100,\"%\")\nprint ('F1 score:', f1_score(ytest, dt_predict,average='weighted')*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['Credit_History', 'ApplicantIncome', 'LoanAmount', 'CoapplicantIncome', 'Loan_Amount_Term']]\nmodel=DecisionTreeClassifier()\nXtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.25,random_state=0)\nmodel.fit(Xtrain,ytrain)\nplt.figure(figsize=(20,20))\ntree.plot_tree(model.fit(Xtrain,ytrain))\ndt_predict = model.predict(Xtest)\nprint ('Accuracy:', accuracy_score(ytest, dt_predict)*100,\"%\")\nprint ('Precision:', precision_score(ytest, dt_predict,average='weighted')*100,\"%\")\nprint ('Recall:', recall_score(ytest, dt_predict,average='weighted')*100,\"%\")\nprint ('F1 score:', f1_score(ytest, dt_predict,average='weighted')*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier()\nXtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.25,random_state=0)\nrf_model.fit(Xtrain,ytrain)\nrf_model_pred = rf_model.predict(Xtest)\nprint ('Accuracy:', accuracy_score(ytest, rf_model_pred)*100,\"%\")\nprint ('Precision:', precision_score(ytest, rf_model_pred,average='weighted')*100,\"%\")\nprint ('F1 score:', f1_score(ytest, rf_model_pred,average='weighted')*100,\"%\")\nprint ('Recall:', recall_score(ytest, rf_model_pred,average='weighted')*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc_model = AdaBoostClassifier()\nXtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.25,random_state=0)\nabc_model.fit(Xtrain,ytrain)\nabc_model_pred = abc_model.predict(Xtest)\nprint ('Accuracy:', accuracy_score(ytest, abc_model_pred)*100,\"%\")\nprint ('Precision:', precision_score(ytest, abc_model_pred,average='weighted')*100,\"%\")\nprint ('Recall:', recall_score(ytest, abc_model_pred,average='weighted')*100,\"%\")\nprint ('F1 score:', f1_score(ytest, abc_model_pred,average='weighted')*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplot()\nsns.heatmap(confusion_matrix(ytest,abc_model_pred), annot=True, ax=ax);\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title('Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}