{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"!git clone 'https://gist.github.com/917986b5f6fbc50ee2ea54b9db75d537.git' model\n!pip install torchsummary\nimport gc\nimport os,h5py\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom torchsummary import summary\nfrom model import resnet\nimport torch\nfrom torch import nn,optim\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"EXAMPLE_DATA_ROOT_FOLDER = '/kaggle/input/textiledefectdetection'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class TextureDataset(Dataset):\n    filenames = {64: dict(train=('train64.csv', 'train64.h5'), test=('test64.csv', 'test64.h5')),\n                 32: dict(train=('train32.csv', 'train32.h5'), test=('test32.csv','test32.h5'))}\n    indication_classes = ['index', 'angle', 'indication_type', 'indication_value', 'split']\n    def __init__(self,root_folder, train=True, patch_size=64, classification=True, task='defect',keep_angles=True,keep_defects=False, transformation=None, test_sub_sample=200):\n        if os.path.exists(root_folder):\n            self.root_folder = root_folder\n        else:\n            raise IOError('The path of the directory is incorrect or doesn\\'t exist')\n        \n        self.patch_size = patch_size if patch_size in self.filenames.keys() else 32\n        self.infos, self.data = self.load_data(train)\n        if train:\n            if classification:\n                print('Classification Task:', end=' ')\n                if task=='defect' and not keep_angles:    \n                    self.infos = self.infos.loc[self.infos['angle']==0]\n                    print('Drop all angles others than 0')\n                elif task=='angle' and not keep_defects:\n                    self.infos = self.infos.loc[self.infos['indication_type']=='good']\n                    print('Drop all defect (all other than good)')\n                else:\n                    print('Keep√≠ng all the data')\n                    pass\n            else:\n                print('By default only keep healthy')\n                self.infos = self.infos.loc[(self.infos['indication_type']=='good') & (self.infos['angle']==0)]\n            self.data = self.data[self.infos.index]\n        self.transformation = transforms.Compose([transforms.ToTensor()]) if transformation is None  else  transformation\n        \n        \n        if not train and test_sub_sample:\n            X = []\n            newinfo = pd.DataFrame()\n            for (a,t), df in self.infos.groupby(['angle','indication_type']):\n                index = df.index\n                subi = np.random.choice(index, test_sub_sample, replace=False)\n                X.append(self.data[subi])\n                newinfo = newinfo.append(self.infos.iloc[subi],ignore_index=True)\n            self.infos = newinfo\n            self.data = np.concatenate(X)\n    \n    def __len__(self):\n        return self.infos.shape[0]\n        \n    def __getitem__(self,index):\n        info = self.infos.iloc[index]\n        angle, indication_value = int(info['angle'])//20, int(info['indication_value'])\n        img = self.data[index]\n        \n        return self.transformation(img),angle, indication_value\n            \n    \n    def load_data(self,train):\n        files = self.filenames[self.patch_size]\n        infos_filename, data_filename = files['train'] if train else files['test']\n        \n        infos = pd.read_csv(os.path.join(self.root_folder,infos_filename))\n        data = None\n        with h5py.File(os.path.join(self.root_folder,data_filename),mode='r') as h5file:\n            data = h5file['images'][:]\n        return infos, data        \n    \n    @staticmethod\n    def get_angles_classes():\n        return np.arange(8)* 20\n    @staticmethod\n    def get_indication_classes():\n        return ['good', 'color', 'cut',  'hole', 'thread', 'metal_contamination']\n    @staticmethod\n    def compute_normalization_parameters(root):\n        print('#### Compute Mean and Std of image for image scaling')\n        dataset = TextureDataset(root_folder=root, classification=True,task='angle',keep_defects=False)\n        loader = DataLoader(dataset,batch_size=15, num_workers=4, shuffle=True)\n        mean = 0.\n        std = 0.\n        for images, _,_ in loader:\n            batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n            images = images.view(batch_samples, images.size(1), -1)\n            mean += images.mean(2).sum(0)\n            std += images.std(2).sum(0)\n\n        mean /= len(loader.dataset)\n        std /= len(loader.dataset)\n        print(f\"\"\" Normalize the data as following:\n        transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize({mean}, {std} )])\n        this function return the object Normalize the new mean and std: transforms.Normalize({mean}, {std} )\n        \"\"\")\n        return transforms.Normalize(mean, std )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def example():\n    transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.3541], [0.1352])])\n    \n    dataset = TextureDataset(EXAMPLE_DATA_ROOT_FOLDER,classification=True,task='defect',transformation=transformation)\n    loader = DataLoader(dataset=dataset,batch_size=64, shuffle=True)\n    for data in loader:\n        img, angle_value, indication_value = data\n        grid_img = make_grid(img,nrow=8).permute(1, 2, 0)\n        plt.figure()\n        plt.imshow(grid_img)\n        plt.axis('off')\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"example()\nTextureDataset.compute_normalization_parameters(root=EXAMPLE_DATA_ROOT_FOLDER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class Trainer(object):\n    def __init__(self, model,  optimizer, criterion,  train_loader, val_loader,no_epochs=5, device='cpu',target_type='defect',lr=1e-1, *args,**kwargs):\n        \n        assert target_type in ['angle','defect']\n        self.no_epochs = no_epochs\n        self.target_type = target_type\n        self.model = model.to(device)\n        self.criterion = criterion\n        self.optimizer = optimizer(self.model.parameters(),lr=lr) \n        \n        self.train_loader, self.valid_loader = train_loader, val_loader\n        print(\"Total_params\",sum(p.numel() for p in model.parameters()))\n        print(\"Trainable_params\",sum(p.numel() for p in model.parameters() if p.requires_grad))\n        self.device = device\n        \n\n    def train_an_epoch(self):\n        self.model.train()\n        train_loss = 0.0\n        for data in self.train_loader:\n            \n            if self.target_type =='angle':\n                data, target, _ = data\n            else:\n                data, _, target = data\n            data, target = data.to(self.device), target.to(self.device)\n            # clear the gradients of all optimized variables\n            self.optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = self.model(data)\n            # calculate the batch loss\n            loss = self.criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            self.optimizer.step()\n            # update training loss\n            train_loss += loss.item()*data.size(0)\n        return train_loss\n        \n    def valid_an_epoch(self):\n        valid_loss = 0.0\n        self.model.eval()\n        \n        OUTPUT = []\n        TARGET = []\n        for data in self.valid_loader:\n            if self.target_type =='angle':\n                data, target, _ = data\n            else:\n                data, _, target = data\n            data, target = data.to(self.device), target.to(self.device)\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = self.model(data)\n            # calculate the batch loss\n            loss = self.criterion(output, target)\n            # update average validation loss \n            valid_loss += loss.item()*data.size(0)            \n            # converting raw _logits to softmax output\n            output = nn.functional.softmax(output,dim=-1).detach().cpu().numpy()\n            OUTPUT.append(np.argmax(output, axis=-1))\n            TARGET.append(target.detach().cpu().numpy())\n        return valid_loss, self.accuracy_per_class(np.concatenate(OUTPUT),np.concatenate(TARGET))\n    \n    def accuracy_per_class(self,ouput,target):\n        matrix = confusion_matrix(target, ouput)\n        return matrix.diagonal()/matrix.sum(axis=1)\n    \n    def train(self):\n        accuracy = []\n        alltrainloss = []\n        allvalidloss = []\n        for epoch in range(1, self.no_epochs+1):\n\n            train_loss = self.train_an_epoch()\n            valid_loss, accuracy_perclass = self.valid_an_epoch()\n            train_loss = train_loss/len(self.train_loader.sampler)\n            valid_loss = valid_loss/len(self.valid_loader.sampler)\n            gc.collect()\n            \n            alltrainloss.append(train_loss)\n            allvalidloss.append(valid_loss)\n            accuracy.append(accuracy_perclass.reshape(1,-1))\n            # print training/validation statistics \n            print(f'Epoch: {epoch} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f}')\n        figure,axes = plt.subplots(1,2,figsize=(10,8))\n        ax1,ax2= axes.ravel()\n        linet, = ax1.plot(alltrainloss,label='Train')\n        linev, = ax1.plot(allvalidloss,label='Valid')\n        ax1.legend([linet,linev],['train','valid'])\n        labels = TextureDataset.get_indication_classes() if self.target_type=='defect' else TextureDataset.get_angles_classes()\n        lines = []\n        for i, c in  zip(labels, np.concatenate(accuracy).T):\n            line, = ax2.plot(c, label=i)\n            lines.append(line)\n        ax2.legend(lines,labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"\n\nclass CNN(nn.Module):\n    def __init__(self, nclasses, in_channel=1,*args,**kwargs):\n        super(CNN, self).__init__()\n        print(nclasses)\n        self.encoder = nn.Sequential(nn.Conv2d(1, 32, kernel_size=3),nn.BatchNorm2d(32),nn.ReLU(),\n                                     nn.AvgPool2d(2),\n                                     nn.Conv2d(32, 64, kernel_size=3),nn.BatchNorm2d(64),nn.ReLU(),\n                                     nn.AvgPool2d(2),\n                                     nn.Conv2d(64, 32, kernel_size=1),\n                                     nn.AdaptiveAvgPool2d(1))\n        self.l = nn.Linear(32, nclasses)\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        x= x.view(x.shape[0],-1)\n        return self.l(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getdataloader(root, patch_size=64, task='defect',batch_size=32,keep_angles=False,test_sub_sample=False,num_workers=5, *args,**kwargs):\n    transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.3541], [0.1352]), transforms.RandomErasing(0.3, value=0)])\n    dataset = TextureDataset(root,train=True, classification=True, task=task, transformation=transformation,keep_angles=keep_angles)\n    train_loader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers, pin_memory=True)\n    transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.3541], [0.1352])])\n    dataset = TextureDataset(root,train=False, classification=True, task=task, transformation=transformation,test_sub_sample=test_sub_sample)\n    val_loader = DataLoader(dataset=dataset,batch_size=int(batch_size*2),shuffle=False,num_workers=num_workers)\n    return train_loader, val_loader\n    \ndef get_model(modeltype='cnn',  nclasses=6, no_of_channels=1,*args,**kwargs):\n    if modeltype =='resnet18': model = resnet.resnet18(in_channels=no_of_channels, n_classes=nclasses)\n    elif modeltype =='resnet34': model = resnet.resnet34(in_channels=no_of_channels, n_classes=nclasses)\n    elif modeltype =='resnet50': model = resnet.resnet50(in_channels=no_of_channels, n_classes=nclasses)\n    elif modeltype =='resnet101': model = resnet.resnet101(in_channels=no_of_channels, n_classes=nclasses)\n    elif modeltype =='resnet152': model = resnet.resnet152(in_channels=no_of_channels, n_classes=nclasses)\n    else:model = CNN(nclasses=nclasses, in_channel=no_of_channels)\n    return model\n        \n    \ndef run(options):\n    train_loader, val_loader = getdataloader(**options)        \n    model = get_model(**options)\n    summary(model, (1,options['patch_size'],options['patch_size']),1,'cpu')\n    trainer = Trainer(model=model, optimizer=optim.Adam, criterion=nn.CrossEntropyLoss(),  train_loader=train_loader, val_loader=val_loader, **options)\n    trainer.train()\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter_dict=dict(root='/kaggle/input/textiledefectdetection', patch_size=32, batch_size=64, task='defect', nclasses = len(TextureDataset.get_indication_classes()), \n                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), lr=5e-3, no_epochs=20, keep_angles=True, test_sub_sample=200,num_workers=4, modeltype='resnet18')\nrun(parameter_dict)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}