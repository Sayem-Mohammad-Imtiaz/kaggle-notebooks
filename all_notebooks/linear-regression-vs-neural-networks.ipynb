{"cells":[{"metadata":{},"cell_type":"markdown","source":"So, this is based off a simple example from the book \"Deep Learning with Python,\" by Francois Chollet. Chollet himself is a scientist at Google Brain, and the creator of Keras. This is a fantastic book on getting started with using Keras for deep learning purposes, if you haven't read it yet.\n\nRecently, however, I have been using PyTorch. Anytime I have to build something more complicated from scratch, I find PyTorch provides a much easier framework to work with. It also is a much more Pythonic library, making it feel like a much more natural extension of Python."},{"metadata":{},"cell_type":"markdown","source":"# EDA\n\nWe will be using the famous Boston housing pricing data. Having 506 data points, the dataset is famously small. I will be constructing both a linear model, using Scikit-Learn; and a neural network using PyTorch. Chollet constructs a neural network using Keras. My model has comparable performance.\n\nBut first, let us have a quick look at the data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncolumn_names = [\n    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', \n    'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', \n    'LSTAT', 'MEDV'\n]\n\nboston_data = pd.read_csv('../input/boston-house-prices/housing.csv', \n                          header=None, \n                          delimiter=r\"\\s+\", \n                          names=column_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, let us have a look at the"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a correlation matrix\ncorrelations = boston_data.corr()\nsns.heatmap(correlations, square=True, cmap=\"YlGnBu\")\n\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, taking a look at various histograms."},{"metadata":{"trusted":true},"cell_type":"code","source":"boston_data.hist(bins=10, figsize=(9,7), grid=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Model"},{"metadata":{},"cell_type":"markdown","source":"First, let us build a linear model. Due to the large number of parameters relative to number of training examples, I will be using Ridge regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\n# Translating our data into arrays for processing.\nx = np.array(boston_data.drop(['MEDV'], axis=1))\ny = boston_data['MEDV'].values\n\n# Train/test split for validation.\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)\n\n# Our Model\nlr = Ridge(alpha=0.5)\nlr.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nr2_score(lr.predict(x_test), y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Our Neural Network"},{"metadata":{},"cell_type":"markdown","source":"Now, let us build our neural network. I will be using two latent layers of neurons, as you can see below from the code. Some tweaking was done on my end to achieve good performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\n\nbatch_size = 50\nnum_epochs = 250\nlearning_rate = 0.001\nhidden_size = 64\nbatch_no = len(x_train) // batch_size\ninput_dim = x.shape[1]\n\n# Use a single hidden layer NN.\nmodel = nn.Sequential(\n    nn.Linear(input_dim, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, 1)\n)\n\n# Use mean squared error loss.\nloss = nn.MSELoss(reduce='mean')\n\n# Use Adam to optimize our NN.\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"running_loss = 0\n\nfor epoch in range(num_epochs):\n    for i in range(402):\n        start = i\n        end = start + 1\n        \n        x_batch = autograd.Variable(torch.FloatTensor(x_train[start:end]))\n        y_batch = autograd.Variable(torch.FloatTensor(y_train[start:end]))\n                \n        y_pred = model(x_batch)\n        \n        loss_step = loss(y_pred, torch.unsqueeze(y_batch, dim=1))\n        optimizer.zero_grad()\n        loss_step.backward()\n        optimizer.step()\n        running_loss += loss_step.item()\n    \n    print(\"Epoch {}, Loss: {}. Validation R2: {}\".format(\n        epoch + 1, running_loss, r2_score(model(torch.Tensor(x_test)).detach().numpy(), y_test)))\n    running_loss = 0.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It should be noted that even though I'm getting, fairly significant improvements, there are two downsides to this approach.\n\n1. Some more training time was needed. However, this was not a huge issue in this case. On a CPU this was trained in about 30 seconds.\n2. More time was taken on my end to tweak all the addition parameters."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}