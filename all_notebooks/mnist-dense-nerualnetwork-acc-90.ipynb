{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n# Change Label To 10 NumpyArry\nfrom keras.utils import np_utils\n# For Neural Netword \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import SGD\nfrom keras.losses import categorical_crossentropy\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task1: Load Our Data Into Train And Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/mnist-in-csv/mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If You Look At The Data See The Both Of Theme Has label Columns We Seprate It Into y_train, y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = test.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('label', axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop('label', axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test.iloc[:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.iloc[:, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For Normalize The Data Our Type Must Be Float Not Int"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalize:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train /= 255\nx_test /= 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Change To 10 Part Of Number Not Just 0, 8, 9 "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TASK2: Creat Model:"},{"metadata":{},"cell_type":"markdown","source":"### Befor We Go Down Some Option Exists For Loss\n### If You Use ** sparse_categorical_crossentropy ** We must Have Sparse Label (For Each Instance, There Is Just A Target Class Index From 0 To 9\n### So If You Want Use this loss You Dont Need To Use np_utils.to_categorical\n### If We Were Doing Binary Classification loss Must Be ** binary_crossentropy **\n### For One Probability Per class For Each Instance Use ** categorical_crossentropy **"},{"metadata":{"trusted":true},"cell_type":"code","source":"myModel = Sequential()\n# just input layer need input_shape\nmyModel.add(Dense(400, activation='relu', input_shape=(784,)))\nmyModel.add(Dropout(0.2))\nmyModel.add(Dense(300, activation='relu'))\n#myModel.add(Dropout(0.2))\nmyModel.add(Dense(300, activation='relu'))\n#myModel.add(Dropout(20))\nmyModel.add(Dense(10, activation='softmax'))\n\nmyModel.summary()\nmyModel.compile(optimizer=SGD(lr=0.001), loss=categorical_crossentropy, metrics=['accuracy'])\n# for Reduce The Overfiting Use Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dou You Know How Calculate param 235500? \n## Dont Worry I Explain It As Clear As I Can \n## Our Input Are 768 So Multiply On Next Neural Which Is 300 , And Each Neural Has Bias Finally ==> 768*300+300 = 235500\n## Next Hidden Layer 300*300+300 = 90300\n## I Hope You Undrestand How This Param Work"},{"metadata":{},"cell_type":"markdown","source":"# -----------------------------Done-----------------------------------\n## Fit Our Model And See loss Must Be Near To 0 And Accuracy Must Near To 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_nerual = myModel.fit(x_train, y_train, batch_size=128, epochs=25, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = myModel.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If See The Result Of Predict It's Not Undrestandable So We Use argmax To Return It As It Before Like 0, 8, 7, "},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = np.argmax(predict, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = result_nerual.history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize "},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history['loss']\nval_loss = history['val_loss']\nplt.plot(loss)\nplt.plot(val_loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['loss', 'val_loss'])\nplt.figure()\n\naccuracy = history['accuracy']\nval_accuracy = history['val_accuracy']\nplt.plot(accuracy)\nplt.plot(val_accuracy)\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend(['accuracy', 'val_acuracy'])\nplt.legend()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}