{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Goal: To classify future SMS messages as either spam or ham with a Naive Bayes model.\n\nSteps:\n\n1.  Convert the words ham and spam to a binary indicator variable(0/1)\n\n2.  Convert the txt to a sparse matrix of TFIDF vectors\n\n3.  Fit a Naive Bayes Classifier\n\n4.  Measure your success using roc_auc_score\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import naive_bayes\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nltk.download()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv(\"../input/sms_spam.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"#### Train the classifier if it is spam or ham based on the text"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"#### Convert the spam and ham to 1 and 0 values respectively for probability testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.type.replace('spam', 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.type.replace('ham', 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Our dependent variable will be 'spam' or 'ham' \ny = df.type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF Vectorizer\nstopset = set(stopwords.words('english'))\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert df.txt from text to features\nX = vectorizer.fit_transform(df.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n\n### IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n\n## tf-idf score=TF(t)*IDF(t)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Spliting the SMS to separate the text into individual words\nsplt_txt1=df.text[0].split()\nprint(splt_txt1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Count the number of words in the first SMS\nlen(splt_txt1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It means in the first SMS there are 20(len(splt_txt1)) words & out of which only 14 elements have been taken, that;s why we'll get only 14 tf-idf values for the first the SMS.Likewise elements or words of all other SMSes are taken into consideration"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0 is the first SMS,3536,4316 etc are the positions of the elements or the words & 0.15,0.34,0.27 are the tf_idf value of the words . Like wise we can find the next SMSes & the tf-idf value of the words of the SMSes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.get_feature_names()[8585]## 4316 is the position of the word jurong","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Second SMS"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Spliting the SMS to separate the text into individual words\nsplt_txt2=df.text[1].split()\nprint(splt_txt2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(splt_txt2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[1]## Second SMS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (X[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Finding the most frequent word appearing in the second SMS\nmax(splt_txt2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above in the 2nd SMS there are 6 words  & out of which only 5 elements have been taken, that's why\n### we'll get only 5 tf-idf values for the 2nd the SMS.Likewise elements or words of all other SMSes are taken into consideration"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Last word in the vocabulary\nmax(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (y.shape)\nprint (X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Split the test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Train Naive Bayes Classifier\n## Fast (One pass)\n## Not affected by sparse data, so most of the 8605 words dont occur in a single observation\nclf = naive_bayes.MultinomialNB()\nmodel=clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.feature_log_prob_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class=model.predict(X_test)\nprint(predicted_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First 3 SMSes are correctly assigned to Ham(0) based on the tf-idf scores of the words given in the SMSes"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[[19]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class[19]## This SMS(SMS no. 19) has been classified as Ham but Actually it's SPAM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find the probability of assigning a SMS to a specific class"},{"metadata":{"trusted":true},"cell_type":"code","source":"prd=model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.predict_proba(X_test)[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Check model's accuracy\nroc_auc_score(y_test, clf.predict_proba(X_test)[:,1])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### With the model, the success rate is ~98.60%"},{"metadata":{},"cell_type":"markdown","source":"Identify the words which are the most important in classifying a message as spam"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_most_important_features(vectorizer, model, n=5):\n    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n    \n    # loop for each class\n    classes ={}\n    for class_index in range(model.coef_.shape[0]):\n        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n        bottom = sorted_coeff[-n:]\n        classes[class_index] = {\n            'tops':tops,\n            'bottom':bottom\n        }\n    return classes\n\nimportance = get_most_important_features(vectorizer, clf, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n    y_pos = np.arange(len(top_words))\n    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n    \n    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n    \n    top_words = [a[0] for a in top_pairs]\n    top_scores = [a[1] for a in top_pairs]\n    \n    bottom_words = [a[0] for a in bottom_pairs]\n    bottom_scores = [a[1] for a in bottom_pairs]\n    fig = plt.figure(figsize=(10, 10))  \n\n    plt.subplot(121)\n    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n    plt.title('Ham', fontsize=20)\n    plt.yticks(y_pos, bottom_words, fontsize=14)\n    plt.suptitle('Key words', fontsize=16)\n    plt.xlabel('Importance', fontsize=20)\n    \n    plt.subplot(122)\n    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n    plt.title('Spam', fontsize=20)\n    plt.yticks(y_pos, top_words, fontsize=14)\n    plt.suptitle(name, fontsize=16)\n    plt.xlabel('Importance', fontsize=20)\n    \n    plt.subplots_adjust(wspace=0.8)\n    plt.show()\ntop_scores = [a[0] for a in importance[0]['tops']]\ntop_words = [a[1] for a in importance[0]['tops']]\nbottom_scores = [a[0] for a in importance[0]['bottom']]\nbottom_words = [a[1] for a in importance[0]['bottom']]\n\nplot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# END**"}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}