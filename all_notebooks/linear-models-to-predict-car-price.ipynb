{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-30T15:02:11.691934Z","iopub.execute_input":"2021-05-30T15:02:11.692438Z","iopub.status.idle":"2021-05-30T15:02:11.713994Z","shell.execute_reply.started":"2021-05-30T15:02:11.692311Z","shell.execute_reply":"2021-05-30T15:02:11.712805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import necessary libraries for the project","metadata":{}},{"cell_type":"code","source":"#matplotlib and seaborn are imported for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#splitting the dataset into train & test data\nfrom sklearn.model_selection import train_test_split\n\n#GridSearchCV is used for hyperparameter tuning in Lasso & Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n#three linear models used in the project\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\n#StandardScaler for preprocessing the dataset\nfrom sklearn.preprocessing import StandardScaler\n\n#metrics to evaluate the linear regression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\n#import warnings to ignore any warnings during execution\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:15.614713Z","iopub.execute_input":"2021-05-30T15:02:15.615122Z","iopub.status.idle":"2021-05-30T15:02:16.84618Z","shell.execute_reply.started":"2021-05-30T15:02:15.615087Z","shell.execute_reply":"2021-05-30T15:02:16.845016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**EXPLORATORY DATA ANALYSIS(EDA)**","metadata":{}},{"cell_type":"markdown","source":"load the csv file ","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"../input/car-price-dataset/CarPrice.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:16.848135Z","iopub.execute_input":"2021-05-30T15:02:16.848504Z","iopub.status.idle":"2021-05-30T15:02:16.866765Z","shell.execute_reply.started":"2021-05-30T15:02:16.848466Z","shell.execute_reply":"2021-05-30T15:02:16.865673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"display the first five entries of data","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:16.974043Z","iopub.execute_input":"2021-05-30T15:02:16.974458Z","iopub.status.idle":"2021-05-30T15:02:17.029987Z","shell.execute_reply.started":"2021-05-30T15:02:16.974418Z","shell.execute_reply":"2021-05-30T15:02:17.028812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset contains 205 rows with 26 features","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:17.49931Z","iopub.execute_input":"2021-05-30T15:02:17.499706Z","iopub.status.idle":"2021-05-30T15:02:17.506635Z","shell.execute_reply.started":"2021-05-30T15:02:17.499668Z","shell.execute_reply":"2021-05-30T15:02:17.505353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features in the dataset does not contain any null values","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:17.887117Z","iopub.execute_input":"2021-05-30T15:02:17.887551Z","iopub.status.idle":"2021-05-30T15:02:17.897258Z","shell.execute_reply.started":"2021-05-30T15:02:17.8875Z","shell.execute_reply":"2021-05-30T15:02:17.896336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"info() will return the informations of columns(features),count of non-null values and datatype of individual columns","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:18.257287Z","iopub.execute_input":"2021-05-30T15:02:18.257848Z","iopub.status.idle":"2021-05-30T15:02:18.281372Z","shell.execute_reply.started":"2021-05-30T15:02:18.257811Z","shell.execute_reply":"2021-05-30T15:02:18.280302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**describe()** describes the features of the dataframe by & default it will show the description of only int and float features but by specifying **include=\"all\"** we will be able to get the description of all features irrespective of its datatypes","metadata":{}},{"cell_type":"code","source":"data.describe(include=\"all\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:21.777979Z","iopub.execute_input":"2021-05-30T15:02:21.778489Z","iopub.status.idle":"2021-05-30T15:02:21.890561Z","shell.execute_reply.started":"2021-05-30T15:02:21.778453Z","shell.execute_reply":"2021-05-30T15:02:21.889295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have dropped two features **car_ID** & **CarName** because they does not effect the price of the car in the dataset.","metadata":{}},{"cell_type":"code","source":"#data.drop(axis=0) by default so its important to specify the axis=1 else you can specify \n#columns=[\"car_ID\",\"CarName\"]\ndf_car=data.drop([\"car_ID\",\"CarName\"],\n         axis=1\n         )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:22.19417Z","iopub.execute_input":"2021-05-30T15:02:22.194555Z","iopub.status.idle":"2021-05-30T15:02:22.201048Z","shell.execute_reply.started":"2021-05-30T15:02:22.194522Z","shell.execute_reply":"2021-05-30T15:02:22.19945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now able to see the remaining columns in the dataset after dropping **car_ID** and **CarName**","metadata":{}},{"cell_type":"code","source":"df_car.columns","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:22.679688Z","iopub.execute_input":"2021-05-30T15:02:22.680041Z","iopub.status.idle":"2021-05-30T15:02:22.687405Z","shell.execute_reply.started":"2021-05-30T15:02:22.680011Z","shell.execute_reply":"2021-05-30T15:02:22.686163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Display the first five entries of dataframe","metadata":{}},{"cell_type":"code","source":"df_car.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:23.072189Z","iopub.execute_input":"2021-05-30T15:02:23.072678Z","iopub.status.idle":"2021-05-30T15:02:23.105522Z","shell.execute_reply.started":"2021-05-30T15:02:23.072584Z","shell.execute_reply":"2021-05-30T15:02:23.104377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the dataset into **dependent(y)** & **independent(X)** variables,\n\nDependent variable is also called the target variable which is **price** of the car in our case","metadata":{}},{"cell_type":"code","source":"X=df_car.drop(columns=[\"price\"])\ny=df_car[\"price\"]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:23.440358Z","iopub.execute_input":"2021-05-30T15:02:23.440878Z","iopub.status.idle":"2021-05-30T15:02:23.447475Z","shell.execute_reply.started":"2021-05-30T15:02:23.440844Z","shell.execute_reply":"2021-05-30T15:02:23.446556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Specify two empty lists **cat_col** & **num_col** to store **categorical** and **numerical** columns respectively","metadata":{}},{"cell_type":"code","source":"cat_col=[]#will store categorical features\nnum_col=[]#will store numerical features\n\n#iterating thourgh all columns in X\nfor col in X:\n    #append the features whose datatype is object in cat_col\n    if df_car[col].dtype==\"O\":\n        cat_col.append(col)\n    #append those features whose datatype is other than object in num_col    \n    else:\n        num_col.append(col)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:23.792024Z","iopub.execute_input":"2021-05-30T15:02:23.792627Z","iopub.status.idle":"2021-05-30T15:02:23.799697Z","shell.execute_reply.started":"2021-05-30T15:02:23.792568Z","shell.execute_reply":"2021-05-30T15:02:23.798895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create dataFrames **df_cat** & **df_num** to store the features with datatypes **object** and **numerical** respectively","metadata":{}},{"cell_type":"markdown","source":"By dividing the dataframe into **numerical** & **categorical** features seperately, it will allow an ease handling of numerical and categorical features in their respective dataframes.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#dataframe to store the categorical features\ndf_cat=pd.DataFrame(\n    data=df_car,\n    #we will use the column names from the cat_col list\n    columns=cat_col\n)\n\n#dataframe to store the categorical features\ndf_num=pd.DataFrame(\n    data=df_car,\n    #we will use the column names from the num_col list\n    columns=num_col\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:24.421818Z","iopub.execute_input":"2021-05-30T15:02:24.422456Z","iopub.status.idle":"2021-05-30T15:02:24.429648Z","shell.execute_reply.started":"2021-05-30T15:02:24.422405Z","shell.execute_reply":"2021-05-30T15:02:24.428509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display head of Dataframe with numerical features","metadata":{}},{"cell_type":"code","source":"df_num.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:24.752695Z","iopub.execute_input":"2021-05-30T15:02:24.753085Z","iopub.status.idle":"2021-05-30T15:02:24.78564Z","shell.execute_reply.started":"2021-05-30T15:02:24.753046Z","shell.execute_reply":"2021-05-30T15:02:24.784396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display head of Dataframe with categorical features","metadata":{}},{"cell_type":"code","source":"df_cat.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:25.08583Z","iopub.execute_input":"2021-05-30T15:02:25.086543Z","iopub.status.idle":"2021-05-30T15:02:25.107466Z","shell.execute_reply.started":"2021-05-30T15:02:25.086493Z","shell.execute_reply":"2021-05-30T15:02:25.106509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print the **labels** of each columns in **df_cat**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"for cols in df_cat:\n    print(cols,\" contains :\",df_cat[cols].nunique(),\" labels\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:25.680906Z","iopub.execute_input":"2021-05-30T15:02:25.681486Z","iopub.status.idle":"2021-05-30T15:02:25.6971Z","shell.execute_reply.started":"2021-05-30T15:02:25.681438Z","shell.execute_reply":"2021-05-30T15:02:25.696158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the dataset does not contain any null values hence the df_num & df_cat will also have no null values, however we are intrested in df_cat whose datatype is object.\n\nTo feed our data to the Machine Learning Models the data values must be converted into numerical values.\n\n","metadata":{}},{"cell_type":"markdown","source":"Perform **one-hot encoding** to the categorical features, using **pd.get_dummies()**\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_cat=pd.get_dummies(\n    data=df_cat,\n    drop_first=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:26.348826Z","iopub.execute_input":"2021-05-30T15:02:26.349441Z","iopub.status.idle":"2021-05-30T15:02:26.364588Z","shell.execute_reply.started":"2021-05-30T15:02:26.349405Z","shell.execute_reply":"2021-05-30T15:02:26.36335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display the head of **one-hot encoded** dataframe","metadata":{}},{"cell_type":"code","source":"df_cat.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:26.639187Z","iopub.execute_input":"2021-05-30T15:02:26.639577Z","iopub.status.idle":"2021-05-30T15:02:26.662704Z","shell.execute_reply.started":"2021-05-30T15:02:26.639545Z","shell.execute_reply":"2021-05-30T15:02:26.661801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have converted categorical features into **numerical** values by perfoming **one-hot** encoding & now we have all the features on both **df_num** & **df_cat** in numerical form so we **concatenate** them to get the final desired dataframe.","metadata":{}},{"cell_type":"code","source":"car_final=pd.concat(\n    [df_num,df_cat,y],\n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:27.005685Z","iopub.execute_input":"2021-05-30T15:02:27.00606Z","iopub.status.idle":"2021-05-30T15:02:27.015133Z","shell.execute_reply.started":"2021-05-30T15:02:27.006029Z","shell.execute_reply":"2021-05-30T15:02:27.013865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data into **dependent(y)** and **independent(X)** variables","metadata":{}},{"cell_type":"code","source":"X=car_final.drop(\"price\",\n                axis=1)\ny=car_final[\"price\"]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:27.507299Z","iopub.execute_input":"2021-05-30T15:02:27.507675Z","iopub.status.idle":"2021-05-30T15:02:27.515121Z","shell.execute_reply.started":"2021-05-30T15:02:27.507638Z","shell.execute_reply":"2021-05-30T15:02:27.513755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data into **training** and **testing** data, with **test data** of size of **20%** of total dataset.","metadata":{}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.2\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:27.711969Z","iopub.execute_input":"2021-05-30T15:02:27.712375Z","iopub.status.idle":"2021-05-30T15:02:27.72204Z","shell.execute_reply.started":"2021-05-30T15:02:27.712338Z","shell.execute_reply":"2021-05-30T15:02:27.720849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform preprocessing on the **X_train** and **X_test** using **StandardScaler()**\n\nIt will scale the data values in such a way that the **mean is zero** and a **variance of one**","metadata":{}},{"cell_type":"code","source":"scaler=StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train)\nX_test_scaled=scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:28.289951Z","iopub.execute_input":"2021-05-30T15:02:28.290358Z","iopub.status.idle":"2021-05-30T15:02:28.307722Z","shell.execute_reply.started":"2021-05-30T15:02:28.290321Z","shell.execute_reply":"2021-05-30T15:02:28.306533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since I am comparing three linear models namely **LinearRegression() Ridge() & Lasso()**, I will have to write the same codes again and again for individual linear models so I have used **functions** that would perform the same job for all three regression models to **ease** and **shorten** my work","metadata":{}},{"cell_type":"markdown","source":"The function is used to fit the models in given linear model and return **training** and **testing** **scores**\n\n","metadata":{}},{"cell_type":"code","source":"#the function takes model, train & test split as an argument\ndef fit_model_getScores(model,X_train,y_train,X_test,y_test):\n    #fit the model with training dataset\n    model.fit(X_train,y_train)\n    \n    #score the training data\n    train_score=model.score(X_train,y_train)\n    #score the test data\n    test_score=model.score(X_test,y_test)\n    \n    #Display the scores\n    print(\"Scores of {}\".format(model),\"\\n\")\n    print(\"Training Score:{:.2f}\".format(train_score))\n    print(\"Testing Score:{:.2f}\".format(test_score))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:29.012541Z","iopub.execute_input":"2021-05-30T15:02:29.012942Z","iopub.status.idle":"2021-05-30T15:02:29.0223Z","shell.execute_reply.started":"2021-05-30T15:02:29.012908Z","shell.execute_reply":"2021-05-30T15:02:29.020468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function given below will help return the **metrics** used for **evaluating linear models** & that includes **mse,mae,rmse,r2_score**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#function takes model,and test data split as an argument \ndef get_metrics(model,X_test,y_test):\n    #calculate the predicted value of y \n    y_pred=model.predict(X_test)\n    mse=mean_squared_error(y_test,y_pred)#mse\n    r2__score=r2_score(y_test,y_pred)#r2_score\n    mae=mean_absolute_error(y_test,y_pred)#mae\n    rmse=mean_squared_error(y_test,y_pred,squared=False)#rmse\n    \n    #print the metrics \n    print(\"The Metrics for {}:\".format(model))\n    print(\"----------------------------\")\n    print(\"Mean Squared Error:{:.2f}\".format(mse))\n    print(\"Root Mean Squared Error:{:.2f}\".format(rmse))\n    print(\"Mean Absolute Error:{:.2f}\".format(mae))\n    print(\"r2_score:{:.2f}\".format(r2__score))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:29.664966Z","iopub.execute_input":"2021-05-30T15:02:29.66536Z","iopub.status.idle":"2021-05-30T15:02:29.672033Z","shell.execute_reply.started":"2021-05-30T15:02:29.665322Z","shell.execute_reply":"2021-05-30T15:02:29.671127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function will calculate the **coefficient** of a given **linear model** and will return the **series of coefficient** with **independent features(columns) as an index**.\n\n**Note**: In a linear model the **numbers(count) of coefficient** is always **equal** to **the number of independent features** present in the dataset ","metadata":{}},{"cell_type":"code","source":"#the function takes model and independent dataframe as an argument\ndef return_coef_series(model,X):\n    #it will give the coefficeint pertaining to a specific linear model\n    coef=model.coef_\n    \n    #make a series out of coefficient with columns of X as an index \n    coef_series=pd.Series(\n        data=coef,\n        index=X.columns\n    )\n    \n    #return the series\n    return coef_series\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:29.916646Z","iopub.execute_input":"2021-05-30T15:02:29.917164Z","iopub.status.idle":"2021-05-30T15:02:29.92144Z","shell.execute_reply.started":"2021-05-30T15:02:29.917128Z","shell.execute_reply":"2021-05-30T15:02:29.920659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **plot_coef**() will help **visualize** the **coefficient** of a particular **linear model**","metadata":{}},{"cell_type":"code","source":"#takes coefficient of linear_model as an argument\ndef plot_coef(model_coef):\n    fig=plt.figure(figsize=(12,8))\n    model_coef.plot(\n        kind=\"bar\"\n    )\n    plt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:30.319893Z","iopub.execute_input":"2021-05-30T15:02:30.320438Z","iopub.status.idle":"2021-05-30T15:02:30.32493Z","shell.execute_reply.started":"2021-05-30T15:02:30.320402Z","shell.execute_reply":"2021-05-30T15:02:30.324077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below given function is used to perform h**yper-parameter tuning** for **Ridge()** and **Lasso()** regression.\n\n**GridSearchCV** is used for **hypertuning** and return the **best parameters fitting the linear model.**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#takes model, user-defined hyper-parameters, train & test data splits as argument \ndef gridSearch(model,params,X_train,y_train):\n    grid=GridSearchCV(\n        estimator=model,\n        param_grid=params,\n        cv=5\n    )\n    grid.fit(X_train,y_train)\n   \n    return grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:30.969827Z","iopub.execute_input":"2021-05-30T15:02:30.970373Z","iopub.status.idle":"2021-05-30T15:02:30.975202Z","shell.execute_reply.started":"2021-05-30T15:02:30.970335Z","shell.execute_reply":"2021-05-30T15:02:30.974257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BUILDING LINEAR MODELS**","metadata":{}},{"cell_type":"markdown","source":"**1-LinearRegression() Model**\n\n**LinearRegression(aka ordinary least squares):** Simplest & most classic linear method for regression. It finds the parameters w & b that minimize the mean squard error between predicted value and true value.\n\ny=wx + b\n\nw->Weights associated with individual independent features(Slope of a line)\n\nb->y intercept\n\n","metadata":{}},{"cell_type":"code","source":"#initializing the model\nlinear_model=LinearRegression()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:31.37144Z","iopub.execute_input":"2021-05-30T15:02:31.371846Z","iopub.status.idle":"2021-05-30T15:02:31.375312Z","shell.execute_reply.started":"2021-05-30T15:02:31.371811Z","shell.execute_reply":"2021-05-30T15:02:31.374419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets fit the LinearRegression and fetch **training** and **testing** scores","metadata":{}},{"cell_type":"code","source":"fit_model_getScores(linear_model,\n                    X_train_scaled,y_train,\n                    X_test_scaled,y_test\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:31.97448Z","iopub.execute_input":"2021-05-30T15:02:31.974981Z","iopub.status.idle":"2021-05-30T15:02:32.002838Z","shell.execute_reply.started":"2021-05-30T15:02:31.974948Z","shell.execute_reply":"2021-05-30T15:02:32.000577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the **metrics** to evaluate LinearRegression","metadata":{}},{"cell_type":"code","source":"get_metrics(linear_model,\n            X_test_scaled,y_test\n           )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:32.249715Z","iopub.execute_input":"2021-05-30T15:02:32.250082Z","iopub.status.idle":"2021-05-30T15:02:32.258692Z","shell.execute_reply.started":"2021-05-30T15:02:32.250051Z","shell.execute_reply":"2021-05-30T15:02:32.257359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function **return_coef_series** will return the series of **coefficient** along with **features** as its **index**.","metadata":{}},{"cell_type":"code","source":"linear_coef=return_coef_series(linear_model,X).sort_values()\nlinear_coef","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:32.879611Z","iopub.execute_input":"2021-05-30T15:02:32.880139Z","iopub.status.idle":"2021-05-30T15:02:32.89156Z","shell.execute_reply.started":"2021-05-30T15:02:32.880088Z","shell.execute_reply":"2021-05-30T15:02:32.890333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets visualize the coefficient of LinearRegression() model","metadata":{}},{"cell_type":"code","source":"plot_coef(linear_coef)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:33.124391Z","iopub.execute_input":"2021-05-30T15:02:33.124933Z","iopub.status.idle":"2021-05-30T15:02:33.681631Z","shell.execute_reply.started":"2021-05-30T15:02:33.124895Z","shell.execute_reply":"2021-05-30T15:02:33.680784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2-Ridge():** It is a linear model which uses **L2 regularization** technique.\n\n**L2 Regularization:** Regularization techniques explicitly restricts a model to aviod overfitting.\n\n**LinearRegression()** does not allow us to control its complexity so its very likely that it will **overfit** the models when the dataset is **relatively small**.\n\n**l2 regularization** reduces the cofficient of the independent features to small magnitude as possible i.e all entries of **w should be close to zero**\n\n**Ridge** have **alpha parameter** which makes a trade-off between the simplicity of the model and its perfomance on training set & hence tuning it will yeild different model performance.","metadata":{}},{"cell_type":"code","source":"ridge_model=Ridge()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:33.683378Z","iopub.execute_input":"2021-05-30T15:02:33.683967Z","iopub.status.idle":"2021-05-30T15:02:33.688465Z","shell.execute_reply.started":"2021-05-30T15:02:33.683928Z","shell.execute_reply":"2021-05-30T15:02:33.687323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets perform Hyperparameter tuning and fit the ridge model with the best parameters generated by GridSeachCV hypertuning method","metadata":{}},{"cell_type":"markdown","source":"GridSeachCV gave us **alpha=1** and **max_iter=1000** as the best parameters for the model","metadata":{}},{"cell_type":"code","source":"params={\n    \"alpha\":[1e-9,1e-6,1e-3,1,100,1000,10000],\n    \"max_iter\":[1e3,1e4,1e5,1e6]#maximum number of iterations to run\n}\n\nridge_best_params=gridSearch(ridge_model,params,X_train_scaled,y_train)\nridge_best_params","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:34.6183Z","iopub.execute_input":"2021-05-30T15:02:34.618664Z","iopub.status.idle":"2021-05-30T15:02:35.159579Z","shell.execute_reply.started":"2021-05-30T15:02:34.618632Z","shell.execute_reply":"2021-05-30T15:02:35.158346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit ridge model with best parameters","metadata":{}},{"cell_type":"code","source":"ridge1_model=Ridge(**ridge_best_params)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:35.165826Z","iopub.execute_input":"2021-05-30T15:02:35.169136Z","iopub.status.idle":"2021-05-30T15:02:35.177922Z","shell.execute_reply.started":"2021-05-30T15:02:35.169055Z","shell.execute_reply":"2021-05-30T15:02:35.176178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit the ridge model and return the **test** and **train** scores\n\nFitting the model we get scores equivalent to the LinearRegression()","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"fit_model_getScores(ridge1_model,\n                    X_train_scaled,y_train,\n                    X_test_scaled,y_test\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:35.430133Z","iopub.execute_input":"2021-05-30T15:02:35.43053Z","iopub.status.idle":"2021-05-30T15:02:35.444419Z","shell.execute_reply.started":"2021-05-30T15:02:35.430494Z","shell.execute_reply":"2021-05-30T15:02:35.443291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the metrics to evaluate ridge model","metadata":{}},{"cell_type":"code","source":"get_metrics(ridge1_model,\n            X_test_scaled,y_test\n           )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:36.075322Z","iopub.execute_input":"2021-05-30T15:02:36.075685Z","iopub.status.idle":"2021-05-30T15:02:36.084313Z","shell.execute_reply.started":"2021-05-30T15:02:36.075653Z","shell.execute_reply":"2021-05-30T15:02:36.083104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the coefficeint series of ridge model\n\nFrom the values returned in series, we can see that the **coefficients** have been **reduced** to the fractions of its original value.\n\n**L2 regularization** technique in Ridge reduces the coefficient the features **as close to zero.**\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"ridge_coef=return_coef_series(ridge1_model,X).sort_values()\nridge_coef","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:36.727084Z","iopub.execute_input":"2021-05-30T15:02:36.727504Z","iopub.status.idle":"2021-05-30T15:02:36.737039Z","shell.execute_reply.started":"2021-05-30T15:02:36.727465Z","shell.execute_reply":"2021-05-30T15:02:36.735875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the coefficient series of ridge model\n\nWe can see that the upper & lower x-limmits have been reduced.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"plot_coef(ridge_coef)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:37.420066Z","iopub.execute_input":"2021-05-30T15:02:37.420471Z","iopub.status.idle":"2021-05-30T15:02:38.20084Z","shell.execute_reply.started":"2021-05-30T15:02:37.420433Z","shell.execute_reply":"2021-05-30T15:02:38.199694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3-Lasso():** It is a linear model which uses **L1 regularization** technique.\n\n**l1 regularization** also reduces the coefficient magnitude however unlike Ridge it **reduces magnitude of some of the features to zero**. Hence it **neglects** some of the features completely.\n\nHence it is also used for **automatic feature selection** as it ignores some of the features.\n\n**Lasso** also have **alpha parameter** which makes a trade-off between the simplicity of the model and its perfomance on training set & hence tuning it will yeild different model performance.","metadata":{"execution":{"iopub.status.busy":"2021-05-30T09:04:06.840186Z","iopub.execute_input":"2021-05-30T09:04:06.840528Z","iopub.status.idle":"2021-05-30T09:04:06.845909Z","shell.execute_reply.started":"2021-05-30T09:04:06.840499Z","shell.execute_reply":"2021-05-30T09:04:06.844644Z"}}},{"cell_type":"code","source":"lasso_model=Lasso()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:40.359713Z","iopub.execute_input":"2021-05-30T15:02:40.360075Z","iopub.status.idle":"2021-05-30T15:02:40.365924Z","shell.execute_reply.started":"2021-05-30T15:02:40.360043Z","shell.execute_reply":"2021-05-30T15:02:40.364667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got **alpha=100** & **max_iter=1000** for the lasso model \n\n","metadata":{}},{"cell_type":"code","source":"params={\n    \"alpha\":[1e-9,1e-6,1e-3,1,100,1000,10000],\n    \"max_iter\":[1e3,1e4,1e5,1e6]#maximum number of iterations to run\n}\n\nlasso_best_params=gridSearch(lasso_model,params,X_train_scaled,y_train)\nlasso_best_params","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:02:42.347868Z","iopub.execute_input":"2021-05-30T15:02:42.348284Z","iopub.status.idle":"2021-05-30T15:03:22.445346Z","shell.execute_reply.started":"2021-05-30T15:02:42.34822Z","shell.execute_reply":"2021-05-30T15:03:22.444343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets try fitting the Lasso model using the parameters that have been returned from Hypertuning","metadata":{}},{"cell_type":"code","source":"lasso1_model=Lasso(**lasso_best_params)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.446824Z","iopub.execute_input":"2021-05-30T15:03:22.447139Z","iopub.status.idle":"2021-05-30T15:03:22.451443Z","shell.execute_reply.started":"2021-05-30T15:03:22.44711Z","shell.execute_reply":"2021-05-30T15:03:22.450357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scores are **93% for training set** and **86% for testing set** which is better generalized model than the above two models i.e LinearRegression() & Ridge()","metadata":{}},{"cell_type":"code","source":"fit_model_getScores(lasso1_model,\n                    X_train_scaled,y_train,\n                    X_test_scaled,y_test\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.453435Z","iopub.execute_input":"2021-05-30T15:03:22.453759Z","iopub.status.idle":"2021-05-30T15:03:22.472772Z","shell.execute_reply.started":"2021-05-30T15:03:22.453727Z","shell.execute_reply":"2021-05-30T15:03:22.47154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print the metrics of lasso model","metadata":{}},{"cell_type":"code","source":"get_metrics(lasso1_model,\n            X_test_scaled,y_test\n           )","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.474508Z","iopub.execute_input":"2021-05-30T15:03:22.474906Z","iopub.status.idle":"2021-05-30T15:03:22.48359Z","shell.execute_reply.started":"2021-05-30T15:03:22.474872Z","shell.execute_reply":"2021-05-30T15:03:22.482322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see the magnitude of coefficients returned by the lasso model.\n\nThis is where it gets very intresting we can see that there are many features whose cofficients are reduced to zero. It means that the Lasso model have completely ignored those features with coefficients equals to zero while fitting the model.","metadata":{}},{"cell_type":"code","source":"lasso_coef=return_coef_series(lasso1_model,X).sort_values()\nlasso_coef","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.485733Z","iopub.execute_input":"2021-05-30T15:03:22.486383Z","iopub.status.idle":"2021-05-30T15:03:22.498809Z","shell.execute_reply.started":"2021-05-30T15:03:22.486334Z","shell.execute_reply":"2021-05-30T15:03:22.497821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets create Dataframe that stores the features with its corresponding coefficient values","metadata":{}},{"cell_type":"code","source":"lasso_coef_df=pd.DataFrame(\n    data=lasso_coef,\n    columns=[\"Coefficient\"]\n)\nlasso_coef_df","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.500198Z","iopub.execute_input":"2021-05-30T15:03:22.500599Z","iopub.status.idle":"2021-05-30T15:03:22.521669Z","shell.execute_reply.started":"2021-05-30T15:03:22.50051Z","shell.execute_reply":"2021-05-30T15:03:22.520761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets us see how many features have been used in the model and how many have been neglected by the Lasso model","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"The dataframes below shows the features as an indexes whose coefficient has been reduced to zero and are completely neglected by the Lasso model","metadata":{}},{"cell_type":"code","source":"features_used=lasso_coef_df[lasso_coef_df[\"Coefficient\"]==0]\nfeatures_used","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.522907Z","iopub.execute_input":"2021-05-30T15:03:22.523457Z","iopub.status.idle":"2021-05-30T15:03:22.544683Z","shell.execute_reply.started":"2021-05-30T15:03:22.523403Z","shell.execute_reply":"2021-05-30T15:03:22.543545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Lasso model have used 26 features out of 43 and have neglected 17 features","metadata":{}},{"cell_type":"code","source":"print(\"Total Features:{}\".format(X.shape[1]))\nprint(\"Features Neglected:{}\".format(features_used.shape[0]))\nprint(\"Features Used:{}\".format(X.shape[1]-features_used.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.546634Z","iopub.execute_input":"2021-05-30T15:03:22.546954Z","iopub.status.idle":"2021-05-30T15:03:22.5644Z","shell.execute_reply.started":"2021-05-30T15:03:22.546921Z","shell.execute_reply":"2021-05-30T15:03:22.56274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its is very clear from the plot that many of the features are neglected and its bar are being reduced to 0 magnitude, and hence lasso is moslty used for automatic feature selection.","metadata":{}},{"cell_type":"code","source":"plot_coef(lasso_coef)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:03:22.566264Z","iopub.execute_input":"2021-05-30T15:03:22.566805Z","iopub.status.idle":"2021-05-30T15:03:23.077683Z","shell.execute_reply.started":"2021-05-30T15:03:22.566753Z","shell.execute_reply":"2021-05-30T15:03:23.076618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparison between the three Linear Models:**\n\n* From the plot below comparing the coefficient of independent features, its clear that **LinearRegression()** model have most coefficients nonzero and are of large magnitude and most of its values are out of y-lim, Which are represented by blue square blocks.\n\n* The **Ridge()** model however have the coefficents whose magnitude are smaller and are close to zero, which are represented with orange **'^'**.\n\n* Comming to **Lasso()** model most of its values are lying either on the horizotal line and few which are very close to horizontal line, owing to its smaller magnitude represented by green **'v'**","metadata":{}},{"cell_type":"code","source":"#specify the figure & size\nfig=plt.figure(figsize=(12,7))\n\n#plot the coefficient of individual linear models\nplt.plot(linear_model.coef_,'s',label=\"Linear Regression\")\nplt.plot(ridge1_model.coef_,'^',label=\"Ridge\")\nplt.plot(lasso1_model.coef_,'v',label=\"Lasso\")\n\n#specify columns/features as the xticks\nplt.xticks(range(X.shape[1]), X.columns,rotation=90)\n\n#the length of horizontal line equals to the length of features\nplt.hlines(0,0,X.shape[1])\n\n#specify the x & y labels\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient magnitude\")\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:12:07.991264Z","iopub.execute_input":"2021-05-30T15:12:07.991639Z","iopub.status.idle":"2021-05-30T15:12:08.57333Z","shell.execute_reply.started":"2021-05-30T15:12:07.991607Z","shell.execute_reply":"2021-05-30T15:12:08.572208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}