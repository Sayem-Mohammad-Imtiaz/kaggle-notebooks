{"cells":[{"metadata":{},"cell_type":"markdown","source":"The Goal here (pun intended) is to design a prediction system which can accurately predict if the home team will win or not. We will use the final dataset got by our earlier \"Scraping and Cleaning\" Notebook build our prediction model on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary libraries.\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom IPython.display import display\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data and drop redundant column.\ndata = pd.read_csv('../input/football-results-and-betting-odds-data-of-epl/final_dataset.csv')\n\n# Remove first 3 matchweeks\ndata = data[data.MW > 3]\n\ndata.drop(['Unnamed: 0','HomeTeam', 'AwayTeam', 'Date', 'MW', 'HTFormPtsStr', 'ATFormPtsStr', 'FTHG', 'FTAG',\n           'HTGS', 'ATGS', 'HTGC', 'ATGC','HomeTeamLP', 'AwayTeamLP','DiffPts','HTFormPts','ATFormPts',\n           'HM4','HM5','AM4','AM5','HTLossStreak5','ATLossStreak5','HTWinStreak5','ATWinStreak5',\n           'HTWinStreak3','HTLossStreak3','ATWinStreak3','ATLossStreak3'],1, inplace=True)\n\n\n# Preview data.\ndisplay(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total number of students.\nn_matches = data.shape[0]\n\n# Calculate number of features.\nn_features = data.shape[1] - 1\n\n# Calculate matches won by home team.\nn_homewins = len(data[data.FTR == 'H'])\n\n# Calculate win rate for home team.\nwin_rate = (float(n_homewins) / (n_matches)) * 100\n\n# Print the results\nprint(\"Total number of matches: {}\".format(n_matches))\nprint(\"Number of features: {}\".format(n_features))\nprint(\"Number of matches won by home team: {}\".format(n_homewins))\nprint(\"Win rate of home team: {:.2f}%\".format(win_rate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising distribution of data\nfrom pandas.plotting import scatter_matrix\n\n\nscatter_matrix(data[['HTGD','ATGD','HTP','ATP','DiffFormPts','DiffLP']], figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate into feature set and target variable\nX_all = data.drop(['FTR'],1)\ny_all = data['FTR']\n\n# Standardising the data.\nfrom sklearn.preprocessing import scale\n\n\ncols = [['HTGD','ATGD','HTP','ATP','DiffLP']]\nfor col in cols:\n    X_all[col] = scale(X_all[col])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all.HM1 = X_all.HM1.astype('str')\nX_all.HM2 = X_all.HM2.astype('str')\nX_all.HM3 = X_all.HM3.astype('str')\nX_all.AM1 = X_all.AM1.astype('str')\nX_all.AM2 = X_all.AM2.astype('str')\nX_all.AM3 = X_all.AM3.astype('str')\n\ndef preprocess_features(X):\n    ''' Preprocesses the football data and converts catagorical variables into dummy variables. '''\n    \n    # Initialize new output DataFrame\n    output = pd.DataFrame(index = X.index)\n\n    # Investigate each feature column for the data\n    for col, col_data in X.iteritems():\n\n        # If data type is categorical, convert to dummy variables\n        if col_data.dtype == object:\n            col_data = pd.get_dummies(col_data, prefix = col)\n                    \n        # Collect the revised columns\n        output = output.join(col_data)\n    \n    return output\n\nX_all = preprocess_features(X_all)\nprint(\"Processed feature columns ({} total features):\\n{}\".format(len(X_all.columns), list(X_all.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the feature information by printing the first five rows\nprint(\"\\nFeature values:\")\ndisplay(X_all.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Shuffle and split the dataset into training and testing set.\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, \n                                                    test_size = 50,\n                                                    random_state = 2,\n                                                    stratify = y_all)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Evaluating Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time \nfrom sklearn.metrics import f1_score\n\ndef train_classifier(clf, X_train, y_train):\n    ''' Fits a classifier to the training data. '''\n    \n    # Start the clock, train the classifier, then stop the clock\n    start = time()\n    clf.fit(X_train, y_train)\n    end = time()\n    \n    # Print the results\n    print(\"Trained model in {:.4f} seconds\".format(end - start))\n\n    \ndef predict_labels(clf, features, target):\n    ''' Makes predictions using a fit classifier based on F1 score. '''\n    \n    # Start the clock, make predictions, then stop the clock\n    start = time()\n    y_pred = clf.predict(features)\n    \n    end = time()\n    # Print and return results\n    print(\"Made predictions in {:.4f} seconds.\".format(end - start))\n    \n    return f1_score(target, y_pred, pos_label='H'), sum(target == y_pred) / float(len(y_pred))\n\n\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n    ''' Train and predict using a classifer based on F1 score. '''\n    \n    # Indicate the classifier and the training set size\n    print(\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n    \n    # Train the classifier\n    train_classifier(clf, X_train, y_train)\n    \n    # Print the results of prediction for both training and testing\n    f1, acc = predict_labels(clf, X_train, y_train)\n    print(f1, acc)\n    print(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))\n    \n    f1, acc = predict_labels(clf, X_test, y_test)\n    print(\"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Initialize the three models (XGBoost is initialized later)\nclf_A = LogisticRegression(random_state = 42)\nclf_B = SVC(random_state = 912, kernel='rbf')\nclf_C = xgb.XGBClassifier(seed = 82)\n\ntrain_predict(clf_A, X_train, y_train, X_test, y_test)\nprint('')\ntrain_predict(clf_B, X_train, y_train, X_test, y_test)\nprint('')\ntrain_predict(clf_C, X_train, y_train, X_test, y_test)\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clearly XGBoost seems like the best model as it has the highest F1 score and accuracy score on the test set.**"},{"metadata":{},"cell_type":"markdown","source":"# Tuning the parameters of XGBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Import 'GridSearchCV' and 'make_scorer'\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n\n# TODO: Create the parameters list you wish to tune\nparameters = { 'learning_rate' : [0.1],\n               'n_estimators' : [40],\n               'max_depth': [3],\n               'min_child_weight': [3],\n               'gamma':[0.4],\n               'subsample' : [0.8],\n               'colsample_bytree' : [0.8],\n               'scale_pos_weight' : [1],\n               'reg_alpha':[1e-5]\n             }  \n\n# TODO: Initialize the classifier\nclf = xgb.XGBClassifier(seed=2)\n\n# TODO: Make an f1 scoring function using 'make_scorer' \nf1_scorer = make_scorer(f1_score,pos_label='H')\n\n# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\ngrid_obj = GridSearchCV(clf,\n                        scoring=f1_scorer,\n                        param_grid=parameters,\n                        cv=5)\n\n# TODO: Fit the grid search object to the training data and find the optimal parameters\ngrid_obj = grid_obj.fit(X_train,y_train)\n\n# Get the estimator\nclf = grid_obj.best_estimator_\nprint(clf)\n\n# Report the final F1 score for training and testing after parameter tuning\nf1, acc = predict_labels(clf, X_train, y_train)\nprint(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))\n\nf1, acc = predict_labels(clf, X_test, y_test)\nprint(\"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Fitting the model on the whole dataset for future predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\n\n\n# TODO: Create the parameters list you wish to tune\nparameters = { 'learning_rate' : [0.03],\n               'n_estimators' : [20],\n               'max_depth': [5],\n               'min_child_weight': [5],\n               'gamma':[0.2],\n               'subsample':[0.8],\n               'colsample_bytree':[0.8],\n               'scale_pos_weight' : [1],\n               'reg_alpha':[1e-2],\n               'early_stopping_rounds':[10]\n             }  \n\n# TODO: Initialize the classifier\nclf = xgb.XGBClassifier(seed=2)\n\n# TODO: Make an f1 scoring function using 'make_scorer' \nf1_scorer = make_scorer(f1_score,pos_label='H')\n\n# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\ngrid_obj = GridSearchCV(clf,\n                        scoring=f1_scorer,\n                        param_grid=parameters,\n                        cv=5)\n\n# TODO: Fit the grid search object to the training data and find the optimal parameters\ngrid_obj = grid_obj.fit(X_all,y_all)\n\n# Get the estimator\nclf = grid_obj.best_estimator_\nprint(clf)\n\n# Report the final F1 score for training and testing after parameter tuning\nf1, acc = predict_labels(clf, X_train, y_train)\nprint(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}