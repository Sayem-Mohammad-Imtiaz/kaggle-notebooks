{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  All about Accuracy, Recall, Precision, F-Score & Specificity\n## which one to Choose?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Many of us participate in alot of competitions on various platforms and these competitions always comeup with some different *Evaluation Metrics* or *Evaluation Criteria*. Only half of the people check for the evaluation metrics and of them only few are aware of these all Metrics. The main aim of this Notebook is to serve some basic overview about Basic metrics that every Competitor should know about.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With out wasting some more time lets just dive into the topic.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Lets go :**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us suppose that we are dealing with any classification problem and where the outcome is all about whether a patient has Cancer or not.\nThis is a very basic kind of classification problem where we have to answer in Yes or No (Binary Classification).\nNow lets suppose that we have actually created a *Machine Learning Model* how would we decide whether our model is good or bad ?\nFor answering this question we first need to look at all the possible outcomes that our model can predict. Lets have a look at all the Possible outcomes:\n1. A person has Cancer and we predicted it correctly.\n2. A person doesn't have cancer and We predicted it correctly again. But things always don't go in right directions...\n3. A person donot have Cancer and we have predicted that he is suffering from it. This would be bad as the patient would go in depression about a disease that he never really have.\n4. A person have cancer and we have predicted that he is safe and This is the most risky thing for the patient as he would never really go through the treatment as he always trusted upon us where the Model actually broke his trust.\n\nSo these four cases are called as **True Positive(TP), True Negative(TP), False Positive(FP), False Negative(FN)** respectively.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The Last outcome would be the worst possible outcome even if our model comes up with 98% of Accuracy...\nWait! Does it mean that a model with 98% accuracy is Bad? It is not but still failed to diagnose the outcome correctly.'\nThen what is the solution for it? And the Answer for this is **Recall, Precision, F-Score & Specificity**.\nI know the answer is bit of confusing lets firstly understand them and then we can come to the conclusion behind this answer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Precision:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is the ratio of **correctly predicted Positives** to the **All positives**. This simply means the ratio of **True Positives** over all the **predicted true positives** in our Dataset \n\n**Precision=(TP)/(TP+FP)**\n\n**When to use Precision ?**\n\nIt is useful to know How many of those who we labeled as having Cancer did actually have Cancer?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Recall or Sensitivity:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is same as the precision but the only difference is that precision is the count of all True Positives over all True Positive Predictions where as **Recall** is the **count of True Positives** over **all True positives.**\n\n**Recall=(TP)/(TP+FN)**\n\n**When to use Recall ?**\n\nIt is useful to know of all the people who had Cancer, how many of those we correctly predict ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. F-Score or F1-Score or F-Measure:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"F1 Score considers both precision and recall.\nIt is the harmonic mean(average) of the precision and recall.\nF1 Score is best if there is some sort of balance between precision (p) & recall (r) in the system. Oppositely F1 Score isnâ€™t so high if one measure is improved at the expense of the other.\n\n**F-Score=2*(Recall * Precision) / (Recall + Precision)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. Specificity:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is the opposite of what recall does as it is the ratio of correctly predicted Negatives over all Negatives.\n\n**Specificity=TN/(TN+FP)**\n\n**When is Specificity useful ?**\n\nIt is useful to know of all the people who donot have Cancer, how many of those did we correctly predict ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5. Accuracy:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This score is for overall review of our model. It is the ratio of all correct predictions to the all data points.\n\n**Accuracy=(TP+TN)/(TP+FP+FN+TN)**\n\nIt doesn't care about the predictions that went right in any of the specific caegory. Its the the Correct Prediction and These Correct Predictions can be about Whether the Patient had or didnot had Cancer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Now the question is which one should be our choice for evaluation ????\n# And the Answer is Precison because it reduces the wrong predictions about a person not having Cancer as this would be the worst possible case. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thats all about these basic metrcis if you really enjoyed this read please give it an upvote and that would encourage me to write conepts in a Simplified model to serve the Learners\n**Thanks alot for reading**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}