{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nred_wine_path= (\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\nred_wine=pd.read_csv(red_wine_path)\n\nred_wine.head()\n#red_wine.describe()\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine_columns= red_wine.columns\nprint(wine_columns)\n\n#check dataset for missing values\nmissing_values= [i for i in red_wine.columns if red_wine[i].isnull().any()]\n\n# eine leere Liste gibt den Wert false, der wieder negiert wird wodurch die Bedingnug erfüllt würde\nif not missing_values:\n    print(\"Keine fehlenden Werte\")\n    \ncols_with_object= red_wine.dtypes == \"object\"\n\nif not list(cols_with_object[cols_with_object].index): #Notiz zum recherchieren: er nimmt bei cols with object[object] nur die Werte mit true, aber wie genau?\n    print(\"Keine kateogirschen Werte\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#target, features einreichten und Datensatz splitten\nfrom sklearn.model_selection import train_test_split\n\n#features: alles außer quality\nX=red_wine.copy()\ny=X.pop(\"quality\")\n\n#oder:\n#red_wine.head()\n\n#X= red_wine[red_wine.columns[0:11]]\n#y=red_wine[\"quality\"]\n\nX_train, X_valid, y_train, y_valid = train_test_split (X,y, test_size=0.2, random_state=0)\nprint(\"train_test_split done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trainingsdatensatz für das Modell Standartisieren\nfrom sklearn.preprocessing import StandardScaler\n\nsc= StandardScaler()\nX_train= sc.fit_transform(X_train)\nX_valid= sc.transform(X_valid)\n\ninput_shape= [X_train.shape[1]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model definieren\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nregression_model = keras.Sequential([\n    layers.Dense(1500, activation =\"tanh\", input_shape=input_shape), #1500\n    layers.Dense(64, activation =\"tanh\"), #64 mit tanh hat auch gut verbessert um 0.15 /der wert 0.15 erhielt ich bei \"relu\" und 250 units\n    layers.Dense(64, activation =\"relu\"), #64\n    layers.Dense(1) #Notiz: Warum braucht man hier gar keine activaton Function im Vergleich zu Softmax --> Softmax rechnet einen Zahlenvector in Wahrscheinlichkeiten, aber wie funktioniert das?\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model compilieren\n\nregression_model.compile(\n    optimizer=\"adam\", #adam oder rmsprop\n    loss=\"mae\", #mae oder mse\n    #metrics=\"accuracy\" hier nicht brauchbar, da nur für classification\n   metrics=[tf.keras.metrics.RootMeanSquaredError()]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history= regression_model.fit(\n    X_train, y_train,\n    batch_size=64, #105\n    epochs=20, #20\n    #verbose=0, #turn of training log\n    \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5. You can change this to get a different view.\nhistory_df.loc[5:, ['loss']].plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred= regression_model.predict(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(y_pred) == len(y_pred):\n    len_y_pred=len(y_pred)\nelse:\n    print(\"Anzahl der Instanzen, die dem Model zum validieren in X_valid übergeben werden stimmt nicht mit den zu überprüfenden Instanzen in y_valid \")\ny_pred_list=[]\nfor b in range(len_y_pred):\n    \n    y_pred_list.append(y_pred[b][0])\n\n\n#print(y_pred_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_list= y_valid.values.tolist()\n#print(y_valid_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    print(\"Predicted %s ---> Expected %d\"%(y_pred_list[i],y_valid_list[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"Der r2_Score beträgt:\",r2_score(y_valid_list, y_pred_list))\nprint(\"Der r2_Score beträgt:\",r2_score(y_valid, y_pred))\nprint(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nshap.initjs()\n\n# rather than use the whole training set to estimate expected values, we summarize with\n# a set of weighted kmeans, each weighted by the number of points they represent.\nX_train_summary = shap.kmeans(X_train, 10)\n\nexplainer = shap.KernelExplainer(regression_model.predict, X_train_summary)\nshap_values = explainer.shap_values(X_valid)\nshap.summary_plot(shap_values, X_valid)\n\n\nprint(\"DONE\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}