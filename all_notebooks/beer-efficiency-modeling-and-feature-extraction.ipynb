{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))        \n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Beer Efficiency\nI usually like to do EDA with these kernels, however there really isn't much in this data set! As we can see below we only have a few hundred samples and a few features, so I thought it would be interesting to see if we can take this data and wrangle more features that could be used for a machine learning problem. The data set has the following features about beer: name, calories, ABV, and efficiency. Efficiency is the interesting feature, this tell us how effective a beer is at delivering ABV while minimizing calories. We'll use efficiency as the output, everything else is input."},{"metadata":{"trusted":true},"cell_type":"code","source":"# magic numbers bad\npal1 = '#ac4b1c'\nseed = 121285\n\ndf = pd.read_csv('/kaggle/input/beer-efficiency/beer_efficiency.csv')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nOne of the first steps when working on an ML problem is to examine the feature distributions. Normal shaped inputs help a model learn better, if they are distributed in an odd way we can use scalers to fix this. Also if the output is normally distributed, it is often easier to create a model. As we can see below all of the features are relatively normal with a small right skew, we can try and normalize these later but so far there is not anything concerning."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3, figsize=(18,6))\n\nsns.distplot(df['abv'], ax=ax[0], color=pal1, kde=False)\nsns.distplot(df['calories'], ax=ax[1], color=pal1, kde=False)\nsns.distplot(df['efficiency'], ax=ax[2], color=pal1, kde=False)\n\nax[0].set_title('ABV')\nax[1].set_title('Calories')\nax[2].set_title('Efficiency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've determined the features are more or less healthy we can begin determining if any of them are useful, I like to begin this process with a correlation matrix. A correlation matrix tells us how similar all of the features are, do they rise or fall at a similar rate. If an input correlates to the output then that means it will likely be a good predictor, so keep an eye out for values close to 1 or -1.\n\nWe can also use this same plot to determine if any of our inputs are collinear, meaning that we are duplicating our training data. Some models will ignore this, though others will not function at all with them. Below we can see that calories and abv are collinear, it is possible the model could be harmed by including them both! We can also see that calories is more highly correlated than abv to the output, if we need to drop one we have an easy choice."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Feature Engineering\nWith such a limitation on features it would be nice if we could extract something from the name. We have sklearn tools to do some of this, but first I wanted to show an approach using Python on its own. What we need to do is see if there are any commonly repated terms that correlate to the output, so first lets see if there are any terms that repeat."},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\nfrom collections import Counter\n\n# split all strings into lists, for example we now have [['Blue', 'Moon', 'Belgian', 'White'], ... ]\nwords = [x.split() for x in df.name.to_list()]\n# now lets just throw all these sub lists into a single list\nwords = [x for x in chain.from_iterable(words)]\n\n# have counter do the heavy lifting and throw it into a series for easy manipulation\ncount = pd.Series(Counter(words))\ncount = count[count > 5]\n\nfig, ax = plt.subplots(figsize=(5,10))\nsns.barplot(count, count.index, color=pal1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it looks like we have lots of repeated terms that could be useful and related to what our output features is: amber, light, porter, IPA, etc. However we also get all the brand information as well: Blue, Moon, Brooklyn, Bud, etc. If we had another feature or data set we could filter those out, or manually build a list, but for now we're just going to leave it in and see what happens."},{"metadata":{},"cell_type":"markdown","source":"Now that we've looked at this by hand, lets let sklearn handle the extraction with the CountVectorizer. This tool examines tokens (highly recurring words) and creates a matrix with their occurence for each sample. There are a few different ways to use it but I like specifying the number of features, this way I can fine tune the complexity of our new inputs as well as control which words appear. The output is a sparse matrix that we can pretty easily just concatenate to our original dataframe. The vocabulary are the common tokens found (basically what we did above manually)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(max_features=10)\nmat = cv.fit_transform(df.name).toarray()\nprint(cv.vocabulary_)\n\nnewdf = pd.DataFrame(mat, columns=cv.vocabulary_.keys())\ndf = pd.concat([df, newdf], axis=1)\ndf = df.drop('name', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the moment of truth, lets draw another correlation plot and see if any of these inputs have potential. Below you can see that we actually did pretty good, at least one of the new inputs is as good as calories, and the remaining are as good as abv (mostly)!"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should also try alternate scoring just to help sanity check the usefulness of our features, I like to use SelectKBest which uses a p-value to determine the usefulness of an input. This more or less confirms what we discovered with the correlation matrix, two of our features are highly useful while the rest are ok."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_regression\n\n# seperate into input/output features\nX = df.drop('efficiency', axis=1)\ny = df['efficiency']\n\nksel = SelectKBest(k='all', score_func=f_regression)\nksel.fit(X, y)\n\nsns.barplot(x=ksel.scores_, y=X.columns, color=pal1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training / Validation\nWell we have some features, however i'm worried about overfit due to the large number of features compared to the number of samples we have. First lets train and validate to see how a model responds before we start trimming anything. Also note that typically we would do cross validation here to get a more robust idea of model performance and to avoid data leakage, but I am going to keep this problem simplifed."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n\nmodel = LinearRegression()\n\nmodel.fit(X_train, y_train)\nprint(model.score(X_train, y_train))\nprint(model.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just what I was afraid of, over fit! There is not any strict value to determine this, but I typically use a difference of at least 10 between training and testing scores. This means that our model has memorized the training and is not doing as well on its test. Lets check a residual plot and see if we can get any extra information about this."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot(model.predict(X_test), y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like our model actually does pretty good in most cases, when we get to higher efficiency beers the model really falls apart. There is a slight 'shape' to the graph so it is likely that a polynomial model might help. There is also the possibility that the 'outlier' beer around (100, -20) is throwing our model off, we could examine that sample and see if we feel it is important to keep.\n\nIn addition to training / testing scores and the residual plot it is also good to look at the error a model generates, a good start to this is MAE and MSE. Mean Absolute Error (MAE) is on average how much error the model generates, as you can see below our model is typically only 2.38 effeciency off when making predictions. This should make sense, if we look at our residual plot above you can see most of the residuals are within the range 5 to -5. However whats up with our Mean Squared Error (MSE), its way higher at nearly 21! MSE is the error squared, the effect of this is that larger errors have a larger contribution to the result. So we can say that our model does well on average, but in some caes it really fails, again we can look at the residual plot to confirm these findings."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nprint(mean_squared_error(model.predict(X_test), y_test))\nprint(mean_absolute_error(model.predict(X_test), y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Be sure to look at these values with respect to the output feature, you might be initially excited to get an MSE of 2.38, though if the range of your output is 0.1 to 0.5 then you are in some serious trouble. Another metric we can use which ignores units is Mean Absolute Percentage Error (MAPE), this metric is simply a measure of prediction accuracy. We have to be careful when using this metric since we divide by the test output, if any of these values are 0 then we obviously have a problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"mape = 100 - (np.mean(np.abs(y_test - model.predict(X_test)) / y_test) * 100)\nprint(mape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next I would like to try and trim features that are not contributing using SelectKBest, however it can be difficult to pick a good value for 'k' (how many features to keep) so we can just collect all possible values and graph the results. Here we can see that at around 7 features the model is not getting any better."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores = []\ntest_scores = []\n\n# iterate through all possible features counts, 1 to keep all features\nfor i in range(1, X.shape[1]):\n    ksel = SelectKBest(k=i, score_func=f_regression)\n    X_ = ksel.fit_transform(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    train_scores.append(model.score(X_train, y_train))\n    test_scores.append(model.score(X_test, y_test))\n\nplt.plot(range(1, X.shape[1]), train_scores)\nplt.plot(range(1, X.shape[1]), test_scores)\nplt.xlabel('# Features Kept')\nplt.ylabel('Score')\nplt.legend(['train', 'test'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the issues with this data set is the small number of samples, we can use a learning curve to see how much better the model could potentially be. What we're looking for is a trend with the increase in training samples. As you can see below it does appear that the testing score could be improved with an increase in samples, this should not be surprising with the very small number that we started with."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\nsizes, t_scores, v_scores = learning_curve(LinearRegression(), X, y, train_sizes=[0.2, 0.4, 0.6, 0.8, 1.0])\n\nt_scores_mean = np.mean(t_scores, axis=1)\nv_scores_mean = np.mean(v_scores, axis=1)\n\nfig, ax = plt.subplots()\nplt.plot(sizes, t_scores_mean, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(sizes, v_scores_mean, 'o-', color=\"g\",label=\"Test score\")\nax.set(xlabel='Training Samples', ylabel='Score')\nax.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Moving On\nThere is a lot more we could do here: different models, tuning hyper parameters, validation curves, cluster analysis, polynomial models, etc.. but I think this kernel has already gotten long enough for what I initially set out to do, exploring feature extraction with some basic validation. I've learned a lot from this cumminity so I hope this brings something to the table for somebody else. If anybody has questions, concerns, suggestions I would love to hear them!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}