{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Linear Regression with Regularisation","metadata":{}},{"cell_type":"markdown","source":"Task:\n\n1-Return to Question 15 at the end of Chapter 3 of the textbook ISLR (James et al, 2014). Complete part (b) of this question:\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 :βj =0?\n\n2-Now repeat this using each of the following regularisation approaches:\n• ridge regression (l2)\n• lasso (l1)\n• elastic net (l1 and l2).\n\n3-Now do all of this on a publically available dataset with one output variable and at least 20 predictors (input variables). Explain your choice of dataset\n\n** Where appropriate, use k-fold cross-validation (splitting into training and validation sets k times) to estimate the model quality.","metadata":{}},{"cell_type":"markdown","source":"### Answer Structure","metadata":{}},{"cell_type":"markdown","source":"1. Data Exploration\n\n    1.1 Check missing value\n    1.2 Correlation analysis\n    1.3 Identify outliers\n2. Data preparation\n\n    2.1 Remove outliers\n    2.2 Scaled the dataset\n3. Model fitting\n\n    3.1 OLS\n    3.2 Ridge\n    3.3 Lasso\n    3.4 ElasticNet\n    \n4. Try the whole process in new dataset (repeat 1-3 with the energy dataset)\n\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Data Exploration","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom scipy import stats\nfrom sklearn.model_selection import KFold, GridSearchCV, RepeatedKFold, RandomizedSearchCV, learning_curve, cross_val_score\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:36.404256Z","iopub.execute_input":"2021-07-16T18:18:36.404701Z","iopub.status.idle":"2021-07-16T18:18:38.074474Z","shell.execute_reply.started":"2021-07-16T18:18:36.404658Z","shell.execute_reply":"2021-07-16T18:18:38.073555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\nboston = pd.read_csv(\"../input/boston/Boston.csv\")\nboston.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:38.076655Z","iopub.execute_input":"2021-07-16T18:18:38.07692Z","iopub.status.idle":"2021-07-16T18:18:38.18231Z","shell.execute_reply.started":"2021-07-16T18:18:38.076875Z","shell.execute_reply":"2021-07-16T18:18:38.181481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1 Check missing value","metadata":{}},{"cell_type":"code","source":"# 1.1 Check missing value\nboston.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:38.183834Z","iopub.execute_input":"2021-07-16T18:18:38.184117Z","iopub.status.idle":"2021-07-16T18:18:38.192595Z","shell.execute_reply.started":"2021-07-16T18:18:38.184066Z","shell.execute_reply":"2021-07-16T18:18:38.191607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 Check linearity (correlation analysis)","metadata":{}},{"cell_type":"code","source":"# 1.2 Correlation analysis\nsns.set(rc={'figure.figsize':(15,12)})\nboston_cor_matrix = boston.corr().round(2)\nsns.heatmap(data=boston_cor_matrix, annot=True,cmap=\"vlag\")\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:38.196585Z","iopub.execute_input":"2021-07-16T18:18:38.196882Z","iopub.status.idle":"2021-07-16T18:18:39.537758Z","shell.execute_reply.started":"2021-07-16T18:18:38.196823Z","shell.execute_reply":"2021-07-16T18:18:39.536947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_matrix = pd.plotting.scatter_matrix(boston, figsize=(25,25))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:39.541664Z","iopub.execute_input":"2021-07-16T18:18:39.541975Z","iopub.status.idle":"2021-07-16T18:18:52.288765Z","shell.execute_reply.started":"2021-07-16T18:18:39.541917Z","shell.execute_reply":"2021-07-16T18:18:52.287581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3 Check outliers\n\nHere we will use Z-score to identify outliers. \nWhile calculating the Z-score we need to rescale and center the data and look for data points which are too faraway from zero. \nA data point will be indentified as an outlier if the absolute value of Z-score is greater than 3.\n\nReference: https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n","metadata":{}},{"cell_type":"code","source":"z = np.abs(stats.zscore(boston))\nx = np.where(z>3)\n\ni=np.where(x[1]==3)\nlen(i[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.292048Z","iopub.execute_input":"2021-07-16T18:18:52.292413Z","iopub.status.idle":"2021-07-16T18:18:52.301893Z","shell.execute_reply.started":"2021-07-16T18:18:52.292354Z","shell.execute_reply":"2021-07-16T18:18:52.30095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# boston[boston[\"chas\"]>0].shape\n# there are 35 records that with chas=1","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.304058Z","iopub.execute_input":"2021-07-16T18:18:52.304454Z","iopub.status.idle":"2021-07-16T18:18:52.310408Z","shell.execute_reply.started":"2021-07-16T18:18:52.304396Z","shell.execute_reply":"2021-07-16T18:18:52.309262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, the Z-score method cannot be applied for Boolean variables, it will treat all the minority records as outliers. For example, chas is a Boolean variable in the Boston dataset. Using the Z-score method, all of the records with 1 value are treated as outliers since only 7% (35/505) of the records with chas equals 1. Therefore, we excluded “chas” from the outlier processing.","metadata":{}},{"cell_type":"markdown","source":"### 2. Data Preparation","metadata":{}},{"cell_type":"markdown","source":"#### 2.1 Remove outliers","metadata":{}},{"cell_type":"code","source":"# Let's build a function for it\ndef outliers_del(dataset, threshold):\n    z = np.abs(stats.zscore(dataset))\n    dataset_no_ol = dataset[(z<threshold).all(axis=1)]\n    return dataset_no_ol","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.311911Z","iopub.execute_input":"2021-07-16T18:18:52.312281Z","iopub.status.idle":"2021-07-16T18:18:52.321577Z","shell.execute_reply.started":"2021-07-16T18:18:52.312227Z","shell.execute_reply":"2021-07-16T18:18:52.320621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boston_no_ol = outliers_del(boston, 3)\n# add the 35 records that has a \"chad\" value of 1 \nboston_no_ol = pd.concat([boston_no_ol, boston[boston[\"chas\"]>0]], ignore_index=True)\nboston_no_ol.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.323411Z","iopub.execute_input":"2021-07-16T18:18:52.323718Z","iopub.status.idle":"2021-07-16T18:18:52.341141Z","shell.execute_reply.started":"2021-07-16T18:18:52.323669Z","shell.execute_reply":"2021-07-16T18:18:52.340459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boston.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.342622Z","iopub.execute_input":"2021-07-16T18:18:52.342906Z","iopub.status.idle":"2021-07-16T18:18:52.348632Z","shell.execute_reply.started":"2021-07-16T18:18:52.342862Z","shell.execute_reply":"2021-07-16T18:18:52.347662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remove 56 records/outliers from the boston dataset.","metadata":{}},{"cell_type":"markdown","source":"#### 2.2 Scale the dataset\nAs mentioned in the ISLR book, it is best to apply regularisation after standardizing the predictors. It won't change the scores.","metadata":{}},{"cell_type":"code","source":"# scaled the whole dataset for better comparison between dataset while using MSE\ndef get_data(dataset, target):\n    dataset = dataset\n    X = dataset.drop(columns=[target])\n    y = dataset[target]\n    sc = StandardScaler()\n    dataset_scaled = sc.fit_transform(dataset)\n    dataset_scaled = pd.DataFrame(dataset_scaled, columns=dataset.columns)\n    X_scaled = dataset_scaled.drop(columns=[target])\n    y_scaled = dataset_scaled[target]\n    return X_scaled, y_scaled","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.350348Z","iopub.execute_input":"2021-07-16T18:18:52.350595Z","iopub.status.idle":"2021-07-16T18:18:52.358152Z","shell.execute_reply.started":"2021-07-16T18:18:52.350548Z","shell.execute_reply":"2021-07-16T18:18:52.357441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Model fitting","metadata":{}},{"cell_type":"markdown","source":"#### build the stats result function","metadata":{}},{"cell_type":"markdown","source":"the functions can be used for each model and pass parameters (if any).\nReference:  https://www.kaggle.com/suugaku/islr-lab-2-python","metadata":{}},{"cell_type":"code","source":"def stats_result(model_input, X_input, y_input):\n\n    model = model_input\n    X = X_input\n    y = y_input\n    model.fit(X, y)\n    \n    # Store the coefficients (regression intercept and coefficients) and predictions\n    coefficients = np.append(model.intercept_, model.coef_)\n    predictions = model.predict(X)\n    \n    # Create matrix with shape (num_samples, num_features + 1)\n    # Where the first column is all ones and then there is one column for the values of each feature or predictor\n    X_mat = np.append(np.ones((X.shape[0], 1)), X, axis = 1)\n    \n    # Compute residual sum of squares\n    RSS = np.sum((y - predictions)**2)\n    \n    # Compute total sum of squares\n    TSS = np.sum((np.mean(y) - y)**2)\n    \n    # Mean squared error \n    MSE = RSS / X_mat.shape[0]\n    \n    # Estimate the variance of the y-values\n    obs_var = RSS/(X_mat.shape[0] - X_mat.shape[1]) \n    # Variances of the parameter estimates are on the diagonal of the variance-covariance matrix of the parameter estimates\n    var_beta = obs_var*(np.linalg.inv(np.matmul(X_mat.T, X_mat)).diagonal())\n    # Standard error is square root of variance\n    se_beta = np.sqrt(var_beta)\n    \n    # t-statistic for beta_i is beta_i/se_i where se_i is the standard error for beta_i\n    t_stats_beta = coefficients/se_beta\n    \n    # Compute p-values for each parameter using a t-distribution with (num_samples - 1) degrees of freedom\n    beta_p_values = [2 * (1 - stats.t.cdf(np.abs(t_i), X_mat.shape[0] - 1))\n                    for t_i in t_stats_beta]\n    \n    # Construct dataframe for the overall model statistics:\n    \n    # MSE, R^2\n    model_scores = pd.Series({\"MSE\": MSE, \"R-squared\": model.score(X, y)})\n    \n    # Construct dataframe for parameter statistics:\n    # coefficients, standard errors, t-statistic, p-values for t-statistics\n    xlabels = X.columns.insert(0, \"Intercept\")\n    coef_stats = pd.DataFrame({\"Coefficient\": coefficients, \"Standard Error\": se_beta,\n                                \"t-value\": t_stats_beta, \"Prob(>|t|)\": beta_p_values}, index=xlabels)\n    return {\"model\": model, \"coef_stats\": coef_stats, \"scores\": model_scores}","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.359678Z","iopub.execute_input":"2021-07-16T18:18:52.35994Z","iopub.status.idle":"2021-07-16T18:18:52.376005Z","shell.execute_reply.started":"2021-07-16T18:18:52.359889Z","shell.execute_reply":"2021-07-16T18:18:52.375071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_stats_result(stats_result):\n    print(stats_result[\"model\"])\n    print(\"{:=^60}\".format(\"Score\"))\n    print(np.round(stats_result[\"scores\"], 4))\n    print(\"{:=^60}\".format(\"Coefficients Statistics\"))\n    print(np.round(stats_result[\"coef_stats\"], 4))\n    print(\"{:=^60}\".format(\"Predictors We Can Reject (P < 0.05)\"))\n    coef_stats = stats_result[\"coef_stats\"]\n    print(np.round(coef_stats[coef_stats[\"Prob(>|t|)\"] <= 0.05], 4))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.377422Z","iopub.execute_input":"2021-07-16T18:18:52.377672Z","iopub.status.idle":"2021-07-16T18:18:52.390279Z","shell.execute_reply.started":"2021-07-16T18:18:52.377631Z","shell.execute_reply":"2021-07-16T18:18:52.389323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1 OLS","metadata":{}},{"cell_type":"code","source":"# OLS linear regression model\nX_boston_org, y_boston_org = get_data(boston, \"crim\")\nlr = LinearRegression()\nlr_result = stats_result(lr, X_boston_org, y_boston_org)\nprint_stats_result(lr_result)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.391799Z","iopub.execute_input":"2021-07-16T18:18:52.392273Z","iopub.status.idle":"2021-07-16T18:18:52.496812Z","shell.execute_reply.started":"2021-07-16T18:18:52.392227Z","shell.execute_reply":"2021-07-16T18:18:52.495852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_boston, y_boston = get_data(boston_no_ol, \"crim\")\nlr_result_no_ol = stats_result(lr, X_boston, y_boston)\nprint_stats_result(lr_result_no_ol)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.49873Z","iopub.execute_input":"2021-07-16T18:18:52.499077Z","iopub.status.idle":"2021-07-16T18:18:52.537488Z","shell.execute_reply.started":"2021-07-16T18:18:52.499013Z","shell.execute_reply":"2021-07-16T18:18:52.53652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above 2 results, we can see that after remove the outliers, the result for both R^2 and MSE are improved. \n\nTherefore, in the following steps, we will use the outlier removed and scaled dataset.","metadata":{}},{"cell_type":"markdown","source":"#### Build a para-tuning function for regularisations","metadata":{}},{"cell_type":"code","source":"def param_tuning(model, X, y, params, n):\n    '''use grid search and K-Fold cross validation to find the best parameters for the regularisation models\n    where n is the number of folds'''\n    cv = RepeatedKFold(n_splits=n, n_repeats=3, random_state=1)\n#     cv = KFold(n_splits=n, shuffle=True)\n    gs_r2 = GridSearchCV(model,\n                      params,\n                      scoring=\"r2\",\n                      cv=cv,\n                      n_jobs=-1,\n                      return_train_score=True)\n    \n    gs_mse = GridSearchCV(model,\n                      params,\n                      scoring=\"neg_mean_squared_error\",\n                      cv=cv,\n                      n_jobs=-1,\n                      return_train_score=True)\n    results_r2 = gs_r2.fit(X, y)\n    results_mse = gs_mse.fit(X, y)\n    return results_r2, results_mse\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.539501Z","iopub.execute_input":"2021-07-16T18:18:52.539779Z","iopub.status.idle":"2021-07-16T18:18:52.548167Z","shell.execute_reply.started":"2021-07-16T18:18:52.539729Z","shell.execute_reply":"2021-07-16T18:18:52.5472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def result_plot(alphas, results):\n    print(\"%s: %.3f\" % (results.scoring, results.best_score_))\n    print(\"best config: %s\" % results.best_params_)\n    train_scores_mean = results.cv_results_[\"mean_train_score\"]\n    test_scores_mean = results.cv_results_[\"mean_test_score\"]\n    plt.figure(figsize=(8, 6))\n    plt.title(\"LR with %s\" %(results.estimator.__class__.__name__))\n    plt.xlabel('$\\\\alpha$ (alpha)')\n    plt.ylabel(results.scoring)\n    # plot train scores\n    plt.plot(alphas, train_scores_mean, label='Mean Train score', color=\"r\", linewidth=2.0)\n    plt.plot(alphas, test_scores_mean, label='Mean Test score', color=\"b\", linewidth=2.0)\n    plt.legend(loc='best')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.549683Z","iopub.execute_input":"2021-07-16T18:18:52.549911Z","iopub.status.idle":"2021-07-16T18:18:52.562175Z","shell.execute_reply.started":"2021-07-16T18:18:52.549878Z","shell.execute_reply":"2021-07-16T18:18:52.561085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2 Ridge Regression","metadata":{}},{"cell_type":"code","source":"# tuning ridge model\nridge = Ridge()\nalphas = np.arange(0, 20, 0.1)\nparams = {'alpha': alphas}\nresult_r2, result_mse = param_tuning(ridge, X_boston, y_boston, params, n=5)\nresult_plot(alphas, result_r2)\nresult_plot(alphas, result_mse)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:18:52.563698Z","iopub.execute_input":"2021-07-16T18:18:52.564061Z","iopub.status.idle":"2021-07-16T18:19:10.61684Z","shell.execute_reply.started":"2021-07-16T18:18:52.56391Z","shell.execute_reply":"2021-07-16T18:19:10.615786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# according to tunning result of MSE score, the best parameter is when alpha = 3.5\nridge_tunned = Ridge(alpha=3.5)\nridge_result = stats_result(ridge_tunned, X_boston, y_boston)\nprint_stats_result(ridge_result)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:10.618528Z","iopub.execute_input":"2021-07-16T18:19:10.618776Z","iopub.status.idle":"2021-07-16T18:19:10.653638Z","shell.execute_reply.started":"2021-07-16T18:19:10.618732Z","shell.execute_reply":"2021-07-16T18:19:10.652572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3 Lasso","metadata":{}},{"cell_type":"code","source":"# tuning lasso\nlasso = Lasso()\nalphas = np.arange(0, 2, 0.01)\nparams = {'alpha': alphas}\nresult_r2, result_mse = param_tuning(lasso, X_boston, y_boston, params, n=5)\nresult_plot(alphas, result_r2)\nresult_plot(alphas, result_mse)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:10.655037Z","iopub.execute_input":"2021-07-16T18:19:10.655293Z","iopub.status.idle":"2021-07-16T18:19:25.98074Z","shell.execute_reply.started":"2021-07-16T18:19:10.65525Z","shell.execute_reply":"2021-07-16T18:19:25.979713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#according to tunning result of MSE score, the best parameter is when alpha = 0.011\nlasso = Lasso(alpha=0.01)\nlasso_result = stats_result(lasso, X_boston, y_boston)\nprint_stats_result(lasso_result)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:25.983492Z","iopub.execute_input":"2021-07-16T18:19:25.983883Z","iopub.status.idle":"2021-07-16T18:19:26.016139Z","shell.execute_reply.started":"2021-07-16T18:19:25.983815Z","shell.execute_reply":"2021-07-16T18:19:26.015387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuning elastic net \nelasticnet = ElasticNet()\nalphas = np.arange(0, 2, 0.1)\nl1_ratio = np.arange(0, 1, 0.1)\nparams = {'alpha': alphas, 'l1_ratio':l1_ratio}\nresult_r2, result_mse = param_tuning(elasticnet, X_boston, y_boston, params, n=5)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:26.018308Z","iopub.execute_input":"2021-07-16T18:19:26.01867Z","iopub.status.idle":"2021-07-16T18:19:42.185922Z","shell.execute_reply.started":"2021-07-16T18:19:26.018617Z","shell.execute_reply":"2021-07-16T18:19:42.185261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"%s: %.3f\" % (result_r2.scoring, result_r2.best_score_))\nprint(\"best config: %s\" % result_r2.best_params_)\nprint(\"%s: %.3f\" % (result_mse.scoring, result_mse.best_score_))\nprint(\"best config: %s\" % result_mse.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:42.187034Z","iopub.execute_input":"2021-07-16T18:19:42.187395Z","iopub.status.idle":"2021-07-16T18:19:42.193616Z","shell.execute_reply.started":"2021-07-16T18:19:42.187358Z","shell.execute_reply":"2021-07-16T18:19:42.192935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# according to tunning result of r^2 score, the best parameter is when both alpha and l1 ratio are 0\nelasticnet = ElasticNet(alpha=0, l1_ratio=0)\nelasticnet_result = stats_result(elasticnet, X_boston, y_boston)\nprint_stats_result(elasticnet_result)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:42.194635Z","iopub.execute_input":"2021-07-16T18:19:42.195052Z","iopub.status.idle":"2021-07-16T18:19:42.247005Z","shell.execute_reply.started":"2021-07-16T18:19:42.195006Z","shell.execute_reply":"2021-07-16T18:19:42.246021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Try the model on new dataset","metadata":{}},{"cell_type":"code","source":"energy = pd.read_csv(\"../input/appliances-energy-prediction/KAG_energydata_complete.csv\")\nenergy = energy.drop(columns=[\"date\", \"rv1\", \"rv2\"])\nenergy.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:42.248429Z","iopub.execute_input":"2021-07-16T18:19:42.24871Z","iopub.status.idle":"2021-07-16T18:19:42.673817Z","shell.execute_reply.started":"2021-07-16T18:19:42.248659Z","shell.execute_reply":"2021-07-16T18:19:42.672952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check missing value\nenergy.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:42.674975Z","iopub.execute_input":"2021-07-16T18:19:42.675224Z","iopub.status.idle":"2021-07-16T18:19:42.683841Z","shell.execute_reply.started":"2021-07-16T18:19:42.67517Z","shell.execute_reply":"2021-07-16T18:19:42.68275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation analysis\nsns.set(rc={'figure.figsize':(15,12)})\nenergy_cor_matrix = energy.corr().round(2)\nsns.heatmap(data=energy_cor_matrix, annot=True,cmap=\"vlag\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:42.685069Z","iopub.execute_input":"2021-07-16T18:19:42.685482Z","iopub.status.idle":"2021-07-16T18:19:45.987521Z","shell.execute_reply.started":"2021-07-16T18:19:42.685419Z","shell.execute_reply":"2021-07-16T18:19:45.986365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like the correlations between \"Appliances\" and other indicators are not very significant","metadata":{}},{"cell_type":"code","source":"# scatter_matrix = pd.plotting.scatter_matrix(energy, figsize=(25,25))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:45.989307Z","iopub.execute_input":"2021-07-16T18:19:45.989642Z","iopub.status.idle":"2021-07-16T18:19:45.993493Z","shell.execute_reply.started":"2021-07-16T18:19:45.989591Z","shell.execute_reply":"2021-07-16T18:19:45.99285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove outliers\nenergy_no_ol = outliers_del(energy, 3)\nenergy_no_ol.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:45.994686Z","iopub.execute_input":"2021-07-16T18:19:45.994918Z","iopub.status.idle":"2021-07-16T18:19:46.044426Z","shell.execute_reply.started":"2021-07-16T18:19:45.994884Z","shell.execute_reply":"2021-07-16T18:19:46.043415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"energy.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:46.045665Z","iopub.execute_input":"2021-07-16T18:19:46.04606Z","iopub.status.idle":"2021-07-16T18:19:46.050959Z","shell.execute_reply.started":"2021-07-16T18:19:46.04587Z","shell.execute_reply":"2021-07-16T18:19:46.050139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remove 2391 rows of outliers.","metadata":{}},{"cell_type":"code","source":"# Scaled the dataset\nX_energy, y_energy = get_data(energy_no_ol, \"Appliances\")\nX_energy_org, y_energy_org = get_data(energy, \"Appliances\")","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:46.052703Z","iopub.execute_input":"2021-07-16T18:19:46.053076Z","iopub.status.idle":"2021-07-16T18:19:46.100004Z","shell.execute_reply.started":"2021-07-16T18:19:46.052941Z","shell.execute_reply":"2021-07-16T18:19:46.099153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OLS with outliers\nenergy_lr_result = stats_result(lr, X_energy_org, y_energy_org)\nprint_stats_result(energy_lr_result)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:46.103943Z","iopub.execute_input":"2021-07-16T18:19:46.104365Z","iopub.status.idle":"2021-07-16T18:19:46.169325Z","shell.execute_reply.started":"2021-07-16T18:19:46.10432Z","shell.execute_reply":"2021-07-16T18:19:46.168392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OLS (without outlier)\nenergy_lr_result = stats_result(lr, X_energy, y_energy)\nprint_stats_result(energy_lr_result)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:46.170659Z","iopub.execute_input":"2021-07-16T18:19:46.170876Z","iopub.status.idle":"2021-07-16T18:19:46.215057Z","shell.execute_reply.started":"2021-07-16T18:19:46.170843Z","shell.execute_reply":"2021-07-16T18:19:46.214074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuning ridge model\nalphas = np.arange(0, 20, 0.1)\nparams = {'alpha': alphas}\nresult_r2, result_mse = param_tuning(ridge, X_energy, y_energy, params, n=5)\nresult_plot(alphas, result_r2)\nresult_plot(alphas, result_mse)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:19:46.21669Z","iopub.execute_input":"2021-07-16T18:19:46.217042Z","iopub.status.idle":"2021-07-16T18:20:21.350472Z","shell.execute_reply.started":"2021-07-16T18:19:46.216975Z","shell.execute_reply":"2021-07-16T18:20:21.349746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# according to tunning result of MSE score, the best parameter is when alpha = 8.5\nridge_tunned_energy = Ridge(alpha=8.7)\nridge_result = stats_result(ridge_tunned_energy, X_energy, y_energy)\nprint_stats_result(ridge_result)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:20:21.351537Z","iopub.execute_input":"2021-07-16T18:20:21.35188Z","iopub.status.idle":"2021-07-16T18:20:21.394579Z","shell.execute_reply.started":"2021-07-16T18:20:21.351836Z","shell.execute_reply":"2021-07-16T18:20:21.393806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuning lasso model\nalphas = np.arange(0, 2, 0.1)\nparams = {'alpha': alphas}\nresult_r2, result_mse = param_tuning(lasso, X_energy, y_energy, params, n=5)\nresult_plot(alphas, result_r2)\nresult_plot(alphas, result_mse)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:20:21.395647Z","iopub.execute_input":"2021-07-16T18:20:21.396013Z","iopub.status.idle":"2021-07-16T18:20:33.394875Z","shell.execute_reply.started":"2021-07-16T18:20:21.395974Z","shell.execute_reply":"2021-07-16T18:20:33.39417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# according to tunning result of r^2 score, the best parameter is when alpha = 0\nlasso_tunned_energy = Lasso(alpha=0)\nlasso_result_energy = stats_result(lasso_tunned_energy, X_energy, y_energy)\nprint_stats_result(lasso_result_energy)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:20:33.396397Z","iopub.execute_input":"2021-07-16T18:20:33.396976Z","iopub.status.idle":"2021-07-16T18:20:33.923034Z","shell.execute_reply.started":"2021-07-16T18:20:33.396905Z","shell.execute_reply":"2021-07-16T18:20:33.922175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuning elastic net \nalphas = np.arange(0, 2, 0.1)\nl1_ratio = np.arange(0, 1, 0.1)\nparams = {'alpha': alphas, 'l1_ratio':l1_ratio}\nresult_r2, result_mse = param_tuning(elasticnet, X_energy, y_energy, params, n=5)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:20:33.924383Z","iopub.execute_input":"2021-07-16T18:20:33.924615Z","iopub.status.idle":"2021-07-16T18:24:28.435463Z","shell.execute_reply.started":"2021-07-16T18:20:33.924574Z","shell.execute_reply":"2021-07-16T18:24:28.4347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"%s: %.3f\" % (result_r2.scoring, result_r2.best_score_))\nprint(\"best config: %s\" % result_r2.best_params_)\nprint(\"%s: %.3f\" % (result_mse.scoring, result_mse.best_score_))\nprint(\"best config: %s\" % result_mse.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:24:28.436793Z","iopub.execute_input":"2021-07-16T18:24:28.437065Z","iopub.status.idle":"2021-07-16T18:24:28.44375Z","shell.execute_reply.started":"2021-07-16T18:24:28.437015Z","shell.execute_reply":"2021-07-16T18:24:28.442548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elasticnet_tunned_energy = ElasticNet(alpha=0, l1_ratio=0)\nelasticnet_result_energy = stats_result(elasticnet_tunned_energy, X_energy, y_energy)\nprint_stats_result(elasticnet_result_energy)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:24:28.445181Z","iopub.execute_input":"2021-07-16T18:24:28.445506Z","iopub.status.idle":"2021-07-16T18:24:28.977723Z","shell.execute_reply.started":"2021-07-16T18:24:28.44544Z","shell.execute_reply":"2021-07-16T18:24:28.97698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, the model with regularisation perform worse than OLS.","metadata":{}}]}