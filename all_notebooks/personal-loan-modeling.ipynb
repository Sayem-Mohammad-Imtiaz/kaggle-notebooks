{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading file...**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/personal-loan-modeling/Bank_Personal_Loan_Modelling.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above info shows that some of the categorical feature treated as integer, we need to change type to category"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**dropping ID and ZIP code column as it's not relevant for our analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['ID', 'ZIP Code'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dividing the columns in the dataset in to numeric and categorical attributes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = set(df.columns)\ncols_numeric = set(['Age', 'Experience', 'Income', 'CCAvg', 'Mortgage'])\ncols_categorical = list(cols - cols_numeric)\ncols_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in cols_categorical:\n    df[x] = df[x].astype('category')\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have category type for categorical feature variable"},{"metadata":{},"cell_type":"markdown","source":"# Plotting graph for analysis different columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Creating function to show Density distribution for non-category column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def summary_non_category(x):\n    x_min = df[x].min()\n    x_max = df[x].max()\n    Q1 = df[x].quantile(0.25)\n    Q2 = df[x].quantile(0.50)\n    Q3 = df[x].quantile(0.75)\n    print(f'Summary of {x.capitalize()} Attribute:\\n'\n          f'{x.capitalize()}(min) : {x_min}\\n'\n          f'Q1                    : {Q1}\\n'\n          f'Q2(Median)            : {Q2}\\n'\n          f'Q3                    : {Q3}\\n'\n          f'{x.capitalize()}(max) : {x_max}')\n# Plotting Graph\n    sns.distplot(df[x])\n    plt.title(f'{x.capitalize()} Density Distribution')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in cols_numeric:\n    summary_non_category(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****From above plot Income,Ccavg,Mortgage doesn't seeems good,need to alter the outlier,which we will do in later process***"},{"metadata":{},"cell_type":"markdown","source":"****Now Creating function to show Density distribution for category column****"},{"metadata":{"trusted":true},"cell_type":"code","source":"def summary_category(category_column):\n    count_category= []\n    value_category = []\n    category_loan = []\n    category_no_loan =[]\n    category = df[category_column].unique()\n    for x in category:\n        value_category.append(x)\n        count_category.append(df[category_column][df[category_column] ==x].count())\n    value_category = np.array(value_category)  \n    for x in np.nditer(value_category):\n        category_loan.append(df[category_column][df[category_column]==x][df[\"Personal Loan\"] ==1].count())\n        category_no_loan.append(df[category_column][df[category_column]==x][df[\"Personal Loan\"] ==0].count())\n# Plotting Graph\n    fig, (ax1,ax2) = plt.subplots(1,2)\n    ax1.pie(count_category,labels=value_category, autopct='%1.1f%%')\n    ax2.bar(value_category-0.2,category_loan, width=0.4, label=\"Loan\")\n    ax2.bar(value_category+0.2,category_no_loan, width=0.4,label=\"No Loan\")\n    plt.title(category_column)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category_column in cols_categorical:\n    summary_category(category_column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****The distribution of 'CD Account' and 'Securities Account' are not in proper ratio.Ideally we should have more data for these columns for good analysis****"},{"metadata":{},"cell_type":"markdown","source":"# Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Personal Loan', axis = 1)\ny = df['Personal Loan']\ndata_num = df.select_dtypes(include='number')\nsns.pairplot(X ,diag_kind = 'kde', vars = list(data_num.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Personal Loan', axis = 1)\ny = df['Personal Loan']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# thanks to Anirban Datta\ncorr = X.corr()\nplt.figure(figsize=(10, 8))\ng = sns.heatmap(corr, annot=True, cmap = 'summer_r', square=True, linewidth=1, cbar_kws={'fraction' : 0.02})\ng.set_yticklabels(g.get_yticklabels(), rotation=0, horizontalalignment='right')\nbottom, top = g.get_ylim()\ng.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see there is a very strong positive correlation between Age and Work Experience, which is expected. There is also a positive correlation between Income and Credit Card spending."},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Experience.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see some negative value, let's count it"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Experience\"][df[\"Experience\"]<0].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Experience\"][df[\"Experience\"]>=0].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that count is very less for negative experience as compare to positive,so we gonna drop negative value as experience negative value doesn't make sense to me"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df[df[\"Experience\"]>=0]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see earlier Mortgage,income and CCavg contains outlier"},{"metadata":{},"cell_type":"markdown","source":"Let's look form mortgage first"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2[\"Mortgage\"][df2[\"Mortgage\"]==0].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the count of not having Mortagage is very large, we might think to treat mortgage as category variable. Let's see whether it will be a good choice or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"mortgage = {}\nmortgage[\"Personal_loan_and_no_mortagage\"]    = df2[\"Mortgage\"][df2[\"Mortgage\"]==0][df2[\"Personal Loan\"]==1].count()\nmortgage[\"no_Personal_loan_and_no_mortagage\"] = df2[\"Mortgage\"][df2[\"Mortgage\"]==0][df2[\"Personal Loan\"]==0].count()\nmortgage[\"no_Personal_loan_and_mortagage\"]    = df2[\"Mortgage\"][df2[\"Mortgage\"]>0][df2[\"Personal Loan\"]==0].count()\nmortgage[\"Personal_loan_and_mortagage\"]       = df2[\"Mortgage\"][df2[\"Mortgage\"]>0][df2[\"Personal Loan\"]==1].count()\nmortgage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xpos = np.arange(len(mortgage))\nvalue = [x for x in mortgage.values()]\nkeys = [x for x in mortgage.keys()]\nplt.bar(xpos,value)\nplt.xticks(xpos)\nplt.ylabel(\"Count\")\nplt.title('Mortgage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above we can't neglect any value as it might affect our target variable \"Personal loan,so we can't treat it as category."},{"metadata":{},"cell_type":"markdown","source":"now SCALING the non-category column"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df2[['Income', 'CCAvg',\"Mortgage\",\"Age\",\"Experience\"]]\nscaledX = scale.fit_transform(X)\ndf2['Income']     = (scaledX[:,0])\ndf2[\"CCAvg\"]      = (scaledX[:,1])\ndf2[\"Mortgage\"]   = (scaledX[:,2])\ndf2[\"Age\"]        = (scaledX[:,3])\ndf2[\"Experience\"] = (scaledX[:,4])\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df2[\"Income\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df2[\"CCAvg\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As income and CCAvg graph is skewed left ,we will remove outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_limit_income = df2[\"Income\"].mean() + 3*df2[\"Income\"].std()\nupper_limit_income","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_limit_ccavg = df2[\"CCAvg\"].mean() + 2*df2[\"CCAvg\"].std()\nupper_limit_ccavg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df2[df2[\"Income\"]<upper_limit_income][df2[\"CCAvg\"]<upper_limit_ccavg]\ndf3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df3[\"Income\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df3[\"CCAvg\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above graph looks better than previous"},{"metadata":{},"cell_type":"markdown","source":"Now we have prepared the good data..Let's build the model"},{"metadata":{},"cell_type":"markdown","source":"# Building model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature variable X"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df3.drop(['Personal Loan'],axis='columns')\nX.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target Variable y"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df3[\"Personal Loan\"]\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train-test data split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing Random forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit          # for random suffle rather than in order\nfrom sklearn.model_selection import cross_val_score\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\ncross_val_score(RandomForestClassifier(), X, y, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit          # for random suffle rather than in order\nfrom sklearn.model_selection import cross_val_score\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n\ncross_val_score(DecisionTreeClassifier(), X, y, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter tuning using GridSearhCv**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params = {\n    'svm': {\n        'model': svm.SVC(gamma='auto'),\n        'params' : {\n            'C': [1,10,20],\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': [1,5,10]\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': [1,5,10]\n        }\n    },\n    'DecisionTree': {\n        'model' : DecisionTreeClassifier(),\n        'params' : {\n            'criterion' : [\"gini\", \"entropy\"]\n        }\n    },\n    'GaussianNB' : {\n        'model' : GaussianNB(),\n        'params' : {}\n          \n },\n    'MultinomialNB' : {\n        'model' : MultinomialNB(),\n        'params' : {}\n            \n            \n        \n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nscores = []\nbest_estimators = {}\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=cv, return_train_score=False)\n    clf.fit(X_train, y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    best_estimators[model_name] = clf.best_estimator_\ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from above we can see Decision Tree comes out to be best for our case"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf = best_estimators[\"DecisionTree\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONFUSION Matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, best_clf.predict(X_test))\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relative importance of feature variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to Anirban Datta\nbest_clf.fit(X_train, y_train)\n\nfeatures = list(X_train.columns)\nimportances = best_clf.feature_importances_\nindices = np.argsort(importances)\n\nfig, ax = plt.subplots(figsize=(10, 7))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nax.tick_params(axis=\"x\", labelsize=12)\nax.tick_params(axis=\"y\", labelsize=14)\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance', fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Thus we conclude Income is the main key feature then comes education,CCavg,Family. Seems logical too. and best fit model for our case comes out to be Decision Tree****"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}