{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.formula.api as sm\nfrom sklearn.linear_model import LinearRegression\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# importing the dataset\ndataset = pd.read_csv('../input/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4360d7d787f5edc299da43b7ef34e6cc7413e9bc"},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a2717fbbca7439786f8c01132d98fe323d0aaab"},"cell_type":"code","source":"# getting the summary of the data\n# as we can see there is huge difference between 75% percentile and max values for residual sugar, free sulfur dioxide and total sulfur\n# dioxide - which means there are some huge outliers in the data\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cd947b16f118c66c97058a740d119421ad18a46"},"cell_type":"code","source":"# let's build a correlation matrix and use seaborn to plot the heatmap of these\n# correlation matrix\ncorr_matrix = dataset.corr()\nsns.heatmap(corr_matrix,cmap=\"YlGnBu\")\n# from the heatmap, dark shades represent positive correlation and light shades represent negative correlation\n# we can see that \"fixed acidity\" and \"citric acid\", \"density\" and \"fixed acidity\", \n# \"free sulfur dioxide\" and \"total sulfur dioxide\" are highly correlated\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34bb1cb8b81b9ad6ad06ac9763add5acaa10e4f2"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr_matrix,cmap=\"YlGnBu\", annot=True, linewidths=.5, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa964f8f6ca509627d6c64aa6cd6d1bf6e7834e3"},"cell_type":"code","source":"# I will use linear regression model to determine the quality of the wine\n# I will use backward elimination method\n# Steps are as follows :\n# 1. select the signifcance level to stay in the model (eg : SL = 0.05)\n# 2. Fit the model to all the possible predictors\n# 3. Consider the predictor with highest P-value, if P > SL then go to STEP 4 or else to STEP 6\n# 4. Remove the predictor\n# 5. Fit the model without this variable and go to step 3\n# 6. Finish the model\n\n# let us remove the multicollinear variables from the dataset\nlinear_dataset = dataset.drop(['fixed acidity','density','citric acid','free sulfur dioxide','total sulfur dioxide'],axis = 1,inplace=False)\n\nX = linear_dataset.loc[:,'volatile acidity' : 'alcohol'].values\ny = linear_dataset.loc[:, 'quality'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5666abcd76f01096982b3796833284cfb69d6452"},"cell_type":"code","source":"# applying backward elimination\nX_opt = X[:,0:6]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ebaccc5046bb368a1eb6b1aedce388caf8c486e"},"cell_type":"code","source":"# now looking at the P-value column(P>|t|) we have to eliminate the columns whose P-value SL(0.05)\n# we have to remove x2 column\nX_opt = X[:,[0,2,3,4,5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()\n\n# that's it. There is no more predictor with p-value > SL so will stop here \n# and we can see that R-squared and Adj.R-squared are very close to 1\n# our model fits well","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84c22f9a4f9a27135a29bd73236c1e6be9cba118"},"cell_type":"code","source":"# now rebuilding our training and test dataset and predicting the result\n# after backward elimination we found that few predictors to be removed\n# so let us bulid a new training and test data set from these observations\n\n\nX_train,X_test,y_train,y_test = train_test_split(X_opt,y,test_size = 0.2, random_state = 0)\n# feature scaling to get optimized result\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n# fitting linear regression to training set\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5961fa74360e7c1556539ce59a20c26297ddb526"},"cell_type":"code","source":"# calulating the RMSE value to see how well the model predicts\ny_pred = regressor.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nrmse","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}