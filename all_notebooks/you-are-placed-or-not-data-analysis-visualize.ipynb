{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Campus Recruitment Visualizations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this dataset we have to analyze and visualize the dataset and predict whether you are placed or not.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we need to import some libraries that will help in visualizing and analyzing techniques on the given dataset.\nLibraries : \n* Numpy\n* Pandas\n* Matplotlib\n* Sklearn\n* Seaborn","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt # visualizations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at all the values in columns i.e. categorical values( suppose in gender column there are two categories M or F ). By this **value_counts()** function we can see how many values of \"M\" are in the column and \"F\" are in the column of gender.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Why only gender ?** Have a look at all the categorical values in all columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.ssc_b.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hsc_b.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hsc_s.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.degree_t.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.workex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.specialisation.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see in the dataset, there are some categorical features i.e. gender, ssc_b, hsc_s, status etc. So we have to do some preprocessing in that columns. And we can do this little bit of preprocessing by using **sklearn library( LabelEncoder )**.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you do not know what is LabelEncoder so don't worry. I explain. LabelEncoder is in built function that converts the categories into some numerical values i.e. M in gender column so labelencoder converts this \"M\" into 0 and F into 1 so this is the real encoding of the categorical features in the datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = LabelEncoder()\ndata[\"gender\"] = label.fit_transform(data[\"gender\"])\ndata[\"ssc_b\"] = label.fit_transform(data[\"ssc_b\"])\ndata[\"hsc_b\"] = label.fit_transform(data[\"hsc_b\"])\ndata[\"hsc_s\"] = label.fit_transform(data[\"hsc_s\"])\ndata[\"degree_t\"] = label.fit_transform(data[\"degree_t\"])\ndata[\"workex\"] = label.fit_transform(data[\"workex\"])\ndata[\"specialisation\"] = label.fit_transform(data[\"specialisation\"])\ndata[\"status\"] = label.fit_transform(data[\"status\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I have finished this encoding.\nGood we have completed our first major step.\nHave a look at the dataset now after some preprocessing part.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A small visual of the dataset is to plot a histogram of all the columns of dataset. All the columns I mean that columns which have integers values. Have a look at the small visual part of our dataset. This can do simply by a matplotlib function i.e \"**your_data_name.hist()**\" and then write plt.show()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize = (20, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this dataset our first question is which factor influenced a candidate of getting placed or not ? and the answer of this question is **FEATURE SELECTION** This is the part of Analyzing the dataset. So lets do the Feature Selection.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Feature Selection is a process of extracting features from the dataset that have great importance of predicting our labels.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here is the new library for displaying the dataset to show which feature has more importance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first visualize the status of the candidate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_original = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(data_original, x=\"salary\", \n                 color=\"degree_p\",\n                 size='degree_p', \n                 hover_data=['gender', 'ssc_p', 'hsc_p', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'degree_p', \n                            'specialisation', 'mba_p', 'status', 'etest_p'], \n                 title = \"Salary Plot\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(data_original, x=\"ssc_p\", \n                 color=\"degree_p\",\n                 size='degree_p', \n                 hover_data=['gender', 'hsc_p', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'degree_p', \n                            'specialisation', 'mba_p', 'status', 'etest_p'], \n                 title = \"ssc_p Plot\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Divide the datset into our data and labels i.e X and y .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:, 0:13].values\ny = data.iloc[:, 13].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is one more type of showing which feature is more important. Using ExtraTreeClassifier it is easy to know which is best or having great importance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X , y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a look at all the feature's importances.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.feature_importances_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the graph and which has great importances?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Oh its 2nd column i.e **ssc_p**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(model.feature_importances_)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the index of that column i.e 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum important feature index is : \", model.feature_importances_.argmax()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all done","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's start making model to predict whether a candidate is placed or not.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use DecisionTreeClassifier from sklearn library. As we see that it is a classification problem and we do it by this function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier()\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a look our predictions that have made by our decision tree classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the accuracy by confusion matrix also lets do it ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see our Classifier has great accuarcy. Just by the ssc_p we can predict the candidate is placed or not.\nIf you like this notebook please upvote.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**YOUR UPVOTE IS MY ENCOURAGEMENT OF MAKING NOTEBOOKS**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Till then **Enjoy Machine Learning**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}