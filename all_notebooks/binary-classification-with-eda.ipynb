{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import skew, kurtosis\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T11:24:16.316918Z","iopub.execute_input":"2021-05-21T11:24:16.317498Z","iopub.status.idle":"2021-05-21T11:24:16.330036Z","shell.execute_reply.started":"2021-05-21T11:24:16.317461Z","shell.execute_reply":"2021-05-21T11:24:16.329033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.331474Z","iopub.execute_input":"2021-05-21T11:24:16.332051Z","iopub.status.idle":"2021-05-21T11:24:16.353056Z","shell.execute_reply.started":"2021-05-21T11:24:16.332009Z","shell.execute_reply":"2021-05-21T11:24:16.351319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(7)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.354607Z","iopub.execute_input":"2021-05-21T11:24:16.355037Z","iopub.status.idle":"2021-05-21T11:24:16.377165Z","shell.execute_reply.started":"2021-05-21T11:24:16.354991Z","shell.execute_reply":"2021-05-21T11:24:16.375928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Already it can be seen that the id feature is unnecessary but we shall deal with it later\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.378694Z","iopub.execute_input":"2021-05-21T11:24:16.379201Z","iopub.status.idle":"2021-05-21T11:24:16.405233Z","shell.execute_reply.started":"2021-05-21T11:24:16.379153Z","shell.execute_reply":"2021-05-21T11:24:16.404341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can already see that there are missing values in the bmi column\nprint(data.isnull().sum().sort_values(ascending=False))\nprint(\"Proportion of missing values for bmi: \", data.isnull().sum()['bmi']*100/len(data),'%')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.406438Z","iopub.execute_input":"2021-05-21T11:24:16.407092Z","iopub.status.idle":"2021-05-21T11:24:16.424222Z","shell.execute_reply.started":"2021-05-21T11:24:16.407052Z","shell.execute_reply":"2021-05-21T11:24:16.422663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far it can be seen that the only null values are in the BMI column and only number 201 out of 5110 samples. This is a low proportion so we can likely drop these samples or impute them without losing too much variance.","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.425924Z","iopub.execute_input":"2021-05-21T11:24:16.426351Z","iopub.status.idle":"2021-05-21T11:24:16.467339Z","shell.execute_reply.started":"2021-05-21T11:24:16.42631Z","shell.execute_reply":"2021-05-21T11:24:16.466183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the numeric features \ndata.select_dtypes(include=np.number)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.470227Z","iopub.execute_input":"2021-05-21T11:24:16.470905Z","iopub.status.idle":"2021-05-21T11:24:16.495826Z","shell.execute_reply.started":"2021-05-21T11:24:16.470855Z","shell.execute_reply":"2021-05-21T11:24:16.494697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Many discrete features are in numeric form so we should extract those too\ndata.select_dtypes(include=np.number).nunique()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.497737Z","iopub.execute_input":"2021-05-21T11:24:16.498393Z","iopub.status.idle":"2021-05-21T11:24:16.517609Z","shell.execute_reply.started":"2021-05-21T11:24:16.498344Z","shell.execute_reply":"2021-05-21T11:24:16.516696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It can be seen that there are three numeric binary features: hypertension,heart_disease,stroke (also the target)\n# Lets note the continous features and the categorical FEATURES seperately\ncat_features = \"hypertension heart_disease\".split()\ncont_features = \"age avg_glucose_level bmi\".split()\ncat_features.extend(data.select_dtypes(exclude=np.number).columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.518802Z","iopub.execute_input":"2021-05-21T11:24:16.519297Z","iopub.status.idle":"2021-05-21T11:24:16.526334Z","shell.execute_reply.started":"2021-05-21T11:24:16.51925Z","shell.execute_reply":"2021-05-21T11:24:16.525336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.527614Z","iopub.execute_input":"2021-05-21T11:24:16.528207Z","iopub.status.idle":"2021-05-21T11:24:16.540725Z","shell.execute_reply.started":"2021-05-21T11:24:16.528168Z","shell.execute_reply":"2021-05-21T11:24:16.539878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_features","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.54187Z","iopub.execute_input":"2021-05-21T11:24:16.542374Z","iopub.status.idle":"2021-05-21T11:24:16.554022Z","shell.execute_reply.started":"2021-05-21T11:24:16.542333Z","shell.execute_reply":"2021-05-21T11:24:16.55292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure we extracted all the features (not including id or the label)\nlen(cat_features)+len(cont_features)+2 == len(data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.555172Z","iopub.execute_input":"2021-05-21T11:24:16.555735Z","iopub.status.idle":"2021-05-21T11:24:16.568356Z","shell.execute_reply.started":"2021-05-21T11:24:16.555701Z","shell.execute_reply":"2021-05-21T11:24:16.567528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far we have checked out which features have missing values (which isn't a big deal) and which features are categorical and continous. Now for some Exploratory Data Analysis.","metadata":{}},{"cell_type":"code","source":"# Make subplots checking the distribution of the numeric features\nsns.set_style(\"white\")\nfig,axes = plt.subplots(1,3,figsize=(13,11))\nfor feat,ax in zip(cont_features,axes.flat):\n    sns.histplot(x=feat,data=data,hue='stroke',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:16.569562Z","iopub.execute_input":"2021-05-21T11:24:16.570052Z","iopub.status.idle":"2021-05-21T11:24:18.108066Z","shell.execute_reply.started":"2021-05-21T11:24:16.570009Z","shell.execute_reply":"2021-05-21T11:24:18.107124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plots are not very conclusive about which features point to a higher risk of stroke: the first subplot does suggest that a person is at a higher risk if above the age of 40, and this goes up dramatically as s/he approaches 80.\n\nAlso note that two graphs look rather skewed (both positively skewed).","metadata":{}},{"cell_type":"code","source":"for feat in cont_features:\n    print(\"Skewness of {}= {}\".format(feat,skew(data[feat].dropna())))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:18.109407Z","iopub.execute_input":"2021-05-21T11:24:18.10971Z","iopub.status.idle":"2021-05-21T11:24:18.121241Z","shell.execute_reply.started":"2021-05-21T11:24:18.109681Z","shell.execute_reply":"2021-05-21T11:24:18.12014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make a mental note to:\n- Impute values for BMI NaNs\n- Standardize/Logarithmically scale the BMI and avg_glucose_level features","metadata":{}},{"cell_type":"code","source":"print(\"Skewness of bmi feature BEFORE logarithmic scaling:\")\nprint(skew(data['bmi'].dropna()))\nprint(\"Skewness of bmi feature AFTER logarithmic scaling:\")\nprint(skew(np.log1p(data['bmi'].dropna())))\nprint('-'*120)\nprint(\"Skewness of avg_glucose_level feature BEFORE logarithmic scaling:\")\nprint(skew(data['avg_glucose_level']))\nprint(\"Skewness of avg_glucose_level feature AFTER logarithmic scaling:\")\nprint(skew(np.log1p(data['avg_glucose_level'])))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:18.122762Z","iopub.execute_input":"2021-05-21T11:24:18.12326Z","iopub.status.idle":"2021-05-21T11:24:18.138879Z","shell.execute_reply.started":"2021-05-21T11:24:18.123213Z","shell.execute_reply":"2021-05-21T11:24:18.137785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Skewness of these features does seem to go down considerably by applying logarithmic scaling. We need to address avg_glucose_level a bit more seriously though since any skewness above 0.5 is not very healthy.","metadata":{"execution":{"iopub.status.busy":"2021-05-20T17:57:04.808029Z","iopub.execute_input":"2021-05-20T17:57:04.808551Z","iopub.status.idle":"2021-05-20T17:57:04.814482Z","shell.execute_reply.started":"2021-05-20T17:57:04.808512Z","shell.execute_reply":"2021-05-20T17:57:04.813275Z"}}},{"cell_type":"code","source":"np.log1p(data.dropna()['bmi']).plot(kind='hist')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:18.140383Z","iopub.execute_input":"2021-05-21T11:24:18.140821Z","iopub.status.idle":"2021-05-21T11:24:18.370139Z","shell.execute_reply.started":"2021-05-21T11:24:18.140776Z","shell.execute_reply":"2021-05-21T11:24:18.369408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.log1p(data['avg_glucose_level']).plot(kind='hist')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:18.371108Z","iopub.execute_input":"2021-05-21T11:24:18.3715Z","iopub.status.idle":"2021-05-21T11:24:18.617659Z","shell.execute_reply.started":"2021-05-21T11:24:18.371472Z","shell.execute_reply":"2021-05-21T11:24:18.616859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have played with the continous features' distributions, lets check out those of the Categorical Features.","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(4,2,figsize=(18,14))\nfor feat,ax in zip(cat_features,axes.flat):\n    sns.countplot(data=data,x=feat,hue='stroke',ax=ax)\nsns.countplot(data=data,x='stroke',ax=axes[3,1])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:18.619985Z","iopub.execute_input":"2021-05-21T11:24:18.620372Z","iopub.status.idle":"2021-05-21T11:24:19.940098Z","shell.execute_reply.started":"2021-05-21T11:24:18.620343Z","shell.execute_reply":"2021-05-21T11:24:19.939341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- hypertension and heart_disease happen to be the best indicators out of the lot for predicting strokes (obviously).  \n- smoking_status seems to be a fairly decent indicator as well.\n- Unfortunately, it seems our target variable has an extremely unbalanced distribution so we cannot expect our models to be fantastic right off the bat.","metadata":{}},{"cell_type":"code","source":"# Check proportion of samples suffering strokes\nprint(\"Percentage of People in this dataset suffering from a stroke: {}%\".format(len(data[data['stroke']==0]) * 100 /len(data)))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:19.941681Z","iopub.execute_input":"2021-05-21T11:24:19.94209Z","iopub.status.idle":"2021-05-21T11:24:19.949192Z","shell.execute_reply.started":"2021-05-21T11:24:19.942059Z","shell.execute_reply":"2021-05-21T11:24:19.948061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Over 95% of our samples are not positive instances- the dataset is not likely to make it easy for the algorithms to find patterns.\n\nNow that we have explored the data, we can start some feature engineering and some transformations:\n- get rid of the residence_type variable since that shows absolutely no patterns with the stroke\n- apply One-Hot Encoding to the categorical variables and StandardScaler to everything else; DONT forget to impute values for bmi\n- finalize our pipeline with ColumnTransformer","metadata":{}},{"cell_type":"code","source":"# Get rid of the Residence Type variable since it will only serve to slow down training\ndata.drop('Residence_type',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:19.950756Z","iopub.execute_input":"2021-05-21T11:24:19.951208Z","iopub.status.idle":"2021-05-21T11:24:19.961931Z","shell.execute_reply.started":"2021-05-21T11:24:19.951161Z","shell.execute_reply":"2021-05-21T11:24:19.96071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Also drop the id column since we are just playing with the data\ndata.drop('id',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:19.963728Z","iopub.execute_input":"2021-05-21T11:24:19.964207Z","iopub.status.idle":"2021-05-21T11:24:19.975919Z","shell.execute_reply.started":"2021-05-21T11:24:19.964158Z","shell.execute_reply":"2021-05-21T11:24:19.974783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There is only one instance of a person being 'Other' so we can make it easier for the algorithm to get rid of this class altogether\ndata[data['gender'] == 'Other']","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:19.977753Z","iopub.execute_input":"2021-05-21T11:24:19.978412Z","iopub.status.idle":"2021-05-21T11:24:20.001306Z","shell.execute_reply.started":"2021-05-21T11:24:19.978374Z","shell.execute_reply":"2021-05-21T11:24:20.00049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cop = data.copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.002859Z","iopub.execute_input":"2021-05-21T11:24:20.003635Z","iopub.status.idle":"2021-05-21T11:24:20.015293Z","shell.execute_reply.started":"2021-05-21T11:24:20.003582Z","shell.execute_reply":"2021-05-21T11:24:20.014377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cop['gender'].replace(['Other'],'Male',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.016626Z","iopub.execute_input":"2021-05-21T11:24:20.017218Z","iopub.status.idle":"2021-05-21T11:24:20.028194Z","shell.execute_reply.started":"2021-05-21T11:24:20.017174Z","shell.execute_reply":"2021-05-21T11:24:20.027241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cop['gender'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.029481Z","iopub.execute_input":"2021-05-21T11:24:20.030145Z","iopub.status.idle":"2021-05-21T11:24:20.044757Z","shell.execute_reply.started":"2021-05-21T11:24:20.030095Z","shell.execute_reply":"2021-05-21T11:24:20.043834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply this logoc to the main dataframe\ndata['gender'].replace(['Other'],'Male',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.046199Z","iopub.execute_input":"2021-05-21T11:24:20.046777Z","iopub.status.idle":"2021-05-21T11:24:20.05894Z","shell.execute_reply.started":"2021-05-21T11:24:20.04673Z","shell.execute_reply":"2021-05-21T11:24:20.057715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in data.columns:\n    print(col,data[col].nunique())","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.060227Z","iopub.execute_input":"2021-05-21T11:24:20.060785Z","iopub.status.idle":"2021-05-21T11:24:20.088783Z","shell.execute_reply.started":"2021-05-21T11:24:20.060739Z","shell.execute_reply":"2021-05-21T11:24:20.088073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make our categorical features and continous features list again so we can apply the transformations\ncat_features = 'gender hypertension heart_disease ever_married work_type smoking_status'.split()\ncont_features = 'age avg_glucose_level bmi'.split()\n\n# Make sure we captured all the features (exluding the label)\nlen(cat_features)+len(cont_features) + 1 == len(data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.089933Z","iopub.execute_input":"2021-05-21T11:24:20.090224Z","iopub.status.idle":"2021-05-21T11:24:20.097863Z","shell.execute_reply.started":"2021-05-21T11:24:20.090197Z","shell.execute_reply":"2021-05-21T11:24:20.096859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the imputer for the BMI based on the mean a\ndata.fillna(value={'bmi':data['bmi'].mean()}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.099298Z","iopub.execute_input":"2021-05-21T11:24:20.099674Z","iopub.status.idle":"2021-05-21T11:24:20.111237Z","shell.execute_reply.started":"2021-05-21T11:24:20.099641Z","shell.execute_reply":"2021-05-21T11:24:20.110047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformation final steps\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\n\nX = data.drop('stroke',axis=1)\ny = data['stroke']\n\npipeline = ColumnTransformer([\n    (\"num_scaler\",StandardScaler(),cont_features),\n    (\"encoder\",OneHotEncoder(),cat_features)\n])\n\nX = pipeline.fit_transform(X)\n\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.15,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.112701Z","iopub.execute_input":"2021-05-21T11:24:20.113149Z","iopub.status.idle":"2021-05-21T11:24:20.149562Z","shell.execute_reply.started":"2021-05-21T11:24:20.113099Z","shell.execute_reply":"2021-05-21T11:24:20.148364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.15104Z","iopub.execute_input":"2021-05-21T11:24:20.151496Z","iopub.status.idle":"2021-05-21T11:24:20.159517Z","shell.execute_reply.started":"2021-05-21T11:24:20.151452Z","shell.execute_reply":"2021-05-21T11:24:20.158402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import models for classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import LinearSVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.161322Z","iopub.execute_input":"2021-05-21T11:24:20.162033Z","iopub.status.idle":"2021-05-21T11:24:20.168299Z","shell.execute_reply.started":"2021-05-21T11:24:20.161984Z","shell.execute_reply":"2021-05-21T11:24:20.167607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log = LogisticRegression()\nmlp = MLPClassifier(random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrfc = RandomForestClassifier(n_estimators=90,random_state=42)\ngbc = GradientBoostingClassifier(random_state=42)\nada = AdaBoostClassifier(random_state=42)\nsvc = LinearSVC(random_state=42)\nxgb = XGBClassifier()\n\nestimators = [log,mlp,knn,rfc,gbc,ada,svc,xgb]\n\n# Cross Validate for evaluation of each \nfor estimator in estimators:\n    print(\"Training {}\".format(estimator))\n    estimator.fit(X_train,y_train)\n    print(\"Score: {}\".format(estimator.score(X_val,y_val)))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:20.169622Z","iopub.execute_input":"2021-05-21T11:24:20.170302Z","iopub.status.idle":"2021-05-21T11:24:30.359138Z","shell.execute_reply.started":"2021-05-21T11:24:20.170257Z","shell.execute_reply":"2021-05-21T11:24:30.358186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that most of our classifiers look to perform extremely well but it fact this is not true. Recall that the distribution of the target variable is extremely unbalanced- over 95% of the instances have positive labels hence we could have gotten a higher accuracy by having a classifier that simply predicted '1' each and every time.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator\n\nclass Always0Classifier(BaseEstimator):\n    def fit(self,X,y=None):\n        return self\n    def predict(self,X,y=None):\n        return np.zeros((len(X),1), dtype=bool)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:30.364245Z","iopub.execute_input":"2021-05-21T11:24:30.366255Z","iopub.status.idle":"2021-05-21T11:24:30.37549Z","shell.execute_reply.started":"2021-05-21T11:24:30.366204Z","shell.execute_reply":"2021-05-21T11:24:30.374148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy = Always0Classifier()\ncross_val_score(dummy,X_train,y_train,cv=6,scoring=\"accuracy\").mean()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:30.378287Z","iopub.execute_input":"2021-05-21T11:24:30.378639Z","iopub.status.idle":"2021-05-21T11:24:30.404519Z","shell.execute_reply.started":"2021-05-21T11:24:30.378606Z","shell.execute_reply":"2021-05-21T11:24:30.403311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is rather funny to see us getting a higher accuracy simply off always saying the person has less risk of a stroke- this is taking advantage of the dataset so don't do this lol.","metadata":{}},{"cell_type":"code","source":"# Make an ensemble just to see if we can do better\nnamed_estimators = [\n    (\"log\",log),(\"mlp\",mlp),(\"knn\",knn),(\"rfc\",rfc),(\"gbc\",gbc),(\"ada\",ada),(\"svc\",svc),(\"xgb\",xgb)\n]\n\nvoting_clf = VotingClassifier(named_estimators)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:30.406088Z","iopub.execute_input":"2021-05-21T11:24:30.406484Z","iopub.status.idle":"2021-05-21T11:24:30.41287Z","shell.execute_reply.started":"2021-05-21T11:24:30.406444Z","shell.execute_reply":"2021-05-21T11:24:30.411782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training\")\nvoting_clf.fit(X_train,y_train)\nprint(\"Done. Score: {}\".format(voting_clf.score(X_val,y_val)))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:30.414512Z","iopub.execute_input":"2021-05-21T11:24:30.414939Z","iopub.status.idle":"2021-05-21T11:24:40.415853Z","shell.execute_reply.started":"2021-05-21T11:24:30.414903Z","shell.execute_reply":"2021-05-21T11:24:40.414718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While none of our estimators technically performed bad- they did not perform better than a dummy classifier that always predicted the same label again and again and again. The Hard Voting classifier here did not perform better than the best constituent classifier here so:\n- If we wanted a legit submission, we would use the XGB Classifier here\n- If we were naughty, we would use the Always0Classifier since that has the best technical accuracy here","metadata":{}},{"cell_type":"code","source":"# Once again, checking the metrics for xgb\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n\npredictions = xgb.predict(X_val)\n\nprint(confusion_matrix(predictions,y_val))\nprint(\"Accuracy: \",xgb.score(X_val,y_val))\nprint(\"Recall: \",recall_score(y_val,predictions,average=\"micro\"))\nprint(\"Precision: \",precision_score(y_val,predictions,average=\"micro\"))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:24:40.420306Z","iopub.execute_input":"2021-05-21T11:24:40.422907Z","iopub.status.idle":"2021-05-21T11:24:40.491985Z","shell.execute_reply.started":"2021-05-21T11:24:40.422846Z","shell.execute_reply":"2021-05-21T11:24:40.490714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Life is weird when you have an unbalanced dataset :(","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}