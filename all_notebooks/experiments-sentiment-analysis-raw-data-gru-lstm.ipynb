{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\nplt.style.use('fivethirtyeight')\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM, GRU\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.datasets import imdb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"reading data..\")\ndata = pd.read_csv('/kaggle/input/imdb-review-dataset/imdb_master.csv',encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"class IMDBSentiMentAnalysis:\n    \n    def __init__(self, data, maxlen=100, num_words=10000):\n        \n        self.model = None\n        self.history = None\n        self.data = data\n        self.maxlen = maxlen\n        self.num_words = num_words\n        \n    def process_data(self):\n        print(\"processing data...\")\n        data = self.data[self.data.label!='unsup']\n        sns.countplot(x='label',data=data)\n        data['out'] = data['label']\n        \n        data['out'][data.out=='neg']=0\n        data['out'][data.out=='pos']=1\n        # Another way data['out'] = data['out'].map({1:'pos',0:'neg'})\n        np.unique(data.out)\n        #data['out'] = data['label'].map({1:'pos',0:'neg'})\n        \n        req_data = data[['review','out']]\n\n        self.texts = np.array(req_data.review)\n        self.labels = np.array(req_data.out)\n        self.convert_data_to_padded_sequence()\n            \n            \n    def convert_data_to_padded_sequence(self):\n        print(\"Converting data to Sequences\")\n        # num_words: Top No. of words to be tokenized. Rest will be marked as unknown or ignored.\n        tokenizer = Tokenizer(num_words=self.num_words) \n        \n        # tokenizing based on \"texts\". This step generates the word_index and map each word to an integer other than 0.\n        tokenizer.fit_on_texts(self.texts)\n        \n        # generating sequence based on tokenizer's word_index. Each sentence will now be represented by combination of numericals\n        # Example: \"Good movie\" may be represented by [22, 37]\n        seq = tokenizer.texts_to_sequences(self.texts)\n        \n        self.word_index = tokenizer.word_index\n        # padding each numerical representation of sentence to have fixed length.\n\n        self.padded_seq = np.array(pad_sequences(seq,maxlen=self.maxlen))\n        print(\"Data converted to Sequences...\")\n        \n    \n    \n    def plot_model_output(self):\n        history = self.history\n        epochs = self.epochs\n        plt.figure()\n        plt.plot(range(epochs,),history.history['loss'],label = 'training_loss')\n        plt.plot(range(epochs,),history.history['val_loss'],label = 'validation_loss')\n        plt.legend()\n        plt.figure()\n        plt.plot(range(epochs,),history.history['acc'],label = 'training_accuracy')\n        plt.plot(range(epochs,),history.history['val_acc'],label = 'validation_accuracy')\n        plt.legend()\n        plt.show()\n\n    def init_model(self, model = None, gru=False):\n        \n        if model is None:\n            print(\"Initialising default model\")\n            model = Sequential()\n            embedding = Embedding(self.num_words, 32, input_length = self.maxlen, name='embedding')\n            model.add(embedding)\n            if gru:\n                model.add(GRU(32))\n            else:\n                model.add(LSTM(32))\n            model.add(Flatten())\n            model.add(Dense(1,activation='sigmoid'))\n            self.model = model\n        else:\n            print(\"Initialising model passed\")\n            self.model = model\n\n        return self.model.summary()\n\n    def run_the_model(self,optimizer = 'rmsprop', epochs = 10, validation_split=0.2):\n        \n        self.model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc'])\n        self.epochs = epochs\n\n        self.history = self.model.fit(self.padded_seq,np.asarray(self.labels).astype(np.uint8),epochs=epochs,validation_split=validation_split)\n        \n        self.plot_model_output()\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(\"Initialising IMDB object\")\n\nimdb_deep_learning = IMDBSentiMentAnalysis(data,)\n\nimdb_deep_learning.process_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using GRU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_deep_learning.init_model(gru=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_deep_learning.run_the_model(epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# USING LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_deep_learning.init_model(gru=False)\nimdb_deep_learning.run_the_model(epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}