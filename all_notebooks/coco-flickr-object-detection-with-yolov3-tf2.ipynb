{"cells":[{"metadata":{"id":"K2s1A9eLRPEj"},"cell_type":"markdown","source":"# Introduction\nHi guys, this notebook illustrates how to employ yoloV3 to perform Object Detection in Flickr images and COCO 2014 dataset. 99% of the codes are from this amazing repo https://github.com/zzh8829/yolov3-tf2 please star him.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/zzh8829/yolov3-tf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n%cd yolov3-tf2\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls data\nprint('coco.names')\n!cat data/coco.names | wc\nprint('voc2012.names')\n!cat data/voc2012.names | wc\n!cat data/coco.names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -r requirements-gpu.txt","execution_count":null,"outputs":[]},{"metadata":{"id":"U8l4RJ0XRPEm","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport numpy as np\nimport os\nimport time\nimport json\nfrom glob import glob\nfrom PIL import Image\nimport pickle\n\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat convert.py # this function use to convert official yolov3 weights to Keras model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://pjreddie.com/media/files/yolov3.weights -O data/yolov3.weights\n!python convert.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nfrom absl import app, logging, flags\nfrom absl.flags import FLAGS\nimport time\nimport cv2\nfrom yolov3_tf2.models import (\n    YoloV3, YoloV3Tiny\n)\nfrom yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\nfrom yolov3_tf2.utils import draw_outputs\nfrom IPython.display import Image, display","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nflags.DEFINE_string('classes', './data/coco.names', 'path to classes file')\nflags.DEFINE_string('weights', './checkpoints/yolov3.tf',\n                    'path to weights file')\nflags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\nflags.DEFINE_integer('size', 416, 'resize images to')\nflags.DEFINE_string('image', './data/girl.png', 'path to input image')\nflags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\nflags.DEFINE_string('output', './output.jpg', 'path to output image')\nflags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n\napp._run_init(['yolov3'], app.parse_flags_with_usage)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trick to better allocate GPU memory, otherwise, we will get OOM\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FLAGS.tiny, FLAGS.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nif FLAGS.tiny:\n    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\nelse:\n    yolo = YoloV3(classes=FLAGS.num_classes)\n      \nyolo.load_weights(FLAGS.weights).expect_partial() # expect_partial just suppress some loading warning\nlogging.info('weights loaded')\n\nclass_names = [c.strip() for c in open(FLAGS.classes).readlines()]\nlogging.info('classes loaded')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(class_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Flickr dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"flickr_path = '/kaggle/input/flickr8k-sau/Flickr_Data/Images/'\npaths2 = sorted(os.listdir(flickr_path))\nprint(len(paths2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ngirl.png  meme2.jpeg    street.jpg\t    \nmeme.jpg  meme_out.jpg  street_out.jpg \n'''\nFLAGS.image = 'data/meme.jpg'\nFLAGS.image = 'data/meme2.jpeg'\nFLAGS.image = 'data/girl.png'\nFLAGS.image = 'data/street.jpg'\n\nfor jj in range(20):\n    FLAGS.image = flickr_path + paths2[jj]\n\n    img_raw = tf.image.decode_image(\n        open(FLAGS.image, 'rb').read(), channels=3)\n\n    img = tf.expand_dims(img_raw, 0)\n    img = transform_images(img, FLAGS.size)\n\n    t1 = time.time()\n    boxes, scores, classes, nums = yolo(img)\n    t2 = time.time()\n#     logging.info('time: {}'.format(t2 - t1))\n\n#     logging.info('detections:')\n    for i in range(nums[0]):\n        logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n                                        np.array(scores[0][i]),\n                                        np.array(boxes[0][i])))\n\n    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n    img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n\n    \n    display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nums_np = nums[0].numpy()\nprint(nums_np)\n\nscore_np = scores[0].numpy()\nprint(score_np[:(nums[0].numpy()+1)])\n\nclasses_np = classes[0].numpy().astype(int)\nprint(classes_np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## see the details of drawing function\n!cat yolov3_tf2/utils.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(nums_np):\n    print(class_names[classes_np[i]]) # Note that Person = class0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# COCO dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"annotation_file = '/kaggle/input/coco2014/captions/annotations/captions_train2014.json'\nCOCOPATH = '/kaggle/input/coco2014/train2014/train2014/'\n!ls {COCOPATH} | wc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(annotation_file, 'r') as f:\n    annotations = json.load(f)\n\n# Store captions and image names in vectors\nall_captions = {}\nall_img_name_vector = []\n\nfor annot in annotations['annotations']:\n    caption = '<start> ' + annot['caption'] + ' <end>'\n    image_id = annot['image_id']\n    full_coco_image_path = COCOPATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n\n    all_img_name_vector.append(full_coco_image_path)\n    \n    if all_captions.get(all_img_name_vector[-1]) is None:\n        all_captions[all_img_name_vector[-1]] = []\n    \n    all_captions[all_img_name_vector[-1]].append(caption)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(all_captions), len(all_img_name_vector), image_id\nprint(all_img_name_vector[:5])\nprint(all_captions[list(all_captions.keys())[0]])","execution_count":null,"outputs":[]},{"metadata":{"id":"4G3b8x8_RPFD","trusted":true},"cell_type":"code","source":"\n\npaths2 = sorted(os.listdir(COCOPATH))\nprint(len(paths2))\n\nfor jj in range(20):\n    FLAGS.image =  COCOPATH + paths2[jj] # all_img_name_vector[jj] is repeated\n    print('\\n***',all_captions[COCOPATH + paths2[jj]],'***\\n')\n    img_raw = tf.image.decode_image(\n        open(FLAGS.image, 'rb').read(), channels=3)\n\n    img = tf.expand_dims(img_raw, 0)\n    img = transform_images(img, FLAGS.size)\n\n    t1 = time.time()\n    boxes, scores, classes, nums = yolo(img)\n    t2 = time.time()\n#     logging.info('time: {}'.format(t2 - t1))\n\n#     logging.info('detections:')\n    for i in range(nums[0]):\n        logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n                                        np.array(scores[0][i]),\n                                        np.array(boxes[0][i])))\n\n    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n    img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n\n    \n    display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"image_captioning.ipynb","private_outputs":true,"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}