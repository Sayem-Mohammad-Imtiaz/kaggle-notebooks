{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install beautifulsoup4 ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:41.666171Z","iopub.execute_input":"2021-06-11T21:15:41.666633Z","iopub.status.idle":"2021-06-11T21:15:52.350546Z","shell.execute_reply.started":"2021-06-11T21:15:41.666541Z","shell.execute_reply":"2021-06-11T21:15:52.349109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport string\nimport json\nimport emoji\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom bs4 import BeautifulSoup\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option(\"display.max_columns\", None)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:52.355454Z","iopub.execute_input":"2021-06-11T21:15:52.355779Z","iopub.status.idle":"2021-06-11T21:15:55.986117Z","shell.execute_reply.started":"2021-06-11T21:15:52.355748Z","shell.execute_reply":"2021-06-11T21:15:55.985146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/goemotions/data/train.tsv\", sep='\\t', header=None, names=['Text', 'Class', 'ID'])\ndf_dev = pd.read_csv(\"../input/goemotions/data/dev.tsv\", sep='\\t', header=None, names=['Text', 'Class', 'ID'])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:55.98782Z","iopub.execute_input":"2021-06-11T21:15:55.988297Z","iopub.status.idle":"2021-06-11T21:15:56.161429Z","shell.execute_reply.started":"2021-06-11T21:15:55.98824Z","shell.execute_reply":"2021-06-11T21:15:56.160471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['List of classes'] = df_train['Class'].apply(lambda x: x.split(','))\ndf_train['Len of classes'] = df_train['List of classes'].apply(lambda x: len(x))\ndf_dev['List of classes'] = df_dev['Class'].apply(lambda x: x.split(','))\ndf_dev['Len of classes'] = df_dev['List of classes'].apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.163002Z","iopub.execute_input":"2021-06-11T21:15:56.16344Z","iopub.status.idle":"2021-06-11T21:15:56.313325Z","shell.execute_reply.started":"2021-06-11T21:15:56.163398Z","shell.execute_reply":"2021-06-11T21:15:56.312444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/goemotions/data/ekman_mapping.json') as file:\n    ekman_mapping = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.318701Z","iopub.execute_input":"2021-06-11T21:15:56.319079Z","iopub.status.idle":"2021-06-11T21:15:56.329073Z","shell.execute_reply.started":"2021-06-11T21:15:56.319041Z","shell.execute_reply":"2021-06-11T21:15:56.328036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion_file = open(\"../input/goemotions/data/emotions.txt\", \"r\")\nemotion_list = emotion_file.read()\nemotion_list = emotion_list.split(\"\\n\")\nprint(emotion_list)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.33451Z","iopub.execute_input":"2021-06-11T21:15:56.334864Z","iopub.status.idle":"2021-06-11T21:15:56.344857Z","shell.execute_reply.started":"2021-06-11T21:15:56.334832Z","shell.execute_reply":"2021-06-11T21:15:56.343351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def idx2class(idx_list):\n    arr = []\n    for i in idx_list:\n        arr.append(emotion_list[int(i)])\n    return arr","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.347217Z","iopub.execute_input":"2021-06-11T21:15:56.347715Z","iopub.status.idle":"2021-06-11T21:15:56.354502Z","shell.execute_reply.started":"2021-06-11T21:15:56.34767Z","shell.execute_reply":"2021-06-11T21:15:56.352707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Emotions'] = df_train['List of classes'].apply(idx2class)\ndf_dev['Emotions'] = df_dev['List of classes'].apply(idx2class)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.356826Z","iopub.execute_input":"2021-06-11T21:15:56.357351Z","iopub.status.idle":"2021-06-11T21:15:56.579506Z","shell.execute_reply.started":"2021-06-11T21:15:56.357305Z","shell.execute_reply":"2021-06-11T21:15:56.577893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def EmotionMapping(emotion_list):\n    map_list = []\n    \n    for i in emotion_list:\n        if i in ekman_mapping['anger']:\n            map_list.append('anger')\n        if i in ekman_mapping['disgust']:\n            map_list.append('disgust')\n        if i in ekman_mapping['fear']:\n            map_list.append('fear')\n        if i in ekman_mapping['joy']:\n            map_list.append('joy')\n        if i in ekman_mapping['sadness']:\n            map_list.append('sadness')\n        if i in ekman_mapping['surprise']:\n            map_list.append('surprise')\n        if i == 'neutral':\n            map_list.append('neutral')\n            \n    return map_list","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.585994Z","iopub.execute_input":"2021-06-11T21:15:56.589243Z","iopub.status.idle":"2021-06-11T21:15:56.607925Z","shell.execute_reply.started":"2021-06-11T21:15:56.589192Z","shell.execute_reply":"2021-06-11T21:15:56.605808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Mapped Emotions'] = df_train['Emotions'].apply(EmotionMapping)\ndf_dev['Mapped Emotions'] = df_dev['Emotions'].apply(EmotionMapping)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.615104Z","iopub.execute_input":"2021-06-11T21:15:56.619044Z","iopub.status.idle":"2021-06-11T21:15:56.777406Z","shell.execute_reply.started":"2021-06-11T21:15:56.618994Z","shell.execute_reply":"2021-06-11T21:15:56.776307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['anger'] = np.zeros((len(df_train),1))\ndf_train['disgust'] = np.zeros((len(df_train),1))\ndf_train['fear'] = np.zeros((len(df_train),1))\ndf_train['joy'] = np.zeros((len(df_train),1))\ndf_train['sadness'] = np.zeros((len(df_train),1))\ndf_train['surprise'] = np.zeros((len(df_train),1))\ndf_train['neutral'] = np.zeros((len(df_train),1))\n\ndf_dev['anger'] = np.zeros((len(df_dev),1))\ndf_dev['disgust'] = np.zeros((len(df_dev),1))\ndf_dev['fear'] = np.zeros((len(df_dev),1))\ndf_dev['joy'] = np.zeros((len(df_dev),1))\ndf_dev['sadness'] = np.zeros((len(df_dev),1))\ndf_dev['surprise'] = np.zeros((len(df_dev),1))\ndf_dev['neutral'] = np.zeros((len(df_dev),1))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.782305Z","iopub.execute_input":"2021-06-11T21:15:56.78493Z","iopub.status.idle":"2021-06-11T21:15:56.814987Z","shell.execute_reply.started":"2021-06-11T21:15:56.784865Z","shell.execute_reply":"2021-06-11T21:15:56.813848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise','neutral']:\n    df_train[i] = df_train['Mapped Emotions'].apply(lambda x: 1 if i in x else 0)\n    df_dev[i] = df_dev['Mapped Emotions'].apply(lambda x: 1 if i in x else 0)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:56.819566Z","iopub.execute_input":"2021-06-11T21:15:56.822256Z","iopub.status.idle":"2021-06-11T21:15:57.12626Z","shell.execute_reply.started":"2021-06-11T21:15:56.822194Z","shell.execute_reply":"2021-06-11T21:15:57.125242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.127775Z","iopub.execute_input":"2021-06-11T21:15:57.128182Z","iopub.status.idle":"2021-06-11T21:15:57.162323Z","shell.execute_reply.started":"2021-06-11T21:15:57.128141Z","shell.execute_reply":"2021-06-11T21:15:57.161217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dev.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.164467Z","iopub.execute_input":"2021-06-11T21:15:57.164931Z","iopub.status.idle":"2021-06-11T21:15:57.18939Z","shell.execute_reply.started":"2021-06-11T21:15:57.164889Z","shell.execute_reply":"2021-06-11T21:15:57.187815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop(df_train[df_train['neutral'] == 1].index, inplace=True)\ndf_dev.drop(df_dev[df_dev['neutral'] == 1].index, inplace=True)\ndf_train.drop(df_train[df_train['disgust'] == 1].index, inplace=True)\ndf_dev.drop(df_dev[df_dev['disgust'] == 1].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.191985Z","iopub.execute_input":"2021-06-11T21:15:57.192619Z","iopub.status.idle":"2021-06-11T21:15:57.252171Z","shell.execute_reply.started":"2021-06-11T21:15:57.192562Z","shell.execute_reply":"2021-06-11T21:15:57.251185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop(['Class', 'List of classes', 'Len of classes', 'Emotions', 'Mapped Emotions', 'neutral', 'disgust'], axis=1, inplace=True)\ndf_dev.drop(['Class', 'List of classes', 'Len of classes', 'Emotions', 'Mapped Emotions', 'neutral', 'disgust'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.253775Z","iopub.execute_input":"2021-06-11T21:15:57.254182Z","iopub.status.idle":"2021-06-11T21:15:57.265111Z","shell.execute_reply.started":"2021-06-11T21:15:57.254139Z","shell.execute_reply":"2021-06-11T21:15:57.263697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n\npunct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n                'demonetisation': 'demonetization'}","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.267416Z","iopub.execute_input":"2021-06-11T21:15:57.268335Z","iopub.status.idle":"2021-06-11T21:15:57.298188Z","shell.execute_reply.started":"2021-06-11T21:15:57.268271Z","shell.execute_reply":"2021-06-11T21:15:57.296837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = emoji.demojize(text)\n    text = re.sub(r'\\:(.*?)\\:','',text)\n    text = str(text).lower()    #Making Text Lowercase\n    text = re.sub('\\[.*?\\]', '', text)\n    #The next 2 lines remove html text\n    text = BeautifulSoup(text, 'lxml').get_text()\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n    return text\n\ndef clean_contractions(text, mapping):\n    '''Clean contraction using contraction mapping'''    \n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    for word in mapping.keys():\n        if \"\"+word+\"\" in text:\n            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n    #Remove Punctuations\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n    text = re.sub(r'[\" \"]+', \" \", text)\n    return text\n\ndef clean_special_chars(text, punct, mapping):\n    '''Cleans special characters present(if any)'''   \n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text\n\ndef correct_spelling(x, dic):\n    '''Corrects common spelling errors'''   \n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\ndef remove_space(text):\n    '''Removes awkward spaces'''   \n    #Removes awkward spaces \n    text = text.strip()\n    text = text.split()\n    return \" \".join(text)\n\ndef text_preprocessing_pipeline(text):\n    '''Cleaning and parsing the text.'''\n    text = clean_text(text)\n    text = clean_contractions(text, contraction_mapping)\n    text = clean_special_chars(text, punct, punct_mapping)\n    text = correct_spelling(text, mispell_dict)\n    text = remove_space(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.300173Z","iopub.execute_input":"2021-06-11T21:15:57.300697Z","iopub.status.idle":"2021-06-11T21:15:57.320935Z","shell.execute_reply.started":"2021-06-11T21:15:57.300598Z","shell.execute_reply":"2021-06-11T21:15:57.319557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['Text'] = df_train['Text'].apply(text_preprocessing_pipeline)\n# df_dev['Text'] = df_dev['Text'].apply(text_preprocessing_pipeline)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.322728Z","iopub.execute_input":"2021-06-11T21:15:57.323288Z","iopub.status.idle":"2021-06-11T21:15:57.335926Z","shell.execute_reply.started":"2021-06-11T21:15:57.323244Z","shell.execute_reply":"2021-06-11T21:15:57.334754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.reset_index(drop=True).to_csv(\"train.csv\", index=False)\ndf_dev.reset_index(drop=True).to_csv(\"val.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.338243Z","iopub.execute_input":"2021-06-11T21:15:57.338821Z","iopub.status.idle":"2021-06-11T21:15:57.849493Z","shell.execute_reply.started":"2021-06-11T21:15:57.338776Z","shell.execute_reply":"2021-06-11T21:15:57.848595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_dev = df_dev.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.851104Z","iopub.execute_input":"2021-06-11T21:15:57.851526Z","iopub.status.idle":"2021-06-11T21:15:57.864489Z","shell.execute_reply.started":"2021-06-11T21:15:57.851482Z","shell.execute_reply":"2021-06-11T21:15:57.863214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.867158Z","iopub.execute_input":"2021-06-11T21:15:57.867879Z","iopub.status.idle":"2021-06-11T21:15:57.885815Z","shell.execute_reply.started":"2021-06-11T21:15:57.867833Z","shell.execute_reply":"2021-06-11T21:15:57.884835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_dev.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.889018Z","iopub.execute_input":"2021-06-11T21:15:57.889981Z","iopub.status.idle":"2021-06-11T21:15:57.897706Z","shell.execute_reply.started":"2021-06-11T21:15:57.889795Z","shell.execute_reply":"2021-06-11T21:15:57.896231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.899552Z","iopub.execute_input":"2021-06-11T21:15:57.900622Z","iopub.status.idle":"2021-06-11T21:15:57.949845Z","shell.execute_reply.started":"2021-06-11T21:15:57.900528Z","shell.execute_reply":"2021-06-11T21:15:57.948873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sections of config\n\n# Defining some key variables that will be used later on in the training\nMAX_LEN = 200\nTRAIN_BATCH_SIZE = 64\nVALID_BATCH_SIZE = 64\nEPOCHS = 10\nLEARNING_RATE = 2e-5\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:15:57.952436Z","iopub.execute_input":"2021-06-11T21:15:57.952908Z","iopub.status.idle":"2021-06-11T21:16:02.467867Z","shell.execute_reply.started":"2021-06-11T21:15:57.952876Z","shell.execute_reply":"2021-06-11T21:16:02.466507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_cols = [col for col in df_train.columns if col not in ['Text', 'ID']]\ntarget_cols","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:02.469627Z","iopub.execute_input":"2021-06-11T21:16:02.470336Z","iopub.status.idle":"2021-06-11T21:16:02.479507Z","shell.execute_reply.started":"2021-06-11T21:16:02.470288Z","shell.execute_reply":"2021-06-11T21:16:02.478316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.text = df.Text\n        self.tokenizer = tokenizer\n        self.targets = df[target_cols].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:02.4815Z","iopub.execute_input":"2021-06-11T21:16:02.482289Z","iopub.status.idle":"2021-06-11T21:16:02.494786Z","shell.execute_reply.started":"2021-06-11T21:16:02.482242Z","shell.execute_reply":"2021-06-11T21:16:02.493574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = BERTDataset(df_train, tokenizer, MAX_LEN)\nvalid_dataset = BERTDataset(df_dev, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:02.496433Z","iopub.execute_input":"2021-06-11T21:16:02.497124Z","iopub.status.idle":"2021-06-11T21:16:02.514403Z","shell.execute_reply.started":"2021-06-11T21:16:02.497075Z","shell.execute_reply":"2021-06-11T21:16:02.513311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, \n                          num_workers=4, shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, \n                          num_workers=4, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:02.516643Z","iopub.execute_input":"2021-06-11T21:16:02.517567Z","iopub.status.idle":"2021-06-11T21:16:02.526959Z","shell.execute_reply.started":"2021-06-11T21:16:02.517518Z","shell.execute_reply":"2021-06-11T21:16:02.525779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.roberta = AutoModel.from_pretrained('roberta-base')\n#         self.l2 = torch.nn.Dropout(0.3)\n        self.fc = torch.nn.Linear(768,5)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, features = self.roberta(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n#         output_2 = self.l2(output_1)\n        output = self.fc(features)\n        return output\n\nmodel = BERTClass()\nmodel.to(device);","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:02.52928Z","iopub.execute_input":"2021-06-11T21:16:02.52987Z","iopub.status.idle":"2021-06-11T21:16:29.732355Z","shell.execute_reply.started":"2021-06-11T21:16:02.529823Z","shell.execute_reply":"2021-06-11T21:16:29.730914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:29.734182Z","iopub.execute_input":"2021-06-11T21:16:29.734926Z","iopub.status.idle":"2021-06-11T21:16:29.743031Z","shell.execute_reply.started":"2021-06-11T21:16:29.734879Z","shell.execute_reply":"2021-06-11T21:16:29.739007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:29.755426Z","iopub.execute_input":"2021-06-11T21:16:29.758432Z","iopub.status.idle":"2021-06-11T21:16:29.773652Z","shell.execute_reply.started":"2021-06-11T21:16:29.758386Z","shell.execute_reply":"2021-06-11T21:16:29.772156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for _,data in enumerate(train_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        loss = loss_fn(outputs, targets)\n        if _%500 == 0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:29.780186Z","iopub.execute_input":"2021-06-11T21:16:29.783393Z","iopub.status.idle":"2021-06-11T21:16:29.797381Z","shell.execute_reply.started":"2021-06-11T21:16:29.783347Z","shell.execute_reply":"2021-06-11T21:16:29.795788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:29.803458Z","iopub.execute_input":"2021-06-11T21:16:29.80705Z","iopub.status.idle":"2021-06-11T21:16:53.542089Z","shell.execute_reply.started":"2021-06-11T21:16:29.807005Z","shell.execute_reply":"2021-06-11T21:16:53.539269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation():\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(valid_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:53.543716Z","iopub.status.idle":"2021-06-11T21:16:53.544488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs, targets = validation()\noutputs = np.array(outputs) >= 0.5\naccuracy = metrics.accuracy_score(targets, outputs)\nf1_score_micro = metrics.f1_score(targets, outputs, average='micro')\nf1_score_macro = metrics.f1_score(targets, outputs, average='macro')\nprint(f\"Accuracy Score = {accuracy}\")\nprint(f\"F1 Score (Micro) = {f1_score_micro}\")\nprint(f\"F1 Score (Macro) = {f1_score_macro}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:53.546447Z","iopub.status.idle":"2021-06-11T21:16:53.547403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.bin')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:53.549196Z","iopub.status.idle":"2021-06-11T21:16:53.550135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoConfig, AutoModel","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:53.551832Z","iopub.status.idle":"2021-06-11T21:16:53.552746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config = AutoConfig.from_pretrained('bert-base-uncased')\n#model =  AutoModel.from_config(config)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:53.554386Z","iopub.status.idle":"2021-06-11T21:16:53.555447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PATH = \"\"\n#model.load_state_dict(torch.load(PATH))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T21:16:53.557147Z","iopub.status.idle":"2021-06-11T21:16:53.558254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}