{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mplib\nfrom matplotlib import pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n# /kaggle/input/life-expectancy-who/led.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understanding the dataset\nThe focus here is to take a look at the dataset and understand what sort of preprocessing it may need. I am interested to see where values are missing and what kinds of datatypes I will be working with. \n\n#### Metadata \n- Country -                       Country\n\n- Year -                          Year\n\n- Status -                        Developed or Developing status\n\n- Lifeexpectancy -                Life Expectancy in age\n\n- AdultMortality -                Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\n\n- infantdeaths -                  Number of Infant Deaths per 1000 population\n\n- Alcohol -                       Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\n\n- percentageexpenditure -         Expenditure on health as a percentage of Gross Domestic Product per capita(%)\n\n- HepatitisB -                    Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\n\n- Measles -                       Measles - number of reported cases per 1000 population\n\n- BMI -                           Average Body Mass Index of entire population\n\n- under-fivedeaths -              Number of under-five deaths per 1000 population\n\n- Polio -                         Polio (Pol3) immunization coverage among 1-year-olds (%)\n\n- Totalexpenditure -              General government expenditure on health as a percentage of total government expenditure (%)\n\n- Diphtheria -                    Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n\n- HIV/AIDS -                      Deaths per 1 000 live births HIV/AIDS (0-4 years)\n\n- GDP -                           Gross Domestic Product per capita (in USD)\n\n- Population -                    Population of the country\n\n- thinness1-19years -             Prevalence of thinness among children and adolescents for Age 10 to 19 (% )\n\n- thinness5-9years -              Prevalence of thinness among children for Age 5 to 9(%)\n\n- Incomecompositionofresources -  Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n\n- Schooling -                     Number of years of Schooling(years)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/life-expectancy-who/led.csv\")\ndata_df = pd.DataFrame(data)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(data_df.info())\nprint(data_df.describe())\n# data_df.head()\nprint(\"Null values:\\n\", data_df.isnull().sum())\nprint(\"Percent missingness:\\n\", data_df.isnull().sum() / data_df.count())\nprint(\"Shape:\\n\", data_df.shape)\nprint(\"Data Types:\\n\", data_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial thoughts\n\n\n- There are missing values for columns: GDP, Population, Totalexpenditure, HepatitisB, Incomecompositionofresources, and Schooling. I could input missing data or it might be easier to drop some of the columns altogether if they are missing too much data. \n\n\n- After doing some research, with a large dataset, a missingness of up to 40% may even be acceptable. In this case, we may be fine without inputting missing data. \n\n\n- We will be doing linear regression because life expectancy is a continuous number. \n\n"},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix\nNext, I want to run a correlation matrix with all the variables so I can get an understanding on which variables are most influential."},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix = data_df.corr()\ncorrMatrix.style.background_gradient(cmap='plasma', low=.5, high=0).highlight_null('red') # from https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_last5 = data_df[data_df['Year'].isin([2011,2012,2013,2014,2015])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_last5_avg = df_last5.groupby(['Country'],as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check to make sure data is accurate \n\n\nI will average a variable over the last five years for a random country, Italy for example, and check to make sure I have the same value in my new dataframe, df_last5_avg."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_last5[df_last5['Country']=='Italy'])\nprint(df_last5_avg[df_last5_avg['Country']=='Italy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Take a look at the new dataset information\n\n\nI will use the same method as before to check for missing data. Hopefully by only taking the last 5 years for each country, we will have less missing data. \n\n\nA benefit to selecting and averaging the last 5 years of data for each country is that we will have an easier time running models as their is only one record per country now. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percent Missingness:\\n', df_last5_avg.isnull().sum()/df_last5_avg.count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Since there are such a large number of missing values in the Population column, and the correlation is so low (-0.02), I will drop it instead of imputing values. \n\n\n- However, I will want to impute missing values for the GDP column as it has a higher correlation (0.46). I think its best to take the median value of developing and developed countries and insert based on respective status. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_last5_avg.drop(['Population'],1,inplace=True)\ndf_last5_avg.drop(['Year'],1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) # this allows you to see the full dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['GDP']])\ndf_last5_avg['GDP'] = imp.transform(df_last5_avg[['GDP']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['Lifeexpectancy']])\ndf_last5_avg['Lifeexpectancy'] = imp.transform(df_last5_avg[['Lifeexpectancy']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['AdultMortality']])\ndf_last5_avg['AdultMortality'] = imp.transform(df_last5_avg[['AdultMortality']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['Alcohol']])\ndf_last5_avg['Alcohol'] = imp.transform(df_last5_avg[['Alcohol']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['HepatitisB']])\ndf_last5_avg['HepatitisB'] = imp.transform(df_last5_avg[['HepatitisB']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['BMI']])\ndf_last5_avg['BMI'] = imp.transform(df_last5_avg[['BMI']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['Totalexpenditure']])\ndf_last5_avg['Totalexpenditure'] = imp.transform(df_last5_avg[['Totalexpenditure']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['thinness1-19years']])\ndf_last5_avg['thinness1-19years'] = imp.transform(df_last5_avg[['thinness1-19years']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['thinness5-9years']])\ndf_last5_avg['thinness5-9years'] = imp.transform(df_last5_avg[['thinness5-9years']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['Incomecompositionofresources']])\ndf_last5_avg['Incomecompositionofresources'] = imp.transform(df_last5_avg[['Incomecompositionofresources']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan, strategy='median')\nimp = imp.fit(df_last5_avg[['Schooling']])\ndf_last5_avg['Schooling'] = imp.transform(df_last5_avg[['Schooling']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Percent missingness:\\n\", df_last5_avg.isnull().sum() / df_last5_avg.count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some Visualization Before Multiple Linear Regression\n\nI would like to run some visualizations just to get a sense of the dataset and to examine if intuitions are correct"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_last5_avg['AdultMortality']\ny = df_last5_avg['Lifeexpectancy']\nplt.scatter(X,y)\nplt.ylabel('Life Expectancy')\nplt.xlabel('Adult Mortality per 1000 pop.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_last5_avg['GDP']\ny = df_last5_avg['Lifeexpectancy']\nplt.scatter(X,y)\nplt.ylabel('Life Expectancy')\nplt.xlabel('GDP')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit(df_last5_avg[['AdultMortality']],df_last5_avg['Lifeexpectancy'])\nprediction_space = np.linspace(min(df_last5_avg['AdultMortality']),max(df_last5_avg['AdultMortality'])).reshape(-1,1)\nplt.scatter(df_last5_avg['AdultMortality'],df_last5_avg['Lifeexpectancy'],color='yellow')\nplt.plot(prediction_space,reg.predict(prediction_space),color='blue',linewidth=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlated Variables\n\nBefore I run variable selection, I would like to eliminate variables that already have a high correlation. This step will increase the speed of variable selection. \n\nI will follow the model demonstrated on this page.\n\nhttps://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlated_features = set()\ncorrelation_matrix = df_last5_avg.drop('Lifeexpectancy', axis=1).corr()\n\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8: #.iloc method is used to extract rows\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\nprint(correlated_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_last5_avg.drop(['Diphtheria'],1,inplace=True)\ndf_last5_avg.drop(['thinness5-9years'],1,inplace=True)\ndf_last5_avg.drop(['under-fivedeaths'],1,inplace=True)\ndf_last5_avg.drop(['Schooling'],1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_last5_avg.head()\ndf_last5_avg.set_index('Country',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection \n#### RFE: Recursive Feature Elimination from SKLearn\n\nhttps://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n\nThe next step I wanted to do before running a linear regression was to do variable selection. Variable selection is an important part of machine learning becuase it...\n- reduces the chance of overfitting\n- improves the accuracy of your model\n- improves the speed of training the model \n\n\nWith the guidance provided from the link above I will perform feature selection\n\nIn this procedure, I will first get a ranking of variables just to make sure the feature selection is working correctly. Then, I will have the model tell me what is the optimal amount of variables. Lastly, I will use the rfe.fit_transform method using the number of optimal variables as the amount of variables to select. Once it returns the optimal variables, I will plot the coefficients to get a look at their values. These variables will be selected to run the multiple linear regression. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nX = df_last5_avg.drop('Lifeexpectancy',axis=1)\ny = df_last5_avg['Lifeexpectancy']\n\nlr = linear_model.LinearRegression()\nrfe = RFE(estimator=lr, n_features_to_select=8, step=1)\nrfe.fit(X, y)\n\n#print(rfe.get_support)\nprint(rfe.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#no of features\nnof_list=np.arange(1,13)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = linear_model.LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(X.columns)\nmodel = linear_model.LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 7)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)             \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = model.fit(X_rfe,y).coef_  \n_ = plt.plot(coefs)\n_ = plt.xticks(np.arange(7),('AdultMortality', 'Alcohol', 'HepatitisB', 'Totalexpenditure',\n       'HIV/AIDS', 'thinness1-19years', 'Incomecompositionofresources'), rotation=60)\n_ = plt.ylabel('Coefficients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef_dict = dict(enumerate(coefs))\ncoef_dict = {'AdultMortality':coef_dict[0],'Alcohol':coef_dict[1],'HepatitisB':coef_dict[2],\n             'Totalexpenditure':coef_dict[3],'HIV/AIDS':coef_dict[4],\n             'thinness1-19years':coef_dict[5],'Incomecompositionofresources':coef_dict[6]}\ncoef_dict","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncorr = df_last5_avg.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression\n\nI will run two different regression model using Ridge and Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_regress = df_last5_avg[['AdultMortality', 'Alcohol', 'HepatitisB', 'Totalexpenditure',\n       'HIV/AIDS', 'thinness1-19years', 'Incomecompositionofresources','Lifeexpectancy']]\nX = df_regress.drop('Lifeexpectancy',axis=1)\ny = df_regress['Lifeexpectancy']\n\nfrom sklearn.linear_model import Ridge\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)\nridge = Ridge(alpha = 0.1, normalize=True)\nridge.fit(X_train,y_train)\nridge_pred=ridge.predict(X_test)\nridge_pred1=ridge.predict(X_train)\nprint(ridge.score(X_train,y_train))\nprint(ridge.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)\nlasso = Lasso(alpha = 0.1, normalize=True)\nlasso.fit(X_train,y_train)\nlasso_pred=lasso.predict(X_test)\nlasso.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Conclusions\n\nFrom the models we ran, we end up with a very accurate predictions, Ridge R^2 = .89 and Lasso R^2 = .86. By doing feature selection we can also note that the most influential factors on life expectancy are:\n\n{'AdultMortality': -0.04432281224565553,\n 'Alcohol': 0.14676844206224568,\n 'HepatitisB': 0.03433452490962368,\n 'Totalexpenditure': 0.25275165023031065,\n 'HIV/AIDS': -0.3195821205985184,\n 'thinness1-19years': -0.08129266307963838,\n 'Incomecompositionofresources': 21.411124433259623}\n \n- Adult mortality - Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\n- Alcohol - recorded per capita (15+) consumption (in litres of pure alcohol)\n- Hepatitis B - Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\n- total expenditure - General government expenditure on health as a percentage of total government expenditure (%)\n- HIV/AIDS - Deaths per 1 000 live births HIV/AIDS (0-4 years)\n- Thinness 1-19 years - Prevalence of thinness among children and adolescents for Age 10 to 19 (% )\n- Income composition of resources - Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n\nOne reason to why our predictive models are so accurate may be because I reduced the dataset considerably to 192 records. Although, there isn't a problem of overfitting because the training accuracy is very similar to the test accuracy."},{"metadata":{},"cell_type":"markdown","source":"### Project Report\n\n1. The first steps I took, were to look at the dataset and then take a look at the amount of missing values.\n2. Next, I wanted to reduce the dataset so that I only had individual countries, which I did by taking data from the  last five years of each country and then averaging it to make a single record.\n3. Next, I wanted to check the amount of missing data again to see if I needed to impute values or drop columns altogether. \n4. After getting a clean dataset, I ran some visualizations to test my intuition and make sure things are working correctly. \n5. Once that was done, I moved on to removing heavily correlated variables and performed feature selection. \n6. Finally, after feature selection, I ran the linear regressions. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}