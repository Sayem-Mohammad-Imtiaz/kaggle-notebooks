{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport re\nimport numpy as np\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"southpark=pd.read_csv('../input/southparklines/All-seasons.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"southpark.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    \n    text = re.sub(r\"\\n\", \"\",  text)\n    text = re.sub(r\"[-()]\", \"\", text)\n    text = re.sub(r\"\\.\", \" .\", text)\n    text = re.sub(r\"\\!\", \" !\", text)\n    text = re.sub(r\"\\?\", \" ?\", text)\n    text = re.sub(r\"\\,\", \" ,\", text)\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"ohh\", \"oh\", text)\n    text = re.sub(r\"ohhh\", \"oh\", text)\n    text = re.sub(r\"ohhhh\", \"oh\", text)\n    text = re.sub(r\"ohhhhh\", \"oh\", text)\n    text = re.sub(r\"ohhhhhh\", \"oh\", text)\n    text = re.sub(r\"ahh\", \"ah\", text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text=[]\nfor line in southpark.Line:\n    text.append(clean_text(line))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#counting length of each sentence by splitting a sentence into words\nlength=[]\n\nfor line in text:\n    #print(line.split())\n    length.append(len(line.split()))\nlengths = pd.DataFrame(length, columns=['counts'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lengths.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.percentile(lengths, 80))\nprint(np.percentile(lengths, 85))\nprint(np.percentile(lengths, 90))\nprint(np.percentile(lengths, 95))\n\nprint(np.percentile(lengths, 99))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_line_len=30\nshort_text=[]\nfor line in text:\n    if(len(line.split() )<= max_line_len):\n        short_text.append(line)\nshort_text[:5]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab={}\nfor line in short_text:\n    for word in line.split():\n        if word not in vocab:\n            vocab[word]=1\n        else :\n            vocab[word]+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size of vocab is\",len(vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#limit occurance of word, that is used more than 3 times\nthreshold = 3\ncount=0\nfor k,v in vocab.items():\n    \n    if v>=threshold:\n        count+=1\n    else: \n        #print(v)\n        pass\n#count\nprint(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source_vocab_to_int = {}\nword_num=0\nfor k,v in vocab.items():\n    if v >= threshold:\n        source_vocab_to_int[k]=word_num\n        word_num+=1\nlen(source_vocab_to_int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_vocab_to_int={}\nword_num=0\nfor k,v in vocab.items():\n    if v>= threshold:\n        target_vocab_to_int[k]=word_num\n        word_num+=1\nlen(target_vocab_to_int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding essential token to the vocab (dictionary)\ntokens = ['<PAD>','<EOS>','<UNK>','<GO>']\nfor token in tokens:\n    source_vocab_to_int[token]=len(source_vocab_to_int)+1\nfor token in tokens:\n    target_vocab_to_int[token]=len(target_vocab_to_int)+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# int to vocab mapping\nsource_int_to_vocab={v_i:v for v,v_i in source_vocab_to_int.items()}\ntarget_int_to_vocab={v_i:v for v,v_i in target_vocab_to_int.items()}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the length of the dictionaries.\nprint(len(source_vocab_to_int))\nprint(len(source_int_to_vocab))\nprint(len(target_vocab_to_int))\nprint(len(target_int_to_vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating source and  target text\nsource_text = short_text[:-1]\ntarget_text = short_text[1:]\n\nfor i in range(len(target_text)):\n    target_text[i] += ' <EOS>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(source_text))\nprint(len(target_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source_text[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional,GRU,CuDNNLSTM,CuDNNGRU\nfrom keras.models import Model, load_model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_samples=3000\nshort_source=source_text[:3000]\nshort_target=target_text[:3000]\nshort_source_token = [nltk.word_tokenize(sent) for sent in short_source]\nshort_target_token = [nltk.word_tokenize(sent) for sent in short_target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(short_source_token[:5])\nprint(short_target_token[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_size = len(short_source_token)\n\n# We will use the first 0-80th %-tile (80%) of data for the training\ntraining_input  = short_source_token[:round(data_size*(80/100))]\ntraining_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\ntraining_output = short_target_token[:round(data_size*(80/100))]\n\n# We will use the remaining for validation\nvalidation_input = short_source_token[round(data_size*(80/100)):]\nvalidation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\nvalidation_output = short_target_token[round(data_size*(80/100)):]\n\nprint('training size', len(training_input))\nprint('validation size', len(validation_input))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word encoding for dictionary\nvocab = {}\nfor question in short_source_token:\n    for word in question:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1\n\nfor answer in short_target_token:\n    for word in answer:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove rare words\nthreshold = 3\ncount = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        count += 1\nprint(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating dictionaries\nword_code_start=1\nword_code_padding=0\n\n\nword_num=2 # position 1 is left for word_code_start\nencoding = {}\ndecoding = {1 : 'START'}\nfor word, count in vocab.items():\n    if count>=threshold:\n        encoding[word] = word_num\n        decoding[word_num]= word\n        word_num+=1\nprint(\"No. of vocab used : \", word_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word not in dictionary makes as <UNK>\ndecoding[len(encoding)+2] = '<UNK>'\nencoding['<UNK>'] = len(encoding)+2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_size=word_num+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting into vector\ndef transform_into_vector(encoding, data, vector_size=20):\n    transformed_data=np.zeros(shape=(len(data),vector_size))\n    for i in range(len(data)):\n        for j in range(min(len(data[i]),vector_size)):\n            try:\n                transformed_data[i][j] = encoding[data[i][j]]\n            except:\n                transformed_data[i][j] = encoding['<UNK>']\n    return transformed_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\nencoded_training_input = transform_into_vector(\n    encoding, training_input, vector_size=INPUT_LENGTH)\nencoded_training_output = transform_into_vector(\n    encoding, training_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_training_input', encoded_training_input.shape)\nprint('encoded_training_output', encoded_training_output.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_training_input[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding validation dataset\nencoded_validation_input = transform_into_vector(encoding, validation_input,vector_size=INPUT_LENGTH)\nencoded_validation_output = transform_into_vector(encoding, validation_output,vector_size=OUTPUT_LENGTH)\nprint('Encoded_validation_input',encoded_validation_input.shape)\nprint('ENcoded_validation_output',encoded_validation_output.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Building******"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.keras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_LENGTH=20\nOUTPUT_LENGTH=20\nencoder_input = Input(shape=(INPUT_LENGTH,))\ndecoder_input= Input(shape=(OUTPUT_LENGTH,))\nprint(encoder_input.shape)\nprint(decoder_input.shape)\nfrom keras.layers import SimpleRNN\nencoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH,mask_zero=True)(encoder_input)\nencoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\nencoder_last=encoder[:,-1,:]\n\nprint('Encoder',encoder)\nprint('Encoder_last', encoder_last)\n\ndecoder = Embedding(dict_size,128, input_length=OUTPUT_LENGTH,mask_zero=True)(decoder_input)\ndecoder = LSTM(512, return_sequences=True, unroll=True)(decoder,initial_state=[encoder_last, encoder_last])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Activation, dot, concatenate\n\n# Equation (7) with 'dot' score from Section 3.1 in the paper.\n# Note that we reuse Softmax-activation layer instead of writing tensor calculation\nattention = dot([decoder, encoder], axes=[2, 2])\nattention = Activation('softmax', name='attention')(attention)\nprint('attention', attention)\n\ncontext = dot([attention, encoder], axes=[2,1])\nprint('context', context)\n\ndecoder_combined_context = concatenate([context, decoder])\nprint('decoder_combined_context', decoder_combined_context)\n\n# Has another weight + tanh layer as described in equation (5) of the paper\noutput = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\noutput = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\nprint('output', output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(inputs=[encoder_input,decoder_input],outputs=[output])\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_encoder_input = encoded_training_input\ntraining_decoder_input = np.zeros_like(encoded_training_output)\ntraining_decoder_input[:, 1:] = encoded_training_output[:,:-1]\ntraining_decoder_input[:, 0] = word_code_start\ntraining_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n\nvalidation_encoder_input = encoded_validation_input\nvalidation_decoder_input = np.zeros_like(encoded_validation_output)\nvalidation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\nvalidation_decoder_input[:, 0] = word_code_start\nvalidation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_encoder_input.shape)\nprint(training_decoder_output.shape)\nprint(training_decoder_input.shape)\nprint(validation_encoder_input.shape)\nprint(validation_decoder_input.shape)\nprint(validation_decoder_output.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n          #validation_split=0.1,\n          batch_size=128, epochs=100)\n\nfrom keras.models import load_model\n\nmodel.save('model_attention_enc_dec.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('model_attention_enc_dec.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model testing\ndef prediction(raw_text):\n    clean_input = clean_text(raw_text)\n    input_token = [nltk.word_tokenize(clean_input)]\n    input_token = [input_token[0][::-1]] #reversing input token\n    encoder_input = transform_into_vector(encoding, input_token, 20)\n    decoder_input = np.zeros(shape=(len(encoder_input),OUTPUT_LENGTH))\n    decoder_input[:,0] = word_code_start\n    for i in range(1, OUTPUT_LENGTH):\n        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n        decoder_input[:,i] = output[:,i]\n    return output\ndef decode(decoding, vector):\n    \n    text = ''\n    for i in vector:\n        if i == 0:\n            break\n        text += ' '\n        text += decoding[i]\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(100):\n    seq_index = np.random.randint(1, len(short_source))\n    output = prediction(short_source[seq_index])\n    print ('Query:', short_source[seq_index])\n    print ('Bot:', decode(decoding, output[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''raw_input = input()\noutput = prediction(raw_input)\nprint (decode(decoding, output[0]))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would like to thank this [notebook](http://www.kaggle.com/currie32/a-south-park-chatbot) for helping in input pipeline preparation"},{"metadata":{},"cell_type":"markdown","source":"**END******"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}