{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. <a href=\"#intro\"> Introduction </a>\n2. <a href=\"#load\"> Loading data </a>\n3. <a href=\"#exp\"> Exploring Data </a>\n4. <a href=\"#prep\"> Data preparation </a>\n5. <a href=\"#model\"> Build Model </a>\n    * <a href=\"#first\"> First experiment </a>\n    * <a href=\"#cross\"> Cross validation </a>\n    * <a href=\"#grid\"> Grid Search Cross Validation</a>\n    * <a href=\"#final\"> Final training </a>\n6. <a href=\"#eval\"> Evaluate Model </a>","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"intro\"> Introduction </a>","metadata":{}},{"cell_type":"markdown","source":"_Dataset information:_ https://archive.ics.uci.edu/ml/datasets/haberman's+survival\n\nThe dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\n\n\n_Attribute Information:_\n\n1. Age of patient at time of operation (numerical)\n2. Patient's year of operation (year - 1900, numerical)\n3. Number of positive axillary nodes detected (numerical)\n4. Survival status (class attribute)\n   * 1 = the patient survived 5 years or longer\n   * 2 = the patient died within 5 year\n   \nOur task here will be to build a model to predict if a patient will survive to breast cancer surgery, given these 3 available features.\n\nIMPORTANT NOTE: By any chance I am trying to solve a \"real world\" breast cancer survival prediction, and this notebook should not be used for any medical application. This is just a brief classification problem exploration.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"load\">Loading data</a>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, fbeta_score\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = ['age', 'year_operation', 'axillary_nodes', 'survival']\ndf = pd.read_csv(\"/kaggle/input/habermans-survival-data-set/haberman.csv\", names=names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"exp\"> Exploring Data </a>","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are indeed only 3 predictive features and 306 sample rows. Probably, we can deal with this problem using a small network, with a small batch size and using some regularization strategy, in order to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, hue=\"survival\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(df.iloc[:,:-1]))\nX_train.columns = df.iloc[:,:-1].columns\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\nsns.boxplot(data=X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_age_ seems to have a gaussian like distribution, and _auxiliary nodes_ looks more like an exponential one.\n\nAlso, our target feature - _survival_ - is clearly imbalanced: there is much more samples from class 1 than class 2. Let's check how imbalanced it is.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'survived_qty': df.survival.value_counts(), 'survived_pct': round(df.survival.value_counts()/306,3)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"~73% percent (225 samples) are from class 1, i.e. patients that survived 5 years or longer.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"prep\">Data Preparation</a>","metadata":{}},{"cell_type":"code","source":"# split into input and target\nX, y = df.values[:, :-1], df.values[:, -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensure all data are floating point\nX = X.astype('float32')\n\n# label encode strings to 0/1\ny = LabelEncoder().fit_transform(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=3)\n\n# set number of input features\nn_features = X_train.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"model\"> Build Model </a>","metadata":{}},{"cell_type":"markdown","source":"### <a id=\"first\"> First experiment </a>","metadata":{}},{"cell_type":"markdown","source":"Let's build a simple MLP, just to have a feeling on the behaviour of such a model applied to our problem, with architecture and parameters chosen arbitrarily.","metadata":{}},{"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_features,)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='adam', \n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()\n\nhistory = model.fit(X_train, \n                    y_train, \n                    epochs=200, \n                    batch_size=16, \n                    verbose=0, \n                    validation_data=(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses[['loss','val_loss']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses[['accuracy','val_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n# classification_report\nprint(classification_report(y_test, predictions))\n\n# confusion matrix\npd.DataFrame(confusion_matrix(y_test, predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This simple test, without any tunning, achieved 77% accuracy on test set and an acceptable learning curve. As the accuracy obtained is above the percentage of 73% (class 1 full dataset share), there is an indicative that our approach is promising at solving this task.","metadata":{}},{"cell_type":"markdown","source":"### <a id=\"cross\">Cross validation</a>","metadata":{}},{"cell_type":"markdown","source":"Let's apply a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\" target='_blank'>stratified k-fold cross-validation</a>, to have a more reliable estimate of our model performance. i.e. we are going to fit k models to our data and check mean accuracy.","metadata":{}},{"cell_type":"code","source":"# set 10-fold pertinent objects\nkfold = StratifiedKFold(10)\nscores = list()\nn_features = X.shape[1]\n\n# perform kfold\nfor fold, (train_K, test_K) in enumerate(kfold.split(X, y)):\n    # split data\n    X_train, X_test, y_train, y_test = X[train_K], X[test_K], y[train_K], y[test_K]\n\n    # define model (same as before)\n    model = create_model()\n\n    # fit model\n    model.fit(X_train, y_train, epochs=200, batch_size=16, verbose=0)\n\n    # predict in test set\n    predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n\n    # evaluate predictions\n    score = accuracy_score(y_test, predictions)\n\n    print(f\"fold {fold+1}, score: {round(score,2)}\")\n    scores.append(score)\n    # summarize all scores\nprint(f\"Mean Accuracy: {round(np.mean(scores),2)} ({round(np.std(scores),2)})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"grid\"> Grid Search CV </a>","metadata":{}},{"cell_type":"markdown","source":"In order to optimize the hyperparameters of our model, let's use grid search capability from scikit-learn library (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" target=\"_blank\">GridSearchCV class</a>) to tune Keras deep learning models.\n\nKeras models can be used along scikit-learn by wrapping them with the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn/KerasClassifier\" target=\"_blank\">KerasClassifier</a>. We just have to define a Keras model function and pass it to the build_fn KerasClassifier argument. Let's go and build the framework to optimize epochs and batch_size:","metadata":{}},{"cell_type":"code","source":"model = KerasClassifier(build_fn=create_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set metric scores to monitor in gridsearch (need to use scikit-learn make_scorer function)\n\nmy_scores = {'accuracy' :make_scorer(accuracy_score),\n             'recall'   :make_scorer(recall_score),\n             'precision':make_scorer(precision_score),\n             'f1'       :make_scorer(fbeta_score, beta = 1)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set dict with parameter to grid\nparam_grid = dict(epochs=[100,150,200,250], batch_size=[8,16,32,64])\n\ngrid = GridSearchCV(estimator=model,\n                    param_grid=param_grid,\n                    n_jobs=-1,\n                    refit='accuracy',\n                    scoring = my_scores,\n                    cv=3,\n                    verbose=0)\n\ngrid_result = grid.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the best parameters and score (based on refit argument) can now be accessed:","metadata":{}},{"cell_type":"code","source":"print(grid_result.best_score_)\nprint(grid_result.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And in a more detailed analysis:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(grid_result.cv_results_).columns.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting a dataframe with top 5 results from gridsearchCV\n\npd.DataFrame(grid_result.cv_results_)[['params',\n                                       'mean_test_accuracy',\n                                       'mean_test_recall',\n                                       'mean_test_precision',\n                                       'mean_test_f1',\n                                       'rank_test_accuracy']].sort_values('rank_test_accuracy').head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"final\">Final training</a>","metadata":{}},{"cell_type":"markdown","source":"Our mean accuracy, according to the applied cross validation strategy, is now about ~77%.\n\nA deeper discussion with domain experts should provide a guidance on tha acceptance of this performance level. Assuming that this result is acceptable, the next step is to train our verified model on full dataset, using the optimized parameters found ('batch_size': 8, 'epochs': 250).","metadata":{}},{"cell_type":"code","source":"X, y = df.values[:, :-1], df.values[:, -1]\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y)\nn_features = X_train.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BS = grid_result.best_params_['batch_size']\nep = grid_result.best_params_['epochs']\n\nmodel.fit(X,y, epochs=ep, batch_size=BS, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"eval\"> Evaluate Model </a>","metadata":{}},{"cell_type":"markdown","source":"Now, we are in a position to apply the trained model to make predictions on new data.\n\nTo simulate a new data input, let's choose an aleatory sample from our dataset:","metadata":{}},{"cell_type":"code","source":"random_ind = random.randint(0,len(X))\n\nnew_data = X[random_ind]\nexp_out = y[random_ind]\npred = (model.predict(new_data.reshape(1,3)) > 0.5).astype('int32')[0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\n new_data: {new_data}\\n \\n expected output: {exp_out} \\n \\n predicted output: {pred}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}