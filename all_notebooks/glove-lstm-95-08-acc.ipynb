{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement","metadata":{}},{"cell_type":"markdown","source":"1. We are given a dataset consisting of two csv files train_bodies.csv which contains the set of news articles bodies,while train-stances.csv resembles the articles for each of these bodies being identified using the body id.\n\n2. After training from these samples we need to detect whether the given headline agrees,disagrees,discusses,unrelated with the body id\n","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical,plot_model\n\nfrom keras.models import Input,Model,Sequential\nfrom keras.layers import LSTM,Embedding,Dropout,Activation,Reshape,Dense,GRU,Add,Flatten,concatenate,Bidirectional\n\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.utils import to_categorical,plot_model\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import ModelCheckpoint\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset understanding\nThe train_bodies contain the entries for the body id and associated article Body\nThe train_stances contain the entries for the headlines associated with the particular body id and its labelled stance\nOne body present in train_bodies can have multiple associated headlines present in train_stances and it's corresponding stance label\n1683 :- Number of article Body present\n49972 number of total headlines present for the 1683 different article body","metadata":{}},{"cell_type":"markdown","source":"## Dataset Preparation\n\n**train_bodies.csv** contains body id and article body for training  \n**train_stances.csv** contains headlines corresponding to body id and associated labelled stance with it\n","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = \"../input/fake-news-challenge/\"\n\ntrain_bodies = pd.read_csv(os.path.join(DATASET_PATH,'train_bodies.csv'))\n# train_bodies.head()\ntrain_stance = pd.read_csv(os.path.join(DATASET_PATH,'train_stances.csv'))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combining the CSV\n\nI am preparing a final csv in each row will correspond to a unique entry\ni.e each row will correspond to a unique combination of headline,bodyid and article body \n\nThe above is needed for making simplicity in further data preparation steps we need to execute\n","metadata":{}},{"cell_type":"code","source":"# Run commented code to combine the two csv file{train_bodies.csv,train_stances.csv} into data_combined.csv file\nfrom tqdm.notebook import tqdm\ncount=0\nfor i in tqdm(range(train_stance.shape[0])):\n    for j in range(train_bodies.shape[0]):\n        if train_bodies.loc[j,'Body ID']==train_stance.loc[i,'Body ID']:\n            train_stance.loc[i,'articleBody'] = train_bodies.loc[j,'articleBody']\n\n\ntrain_stance.to_csv(os.path.join(os.getcwd(),'data_combined.csv'),index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(os.path.join(os.getcwd(),'data_combined.csv'))#generated from Fake News stanford.ipynb\ndata.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['stance_cat'] = data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\ndata['Stance'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = np.r_[data['Headline'].values,data['articleBody'].values]\nprint(49972*2)\nprint(len(corpus)) # first 49972 contains the Headline and next 49972 contains the articleBody\n\nvocabulary = []\nfor sentence in corpus:\n    vocabulary.extend(sentence.split(' '))\n\nvocabulary = list(set(vocabulary))\nvocab_length = len(vocabulary)\nprint(\"Vocabulary Length is {0}\".format(vocab_length))\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training Parameters","metadata":{}},{"cell_type":"code","source":"max_features = 5000\nMAX_NB_WORDS = 24000\nEMBEDDING_DIM = 50\nMAX_SEQUENCE_LENGTH = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Embedding Matrix For Headline and Body\n\nWe create Emebdding Matrix for headline and Body to be served as a first layer of Deep learning Model","metadata":{}},{"cell_type":"code","source":"GLOVE_DIR = \"../input/glove50d/\"\ndef setup_embedding_index():\n    embedding_index=dict()\n    f = open(os.path.join(GLOVE_DIR,\"glove.6B.50d.txt\"),encoding='utf-8')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.array(values[1:],dtype='float32')\n        embedding_index[word] = coefs\n    f.close()\n    return embedding_index\nembeddings_index = setup_embedding_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Padding headline and body \n\nWe pad the headline into length of 16 as headline is of shorter length and body into length of 48 as observed best performing parameter for body is 48.","metadata":{}},{"cell_type":"code","source":"tokenizer_headline = Tokenizer(num_words=max_features, split=' ')\ntokenizer_headline.fit_on_texts(data.loc[:,'Headline'].values)\nvocab_headline_length = len(tokenizer_headline.word_index)+1\n\nencoded_docs_headline = tokenizer_headline.texts_to_sequences(data.loc[:,'Headline'])\npadded_docs_headline = pad_sequences(encoded_docs_headline, maxlen=16, padding='post')\n\nprint(vocab_headline_length)\nword_index_headline = tokenizer_headline.word_index\n\nNUM_WORDS_HEADLINE = vocab_headline_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_body = Tokenizer(num_words=max_features, split=' ')\ntokenizer_body.fit_on_texts(data.loc[:,'articleBody'].values)\nvocab_body_length = len(tokenizer_body.word_index)+1\n\nencoded_docs_body = tokenizer_body.texts_to_sequences(data.loc[:,'articleBody'])\npadded_docs_body = pad_sequences(encoded_docs_body, maxlen=48, padding='post')\n\nprint(vocab_body_length)\nword_index_body = tokenizer_body.word_index\n\n\nNUM_WORDS_BODY = vocab_body_length\nprint(NUM_WORDS_BODY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nembedding_matrix_headline = np.zeros((NUM_WORDS_HEADLINE, EMBEDDING_DIM))\n\nfor word, i in tokenizer_headline.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_headline[i] = embedding_vector\ndims = len(embedding_matrix_headline[0])\n\nprint(dims)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nembedding_matrix_body = np.zeros((NUM_WORDS_BODY, EMBEDDING_DIM))\n\nfor word, i in tokenizer_body.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_body[i] = embedding_vector\ndims = len(embedding_matrix_body[0])\n\nprint(dims)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(padded_docs_headline.shape)\nprint(padded_docs_body.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"input_headline = Input(shape=16,name='input_headline')\nembedding_layer_headline = Embedding(input_dim = vocab_headline_length,output_dim = 50,\n                                     weights=[embedding_matrix_headline],\n                                     input_length = 16,trainable=True)(input_headline)\n\n# lstm_headline = LSTM(units=16)(embedding_layer_headline)\n\ninput_body = Input(shape=48,name='input_body')\nembedding_layer_body = Embedding(input_dim = vocab_body_length,output_dim = 50,weights = [embedding_matrix_body],\n                                 input_length=48,trainable = True)(input_body)\nlstm_body = LSTM(units=48)(embedding_layer_body)\n\naddition_layer = concatenate([embedding_layer_headline,embedding_layer_body],axis=1)\n\n# addition_layer = concatenate([lstm_headline,lstm_body],axis=1)\nlstm = LSTM(units=64,)(addition_layer)\ndrop = Dropout(0.25)(lstm)\n# dense = Dense(64,activation='relu')(drop)\n# flatten = Flatten()(addition_layer)\n\noutput = Dense(4,activation='sigmoid')(drop)\n\nmodel = Model(inputs=[input_headline,input_body],outputs=output)\n# from keras.optimizers import SGD\n# sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n\n# model.compile(loss = \"categorical_crossentropy\", optimizer = sgd,metrics = ['accuracy'])\n\nmodel.compile(optimizer = 'adam',loss ='categorical_crossentropy',metrics = ['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file='model_glove_lstm.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_docs_headline_train = padded_docs_headline[:int(len(padded_docs_headline)*0.9),:]\npadded_docs_headline_test = padded_docs_headline[int(len(padded_docs_headline)*0.9):,:]\n\npadded_docs_body_train = padded_docs_body[:int(len(padded_docs_body)*0.9),:]\npadded_docs_body_test = padded_docs_body[int(len(padded_docs_body)*0.9):,:]\n\nlabels = to_categorical(data.loc[:,'stance_cat'])\n\nlabels_train = labels[:int(len(labels)*0.9),:]\nlabels_test = labels[int(len(labels)*0.9):,:]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Checkpoints \n\nFor saving the latest model trained after every epoch","metadata":{}},{"cell_type":"code","source":"# MODELS_DIR = os.path.join(\"/home/abhinav/fake_news_challenge/model/glove_lstm\")\nfilepath = os.path.join(os.getcwd(),\"{epoch:02d}-{val_accuracy:.2f}.hdf5\")\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"model_history = model.fit([padded_docs_headline_train,padded_docs_body_train],labels_train,epochs=40,shuffle=True,verbose=1,\n                          validation_data=([padded_docs_headline_test,padded_docs_body_test],labels_test),\n                                          callbacks=[checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training History","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\nax1.plot(model_history.history['loss'], color='b', label=\"Training loss\")\nax1.plot(model_history.history['val_loss'], color='r', label=\"validation loss\")\nax1.set_xticks(np.arange(1, 40, 1))\nax1.set_yticks(np.arange(0, 1, 0.1))\n\nax2.plot(model_history.history['accuracy'], color='b', label=\"Training accuracy\")\nax2.plot(model_history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, 40, 1))\n\nlegend = plt.legend(loc='best', shadow=True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}