{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solo carga de datos"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")\ndata_test = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\ndata_train['label'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_train, data_val = train_test_split(data_train,train_size=0.8)\n\ndata_val.info()\ndata_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cambios, dejo como 1 mi digito verificador (3) y todo los demas como 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['label'].replace(to_replace=[0,1,2,3,4,5,6,7,8,9],value=[0,0,0,1,0,0,0,0,0,0],inplace =True)\ndata_train['label'].unique()\n\ndata_val['label'].replace(to_replace=[0,1,2,3,4,5,6,7,8,9],value=[0,0,0,1,0,0,0,0,0,0],inplace =True)\ndata_val['label'].unique()\n\ndata_test['label'].replace(to_replace=[0,1,2,3,4,5,6,7,8,9],value=[0,0,0,1,0,0,0,0,0,0],inplace =True)\ndata_test['label'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solo separando datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = data_train.iloc[:,1:].values.astype('float32')\ny_train = data_train.iloc[:,0].values.astype('int32')\n\nx_val = data_val.iloc[:,1:].values.astype('float32')\ny_val = data_val.iloc[:,0].values.astype('int32')\n\nx_test = data_test.iloc[:,1:].values.astype('float32')\ny_test = data_test.iloc[:,0].values.astype('int32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_train son para entrenar\n\n_val son para validar\n\n_test son para probar"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Datos para entrenar: \", x_train.shape[0])\nprint(\"Datos para validar: \",  x_val.shape[0])\nprint(\"Datos para probar: \",   x_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solo visualizacion"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i in range(40):\n    plt.subplot(8,5,i+1)\n    plt.imshow(np.reshape(x_val[i],(28,28)),cmap='gray')\n    plt.title(\"Clase:{}\".format(y_val[i]))\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalizacion (Por ahi lo llaman buenas practicas)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train/255\n\nx_val = x_val/255\n\nx_test = x_test/255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Codificacion One-hot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\nnum_clases = 2\nY_train = np_utils.to_categorical(y_train,num_clases)\nY_val = np_utils.to_categorical(y_val,num_clases)\nY_test = np_utils.to_categorical(y_test,num_clases)\nprint(y_train.shape, \" \",Y_train.shape)\nprint(y_val.shape, \" \",Y_val.shape)\nprint(y_test.shape, \" \",Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basicamente la red neuronal con los parametros que pidio el profe\n\nEpocas: 100\n\nCapas ocultas: 1\n\nCantidad de neuronas en la capa oculta: 25\n\nFuncion de activacion de capa oculta: sigmoid\n\nEntradas en la capa oculta: 784 (La imagen se compone de 28x28 celdas en el archivo.csv, por lo tanto las columnas en el archivo.csv son 1x1,1x2,1x3...,1x27,1x28,2x1,2x2,2x3,...,2x27,2x28,...,28x1,...,28x27,28x28  y la cantidad de columnas es 784 = 28x28)\n\nCapa de salida: 2 neuronas (Es un 1 o es un 0)\n\nFuncion de activacion en capa de salida: softmax\n\nEarlyStopping: 15 que monitorea loss de validacion (val_loss)\n\nFuncion de perdida: Cross Entropy (Como: **es** 3 o **no es** 3, utilizamos binary_crossentropy, si fueran mas numeros y si utilizamos One-hot encoding se utiliza categorical_crossentropy, el otro ni idea para cuando se utiliza)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras import activations, initializers\nfrom keras.optimizers import SGD, Adam\nfrom keras.callbacks import EarlyStopping\n\nEpocas = 100\n\n\nMLP = Sequential()\n\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=15)\n\noptim = SGD(lr=0.1)\n\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Historia = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val)\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solo visualizacion"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,nice in enumerate(Correctos[:100]):\n    plt.subplot(10,10,i+1)\n    plt.imshow(x_test[nice].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\n              \"Predict: {}, Real: {}\".format(resultado[nice],\n                                            y_test[nice]))\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,nice in enumerate(Incorrectos[:100]):\n    plt.subplot(10,10,i+1)\n    plt.imshow(x_test[nice].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\n              \"Predict: {}, Real: {}\".format(resultado[nice],\n                                            y_test[nice]))\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graficos"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cambiando funcion objetivo a MSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.losses import MeanSquaredError\n\nMLP = Sequential()\n\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=15)\n\noptim = SGD(lr=0.1)\n\nMLP.compile(loss=MeanSquaredError(), optimizer = optim, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Historia = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val)\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LR = 0.01"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=15)\n\noptim = SGD(lr=0.01)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,nice in enumerate(Correctos[:100]):\n    plt.subplot(10,10,i+1)\n    plt.imshow(x_test[nice].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\n              \"Predict:{}, Real:{}\".format(resultado[nice],\n                                            y_test[nice]))\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,nice in enumerate(Incorrectos[:100]):\n    plt.subplot(10,10,i+1)\n    plt.imshow(x_test[nice].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\n              \"Predict:{}, Real:{}\".format(resultado[nice],\n                                            y_test[nice]))\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solo graficos"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LR = 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=15)\n\noptim = SGD(lr=1)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LR = 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=15)\n\noptim = SGD(lr=10)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Red neuronal con units = 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 1, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=1000)\n\noptim = SGD(lr=0.1)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\n\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Red neuronal con units = 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 10, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=1000)\n\noptim = SGD(lr=0.1)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\n\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Red neuronal con units = 25"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=1000)\n\noptim = SGD(lr=0.1)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\n\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Red neuronal con units = 100"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 100, kernel_initializer= pesos_inicial, input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=1000)\n\noptim = SGD(lr=0.1)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\n\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utilizando ADAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"Epocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial , input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=15)\n\noptim = Adam(lr=0.1)\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,nice in enumerate(Correctos[:100]):\n    plt.subplot(10,10,i+1)\n    plt.imshow(x_test[nice].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\n              \"Predict:{}, Real:{}\".format(resultado[nice],\n                                            y_test[nice]))\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,nice in enumerate(Incorrectos[:100]):\n    plt.subplot(10,10,i+1)\n    plt.imshow(x_test[nice].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\n              \"Predict:{}, Real:{}\".format(resultado[nice],\n                                            y_test[nice]))\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Con weight decay (weight regularizer)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.regularizers import l2\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, kernel_regularizer=l2() , input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=15)\n\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nEpocas = 100\n\nMLP = Sequential()\n#Distribucion para inicializar los pesos\npesos_inicial = initializers.RandomUniform(minval=-0.2,maxval=0.2)\n#capa oculta\nMLP.add(Dense(units= 25, kernel_initializer= pesos_inicial, kernel_regularizer=l2() , input_dim= 784, activation= activations.sigmoid))\n#output\nMLP.add(Dense(2, activation = activations.softmax))\n\nearlyStopping = EarlyStopping(monitor='val_loss',patience=1000)\n\nMLP.compile(loss='categorical_crossentropy', optimizer = optim, metrics=['accuracy'])\nHistoria = MLP.fit( x_train, Y_train, \n                    epochs=Epocas,\n                    batch_size=32, #mini batch\n                    callbacks=[earlyStopping], #Ojala funcione\n                    validation_data = (x_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultado = MLP.predict_classes(x_test)\nCorrectos = np.nonzero(resultado == y_test)[0]\nIncorrectos = np.nonzero(resultado != y_test)[0]\n\nprint(\"Set de entrenamiento: \", y_test.shape[0])\nprint(\"Acertados: \",len(Correctos))\nprint(\"No acertados: \",len(Incorrectos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['loss'], label='training data')\nplt.plot(Historia.history['val_loss'], label='validation data')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.plot(Historia.history['accuracy'], label='accuracy')\nplt.plot(Historia.history['val_accuracy'], label='validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}