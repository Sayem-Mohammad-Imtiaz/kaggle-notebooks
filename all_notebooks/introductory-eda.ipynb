{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **QUALITATIVE ANALYSIS OF TWEETS ON VACCINATION** \n\nA project geared towards a qualitative analysis of historical data from Twitter on Vaccination.\nby Dr William Kane Olwit\n\n## **WHY SHOULD WE BE CONCERNEDABOUT TWEETS ON VACCINATION?**\n\n### **The fatal impact of vaccine hesitancy**\n\nIn 2018, there were over 82,000 cases of measles confirmed in the EU – three times more than in 2017 – and measles led to 72 deaths. Cases of measles are affecting all unvaccinated groups, adults and children alike, with large numbers of cases and fatalities in countries which had previously eliminated the disease.\nVaccine hesitancy is a key reason for this worrying trend. Europe is the most vaccine-hesitant region in the world, and we are now witnessing the results. Last year’s wide-ranging survey of vaccine confidence in Europe, led by Heidi Larson and her colleagues from the Vaccine Confidence Project, found that the picture in the EU is complex with varying levels of vaccine confidence between countries.\n\n### **The role of social media**\n\nAt the core of social media is the ability for us to share ideas and content with our peers. While this freedom of information is what makes social media so appealing, it is also what can make it dangerous. Social media is not the cause of vaccine hesitancy, but it has certainly played a role in making anti-vaccination arguments and pseudoscience accessible to a wider audience.\n\n### **Data Gathering:**\n\nWe collected all tweets containing  the search string: vaccination. Along with the tweet text, we downloaded the date and time when the tweet was published, and the location of the user (if provided). We also downloaded the user id, follower ids, and friends ids. The followers of a user A are those users who will receive messages from user A. The friends of a user A are those users from whom user A receives messages. Thus, information flows from a user to his followers. We collected tweets using the open source information tool, TWINT.(https://github.com/twintproject) and a python algorithm.\n\nIn contrast to the open Twitter Search API, which only allows one to query tweets posted within the last seven days, Twint makes it possible to collect a much larger sample of Twitter posts, ranging several years. We queried Twint for different key terms that relate to the topic of vaccination ranging from the year 2006 to 30th of November 2019 and stored in an aggregated CSV file.\n\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# **Loading Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re # for regular expressions\nimport pandas as pd \npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk # for text manipulation\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nLet's read train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/twitter-vaccination-dataset/vaccination2.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Text PreProcessing and Cleaning**\n### **Data Inspection** \n\nLet's check out  tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dataset size:',df.shape)\nprint('Columns are:',df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset has 89,973 rows and 31 columns.\n\nFrom our case we are only interested in the `tweet`, `hashtags` and `date` columns. We shall drop the rest for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the drop columns function to streamline the dataset\ndf = df.drop(columns=['id', 'time','user_id','username','conversation_id','created_at','timezone', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'likes_count', 'cashtags', 'link', 'retweet','retweets_count', 'quote_url', 'video', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date'])\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dataset size:',df.shape)\nprint('Columns are:',df.columns)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert data to datetime and strings for manipulation.\ndf[\"tweet\"]= df[\"tweet\"].astype(str)\ndf['date']= pd.to_datetime(df['date'], infer_datetime_format=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will only work on a sample of the dataset to make the execeution run quicker (10% Of the total dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac=.1, random_state=1111)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-processing\n\nThe preprocessing of the text data is an essential step as it makes the raw text ready for mining, i.e., it becomes easier to extract information from the text and apply machine learning algorithms to it. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don’t carry much weightage in context to the text.\n\n### Characteristic features of Tweets \n\nFrom the perspective of Sentiment\nAnalysis, we discuss a few characteristics of Twitter:\n\n**Length of a Tweet**\n     The maximum length of a Twitter message is 140 characters. This means that we can practically consider a tweet to be a single sentence, void of complex grammatical constructs. This is a vast difference from traditional subjects of Sentiment Analysis, such as movie reviews. \n     \n**Language used**\n     Twitter is used via a variety of media including SMS and mobile phone apps. Because of this and the 140-character limit, language used in Tweets tend be more colloquial, and filled with slang and misspellings. Use of hashtags also gained popularity on Twitter and is a primary feature in any given tweet. \n     \n**Data availability**\n     Another difference is the magnitude of data available. With the Twitter API, it is easy to collect millions of tweets for training. There also exist a few datasets that have automatically and manually labelled the tweets. \n     \n**Domain of topics**\n     People often post about their likes and dislikes on social media. These are not all concentrated around one topic. "},{"metadata":{},"cell_type":"markdown","source":"### Cleaning The Data\nWhen dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n\nWith text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n\n### Common data cleaning steps on all text:\n\nMake text all lower case\nRemove punctuation\nRemove numerical values\nRemove common non-sensical text (/n)\nTokenize text\nRemove stop words\nMore data cleaning steps after tokenization:\n\nStemming / lemmatization\nParts of speech tagging\nCreate bi-grams or tri-grams\nDeal with typos\n\n### Specific Tweet oriented cleaning using the  tweet-preprocessor module\n\n### A) Removing Twitter Handles (@user)\n\nAs mentioned above, the tweets contain lots of twitter handles (@user), that is how a Twitter user acknowledged on Twitter. We will remove all these twitter handles from the data as they don’t convey much information.\n\n### B) Removing Punctuations,Links, Numbers, and Special Characters\n\nAs discussed, punctuations, numbers and special characters do not help much. It is better to remove them from the text just as we removed the twitter handles.\n\n### C) Tokenization\nTokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n\n### D) Stemming\nStemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word. For example, For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import re\n\nMIN_YEAR = 1900\nMAX_YEAR = 2100\n\n\ndef get_url_patern():\n    return re.compile(\n        r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n\n\ndef get_emojis_pattern():\n    try:\n        # UCS-4\n        emojis_pattern = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n    except re.error:\n        # UCS-2\n        emojis_pattern = re.compile(\n            u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')\n    return emojis_pattern\n\n\ndef get_hashtags_pattern():\n    return re.compile(r'#\\w*')\n\n\ndef get_single_letter_words_pattern():\n    return re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])')\n\n\ndef get_blank_spaces_pattern():\n    return re.compile(r'\\s{2,}|\\t')\n\n\ndef get_twitter_reserved_words_pattern():\n    return re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n\n\ndef get_mentions_pattern():\n    return re.compile(r'@\\w*')\n\n\ndef is_year(text):\n    if (len(text) == 3 or len(text) == 4) and (MIN_YEAR < len(text) < MAX_YEAR):\n        return True\n    else:\n        return False\n\n\nclass TwitterPreprocessor:\n\n    def __init__(self, text: str):\n        self.text = text\n\n    def fully_preprocess(self):\n        return self \\\n            .remove_urls() \\\n            .remove_mentions() \\\n            .remove_hashtags() \\\n            .remove_twitter_reserved_words() \\\n            .remove_punctuation() \\\n            .remove_single_letter_words() \\\n            .remove_blank_spaces() \\\n            .remove_stopwords() \\\n            .remove_numbers()\n\n    def remove_urls(self):\n        self.text = re.sub(pattern=get_url_patern(), repl='', string=self.text)\n        return self\n\n    def remove_punctuation(self):\n        self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n        return self\n\n    def remove_mentions(self):\n        self.text = re.sub(pattern=get_mentions_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_hashtags(self):\n        self.text = re.sub(pattern=get_hashtags_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_twitter_reserved_words(self):\n        self.text = re.sub(pattern=get_twitter_reserved_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_single_letter_words(self):\n        self.text = re.sub(pattern=get_single_letter_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_blank_spaces(self):\n        self.text = re.sub(pattern=get_blank_spaces_pattern(), repl=' ', string=self.text)\n        return self\n\n    def remove_stopwords(self, extra_stopwords=None):\n        if extra_stopwords is None:\n            extra_stopwords = []\n        text = nltk.word_tokenize(self.text)\n        stop_words = set(stopwords.words('english'))\n\n        new_sentence = []\n        for w in text:\n            if w not in stop_words and w not in extra_stopwords:\n                new_sentence.append(w)\n        self.text = ' '.join(new_sentence)\n        return self\n\n    def remove_numbers(self, preserve_years=False):\n        text_list = self.text.split(' ')\n        for text in text_list:\n            if text.isnumeric():\n                if preserve_years:\n                    if not is_year(text):\n                        text_list.remove(text)\n                else:\n                    text_list.remove(text)\n\n        self.text = ' '.join(text_list)\n        return self\n\n    def lowercase(self):\n        self.text = self.text.lower()\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean tweets and append to new column\ntweets = df['tweet']\nclean_tweets = []\nfor tweet in tweets:\n    c = TwitterPreprocessor((tweet))\n    c.fully_preprocess()\n    c = c.text\n    clean_tweets.append(c)\n    \ndf['clean_tweets'] = clean_tweets \ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Story Generation and Visualization from Tweets**\n### **A) Understanding the common words used in the tweets: WordCloud**\n\nNow we want to understand the common words by plotting wordclouds.\n\nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet’s visualize all the words our data using the wordcloud plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nall_words = ' '.join([text for text in df['clean_tweets']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Sentiment Analysis Using VADER**\n\nCreating your own sentiment analysis model from scratch can be very difficult and tedious for a few reason. You need to find relevant data to your problem, create a LOT of labeled data for training, and you must perform data clean up and NLP pre-processing. Luckily for us, VADER is a readily available pre-trained sentiment analysis that thrives on social media data. \nSome of the big advantages include:\n1. Analysis of polarity (positive or negative sentiment) as well as valence (intensity of the sentiment — i.e. ‘excellent’ has a higher intensity than ‘good’).\n2. Handles slang (‘lol’, ‘sux’) and emojis, which are prevalent in tweets\n3. Accounts for capital letters and punctuation (i.e. ‘GOOD!!’ is more positive than ‘good’)\n\nFor more information on VADER you can access the [github repository](https://github.com/cjhutto/vaderSentiment) or the [paper written by the authors](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf). \nFor other sentiment analysis tools you can check out this [github page.](https://github.com/laugustyniak/awesome-sentiment-analysis).\n\nThe first analysis we are going to do is to plot a histogram of all of the sentiment scores we collected on our tweets. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download(\"vader_lexicon\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create analyzer object \nanalyzer = SentimentIntensityAnalyzer()\n\n# get a list of scores and plot\nscores = [analyzer.polarity_scores(tweet)['compound'] for tweet in df['clean_tweets']]\nplt.hist(scores, bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VADER gives back 4 types of polarity scores for every call: Positive, negative, neutral or compound. \n\nIn our code, we only consider the compound score  which is a combination of the other three plus some additional rules and a normalization between -1 and 1. \n\nOne thing to notice from our histogram is that many tweets have a neutral score, and there are only slightly more negative ones in this sample.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment = df['clean_tweets'].apply(lambda x: analyzer.polarity_scores(x))\ndf = pd.concat([df,sentiment.apply(pd.Series)],1)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Sentiment\nFirst let’s just call `df.describe()` and get some basic information on our dataset now ."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the compound score we can see on average tweets are neutral, with a mean sentiment of .001.\n\nPlotting this data will give us a better idea of what it looks like. Before we plot we make a few changes to the dataframe for ease of use, sorting all the values by timestamp so they’re in order, copying the timestamp to the index to make graphing easier, and calculating an expanding and rolling mean for compound sentiment scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.index = pd.to_datetime(df['date'])\ndf = df.sort_index()\ndf['mean'] = df['compound'].expanding().mean()\ndf['rolling'] = df['compound'].rolling('1d').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now using matplotlib, with import matplotlib.pyplot as plt, we can create a quick chart of our tweets and their sentiment over time."},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\n\nfig = plt.figure(figsize=(20,5))\nax = fig.add_subplot(111)\nax.scatter(df['date'],df['compound'], label='Tweet Sentiment')\nax.plot(df['date'],df['rolling'], color ='r', label='Rolling Mean')\nax.plot(df['date'],df['mean'], color='g', label='Expanding Mean')\n#ax.set_xlim([dt.date(2019,6,15),dt.date(2019,10,15)])\nax.set(title='Vaccination Tweets over Time', xlabel='Date', ylabel='Sentiment')\nax.legend(loc='best')\nfig.tight_layout()\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s try to tackle things one at a time here. First let’s look at those tweets with a sentiment of 0. Seborn’s distplot is a quick way to see the distribution of sentiment scores across our tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(111)\nax.set(title='Vaccination Tweets Sentiment Score', xlabel='Compund Sentiment Score', ylabel='Frequency')\nsns.distplot(df['compound'], bins=15, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s see if we can get a little bit clearer picture of our sentiment over time. \n\nOverall our data is noisy, there is just too much of it. \nTaking a sample of our data might make it easier to see the trends happening. \nWe’ll use pandas sample() function to retain just a tenth of our 89,973 tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"ot = df.sample(frac=.05, random_state=1111)\not.sort_index(inplace=True)\n\not['mean'] = ot['compound'].expanding().mean()\not['rolling'] = ot['compound'].rolling('1d').mean()\n\nfig = plt.figure(figsize=(30,5))\nax = fig.add_subplot(111)\nax.scatter(ot['date'],ot['compound'], label='Tweet Sentiment')\nax.plot(ot['date'],ot['rolling'], color ='r', label='Rolling Mean')\nax.plot(ot['date'],ot['mean'], color='g', label='Expanding Mean')\nax.set(title='Vaccination Tweets over Time', xlabel='Date', ylabel='Sentiment')\nax.legend(loc='best')\nfig.tight_layout()\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save cleaned dataset with sentiment appended\n#df.to_csv('/kaggle/working/df_cleaned_sent.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n###  Understanding the impact of Hashtags on tweets sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"def hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting hashtags from neutral tweets\n\nHT_neutral = hashtag_extract(df['hashtags'][df['compound'] == 0])\n\n# extracting hashtags from negative tweets\nHT_negative = hashtag_extract(df['hashtags'][df['compound'] < 0])\n\n# extracting hashtags from positive tweets\nHT_positive = hashtag_extract(df['hashtags'][df['compound'] > 0])\n\n# unnesting list\nHT_neutral = sum(HT_neutral,[])\nHT_negative = sum(HT_negative,[])\nHT_positive = sum(HT_positive,[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Neutral Tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = nltk.FreqDist(HT_neutral)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"b = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n\n# selecting top 20 most frequent hashtags     \ne = e.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Positive Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = nltk.FreqDist(HT_positive)\nf = pd.DataFrame({'Hashtag': list(c.keys()), 'Count': list(c.values())})\n\n# selecting top 20 most frequent hashtags     \nf = f.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=f, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Topic Model Analysis**\n\nTopic Modeling (TM) consists of finding the information contained in textual documents (information retrieval in English) and presenting it in the form of themes (depending on the technique used, the relative importance of the themes can also be found ).\n\nTM is therefore an unsupervised technique for classifying documents in multiple themes (Unsupervised Learning in English).\n\nFrom the point of view of the representation space, the TM is a reduction of dimensions in the vector representation of a document : instead of representing a document of a corpus by a vector in the space of the words composing the vocabulary of this corpus is represented by a vector in the space of the themes of this corpus , each value of this vector corresponding to the relative importance of the theme in this document. \n\n### **Latent Dirichlet Allocation**\n\nLatent Dirichlet Allocation (LDA) is one example of a topic model used to extract topics from a document. LDA is an unsupervised machine learning algorithm that allows a a set of textual observations to be explained by unobserved groups that explain similarities within the data. LDA represents documents as mixtures of topics that spit out words with certain probabilities.\n\n![image.png](attachment:image.png)\n\n","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n# !{sys.executable} -m spacy download en\nimport re, numpy as np, pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim, spacy, logging, warnings\nimport gensim.corpora as corpora\nfrom gensim.utils import lemmatize, simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize Sentences and Clean\nRemoving new line characters, single quotes and finally split the sentence into a list of words using gensim’s `simple_preprocess()`. Setting the `deacc=True` option removes punctuations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent_to_words(sentences):\n    for sent in sentences:\n        sent = re.sub('\\S*@\\S*\\s?', '', sent) \n        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n        yield(sent)  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to list\ndata = df.clean_tweets.values.tolist()\ndata_words = list(sent_to_words(data))\nprint(data_words[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the Bigram, Trigram Models and Lemmatize\nLet’s form the bigram and trigrams using the Phrases model. This is passed to Phraser() for efficiency in speed of execution.\n\nNext, lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n\nWe keep only these POS tags because they are the ones contributing the most to the meaning of the sentences. Here, we use `spacy` for lemmatization."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\ndef process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    for tweet in texts:\n        doc = nlp(\" \".join(tweet)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\ndata_ready = process_words(data_words)  # processed Tweet Data!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ready[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Build the Topic Model**\nTo build the LDA topic model using LdaModel(), you need the corpus and the dictionary. Let’s create them first and then build the model. The trained topics (keywords and weights) are printed below as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_ready)\n\n# Create Corpus: Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in data_ready]\n\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=4, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=10,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           iterations=100,\n                                           per_word_topics=True)\n\nprint(lda_model.print_topics())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **What is the Dominant topic and its percentage contribution in each document**\n\nIn LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n\nThis way, we will know which document belongs predominantly to which topic."},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    tweet_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                tweet_topics_df = tweet_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    tweet_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    tweet_topics_df = pd.concat([tweet_topics_df, contents], axis=1)\n    return(tweet_topics_df)\n\n\ndf_topic_tweet_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n\n# Format\ndf_dominant_topic = df_topic_tweet_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Tweet']\ndf_dominant_topic.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **The most representative sentence for each topic**\n\nSometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display setting to show more characters in column\npd.options.display.max_colwidth = 100\n\ntweet_topics_sorteddf_mallet = pd.DataFrame()\ntweet_topics_outdf_grpd = df_topic_tweet_keywords.groupby('Dominant_Topic')\n\nfor i, grp in tweet_topics_outdf_grpd:\n    tweet_topics_sorteddf_mallet = pd.concat([tweet_topics_sorteddf_mallet, \n                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n                                            axis=0)\n\n# Reset Index    \ntweet_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n# Format\ntweet_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Tweet\"]\n\n# Show\ntweet_topics_sorteddf_mallet.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequency Distribution of Word Counts in Tweets\nLet’s plot the tweet word counts distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_lens = [len(d) for d in df_dominant_topic.Tweet]\n\n# Plot\nplt.figure(figsize=(16,7), dpi=55)\nplt.hist(doc_lens, bins = 100, color='navy')\n#plt.text(50, 1000, \"Mean   : \" + str(round(np.mean(doc_lens))))\n#plt.text(50,  2000, \"Median : \" + str(round(np.median(doc_lens))))\n#plt.text(50,  3000, \"Stdev   : \" + str(round(np.std(doc_lens))))\n#plt.text(50,  4000, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n#plt.text(50,  5000, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n\nplt.gca().set(xlim=(0, 50), ylabel='Number of Tweets', xlabel='Tweet Word Count')\nplt.tick_params(size=16)\nplt.xticks(np.linspace(0,50,9))\nplt.title('Distribution of Tweet Word Counts', fontdict=dict(size=22))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.colors as mcolors\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\nfig, axes = plt.subplots(2,2,figsize=(10,8), dpi=55, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):    \n    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n    doc_lens = [len(d) for d in df_dominant_topic_sub.Tweet]\n    ax.hist(doc_lens, bins = 50, color=cols[i])\n    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n    ax.set(xlim=(0, 50), xlabel='Tweet Word Count')\n    ax.set_ylabel('Number of Tweets', color=cols[i])\n    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)\nplt.xticks(np.linspace(0,50,9))\nfig.suptitle('Distribution of Tweet Word Counts by Dominant Topic', fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Clouds of Top N Keywords in Each Topic\n\nThough we’ve already seen what are the topic keywords in each topic, a word cloud with the size of the words proportional to the weight is a pleasant sight. The coloring of the topics we’ve taken here is followed in the subsequent plots as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Wordcloud of Top N words in each topic\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 2, figsize=(7,7), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Counts of Topic Keywords\nWhen it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the tweets is also interesting to look.\n\nLet’s plot the word counts and the weights of each keyword in the same chart.\n\nWe want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart we’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ntopics = lda_model.show_topics(formatted=False)\ndata_flat = [w for w_list in data_ready for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf_topics = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(10,8), sharey=True, dpi=75)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df_topics.loc[df_topics.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df_topics.loc[df_topics.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    ax_twin.set_ylim(0, 0.050); ax.set_ylim(0, 1000)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df_topics.loc[df_topics.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentence Chart Colored by Topic\n\nEach word in the document is representative of one of the 4 topics. Let’s color each word in the given documents by the topic id it is attributed to.\nThe color of the enclosing rectangle is the topic assigned to the document."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence Coloring of N Sentences\nfrom matplotlib.patches import Rectangle\n\ndef sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n    corp = corpus[start:end]\n    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\n    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=70)       \n    axes[0].axis('off')\n    for i, ax in enumerate(axes):\n        if i > 0:\n            corp_cur = corp[i-1] \n            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n\n            # Draw Rectange\n            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n\n            word_pos = 0.06\n            for j, (word, topics) in enumerate(word_dominanttopic):\n                if j < 14:\n                    ax.text(word_pos, 0.5, word,\n                            horizontalalignment='left',\n                            verticalalignment='center',\n                            fontsize=16, color=mycolors[topics],\n                            transform=ax.transAxes, fontweight=700)\n                    word_pos += .009 * len(word)  # to move the word for the next iter\n                    ax.axis('off')\n            ax.text(word_pos, 0.5, '. . .',\n                    horizontalalignment='left',\n                    verticalalignment='center',\n                    fontsize=16, color='black',\n                    transform=ax.transAxes)       \n\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n    plt.tight_layout()\n    plt.show()\n\nsentences_chart()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What are the most discussed topics in the documents?\n\nLet’s compute the total number of documents attributed to each topic."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence Coloring of N Sentences\ndef topics_per_document(model, corpus, start=0, end=1):\n    corpus_sel = corpus[start:end]\n    dominant_topics = []\n    topic_percentages = []\n    for i, corp in enumerate(corpus_sel):\n        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n        dominant_topics.append((i, dominant_topic))\n        topic_percentages.append(topic_percs)\n    return(dominant_topics, topic_percentages)\n\ndominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n\n# Distribution of Dominant Topics in Each Document\ndf_dominant = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\ndominant_topic_in_each_doc = df_dominant.groupby('Dominant_Topic').size()\ndf_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n\n# Total Topic Distribution by actual weight\ntopic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\ndf_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n\n# Top 3 Keywords for each Topic\ntopic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n\ndf_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\ndf_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\ndf_top3words.reset_index(level=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let’s make two plots:**\n\n1. The number of documents for each topic by assigning the document to the topic that has the most weight in that document.\n2. The number of documents for each topic by by summing up the actual weight contribution of each topic to respective documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.ticker import FuncFormatter\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=75, sharey=True)\n\n# Topic Distribution by Dominant Topics\nax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\nax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\nax1.xaxis.set_major_formatter(tick_formatter)\nax1.set_title('Number of Tweets by Dominant Topic', fontdict=dict(size=10))\nax1.set_ylabel('Number of Tweets')\nax1.set_ylim(0, 5000)\n\n# Topic Distribution by Topic Weights\nax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\nax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\nax2.xaxis.set_major_formatter(tick_formatter)\nax2.set_title('Number of Tweets by Topic Weightage', fontdict=dict(size=10))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## t-SNE Clustering Chart\nLet’s visualize the clusters of documents in a 2D space using t-SNE (t-distributed stochastic neighbor embedding) algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get topic weights and dominant topics ------------\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\n# n-1 rows each is a vector with i-1 posisitons, where n the number of documents\n# i the topic number and tmp[i] = probability of topic i\n\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n    \n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 4\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=900, plot_height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pyLDAVis\n\nFinally, pyLDAVis is the most commonly used and a nice way to visualise the information contained in a topic model. Below is the implementation for `LdaModel()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\nvis","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}