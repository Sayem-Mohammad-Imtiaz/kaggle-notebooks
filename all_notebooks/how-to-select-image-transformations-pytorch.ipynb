{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nWhenever you have to decide which transformations to apply on image data, you shold keep the structure of you dataset in mind. \n \nI will demonstrate some `torchvision.transforms` transformations on image data to provide the necessary knowledge about what the transformations actually do. \nTake a look at the [docs](https://pytorch.org/vision/0.8/transforms.html) to get some further infomration.\nFurthermore, I will a custom Dataset and loader for image data and perform some file handling.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas as pd\nimport torch\nfrom torchvision import transforms\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom transformers import AdamW\nimport matplotlib.pyplot as plt\nnp.random.seed(111)\n\ndata = pd.read_csv(\"../input/litter-on-forest-floor/labels.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-08T17:08:45.209663Z","iopub.execute_input":"2021-06-08T17:08:45.210061Z","iopub.status.idle":"2021-06-08T17:08:45.221002Z","shell.execute_reply.started":"2021-06-08T17:08:45.210025Z","shell.execute_reply":"2021-06-08T17:08:45.22012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data format is suitable for `ImageFolder`.\nHowever, instead of using that, I prefer using a custom dataset for image data.\nThus, I need to change the way the data is stored:","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\ndef encode(observation):\n    \"\"\"\n    Encodes the .csv file. \n    \"\"\"\n    if observation[\"label\"] == \"clean\":\n        observation[\"label\"] = 0\n        observation[\"file\"] = observation[\"file\"][6:]\n        return observation\n    observation[\"label\"] = 1\n    observation[\"file\"] = observation[\"file\"][7:]\n    return observation\n\ndata = data.apply(encode, axis=1)\ndata = data.sample(frac=1)  # shuffle\ndata.index = range(len(data))\ntrain_data = data.head(150)\ntest_data = data = data.tail(16)\n\n# reorganize the images and copy them to one directory\nclean_path = \"../input/litter-on-forest-floor/clean\"\nclean_files = os.listdir(clean_path)\nlitter_path = \"../input/litter-on-forest-floor/litter\"\nlitter_files = os.listdir(litter_path)\nnew_path = \"img\"\nif not os.path.exists(new_path):\n    os.makedirs(new_path)\n\nfor file in litter_files:\n    shutil.copy(litter_path + \"/\" + file, new_path)\n    \nfor file in clean_files:\n    shutil.copy(clean_path + \"/\" + file, new_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:45.222301Z","iopub.execute_input":"2021-06-08T17:08:45.222628Z","iopub.status.idle":"2021-06-08T17:08:46.294558Z","shell.execute_reply.started":"2021-06-08T17:08:45.222596Z","shell.execute_reply":"2021-06-08T17:08:46.293445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create the custom dataset for image data:","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\n\nclass CustomDataset(Dataset):\n    \"\"\"\n    A custom Image Dataset that performs transformations on the images contained in it and shifts them to\n    a given device.\n    \"\"\"\n\n    def __init__(self, data, transform_pipe, x_name, y_name, device):\n        \"\"\"\n        Constructor.\n\n        :param pd.DataFrame data: A DataFrame containing one column of image paths and another columns of image labels.\n        :param transform_pipe: a transform:Composition of all transformations that have to be applied to the images\n        :param str x_name: name of the image column\n        :param str y_name: name of the label column\n        :param str device: name of the device that has to be used\n        \"\"\"\n        self.data = data\n        self.transform_pipe = transform_pipe\n        self.x_name = x_name\n        self.y_name = y_name\n        self.device = device\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of observations in the whole dataset\n\n        :return: the length of the dataset\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, i):\n        \"\"\"\n        Is used by DataLoaders to draw the observation at index i in the dataset.\n\n        :param int i: index of an observation\n        :return: a list containing the image-data and the label of one observation\n        \"\"\"\n        img_path = \"img/\" + self.data[self.x_name].iloc[i]\n        x = self.transform_pipe(Image.open(img_path)).to(self.device)\n        y = torch.tensor(self.data[self.y_name][i], dtype=torch.float).to(self.device)\n        return [x, y]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:46.296619Z","iopub.execute_input":"2021-06-08T17:08:46.297034Z","iopub.status.idle":"2021-06-08T17:08:46.306032Z","shell.execute_reply.started":"2021-06-08T17:08:46.296997Z","shell.execute_reply":"2021-06-08T17:08:46.304829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a function that creates a DataLoader:","metadata":{}},{"cell_type":"code","source":"def create_loader(data, transform_pipe, batch_size, x_name, y_name, device):\n    \"\"\"\n    Creates a DataLoader for image data.\n    \"\"\"\n    custom_dataset = CustomDataset(data=data,\n                                   transform_pipe=transform_pipe,\n                                   x_name=x_name,\n                                   y_name=y_name,\n                                   device=device)\n    sampler = SequentialSampler(data_source=custom_dataset)\n    loader = DataLoader(dataset=custom_dataset,\n                        batch_size=batch_size, \n                        sampler=sampler)\n    return loader","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:46.307769Z","iopub.execute_input":"2021-06-08T17:08:46.308113Z","iopub.status.idle":"2021-06-08T17:08:46.32296Z","shell.execute_reply.started":"2021-06-08T17:08:46.30807Z","shell.execute_reply":"2021-06-08T17:08:46.321767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_image(transform_pipe, title, verbose=0):\n    \"\"\"\n    Displays an image after applying a number of transformations.\n    \"\"\"\n    loader = create_loader(data=train_data, \n                       transform_pipe=transform_pipe, \n                       batch_size=1, \n                       x_name=\"file\", \n                       y_name=\"label\", \n                       device=\"cpu\")\n    example_batch = next(iter(loader))\n    example_image = example_batch[0]\n    if verbose > 0:\n        print(\"tensor size:\", example_image.size())\n        print(\"size after applying squeeeze:\", example_image.squeeze().size())\n        print(\"size after applying permutate:\", example_image.squeeze().permute(1, 2, 0).size())\n\n    plt.title(title)\n    plt.imshow(example_image.squeeze().permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:46.324395Z","iopub.execute_input":"2021-06-08T17:08:46.324772Z","iopub.status.idle":"2021-06-08T17:08:46.340222Z","shell.execute_reply.started":"2021-06-08T17:08:46.324727Z","shell.execute_reply":"2021-06-08T17:08:46.339315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's display what our DataLoader is actually doing without performing any further transformations:","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Identity mapping\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:46.341363Z","iopub.execute_input":"2021-06-08T17:08:46.341824Z","iopub.status.idle":"2021-06-08T17:08:48.519073Z","shell.execute_reply.started":"2021-06-08T17:08:46.341785Z","shell.execute_reply":"2021-06-08T17:08:48.518268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformation Visualization\nLet's finally investigate the impact of some [transforms.transformations](https://pytorch.org/vision/0.8/transforms.html)!","metadata":{}},{"cell_type":"markdown","source":"# CenterCrop","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.CenterCrop(size=1024), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Symmetric CenterCrop\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:48.520146Z","iopub.execute_input":"2021-06-08T17:08:48.520592Z","iopub.status.idle":"2021-06-08T17:08:49.010571Z","shell.execute_reply.started":"2021-06-08T17:08:48.52055Z","shell.execute_reply":"2021-06-08T17:08:49.009702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.CenterCrop(size=[1024, 256*2]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Non-symmetric CenterCrop\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:49.012807Z","iopub.execute_input":"2021-06-08T17:08:49.013252Z","iopub.status.idle":"2021-06-08T17:08:49.401107Z","shell.execute_reply.started":"2021-06-08T17:08:49.013197Z","shell.execute_reply":"2021-06-08T17:08:49.399953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ColorJitter\nThe ColorJitter randomly adds some noise to the data (one form of [Data Augmentation](https://en.wikipedia.org/wiki/Data_augmentation)). \n\nTo display the randomness, I will perform each transformation twice given the same values! You will see the differences.","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(brightness=[0, 10]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for brightness; first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:49.403633Z","iopub.execute_input":"2021-06-08T17:08:49.404076Z","iopub.status.idle":"2021-06-08T17:08:51.711165Z","shell.execute_reply.started":"2021-06-08T17:08:49.40403Z","shell.execute_reply":"2021-06-08T17:08:51.71008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(brightness=[0, 10]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for brightness; second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:51.712636Z","iopub.execute_input":"2021-06-08T17:08:51.712948Z","iopub.status.idle":"2021-06-08T17:08:54.034304Z","shell.execute_reply.started":"2021-06-08T17:08:51.712918Z","shell.execute_reply":"2021-06-08T17:08:54.033346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(contrast=[0, 10]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for contrast; first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:54.035601Z","iopub.execute_input":"2021-06-08T17:08:54.035891Z","iopub.status.idle":"2021-06-08T17:08:56.319707Z","shell.execute_reply.started":"2021-06-08T17:08:54.03586Z","shell.execute_reply":"2021-06-08T17:08:56.318669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(contrast=[0, 10]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for contrast; second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:56.321273Z","iopub.execute_input":"2021-06-08T17:08:56.321707Z","iopub.status.idle":"2021-06-08T17:08:58.648544Z","shell.execute_reply.started":"2021-06-08T17:08:56.321659Z","shell.execute_reply":"2021-06-08T17:08:58.647474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(saturation=[0, 10]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for saturation; first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:08:58.65012Z","iopub.execute_input":"2021-06-08T17:08:58.650749Z","iopub.status.idle":"2021-06-08T17:09:00.941339Z","shell.execute_reply.started":"2021-06-08T17:08:58.650691Z","shell.execute_reply":"2021-06-08T17:09:00.940307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(saturation=[0, 10]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for saturation; second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:00.942829Z","iopub.execute_input":"2021-06-08T17:09:00.943451Z","iopub.status.idle":"2021-06-08T17:09:03.239913Z","shell.execute_reply.started":"2021-06-08T17:09:00.943401Z","shell.execute_reply":"2021-06-08T17:09:03.238748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(hue=[-0.5, 0.5]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for hue; first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:03.241525Z","iopub.execute_input":"2021-06-08T17:09:03.241911Z","iopub.status.idle":"2021-06-08T17:09:06.292929Z","shell.execute_reply.started":"2021-06-08T17:09:03.241869Z","shell.execute_reply":"2021-06-08T17:09:06.292075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ColorJitter(hue=[-0.5, 0.5]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"ColorJitter for hue; second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:06.294339Z","iopub.execute_input":"2021-06-08T17:09:06.294659Z","iopub.status.idle":"2021-06-08T17:09:09.298888Z","shell.execute_reply.started":"2021-06-08T17:09:06.294629Z","shell.execute_reply":"2021-06-08T17:09:09.297904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resize","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Resize(size=[256, 256]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Resize to smaller size, symmetric\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:09.300112Z","iopub.execute_input":"2021-06-08T17:09:09.300525Z","iopub.status.idle":"2021-06-08T17:09:09.66724Z","shell.execute_reply.started":"2021-06-08T17:09:09.300456Z","shell.execute_reply":"2021-06-08T17:09:09.666467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Resize(size=[2**12, 2**10]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Resize to larger size, non-symmetric\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:09.668285Z","iopub.execute_input":"2021-06-08T17:09:09.668715Z","iopub.status.idle":"2021-06-08T17:09:10.624432Z","shell.execute_reply.started":"2021-06-08T17:09:09.668682Z","shell.execute_reply":"2021-06-08T17:09:10.623337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Resize(size=256), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Resize smaller axis and the other one accordingly\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:10.625756Z","iopub.execute_input":"2021-06-08T17:09:10.626069Z","iopub.status.idle":"2021-06-08T17:09:10.984649Z","shell.execute_reply.started":"2021-06-08T17:09:10.626038Z","shell.execute_reply":"2021-06-08T17:09:10.983527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pad\nDefault `padding_mode` is `constant`, which pads using some constant value","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Pad(padding=256), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Pad\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:10.985959Z","iopub.execute_input":"2021-06-08T17:09:10.98627Z","iopub.status.idle":"2021-06-08T17:09:13.601417Z","shell.execute_reply.started":"2021-06-08T17:09:10.986238Z","shell.execute_reply":"2021-06-08T17:09:13.600274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Pad(padding=[0, 256, 1024, 562], fill=64), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Pad, custom fill value\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:13.602858Z","iopub.execute_input":"2021-06-08T17:09:13.603194Z","iopub.status.idle":"2021-06-08T17:09:16.621957Z","shell.execute_reply.started":"2021-06-08T17:09:13.603159Z","shell.execute_reply":"2021-06-08T17:09:16.620905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Pad(padding=256, padding_mode=\"reflect\"), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Reflected Padding\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:16.623205Z","iopub.execute_input":"2021-06-08T17:09:16.623526Z","iopub.status.idle":"2021-06-08T17:09:19.224753Z","shell.execute_reply.started":"2021-06-08T17:09:16.62348Z","shell.execute_reply":"2021-06-08T17:09:19.223986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Pad(padding=256, padding_mode=\"edge\"), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Edge Padding\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:19.227908Z","iopub.execute_input":"2021-06-08T17:09:19.228333Z","iopub.status.idle":"2021-06-08T17:09:21.878383Z","shell.execute_reply.started":"2021-06-08T17:09:19.228292Z","shell.execute_reply":"2021-06-08T17:09:21.877582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.Pad(padding=256, padding_mode=\"symmetric\"), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"Symmetric Padding\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:21.879886Z","iopub.execute_input":"2021-06-08T17:09:21.880275Z","iopub.status.idle":"2021-06-08T17:09:24.593756Z","shell.execute_reply.started":"2021-06-08T17:09:21.880246Z","shell.execute_reply":"2021-06-08T17:09:24.592594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"note: The only difference between symmetric padding and reflected padding is that symmetric padding reflects the edge pixels as well.","metadata":{}},{"cell_type":"markdown","source":"# RandomAffine","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=[0,120]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine rotation; first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:24.595546Z","iopub.execute_input":"2021-06-08T17:09:24.596338Z","iopub.status.idle":"2021-06-08T17:09:26.771222Z","shell.execute_reply.started":"2021-06-08T17:09:24.596285Z","shell.execute_reply":"2021-06-08T17:09:26.770489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=[0,120]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine rotation; second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:26.772205Z","iopub.execute_input":"2021-06-08T17:09:26.772624Z","iopub.status.idle":"2021-06-08T17:09:28.914705Z","shell.execute_reply.started":"2021-06-08T17:09:26.772593Z","shell.execute_reply":"2021-06-08T17:09:28.91389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=0, translate=[0.9, 0.1]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine translation, first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:28.915762Z","iopub.execute_input":"2021-06-08T17:09:28.916159Z","iopub.status.idle":"2021-06-08T17:09:31.018537Z","shell.execute_reply.started":"2021-06-08T17:09:28.916129Z","shell.execute_reply":"2021-06-08T17:09:31.017667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=0, translate=[0.9, 0.1]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine translation, second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:31.019638Z","iopub.execute_input":"2021-06-08T17:09:31.020033Z","iopub.status.idle":"2021-06-08T17:09:33.385478Z","shell.execute_reply.started":"2021-06-08T17:09:31.020004Z","shell.execute_reply":"2021-06-08T17:09:33.384335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=0, scale=[0.1, 0.9]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine scaling, first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:33.387146Z","iopub.execute_input":"2021-06-08T17:09:33.387582Z","iopub.status.idle":"2021-06-08T17:09:36.780361Z","shell.execute_reply.started":"2021-06-08T17:09:33.387536Z","shell.execute_reply":"2021-06-08T17:09:36.778444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=0, scale=[0.1, 0.9]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine scaling, second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:36.785673Z","iopub.execute_input":"2021-06-08T17:09:36.786827Z","iopub.status.idle":"2021-06-08T17:09:41.610558Z","shell.execute_reply.started":"2021-06-08T17:09:36.786787Z","shell.execute_reply":"2021-06-08T17:09:41.606313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=0, shear=[0, 64]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine translation, first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:41.616768Z","iopub.execute_input":"2021-06-08T17:09:41.617966Z","iopub.status.idle":"2021-06-08T17:09:46.513444Z","shell.execute_reply.started":"2021-06-08T17:09:41.617756Z","shell.execute_reply":"2021-06-08T17:09:46.51039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=0, shear=[0, 64]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine translation, second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:46.517884Z","iopub.execute_input":"2021-06-08T17:09:46.518579Z","iopub.status.idle":"2021-06-08T17:09:51.251882Z","shell.execute_reply.started":"2021-06-08T17:09:46.518546Z","shell.execute_reply":"2021-06-08T17:09:51.250952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combined:","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomAffine(degrees=[0, 15], translate=[0.1, 0.1], scale=[0.8, 1], shear=[0, 32]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomAffine translation, second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:51.253333Z","iopub.execute_input":"2021-06-08T17:09:51.253778Z","iopub.status.idle":"2021-06-08T17:09:53.543121Z","shell.execute_reply.started":"2021-06-08T17:09:51.253739Z","shell.execute_reply":"2021-06-08T17:09:53.542075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RandomCrop\nDifference to CenterCrop: doesn't necessarily crop at the center!","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomCrop(size=[1024, 1024]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomCrop, first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:53.544396Z","iopub.execute_input":"2021-06-08T17:09:53.544724Z","iopub.status.idle":"2021-06-08T17:09:54.039528Z","shell.execute_reply.started":"2021-06-08T17:09:53.544694Z","shell.execute_reply":"2021-06-08T17:09:54.03831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomCrop(size=[1024, 1024]), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomCrop, second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:54.040734Z","iopub.execute_input":"2021-06-08T17:09:54.04104Z","iopub.status.idle":"2021-06-08T17:09:54.540289Z","shell.execute_reply.started":"2021-06-08T17:09:54.040986Z","shell.execute_reply":"2021-06-08T17:09:54.539297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomCrop(size=[1024*4, 1024*4], pad_if_needed=True, padding_mode=\"constant\"), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomCrop padded, first time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:54.541855Z","iopub.execute_input":"2021-06-08T17:09:54.542257Z","iopub.status.idle":"2021-06-08T17:09:58.192347Z","shell.execute_reply.started":"2021-06-08T17:09:54.542213Z","shell.execute_reply":"2021-06-08T17:09:58.19134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomCrop(size=[1024*4, 1024*4], pad_if_needed=True, padding_mode=\"constant\"), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomCrop padded, second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:09:58.193852Z","iopub.execute_input":"2021-06-08T17:09:58.194165Z","iopub.status.idle":"2021-06-08T17:10:01.6822Z","shell.execute_reply.started":"2021-06-08T17:09:58.194134Z","shell.execute_reply":"2021-06-08T17:10:01.68114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RandomHorizontalFlip\nIn this case I once chose `p=1` to display the (randomly occurring) outcome for sure. In a real context, a moderate value would make more sense. Analogously RandomVerticalFlip.","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomHorizontalFlip(p=1), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomHorizontalFlip, second time\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:10:01.683811Z","iopub.execute_input":"2021-06-08T17:10:01.684216Z","iopub.status.idle":"2021-06-08T17:10:03.895756Z","shell.execute_reply.started":"2021-06-08T17:10:01.684163Z","shell.execute_reply":"2021-06-08T17:10:03.892597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RandomPerspective\nIn this case I once chose `p=1` to display the (randomly occurring) outcome for sure. In a real context, a moderate value would make more sense.","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomPerspective(distortion_scale=0.2, p=1), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomPerspective, small distortion\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:10:03.897102Z","iopub.execute_input":"2021-06-08T17:10:03.897394Z","iopub.status.idle":"2021-06-08T17:10:06.6837Z","shell.execute_reply.started":"2021-06-08T17:10:03.897367Z","shell.execute_reply":"2021-06-08T17:10:06.682675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.RandomPerspective(distortion_scale=0.8, p=1), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"RandomPerspective, large distortion\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:10:06.685179Z","iopub.execute_input":"2021-06-08T17:10:06.685769Z","iopub.status.idle":"2021-06-08T17:10:09.195632Z","shell.execute_reply.started":"2021-06-08T17:10:06.685722Z","shell.execute_reply":"2021-06-08T17:10:09.194608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GaussianBlur\nvary the kenrel size to change the affected field - and sigma to cahnge the strength of the blur effect.","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.GaussianBlur(kernel_size=33, sigma=100000), transforms.ToTensor()])\ndisplay_image(transform_pipe=transform_pipe, title=\"GaussianBlurr\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:10:09.197224Z","iopub.execute_input":"2021-06-08T17:10:09.197667Z","iopub.status.idle":"2021-06-08T17:10:15.388567Z","shell.execute_reply.started":"2021-06-08T17:10:09.197619Z","shell.execute_reply":"2021-06-08T17:10:15.387513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalize\nnote: can only be performed on `Tensors` $\\rightarrow$ perform `ToTensor` before applying this transformation!\n\nNormalization might vastly improve the performance of your model. \nNormalizes each channel individually. Since the given dataset contains `RGB` images, we have to define 3 ways of normalization. YOu might need to calculate the mean and the std of your dataset before applying this function:","metadata":{}},{"cell_type":"code","source":"def find_mean_std(data):\n    \"\"\"\n    Calculates the averaged mean and std for all channels \n    of a given RGB-image dataset.\n    \"\"\"\n    mean_ch1 = 0\n    mean_ch2 = 0\n    mean_ch3 = 0\n    std_ch1 = 0\n    std_ch2 = 0\n    std_ch3 = 0\n\n    loader = create_loader(data=data, \n                       transform_pipe=transforms.ToTensor(), \n                       batch_size=1, \n                       x_name=\"file\", \n                       y_name=\"label\", \n                       device=\"cpu\")\n\n    for image in loader:\n        means = torch.mean(input=image[0], dim=[2, 3])[0] # mean for each channel\n        mean_ch1 += means[0].item()\n        mean_ch2 += means[1].item()\n        mean_ch3 += means[2].item()\n\n        stds = torch.std(input=image[0], dim=[2, 3])[0] # std for each channel\n        std_ch1 += stds[0].item()\n        std_ch2 += stds[1].item()\n        std_ch3 += stds[2].item()\n        \n    mean_ch1 /= len(loader)\n    mean_ch2 /= len(loader)\n    mean_ch3 /= len(loader)\n    std_ch1 /= len(loader)\n    std_ch2 /= len(loader)\n    std_ch3 /= len(loader)\n    return {\"mean\": [mean_ch1, mean_ch2, mean_ch3], \"std\": [std_ch1, std_ch2, std_ch3]}\n\nstats = find_mean_std(data=train_data)\nprint(stats)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:10:15.389804Z","iopub.execute_input":"2021-06-08T17:10:15.39007Z","iopub.status.idle":"2021-06-08T17:11:36.378312Z","shell.execute_reply.started":"2021-06-08T17:10:15.390043Z","shell.execute_reply":"2021-06-08T17:11:36.377374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you are too lazy to calculate them on your own, using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] is a valid option if you have regular images of objects/nature. Those are the value sof the `ImageNet` dataset. ","metadata":{}},{"cell_type":"code","source":"transform_pipe = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=stats[\"mean\"], std=stats[\"std\"])])\ndisplay_image(transform_pipe=transform_pipe, title=\"Normalize\", verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T17:11:36.379845Z","iopub.execute_input":"2021-06-08T17:11:36.380259Z","iopub.status.idle":"2021-06-08T17:11:38.737147Z","shell.execute_reply.started":"2021-06-08T17:11:36.380205Z","shell.execute_reply":"2021-06-08T17:11:38.735981Z"},"trusted":true},"execution_count":null,"outputs":[]}]}