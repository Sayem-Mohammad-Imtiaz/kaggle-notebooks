{"cells":[{"metadata":{},"cell_type":"markdown","source":"Report here ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nds=pd.read_csv('../input/airplane-crash-data-since-1908/Airplane_Crashes_and_Fatalities_Since_1908_20190820105639.csv')\nprint(\"The shape of the dataset: \", ds.shape)\nds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\nimport matplotlib.pyplot as plt\n\ncorrMatrix=ds.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of rows before removing the missing data:\")\nprint(ds.shape[0])\n#dsDrop = ds.dropna()\n\nds = ds.dropna() #________added\n\nprint(\"Number of rows after removing the missing data:\")\nprint(ds.shape[0]) #________added\n#print(dsDrop.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dso = ds.drop(['Date', 'Time', 'Location', 'Operator', 'Flight #', 'Route', 'AC Type', 'Registration', 'cn/ln', 'Summary'],axis=1)\nprint(dso.shape)\nQL1=dso.quantile(0.25)\nQL3=dso.quantile(0.75)\nIQRL=QL3-QL1\nFLL=QL1-(1.5*IQRL)\nFHL=QL3+(1.5*IQRL)\ndso=dso.loc[(dso>FLL).all(axis=1) & (dso<FHL).all(axis=1)]\nprint(\"Number of rows after removing the outliers:\")\nprint(dso.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dup = ds.duplicated()\nprint('Number of duplicate rows = %d' % (dup.sum()))\nds=ds.drop_duplicates()\nprint(\"Number of rows after removing the duplicate data:\")\nprint(ds.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace NULL with NaN\nimport numpy as np\n\n#drop_Sum = ds #save Summary column\n\n#ds = ds.drop(['Summary'],axis=1)\n\n#print('Number of missing values:')\n#for col in ds.columns:\n#    print(col +': ' + str(ds[col].isna().sum()))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ds.replace('NULL',np.NaN)\n#ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill empty rows with NULL \n#ds[\"Summary\"].fillna(\"NULL\", inplace = True) \n#ds[\"Summary\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Noise Removal\n\nimport string \n\ni = 0\npunct = \"\\n\\r\"+string.punctuation\n\nds[\"Summary\"] = ds[\"Summary\"].str.translate(str.maketrans('','',punct))  \nds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stop Words Removal\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\nds[\"Summary\"] = ds[\"Summary\"].str.lower().str.split()\nds[\"Summary\"]= ds[\"Summary\"].apply(lambda x: ' '.join([item for item in x if item not in stop]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nX= ds.Summary.fillna(' ')\n\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(ds[\"Summary\"])\nfeature_names = vectorizer.get_feature_names()\nprint (feature_names)\nprint (\"Number of words = \",len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\nvectors = vectorizer.fit_transform(ds[\"Summary\"])\n\nfeature_names = vectorizer.get_feature_names()\n\nprint(feature_names)\nprint(\"number of words = \", len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stemming\n\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\n\nstemmer = PorterStemmer()\ndef tokenize(str_input):\n    \n    words = re.sub(r\"(?u)[^A-Za-z]\", \" \", str_input).lower().split()\n    words = [stemmer.stem(word) for word in words]\n    return words\n\n\nvectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\nvectors = vectorizer.fit_transform(ds[\"Summary\"])\nfeature_names = vectorizer.get_feature_names()\n\nprint(feature_names)\nprint(\"number of words = \", len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lemmatization\nfrom nltk.stem import WordNetLemmatizer\n\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef tokenize(str_input):\n    \n    words = re.sub(r\"(?u)[^A-Za-z]\", \" \", str_input).lower().split(\" \")\n    words = [stemmer.stem(word) for word in words if len(word)>2]\n    words = [wordnet_lemmatizer.lemmatize(word) for word in words if len(word)>2]\n    return words\n\nvectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\nvectors = vectorizer.fit_transform(ds[\"Summary\"])\nfeature_names = vectorizer.get_feature_names()\n\nprint(feature_names)\nprint(\"number of words = \", len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert categorical data to numerical data\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nds['Date'] = le.fit_transform (ds['Date'])\nds['Time'] = le.fit_transform (ds['Time'])\nds['Location'] = le.fit_transform (ds['Location'])\nds['Operator'] = le.fit_transform (ds['Operator'])\nds['Flight #'] = le.fit_transform (ds['Flight #'])\nds['Route'] = le.fit_transform (ds['Route'])\nds['AC Type'] = le.fit_transform (ds['AC Type'])\nds['Registration'] = le.fit_transform (ds['Registration'])\nds['cn/ln'] = le.fit_transform (ds['cn/ln'])\nds['Summary'] = le.fit_transform (ds['Summary'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sum-of-Squared Errors (SSE)\n\nfrom sklearn import cluster\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnumClusters = [1,2,3,4,5,6]\nSSE = []\nfor k in numClusters:\n    k_means = cluster.KMeans(n_clusters=k)\n    k_means.fit(ds)\n    SSE.append(k_means.inertia_)\n\nplt.plot(numClusters, SSE)\nplt.xlabel('Number of Clusters')\nplt.ylabel('SSE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#K-means Clustering\n\nClus_ds = pd.DataFrame(ds)\nClus_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#data = Clus_ds.drop('Summary',axis=1) <____ Not sure if we have to remove Summary or not!\nk_means = cluster.KMeans(n_clusters=2, max_iter=50, random_state=1)\nk_means.fit(data) \nlabels = k_means.labels_\npd.DataFrame(labels, index=Clus_ds.Fatalities, columns=['Cluster ID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids = k_means.cluster_centers_\npd.DataFrame(centroids,columns=data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TestData","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}