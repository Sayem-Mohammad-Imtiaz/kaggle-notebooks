{"cells":[{"metadata":{},"cell_type":"markdown","source":"As mentioned in the assignment the problem is:\n\n> Often the urgency indication turns out to be wrong: patients that are classified as urgent, are in reality not so urgent when they see the GP. Also, the opposite happens. Can we predict better how urgent a patient will be?\n\n> We then want to use the data from the triage to train a machine learning model to predict the label.\n\nTo predict the label (urgent or non-urgent), we can have three approaches:\n\n1. supervised classification\n2. un-supervised classification\n3. semi-supervised classification\n\nwhen training data are labelled and the training set is big enough, **supervised classification methods** can be applied and the result of classification will be highly accurate. At the first glance to dataset we figure out that the problem can be solved with applying supervised methods because data are labelled in the dataset (urgent vs non-urgent).\n\nHowever, from information in another part of assignment, we know that:\n\n> Important to know is that this labelling is imperfect.\n\nBased on this information, using the labels may lead classifier to produce wrong predictions. We can ignore the labels completely and solve the problem using **unsupervised classification methods**, or we can use part of labels which are valid and remove the rest and solve it as **semi-supervised classification method**.\n\nI break this assignment to three approaches: supervised, unsupervised and semi-supervised. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"First of all, I import all required libraries and I read input data from the file:\n\nThe main required libraries are:\n\n* matplotlib and seaborn for visualization\n* pandas and numpy for data structure\n* nltk for natural language processing\n\n**Note:** Based on my research, we can use *Frog* which is an integration of memory-based natural language processing (NLP) modules developed for Dutch, instead of NLTK. \n\n* sklearn for machine-learning\n* keras for deep learning\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential,Model\nfrom keras import optimizers\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Input,Concatenate\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import AgglomerativeClustering\n\nstop=set(stopwords.words('dutch'))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_excel(\"/kaggle/input/pacmed/triage_example.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EDA on Dataset:**\n\nThe first step is Exploratory Data Analysis (EDA). We need to know more about our data before going to next steps."},{"metadata":{},"cell_type":"markdown","source":"Dataset Columns are:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking the first rows of dataset to get more insight about the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**checking class distribution:**\n\nIt is impoertant to know how classes are distributed. sometimes the class distribution is unbalanced which needs furthur steps to handle that."},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df['Urgency indication'].value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: because classification is not possible with only one class, I added an augmented row to dataset (so we can run the whole code and make prediction)"},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented_row={\"Ingangsklacht\":[\"griep\"] ,\"Triage: H\":[\"Wat te doen?\"] ,\"Triage: B\":[\"plotselinge koorts, pijntjes, zwakte of verlies van eetlust. In het bijzonder samen hoesten en koorts hebben\"] , \"Triage: M\":[\"Geen\"],\"Triage: V\": [\"Geen\"],\"Triage: Checklist\":[\"Kortademig: Nee; Blaarvormig: Nee; Zieke indruk: Nee; Ontsteking: Nee\"],\"Age\": [23],\"Gender\": [\"Female\"], \"Urgency indication\":[\"Non-urgent\"]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_augmented = pd.DataFrame(augmented_row, columns = list(df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([df,df_augmented])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plotting the class distribution again :"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df['Urgency indication'].value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spliting features from target variable:**\n\nIn this assignment we consider that all columns are important and should be considered as features ( predictors ).However, we may drop some unuseful ones ( such as Triage: H ).\nWe record features as X_train and the target as y_train. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=df.iloc[:, :-1]\ny_train=df.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the target is categorical,first we need to convert it to numerical:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train= pd.Categorical(y_train).codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this assignment, there are two types of features: **structured categorical** and **free unstructured text**. So, we need diffrent feature engineering techiniques for each one"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features=X_train.drop(['Triage: B'],axis=1)\ntext_feature=X_train['Triage: B']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**feature engineering for categorical features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(categorical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the column \"Triage: Checklist\" is a list of four other features. We should break this column to 4 different column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Kortademig=[]\nBlaarvormig=[]\nindruk=[]\nOntsteking=[]\nfor i in range(len(categorical_features)):\n    checklist_text=categorical_features.iloc[i]['Triage: Checklist']\n    items=checklist_text.split(';')\n    Kortademig.append(items[0].split(\": \",1)[1])\n    Blaarvormig.append(items[1].split(\": \",1)[1])\n    indruk.append(items[2].split(\": \",1)[1])\n    Ontsteking.append(items[3].split(\": \",1)[1])\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features['Kortademig']=Kortademig\ncategorical_features['Blaarvormig']=Blaarvormig\ncategorical_features['indruk']=indruk\ncategorical_features['Ontsteking']=Ontsteking\ncategorical_features=categorical_features.drop(['Triage: Checklist'],axis=1)\ncategorical_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we convert all categorical values to numerics (except for Age). if we had more data, we could do EDA on these features as well. but in this assignment we stop at this step. "},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features['Ingangsklacht'] = pd.Categorical(categorical_features['Ingangsklacht']).codes\ncategorical_features['Triage: H'] = pd.Categorical(categorical_features['Triage: H']).codes\ncategorical_features['Triage: M'] = pd.Categorical(categorical_features['Triage: M']).codes\ncategorical_features['Triage: V'] = pd.Categorical(categorical_features['Triage: V']).codes\ncategorical_features['Gender'] = pd.Categorical(categorical_features['Gender']).codes\ncategorical_features['Kortademig'] = pd.Categorical(categorical_features['Kortademig']).codes\ncategorical_features['Blaarvormig'] = pd.Categorical(categorical_features['Blaarvormig']).codes\ncategorical_features['indruk'] = pd.Categorical(categorical_features['indruk']).codes\ncategorical_features['Ontsteking'] = pd.Categorical(categorical_features['Ontsteking']).codes\ncategorical_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis of free text:** (NLP)\n\nFirst,we will do very basic analysis to get more insight for the data; to do so, we need to create a list of all words and analyze these wors. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#put all documents together\ncorpus=text_feature.str.cat(sep=' ,')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the words\nwords=corpus.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking Stop-words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dic=defaultdict(int)\nfor word in words:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No, we want to know which stop words are repeated more frequently in the corpus. Most of stop words should be removed from corpus, however, some of them may be useful ans should be kept in text. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y=zip(*top)\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to check if there is any punctutations or special chractor in the corpus and what are these punctutations. the reason is that, most of panctuation marks are useless and should be removed from text but some of them are important and should be kept. Thus, by looking at this plot we get more information about pancuatations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor word in words:\n    for ch in word:\n        if ch in special:\n            dic[ch]+=1\n        \n\nx,y=zip(*dic.items())\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, we need to know which words are more more repeated in the text. These words may have some very useful information. "},{"metadata":{"trusted":true},"cell_type":"code","source":"counter=Counter(words)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=y,y=x)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data cleaning:**\n\n1. making all words lower case ( should be skipped or patially done for word-embedding and BERT solutions )\n2. cleaning all panctuation marks \n3. cleaning special chractors\n4. cleaning all stop words\n5. correcting words spellings\n6. stemming and lemmatization\n\nIn this assignment, I followed steps 1 to 4 to clean data. Step 5 and 6 are very related to language, and because of limitations of libraries for Dutch, I skipped these two steps. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_feature=text_feature.str.lower()\ntext_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing punctuations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_feature=text_feature.apply(lambda x : remove_punct(x))\ntext_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove stop-words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_stopwords(text):\n    pattern = re.compile(r'\\b(' + r'|'.join(stop) + r')\\b\\s*')\n    text = pattern.sub(' ', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_feature=text_feature.apply(lambda x : clean_stopwords(x))\ntext_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"**Ngram analysis**\n\nwords can be more meaningful when the comes together. for example \"fish\" has less information than \"aquatics fish\" and \"food fish\". So, we need to extract phrases with length more than 1 for analysis. \nIn this assignment, I extracted bi-grams (phrases with length 2) and tri-grams (phrases with length 3):\n"},{"metadata":{},"cell_type":"markdown","source":"**Bi-gram**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_text_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ntop_text_bigrams=get_top_text_bigrams(text_feature)[:10]\nx,y=map(list,zip(*top_text_bigrams))\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tri-gram"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_text_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ntop_text_trigrams=get_top_text_trigram(text_feature)[:10]\nx,y=map(list,zip(*top_text_trigrams))\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Supervised Classification:**"},{"metadata":{},"cell_type":"markdown","source":"First solution is simply vectorize text into a **bag of words** representation (word or n-gram frequencies) "},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(ngram_range=(1,4))\ntext_feature_vector= vectorizer.fit_transform(text_feature)\nprint(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_feature_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_feature_vector=text_feature_vector.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting matrix to numpy array\ntext_feature_vector=np.array(text_feature_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting dataframe to numpy array\ncategorical_features=categorical_features.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can combine these vectors by numerical vectors, and create a new Dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features_vectors=[]\nfor i in range(len(categorical_features)):\n    vector=np.concatenate((categorical_features, text_feature_vector), axis=None)\n    final_features_vectors.append(vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features_vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature vector is ready now to be fed into a classifier. In this assignemt, I choosed SVM classifier from sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC(gamma='auto')\nclf.fit(final_features_vectors, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is trained and ready for making prediction. In a real application, we have to split test and train and do the prediction on test data. But in this assignement we have to use the same training data for testing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=clf.predict(final_features_vectors)\npredictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To evaluate the model, I calculte the accuracy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(predictions,realValues):\n    correct=0\n    incorrect=0\n    for i in range(len(predictions)):\n        if predictions[i]==realValues[i]:\n            correct=correct+1\n        else:\n            incorrect=incorrect+1\n    return correct/(correct+incorrect)\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(predictions,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As results show, the accuracy is 50% which is low. We have many options to improve the accuracy:\n\n1. changing feature selection method\n2. changing different parameteres that we used during impelementation\n3. changing classification method\n4. a mixture of all above items\n"},{"metadata":{},"cell_type":"markdown","source":"**Word Embedding & A simple LSTM Deep Neural Network**\n\nIn the first classification method, I used Bag of Words as feature selection method. However, Bag of word is only big for very small datasets. \n\nOne state-of-the-art method to handle this problem is using word-embedding method. Three popular word-embedding methods, Word2vec, Glove and FastText have been commonly used. We can train or use a pre-trained model. Each of these pre-trained models have a vocabulary in addition to a vector for each word and thus for each language a different pre-trained model is required. \n\nBecause I only had two observations, I could not train a model, so I tried to find a pre-trained model for Dutch langauge. I found that Fasttext has one pre-trained model for Dutch. \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# please un-comment these lines to download fasttext model:\n# import urllib.request\n# urllib.request.urlretrieve(\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\", \"cc.nl.300.vec\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('/kaggle/input/fasttext-dutch/cc.nl.300.vec','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(text_feature)\nsequences=tokenizer_obj.texts_to_sequences(text_feature)\n\ntext_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"found=0\nnot_found=0\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,300))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n        found=found+1\n    else:\n        not_found=not_found+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"number of words which found a vector from vocabulary is: \",found)\nprint(\"number of words which haven't found a vector from vocabulary is: \",not_found)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can use the embedding matrix as the weights to convert input text to vectors while feeding it into a LSTM deep neural network. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nembedding=Embedding(num_words, 300,\n          weights=[embedding_matrix], input_length=MAX_LEN, trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=optimizers.Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(text_pad,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is trained so we can use it for prediction :"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=model.predict(text_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_binary_output(val):\n    if val>=0.5:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=[get_binary_output(val) for val in predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(predictions, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have a structured feature list as well, I added it as an input to a middle layer of LSTM:\n<img src=\"http://digital-thinking.de/wp-content/uploads/2018/07/combine.png\" width=\"20%\">\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_input=Input(shape=(MAX_LEN,),name='nlp_input')\ncategorical_input = Input((10,))\n\n\nembedding=Embedding(input_dim=num_words,output_dim=300,weights=[embedding_matrix], input_length=MAX_LEN, trainable=False)(nlp_input)\ndrop_layer=SpatialDropout1D(0.2)(embedding)\nLstm_layer = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(drop_layer)\n\n# Concatenate the convolutional features and the vector input\nconcat_layer= Concatenate()([categorical_input, Lstm_layer])\noutput = Dense(1, activation='sigmoid')(concat_layer)\n\n# define a model with a list of two inputs\nmodel = Model(inputs=[nlp_input, categorical_input], outputs=output)\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([text_pad,categorical_features],y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=model.predict([text_pad,categorical_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=[get_binary_output(val) for val in predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_accuracy(predictions, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Unsupervised classification **(Categorization)\n\nIn this group of methods, we don't considers the labels of documnets for training a classifier. \nThere are many methods for clustering, in this assignment, I impelemented two of them:\n\n- Agglomerative Clustering\n- Topic Modeling\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Agglomerative Clustering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_Hierarchical_Clusters(words_vectors,NUMBER_OF_CLUSTERS):\n\n    cluster = AgglomerativeClustering(n_clusters=NUMBER_OF_CLUSTERS, affinity='euclidean', linkage='ward')\n#     fit_predict fits the hierarchical clustering from features, and return cluster labels.\n    cluster_labels= cluster.fit_predict(words_vectors)\n    return cluster_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = get_Hierarchical_Clusters(final_features_vectors, 2)\nclusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Topic modeling is a popular and easy-to-impelement unsupervised methods. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation as LDA\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nnumber_words=5\n\ndef lda(data, NUMBER_OF_CLUSTERS):\n    count_vectorizer = CountVectorizer(stop)\n    count_data = count_vectorizer.fit_transform(data)\n\n    lda = LDA(n_components=NUMBER_OF_CLUSTERS)\n    lda.fit(count_data)\n    \n    all_topics_words=[]\n    words=count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(lda.components_):\n        each_topic_words=([words[i] for i in topic.argsort()[:-number_words - 1:-1]])\n        # each_topic_words=([words[i] for i  in topic.argsort()[::-1]])\n        all_topics_words.append(each_topic_words)\n\n    return lda, all_topics_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUMBER_OF_CLUSTERS=2\nlda, topic_words=lda(text_feature,NUMBER_OF_CLUSTERS)\n#printing topic words:\nfor i in range(NUMBER_OF_CLUSTERS):\n    print(\"topic \"+str(i)+\": \"+'[%s]' % ', '.join(map(str, topic_words[i])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clustering based on the lda model:\ncount_vectorizer = CountVectorizer(stop)\ncount_data = count_vectorizer.fit_transform(text_feature)\nlda.transform(count_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As resluts shows, the first document is more likely to belong to topic 1; the second topic is more likely to belong to topic 0;"},{"metadata":{},"cell_type":"markdown","source":"**Semi-supervised classification **\n\nSince the labels (\"Urgent indication\") is not perfect, we cannot use them for classification. However, there is another dataset based on the medication / referral as done by the doctor. The lables in cloumn \"Urgent in hindsight\" seem to be correct. The problem is that, we only have these labels if, prior to visiting doctor, \"Urgent indication\" is urgent. So, we have the correct lables for patients who are reffered to doctor, but we don't have correct label for the other group. "},{"metadata":{},"cell_type":"markdown","source":"<img src=\"http://parvaneh.me/wp-content/uploads/2020/02/screenshot_20200212_170221.png\">"},{"metadata":{},"cell_type":"markdown","source":"In semi-supervised classification, the labels of unlabled data are found based on the labled ones ( for example by co-training mothod ) and then the whole dataset can be used to train a supervised classifier. \n\nDue to the time constraint for this assignment, I have left this last part for future endeavours."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}