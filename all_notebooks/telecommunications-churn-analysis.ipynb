{"cells":[{"metadata":{"_uuid":"a800536c7240d473c02483aa9abec3372d775483"},"cell_type":"markdown","source":"# Evaluating Customer Churn for a Telecommunications Company"},{"metadata":{"_uuid":"bdc2ce36e3583317d966642f4ab0abf457cc804f"},"cell_type":"markdown","source":"This machine learning project predicts customer churn of a telecommunications company. This project is a Kaggle project. The link to the project is: https://www.kaggle.com/blastchar/telco-customer-churn/home. "},{"metadata":{"trusted":false,"_uuid":"4b2ec7b310a197303a25101e81a3bc8fc0d461af"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom numpy import around\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"01e0aa09ab7c151cdd9ada6c6270749452ddcda3"},"cell_type":"code","source":"#  Metrics\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n# Models\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45d1baafdedb1e8305356df8edd8dc5a8c4ab5a8"},"cell_type":"markdown","source":"### Import data and convert churn yes/no values into binary"},{"metadata":{"trusted":false,"_uuid":"ee390e3932303ba2ca5f5a4e5b1d5ce7f3832832"},"cell_type":"code","source":"# Import Data\ndf = pd.read_csv(\"../input/telecommunications-churn/train-scaled.csv\")\n# Change Churn from string (yes/no) to binary.\ny = pd.Series(np.where(df.Churn.values == 'Yes', 1, 0),\n          df.index)\n# Gather the ID\ncustomer_id = df[\"customerID\"]\n# Training data\nX = df.drop([\"Unnamed: 0\", \"customerID\", \"Churn\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5e2a7fcfeedd9ed6aa9e6e1a75be4559afac2c2b"},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ad209112a3f8c048ed7248d94855e25625301b2"},"cell_type":"markdown","source":"### Feature selection and PCA"},{"metadata":{"_uuid":"57f82107c4d1eca9c0318f87582b9401d133dbaa"},"cell_type":"markdown","source":"We have 7043 records and 61 features. Although 61 features may not seem a lot, when I use K-Cross Fold validation with the SVM algorithm, it takes considerable amount of time. "},{"metadata":{"_uuid":"2645254251c3d9049c4b020a32a44159c39b1744"},"cell_type":"markdown","source":"Our goal is to reduce the feature set while retaining the most important information for the model."},{"metadata":{"_uuid":"02a95c038bca9c8074ee4b5ff375be11582f8eb3"},"cell_type":"markdown","source":"The feature selection algorithm we will use is MRMR. MRMR is a filter based feature selection algorithm which tries to select the most relevant features with the target class labels and minimize the redundancy\namong those selected features simultaneously. Thanks to [Hanchuan Peng](http://home.penglab.com/proj/mRMR/) for creating an [implementation](https://github.com/fbrundu/pymrmr) of MRMR for python. "},{"metadata":{"_uuid":"3d36f9ee553d54c33381e2171938e4ae93f3a417"},"cell_type":"markdown","source":"Note: We will be using the Mutual Information Difference (MID) feature evaluation meth-\nod inside MRMR. Thanks to Gokhan Gulgezen, Zehra Cataltepe, and Lei Yu for their [analysis of the MRMR selection algorithms](http://web.itu.edu.tr/~cataltepe/pdf/2009_ECMLGulgezen.pdf). They found that MID was the most stable feature selection algorithm out of all the MRMR selction algorithms."},{"metadata":{"trusted":false,"_uuid":"ec39cf5cff8bbc66827227225c9c563cf6a8899f"},"cell_type":"code","source":"# Feature Selection algorithm\nfrom pymrmr import mRMR\n# Selects the import features\nf_select = mRMR(X, 'MID', 10)\n# Important features\nf_select","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3310d2074a64256fc6f06a69d11244f8c24d8801"},"cell_type":"markdown","source":"We use PCA to transform and decouple the dataset defined by the selected features"},{"metadata":{"trusted":false,"_uuid":"89eb8b966950fd7adb2a18206503660e36fa4e8b"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()  \nXX = pca.fit_transform(X[f_select]) # XX is out new training matrix that has two basis vectors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c17d76a35482a2b2546947817ab39afce2de7033"},"cell_type":"markdown","source":"### Creating training and test sets"},{"metadata":{"trusted":false,"_uuid":"ad30b826e1b009c43328b28cb474728b0d7b58a9"},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(XX, y, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6e12ccf44308f1158905c1c8666d2677d9a1c09b"},"cell_type":"code","source":"# prepare configuration for cross validation test harness\nseed = 0\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(random_state=seed)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('RF', RandomForestClassifier(n_estimators=10, criterion='gini', random_state=seed)))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(kernel='rbf', probability=True, random_state=seed)))\nmodels.append(('ADA', AdaBoostClassifier(n_estimators=50, learning_rate=1,random_state=seed)))\nmodels.append(('XGB', XGBClassifier(max_depth=3, n_estimators=100, learning_rate=0.05)))\n\n\n# evaluate each model in turn\ncross_val_results = list()\naccuracy_results = list()\nnames = list()\nscoring = 'accuracy'\n\nfor name, model in models:\n    print(\"-\" * 70)\n    print(\"This is {name} model.\".format(name=name))\n    print(\"-\" * 70)\n    # Test each model on one run through    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    # Making the Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # Model statistics for using the test data in the training set\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_results.append(accuracy)\n    print('This is the accuracy score: {}'.format(accuracy))\n    print('Here is the classification report: ')\n    print(classification_report(y_test, y_pred))\n    print('Confusion Matrix')\n    print(cm)\n    print('\\n')\n    \n    # Test model on its cross-validation score\n    cv_results = cross_val_score(model, X_test, y_test, cv=10, scoring=scoring)\n    cross_val_results.append(cv_results)\n    names.append(name)\n    msg = \"Cross validation score of %s Model: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    print(\"-\" * 70)\n    print('\\n\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc6070c9f04607cfcece65ac49d811ec2310ea74"},"cell_type":"markdown","source":"## Results"},{"metadata":{"_uuid":"67a78cfa9764320ce0681ffa162f2392883ba284"},"cell_type":"markdown","source":"Judging by the accuracy, cross validation accuracy, and F-1 scores, the top 3 algorithms are: XGBoost, AdaBoost, and Logistic regression. Overall, the models did well. Their mean accuracy and cross validation accuracy is 77% each.  "},{"metadata":{"_uuid":"9ee0f85d5ec7a3450ce69e7136295b6b2a117c41"},"cell_type":"markdown","source":"For a telecommunications company, it is important to identify which customers may their service. Since telecommunications comapnies make their money by long-term contracts. If they can keep their customers from leaving their firm, then their business will not decline."},{"metadata":{"_uuid":"5648ac1c779fd82657e8a53d4dfa73e7718da4f2"},"cell_type":"markdown","source":"Businesses want to minimize the risk that they misidentify a customer who is leaving the firm, which is called a false negative. However, it is also important that the firm does not overcorrect and overly identify people who are staying with the firm as those who may leave. This is wasted marketing dollars. "},{"metadata":{"_uuid":"3022bae236949ec63061560277a489e33911bb41"},"cell_type":"markdown","source":"There is a trade off between identifying which customers are either staying or leaving the firm and minimizing false negatives and positives. XGBoost, AdaBoost, and Logistic Regression have high accuracy scores but average false negative and false positive rates."},{"metadata":{"_uuid":"4b93ee302623d67a73c85a304ed3f4ebdd79ca7a"},"cell_type":"markdown","source":"## Potential Improvements"},{"metadata":{"_uuid":"6dfa54ee48f4271ec008a7dc523885b0379e8982"},"cell_type":"markdown","source":"One can potentially improve this analysis by implementing ensemble methods and compare their results to XGBoost, AdaBoost, and Logistic regression. Another potential improvement is to use grid search to optimize the parameters of feature selection, PCA, and the various machine learning algorithms used."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}