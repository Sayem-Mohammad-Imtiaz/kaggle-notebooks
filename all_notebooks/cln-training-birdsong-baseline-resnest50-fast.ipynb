{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my attempt to refactor and clean [this](https://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast/) excellent notebook. It's work in progress and I'll update it as I go.\n\nType-checking, style checking, and formatting done with `black`, `flake8`, `isort`, and `mypy` via [nbQA](https://github.com/nbQA-dev/nbQA)\n\nConfigurations used:\n\n`.pre-commit-config.yaml`:\n\n```yaml\n  - repo: https://github.com/nbQA-dev/nbQA\n    rev: 0.1.20\n    hooks:\n      - id: nbqa\n        args: ['black']\n        name: nbqa-black\n        additional_dependencies: ['black']\n      - id: nbqa\n        args: ['flake8']\n        name: nbqa-flake8\n        additional_dependencies: ['flake8']\n      - id: nbqa\n        args: ['isort']\n        name: nbqa-isort\n        additional_dependencies: ['isort']\n      - id: nbqa\n        args: ['blackdoc']\n        name: nbqa-blackdoc\n        additional_dependencies: ['blackdoc']\n      - id: nbqa\n        args: ['mypy']\n        name: nbqa-mypy\n        additional_dependencies: ['mypy']\n      - id: nbqa\n        args: ['pydocstyle']\n        name: nbqa-pydocstyle\n        additional_dependencies: ['pydocstyle']\n```\n\n`.nbqa.ini`:\n\n```ini\n[black]\naddopts = --line-length=96\nmutate = 1\n\n[flake8]\nconfig=.flake8\n\n[isort]\naddopts = --profile=black\nmutate = 1\n\n[blackdoc]\naddopts = --line-length=96\nmutate = 1\n\n[mypy]\naddopts = --ignore-missing-imports --disallow-untyped-defs\n\n[pydocstyle]\naddopts = --add-ignore=D100,D101,D105,D103,D107\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Birdsong Pytorch Baseline: ResNeSt50-fast (Training)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## About\n\nIn this notebook, I try ResNeSt, which is the one of state of the art in image recognition.  \n\nFor the fair comparison with @hidehisaarai1213 's [great baseline](https://www.kaggle.com/hidehisaarai1213/inference-pytorch-birdcall-resnet-baseline), I used a model with the same depth and as the same experimental settings as possible. But There are some differences mainly because of the GPU resource limitation.\n\nThe experimental settings are as follows:\n\n* Randomly crop 5 seconds for each train audio clip each epoch.\n* No augmentation.\n* Used pretrained weight of _`ResNeSt50-fast-1s1x64d`_ provided by the authors at [their repository](https://github.com/zhanghang1989/ResNeSt).\n* Used `BCELoss`\n* Trained **_50_** epoch and saved the weight which got best **_loss_** (this is because f1 score relies on thresholds.)\n* `Adam` optimizer (`lr=0.001`) with `CosineAnnealingLR` (`T_max=10`).\n* Used `StratifiedKFold(n_splits=5)` to split dataset and used only first fold\n* `batch_size`: **_50_**\n* melspectrogram parameters\n  - `n_mels`: 128\n  - `fmin`: 20\n  - `fmax`: 16000\n* image size: 224x547\n\nI forked a lot of codes such as preprocessing from @hidehisaarai1213 's [notebook](https://www.kaggle.com/hidehisaarai1213/inference-pytorch-birdcall-resnet-baseline) and [GitHub repository](https://github.com/koukyo1994/kaggle-birdcall-resnet-baseline-training). Many thanks!!!\n\n\n### Note\n\n#### about dataset\nI prepared resmpaled train dataset for this notebook, see more details in:\nhttps://www.kaggle.com/c/birdsong-recognition/discussion/164197\n\n\n#### about custom packages\nIn this **_training notebook_**, I used two custom packages, `pytorch-pfn-extras` for training and the authors' official implementation of `ResNeSt` for building model.  \nOn the other hand, as stated in [code requirements](https://www.kaggle.com/c/birdsong-recognition/overview/code-requirements), participants are **not allowed** to use custom packages in **_submission notebook_**.\n\nIf you fork this notebook, keep the above things in mind.\n\n\n### Reference\n\n#### ResNeSt: Split-Attention Networks\n* author: Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Muller, R. Manmatha, Mu Li and Alex Smola \n* paper: [arXiv 2004.08955](https://arxiv.org/abs/2004.08955)\n* code: [GitHub](https://github.com/koukyo1994/kaggle-birdcall-resnet-baseline-training)\n\n#### pytorch-pfn-extras\n* author: Preferred Networks, Inc.\n* code: [GitHub](https://github.com/pfnet/pytorch-pfn-extras)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prepare","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### import libraries","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"!pip install ../input/pytorch-pfn-extras/pytorch-pfn-extras-0.2.1/\n!pip install ../input/resnest50-fast-package/resnest-0.0.6b20200701/resnest/","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import gc\nimport os\nimport random\nimport shutil\nimport typing as tp\nfrom pathlib import Path\n\nimport cv2\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport pytorch_pfn_extras as ppe\nimport resnest.torch as resnest_torch\nimport soundfile as sf\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport yaml\nfrom pytorch_pfn_extras.training import extensions as ppe_extensions\nfrom sklearn.model_selection import StratifiedKFold\n\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"QUICK_RUN = True\n\nif QUICK_RUN:\n    NUM_EPOCHS = 1\n    N_SPLITS = 2\nelse:\n    NUM_EPOCHS = 50\n    N_SPLITS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Path(\"/root/.cache/torch/checkpoints\").mkdir(parents=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!cp ../input/resnest50-fast-package/resnest50_fast_*.pth /root/.cache/torch/checkpoints/","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"### define utilities","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def set_seed(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### read data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT_ROOT = ROOT / \"input\"\nRAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\nTRAIN_RESAMPLED_AUDIO_DIRS = [\n    INPUT_ROOT / f\"birdsong-resampled-train-audio-{i:0>2}\" for i in range(5)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv(TRAIN_RESAMPLED_AUDIO_DIRS[0] / \"train_mod.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"train.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### settings","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"settings_str = f\"\"\"\nglobals:\n  seed: 1213\n  device: cuda\n  num_epochs: {NUM_EPOCHS}\n  output_dir: /kaggle/training_output/\n  use_fold: 0\n\ndataset:\n  name: SpectrogramDataset\n  params:\n    img_size: 224\n    melspectrogram_parameters:\n      n_mels: 128\n      fmin: 20\n      fmax: 16000\n\nsplit:\n  name: StratifiedKFold\n  params:\n    n_splits: {N_SPLITS}\n    random_state: 42\n    shuffle: True\n\nloader:\n  train:\n    batch_size: 50\n    shuffle: True\n    num_workers: 2\n    pin_memory: True\n    drop_last: True\n  val:\n    batch_size: 100\n    shuffle: False\n    num_workers: 2\n    pin_memory: True\n    drop_last: False\n\nmodel:\n  name: resnest50_fast_1s1x64d\n  params:\n    pretrained: True\n    n_classes: 264\n\nloss:\n  name: BCEWithLogitsLoss\n  params: {{}}\n\noptimizer:\n  name: Adam\n  params:\n    lr: 0.001\n\nscheduler:\n  name: CosineAnnealingLR\n  params:\n    T_max: 10\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"settings = yaml.safe_load(settings_str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### preprocess audio data\n\nCode is forked from: https://github.com/koukyo1994/kaggle-birdcall-resnet-baseline-training/blob/master/input/birdsong-recognition/prepare.py\n\nI modified this partially. \n\nHowever, in this notebook, I used uploaded resampled audio because this preprocessing is too heavy for kaggle notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Definition","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dataset\n* forked from: https://github.com/koukyo1994/kaggle-birdcall-resnet-baseline-training/blob/master/src/dataset.py\n* modified partialy\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"BIRD_CODE = {\n    \"aldfly\": 0,\n    \"ameavo\": 1,\n    \"amebit\": 2,\n    \"amecro\": 3,\n    \"amegfi\": 4,\n    \"amekes\": 5,\n    \"amepip\": 6,\n    \"amered\": 7,\n    \"amerob\": 8,\n    \"amewig\": 9,\n    \"amewoo\": 10,\n    \"amtspa\": 11,\n    \"annhum\": 12,\n    \"astfly\": 13,\n    \"baisan\": 14,\n    \"baleag\": 15,\n    \"balori\": 16,\n    \"banswa\": 17,\n    \"barswa\": 18,\n    \"bawwar\": 19,\n    \"belkin1\": 20,\n    \"belspa2\": 21,\n    \"bewwre\": 22,\n    \"bkbcuc\": 23,\n    \"bkbmag1\": 24,\n    \"bkbwar\": 25,\n    \"bkcchi\": 26,\n    \"bkchum\": 27,\n    \"bkhgro\": 28,\n    \"bkpwar\": 29,\n    \"bktspa\": 30,\n    \"blkpho\": 31,\n    \"blugrb1\": 32,\n    \"blujay\": 33,\n    \"bnhcow\": 34,\n    \"boboli\": 35,\n    \"bongul\": 36,\n    \"brdowl\": 37,\n    \"brebla\": 38,\n    \"brespa\": 39,\n    \"brncre\": 40,\n    \"brnthr\": 41,\n    \"brthum\": 42,\n    \"brwhaw\": 43,\n    \"btbwar\": 44,\n    \"btnwar\": 45,\n    \"btywar\": 46,\n    \"buffle\": 47,\n    \"buggna\": 48,\n    \"buhvir\": 49,\n    \"bulori\": 50,\n    \"bushti\": 51,\n    \"buwtea\": 52,\n    \"buwwar\": 53,\n    \"cacwre\": 54,\n    \"calgul\": 55,\n    \"calqua\": 56,\n    \"camwar\": 57,\n    \"cangoo\": 58,\n    \"canwar\": 59,\n    \"canwre\": 60,\n    \"carwre\": 61,\n    \"casfin\": 62,\n    \"caster1\": 63,\n    \"casvir\": 64,\n    \"cedwax\": 65,\n    \"chispa\": 66,\n    \"chiswi\": 67,\n    \"chswar\": 68,\n    \"chukar\": 69,\n    \"clanut\": 70,\n    \"cliswa\": 71,\n    \"comgol\": 72,\n    \"comgra\": 73,\n    \"comloo\": 74,\n    \"commer\": 75,\n    \"comnig\": 76,\n    \"comrav\": 77,\n    \"comred\": 78,\n    \"comter\": 79,\n    \"comyel\": 80,\n    \"coohaw\": 81,\n    \"coshum\": 82,\n    \"cowscj1\": 83,\n    \"daejun\": 84,\n    \"doccor\": 85,\n    \"dowwoo\": 86,\n    \"dusfly\": 87,\n    \"eargre\": 88,\n    \"easblu\": 89,\n    \"easkin\": 90,\n    \"easmea\": 91,\n    \"easpho\": 92,\n    \"eastow\": 93,\n    \"eawpew\": 94,\n    \"eucdov\": 95,\n    \"eursta\": 96,\n    \"evegro\": 97,\n    \"fiespa\": 98,\n    \"fiscro\": 99,\n    \"foxspa\": 100,\n    \"gadwal\": 101,\n    \"gcrfin\": 102,\n    \"gnttow\": 103,\n    \"gnwtea\": 104,\n    \"gockin\": 105,\n    \"gocspa\": 106,\n    \"goleag\": 107,\n    \"grbher3\": 108,\n    \"grcfly\": 109,\n    \"greegr\": 110,\n    \"greroa\": 111,\n    \"greyel\": 112,\n    \"grhowl\": 113,\n    \"grnher\": 114,\n    \"grtgra\": 115,\n    \"grycat\": 116,\n    \"gryfly\": 117,\n    \"haiwoo\": 118,\n    \"hamfly\": 119,\n    \"hergul\": 120,\n    \"herthr\": 121,\n    \"hoomer\": 122,\n    \"hoowar\": 123,\n    \"horgre\": 124,\n    \"horlar\": 125,\n    \"houfin\": 126,\n    \"houspa\": 127,\n    \"houwre\": 128,\n    \"indbun\": 129,\n    \"juntit1\": 130,\n    \"killde\": 131,\n    \"labwoo\": 132,\n    \"larspa\": 133,\n    \"lazbun\": 134,\n    \"leabit\": 135,\n    \"leafly\": 136,\n    \"leasan\": 137,\n    \"lecthr\": 138,\n    \"lesgol\": 139,\n    \"lesnig\": 140,\n    \"lesyel\": 141,\n    \"lewwoo\": 142,\n    \"linspa\": 143,\n    \"lobcur\": 144,\n    \"lobdow\": 145,\n    \"logshr\": 146,\n    \"lotduc\": 147,\n    \"louwat\": 148,\n    \"macwar\": 149,\n    \"magwar\": 150,\n    \"mallar3\": 151,\n    \"marwre\": 152,\n    \"merlin\": 153,\n    \"moublu\": 154,\n    \"mouchi\": 155,\n    \"moudov\": 156,\n    \"norcar\": 157,\n    \"norfli\": 158,\n    \"norhar2\": 159,\n    \"normoc\": 160,\n    \"norpar\": 161,\n    \"norpin\": 162,\n    \"norsho\": 163,\n    \"norwat\": 164,\n    \"nrwswa\": 165,\n    \"nutwoo\": 166,\n    \"olsfly\": 167,\n    \"orcwar\": 168,\n    \"osprey\": 169,\n    \"ovenbi1\": 170,\n    \"palwar\": 171,\n    \"pasfly\": 172,\n    \"pecsan\": 173,\n    \"perfal\": 174,\n    \"phaino\": 175,\n    \"pibgre\": 176,\n    \"pilwoo\": 177,\n    \"pingro\": 178,\n    \"pinjay\": 179,\n    \"pinsis\": 180,\n    \"pinwar\": 181,\n    \"plsvir\": 182,\n    \"prawar\": 183,\n    \"purfin\": 184,\n    \"pygnut\": 185,\n    \"rebmer\": 186,\n    \"rebnut\": 187,\n    \"rebsap\": 188,\n    \"rebwoo\": 189,\n    \"redcro\": 190,\n    \"redhea\": 191,\n    \"reevir1\": 192,\n    \"renpha\": 193,\n    \"reshaw\": 194,\n    \"rethaw\": 195,\n    \"rewbla\": 196,\n    \"ribgul\": 197,\n    \"rinduc\": 198,\n    \"robgro\": 199,\n    \"rocpig\": 200,\n    \"rocwre\": 201,\n    \"rthhum\": 202,\n    \"ruckin\": 203,\n    \"rudduc\": 204,\n    \"rufgro\": 205,\n    \"rufhum\": 206,\n    \"rusbla\": 207,\n    \"sagspa1\": 208,\n    \"sagthr\": 209,\n    \"savspa\": 210,\n    \"saypho\": 211,\n    \"scatan\": 212,\n    \"scoori\": 213,\n    \"semplo\": 214,\n    \"semsan\": 215,\n    \"sheowl\": 216,\n    \"shshaw\": 217,\n    \"snobun\": 218,\n    \"snogoo\": 219,\n    \"solsan\": 220,\n    \"sonspa\": 221,\n    \"sora\": 222,\n    \"sposan\": 223,\n    \"spotow\": 224,\n    \"stejay\": 225,\n    \"swahaw\": 226,\n    \"swaspa\": 227,\n    \"swathr\": 228,\n    \"treswa\": 229,\n    \"truswa\": 230,\n    \"tuftit\": 231,\n    \"tunswa\": 232,\n    \"veery\": 233,\n    \"vesspa\": 234,\n    \"vigswa\": 235,\n    \"warvir\": 236,\n    \"wesblu\": 237,\n    \"wesgre\": 238,\n    \"weskin\": 239,\n    \"wesmea\": 240,\n    \"wessan\": 241,\n    \"westan\": 242,\n    \"wewpew\": 243,\n    \"whbnut\": 244,\n    \"whcspa\": 245,\n    \"whfibi\": 246,\n    \"whtspa\": 247,\n    \"whtswi\": 248,\n    \"wilfly\": 249,\n    \"wilsni1\": 250,\n    \"wiltur\": 251,\n    \"winwre3\": 252,\n    \"wlswar\": 253,\n    \"wooduc\": 254,\n    \"wooscj2\": 255,\n    \"woothr\": 256,\n    \"y00475\": 257,\n    \"yebfly\": 258,\n    \"yebsap\": 259,\n    \"yehbla\": 260,\n    \"yelwar\": 261,\n    \"yerwar\": 262,\n    \"yetvir\": 263,\n}\n\nINV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"PERIOD = 5\n\n\ndef mono_to_color(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n    \"\"\"\n    Make 2D image 3D. Normalise and scale to be between 0 and 255.\n\n    Parameters\n    ----------\n    X\n        2D image.\n    eps\n        Epsilon, small number to add to std to avoid dividing by zero.\n\n    Returns\n    -------\n    np.ndarray\n        3D image, normalised, scaled.\n    \"\"\"\n    original_shape = X.shape\n    assert len(X.shape) == 2\n\n    X = np.stack([X, X, X], axis=-1)\n    assert X.shape == (*original_shape, 3)\n\n    mean = X.mean()\n    X = X - mean\n    std = X.std()\n    Xstd = X / (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    if (_max - _min) > eps:\n        V = Xstd\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\n\nclass SpectrogramDataset(data.Dataset):\n    def __init__(\n        self,\n        file_list: tp.List[tp.List[str]],\n        img_size: int,\n        melspectrogram_parameters: tp.Dict[str, int],\n    ):\n        \"\"\"\n        Initialise.\n\n        Parameters\n        ----------\n        file_list\n            List of pairs of elements, which are [file_path, ebird_code].\n        img_size\n            Desired width of image that'll be obtained by converting audio.\n        melspectrogram_parameters\n            Parameters to be passed on to `librosa.feature.melspectrogram`.\n        \"\"\"\n        assert all(len(i) == 2 for i in file_list)\n\n        self.file_list = file_list\n        self.img_size = img_size\n        self.melspectrogram_parameters = melspectrogram_parameters\n\n    def __len__(self) -> int:\n        return len(self.file_list)\n\n    def __getitem__(self, idx: int) -> tp.Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Get item.\n\n        Steps are:\n\n        - If audio is too short, pad it with zeros.\n          If it's too long, randomly select a subset of it.\n        - Compute the mel-scaled (power) spectrogram of the signal\n        - Convert the power spectrogram to decibel units.\n        - Stack signal so as to get 3D image, and normalise\n\n        Parameters\n        ----------\n        idx\n            Index\n\n        Returns\n        -------\n        image\n            Processed version of audio.\n        labels\n            Array of zeros, equals one in position corresponding\n            to this item's target.\n        \"\"\"\n        wav_path, ebird_code = self.file_list[idx]\n\n        audio, sound_rate = sf.read(wav_path)\n\n        len_audio = len(audio)\n        effective_length = sound_rate * PERIOD\n\n        if len_audio < effective_length:\n            new_audio = np.zeros(effective_length, dtype=audio.dtype)\n            start = np.random.randint(effective_length - len_audio)\n            new_audio[start : start + len_audio] = audio\n            audio = new_audio.astype(np.float32)\n        elif len_audio > effective_length:\n            start = np.random.randint(len_audio - effective_length)\n            audio = audio[start : start + effective_length].astype(np.float32)\n        else:\n            audio = audio.astype(np.float32)\n\n        melspec = librosa.feature.melspectrogram(\n            audio, sr=sound_rate, **self.melspectrogram_parameters\n        )\n        # Note: 512 is the default hop length\n        n_mels = self.melspectrogram_parameters[\"n_mels\"]\n        assert melspec.shape == (n_mels, 1 + len(audio) // 512)\n\n        melspec = librosa.power_to_db(melspec).astype(np.float32)\n        assert melspec.shape == (n_mels, 1 + len(audio) // 512)\n\n        image = mono_to_color(melspec)\n        assert image.shape == (n_mels, 1 + len(audio) // 512, 3)\n\n        height, width, _ = image.shape\n        image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n        assert image.shape == (self.img_size, int(width / height * self.img_size), 3)\n\n        image = np.moveaxis(image, 2, 0)\n        assert image.shape == (3, self.img_size, int(width / height * self.img_size))\n\n        image = (image / 255.0).astype(np.float32)\n\n        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n        labels[BIRD_CODE[ebird_code]] = 1\n\n        return image, labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Utility","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"def get_loaders_for_training(\n    args_dataset: tp.Dict,\n    args_loader: tp.Dict,\n    train_file_list: tp.List[tp.List[str]],\n    val_file_list: tp.List[tp.List[str]],\n) -> tp.Tuple[data.DataLoader, data.DataLoader]:\n    \"\"\"\n    Make dataloaders from datasets and filelists.\n\n    Parameters\n    ----------\n    args_dataset\n        Additional arguments to pass to SpectrogramDataset\n    args_loader\n        Additional arguments to pass to DataLoader\n    train_file_list\n        List of pairs of elements, which are [file_path, ebird_code], for train.\n    val_file_list\n        List of pairs of elements, which are [file_path, ebird_code], for train.\n    \"\"\"\n    train_dataset = SpectrogramDataset(train_file_list, **args_dataset)\n    val_dataset = SpectrogramDataset(val_file_list, **args_dataset)\n\n    train_loader = data.DataLoader(train_dataset, **args_loader[\"train\"])\n    val_loader = data.DataLoader(val_dataset, **args_loader[\"val\"])\n\n    return train_loader, val_loader","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"def get_model(\n    args: tp.Dict[str, tp.Union[str, tp.Dict[str, tp.Union[bool, int]]]]\n) -> nn.Module:\n    \"\"\"\n    Get pre-trained model and customise head.\n\n    Parameters\n    ----------\n    args\n        Additional arguments to pass to Pytorch model\n    \"\"\"\n    name = args[\"name\"]\n    assert isinstance(name, str)\n    params = args[\"params\"]\n    assert isinstance(params, dict)\n    pretrained = params[\"pretrained\"]\n    assert isinstance(pretrained, bool)\n    n_classes = params[\"n_classes\"]\n    assert isinstance(n_classes, int)\n\n    model = getattr(resnest_torch, name)(pretrained=pretrained)\n    del model.fc\n    model.fc = nn.Sequential(\n        nn.Linear(2048, 1024),\n        nn.ReLU(),\n        nn.Dropout(p=0.2),\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Dropout(p=0.2),\n        nn.Linear(1024, n_classes),\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"def train_loop(\n    manager: ppe.training.ExtensionsManager,\n    model: nn.Module,\n    device: torch.device,\n    train_loader: data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    scheduler: torch.optim.lr_scheduler._LRScheduler,\n    loss_func: tp.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n) -> None:\n    \"\"\"\n    Run minibatch training loop.\n\n    Parameters\n    ----------\n    manager\n        Interface to extend training loop\n    model\n        Model (not yet fine-tuned)\n    device\n        GPU, CPU, ...\n    optimizer\n        Adapts learning rate for each weight.\n    scheduler\n        Decreases learning rate as training progresses\n    loss_func\n        Loss function\n    \"\"\"\n    while not manager.stop_trigger:\n        model.train()\n        for batch_idx, (data_, target) in enumerate(train_loader):\n            with manager.run_iteration():\n                data_, target = data_.to(device), target.to(device)\n                optimizer.zero_grad()\n                output = model(data_)\n                loss = loss_func(output, target)\n                ppe.reporting.report({\"train/loss\": loss.item()})\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n\n\ndef eval_for_batch(\n    model: nn.Module,\n    device: torch.device,\n    data_: torch.Tensor,\n    target: torch.Tensor,\n    loss_func: tp.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n) -> None:\n    \"\"\"\n    Run evaliation for valid for each batch of val loader.\n\n    Parameters\n    ----------\n    model\n        Trained model\n    device\n        GPU, CPU, ...\n    data_\n        features for batch of val data\n    target\n        target for batch of val data\n    loss_func\n        Loss function (here, BCEWithLogits)\n    \"\"\"\n    img_size = settings[\"dataset\"][\"params\"][\"img_size\"]\n    batch_size = settings[\"loader\"][\"val\"][\"batch_size\"]\n    assert data_.shape[0] <= batch_size\n    assert data_.shape[1] == 3\n    assert data_.shape[2] == img_size\n    # Last dimension is the height, which is variable\n    assert target.shape[0] <= batch_size\n    assert target.shape[1] == len(BIRD_CODE)\n\n    model.eval()\n    data_, target = data_.to(device), target.to(device)\n    output = model(data_)\n    val_loss = loss_func(output, target).item()\n    ppe.reporting.report({\"val/loss\": val_loss})","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"def set_extensions(\n    manager: ppe.training.ExtensionsManager,\n    model: nn.Module,\n    device: torch.device,\n    test_loader: data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    loss_func: tp.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n) -> ppe.training.ExtensionsManager:\n    \"\"\"\n    Configure extensions manager.\n\n    The extensions manager will:\n    - plot the train and val losses as a function of epoch number\n    - plot the learning rate\n    - print the train and val losses for each epoch and iteration and the\n      time each takes. This happens at every epoch, I think.\n    - save a snapshot of the model at each epoch. This happens when val\n      loss is lowest, I think.\n\n    Parameters\n    ----------\n    manager\n        Interface to extend training loop\n    device\n        CPU, GPU, ...\n    test_loader\n        Loader for evaluation data\n    optimizer\n        Adapt learning rate for each weight\n    loss_func\n        Loss function\n    \"\"\"\n    evaluation_extensions = (\n        ppe_extensions.Evaluator(\n            test_loader,\n            model,\n            eval_func=lambda data_, target: eval_for_batch(\n                model, device, data_, target, loss_func\n            ),\n            progress_bar=True,\n        ),\n        (1, \"epoch\"),\n    )\n    snapshot_extensions = (\n        ppe_extensions.snapshot(target=model, filename=\"snapshot_epoch_{.updater.epoch}.pth\"),\n        ppe.training.triggers.MinValueTrigger(key=\"val/loss\", trigger=(1, \"epoch\")),\n    )\n\n    my_extensions = [\n        ppe_extensions.observe_lr(optimizer=optimizer),\n        ppe_extensions.LogReport(),\n        ppe_extensions.PlotReport([\"train/loss\", \"val/loss\"], \"epoch\", filename=\"loss.png\"),\n        ppe_extensions.PlotReport([\"lr\"], \"epoch\", filename=\"lr.png\"),\n        ppe_extensions.PrintReport(\n            [\"epoch\", \"iteration\", \"lr\", \"train/loss\", \"val/loss\", \"elapsed_time\"]\n        ),\n        evaluation_extensions,\n        snapshot_extensions,\n    ]\n\n    for ext in my_extensions:\n        if isinstance(ext, tuple):\n            manager.extend(ext[0], trigger=ext[1])\n        else:\n            manager.extend(ext)\n\n    return manager","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### prepare data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### get wav file path","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"tmp_list = [\n    [ebird_dir.name, wav_file.name, wav_file.as_posix()]\n    for audio_dir in TRAIN_RESAMPLED_AUDIO_DIRS\n    for ebird_dir in audio_dir.iterdir()\n    if ebird_dir.name != \"train_mod.csv\"\n    for wav_file in ebird_dir.iterdir()\n]\n\ntrain_wav_path_exist = pd.DataFrame(\n    tmp_list, columns=[\"ebird_code\", \"resampled_filename\", \"file_path\"]\n)\n\ndel tmp_list\n\ntrain_all = pd.merge(\n    train,\n    train_wav_path_exist,\n    on=[\"ebird_code\", \"resampled_filename\"],\n    how=\"inner\",\n    validate=\"1:1\",\n)\n\nprint(train.shape)\nprint(train_wav_path_exist.shape)\nprint(train_all.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"train_all.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### split data","execution_count":null},{"metadata":{"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"skf = StratifiedKFold(**settings[\"split\"][\"params\"])\n\ntrain_all[\"fold\"] = -1\nfor fold_id, (train_index, val_index) in enumerate(\n    skf.split(train_all, train_all[\"ebird_code\"])\n):\n    train_all.iloc[val_index, -1] = fold_id\n\n# check the propotion\nfold_proportion = pd.pivot_table(\n    train_all, index=\"ebird_code\", columns=\"fold\", values=\"xc_id\", aggfunc=len\n)\nprint(fold_proportion.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"fold_proportion","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"use_fold = settings[\"globals\"][\"use_fold\"]\ntrain_file_list = train_all.query(\"fold != @use_fold\")[\n    [\"file_path\", \"ebird_code\"]\n].values.tolist()\nval_file_list = train_all.query(\"fold == @use_fold\")[\n    [\"file_path\", \"ebird_code\"]\n].values.tolist()\n\nprint(\"[fold {}] train: {}, val: {}\".format(use_fold, len(train_file_list), len(val_file_list)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## run training","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"set_seed(settings[\"globals\"][\"seed\"])\ndevice = torch.device(settings[\"globals\"][\"device\"])\noutput_dir = Path(settings[\"globals\"][\"output_dir\"])\n\ntrain_loader, val_loader = get_loaders_for_training(\n    settings[\"dataset\"][\"params\"], settings[\"loader\"], train_file_list, val_file_list\n)\n\nmodel = get_model(settings[\"model\"])\nmodel = model.to(device)\n\noptimizer = getattr(torch.optim, settings[\"optimizer\"][\"name\"])(\n    model.parameters(), **settings[\"optimizer\"][\"params\"]\n)\n\nscheduler = getattr(torch.optim.lr_scheduler, settings[\"scheduler\"][\"name\"])(\n    optimizer, **settings[\"scheduler\"][\"params\"]\n)\n\nloss_func = getattr(nn, settings[\"loss\"][\"name\"])(**settings[\"loss\"][\"params\"])\n\ntrigger = None\nmanager = ppe.training.ExtensionsManager(\n    model,\n    optimizer,\n    settings[\"globals\"][\"num_epochs\"],\n    iters_per_epoch=len(train_loader),\n    stop_trigger=trigger,\n    out_dir=output_dir,\n)\n\nmanager = set_extensions(manager, model, device, val_loader, optimizer, loss_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_loop(manager, model, device, train_loader, optimizer, scheduler, loss_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train_loader\ndel val_loader\ndel model\ndel optimizer\ndel scheduler\ndel loss_func\ndel manager\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## save results","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"for f_name in [\"log\", \"loss.png\", \"lr.png\"]:\n    shutil.copy(output_dir / f_name, f_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"log = pd.read_json(\"log\")\nbest_epoch = log[\"val/loss\"].idxmin() + 1  # PPE starts counts epochs at 1\nlog.iloc[[best_epoch - 1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shutil.copy(output_dir / \"snapshot_epoch_{}.pth\".format(best_epoch), \"best_model.pth\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is how you would load this model for inference / continuing training","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"m = get_model(\n    {\"name\": settings[\"model\"][\"name\"], \"params\": {\"pretrained\": False, \"n_classes\": 264}}\n)\nstate_dict = torch.load(\"best_model.pth\")\nm.load_state_dict(state_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}