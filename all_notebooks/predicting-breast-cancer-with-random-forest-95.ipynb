{"cells":[{"outputs":[],"cell_type":"markdown","source":"## Breast Cancer Wisconsin (Diagnostic) Data Set\n**************","metadata":{"_uuid":"620639bed1c879b535a90df164b952cbe1e75d00"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"1) The dataset given here is about the patients who were detected with 2 kinds of breast cancer : a) Malignant or b) Benign <br>\n2) The features given here are the characteristics of the cell nuclei computed from the fine needle aspirate(FNA) of a breast mass. <br>\n3) Ten real-valued features are computed for each cell nucleus as follows:\n    \n   - radius (mean of distances from center to points on the perimeter)\n   - texture (standard deviation of gray-scale values) \n   - perimeter \n   - area \n   - smoothness (local variation in radius lengths) \n   - compactness (perimeter^2 / area - 1.0)\n   - concavity (severity of concave portions of the contour) \n   - concave points (number of concave portions of the contour)\n   - symmetry \n   - fractal dimension (\"coastline approximation\" - 1)\n    \n\n4) Mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. ","metadata":{"_uuid":"b2197796971127ac8a8b104aad7920431b4afb18"},"execution_count":null},{"source":"# Importing Libraries:\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"5701ca355e9fadfb0d126760bd7976ec90697c29","collapsed":false}},{"source":"# Reading the file\ndata = pd.read_csv('../input/data.csv')","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e8f1155908d3467457eb70af2314dd4d01f7f681","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"### I: Data Wrangling - ","metadata":{"_uuid":"b989c87e4c183667b1b31f8aae22051da4547ad6"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"#### I-a. Doing spot check on the data:","metadata":{"_uuid":"2d4d642b062696a28485856a349e37492d39ea86"},"execution_count":null},{"source":"# Overall view of the data:\ndata.info()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e8a17f1112351b8459dab47a14f2417dba9a9c7a","collapsed":false}},{"source":"# Checking the first few rows:\ndata.head()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"29527532d6efb01e62b932058cd587a5e05aa9ce","collapsed":false}},{"source":"# Target Variable:\ndata.diagnosis.unique()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"957ce5d49f7a4e0873cda0b65e286cc89d9a717d","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### I-b. Summary of Numeric Columns:","metadata":{"_uuid":"fa8c29e3cec11454617c492e63fcf037cad9e927"},"execution_count":null},{"source":"data.describe()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"874e150e46edcd3fe58cd2624835beb6dc20f070","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"* There are no null values\n* ***id*** and ***Unnamed: 32*** are not required columns. So we will get rid of those.\n* There are two outcomes - **Benign Tumor** (which spreads locally) **Malignant Tumor** (which can spread throughout the whole body via blood)","metadata":{"_uuid":"b4eb7ba0d0f7e480c0c8661911b1d2d0c57ebdfe"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"#### I-c. Some operations:","metadata":{"_uuid":"5dc315ae800577b7669f0f2a79bae11f676569e9"},"execution_count":null},{"source":"# Dropping some of the unwanted variables:\ndata.drop('id',axis=1,inplace=True)\ndata.drop('Unnamed: 32',axis=1,inplace=True)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"1b313a15826d2d57114b1bca0a1c6f8bd293a3ea","collapsed":true}},{"source":"# Binarizing the target variable:\ndata['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"7f6953dcff9e56c8a5179961c78c8fd84aec7988","collapsed":true}},{"outputs":[],"cell_type":"markdown","source":"**Important**: The data is highly variable and any feature with low variance will be neglected. We will scale the data to allow more predictive power. <br>\n** Here we are standardizing the dataset - meaning shifting the distribution to have mean of zero and standard deviation of unit variance **","metadata":{"_uuid":"dc81f95c05c0177760cfc1ab56dea0e89da1d2b3"},"execution_count":null},{"source":"datas = pd.DataFrame(preprocessing.scale(data.iloc[:,1:32]))\ndatas.columns = list(data.iloc[:,1:32].columns)\ndatas['diagnosis'] = data['diagnosis']","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"9639700bd2fd401a03b50832afff5060b274a3cc","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"### Doing some EDA:","metadata":{"_uuid":"43f9292870034c484b6769a0209323e8422f3d6d"},"execution_count":null},{"source":"#Looking at the number of patients with Malignant and Benign Tumors:\ndatas.diagnosis.value_counts().plot(kind='bar', alpha = 0.5, facecolor = 'b', figsize=(12,6))\nplt.title(\"Diagnosis (M=1 , B=0)\", fontsize = '18')\nplt.ylabel(\"Total Number of Patients\")\nplt.grid(b=True)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"5b9e2c8956fd9344c50d6cf92bb7eaf3ef851464","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"* ~ 65% of the patients had Benign tumor while the rest of them had Malignant.","metadata":{"_uuid":"22532e9351eb05048b158f80d449d4bc7c759e6d"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"#### Considering only mean features of nucleus","metadata":{"_uuid":"7b15da747353c2c1a889aedbfc2d8adb42218e19"},"execution_count":null},{"source":"data.columns","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"a24f0e07c329839cd59d0531d914343888a1f73a","collapsed":false}},{"source":"data_mean = data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"af48bb45aa9d901dd49a540db13b8ce2eeb136b6","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### We will just see how these features coorelate with the diagnosis using heatmap:","metadata":{"_uuid":"f2bd22d50477c2e5a97079dbd03cec7b2ea21930"},"execution_count":null},{"source":"plt.figure(figsize=(14,14))\nfoo = sns.heatmap(data_mean.corr(), vmax=1, square=True, annot=True)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"2ac1353e2ea74c955138c919d116a053622e6ad6","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"* **radius_mean, perimeter_mean, area_mean, compactness_mean, concavity_mean, concave points_mean** show high coorelation with the **diagnosis**.\n* The other variables do not really show high impact over diagnoses.","metadata":{"_uuid":"abceb3652fe30c168e7e48a9466fe99a9b9a7e0e"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"### Bivariate Exploration:","metadata":{"_uuid":"98b7f2152c6ee2d66bdec8ea38c42809bfaa98fd"},"execution_count":null},{"source":"_ = sns.swarmplot(y='perimeter_mean',x='diagnosis', data=data_mean)\nplt.show()","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"9b7791c211850ff45670f9c83168fb89dfcbc687","collapsed":false}},{"source":"# from pandas.tools.plotting import scatter_matrix\n\n# p = sns.PairGrid(datas.ix[:,20:32], hue = 'diagnosis', palette = 'Reds')\n# p.map_upper(plt.scatter, s = 20, edgecolor = 'w')\n# p.map_diag(plt.hist)\n# p.map_lower(sns.kdeplot)\n# p.add_legend()\n\n# p.figsize = (30,30)","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"5581d789b34d4b7a2857a5e04128ca64270d86c1","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"### Setting up the train and test data:","metadata":{"_uuid":"5e366c97b02f57421d361183c8e5ca29659d7600"},"execution_count":null},{"source":"from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn import metrics\n\npredictors = data_mean.columns[2:11]\ntarget = \"diagnosis\"\n\nX = data_mean.loc[:,predictors]\ny = np.ravel(data.loc[:,[target]])\n\n# Split the dataset in train and test:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint ('Shape of training set : %i || Shape of test set : %i' % (X_train.shape[0],X_test.shape[0]) )\nprint ('The dataset is very small so simple cross-validation approach should work here')\nprint ('There are very few data points so 10-fold cross validation should give us a better estimate')","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"1581541a95d047cff9e946b8684983b77fc3a988","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### Logistic Regression Model:","metadata":{"_uuid":"9706f70b9021fe85a7f5d76eff5f8f08c4711260"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"* This technique is widely used in medical field where dealing with binary classification problems.\n* Firstly we will run over all the mean features against our target feature.\n* We will use the features for our model based on the features that showed correlation in our heatmap.","metadata":{"_uuid":"ed0595400a9232eff9967d644c301e6c10582d02"},"execution_count":null},{"source":"# Importing the model:\nfrom sklearn.linear_model import LogisticRegression\n\n# Initiating the model:\nlr = LogisticRegression()\n\nscores = cross_val_score(lr, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n\nprint(\"The mean accuracy with 10 fold cross validation is %s\" % round(scores*100,2))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"6928ecdf13020fc1578835e581858d3caae83a10","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### SVM:","metadata":{"_uuid":"c1ee099e62b79451eb691bebdfaf00eef7438bcd"},"execution_count":null},{"source":"# Importing the model:\nfrom sklearn import svm\n\n# Initiating the model:\nsvm = svm.SVC()\n\nscores = cross_val_score(svm, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n\nprint(\"The mean accuracy with 10 fold cross validation is %s\" % round(scores*100,2))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"8951dd9f9aad416e09f625043924f509efbef5d3","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"* We will try to hyper tune the parameters and try to fit with other kernels.","metadata":{"_uuid":"5535a682f8b50064ff368862771a02206b390cc0"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"#### kNN:","metadata":{"_uuid":"76a207ee19c8e0a8eb8c223a068e21d725d08293"},"execution_count":null},{"source":"# Importing the model:\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Initiating the model:\nknn = KNeighborsClassifier()\n\nscores = cross_val_score(knn, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n\nprint(\"The mean accuracy with 10 fold cross validation is %s\" % round(scores*100,2))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"7ce6eccdb2a2853c34ec695a819328ae022b69f5","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### Perceptron:","metadata":{"_uuid":"44f58a4e53db57613f65d41135cf5d2a7c832f78"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"- Perceptron is binary linear classification algortithm that purely decides based on the input (vector of numbers) if it belongs to specific class or not.","metadata":{"_uuid":"0073d07b2b852c65d052405a96a6c57df0bc0f97"},"execution_count":null},{"source":"# Importing the model:\nfrom sklearn.linear_model import Perceptron\n\n# Initiating the model:\npct = Perceptron()\n\nscores = cross_val_score(pct, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n\nprint(\"The mean accuracy with 10 fold cross validation is %s\" % round(scores*100,2))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"fd195c95bcc81bdcdf78d8431ada58558efee46d","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### Random Forest Model:","metadata":{"_uuid":"ed544720e61d2455de9ef76f07e6085a199c4d3f"},"execution_count":null},{"source":"# Importing the model:\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initiating the model:\nrf = RandomForestClassifier()\n\nscores = cross_val_score(rf, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n\nprint(\"The mean accuracy with 10 fold cross validation is %s\" % round(scores*100,2))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"ea3bedf1516c46f1f1f94e236d959541db0b90dc","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### Naive Bayes:","metadata":{"_uuid":"b915ffab526810fab15bad3c25789e74f133d6f3"},"execution_count":null},{"source":"# Importing the model:\nfrom sklearn.naive_bayes import GaussianNB\n\n# Initiating the model:\nnb = GaussianNB()\n\nscores = cross_val_score(rf, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n\nprint(\"The mean accuracy with 10 fold cross validation is %s\" % round(scores*100,2))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"fa7afd2c17d4819f5875de08ac5dfe567f7bd3e7","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### Logisitic Regression, Random Forest, Naive Bayes and kNN looks to perform better. Lets try to fine tune the parameters and see if we can get any improvisation.","metadata":{"_uuid":"c5b4617be7afacd3d58ff8a94d56b7aa02c6a924"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"#### Starting with k-Nearest Neighbors:","metadata":{"_uuid":"19620bdd67fc9d643774ed9670e79470c7466109"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"#### The default neighbors is 20. However, lets try rnning kNN for different values of neighbors:","metadata":{"_uuid":"cb183d4997826f6a8e66e869dd9ca19311c34705"},"execution_count":null},{"source":"for i in range(1, 21):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    score = cross_val_score(knn, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n    print(\"N = \" + str(i) + \" :: Score = \" + str(round(score,2)))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"6eb4f249e88eb73a8a511d6b06d6e0e27b188b0b","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### The default number of trees is 10. However, lets try running Random Forest for different values of trees:","metadata":{"_uuid":"fbf081d44ead3cb710b77eeecff6165cc8e2b806"},"execution_count":null},{"source":"for i in range(1, 21):\n    rf = RandomForestClassifier(n_estimators = i)\n    score = cross_val_score(rf, X_train, y_train, scoring='accuracy' ,cv=10).mean()\n    print(\"N = \" + str(i) + \" :: Score = \" + str(round(score,2)))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"07e4b080a033f4002b8b6fb97f808c83ba89f7eb","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### It looks like trees with 18 should give a reasonable estimate of the test data. Let us trying using Random Forest and Naive Bayes on our test dataset and finalize our model.","metadata":{"_uuid":"c29ddba39a3c3144ed1e6aa249664f4cc00f18bf"},"execution_count":null},{"source":"from sklearn.ensemble import RandomForestClassifier\n\n# Initiating the model:\nrf = RandomForestClassifier(n_estimators=18)\n\nrf = rf.fit(X_train, y_train)\n\npredicted = rf.predict(X_test)\n\nacc_test = metrics.accuracy_score(y_test, predicted)\n\nprint ('The accuracy on test data is %s' % (round(acc_test,2)))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"9c686ec5aeb7967e0c867993e2f809a707705d81","collapsed":false}},{"source":"from sklearn.naive_bayes import GaussianNB\n\n# Initiating the model:\nnb = GaussianNB()\n\nnb = nb.fit(X_train, y_train)\n\npredicted = nb.predict(X_test)\n\nacc_test = metrics.accuracy_score(y_test, predicted)\n\nprint ('The accuracy on test data is %s' % (acc_test))","execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_execution_state":"idle","_uuid":"43c4af96e6c05f9415d40099e4c646f93c0337e9","collapsed":false}},{"outputs":[],"cell_type":"markdown","source":"#### More stuff to come. Please comment if any suggestions or advice. Anything would be appreciated!","metadata":{"_uuid":"9b61cb6219e52081a10cbd2ea7cb42c090e73066"},"execution_count":null}],"nbformat_minor":0,"nbformat":4,"metadata":{"anaconda-cloud":{},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","version":"3.6.1","nbconvert_exporter":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}