{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Recommender System Business Objective\n### Increase cross-selling from better product placements & store layout design\nThough recommender systems are most often used in eCommerce and digital media operations, it still offers compelling use cases for offline retailers. \n\nIn this case, we will identify items most often purchased together to create end-of-isle & in-isle displays. These displays have historically proven effective in generating cross sales for grocers.\n\n\n\n<img src=\"https://i.imgur.com/ioWsgSd.jpg\" width=\"800px\">\n\n\n### Overview of Main Types of Recommender Systems\n\n#### 1.Content or search based\nIt tries to construct a user profile that captures user's preferences and compares it  with product attributes. Closest matches are then recommended to the users. \n\nFor very large datasets (many millions of products) this algorithm tends to perform poorly. This is because in order to make recommendations the algorithms must run a search through all items. General strategies to speed up this approach is to limit the search space to known similar categories such as same author, product category etc... but at the cost of reduced recommendation quality. \n\nThis also means for users who has rated fewer items, the search space is smaller and less expensive to compute, and for power users who has a long rating history the computation can get expensive. These main drawbacks limits this approach's viability for large scale deployment that requires recommendations to be made and updated in real time.\n\n#### 2. Collaborative filters\nLooks for users that exhibits similar behavior or items that are often purchased together. Similarity is often measured bycosine similarity. The two popular CFs are\n\n- Item based collaborative filter\n- User based collaborative filter\n\nReceipt based data will inform us about what products are often purchased together\n\n#### 3. Hybrid systems\nA mix of two or more approaches, many of which, like knowledged based systems are not illustrated here.\n\n#### Runtime complexity for N items and M users\n\n| Algorithm | Worst Case | Average Case |Can precompute?|\n|:-----|:-----|:-----|:-----|\n|Content based|O(NM)|O(N+M)|Mostly no|\n|Item & User Collaborative Filter|O(N<sup>2</sup>M) |O(NM)|Yes|\n\n#### Item based cf best for our usecase\nWe are looking to discover highly correlated items, and therefore will use an item-item collaborative filter.\n\n### Roadmap\n1. Build item based collaborative filter from monthly sales data for now\n2. Re-train with receipt level data when it becomes available for more accurate recommendations\n\n### Item based collaborative filter from monthly sales data"},{"metadata":{"trusted":true},"cell_type":"code","source":"__author__ = \"Victor Xu\"\n__email__ = \"victor.c.xu@gmail.com\"\n__website__ = \"victorxu.me\"\n\n__powerpoint_presentation__ = \"https://www.dropbox.com/s/ezenffm4bbrutar/grocer.pdf?dl=0\"\n__write_up__ = \"coming soon\"\n\n__copyright__ = \"Copyright 2019, Victor Xu\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport os\nimport random\n\nimport pandas as pd\nimport numpy as np\n\n#from polish_grocer_utility_script import load_monthly_data\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.spatial.distance import cosine\nfrom multiprocessing import Process, Queue, Lock","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_monthly_data():\n    \"\"\"Loads monthly sales data from  csv\"\"\"\n\n    monthly = pd.read_csv(\n        '/kaggle/input/total-sale-2018-yearly-data-of-grocery-shop/SELL_1.csv',\n        delimiter=';',\n        decimal=',',\n        encoding='latin-1')\n    monthly = monthly.rename(columns={\n        'Date': 'date',\n        'PKod': 'sku',\n        'Pgroup': 'group',\n        'Pname': 'name',\n        'Pquantity': 'quantity',\n        'pce_zn': 'unit_cogs',\n        'pwa_zn': 'agg_cogs',  # aggregate cogs for this sku\n        'pce_sn': 'unit_revenue',\n        'pwn_sn': 'agg_rev',  # aggregate revenue for this sku\n        'pmarza': 'gross_margin',\n        'pmarzajedn': 'unit_contribution',\n        'pkwmarza': 'agg_dollar_contribution',\n        'pudzmarza': 'share_of_margin'\n    })\n\n    monthly = monthly.drop(['pwa_sn', 'pce_sb', 'pwa_sb', 'pudzsb'], axis=1)\n    monthly.group = monthly.group.str.lower()\n    monthly.name = monthly.name.str.lower()\n    monthly.date = pd.to_datetime(monthly.date, format=\"%d.%m.%Y\")\n    monthly.unit_cogs = monthly.unit_cogs.str.replace(\n        ',', '.').str.replace(\n        ' ', '')\n\n    monthly.group = monthly.group.replace(\n        {\"ketch_concetrate_mustard_majo_horseradish\": \"sauce\"})\n\n    return monthly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly = load_monthly_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build product information df\nproduct_description_df = monthly[['sku','group','name']].set_index('sku', drop=True)\nproduct_description_df = product_description_df.drop_duplicates()\nproduct_description_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get SKU sales volume by month\n\nEach month's sale is synonomyous to a user's purchase history. By looking across 12 months of sales data, we attempt to figure out items that have highly correlating sales. We do not expect this to be a high quality recommender system. Receipt level data is required."},{"metadata":{"trusted":true},"cell_type":"code","source":"utility_matrix = monthly.pivot_table(values='quantity', index='sku', columns='date', aggfunc='sum')\n\n# Replace NaN with 0s, because NaN indicates no sale for that month\nutility_matrix = utility_matrix.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"utility_matrix.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Normalzing before computing item-item similarity\n\nThere are a few core difference between our utility matrix and one that most e-Commerce stores will have.\n\nWith millions of items and hundreds of millions of users, Amazon's utility matrix will be highly sparse, whith most entries being blank or 1's. Due to the fact we are aggregating data on a monthly basis, ours utility matrix is not spare and is real valued. \n\n<img src=\"https://imgur.com/hvg5xlQ.jpg\" width=\"300px\">\n\nThe cosine similarity between any large vector (ie from a popular item) and a small vector is going to be close to 1 - making results arbitrary. We do not want large magnitudes to dominate the calculation; and as such, we will normalize each row by subtracting the mean, and bounding each row to the range of [0, 1]. This effectively means we do not take sales volume into consideration and only look to discover items that exhibit similar sales patterns."},{"metadata":{"trusted":true},"cell_type":"code","source":"um_norm = utility_matrix.sub(utility_matrix.mean(axis=1), axis=0)\n\n# MinMaxScaler can only scale column wise, we need to scale row wise\n# have to transpose and then transpose it back\num_norm = np.transpose(um_norm.values)\num_norm = MinMaxScaler(feature_range=(0,1)).fit_transform(um_norm)\num_norm = np.transpose(um_norm)\num_norm_df = pd.DataFrame(um_norm, index=utility_matrix.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building item-item matrix\n\nThe item-item matrix is a symmetric matrix that holds cosine similiarty score of all paris of products that the store sells.\n\nStrategies to speed up computation\n- We will only compute half of the matrix since it is symmetric\n- Multithreading"},{"metadata":{"trusted":true},"cell_type":"code","source":"def producer(queue, lock, um_norm_df):\n    \"\"\"\n    Iterate through the normalized utility matrix DataFrame and\n    adds all the pairs of vectors that need the cosine similarity to\n    be calculated\n    \"\"\"\n    \n    # Acquire lock on console\n    with lock:\n        print('Producer {} is starting.'.format(os.getpid()))\n    \n    counter = 0\n    batch = []\n    # put data required to calculate item-item cosine similarity in queue\n    for idx_i, row_data_i in um_norm_df.iterrows():\n        for idx_j, row_data_j in um_norm_df.iterrows():\n            # skip duplicate computations. w[i,j] and w[j,i] are the same\n            if idx_i < idx_j:\n                data = (idx_i, idx_j, row_data_i.values, row_data_j.values)\n                batch.append(data)\n                \n                # Queue data every 5000 iterations to minimize consumer I/O\n                if counter % 5000 == 0:\n                    queue.put(batch)\n                    batch = []\n                \n                counter += 1\n    \n    # Put left over data in queue\n    queue.put(batch)  \n    \n    # Acquire lock on console\n    with lock:\n        print('Producer {} is finished. Quitting.'.format(os.getpid()))\n \n \ndef consumer(task_queue, lock, result_queue):\n    \"\"\"\n    Get data from the task queue and calculates the cosine similarity.\n    \n    Note: Item similarity with itself will not be calculated and as such,\n    will have a value of 0 instead of 1 in the final W matrix (item-item matrix).\n    This property is desirable because we never want to recommend the same item\n    itself.\n    \"\"\"\n\n    with lock:\n        print('Consumer {} is starting.'.format(os.getpid()))\n     \n    while True:\n        # Get data, if queue is empty, it will block until producer \n        # put data in queue \n        batch = task_queue.get()\n        result_batch= []\n        \n        # Iterate over the batch of data, each is a vector pair\n        for data in batch:\n            idx_i, idx_j, row_data_i, row_data_j = data\n\n            # scipy cosine calculates the cosine distance. 1 - distance = similarity\n            similarity = 1. - cosine(row_data_i, row_data_j)\n            \n            result_batch.append((idx_i, idx_j, similarity))\n            result_batch.append((idx_j, idx_i, similarity))\n        \n        result_queue.put(result_batch)\n\n        \n        \n    with lock:\n        print('Consumer {} is finished. Quiting'.format(os.getpid()))\n\n\ndef process_result_queue(result_queue, item_item_df):\n    \"\"\"\n    Process the return data in result queue and update the item-item DataFrame\n    \"\"\"\n    \n    while result_queue.qsize() > 0:\n        result_batch = result_queue.get()\n        \n        for result in result_batch:\n            idx_i, idx_j, similarity = result\n\n            item_item_df.loc[idx_i,idx_j] = similarity\n    \n    return item_item_df\n    \n    \n    \ndef build_item_item_matrix(um_norm_df):\n    \"\"\"Calculates the item-item similarity matrix\"\"\"\n    \n    num_skus = um_norm.shape[0]\n\n    item_item_df = pd.DataFrame(np.zeros((num_skus, num_skus)),\n                                index=utility_matrix.index, \n                                columns=utility_matrix.index)\n    \n    # For testing\n    # um_norm_df =  um_norm_df.iloc[:4,:4].copy()\n    \n    task_queue = Queue()\n    result_queue = Queue()\n\n    # Create a lock object to synchronize resource access\n    lock = Lock()\n\n    producers = []\n    consumers = []\n\n    # Create producer processes\n    producers.append(Process(target=producer, args=(task_queue, lock, um_norm_df)))\n\n    # Create consumer processes\n    for i in range(8):\n        p = Process(target=consumer, args=(task_queue, lock, result_queue))\n\n        # Set daemon to true so consumers will exit, otherwise it will be in inf loop\n        p.daemon = True\n        consumers.append(p)\n\n    for p in producers:\n        p.start()\n\n    for c in consumers:\n        c.start()\n\n    # Like threading, we have a join() method that synchronizes our program\n    for p in producers:\n        p.join()\n\n    item_item_df = process_result_queue(result_queue, item_item_df)\n\n    print('All done')\n    \n    return item_item_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_n_similar_items_for_sku(sku, n, item_item_df, \n                                product_description_df, print_item_desc=False,\n                               return_as_json=False):\n    \"\"\"\n    Looks through the item-item similarity matrix and returns the \n    top n similar items for a given sku\n    \n    Args:\n        sku: int\n            SKU to get top n similar items\n        \n        n: int\n            Number of similar items to return\n        \n        item_item_df: DataFrame\n            Pre-computed item-item similarity matrix as pd.DataFrame\n            and indexed on integer valued SKUs\n            \n        product_description_df: DataFrame\n            Df holding item descriptions indexed on integer valued SKUs\n            \n        print_item_desc: bool\n            Set true to print requestd sku product name and category\n        \n        return_as_json: bool\n            Returns JSON instead of DataFrame\n    \n    Returns:\n        items: DataFrame or JSON\n            Df or JSON containing top n similar items with product information\n            Use return_as_json argument to set return data type\n    \"\"\"\n    \n    top_n_skus = item_item_df.loc[:,sku].sort_values(ascending=False).iloc[:n]\n    top_n_skus = pd.DataFrame(top_n_skus).join(product_description_df, how='left')   \n    top_n_skus.columns = ['similarity','category','name']\n    \n    if print_item_desc:\n        item = product_description_df.loc[sku]\n        s = \"Top {} items /w similar purchase pattern to {} ({}) are as follows\"\n        print(s.format(n, item.loc[\"name\"], item.group))\n    \n    if return_as_json:\n        return top_n_skus.to_json(orient=\"records\")\n    \n    return top_n_skus","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Build the item-item similarity matrix, aka W matrix\nitem_item_df = build_item_item_matrix(um_norm_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Testing\nLet's try what other item's sales pattern are similar to one of our top selling items during the summer months - Patyczki Bella, a Polish beer. Since this beer is sold as singles (as opposed to a 6 pack) this is considered a convinence item. "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"get_n_similar_items_for_sku(304, 15, item_item_df, \n                            product_description_df, \n                            print_item_desc=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This result makes a lot of sense, as most are also convinence items and are popular during the summer months.\n\nNow lets try a general food item."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"get_n_similar_items_for_sku(9664, 15, item_item_df, \n                            product_description_df, \n                            print_item_desc=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are getting items that are mostly popular during fall and winter months. Again it the results are in line with our learning from the EDA step."},{"metadata":{},"cell_type":"markdown","source":"## Recommender System Conclusion\nWe can see that our recommender system is successful at making similar item recommendations. However, with monthly sales data, we can only look for items that have similar sales partterns on a monthly scale. As such, recommendation quality is only statisfactory.\n\n### Next step - Wait for Receipt Level Data (~1 month)\nWe will be retrailing our recommender system with receipt level data to get better idea of what people are buying together, which will allow us to design better in store promotions and inform on store layout design decisions.\n\n### Prepare for API Deployment\n\n#### Test functions used by API end point to return data"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_n_similar_items_for_sku(304, 2, item_item_df, \n                            product_description_df, \n                            print_item_desc=False,\n                            return_as_json=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pickling the item-item similarity matrix"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#import pickle\n\n#with open('../model/item_item_df.pickle','wb') as f:\n#    pickle.dump((item_item_df, product_description_df), f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Done\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"python3.7","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}