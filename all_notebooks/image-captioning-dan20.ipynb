{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport os\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model\nfrom keras.utils import np_utils\nimport random\nfrom keras.preprocessing import image, sequence\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"../input/flickr8k/captions.txt\", \"r\")\nimages_names = []\ncaption = []\nfor x in f:\n    a = x.split(\",\")\n    images_names.append(a[0])\n    caption.append(a[1])\n    \nimages_names = images_names[1:]\ncaption = caption[1:]\nprint(images_names[0:10])\nprint(caption[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_cap_dict = {}\nfor i in range(len(images_names)):\n    image_name = images_names[i]\n    if image_name not in image_cap_dict.keys():\n        image_cap_dict[image_name] = []\n        image_cap_dict[image_name].append(caption[i])\n    else:\n        image_cap_dict[image_name].append(caption[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for sentences in image_cap_dict.keys():\n    new = []\n    for sent in image_cap_dict[sentences]:\n        a = sent.strip()\n        new.append(a)\n    image_cap_dict[sentences] = new    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = image_cap_dict[\"1000268201_693b08cb0e.jpg\"]\nprint(image_cap_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#../input/flickr8k/Images/1000268201_693b08cb0e.jpg\n\nimages_path = \"../input/flickr8k/Images\"\nimage = \"1012212859_01547e3f17.jpg\"\npath = images_path + \"/\" + image\n\nfrom IPython.display import Image, display\nz = Image(filename=path)\ndisplay(z)\n\nfor sent in image_cap_dict[image]:\n    print(sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(image_cap_dict.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = open('flickr_8k_train_dataset.txt','wb')\ntrain_dataset.write(b\"image_id\\tcaptions\\n\")\n\ntest_dataset = open('flickr_8k_test_dataset.txt','wb')\ntest_dataset.write(b\"image_id\\tcaptions\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [k for k in image_cap_dict.keys()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = image_cap_dict.keys()\n#print(keys)\nfor img_name in a[0:6000]:\n    list_cap = image_cap_dict[img_name]\n    for capt in list_cap:\n        caption = \"<start> \"+ capt + \" <end>\"\n        train_dataset.write((img_name+\"\\t\"+caption+\"\\n\").encode())\n        print((img_name+\"\\t\"+caption+\"\\n\").encode())\n        train_dataset.flush()\ntrain_dataset.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann = []\nkeys = image_cap_dict.keys()\nfor i in keys:\n    ann.append(i)\n    \nprint(ann[10])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img_name in a[6000:]:\n    list_cap = image_cap_dict[img_name]\n    for capt in list_cap:\n        caption = \"<start> \"+ capt + \" <end>\"\n        test_dataset.write((img_name+\"\\t\"+caption+\"\\n\").encode())\n        print(img_name+\"\\t\"+caption+\"\\n\")\n        test_dataset.flush()\ntest_dataset.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('flickr_8k_test_dataset.txt','r')\nf.readlines()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to process images\ndef preprocessing(img_path):\n    im = tf.keras.preprocessing.image.load_img(img_path,target_size=(224,224,3))\n    im = tf.keras.preprocessing.image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nx_train = a[0:6000]\ntrain_data = {}\nimage_path = \"../input/flickr8k/Images\"\nctr=0\nfor image_name in x_train:\n    if image_name == \"\":\n        continue\n    ctr+=1\n    if ctr%100==0:\n        print(ctr)\n        print(pred)\n    path = images_path + \"/\" + image_name\n    img = preprocessing(path)\n    pred = model.predict(img).reshape(2048)\n    train_data[image_name] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['101654506_8eb26cfb60.jpg'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open( \"trained_images_2.p\", \"wb\" ) as pickle_f:\n    pickle.dump(train_data, pickle_f ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"flickr_8k_train_dataset.txt\", delimiter='\\t')\nds = dataset.values\nprint(ds.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []\ni = 0\nfor i in range(ds.shape[0]):\n    sentences.append(ds[i, 1])\n    if i%90==0:\n        print(ds[i, 1])\n    \nprint(len(sentences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = [sentence.split() for sentence in sentences]\nwords[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nall_words = []\nfor k in words:\n    for word in k:\n        all_words.append(word)\nprint(all_words[0:40])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique = list(set(all_words))\nprint(len(unique))\nvocab_size = len(unique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_2_indices = {val:index for index,val in enumerate(unique)}\nindices_2_word = {index:val for index,val in enumerate(unique)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_2_indices['UNK'] = 0\nindices_2_word[0] = 'UNK'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_2_indices.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_2_indices[\"skimpy\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices_2_word[300]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(word_2_indices['<start>'])\nprint(indices_2_word[4090])\nprint(word_2_indices['<end>'])\nprint(indices_2_word[8251])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_2_indices.keys())\nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = -1\nfor k in sentences:\n    a = k.split(\" \")\n    \n    if len(a)>max_len:\n        max_len = len(a)\n        arr = a\nprint(max_len)\nprint(arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded_sequences = []\nsubsequent_words = []\n\nfor i in range(ds.shape[0]):\n    partial_seqs = []\n    next_words = []\n    \n    text = ds[i,1].split(\" \")\n    text = [word_2_indices[i] for i in text]\n    \n    for i in range(0,len(text)):\n        partial_seqs.append(text[:i])\n        next_words.append(text[i])\n    \n    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n    \n    for i,next_word in enumerate(next_words):\n        next_words_1hot[i, next_word] = 1\n        \n    padded_sequences.append(padded_partial_seqs)\n    subsequent_words.append(next_words_1hot)\n   \npadded_sequences = np.asarray(padded_sequences)\nsubsequent_words = np.asarray(subsequent_words)\n\nprint(padded_sequences.shape)\nprint(subsequent_words.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(padded_sequences[0].shape)\nprint(padded_sequences[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#20,36\nfor x in range(len(padded_sequences[0])):\n    array_ = []\n    for y in range(max_len):\n        array_.append(indices_2_word[padded_sequences[0][x][y]])\n    print(array_)\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_images = 2000\ncaptions = np.zeros([0, max_len])\nnext_words = np.zeros([0, vocab_size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(num_of_images):\n    captions = np.concatenate([captions, padded_sequences[i]])\n    next_words = np.concatenate([next_words, subsequent_words[i]])\n    if(i%100==0):\n        print(i)\n\nnp.save(\"captions_2.npy\", captions)\nnp.save(\"next_words_2.npy\", next_words)\n\nprint(captions.shape)\nprint(next_words.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(captions.shape)\nprint(padded_sequences[i].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./trained_images_2.p', 'rb') as f:\n    encoded_images = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs = []\nfor x in range(ds.shape[0]):\n    if ds[x,0] in encoded_images.keys():\n        imgs.append(list(encoded_images[ds[x, 0]]))\n        \nimgs = np.asarray(imgs)\nprint(imgs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#num_of_images = 3600\nimages = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        images.append(imgs[ix])\n        \nimages = np.asarray(images)\n\nnp.save(\"images_2.npy\", images)\n\nprint(images.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_names = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        image_names.append(ds[ix, 0])\n        \nimage_names = np.asarray(image_names)\n\nnp.save(\"image_names_2.npy\", image_names)\n\nprint(len(image_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MODEL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions = np.load(\"./captions_2.npy\")\nnext_words = np.load(\"./next_words_2.npy\")\nimages = np.load(\"./images_2.npy\")\nimage_name = np.load(\"./image_names_2.npy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 128 \nmax_len = 36","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"language_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(256, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n#model.load_weights(\"./model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit([images, captions], next_words, batch_size=512, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"model_weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(img_path):\n    im = tf.keras.preprocessing.image.load_img(img_path,target_size=(224,224,3))\n    im = tf.keras.preprocessing.image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_encoding(model, img):\n    image = preprocessing(img)\n    pred = model.predict(image).reshape(2048)\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = [k for k in image_cap_dict.keys()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(keys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ndef predict_captions(image):\n    start_word = [\"<start>\"]\n    while True:\n        par_caps = [word_2_indices[i] for i in start_word]\n        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n        preds = model.predict([np.array([image]), np.array(par_caps)])\n        word_pred = indices_2_word[np.argmax(preds[0])]\n        start_word.append(word_pred)\n        \n        if word_pred == \"<end>\" or len(start_word) > max_len:\n            break\n            \n    return ' '.join(start_word[1:-1])\n\nimport random\nn = random.randint(2000,8000)\nm = keys[n]\n#print(str(n) + \"\\t\" + m)\nimg = \"../input/flickr8k/Images\" + \"/\" + m\ntest_img = get_encoding(resnet, img)\nArgmax_Search = predict_captions(test_img)\n\nz = Image(filename=img)\ndisplay(z)\n\n#print(Argmax_Search)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_sentence = Argmax_Search\nprint(Argmax_Search)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}