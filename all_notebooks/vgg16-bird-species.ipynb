{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Overview\n\nThe following model is used to predict classes of birds with a deep learning model using TensorFlow. It will consist of extracting the data, displaying the images along with their labels, augmenting the images for better processing in the model, building and fine tuning the model, and finally evaluating the model performance. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import backend, models, layers, optimizers, regularizers\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom IPython.display import display # Library to help view images\nfrom PIL import Image # Library to help view images\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # Library for data augmentation\nimport os, shutil # Library for navigating files\nimport matplotlib.pyplot as plt\nnp.random.seed(42)\nfrom keras.preprocessing.image import img_to_array \nfrom keras.preprocessing.image import array_to_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import VGG16","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Overview\n\nThe dataset is based on 34,325 images. These are split into 32,025 Training images, 1150 Validation images, and 1150 Test images. There are 230 classes of birds in the dataset. \n\nEach image is a 224x224 pixel file that is represented in color. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '../input/100-bird-species'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify the traning, validation, and test dirrectories.  \ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'valid')\ntest_dir = os.path.join(base_dir, 'test')\n\n\n#Normalize the pixels in the images.\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set Epochs\nepoch = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n    train_dir, \n    target_size=(224, 224), \n    batch_size=20, \n    class_mode='categorical') \n\nvalidataion_generator = train_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(224, 224),\n    batch_size=20,\n    class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=20,\n    class_mode='categorical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of all of the bird classes:\ntrain_generator.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print images and their labels\n\ndef getKeybyValue(LabelDict, value):\n    listItems = LabelDict.items()\n    for item in listItems:\n        if item[1] == value:\n            return item[0]\n    \n    return None\n\ndef pltFourImages(dir):\n    datagen2 = ImageDataGenerator()\n    it2 = datagen2.flow_from_directory(\n            dir,\n            target_size=(224, 224),\n            batch_size=20,\n            class_mode='binary')\n\n    labDict = it2.class_indices\n    batchX, batchy = it2.next() \n    num_img = batchX.shape[0]\n    imgs = [array_to_img(batchX[i]) for i in range(num_img)]\n    indx = [int(batchy[i]) for i in range(len(batchy))]\n    labs = [getKeybyValue(labDict, i) for i in indx]\n   \n    # settings\n    h, w = 10, 10        \n    nrows, ncols = 2, 2  \n    figsize = [18,12]     \n\n    # create figure (fig), and array of axes (ax)\n    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi = 80)\n\n    # plot image on each sub-plot\n    for i, axi in enumerate(ax.flat):\n        # i runs from 0 to (nrows*ncols-1)\n        # axi is equivalent with ax[rowid][colid]\n        axi.imshow(imgs[i], aspect = 'auto')\n\n        # write Label as title\n        axi.set_title(labs[i])\n\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.2)\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Print four images from each of train, test and valid\nfor d in ['/train', '/test', '/valid']:\n    print('\\n\\nImages from ', d)\n    pltFourImages(base_dir + d)\n\n# Source: https://www.kaggle.com/jimreed/analysis-of-bird-species-dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary of Models\n\nThe base model used in this analysis is the pretrainde VGG16 neural network. This network is trained on the ImageNet dataset of over 14 million images. Conveniently, the image size used in VGG16 is the same as the image size used in this analysis: 224x224. VGG16 utilizes 5 blocks of 2D Convolutional layers and Max Pooling 2D. I unfroze the final block, which consists of 3 layers of Convolution and 1 layer of Max Pooling. Here is a visual summary of the VGG16 model:\n\n![](https://neurohive.io/wp-content/uploads/2018/11/vgg16.png)\n\nMore reading here: https://neurohive.io/en/popular-networks/vgg16/\n\nAfter the base model is run, I added a Flatten layer and 2 Dense Layers, using Relu and Softmax as the activations. While the results shift with each run, the initial VGG16 test score was 94.1%. The model ran for all 50 epochs without EarlyStopping kicking in. Normally I would increase the number of epochs, but this took over an hour to run, and it was only the first step in the model tuning process. \n\nNext, I added 2 Dropout layers of 0.2, which improved the model accuracy to 96.1%. I also changed the kernel_initializer to he_normal and glorot_normal for the final Dense layers. I tested adding additional Batch Normalization and 2D Convolution layers, but they had adverse effects on performance. \n\nAfter running this model, I ran it again for further training, which made it to 12 additional epochs and an accuracy of 97.8%. \n\nFinally, I took this model and used it as the base for another model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"backend.clear_session()\nvgg_base = VGG16(weights = 'imagenet', include_top = False, input_shape = (224, 224, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('VGG model base  summary:', vgg_base.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we freeze all the layers except the last 4.\nfor layer in vgg_base.layers[:-4]:\n  layer.trainable = False\nfor layer in vgg_base.layers:\n  print(layer, layer.trainable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelvgg_train = models.Sequential()\nmodelvgg_train.add(vgg_base)\nmodelvgg_train.add(layers.Flatten())\nmodelvgg_train.add(layers.Dense(2048, activation = 'relu'))\nmodelvgg_train.add(layers.Dense(250, activation = 'softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('VGG model train  summary:', modelvgg_train.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will still use the same data augmentation from above\n\nmodelvgg_train.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = modelvgg_train.fit_generator(\n    train_generator,\n    steps_per_epoch=200,\n    epochs=epoch,\n    validation_data=validataion_generator,\n    verbose = 2,\n    callbacks=[EarlyStopping(monitor = 'val_accuracy', patience = 5, restore_best_weights = True)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = modelvgg_train.evaluate_generator(test_generator, steps = 50)\n\n\nprint('VGG16_train_test_acc:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initial VGG16 test score was 84.6%. This was using a smaller batch size, which was increased for future models. "},{"metadata":{},"cell_type":"markdown","source":"# Summary of Methods\n\nThe methods used below come from the Keras library. The primary model used is based on pretrained weights using the VGG16 network. In addition to this model, the data is augmented in several ways, such as rotation, zoom, and horizontal flipping. This allows the model to receive more images for training in order to have higher accuracy without needing brand new sources of data. \n\nEvaluation is performed using a comparison of test accuracy and validation accuracy/loss per epoch. Also, a confusion matrix is used to better interpret the results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Augmentation\ntrain_datagen2 = ImageDataGenerator(\n    rescale=1./255,# The image augmentaion function in Keras\n    rotation_range=40, # Rotate the images randomly by 40 degrees\n    width_shift_range=0.2, # Shift the image horizontally by 20%\n    height_shift_range=0.2, # Shift the image veritcally by 20%\n    zoom_range=0.2, # Zoom in on image by 20%\n    horizontal_flip=True, # Flip image horizontally \n    fill_mode='nearest') # How to fill missing pixels after a augmentaion opperation\n\n\ntest_datagen2 = ImageDataGenerator(rescale=1./255) \n\ntrain_generator2 = train_datagen2.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),\n    batch_size=64,\n    class_mode='categorical')\n\nvalidataion_generator2 = train_datagen2.flow_from_directory(\n    validation_dir,\n    target_size=(224, 224),\n    batch_size=64,\n    class_mode='categorical')\n\ntest_generator2 = test_datagen2.flow_from_directory( # Resize test data\n    test_dir,\n    target_size=(224, 224),\n    batch_size=64,\n    class_mode='categorical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Same technique as original with added dropout and kernel initializers; Using an augmented dataset; Updated steps_per_epoch to 500 to match number of samples / batch size\n\nbackend.clear_session()\n\nmodelvgg_dropout = models.Sequential()\nmodelvgg_dropout.add(vgg_base)\n\n#Additional Layers\n# modelvgg_train.add(layers.Conv2D(128, (3,3),  strides=(1, 1), activation = 'relu',kernel_initializer = 'he_uniform',padding='same'))\n# modelvgg_train.add(layers.BatchNormalization())\n# modelvgg_train.add(layers.Conv2D(128, (3,3),  strides=(1, 1), activation = 'relu',kernel_initializer = 'he_uniform',padding='same'))\n# modelvgg_train.add(layers.BatchNormalization())\n# modelvgg_train.add(layers.Conv2D(128, (3,3),  strides=(1, 1), activation = 'relu',kernel_initializer = 'he_uniform',padding='same'))\n#modelvgg_train.add(layers.BatchNormalization())\n# modelvgg_train.add(layers.MaxPool2D((2,2),  strides=(2, 2)))\n\nmodelvgg_dropout.add(layers.Flatten())\n# modelvgg_dropout.add(Dropout(0.2))\nmodelvgg_dropout.add(layers.Dense(2048, activation = 'relu', kernel_initializer='he_normal'))\nmodelvgg_dropout.add(Dropout(0.3))\nmodelvgg_dropout.add(layers.Dense(250, activation = 'softmax', kernel_initializer='glorot_normal'))\n\nprint('VGG model dropout  summary:', modelvgg_dropout.summary())\n\n# We will still use the same data augmentation from above\n\nmodelvgg_dropout.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy'])\n\nhistory = modelvgg_dropout.fit_generator(\n    train_generator2,\n    steps_per_epoch=500,\n    epochs=epoch,\n    validation_data=validataion_generator2,\n    verbose = 2,\n    callbacks=[EarlyStopping(monitor = 'val_accuracy', patience = 5, restore_best_weights = True)])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = modelvgg_dropout.evaluate_generator(test_generator2) #steps = 50)\n\n\nprint('VGG16_dropout_acc:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once adding dropout and new kernels, the score went up to 93.3%"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Try compiling the same fit again with a lower learning rate.\n\n#backend.clear_session()\n\n\nmodelvgg_dropout.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001),\n    loss = 'categorical_crossentropy',\n    metrics = ['accuracy'])\n\nhistory = modelvgg_dropout.fit_generator(\n    train_generator2,\n    steps_per_epoch=500,\n    epochs=epoch, \n    validation_data=validataion_generator2,\n    verbose = 2,\n    callbacks=[EarlyStopping(monitor = 'val_accuracy', patience = 5, restore_best_weights = True)])\n\ntest_loss, test_acc = modelvgg_dropout.evaluate_generator(test_generator2, steps = 50)\n\n\nprint('VGG16_more_epocs_acc:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the model through 8 more epocs brought the accuracy up to 97.8%"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, len(history_dict['accuracy']) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\nplt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Show Predicted Images with Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img,img_to_array\n\ndic=train_generator2.class_indices\nicd={k:v for v,k in dic.items()}\ndef output(location):\n    img=load_img(location,target_size=(224,224,3))\n    img=img_to_array(img)\n    img=img/255\n    img=np.expand_dims(img,[0])\n    answer=modelvgg_dropout.predict_classes(img)\n    probability=round(np.max(modelvgg_dropout.predict_proba(img)*100),2)\n    #print ('Bird Is',icd[answer[0]], 'With probability',probability)\n    print('The model predicts that this bird is:', icd[answer[0]])\n    #print (probability, ' % chances are there that the Bird Is',icd[answer[0]])\n\n#Source: https://www.kaggle.com/anuragmishra2311/birds-classification-using-resnet-101\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimg='../input/100-bird-species/test/EURASIAN MAGPIE/2.jpg' \npic=load_img('../input/100-bird-species/test/EURASIAN MAGPIE/2.jpg',target_size=(224,224,3))\nplt.imshow(pic)\noutput(img)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img='../input/100-bird-species/test/ALBATROSS/5.jpg' \npic=load_img('../input/100-bird-species/test/ALBATROSS/5.jpg',target_size=(224,224,3))\nplt.imshow(pic)\noutput(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img='../input/100-bird-species/test/NORTHERN FLICKER/3.jpg' \npic=load_img('../input/100-bird-species/test/NORTHERN FLICKER/3.jpg',target_size=(224,224,3))\nplt.imshow(pic)\noutput(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img='../input/100-bird-species/test/KOOKABURRA/5.jpg' \npic=load_img('../input/100-bird-species/test/KOOKABURRA/5.jpg',target_size=(224,224,3))\nplt.imshow(pic)\noutput(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of Results\n\nThe original VGG16 model performed quite well on the data set without any assistance needed. After some performance tweaks, it continued to improve in accuracy. However, there were definitely tradeoffs in performance compared to speed. I reached the conclusion that I could possibly continue to improve the model a fraction of a percentage with each new implementation of the model, but each run took 1 to 2 hours. Normally that would be fine, but having to wait for the results certainly impacted the ability to be nimble in making adjustments. \n\nWith a final test accuracy of 0.9779999852180481, I am quite pleased with this model. There appear to be errors in some instances of this workbook, but they are due to the session restarting on kaggle. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}