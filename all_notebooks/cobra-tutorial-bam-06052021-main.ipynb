{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A. Programming with Python\nSome examples to get you started with programming concepts and Python in particular.","metadata":{}},{"cell_type":"code","source":"print(\"hello wooorldddddd\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"from numpy.random import rand # function straight from submodule\nrand(5) # five random numbers between 0 and 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy.random as npr # load submodule\nnpr.rand(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # load main NumPy module\nnp.random.rand(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variables & data structures","metadata":{}},{"cell_type":"code","source":"session = 2\ntype(session)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"session += 1\nprint(session)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lists","metadata":{}},{"cell_type":"code","source":"list_of_strategies = [\"up-sell\", \"cross-sell\", \"down-sell\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(list_of_strategies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_strategies[0] # access through indexing (first element is at index 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_strategies.append(\"stay put\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_strategies","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dictionaries","metadata":{}},{"cell_type":"code","source":"dict_of_participants = { # collection of key-value pairs\n    \"Jan\": (\"XYZ\", 42),\n    \"Sam\": (\"ABC\", 28),\n    \"Daphne\": (\"MNO\", 35)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_of_participants[\"Sam\"] # you can only access the data via a key","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For loops & conditional statements","metadata":{}},{"cell_type":"code","source":"for strategy in list_of_strategies:\n    print(\"Possible strategy:\", strategy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for participant in dict_of_participants:\n    print(\"name:\", participant)\n    if participant == \"Daphne\":\n        value = dict_of_participants[participant]\n        age = value[1]\n        print(\"   Age:\", age)\n    elif participant == \"Jan\":\n        value = dict_of_participants[participant]\n        company = value[0]\n        print(\"   Company:\", company)\n    else:\n        print(\"   We do not want to know your info.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions & parameters","metadata":{}},{"cell_type":"code","source":"def get_age(participants, who):\n    \"\"\"\n    Parameters\n    ------------\n    - participants : dict\n        Dictionary of participants\n        \n    - who : str\n        Name of participant to get age from, as a string\n        Must be a valid key of 'participants' argument\n    \n    Returns\n    ------------\n    Age of selected participant\n    \"\"\"\n    value = participants[who]\n    age = value[1]\n    \n    if age < 20:\n        print(\"you are young\")\n    else:\n        print(\"you are old\")\n    \n    return age","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_age(dict_of_participants, \"Sam\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rectangular data & filtering","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {\n    \"Artist\": [\"Billy Holiday\", \"Jimi Hendrix\", \"Miles Davis\", \"SIA\"],\n    \"Genre\": [\"Jazz\", \"Rock\", \"Jazz\", \"Pop\"],\n    \"Listeners\": [1300000, 2700000, 1500000, 2000000],\n    \"Plays\": [27000000, 70000000, 48000000, 74000000]\n}\n\ndf = pd.DataFrame(data)\n\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.mean() # other available functions: https://pandas.pydata.org/docs/reference/frame.html","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"avg_plays\"] = df.Plays/df.Listeners","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"avg_plays\"].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.set_index(\"Artist\")[\"Plays\"].plot(ylabel=\"Total plays\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alternative way to create your df\ndata2 = [[\"Billy Holiday\", \"Jazz\", 1300000, 27000000],\n         [\"Jimi Hendrix\", \"Rock\", 2700000, 70000000],\n         [\"Miles Davis\", \"Jazz\", 1500000, 48000000],\n         [\"SIA\", \"Pop\", 2000000, 74000000]]\n\ndf2 = pd.DataFrame(data2, columns = [\"Artist\", \"Genre\", \"Listeners\", \"Plays\"])\n\ndf2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_jazz = df[df[\"Genre\"] == \"Jazz\"]\n\ndf_jazz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_popular = df[df[\"Listeners\"] >= 2000000]\n\ndf_popular","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Object-oriented programming (OOP)","metadata":{}},{"cell_type":"code","source":"class Rectangle:\n    def __init__(self, length, breadth, unit_cost=0):\n        self.length = length\n        self.breadth = breadth\n        self.unit_cost = unit_cost\n    def get_perimeter(self):\n        return 2 * (self.length + self.breadth)\n    def get_area(self):\n        return self.length * self.breadth\n    def calculate_cost(self):\n        area = self.get_area()\n        return area * self.unit_cost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = Rectangle(160, 120, unit_cost=2000)\n\nprint(\"Area of rectangle: %s cm^2\" % (r.get_area()))\nprint(\"Cost of rectangular field: EUR%s \" %(r.calculate_cost()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A few exercises you can have a go at if you feel confident...\n\n1. Write a function that takes two lists and outputs them as two named columns of a DataFrame\n2. Compute at least two other summary statistics from the df variable (Google is your friend)\n3. Make a class Customer and add some init variables and functions (no need to fill in the functions, just write keyword 'pass' under the function name)","metadata":{}},{"cell_type":"markdown","source":"# B. Predictive modelling with Cobra\n\nCobra is a Python package for rapid development of predictive models. Cobra focuses on interpretability and its methodology is based on Python Predictions' long experience with statistical modelling.\n\nHow to install Cobra?\n\n  * install the package `pip install -U pythonpredictions-cobra` and you are good to go!","metadata":{}},{"cell_type":"code","source":"# settings --> switch internet option on (requires SMS verification)\n!pip install -U pythonpredictions-cobra","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport warnings\nfrom pathlib import Path\n\n# preprocessing\nfrom cobra.preprocessing import PreProcessor\n\n# feature preselection\nfrom cobra.model_building import univariate_selection\nfrom cobra.evaluation import plot_univariate_predictor_quality\nfrom cobra.evaluation import plot_correlation_matrix\n\n# modelling\nfrom cobra.model_building import ForwardFeatureSelection\nfrom cobra.evaluation import plot_performance_curves\nfrom cobra.evaluation import plot_variable_importance\n\n# evaluation & PIGs\nfrom cobra.evaluation import Evaluator\nfrom cobra.evaluation import generate_pig_tables\nfrom cobra.evaluation import plot_incidence\n\n# Pandas settings\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n# suppress warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PROJECT DEFINITION","metadata":{}},{"cell_type":"markdown","source":"Predict whether income exceeds $50k/year based on U.S. census data.\n\nDataset which will be used:\n\n  * Survey of adults and their earnings\n  * Target variable: \n    * 1 = income > 50k USD\n    * 0 = income <= 50k USD\n  * Source: https://archive.ics.uci.edu/ml/datasets/Adult","metadata":{}},{"cell_type":"markdown","source":"# DATA PREPARATION","metadata":{}},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"ROOT = Path.cwd()\nROOT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pth_to_data = '../input/earnings-dataset/earnings_dataset.csv'\ndf = pd.read_csv(pth_to_data, sep=';')\n\ndf.head(n=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df) # number of rows (= observations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\n\nThe first part focuses on preparing the predictors into an **analytical basetable (ABT)** for modelling by:\n\n  * Splitting the dataset into training, selection and validation datasets.\n  * Binning continuous variables into discrete intervals.\n  * Replacing missing values of both categorical and continuous variables (which are now binned) with an additional \"Missing\" bin/category.\n  * Regrouping categories in new category \"other\".\n  * Replacing bins/categories with their corresponding incidence rate per category/bin.\n","metadata":{}},{"cell_type":"markdown","source":"### General structure\n\n##### Create instance of PreProcessor object\n`preprocessor = PreProcessor.from_params(parameters)`\n        \n##### Split data into train-selection-validation sets\n`basetable = preprocessor.train_selection_validation_split(data)`\n                \n##### Fit the pipeline\n`basetable = preprocessor.fit(basetable)`\n\n##### Transform the data\n`basetable = preprocessor.transform(basetable)`                  ","metadata":{}},{"cell_type":"code","source":"# create instance of PreProcessor object from parameters\npreprocessor = PreProcessor.from_params(\n        n_bins=10,\n        strategy='quantile',\n        serialization_path=ROOT/'pipeline.json')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data into train-selection-validation sets\nbasetable = preprocessor.train_selection_validation_split(\n                data=df,\n                target_column_name='TARGET',\n                train_prop=0.8,\n                selection_prop=0.1,\n                validation_prop=0.1)\n\nbasetable.head(n=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to create a list of variables by their datatype\ncontinuous_vars = ['age', 'education-num', 'capital-gain',\n                   'capital-loss', 'hours-per-week']\n\ndiscrete_vars = ['workclass', 'fnlwgt', 'education',\n                 'marital-status', 'occupation',\n                 'relationship', 'race', 'sex',\n                 'native-country']\n\ntarget_column_name = 'TARGET'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the pipeline\npreprocessor.fit(basetable[basetable['split']=='train'],\n                 continuous_vars=continuous_vars,\n                 discrete_vars=discrete_vars,\n                 target_column_name=target_column_name)\n\n# transform the data (e.g. perform discretisation, incidence replacement, ...)\nbasetable = preprocessor.transform(basetable,\n                                   continuous_vars=continuous_vars,\n                                   discrete_vars=discrete_vars)                        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basetable.head(n=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL BUILDING","metadata":{}},{"cell_type":"markdown","source":"## Feature preselection\nOnce we have the data prepared, we need to select the right variables. Thus, we perform a univariate preselection to rule out any predictor with little to no predictive power.\n\nThis preselection is based on an AUC threshold of a univariate model on the train and selection datasets.\n\nWe select all variables with `preselect_auc_threshold` > 0.55 and to avoid overfitting, we drop all variables where _(auc_train - auc_selection) >= 0.05_.\n\n","metadata":{}},{"cell_type":"markdown","source":"### General structure\n  \n##### Run univariate preselection procedure and plot output\n`df_auc = univariate_selection.compute_univariate_preselection(basetable, thresholds)`\n\n`plot_univariate_predictor_quality(df_auc)`\n\n##### Get a list of predictors selected by the univariate selection\n`preselected_predictors = univariate_selection.get_preselected_predictors(df_auc)`   \n\n##### Compute and plot correlations between preprocessed predictors\n`df_corr = univariate_selection.compute_correlations(basetable)`\n\n`plot_correlation_matrix(df_corr)`","metadata":{}},{"cell_type":"code","source":"preprocessed_predictors = [col for col in basetable.columns.tolist() if '_enc' in col]\n\ndf_auc = univariate_selection.compute_univariate_preselection(\n    target_enc_train_data=basetable[basetable['split']=='train'],\n    target_enc_selection_data=basetable[basetable['split']=='selection'],\n    predictors=preprocessed_predictors,\n    target_column=target_column_name,\n    preselect_auc_threshold=0.55,     \n    preselect_overtrain_threshold=0.05)\n\n# get a list of predictors selected by the univariate selection\npreselected_predictors = univariate_selection.get_preselected_predictors(df_auc)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# univariate feature importance\nplot_univariate_predictor_quality(df_auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute correlations between preprocessed predictors\ndf_corr = (univariate_selection\n           .compute_correlations(basetable[basetable['split']=='train'],\n                                 preprocessed_predictors))\n\nplot_correlation_matrix(df_corr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Forward feature selection\nAfter having preselected the features, we can start modelling using forward feature selection.\n\nSince we use target encoding on all our predictors, we will only consider models with positive coefficients (no sign flip should occur) as this makes the model more interpretable.\n","metadata":{}},{"cell_type":"markdown","source":"### General structure\n  \n##### Initialize forward feature selection procedure\n`forward_selection = ForwardFeatureSelection(parameters)`\n\n`forward_selection.fit(basetable)`\n\n##### Run forward feature selection and plot performance curves\n`performances = forward_selection.compute_model_performances(basetable, target_column_name)`\n\n`plot_performance_curves(performances)`\n\n##### Select and extract model of choice\n`model = forward_selection.get_model_from_step()`\n\n`final_predictors = model.predictors`\n\n##### Compute and plot the importance of each predictor in the model\n`variable_importance = model.compute_variable_importance(basetable)`\n  \n`plot_variable_importance(variable_importance)`","metadata":{}},{"cell_type":"code","source":"forward_selection = ForwardFeatureSelection(max_predictors=30, pos_only=True)\n\nforward_selection.fit(basetable[basetable['split']=='train'],\n                      target_column_name,\n                      preselected_predictors)\n\nperformances = forward_selection.compute_model_performances(basetable, target_column_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot performance curves\nplot_performance_curves(performances)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# after plotting the performances we select our model of choice (watch out: 0-based indexing)\nmodel = forward_selection.get_model_from_step(4)\n\n# we have chosen model with 5 variables, which we extract as follows\nfinal_predictors = model.predictors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can also compute and plot the importance of each predictor in the model\nvariable_importance = model.compute_variable_importance(basetable[basetable['split']=='selection'])\n\n# this is the correlation of the model score and each predictor    \nplot_variable_importance(variable_importance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL VALIDATION","metadata":{}},{"cell_type":"markdown","source":"## Evaluation\n\nThe next step after modelling is evaluating how well our model is performing.","metadata":{}},{"cell_type":"markdown","source":"### General structure\n  \n##### Instantiate Evaluator object\n`evaluator = Evaluator()`\n\n##### Automatically find the best cut-off probability\n`evaluator.fit()`\n\n##### Get and plot various scalar metrics\n`evaluator.scalar_metrics`\n\n`evaluator.plot_confusion_matrix()`\n\n`evaluator.plot_roc_curve()`\n\n`...`","metadata":{}},{"cell_type":"code","source":"# get numpy array of True target labels and predicted scores\ny_true = basetable[basetable['split']=='validation'][target_column_name].values\ny_pred = model.score_model(basetable[basetable['split']=='validation'])\n\nevaluator = Evaluator()\n\n# automatically find the best cut-off probability\nevaluator.fit(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get various scalar metrics such as accuracy, AUC, precision, recall, ...\nevaluator.scalar_metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator.plot_confusion_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator.plot_roc_curve()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator.plot_cumulative_gains()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator.plot_lift_curve()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluator.plot_cumulative_response_curve()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL USAGE","metadata":{}},{"cell_type":"markdown","source":"## PIG tables\nPredictor Insight Graphs, or PIGs, are plots which help us profile how each variable behaves in the model.","metadata":{}},{"cell_type":"markdown","source":"### General structure\n\n##### Generate PIG tables\n`pig_tables = generate_pig_tables(basetable)`\n\n##### Plot PIG tables\n`plot_incidence(pig_tables)`","metadata":{}},{"cell_type":"code","source":"predictor_list = [col for col in basetable.columns if col.endswith('_bin') or col.endswith('_processed')]\n\npig_tables = generate_pig_tables(\n    basetable[basetable['split']=='selection'],\n    id_column_name='ID',\n    target_column_name=target_column_name,\n    preprocessed_predictors=predictor_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_age_order = ['17.0 - 22.0', '22.0 - 26.0', '26.0 - 30.0', '30.0 - 33.0',\n                    '33.0 - 37.0', '37.0 - 41.0', '41.0 - 45.0', '45.0 - 50.0',\n                    '50.0 - 58.0', '58.0 - 90.0']              \n\nplot_incidence(pig_tables, 'age', column_age_order)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pig_tables[\"variable\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_hpw_order = ['1.0 - 24.0', '24.0 - 35.0', '35.0 - 40.0', \n                    '40.0 - 49.0', '49.0 - 55.0', '55.0 - 99.0']  \n\nplot_incidence(pig_tables, 'hours-per-week', column_hpw_order)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_incidence(pig_tables, 'education')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_incidence(pig_tables, 'relationship')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pig_tables.head(n=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Industrialization\nOnce we are happy with our model, we can industrialize it. All the preprocessing is in the output pipeline in a JSON format. The model comes from scikit-learn, which can be easily serialized (= saved) and exported.","metadata":{}},{"cell_type":"code","source":"with open(ROOT/'pipeline.json', \"r\") as read_file:\n    pipeline = json.load(read_file)\n\nprint(pipeline.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline['target_encoder']['_mapping']['age_bin']","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}