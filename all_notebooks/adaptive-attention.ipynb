{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time \nimport os \nimport numpy as np \nimport tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport collections\nimport random\nimport numpy as np\nimport os\nimport time\nimport json\nfrom PIL import Image\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T05:25:43.874618Z","iopub.execute_input":"2021-05-31T05:25:43.874985Z","iopub.status.idle":"2021-05-31T05:25:48.653408Z","shell.execute_reply.started":"2021-05-31T05:25:43.874904Z","shell.execute_reply":"2021-05-31T05:25:48.652341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n  tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:25:53.469887Z","iopub.execute_input":"2021-05-31T05:25:53.470241Z","iopub.status.idle":"2021-05-31T05:25:53.47556Z","shell.execute_reply.started":"2021-05-31T05:25:53.470202Z","shell.execute_reply":"2021-05-31T05:25:53.474521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_caption = \"../input/flickr30k/captions.txt\"\nfile_caption = open(path_caption,'r')\npath_folder_image = \"../input/flickr30k/images/flickr30k_images\"\nimage_path_to_caption = collections.defaultdict(list)\ncount = 0\nfor line in tqdm(file_caption):\n    if count == 0:\n        count += 1\n        continue\n    line = line[:-1]\n    \n    [nameimage, number_caption, text] = line.split(\"|\")\n#     number_caption = int(number_caption)\n    caption = f\"<start> {text} <end>\"\n    image_path = path_folder_image + \"/\" + nameimage\n    image_path_to_caption[image_path].append(caption)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:25:58.225887Z","iopub.execute_input":"2021-05-31T05:25:58.22625Z","iopub.status.idle":"2021-05-31T05:25:58.737198Z","shell.execute_reply.started":"2021-05-31T05:25:58.226213Z","shell.execute_reply":"2021-05-31T05:25:58.736334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_paths = list(image_path_to_caption.keys())\n# # random.shuffle(image_paths)\n# train_image_paths = image_paths[:6000]\n# # val_image_paths = image_paths[24000:]\n# print(len(train_image_paths))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T14:54:14.877325Z","iopub.execute_input":"2021-05-30T14:54:14.877602Z","iopub.status.idle":"2021-05-30T14:54:14.882839Z","shell.execute_reply.started":"2021-05-30T14:54:14.877574Z","shell.execute_reply":"2021-05-30T14:54:14.881032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_paths = list(image_path_to_caption.keys())\nrandom.shuffle(image_paths)\ntrain_image_paths = image_paths[:24000]\nval_image_paths = image_paths[24000:]\nprint(len(train_image_paths))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:26:01.849603Z","iopub.execute_input":"2021-05-31T05:26:01.84993Z","iopub.status.idle":"2021-05-31T05:26:01.888654Z","shell.execute_reply.started":"2021-05-31T05:26:01.849898Z","shell.execute_reply":"2021-05-31T05:26:01.887639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_captions = []\nimg_name_vector = []\n\nfor image_path in train_image_paths:\n  caption_list = image_path_to_caption[image_path]\n  train_captions.extend(caption_list)\n  img_name_vector.extend([image_path] * len(caption_list))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:26:03.857467Z","iopub.execute_input":"2021-05-31T05:26:03.8578Z","iopub.status.idle":"2021-05-31T05:26:03.893787Z","shell.execute_reply.started":"2021-05-31T05:26:03.857769Z","shell.execute_reply":"2021-05-31T05:26:03.892993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n#     if \"jpg\" not in image_path:\n#         print(\"Found\")\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.efficientnet.preprocess_input(img)\n    return img, image_path","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:26:09.280865Z","iopub.execute_input":"2021-05-31T05:26:09.281195Z","iopub.status.idle":"2021-05-31T05:26:09.285991Z","shell.execute_reply.started":"2021-05-31T05:26:09.281161Z","shell.execute_reply":"2021-05-31T05:26:09.284881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_model = tf.keras.applications.EfficientNetB7(include_top=False,\n                                                weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:26:11.199619Z","iopub.execute_input":"2021-05-31T05:26:11.19996Z","iopub.status.idle":"2021-05-31T05:26:25.559801Z","shell.execute_reply.started":"2021-05-31T05:26:11.19993Z","shell.execute_reply":"2021-05-31T05:26:25.558836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dataset_npy = \"./Dataset_npy\"\nos.makedirs(Dataset_npy)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:26:25.561243Z","iopub.execute_input":"2021-05-31T05:26:25.561576Z","iopub.status.idle":"2021-05-31T05:26:25.565415Z","shell.execute_reply.started":"2021-05-31T05:26:25.561541Z","shell.execute_reply":"2021-05-31T05:26:25.564619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique images\nencode_train = sorted(set(img_name_vector))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(\n  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    nameimage = p.numpy().decode(\"utf-8\").split(\"/\")[-1]\n    path = Dataset_npy + \"/\" + nameimage\n#     path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path, bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:26:25.567528Z","iopub.execute_input":"2021-05-31T05:26:25.568086Z","iopub.status.idle":"2021-05-31T05:33:39.746484Z","shell.execute_reply.started":"2021-05-31T05:26:25.568046Z","shell.execute_reply":"2021-05-31T05:33:39.745629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the maximum length of any caption in our dataset\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:39.748209Z","iopub.execute_input":"2021-05-31T05:33:39.748559Z","iopub.status.idle":"2021-05-31T05:33:39.752702Z","shell.execute_reply.started":"2021-05-31T05:33:39.74852Z","shell.execute_reply":"2021-05-31T05:33:39.751744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfolder_save = './Save_Checkpoint'\nos.makedirs(folder_save)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:04:50.584225Z","iopub.execute_input":"2021-05-30T15:04:50.587989Z","iopub.status.idle":"2021-05-30T15:04:50.62065Z","shell.execute_reply.started":"2021-05-30T15:04:50.587946Z","shell.execute_reply":"2021-05-30T15:04:50.619315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose the top 5000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(train_captions)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:39.753908Z","iopub.execute_input":"2021-05-31T05:33:39.754392Z","iopub.status.idle":"2021-05-31T05:33:41.733777Z","shell.execute_reply.started":"2021-05-31T05:33:39.754357Z","shell.execute_reply":"2021-05-31T05:33:41.732928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.word_index['<start>'])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:04:53.739314Z","iopub.execute_input":"2021-05-30T15:04:53.739701Z","iopub.status.idle":"2021-05-30T15:04:53.746241Z","shell.execute_reply.started":"2021-05-30T15:04:53.73966Z","shell.execute_reply":"2021-05-30T15:04:53.744956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\n# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:41.735025Z","iopub.execute_input":"2021-05-31T05:33:41.73555Z","iopub.status.idle":"2021-05-31T05:33:44.664839Z","shell.execute_reply.started":"2021-05-31T05:33:41.735509Z","shell.execute_reply":"2021-05-31T05:33:44.663982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(img_name_vector, cap_vector):\n  img_to_cap_vector[img].append(cap)\n\n# Create training and validation sets using an 80-10-10 split randomly.\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\n\nslice_index = int(len(img_keys)*0.8)\nimg_name_train_keys, img_name_val_keys, img_name_test_keys = img_keys[:slice_index], \\\n                                img_keys[slice_index: slice_index + int(len(img_keys)*0.1)],\\\n                                img_keys[slice_index + int(len(img_keys)*0.1):]\n\nimg_name_train = []\ncap_train = []\nfor imgt in img_name_train_keys:\n  capt_len = len(img_to_cap_vector[imgt])\n  img_name_train.extend([imgt] * capt_len)\n  cap_train.extend(img_to_cap_vector[imgt])\n\nimg_name_val = []\ncap_val = []\nfor imgv in img_name_val_keys:\n  capv_len = len(img_to_cap_vector[imgv])\n  img_name_val.extend([imgv] * capv_len)\n  cap_val.extend(img_to_cap_vector[imgv])\n\nimg_name_test = []\ncap_test = []\nfor imge in img_name_val_keys:\n  cape_len = len(img_to_cap_vector[imge])\n  img_name_test.extend([imge] * cape_len)\n  cap_test.extend(img_to_cap_vector[imge])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:44.666032Z","iopub.execute_input":"2021-05-31T05:33:44.666378Z","iopub.status.idle":"2021-05-31T05:33:44.785378Z","shell.execute_reply.started":"2021-05-31T05:33:44.666343Z","shell.execute_reply":"2021-05-31T05:33:44.784618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feel free to change these parameters according to your system's configuration\n\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = top_k + 1\nnum_steps = len(img_name_train) // BATCH_SIZE\nval_num_steps = len(img_name_val) // BATCH_SIZE\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2560\nattention_features_shape = 81","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:44.788047Z","iopub.execute_input":"2021-05-31T05:33:44.788407Z","iopub.status.idle":"2021-05-31T05:33:44.794561Z","shell.execute_reply.started":"2021-05-31T05:33:44.788371Z","shell.execute_reply":"2021-05-31T05:33:44.793815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the numpy files\ndef map_func(img_name):\n    nameimage = img_name.split(\"/\")[-1]\n    path_image = Dataset_npy + \"/\" + nameimage + '.npy'\n    img_tensor = np.load(path_image)\n    return img_tensor","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:44.796288Z","iopub.execute_input":"2021-05-31T05:33:44.796647Z","iopub.status.idle":"2021-05-31T05:33:44.804334Z","shell.execute_reply.started":"2021-05-31T05:33:44.796602Z","shell.execute_reply":"2021-05-31T05:33:44.80351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(img_name_train), len(img_name_val), len(img_name_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T00:00:21.290862Z","iopub.execute_input":"2021-05-31T00:00:21.291206Z","iopub.status.idle":"2021-05-31T00:00:21.298695Z","shell.execute_reply.started":"2021-05-31T00:00:21.29116Z","shell.execute_reply":"2021-05-31T00:00:21.297569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:44.805388Z","iopub.execute_input":"2021-05-31T05:33:44.805782Z","iopub.status.idle":"2021-05-31T05:33:46.035057Z","shell.execute_reply.started":"2021-05-31T05:33:44.805746Z","shell.execute_reply":"2021-05-31T05:33:46.034223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Flickr30kDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n    \"\"\"\n\n    def __init__(self, img_name_list, caption_list, map_func):\n        self.img_name_list = img_name_list\n        self.caption_list = caption_list\n        self.map_func = map_func\n    def __getitem__(self, i):\n        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n        img_name = self.img_name_list[i]\n        cap = self.caption_list[i]\n        img_tensor = self.map_func(img_name)\n        return img_tensor, cap\n    def __len__(self):\n        return len(self.img_name_list)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.036358Z","iopub.execute_input":"2021-05-31T05:33:46.03672Z","iopub.status.idle":"2021-05-31T05:33:46.043709Z","shell.execute_reply.started":"2021-05-31T05:33:46.036682Z","shell.execute_reply":"2021-05-31T05:33:46.042125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_train_dataset = Flickr30kDataset(img_name_train, cap_train, map_func)\ntorch_val_dataset = Flickr30kDataset(img_name_val, cap_val, map_func)\ntorch_test_dataset = Flickr30kDataset(img_name_test, cap_test, map_func)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.045078Z","iopub.execute_input":"2021-05-31T05:33:46.045424Z","iopub.status.idle":"2021-05-31T05:33:46.057599Z","shell.execute_reply.started":"2021-05-31T05:33:46.04539Z","shell.execute_reply":"2021-05-31T05:33:46.056781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_train_dataset[1][0].shape, torch_train_dataset[1][1].shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:04:58.636027Z","iopub.execute_input":"2021-05-30T15:04:58.636457Z","iopub.status.idle":"2021-05-30T15:04:58.686451Z","shell.execute_reply.started":"2021-05-30T15:04:58.636421Z","shell.execute_reply":"2021-05-30T15:04:58.685352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset = torch_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(dataset = torch_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(dataset = torch_test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.05891Z","iopub.execute_input":"2021-05-31T05:33:46.059359Z","iopub.status.idle":"2021-05-31T05:33:46.068006Z","shell.execute_reply.started":"2021-05-31T05:33:46.059259Z","shell.execute_reply":"2021-05-31T05:33:46.067265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.069131Z","iopub.execute_input":"2021-05-31T05:33:46.069752Z","iopub.status.idle":"2021-05-31T05:33:46.215354Z","shell.execute_reply.started":"2021-05-31T05:33:46.069719Z","shell.execute_reply":"2021-05-31T05:33:46.214567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder(nn.Module):\n    # Since you have already extracted the features and dumped it\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, hidden_size):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 81, embedding_dim)\n        self.fc = nn.Linear(2560, hidden_size)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.relu(self.fc(x))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.216521Z","iopub.execute_input":"2021-05-31T05:33:46.217011Z","iopub.status.idle":"2021-05-31T05:33:46.222409Z","shell.execute_reply.started":"2021-05-31T05:33:46.216972Z","shell.execute_reply":"2021-05-31T05:33:46.221451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# linear = nn.Linear(512, 81, bias=False)\n# V = torch.rand(1, 81, 512)\n# h_t = torch.rand(5,10, 512)\n# out_V = linear(V)\n# out_h = linear(h_t)\n# out_V.shape, out_h.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:04:58.901309Z","iopub.execute_input":"2021-05-30T15:04:58.901975Z","iopub.status.idle":"2021-05-30T15:04:58.916584Z","shell.execute_reply.started":"2021-05-30T15:04:58.90193Z","shell.execute_reply":"2021-05-30T15:04:58.915428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#spatial attention \nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.affine_v = nn.Linear(hidden_size, 81, bias=False) # W_v\n        self.affine_g = nn.Linear(hidden_size, 81, bias=False) # W_g\n        self.affine_s = nn.Linear(hidden_size, 81, bias = False) # W_s\n        self.affine_h = nn.Linear(81, 1, bias=False) # w_h\n\n        self.dropout = nn.Dropout(0.5)\n        self.init_weights()\n\n    def init_weights( self ):\n        \"\"\"Initialize the weights.\"\"\"\n        init.xavier_uniform_( self.affine_v.weight )\n        init.xavier_uniform_( self.affine_g.weight )\n        init.xavier_uniform_( self.affine_h.weight )\n        init.xavier_uniform_( self.affine_s.weight )\n\n    def forward( self, V, h_t, s_t ):\n        '''\n        Input: V=[v_1, v_2, ... v_k], h_t, s_t from LSTM\n        Output: c_hat_t, attention feature map\n        '''\n        \n        # W_v * V + W_g * h_t * 1^T\n\n        content_v = self.affine_v( self.dropout( V )).unsqueeze( 1 ) \\\n                    + self.affine_g( self.dropout( h_t ) ).unsqueeze( 2 )\n#         print('Spatial Attention')\n#         print('V shape: {}'.format(V.shape))\n#         print('h_t shape: {}'.format(h_t.shape))\n#         print('s_t shape: {}'.format(s_t.shape))\n#         print('content_v shape: {}'.format(content_v.shape))\n        # z_t = W_h * tanh( content_v )\n        z_t = self.affine_h( self.dropout( F.tanh( content_v ) ) ).squeeze( 3 )\n        alpha_t = F.softmax( z_t.view( -1, z_t.size( 2 )), dim=-1).view( z_t.size( 0 ), z_t.size( 1 ), -1 )\n#         print('alpha_t shape: {}'.format(alpha_t.shape))\n        # Construct c_t: B x seq x hidden_size\n        c_t = torch.bmm( alpha_t, V ).squeeze( 2 )\n#         print('c_t shape: {}'.format(c_t.shape))\n        # W_s * s_t + W_g * h_t\n        content_s = self.affine_s( self.dropout( s_t ) ) + self.affine_g( self.dropout( h_t ) )\n#         print('content_s shape: {}'.format(content_s.shape))\n        # w_t * tanh( content_s )\n        z_t_extended = self.affine_h( self.dropout( F.tanh( content_s ) ) )\n#         print('z_t shape: {}'.format(z_t.shape))\n#         print('z_t_extended: {}'.format(z_t_extended.shape))\n        # Attention score between sentinel and image content\n        extended = torch.cat( ( z_t, z_t_extended ), dim=2 )\n#         print('extended z shape: {}'.format(extended.shape))\n        alpha_hat_t = F.softmax( extended.view( -1, extended.size( 2 )), dim=-1).view( extended.size( 0 ), extended.size( 1 ), -1 )\n#         print('alpha_hat_t shape: {}'.format(alpha_hat_t.shape))\n        beta_t = alpha_hat_t[ :, :, -1 ]\n        \n        # c_hat_t = beta * s_t + ( 1 - beta ) * c_t\n        beta_t = beta_t.unsqueeze( 2 )\n        c_hat_t = beta_t * s_t + ( 1 - beta_t ) * c_t\n#         print('c_hat_t shape: {}'.format(c_hat_t.shape))\n#         print('-'*50)\n        return c_hat_t, alpha_t, beta_t\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.22374Z","iopub.execute_input":"2021-05-31T05:33:46.224106Z","iopub.status.idle":"2021-05-31T05:33:46.240069Z","shell.execute_reply.started":"2021-05-31T05:33:46.224067Z","shell.execute_reply":"2021-05-31T05:33:46.239328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentinel BLock    \nclass Sentinel( nn.Module ):\n    def __init__( self, input_size, hidden_size ):\n        super( Sentinel, self ).__init__()\n\n        self.affine_x = nn.Linear( input_size, hidden_size, bias=False )\n        self.affine_h = nn.Linear( hidden_size, hidden_size, bias=False )\n        \n        # Dropout applied before affine transformation\n        self.dropout = nn.Dropout( 0.5 )\n        \n        self.init_weights()\n        \n    def init_weights( self ):\n        init.xavier_uniform_( self.affine_x.weight )\n        init.xavier_uniform_( self.affine_h.weight )\n        \n    def forward( self, x_t, h_t_1, cell_t ):\n        \n        # g_t = sigmoid( W_x * x_t + W_h * h_(t-1) )        \n        gate_t = self.affine_x( self.dropout( x_t ) ) + self.affine_h( self.dropout( h_t_1 ) )\n        gate_t = F.sigmoid( gate_t )\n#         print('Sentinel block')\n#         print('x_t shape: {}'.format(x_t.shape))\n#         print('h_t_1 shape: {}'.format(h_t_1.shape))\n#         print('cell_t shape: {}'.format(cell_t.shape))\n#         print('gate_t shape: {}'.format(gate_t.shape))\n        \n        # Sentinel embedding\n        s_t =  gate_t * F.tanh( cell_t )\n#         print('s_t shape: {}'.format(s_t.shape))\n#         print('-'*50)\n        return s_t","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.241101Z","iopub.execute_input":"2021-05-31T05:33:46.241502Z","iopub.status.idle":"2021-05-31T05:33:46.254233Z","shell.execute_reply.started":"2021-05-31T05:33:46.241463Z","shell.execute_reply":"2021-05-31T05:33:46.253343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adaptive Attention Block: C_t, Spatial Attention Weights, Sentinel embedding    \nclass AdaptiveBlock( nn.Module ):\n    \n    def __init__( self, embed_size, hidden_size, vocab_size ):\n        super( AdaptiveBlock, self ).__init__()\n\n        # Sentinel block\n        self.sentinel = Sentinel( embed_size, hidden_size )\n        \n        # Image Spatial Attention Block\n        self.atten = Attention( hidden_size )\n        \n        # Final Caption generator\n        self.mlp = nn.Linear( hidden_size, vocab_size )\n        \n        # Dropout layer inside Affine Transformation\n        self.dropout = nn.Dropout( 0.5 )\n        \n        self.hidden_size = hidden_size\n        self.init_weights()\n        \n    def init_weights( self ):\n        '''\n        Initialize final classifier weights\n        '''\n        init.kaiming_normal_( self.mlp.weight, mode='fan_in' )\n        self.mlp.bias.data.fill_( 0 )\n        \n        \n    def forward( self, x, hiddens, cells, V ):\n#         print('Adaptive attention block')\n#         print('hiddens shape: {}'.format(hiddens.shape))\n#         print('cells shape: {}'.format(cells.shape))\n#         print('V shape: {}'.format(V.shape))\n#         print('-'*50)\n        # hidden for sentinel should be h0-ht-1\n        h0 = self.init_hidden( x.size(0) )[0].transpose( 0,1 )\n        \n        # h_(t-1): B x seq x hidden_size ( 0 - t-1 )\n        if hiddens.size( 1 ) > 1:\n            hiddens_t_1 = torch.cat( ( h0, hiddens[ :, :-1, : ] ), dim=1 )\n        else:\n            hiddens_t_1 = h0\n\n        # Get Sentinel embedding, it's calculated blockly    \n        sentinel = self.sentinel( x, hiddens_t_1, cells )\n        \n        # Get C_t, Spatial attention, sentinel score\n        c_hat, atten_weights, beta = self.atten( V, hiddens, sentinel )\n        \n        # Final score along vocabulary\n        scores = self.mlp( self.dropout( c_hat + hiddens ) )\n        \n        return scores, atten_weights, beta\n    \n    def init_hidden( self, bsz ):\n        '''\n        Hidden_0 & Cell_0 initialization\n        '''\n        weight = next( self.parameters() ).data\n        \n        if torch.cuda.is_available():\n            return ( Variable( weight.new( 1 , bsz, self.hidden_size ).zero_().cuda() ),\n                    Variable( weight.new( 1,  bsz, self.hidden_size ).zero_().cuda() ) ) \n        else: \n            return ( Variable( weight.new( 1 , bsz, self.hidden_size ).zero_() ),\n                    Variable( weight.new( 1,  bsz, self.hidden_size ).zero_() ) ) ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.255515Z","iopub.execute_input":"2021-05-31T05:33:46.25595Z","iopub.status.idle":"2021-05-31T05:33:46.269897Z","shell.execute_reply.started":"2021-05-31T05:33:46.255914Z","shell.execute_reply":"2021-05-31T05:33:46.269106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caption Decoder\nclass Decoder( nn.Module ):\n    def __init__( self, embed_size, vocab_size, hidden_size ):\n        super( Decoder, self ).__init__()\n\n        # word embedding\n        self.embed = nn.Embedding( vocab_size, embed_size )\n        \n        # LSTM decoder: input = [ w_t; v_g ] => 2 x word_embed_size;\n        self.LSTM = nn.LSTM( embed_size, hidden_size, 1, batch_first=True )\n        \n        # Save hidden_size for hidden and cell variable \n        self.hidden_size = hidden_size\n        \n        # Adaptive Attention Block: Sentinel + C_hat + Final scores for caption sampling\n        self.adaptive = AdaptiveBlock( embed_size, hidden_size, vocab_size )\n        \n    def forward( self, V , captions, states=None ):\n#         print('Decoder')\n        \n        # Word Embedding\n        x = self.embed( captions )\n        \n#         print('x shape: {}'.format(x.shape))\n#         print('V shape: {}'.format(V.shape))\n        # Hiddens: Batch x seq_len x hidden_size\n        # Cells: seq_len x Batch x hidden_size, default setup by Pytorch\n        if torch.cuda.is_available():\n            hiddens = Variable( torch.zeros( x.size(0), x.size(1), self.hidden_size ).cuda() )\n            cells = Variable( torch.zeros( x.size(1), x.size(0), self.hidden_size ).cuda() )\n        else:\n            hiddens = Variable( torch.zeros( x.size(0), x.size(1), self.hidden_size ) )\n            cells = Variable( torch.zeros( x.size(1), x.size(0), self.hidden_size ) )            \n        \n        # Recurrent Block\n        # Retrieve hidden & cell for Sentinel simulation\n        for time_step in range( x.size( 1 ) ):\n#             print('Time step: {}'.format(time_step))\n            # Feed in x_t one at a time\n            x_t = x[ :, time_step, : ]\n            x_t = x_t.unsqueeze( 1 )\n            \n            h_t, states = self.LSTM( x_t, states )\n#             print('h_t shape: {}'.format(h_t.squeeze(2).shape))\n#             print('hiddens shape: {}'.format(hiddens.shape))\n            # Save hidden and cell\n#             print(hiddens[:, time_step, :].shape)\n            hiddens[ :, time_step, : ] = torch.squeeze(h_t, 1)  # Batch_first\n            cells[ time_step, :, : ] = states[1]\n        \n        # cell: Batch x seq_len x hidden_size\n        cells = cells.transpose( 0, 1 )\n        \n        scores, atten_weights, beta = self.adaptive( x, hiddens, cells, V )\n        \n        # Return states for Caption Sampling purpose\n        return scores, states, atten_weights, beta","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.271192Z","iopub.execute_input":"2021-05-31T05:33:46.271588Z","iopub.status.idle":"2021-05-31T05:33:46.285261Z","shell.execute_reply.started":"2021-05-31T05:33:46.271551Z","shell.execute_reply":"2021-05-31T05:33:46.284447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# decoder = Decoder(256, 5000, 512)\n# V = torch.rand(16, 81, 512)\n# captions = torch.randint(high=5000, size=(16, 2))\n# out = decoder(V, captions)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T15:04:59.00123Z","iopub.execute_input":"2021-05-30T15:04:59.001798Z","iopub.status.idle":"2021-05-30T15:04:59.014172Z","shell.execute_reply.started":"2021-05-30T15:04:59.001753Z","shell.execute_reply":"2021-05-30T15:04:59.012926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Whole Architecture with Image Encoder and Caption decoder        \nclass Encoder2Decoder( nn.Module ):\n    def __init__( self, embed_size, vocab_size, hidden_size ):\n        super( Encoder2Decoder, self ).__init__()\n        \n        # Image CNN encoder and Adaptive Attention Decoder\n        self.encoder = CNN_Encoder(hidden_size )\n        self.decoder = Decoder( embed_size, vocab_size, hidden_size )\n        \n        \n    def forward( self, images, captions):\n        \n        # V=[ v_1, ..., v_k ] in the original paper\n\n        V = self.encoder( images )\n        \n        # Language Modeling on word prediction\n        scores, _, _,_ = self.decoder( V, captions )\n        return scores\n        # Pack it to make criterion calculation more efficient\n#         packed_scores = pack_padded_sequence( scores, lengths, batch_first=True )\n        \n#         return packed_scores\n    \n    # Caption generator\n    def sampler( self, images, max_len=20 ):\n        \"\"\"\n        Samples captions for given image features (Greedy search).\n        \"\"\"\n        \n\n        V = self.encoder( images )\n            \n        # Build the starting token Variable <start> (index 1): B x 1\n        if torch.cuda.is_available():\n            captions = Variable( torch.LongTensor( images.size( 0 ), 1 ).fill_( 1 ).cuda() )\n        else:\n            captions = Variable( torch.LongTensor( images.size( 0 ), 1 ).fill_( 1 ) )\n        \n        # Get generated caption idx list, attention weights and sentinel score\n        sampled_ids = []\n        attention = []\n        Beta = []\n        \n        # Initial hidden states\n        states = None\n\n        for i in range( max_len ):\n\n            scores, states, atten_weights, beta = self.decoder( V, captions, states ) \n            predicted = scores.max( 2 )[ 1 ] # argmax\n            captions = predicted\n            \n            # Save sampled word, attention map and sentinel at each timestep\n            sampled_ids.append( captions )\n            attention.append( atten_weights )\n            Beta.append( beta )\n        \n        # caption: B x max_len\n        # attention: B x max_len x 49\n        # sentinel: B x max_len\n        sampled_ids = torch.cat( sampled_ids, dim=1 )\n        attention = torch.cat( attention, dim=1 )\n        Beta = torch.cat( Beta, dim=1 )\n        \n        return sampled_ids, attention, Beta","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.286645Z","iopub.execute_input":"2021-05-31T05:33:46.287039Z","iopub.status.idle":"2021-05-31T05:33:46.300058Z","shell.execute_reply.started":"2021-05-31T05:33:46.287003Z","shell.execute_reply":"2021-05-31T05:33:46.299291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T02:22:14.694876Z","iopub.execute_input":"2021-05-31T02:22:14.695195Z","iopub.status.idle":"2021-05-31T02:22:14.701863Z","shell.execute_reply.started":"2021-05-31T02:22:14.695165Z","shell.execute_reply":"2021-05-31T02:22:14.701032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('../input/savedresults')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:33:46.301464Z","iopub.execute_input":"2021-05-31T05:33:46.301853Z","iopub.status.idle":"2021-05-31T05:33:46.313195Z","shell.execute_reply.started":"2021-05-31T05:33:46.301816Z","shell.execute_reply":"2021-05-31T05:33:46.312445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adaptive = Encoder2Decoder(embedding_dim, vocab_size, units)\nadaptive.load_state_dict(torch.load('adaptive-13.pkl'))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:47:55.321016Z","iopub.execute_input":"2021-05-31T05:47:55.321411Z","iopub.status.idle":"2021-05-31T05:48:00.979086Z","shell.execute_reply.started":"2021-05-31T05:47:55.321375Z","shell.execute_reply":"2021-05-31T05:48:00.978246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('../../working')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:13.162019Z","iopub.execute_input":"2021-05-31T05:48:13.162396Z","iopub.status.idle":"2021-05-31T05:48:13.166707Z","shell.execute_reply.started":"2021-05-31T05:48:13.162362Z","shell.execute_reply":"2021-05-31T05:48:13.165446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adaptive = Encoder2Decoder(embedding_dim, vocab_size, units)\nlearning_rate = 0.001\nloss_func = nn.CrossEntropyLoss()\nparams = list(adaptive.encoder.parameters()) + list( adaptive.decoder.parameters())\noptimizer = torch.optim.Adam(params, lr=learning_rate)\nif torch.cuda.is_available():\n    adaptive.cuda()\n    loss_func.cuda()\n# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\n\ncheckpoint_path = \"./checkpoints/train\"\nos.makedirs(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:18.718994Z","iopub.execute_input":"2021-05-31T05:48:18.719332Z","iopub.status.idle":"2021-05-31T05:48:18.754304Z","shell.execute_reply.started":"2021-05-31T05:48:18.719299Z","shell.execute_reply":"2021-05-31T05:48:18.75349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variable wrapper\ndef to_var(x, volatile=False):\n    '''\n    Wrapper torch tensor into Variable\n    '''\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable( x, volatile=volatile )","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:27.346625Z","iopub.execute_input":"2021-05-31T05:48:27.34695Z","iopub.status.idle":"2021-05-31T05:48:27.351283Z","shell.execute_reply.started":"2021-05-31T05:48:27.346918Z","shell.execute_reply":"2021-05-31T05:48:27.350257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('../input/savedresults')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:37.140024Z","iopub.execute_input":"2021-05-31T05:48:37.140381Z","iopub.status.idle":"2021-05-31T05:48:37.143872Z","shell.execute_reply.started":"2021-05-31T05:48:37.14035Z","shell.execute_reply":"2021-05-31T05:48:37.143031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result = pd.read_csv('loss_result.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:41.118017Z","iopub.execute_input":"2021-05-31T05:48:41.118386Z","iopub.status.idle":"2021-05-31T05:48:41.143986Z","shell.execute_reply.started":"2021-05-31T05:48:41.118351Z","shell.execute_reply":"2021-05-31T05:48:41.143192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:43.28096Z","iopub.execute_input":"2021-05-31T05:48:43.281304Z","iopub.status.idle":"2021-05-31T05:48:43.304435Z","shell.execute_reply.started":"2021-05-31T05:48:43.281272Z","shell.execute_reply":"2021-05-31T05:48:43.303444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_plot = df_result['train_loss'].values.tolist()\nval_plot = df_result['val_loss'].values.tolist() ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:48.765042Z","iopub.execute_input":"2021-05-31T05:48:48.765427Z","iopub.status.idle":"2021-05-31T05:48:48.771751Z","shell.execute_reply.started":"2021-05-31T05:48:48.765395Z","shell.execute_reply":"2021-05-31T05:48:48.770792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_plot","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:49.902785Z","iopub.execute_input":"2021-05-31T05:48:49.903104Z","iopub.status.idle":"2021-05-31T05:48:49.910275Z","shell.execute_reply.started":"2021-05-31T05:48:49.903073Z","shell.execute_reply":"2021-05-31T05:48:49.909223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('../../working')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:48:57.427908Z","iopub.execute_input":"2021-05-31T05:48:57.428243Z","iopub.status.idle":"2021-05-31T05:48:57.431742Z","shell.execute_reply.started":"2021-05-31T05:48:57.428208Z","shell.execute_reply":"2021-05-31T05:48:57.430942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:49:02.357865Z","iopub.execute_input":"2021-05-31T05:49:02.358204Z","iopub.status.idle":"2021-05-31T05:49:02.364194Z","shell.execute_reply.started":"2021-05-31T05:49:02.35817Z","shell.execute_reply":"2021-05-31T05:49:02.363307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the Models\ntotal_step = len(train_dataloader)\nnum_epochs = 7\n#     cider_scores = []\n#     best_cider = 0.0\nbest_epoch = 0\n   \n# Start Training \nfor epoch in range(1, num_epochs + 1):\n    start = time.time()\n    total_loss = 0\n    \n    # training phase\n    print('------------------Training for Epoch %d----------------'%( epoch ))\n    for i, (images, captions) in enumerate( train_dataloader ):\n        loss = 0\n        # Set mini-batch dataset\n        images = to_var( images )\n        captions = to_var( captions ).to(torch.int64)\n#         print('Images shape: {}'.format(images.shape))\n#         print('Captions shape: {}'.format(captions.shape))\n\n\n        # Forward, Backward and Optimize\n        adaptive.train()\n        adaptive.zero_grad()\n\n        scores = adaptive(images, captions)\n#         print('Scores shape: {}'.format(scores.shape))\n        loss = loss_func(scores[:, :-1, :].view(scores[:, :-1, :].size(0), scores[:, :-1, :].size(2), -1), captions[:, 1:])\n        total_loss += loss\n        loss.backward()\n        optimizer.step()\n\n\n        # Print log info\n        if i%100 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Batch [{i}/{total_step}], Train Loss: {loss}')\n    \n    loss_plot.append(total_loss/total_step)\n    print(f'Epoch {epoch} loss: {total_loss/total_step}')          \n    \n    # validation phase\n    print('Validation for Epoch %d'%( epoch ))\n    total_loss = 0    \n    for i, (images, captions) in enumerate( val_dataloader ):\n        loss = 0\n        # Set mini-batch dataset\n        images = to_var( images )\n        captions = to_var( captions ).to(torch.int64)\n\n\n        # Forward, Backward and Optimize\n        adaptive.eval()\n        with torch.no_grad():\n            scores = adaptive(images, captions)\n#         print('Scores shape: {}'.format(scores.shape))\n        loss = loss_func(scores[:, :-1, :].view(scores[:, :-1, :].size(0), scores[:, :-1, :].size(2), -1), captions[:, 1:])\n        total_loss += loss\n\n\n        # Print log info\n        if i%100 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Batch [{i}/{len(val_dataloader)}], Val Loss: {loss}')\n    val_plot.append(total_loss/len(val_dataloader))\n    print(f'Epoch {epoch} val loss: {total_loss/len(val_dataloader)}')     \n    print(f'Time taken for epoch: {time.time() - start}s')        \n    # Save the Adaptive Attention model after every epoch\n    torch.save( adaptive.state_dict(), \n                    os.path.join( checkpoint_path, \n                    'adaptive-%d.pkl'%( epoch ) ) )          \n      \n        \n#         # Evaluation on validation set        \n#         cider = coco_eval( adaptive, args, epoch )\n#         cider_scores.append( cider )        \n        \n#         if cider > best_cider:\n#             best_cider = cider\n#             best_epoch = epoch\n       \n#         if len( cider_scores ) > 5:\n            \n#             last_6 = cider_scores[-6:]\n#             last_6_max = max( last_6 )\n            \n#             # Test if there is improvement, if not do early stopping\n#             if last_6_max != best_cider:\n                \n#                 print 'No improvement with CIDEr in the last 6 epochs...Early stopping triggered.'\n#                 print 'Model of best epoch #: %d with CIDEr score %.2f'%( best_epoch, best_cider )\n#                 break","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:50:09.598221Z","iopub.execute_input":"2021-05-31T05:50:09.598542Z","iopub.status.idle":"2021-05-31T07:57:22.091838Z","shell.execute_reply.started":"2021-05-31T05:50:09.598513Z","shell.execute_reply":"2021-05-31T07:57:22.087964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:58:12.26033Z","iopub.execute_input":"2021-05-31T07:58:12.260664Z","iopub.status.idle":"2021-05-31T07:58:12.273311Z","shell.execute_reply.started":"2021-05-31T07:58:12.260632Z","shell.execute_reply":"2021-05-31T07:58:12.272092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(os.path.join( checkpoint_path, \n                    'adaptive-%d.pkl'%(7) ))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:58:21.448152Z","iopub.execute_input":"2021-05-31T07:58:21.448506Z","iopub.status.idle":"2021-05-31T07:58:21.454513Z","shell.execute_reply.started":"2021-05-31T07:58:21.448472Z","shell.execute_reply":"2021-05-31T07:58:21.453592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_plot_num = [val.item() if type(val) == torch.Tensor else val  for val in loss_plot]\nval_plot_num = [val.item() if type(val) == torch.Tensor else val for val in val_plot]","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:58:37.525455Z","iopub.execute_input":"2021-05-31T07:58:37.525766Z","iopub.status.idle":"2021-05-31T07:58:37.532473Z","shell.execute_reply.started":"2021-05-31T07:58:37.525736Z","shell.execute_reply":"2021-05-31T07:58:37.531621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_plot_num, label='Train Loss')\nplt.plot(val_plot_num, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:00:57.550589Z","iopub.execute_input":"2021-05-31T08:00:57.550916Z","iopub.status.idle":"2021-05-31T08:00:57.698838Z","shell.execute_reply.started":"2021-05-31T08:00:57.550885Z","shell.execute_reply":"2021-05-31T08:00:57.697885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_plot_num","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:18:33.74425Z","iopub.execute_input":"2021-05-31T05:18:33.74461Z","iopub.status.idle":"2021-05-31T05:18:33.751999Z","shell.execute_reply.started":"2021-05-31T05:18:33.744558Z","shell.execute_reply":"2021-05-31T05:18:33.751134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_plot_num","metadata":{"execution":{"iopub.status.busy":"2021-05-31T05:18:14.191005Z","iopub.execute_input":"2021-05-31T05:18:14.191331Z","iopub.status.idle":"2021-05-31T05:18:14.196614Z","shell.execute_reply.started":"2021-05-31T05:18:14.191301Z","shell.execute_reply":"2021-05-31T05:18:14.195603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result = pd.DataFrame({'train_loss': loss_plot_num, 'val_loss': val_plot_num})","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:01:27.813829Z","iopub.execute_input":"2021-05-31T08:01:27.81418Z","iopub.status.idle":"2021-05-31T08:01:27.830575Z","shell.execute_reply.started":"2021-05-31T08:01:27.814123Z","shell.execute_reply":"2021-05-31T08:01:27.829446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:01:29.621518Z","iopub.execute_input":"2021-05-31T08:01:29.621843Z","iopub.status.idle":"2021-05-31T08:01:29.657651Z","shell.execute_reply.started":"2021-05-31T08:01:29.621812Z","shell.execute_reply":"2021-05-31T08:01:29.656786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result.to_csv('loss_result.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:01:33.171671Z","iopub.execute_input":"2021-05-31T08:01:33.172022Z","iopub.status.idle":"2021-05-31T08:01:34.209297Z","shell.execute_reply.started":"2021-05-31T08:01:33.171989Z","shell.execute_reply":"2021-05-31T08:01:34.208442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('loss_result.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:01:35.548087Z","iopub.execute_input":"2021-05-31T08:01:35.54846Z","iopub.status.idle":"2021-05-31T08:01:35.5543Z","shell.execute_reply.started":"2021-05-31T08:01:35.548428Z","shell.execute_reply":"2021-05-31T08:01:35.553224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(train_dataloader, 'train_dataloader.pth')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:03:16.993676Z","iopub.execute_input":"2021-05-31T08:03:16.994083Z","iopub.status.idle":"2021-05-31T08:03:18.617002Z","shell.execute_reply.started":"2021-05-31T08:03:16.994038Z","shell.execute_reply":"2021-05-31T08:03:18.616184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(val_dataloader, 'val_dataloader.pth')\ntorch.save(test_dataloader, 'test_dataloader.pth')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:04:24.164641Z","iopub.execute_input":"2021-05-31T08:04:24.164966Z","iopub.status.idle":"2021-05-31T08:04:24.545044Z","shell.execute_reply.started":"2021-05-31T08:04:24.164928Z","shell.execute_reply":"2021-05-31T08:04:24.54418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('train_dataloader.pth')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:03:35.32438Z","iopub.execute_input":"2021-05-31T08:03:35.324707Z","iopub.status.idle":"2021-05-31T08:03:35.329651Z","shell.execute_reply.started":"2021-05-31T08:03:35.324675Z","shell.execute_reply":"2021-05-31T08:03:35.328879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('val_dataloader.pth')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:04:28.561687Z","iopub.execute_input":"2021-05-31T08:04:28.562007Z","iopub.status.idle":"2021-05-31T08:04:28.570634Z","shell.execute_reply.started":"2021-05-31T08:04:28.561974Z","shell.execute_reply":"2021-05-31T08:04:28.569856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink('test_dataloader.pth')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:04:37.022292Z","iopub.execute_input":"2021-05-31T08:04:37.022616Z","iopub.status.idle":"2021-05-31T08:04:37.031197Z","shell.execute_reply.started":"2021-05-31T08:04:37.022577Z","shell.execute_reply":"2021-05-31T08:04:37.030276Z"},"trusted":true},"execution_count":null,"outputs":[]}]}