{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"### In this following notebook I will walk you through steps needed to generate name from given dataset. \n#### I used [Anime names and Images](https://www.kaggle.com/shanmukh05/anime-names-and-image-generation) dataset for this task.\n*   This is basically a RNN network, that takes input a list of encoded words and outputs a letter.We can get any length of Anime name you want.\n\n## **Let's dive in**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"path = \"../input/anime-names-and-image-generation/final_names.csv\"\n\nnames_df = pd.read_csv(path)\nnames_ls = list(names_df[\"0\"])\nprint(\"Number of names: \",len(names_ls))\n\nnames_df.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{}},{"cell_type":"markdown","source":"**In this below cell, I have done two things**\n* Converting every anime name into id's\n* Converting the id's back to name\nAbove two steps are done using `tf.keras.layers.experimental.preprocessing.StringLookup`","metadata":{}},{"cell_type":"code","source":"text = \"\"\nfor name in names_ls:\n    text+=name\n    \nunique_chars = list(set(text))\nnum_chars = len(unique_chars)\nprint(\"Number of unique characters: \",num_chars)\n\nchar_to_id = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_chars)\nid_to_char = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=char_to_id.get_vocabulary(), invert=True)\n\nchar_ls = tf.strings.unicode_split(names_ls,input_encoding=\"UTF-8\")\nname_byte_ls = list(tf.strings.reduce_join(char_ls, axis=-1))\nid_ls = list(char_to_id(char_ls))\nall_ids = char_to_id(tf.strings.unicode_split(text, 'UTF-8'))\n\nprint(\"Example name: \",name_byte_ls[0])\nprint(\"Split: \",char_ls[0])\nprint(\"It's encoding: \",id_ls[0])\n\nprint(\"Number of chars in all names: \",tf.shape(all_ids)[0])\n\nlen(char_to_id.get_vocabulary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Padding sequences","metadata":{}},{"cell_type":"code","source":"padded_id = tf.keras.preprocessing.sequence.pad_sequences(id_ls,padding = \"post\")\nprint(\"Padded ids shape: \",padded_id.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"code","source":"seq_length = padded_id.shape[1]\nBATCH_SIZE = 64\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nSHUFFLE = 60000\nids_dataset = tf.data.Dataset.from_tensor_slices(padded_id)\n\ndef io_split(id_arr):\n    inp = id_arr[:-1]\n    out = id_arr[1:]\n    return inp,out\nids_dataset = ids_dataset.map(io_split)\nids_dataset = ids_dataset.shuffle(SHUFFLE).batch(BATCH_SIZE,drop_remainder=True).prefetch(AUTOTUNE)\n\nids_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(char_to_id.get_vocabulary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Model ","metadata":{}},{"cell_type":"code","source":"rnn_units = 1024\nembedding_dim = 256\n\nclass GenerateModel(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, rnn_units):\n        super().__init__(self)\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(rnn_units,\n                                   return_sequences=True, \n                                   return_state=True)\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, inputs, states=None, return_state=False, training=False):\n        x = inputs\n        x = self.embedding(x, training=training)\n        if states is None:\n            states = self.gru.get_initial_state(x)\n        x, states = self.gru(x, initial_state=states, training=training)\n        x = self.dense(x, training=training)\n\n        if return_state:\n            return x, states\n        else: \n            return x\n        \nmodel = GenerateModel(vocab_size=len(char_to_id.get_vocabulary()),embedding_dim=embedding_dim,rnn_units=rnn_units)\n\n\nfor input_batch, target_batch in ids_dataset.take(1):\n    batch_pred = model(input_batch)\n    print(batch_pred.shape)\n    \nprint(\"MODEL SUMMARY\")\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer='adam', loss=loss)\n\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\nhistory = model.fit(ids_dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GENERATING NAME","metadata":{}},{"cell_type":"markdown","source":"### Below `OutputStep()` Model will generate a letter in each call.\n* I wll append the generated letter to a empty string till required length of name is reached","metadata":{}},{"cell_type":"code","source":"class OutputStep(tf.keras.Model):\n    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n        super().__init__()\n        self.temperature=temperature\n        self.model = model\n        self.id_to_char = id_to_char\n        self.char_to_id = char_to_id\n\n    \n        skip_ids = self.char_to_id(['','?',\"#\",\"-\",'[UNK]'])[:, None]\n        sparse_mask = tf.SparseTensor(\n            values=[-float('inf')]*len(skip_ids),\n            indices = skip_ids,\n            dense_shape=[len(char_to_id.get_vocabulary())]) \n        sparse_mask = tf.sparse.reorder(sparse_mask)\n        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n\n    @tf.function\n    def generate_one_step(self, inputs, states=None):\n        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n        input_ids = self.char_to_id(input_chars).to_tensor()\n\n    \n        predicted_logits, states =  self.model(inputs=input_ids, states=states, \n                                          return_state=True)\n   \n        predicted_logits = predicted_logits[:, -1, :]\n        predicted_logits = predicted_logits/self.temperature\n  \n        predicted_logits = predicted_logits + self.prediction_mask\n\n    \n        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n\n\n        predicted_chars = self.id_to_char(predicted_ids)\n\n        return predicted_chars, states\n    \noutput_model = OutputStep(model, id_to_char, char_to_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Anime Name","metadata":{}},{"cell_type":"markdown","source":"1. Give `req_len` and `start_letter` as input and get final anime name as output","metadata":{}},{"cell_type":"code","source":"req_len = 15\nstart_letter = \"A\"\n\nstates = None\nnext_char = tf.constant([start_letter])\nresult = [next_char]\n\nfor n in range(req_len):\n    next_char, states = output_model.generate_one_step(next_char, states=states)\n    result.append(next_char)\n\nresult = tf.strings.join(result)\nprint(result[0].numpy().decode('utf-8'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How is the generated Anime NameüòÅ.\n### Hope you got some valuable learning from this notebook.\n### Happy Coding‚ù§.","metadata":{}},{"cell_type":"markdown","source":"[](http://)","metadata":{}}]}