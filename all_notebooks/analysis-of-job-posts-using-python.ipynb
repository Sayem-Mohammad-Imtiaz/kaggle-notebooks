{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import necessary libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport spacy\nfrom collections import Counter\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nLet's load the data and look at few entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/data-analyst-jobs/DataAnalyst.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the column 'Unnamed: 0'\ndel df[\"Unnamed: 0\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of our dataframe is : {}\".format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are the types of different features present in the dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except 'Rating' and 'Founded' all other features are defined as 'object' type. Let's find out how many features have null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only the column 'Company Name' has one missing value. But when we look at the data closely, we find that there are some entries like '-1',-1, 'Unknown' in the dataframe. These values are equivalent to NaNs. An easy way to find this out is to look at the unique elements in each column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of unique elements in each column\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at unique values in some of these columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Size'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we replace all these above mentioned entries by NaNs."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.replace([-1.,-1, '-1', 'Unknown', 'Unknown / Non-Applicable'], np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check our dataframe after the replacement\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After this replacement, we check again the number of null values in each of the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate % of null value in each column and sort them in descending order\nnull_percentage = df.isnull().sum().sort_values(ascending=False)/len(df)*100\n\n# make a plot\nnull_percentage[null_percentage>0.1].plot(kind='bar', figsize=(10,8))\nplt.xlabel(\"Features\")\nplt.ylabel(\"% of null values\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the above plot, most of the entries in 'Easy Apply' and 'Competitors' are missing. Therefore, we will drop these two columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Easy Apply', 'Competitors'], axis=1, inplace=True)\nprint(\"The shape of the dataframe after dropping features : {}\".format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now calculate null value percentage in each row of the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate % of null value in each row and sort them in descending order\nrow_null_percentage = df.isnull().sum(axis=1).sort_values(ascending=False)/len(df)*100\nrow_null_percentage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the row has significant amount of null values and therefore no row can be dropped on the basis of null value percentage. \n\nThe first question that one can ask is that **which industries are hiring data analysts actively at present?** "},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 30 industries hiring data analysts\ndf['Industry'].value_counts()[:30].plot(kind='bar', figsize=(14,8))\nplt.xlabel(\"Industry\")\nplt.ylabel(\"Number of job posts\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the data analyst job posts are in the IT and Staffing & Outsourcing sectors. Current data analyst post vacancy in the Health Care sector is almost half of these two.\n\nThe next interesting question could be **What is the distribution of salary across all the industries?** Let's look at the column 'Salary Estimate'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# first 10 entries in 'Salary Estimate'\ndf['Salary Estimate'][:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to clean this. We create three new features from this column - minimum, maximum and average salaries offered in different industries and then drop the original feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# the minimum salary offered\ndf['min_salary'] = df['Salary Estimate'].apply(lambda x:float(x.split()[0].split(\"-\")[0].strip(\"$,K\")) \n                                                          if not pd.isnull(x) else x)\n\n# the maximum salary offered\ndf['max_salary'] = df['Salary Estimate'].apply(lambda x:float(x.split()[0].split(\"-\")[1].strip(\"$,K\")) \n                                                          if not pd.isnull(x) else x)\n\n# the average salary offered\ndf['avg_salary'] = (df['min_salary'] + df['max_salary'])/2.\n\n# drop the original column\ndf.drop('Salary Estimate', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can look at the distribution of average salary in different industries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot top 30 industries with highest offered salaries\ndf.groupby('Industry')['avg_salary'].mean().sort_values(ascending=False)[:30].plot(kind='bar', figsize=(14,10))\nplt.xlabel('Industry')\nplt.ylabel('Average salary');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is interesting! Although IT sector has maximum number of job postings, the salary offered does not fall into top 30. The healthcare industry ('Drug & Health Stores', 'Health Care Products Manufacturing', 'Biotech & Pharmaceuticals', 'Health Care Services & Hospitals') generally has a high average salary. The average salary offered in education industry is almost comparable to that of healthcare industry. This plot tells us that even if some industries have fewer job postings, they offer better salaries.\n\n**Is there any particular location with high demand of data analysts?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the 'Location' column\ndf['Location']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's extract only the state names from the 'Location' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Job_state'] = df['Location'].apply(lambda x:x.split(\",\")[-1].strip())\ndf['Job_state']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many unique values?\ndf['Job_state'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot total number of job posting in each state\ndf['Job_state'].value_counts().plot(kind=\"bar\", figsize=(14,8))\nplt.xlabel(\"Job Location\")\nplt.ylabel(\"Number of job posts\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the vacancies are in CA followed by TX and NY which have almost equal number of vacancies in the data analyst post.\n\n**Which companies are hiring data analysts actively?** Let's look at the column 'Company Name'."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Company Name'].unique()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract only the company name\ndf['Company Name'] = df['Company Name'].apply(lambda x:x.split(\"\\n\")[0].strip() if not pd.isnull(x) else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot top 30 companies with high job postings\ndf['Company Name'].value_counts()[:30].plot(kind='bar', figsize=(14,10))\nplt.xlabel('Company')\nplt.ylabel('Number of job posts');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the above plot, 'Staffigo Technical Services' has a large number of openings for data analysts.\n\n**Which companies are offering high salaries?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot top 30 companies with high average salaries\ndf.groupby('Company Name')['avg_salary'].mean().sort_values(ascending=False)[:30].plot(kind='bar', figsize=(14,10))\nplt.xlabel('Company')\nplt.ylabel('Average salary');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's study these companies closely."},{"metadata":{"trusted":true},"cell_type":"code","source":"# store top 30 companies offering high salaries in a list\ntop_30_comps = list(df.groupby('Company Name')['avg_salary'].mean().sort_values(ascending=False)[:30].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How are the ratings of these companies?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The rating is given on a scale {}-{}.\".format(df['Rating'].min(), df['Rating'].max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot ratings of these companies\nplt.figure(figsize=(14,8))\nsns.barplot(x=df[df['Company Name'].isin(top_30_comps)]['Company Name'], \n            y=df[df['Company Name'].isin(top_30_comps)]['Rating'],\n            order = top_30_comps)\nplt.xlabel('Company')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the companies (except three) have a minimum rating value 3. 'Xcutives.com Inc', 'Applicantz, Inc' and 'Parsoft LLC' - although these three companies are offering high salaries to candidates, no rating is available for them. One reason might be that these are comparatively new companies.\n\n**What about number of currently employed staffs in these companies?** The relevant feature for this question is 'Size'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique values in the feature 'Size'\ndf['Size'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Employee size of top 30 high paying companies\nplt.figure(figsize=(10,8))\nsns.countplot(df[df['Company Name'].isin(top_30_comps)]['Size'])   \nplt.xlabel('Employee Size')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprisingly, most of the high paying companies are small in size. These are likely to be start-ups. So the data is telling us that start-ups are likely to pay more to data analysts as compared to big companies.\n\nAnother important question to explore could be **what are the financial status of these companies?** To find an answer to this, we look at the available feature 'Revenue'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique values in 'Revenue'\ndf['Revenue'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot revenues of top 30 high paying companies\nplt.figure(figsize=(10,8))\nsns.countplot(df[df['Company Name'].isin(top_30_comps)]['Revenue'])    \nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot reiterates the same thing - most of the data analyst job positions are available in small to medium sized companies.\n\nOne more question in this chain - **where are these companies located?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot locations of these top 30 companies\nplt.figure(figsize=(10,8))\nsns.countplot(df[df['Company Name'].isin(top_30_comps)]['Location'])    \nplt.xlabel(\"Company Location\")\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All these companies are situated in CA (mainly in the city San Francisco)! CA not only has a large number of openings for data analysts, it also offers high salaries as compared to other states. All the information that we gathered from this exercise can be summarized as follows:\n\n**Companies offering high salaries to data analysts are small in size with average total revenue, generally have decent ratings and are mostly situated in CA.** \n\nSo, if you are looking for a career in data analytics, CA might be you next destination!"},{"metadata":{},"cell_type":"markdown","source":"To get a job, one must have the required skill sets. **What skills are companies looking for in a data analyst?** The feature of interest in this case is 'Job Description'."},{"metadata":{},"cell_type":"markdown","source":"Let's find out average lengths of available job descriptions in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_len = [len(desc) for desc in df['Job Description']]\nplt.figure(figsize=(14,8))\nplt.xlabel('Job descripiton length')\nplt.hist(desc_len, bins=80, range=(0,4000));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, in general, job descriptions are quite long. But our interest is in some keywords like 'C++', 'pyhton', 'sql' etc that are closely related to required skills of an applicant for the post of data analyst. We use the library 'spaCy' to extract these keywords from a huge corpus. The steps are as follows:\n\n- We look for all the named entities in the corpus.\n- A quick check shows that most of our keywords (skills) of interest are labeled as 'ORG'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the required libraries and create an nlp object\nnlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list to store extracted skill keywords\nskill_list = []\n\n# feed the entire corpus into batches of 100 samples at a time\nfor i in range(0,len(df), 100):\n    # for the last batch\n    if i+np.mod(2253,100)==len(df):\n        # combine job descriptions of 100 samples into a single string\n        text = \" \".join(des for des in df['Job Description'][i:len(df)])\n    else :\n        text = \" \".join(des for des in df['Job Description'][i:i+100])\n        \n    # process raw text with the nlp object that holds all information about the tokens, their linguistic \n    #features and relationships    \n    doc = nlp(text)\n\n    # loop over the named entities\n    for entity in set(doc.ents):\n        # select entities with label 'ORG'\n        if entity.label_ == 'ORG':\n            # add to the list\n            skill_list.append(entity.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count how many times each entity appears in the list\nword_count = Counter(skill_list)\n# print the top 100 named entities\nword_count.most_common(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have been successful in extracting skills like SQL, Python, ETL etc from the corpus. There are also some unrelated words/phrases like healthcare, data governance etc but overall spaCy has done a good job in extracting relevant skills. The most prefered skill is SQL!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a list of actual skills extracted from the corpus\nskill_set = ['SQL', 'Python', 'ETL', 'SAS', 'SAP', 'Oracle', 'PowerPoint', 'AWS', 'Microsoft Office',\n             'XML', 'PL/SQL', 'AI', 'Spark', 'MS Office', 'ERP', 'Big Data',  'Tableau', 'Hadoop', \n             'JavaScript', 'Azure', 'Perl']\n\n# loop over top 100 extracted skill keywords/phrases\n# select skills present in the above list\n# add to a dictionary    \nskill_count_dict = {skill:count for skill, count in word_count.most_common(100) if skill in skill_set}        \n            \n# SQL and SQL server basically point to the same thing. Let's combine them into a single key            \nskill_count_dict['SQL'] = skill_count_dict['SQL'] + skill_count_dict['PL/SQL']\n\n# remove the other key\ndel skill_count_dict['PL/SQL']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataframe with two columns - skills and corresponding counts\nskill_count_df = pd.DataFrame(skill_count_dict.items(), columns=['Skill', 'Total Count'])\nskill_count_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot how many times a skill appeared in the corpus\nskill_count_df.groupby('Skill')['Total Count'].max().plot(kind='bar', figsize=(14,8))\nplt.xlabel(\"Required skills\")\nplt.ylabel(\"Total count\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}