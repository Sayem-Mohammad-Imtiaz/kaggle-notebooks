{"cells":[{"metadata":{"_uuid":"0f7a1581faf91cc8afd920451060d8e8b96e8d1e"},"cell_type":"markdown","source":"# Analysis of Churn Data of a Telecom company #"},{"metadata":{"toc":true,"_uuid":"e0194f8a5d771528cabe0fef33b79a21a3040b92"},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Exploration\" data-toc-modified-id=\"Exploration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploration</a></span></li><li><span><a href=\"#Principal-component-analysis\" data-toc-modified-id=\"Principal-component-analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Principal component analysis</a></span></li><li><span><a href=\"#Comparison-of-differently-preprocessed-datasets-for-classification\" data-toc-modified-id=\"Comparison-of-differently-preprocessed-datasets-for-classification-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Comparison of differently preprocessed datasets for classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Without-additional-balancing-techniques\" data-toc-modified-id=\"Without-additional-balancing-techniques-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Without additional balancing techniques</a></span></li><li><span><a href=\"#Using-class-weights-in-the-loss-function\" data-toc-modified-id=\"Using-class-weights-in-the-loss-function-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Using class weights in the loss function</a></span></li><li><span><a href=\"#Using-Synthetic-Minority-Over-sampling-Technique\" data-toc-modified-id=\"Using-Synthetic-Minority-Over-sampling-Technique-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Using Synthetic Minority Over-sampling Technique</a></span></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Comparison</a></span></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#K-Nearest-Neighbors\" data-toc-modified-id=\"K-Nearest-Neighbors-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>K-Nearest Neighbors</a></span></li><li><span><a href=\"#Support-Vector-Machine\" data-toc-modified-id=\"Support-Vector-Machine-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Support-Vector Machine</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Comparison</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"},{"metadata":{"_uuid":"f056012aa3c50c9410ce66da2a564fc1ba18c9c8"},"cell_type":"markdown","source":"## Introduction ##"},{"metadata":{"_uuid":"662d51f0a916c5d69c69acef38006c023255879e"},"cell_type":"markdown","source":"In this report, a dataset on churn data of a Telecom company is analysed. It can be found here: https://www.kaggle.com/becksddf/churn-in-telecoms-dataset."},{"metadata":{"_uuid":"25c7ce25624081299e006e227775e023b301897e"},"cell_type":"markdown","source":"The dataset contains data on the customers of a Telecom company.\nEach row represents a customer and the columns contain customer’s attributes which are described in the following:\n- state: the state the user lives in\n- account length: the number of days the user has this account\n- area code: the code of the area the user lives in\n- phone number: the phone number of the user\n- international plan: true if the user has the international plan, otherwise false\n- voice mail plan: true if the user has the voice mail plan, otherwise false\n- number vmail messages: the number of voice mail messages the user has sent\n- total day minutes: total number of minutes the user has been in calls during the day\n- total day calls: total number of calls the user has done during the day\n- total day charge: total amount of money the user was charged by the Telecom company for calls during the day\n- total eve minutes: total number of minutes the user has been in calls during the evening\n- total eve calls: total number of calls the user has done during the evening\n- total eve charge: total amount of money the user was charged by the Telecom company for calls during the evening\n- total night minutes: total number of minutes the user has been in calls during the night\n- total night calls: total number of calls the user has done during the night\n- total night charge: total amount of money the user was charged by the Telecom company for calls during the night\n- total intl minutes: total number of minutes the user has been in international calls\n- total intl calls: total number of international calls the user has done\n- total intl charge: total amount of money the user was charged by the Telecom company for international calls\n- customer service calls: number of customer service calls the user has done\n- churn: true if the user terminated the contract, otherwise false"},{"metadata":{"_uuid":"4054fd8f8cdacfaf37ae3152c4be2ad896dc73da"},"cell_type":"markdown","source":"Customer churn is the loss of clients or customers. Predicting churn can help the Telecom company, so it can effectively focus a customer retention marketing program (e.g. a special offer) to the subset of clients which are most likely to change their carrier. Therefore, the “churn” column is chosen as target and the following predictive analysis is a supervised classification problem."},{"metadata":{"_uuid":"9825b9f24c5f81b7cbff8f8328847edfa3fefdbe"},"cell_type":"markdown","source":"The Analysis was conducted in Python 3.7.0 using Jupyter Notebook which is a web application that allows you to create an interactive environment that contains live code, visualizations and text.\nIn addition, the following packages were used:\n- pandas: pandas provides high-performance data structures and operations for manipulating numerical tables and time series.\n- numpy: NumPy provides scientific computing capabilities such as a powerful N-dimensional array object, linear algebra, and random number capabilities.\n- sklearn: scikit-learn provides tools for data mining and data analysis.\n- imblearn: imbalanced-learn provides a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance.\n- plotly: Plot.ly is a graphing library which can produce interactive graphs."},{"metadata":{"trusted":true,"_uuid":"2fb3d9242b00009c7f177cb50061c000a717a6f0"},"cell_type":"code","source":"# scientific computing libaries\nimport pandas as pd\nimport numpy as np\n\n# data mining libaries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA#, FastICA\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n\nfrom imblearn.pipeline import make_pipeline, Pipeline\nfrom imblearn.over_sampling import SMOTE\n\n#plot libaries\nimport plotly\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True) # to show plots in notebook\n\n# online plotly\n#from plotly.plotly import plot, iplot\n#plotly.tools.set_credentials_file(username='XXXXXXXXXXXXXXX', api_key='XXXXXXXXXXXXXXX')\n\n# offline plotly\nfrom plotly.offline import plot, iplot\n\n# do not show any warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 17 # specify seed for reproducable results\npd.set_option('display.max_columns', None) # prevents abbreviation (with '...') of columns in prints","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"trusted":true,"_uuid":"beb0b99224dbc723ae55b85e7d60feddbbfc108b"},"cell_type":"code","source":"RANDOM_FOREST_PARAMS = {\n    'clf__max_depth': [25, 50, 75],\n    'clf__max_features': [\"sqrt\"], # just sqrt is used because values of log2 and sqrt are very similar for our number of features (10-19) \n    'clf__criterion': ['gini', 'entropy'],\n    'clf__n_estimators': [100, 300, 500, 1000]\n}\n\nDECISION_TREE_PARAMS = {\n    'clf__max_depth': [25, 50, 75],\n    'clf__max_features': [\"sqrt\"], # just sqrt is used because values of log2 and sqrt are very similar for our number of features (10-19)\n    'clf__criterion': ['gini', 'entropy'],\n    'clf__min_samples_split': [6, 10, 14],\n}\n\nLOGISTIC_REGRESSION_PARAMS = {\n    'clf__solver': ['liblinear'],\n    'clf__C': [0.1, 1, 10],\n    'clf__penalty': ['l2', 'l1']\n}\n\nKNN_PARAMS = {\n    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n    'clf__weights': ['uniform', 'distance'],\n    'clf__p': [1, 2, 10]\n}\n\nKNN_PARAMS_UNIFORM = {\n    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n    'clf__weights': ['uniform'],\n    'clf__p': [1, 2, 10]\n}\n\nSVM_PARAMS = [\n{\n    'clf__kernel': ['linear'],\n    'clf__C': [0.1, 1, 10],\n}, \n{\n    'clf__kernel': ['rbf'],\n    'clf__C': [0.01, 0.1, 1, 10, 100],\n    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n}]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01927489eb08164b3ad594cfcf31fbb91b9fe407"},"cell_type":"markdown","source":"## Exploration ##"},{"metadata":{"trusted":true,"_uuid":"b425cc23fee0a495430e3a26f3f515a4b5936995"},"cell_type":"code","source":"# load the dataset\ndf = pd.read_csv('../input/bigml_59c28831336c6604c800002a.csv')\n\nprint(\"The dataset has %d rows and %d columns.\" % df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52c2d1df87bfadd2d77f5a6bc648bfc49936a02b"},"cell_type":"code","source":"# check for null values in the dataset\nprint(\"There are \" + (\"some\" if df.isnull().values.any() else \"no\")  + \" null/missing values in the dataset.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0407f17e9adc9a3d2abfa1982dc4c97abfc798e"},"cell_type":"markdown","source":"First, we take a look at some data points to get a feeling what the values of the various columns look like."},{"metadata":{"trusted":true,"_uuid":"89152b940a2ddf2bef93076abb1ea9a2f9f02623"},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e9ae09a86a5c873d38364ee58a83ae0877957f5"},"cell_type":"markdown","source":"We can see that the columns \"state\", \"international plan\", \"voice mail plan\" and \"churn\" have String values. The latter three seem to have just the values \"yes\" or \"no\" and are therefore converted to 1 and 0 respectively.\n\nThe \"state\" column is converted using the LabelEncoder, which replaces each unique label with a unique integer.\nIn this case, a label encode is used instead of dummy variables because of the many distinct values, which when converted into dummy variables would mess up the for example the PCA and the feature importance of the tree-based models.\n\nThe \"phone number\" column is removed, because every customer has its own phone number."},{"metadata":{"trusted":true,"_uuid":"5bcf0d1c60eabeedec21206ce10bda8716d1b413"},"cell_type":"code","source":"def preprocess_data(df):\n    pre_df = df.copy()\n    \n    # Replace the spaces in the column names with underscores\n    pre_df.columns = [s.replace(\" \", \"_\") for s in pre_df.columns]\n    \n    # convert string columns to integers\n    pre_df[\"international_plan\"] = pre_df[\"international_plan\"].apply(lambda x: 0 if x==\"no\" else 1)\n    pre_df[\"voice_mail_plan\"] = pre_df[\"voice_mail_plan\"].apply(lambda x: 0 if x==\"no\" else 1)\n    pre_df = pre_df.drop([\"phone_number\"], axis=1)\n    le = LabelEncoder()\n    le.fit(pre_df['state'])\n    pre_df['state'] = le.transform(pre_df['state'])\n    \n    return pre_df, le","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"678c4f5d7e752403077092a1febb58f0cbb66c1b"},"cell_type":"code","source":"pre_df, _ = preprocess_data(df)\npre_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aa1351e3efcc6ffc69223b7ce6989a392e5d51d"},"cell_type":"markdown","source":"**Statistical overview of the data**\n\nThe following statistical measures can be seen for each column using the describe-function of DataFrame of the pandas library:\n- count: number of samples\n- mean: the mean of this attribute among all samples\n- std: the standard deviation of this attribute\n- min: the minimal value of this attribute\n- 25%: the lower percentile\n- 50%: the median\n- 75%: the upper percentile\n- max: the maximal value of this attribute"},{"metadata":{"trusted":true,"_uuid":"87b4d805c8a5399dcd390c24b2416dc5519b7808"},"cell_type":"code","source":"pre_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8d24585fd31dc1a3a4a4eba2f1d66d8539a343e"},"cell_type":"markdown","source":"These numbers are hard to interpret in this format, so we create some graphs which visualize them in a better way. First, we look at the distribution of the our target variable:"},{"metadata":{"trusted":true,"_uuid":"05cdb923dfd5e0b7fe1d20838dc0d328e4f5a153"},"cell_type":"code","source":"colors = plotly.colors.DEFAULT_PLOTLY_COLORS\nchurn_dict = {0: \"no churn\", 1: \"churn\"}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e91d8ab01770a8cd8a1a6c8452e100fd7e3789b0"},"cell_type":"code","source":"y = df[\"churn\"].value_counts()\n\ndata = [go.Bar(x=[churn_dict[x] for x in y.index], y=y.values, marker = dict(color = colors[:len(y.index)]))]\nlayout = go.Layout(\n    title='Churn distribution',\n    autosize=False,\n    width=400,\n    height=400,\n    yaxis=dict(\n        title='#samples',\n    ),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='basic-bar15')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bf0e580ef6cb4f1fcc5c47644915e77bf23bf47"},"cell_type":"code","source":"churn_perc = df[\"churn\"].sum() * 100 / df[\"churn\"].shape[0]\nprint(\"Churn percentage is %.3f%%.\" % churn_perc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7126a87877dd5a0f6324a821bf8ea2b0e2ebd9e"},"cell_type":"markdown","source":"We can see that we have clearly more samples for customers without churn than for customers with churn. So we have a class imbalance for the target variable which could lead to predictive models which are biased towards the majority (i.e. no churn). In order to deal with this issue we will investigate into the use of oversampling when building the models.\n\nNext, we look at the churn distribution per state, to see how much the state influences our target:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"258d6ba2b7a6de86274a94e67f53a55b6caf3314"},"cell_type":"code","source":"state_churn_df = df.groupby([\"state\", \"churn\"]).size().unstack()\ntrace1 = go.Bar(\n    x=state_churn_df.index,\n    y=state_churn_df[0],\n    marker = dict(color = colors[0]),\n    name='no churn'\n)\ntrace2 = go.Bar(\n    x=state_churn_df.index,\n    y=state_churn_df[1],\n    marker = dict(color = colors[1]),\n    name='churn'\n)\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title='Churn distribution per state',\n    autosize=True,\n    barmode='stack',\n    margin=go.layout.Margin(l=50, r=50),\n    xaxis=dict(\n        title='state',\n        tickangle=45\n    ),\n    yaxis=dict(\n        title='#samples',\n        automargin=True,\n    ),\n    legend=dict(\n        x=0,\n        y=1,\n    ),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='stacked-bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37db2471c9f7638e38c297fd1802d46b38a987b0"},"cell_type":"markdown","source":"We can see that some states have less proportion of customer with churn like AK, HI, IA and some have a higher proportion such as WA, MD and TX. This shows that we should incorporate the state into our further analysis, because it could be help to predict if a customer is going to churn."},{"metadata":{"_uuid":"25ba51c3899a5136e32f085f5c29c0a50029e059"},"cell_type":"markdown","source":"The following interactive graph shows the distribution of each feature for customer with churn and for the ones without churn. The slider can be used to switch between the different features."},{"metadata":{"trusted":true,"_uuid":"3b738a378152a259948b7a13e5ed7e5ee9db1f68"},"cell_type":"code","source":"churn = pre_df[pre_df[\"churn\"] == 1]\nno_churn = pre_df[pre_df[\"churn\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71a31747a497305bcb37bf54cf06ffce991882ba"},"cell_type":"code","source":"def create_churn_trace(col, visible=False):\n    return go.Histogram(\n        x=churn[col],\n        name='churn',\n        marker = dict(color = colors[1]),\n        visible=visible,\n    )\n\ndef create_no_churn_trace(col, visible=False):\n    return go.Histogram(\n        x=no_churn[col],\n        name='no churn',\n        marker = dict(color = colors[0]),\n        visible = visible,\n    )\n\nfeatures_not_for_hist = [\"state\", \"phone_number\", \"churn\"]\nfeatures_for_hist = [x for x in pre_df.columns if x not in features_not_for_hist]\nactive_idx = 0\ntraces_churn = [(create_churn_trace(col) if i != active_idx else create_churn_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\ntraces_no_churn = [(create_no_churn_trace(col) if i != active_idx else create_no_churn_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\ndata = traces_churn + traces_no_churn\n\nn_features = len(features_for_hist)\nsteps = []\nfor i in range(n_features):\n    step = dict(\n        method = 'restyle',  \n        args = ['visible', [False] * len(data)],\n        label = features_for_hist[i],\n    )\n    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [dict(\n    active = active_idx,\n    currentvalue = dict(\n        prefix = \"Feature: \", \n        xanchor= 'center',\n    ),\n    pad = {\"t\": 50},\n    steps = steps,\n)]\n\nlayout = dict(\n    sliders=sliders,\n    yaxis=dict(\n        title='#samples',\n        automargin=True,\n    ),\n)\n\nfig = dict(data=data, layout=layout)\n\niplot(fig, filename='histogram_slider')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cba0b5c59fa924fd263ef6a9ef5f0b43315f2397"},"cell_type":"markdown","source":"One interesting histogram is of the feature \"international_plan\". While the proportion of churn for customers which have the international plan is much higher than the proportion of churn for customers without.\n\nThe histograms for the \"total_day_minutes\" and \"total_day_charge\" are very similar and we can see that the customer with a higher value for these two features are more likely to churn. Interestingly, this does not apply to the number of day calls, which means that these customers seem to do longer calls. The minutes, charge and #calls for other times of the day (i.e. evening, night) do not show different distributions for customers with churn and without churn.\n\nAnother interesting pattern is shown by the \"total_intl_calls\" feature. The data for the customers with churn are more left skewed than the data of the customers of the customer who did not churn."},{"metadata":{"_uuid":"637430444f50b8617badf8dcc3aab577742a2708"},"cell_type":"markdown","source":"Next, we take a look at the box plots for each feature. A box plot visualizes the following statistics:\n- median\n- the first quartile (Q1) and the third quartile (Q3) building the interquartile range (IQR)\n- the lower fence (Q1 - 1.5 * IQR) and the upper fence (Q3 + 1.5 * IQR)\n- the maximum and the minimum value"},{"metadata":{"code_folding":[],"trusted":true,"_uuid":"339019eef8178145a4044283c14bd409dc477d0b"},"cell_type":"code","source":"def create_box_churn_trace(col, visible=False):\n    return go.Box(\n        y=churn[col],\n        name='churn',\n        marker = dict(color = colors[1]),\n        visible=visible,\n    )\n\ndef create_box_no_churn_trace(col, visible=False):\n    return go.Box(\n        y=no_churn[col],\n        name='no churn',\n        marker = dict(color = colors[0]),\n        visible = visible,\n    )\n\nfeatures_not_for_hist = [\"state\", \"phone_number\", \"churn\"]\nfeatures_for_hist = [x for x in pre_df.columns if x not in features_not_for_hist]\n# remove features with too less distinct values (e.g. binary features), because boxplot does not make any sense for them\nfeatures_for_box = [col for col in features_for_hist if len(churn[col].unique())>5]\n\nactive_idx = 0\nbox_traces_churn = [(create_box_churn_trace(col) if i != active_idx else create_box_churn_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\nbox_traces_no_churn = [(create_box_no_churn_trace(col) if i != active_idx else create_box_no_churn_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\ndata = box_traces_churn + box_traces_no_churn\n\nn_features = len(features_for_box)\nsteps = []\nfor i in range(n_features):\n    step = dict(\n        method = 'restyle',  \n        args = ['visible', [False] * len(data)],\n        label = features_for_box[i],\n    )\n    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [dict(\n    active = active_idx,\n    currentvalue = dict(\n        prefix = \"Feature: \", \n        xanchor= 'center',\n    ),\n    pad = {\"t\": 50},\n    steps = steps,\n    len=1,\n)]\n\nlayout = dict(\n    sliders=sliders,\n    yaxis=dict(\n        title='value',\n        automargin=True,\n    ),\n    legend=dict(\n        x=0,\n        y=1,\n    ),\n)\n\nfig = dict(data=data, layout=layout)\n\niplot(fig, filename='box_slider')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bbfaa12ee5eba25721fbde6b91da28766943013"},"cell_type":"markdown","source":"When we look at the box plot for the number of voice mail messages (\"number_vmail_messages\"), we can see that we have some outliers for the customers with churn, but most of them have send zero voice mail messages. The customers which did not churn instead tend to do more voice mail messages.<br/>\nSimilar to our findings in the histograms, we can see also in the box plot that the median of the total day minutes and the total day charge for churn clients is higher than the one of no-churn clients.<br/>\nLooking at the total international calls (\"total_intl_calls\"), the box plot shows that both churn and no-churn  customers are doing a similar amount of international calls, but the churn-customers tend to do longer calls as the median of churn customers for the total international minutes is higher than for the no-churn customers.<br/>\nFinally, the plot for the number of customer service calls shows that clients with churn have a higher median and a higher variance for the customer service calls."},{"metadata":{"_uuid":"1bd97a48a341fe1162e5376145aab2a188d4f47b"},"cell_type":"markdown","source":"In order to investigate the pair-wise correlations between two variables $X$ and $Y$, we use the Pearson correlation. Let $\\sigma_X, \\sigma_Y$ be the standard deviation of X,Y and $\\text{cov}(X, Y) = E[(X-E[X])(Y-E[Y])]$. Then we can define the Pearson correlation as the following:<br/>\n$\\rho_{X, Y} = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y}\\,$.\n\nTo visualize these correlations we use a heatmap plot, in which high correlations are coloured more to the red and lower ones more to the blue."},{"metadata":{"code_folding":[],"scrolled":false,"trusted":true,"_uuid":"62f321db2732b9c1f0223b3e97e10a3d9603d13b"},"cell_type":"code","source":"corr = pre_df.corr()\ntrace = go.Heatmap(z=corr.values.tolist(), x=corr.columns, y=corr.columns)\ndata=[trace]\nlayout = go.Layout(\n    title='Heatmap of pairwise correlation of the columns',\n    autosize=False,\n    width=850,\n    height=700,\n    yaxis=go.layout.YAxis(automargin=True),\n    xaxis=dict(tickangle=40),\n    margin=go.layout.Margin(l=0, r=200, b=200, t=80)\n)\n\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='labelled-heatmap1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e77a2f2097edba7cb8a85774da5dd6c054d9bbec"},"cell_type":"markdown","source":"We can see a high correlation between the voice mail plan and the number of voice mail messages. It makes sense that customers with the voice mail plan also send more voice mail messages.<br/>\nHowever, the international plan is just slightly correlated with the total international minutes and the international charge.<br/>\nAs seen also in our previous analysis, the total day charge and the total day minutes a very highly correlated. Probably, this Telecom company charges per minute. The same behavior can be seen for the evening, the night and the international calls.<br/>\nThe highest correlation with the churn variable have the international plan, the total_day_charge, the total_day_minutes and the number of customer service calls.\n\nIn order to reduce the dimensionality of our dataset, we can identify and remove duplicate features according to their pairwise correlation with others. For this, we conduct a clustering of the features using agglomerative hierarchical clustering with average linkage. This method starts by creating one cluster for each feature and by computing the pair-wise distance/dissimilarity/similarity between all the clusters, which in our case is the correlation. Then it select the two clusters with the highest average correlation to be merged. In the next iteration, the next pair of clusters is selected to be merged. This process is repeated until we end up with one cluster.\n\nTo visualize the clustering process a dendrogram is shown in the following:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"1e2baa7566a01a98a41f8ee1b6d2d45a7da5d399"},"cell_type":"code","source":"from scipy.cluster import hierarchy as hc\nX = np.random.rand(10, 10)\nnames = pre_df.columns\ninverse_correlation = 1 - abs(pre_df.corr())\nfig = ff.create_dendrogram(inverse_correlation.values, orientation='left', labels=names, colorscale=colors, linkagefun=lambda x: hc.linkage(x, 'average'))\nfig['layout'].update(dict(\n    title=\"Dendogram of clustering the features according to correlation\",\n    width=800, \n    height=600,\n    margin=go.layout.Margin(l=180, r=50),\n    xaxis=dict(\n        title='distance',\n    ),\n    yaxis=dict(\n        title='features',\n        automargin=True,\n    ),\n))\niplot(fig, filename='dendrogram_corr_clustering')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96f3deb0f9f3cd30c6b7557b0e035d28889ca7b6"},"cell_type":"markdown","source":"The four feature pairs which are each clustered together at a very low distance are:\n- total_night_minutes and total_night_charge\n- total_eve_minutes and total_eve_charge\n- total_intl_minutes and total_intl_charge\n- total_day_minutes and total_day_charge\n\nWe save the all the charge features as duplicate features to be removed for our reduced dataset."},{"metadata":{"trusted":true,"_uuid":"a0db276ec94b25702c0fc5052515d796724e2b0b"},"cell_type":"code","source":"# save the duplicate features for later usage\nduplicate_features = [\"total_day_charge\", \"total_eve_charge\", \"total_night_charge\", \"total_intl_charge\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01799f394d952bc329a47c8c738c40081b6ca1be"},"cell_type":"markdown","source":"To prepare the dataset for the following analysis we split it into the target column and the other predictors. In addition, we standardize all features to avoid e.g. higher impact of features with higher absolute values in classifiers which are based on a distance metric."},{"metadata":{"trusted":true,"_uuid":"8c8ed072fbc0becda29616ad5591fe27fe1564e5"},"cell_type":"code","source":"# splitting the dataset into feature vectors and the target variable\ndf_y = pre_df[\"churn\"]\ndf_X = pre_df.drop([\"churn\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d0c723ab415a0fa65075ed53e6ddfec97466cca"},"cell_type":"code","source":"# normalize the dataset (note: for decision tree/random forest it would not be needed)\ndf_X_normed = (df_X - df_X.mean()) / df_X.std()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58464a376f9e3e461c456b79f95c0334d33bdb5d"},"cell_type":"markdown","source":"## Principal component analysis ##"},{"metadata":{"_uuid":"b3128d6a3104cf5beb413ddbf0878824cfafa959"},"cell_type":"markdown","source":"Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert the variables of a dataset into a new set of variables which are linearly uncorrelated and called principal components. The principal components are ranked according to the variance of data along them. This technique can be used to reduce the dimensionality of the dataset by considering just the most important principal components."},{"metadata":{"_uuid":"d24ead641d6b58750991769b50b534e1d596ca65"},"cell_type":"markdown","source":"In order to reduce the dimensionality of a dataset $X$ with $n$ variables to a new dataset with $k$ variables using PCA the following steps have to be followed:\n* standardize the data\n* calculate the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n* sort the Eigenvectors according to their Eigenvalues in decreasing order\n* build the $n×k$-dimensional projection matrix $W$ by putting the top $k$ Eigenvectors into the columns of $W$\n* transform the dataset $X$ by multiplying it with $W$: $X_{t} = XW$"},{"metadata":{"_uuid":"c608e4f1d5e00f95a2d4b18a7123dcd208802401"},"cell_type":"markdown","source":"We apply PCA to the dataset in order to reduce the number of features."},{"metadata":{"trusted":true,"_uuid":"e8ed41a703686a1737477b55c468832761d776c0"},"cell_type":"code","source":"# calculate the principal components\npca = PCA(random_state=SEED)\ndf_X_pca = pca.fit_transform(df_X_normed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"336625522bb8fb439f181c68208dc326aeffe187"},"cell_type":"code","source":"tot = sum(pca.explained_variance_) # total explained variance of all principal components\nvar_exp = [(i / tot) * 100 for i in sorted(pca.explained_variance_, reverse=True)] # individual explained variance\ncum_var_exp = np.cumsum(var_exp) # cumulative explained variance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8249a3530e6f7a8b91a3cb5221d6d33a38a00634"},"cell_type":"markdown","source":"In the following we plot the Scree Plot to determine how many components we use."},{"metadata":{"trusted":true,"_uuid":"4b9861a7091fe8f8663c794f544d69363b7419d9"},"cell_type":"code","source":"trace_cum_var_exp = go.Bar(\n    x=list(range(1, len(cum_var_exp) + 1)), \n    y=var_exp,\n    name=\"individual explained variance\",\n)\ntrace_ind_var_exp = go.Scatter(\n    x=list(range(1, len(cum_var_exp) + 1)),\n    y=cum_var_exp,\n    mode='lines+markers',\n    name=\"cumulative explained variance\",\n    line=dict(\n        shape='hv',\n    ))\ndata = [trace_cum_var_exp, trace_ind_var_exp]\nlayout = go.Layout(\n    title='Individual and Cumulative Explained Variance',\n    autosize=True,\n    yaxis=dict(\n        title='percentage of explained variance',\n    ),\n    xaxis=dict(\n        title=\"principal components\",\n        dtick=1,\n    ),\n    legend=dict(\n        x=0,\n        y=1,\n    ),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4aa7e0773a9b2f9e06bba282cadb2737d5c28d10"},"cell_type":"markdown","source":"We can see from the graph that the first five components explain the most individual variance, followed by the next nine components which explain less variance. The last five components explain near no variance. We choose to reduce our dataset by using the first ten components which explain about 80% of the total variance."},{"metadata":{"trusted":true,"_uuid":"23c0837d03d4cb405d671ac10f7922b554bca30d"},"cell_type":"code","source":"n_components = 10\ndf_X_reduced = np.dot(df_X_normed.values, pca.components_[:n_components,:].T)\ndf_X_reduced = pd.DataFrame(df_X_reduced, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c7b8beb8716fb0c205454dcc0b4c6b1e9abc145"},"cell_type":"markdown","source":"## Comparison of differently preprocessed datasets for classification ##"},{"metadata":{"_uuid":"366f7e98a0260901bc7fca1dae33ae097ddeb858"},"cell_type":"markdown","source":"In this chapter we apply one classifier to the different version of the dataset. The Random Forest classifier is used because it is considered as a great baseline model for most applications. It is described in more detail in the next chapter. The following different versions of the dataset are investigated:\n- full dataset\n- dataset with variables reduced by the clustering according to the correlation\n- dataset reduced by considering the first ten principal components after applying PCA"},{"metadata":{"_uuid":"a3ed9a3e7eb71435c298dfb78a9fb2231fb6debd"},"cell_type":"markdown","source":"Furthermore, we apply two different methods to deal with the unbalanced target variable. First, we try adjusting the weights for the penalization when the classifier makes a mistake in the training and then we try to oversample the data using the Synthetic Minority Over-sampling Technique.\n\nFor the evaluation of the classifiers on the different datasets, a hold-out test set is used, which has 20% of all the data. To account for the class imbalance of our target variable, we use the f1-score as our main evaluation metric.\nWe define:\n- TP = #samples for which the prediction is positive and the true label is positive\n- FP = #samples for which the prediction is positive but the true label is negative\n- TN = #samples for which the prediction is negative and the true label is negative\n- FN = #samples for which the prediction is negative but the true label is positive\n\nThen we define the following:\n\n$\\text{precision} = \\frac{TP}{TP + FP} \\;\\;\\; \\text{and} \\;\\;\\; \\text{recall} = \\frac{TP}{TP + FN}$"},{"metadata":{"_uuid":"f0763f8506c7293e25aec21273d876ec7243594f"},"cell_type":"markdown","source":"Then the f1-score is given by the following equation:\n\n$F_{1}=2\\,\\frac{\\text{precision}\\, \\times \\,\\text{recall}}{\\text{precision} \\, +\\, \\text{recall}}$"},{"metadata":{"_uuid":"64fdbd878b18a3527ad7916c1d6bf78f8c03e967"},"cell_type":"markdown","source":"Every classifier has a set of hyperparameters, which can be tuned by training the classifier with different values for these hyperparameters and selecting the classifier with the best score. In order to estimate the performance of a classifier in a more reliable way, k-fold cross validation (CV) is used. In k-fold CV, the training set is divided into a k subsets. Then we train k times our classifier on different unions of k-1 subsets and calculate its score on the subset which was not used for training. Then the final score is calculated by averaging the score of each iteration. In detail, let $C_1, C_2, ... C_k$ be the indices of the samples in each of the $K$ parts of the dataset and let $n_k$ be the number of observations in part $k$. Then the score from the cross validation is computed as follows:\n\n$\\text{Score}_{CV(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n}\\text{Score}_k$.\n\nIn our hyper parameter tuning the Score is the f1-Score defined above.\n\nTo do this analysis, we use the sklearn.model_selection.GridSearchCV object, to which we pass a classifier, a dictionary of hyperparameters with values and a $k$-fold object. For a good trade-off between runtime and accuracy of the score we choose $k=5$, so the classifiers are trained on 80% of the train data in each iteration."},{"metadata":{"code_folding":[0],"trusted":true,"_uuid":"8fafa330b25e0cb8bd044a994bab2e2be130f2c3"},"cell_type":"code","source":"# prints the best grid search scores along with their parameters.\ndef print_best_grid_search_scores_with_params(grid_search, n=5):\n    if not hasattr(grid_search, 'best_score_'):\n        raise KeyError('grid_search is not fitted.')\n    print(\"Best grid scores on validation set:\")\n    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n    means = grid_search.cv_results_['mean_test_score'][indexes]\n    stds = grid_search.cv_results_['std_test_score'][indexes]\n    params = np.array(grid_search.cv_results_['params'])[indexes]\n    for mean, std, params in zip(means, stds, params):\n        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc66bd6cb96dc9b0a2c31fde4a0961cf58455249"},"cell_type":"code","source":"def do_gridsearch_with_cv(clf, params, X_train, y_train, cv, smote=None):\n\n    if smote is None:\n        pipeline = Pipeline([('clf', clf)])\n    else:\n        pipeline = Pipeline([('sm', sm), ('clf', clf)])\n        \n    gs = GridSearchCV(pipeline, params, cv=kf, n_jobs=-1, scoring='f1', return_train_score=True)\n    gs.fit(X_train, y_train)\n    return gs\n\ndef score_on_test_set(clfs, datasets):\n    scores = []\n    for c, (X_test, y_test) in zip(clfs, datasets):\n        scores.append(c.score(X_test, y_test))\n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87616c246ac65749be1d15a84e48709fde39dc23"},"cell_type":"code","source":"# split data into train and test set in proportion 4:1 for all differntly preprocessed datasets\nX_train, X_test, y_train, y_test = train_test_split(df_X_normed, df_y, test_size=0.2, random_state=SEED)\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(df_X_reduced, df_y, test_size=0.2, random_state=SEED)\ncols_without_duplicate = [x for x in df_X_normed.columns if x not in duplicate_features]\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(df_X_normed[cols_without_duplicate], df_y, test_size=0.2, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"709d6b22a87efd2c20dc0251416726f0bdc23e17"},"cell_type":"code","source":"print(\"Shape of the full train dataset:\", X_train.shape)\nprint(\"Shape of the train dataset with reduced features\", X_train_red.shape)\nprint(\"Shape of the transformed train dataset using the first 10 Principal Components\", X_train_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51a0a4cdcf05add7d3eb7f5e860ff2d8d1bb4627"},"cell_type":"code","source":"sm = SMOTE(random_state=SEED)\nkf = StratifiedKFold(n_splits=5, random_state=SEED)\nclf_rf = RandomForestClassifier(random_state=SEED)\nclf_balanced = RandomForestClassifier(random_state=SEED, class_weight=\"balanced\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8893b4be17439206c5a730c3d938d8fa7be4c3d9"},"cell_type":"markdown","source":"### Without additional balancing techniques ###"},{"metadata":{"_uuid":"4570339e9fc2c72e747d6a06082b66fbc8d32e2d"},"cell_type":"markdown","source":"We train the Random Forest on the full dataset, on the dataset with reduced features and on the dataset transformed using the first ten principal components."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"041cfbaf75f4b3b8b6cef7aa9edbb0bfef01dc6c"},"cell_type":"code","source":"%%time\ngs_full = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=None)\ngs_red = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=None)\ngs_pca = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=None)\ngss_raw = [gs_full, gs_red, gs_pca]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bdf0b3ca20f6ea3cafd6a9868233f556b04e6ed"},"cell_type":"code","source":"test_results_raw = score_on_test_set(gss_raw, [(X_test, y_test), (X_test_red, y_test_red), (X_test_pca, y_test_pca)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53a07db757a4923ba9c8261f47e3f48692dd2550"},"cell_type":"markdown","source":"### Using class weights in the loss function ###"},{"metadata":{"_uuid":"b6ae8d909b6c6da7bbcb19683376621d1e4905fb"},"cell_type":"markdown","source":"The sklearn library offers for all parametric classifiers a parameter class_weight, which can be set to \"balanced\". Then, mistakes are weighted inversely proportional to the class frequencies. This means that mistakes for the minority class are penalized more than mistakes made for the majority class."},{"metadata":{"trusted":true,"_uuid":"317a8ae1935562846e0a2714708bb00d577ccd15"},"cell_type":"code","source":"%%time\ngs_full_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=None)\ngs_red_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=None)\ngs_pca_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=None)\ngss_balanced_weights = [gs_full_balanced, gs_red_balanced, gs_pca_balanced]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"854504de7a7b11f49520e30a0bafa921735a7530"},"cell_type":"code","source":"test_results_balanced_weights = score_on_test_set(gss_balanced_weights, [(X_test, y_test), (X_test_red, y_test_red), (X_test_pca, y_test_pca)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81795aaed1fc9022c45c38f4d7e881875995199c"},"cell_type":"markdown","source":"### Using Synthetic Minority Over-sampling Technique ###"},{"metadata":{"_uuid":"0d15e4fb4fc4f697eb4e6fd0cded65aa8a5a8104"},"cell_type":"markdown","source":"The Synthetic Minority Over-sampling Technique (SMOTE) algorithm applies KNN approach where it selects one of the k nearest neighbors and computes the vector between the original point and the selected neighbor. The difference is multiplied by random number between (0, 1) and it is added back to original point to obtain the new synthetic point. Geometrically, the synthetic point is somewhere on the line between the original point and its neighbor.\n\nIn the following, we use the SMOTE implementation SMOTE of the imblearn.over_sampling library. Furthermore, we create a Pipeline of applying Smote and then training the classifier, so that it is executed in every fold of x-fold cross validation."},{"metadata":{"trusted":true,"_uuid":"1610f2cfa3af91c2b8e8eadb8613afea292c82df"},"cell_type":"code","source":"%%time\ngs_full_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=sm)\ngs_red_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=sm)\ngs_pca_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=sm)\ngss_smote = [gs_full_smote, gs_red_smote, gs_pca_smote]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d3bf061d4e02c8c993d2e0b40b56b717014832e"},"cell_type":"code","source":"test_results_smote = score_on_test_set(gss_smote, [(X_test, y_test), (X_test_red, y_test_red), (X_test_pca, y_test_pca)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c34e72b106c6f5933a1cf792beec188aa928e0f0"},"cell_type":"markdown","source":"### Comparison ###"},{"metadata":{"trusted":true,"_uuid":"597034d0471baf61f424deded905c4b2564a5b72"},"cell_type":"code","source":"dataset_strings = [\"full dataset\", \"data set with reduced features\", \"dataset with first 10 principal components\"]\nmethod_strings = [\"without any balancing\", \"using balanced class weights\", \"using SMOTE\"]\n\nresult_strings = dict()\nfor ms, results in zip(method_strings, [test_results_raw, test_results_balanced_weights, test_results_smote]):\n    for ds, res in zip(dataset_strings, results):\n        string = \"%.3f\" % res + \"     \" + ds + \" \" + ms\n        result_strings[string] = res\n        2\nresult_strings = sorted(result_strings.items(), key=lambda kv: kv[1], reverse=True)\nprint(\"F1 score  dataset and method\")\nfor k, _ in result_strings:\n    print(k)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2965c997035a0922e6085c0b0c3a1fa47d3ae75a"},"cell_type":"markdown","source":"The classifier trained on the PCA-transformed dataset performs the worst. The best results are obtained using the full dataset with SMOTE, closely followed by the reduced features dataset with SMOTE. So using SMOTE achieved better results than applying balanced class weights."},{"metadata":{"_uuid":"a1201265db2536cc373a2321d8ffad436c1d154a"},"cell_type":"markdown","source":"In conclusion, we use the full dataset and apply SMOTE in the following classification chapter."},{"metadata":{"_uuid":"97f48a9fbd409c14cd0f6ee01f0c81dc265fb5a1"},"cell_type":"markdown","source":"## Classification ##"},{"metadata":{"code_folding":[4],"trusted":true,"_uuid":"0bd7f4c09debb6331e46735e61d592e76b31d838"},"cell_type":"code","source":"def get_color_with_opacity(color, opacity):\n    return \"rgba(\" + color[4:-1] + \", %.2f)\" % opacity\n\n# partially based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\ndef plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=\"f1\", random_state=SEED)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    trace1 = go.Scatter(\n        x=train_sizes, \n        y=train_scores_mean - train_scores_std, \n        showlegend=False,\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[0], 0.4),\n        ),\n    )\n    trace2 = go.Scatter(\n        x=train_sizes, \n        y=train_scores_mean + train_scores_std, \n        showlegend=False,\n        fill=\"tonexty\",\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[0], 0.4),\n        ),\n    )\n    trace3 = go.Scatter(\n        x=train_sizes, \n        y=train_scores_mean, \n        showlegend=True,\n        name=\"Train score\",\n        line = dict(\n            color = colors[0],\n        ),\n    )\n    \n    trace4 = go.Scatter(\n        x=train_sizes, \n        y=test_scores_mean - test_scores_std, \n        showlegend=False,\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[1], 0.4),\n        ),\n    )\n    trace5 = go.Scatter(\n        x=train_sizes, \n        y=test_scores_mean + test_scores_std, \n        showlegend=False,\n        fill=\"tonexty\",\n        mode=\"lines\",\n        name=\"\",\n        hoverlabel = dict(\n            namelength=20\n        ),\n        line = dict(\n            width = 0.1,\n            color = get_color_with_opacity(colors[1], 0.4),\n        ),\n    )\n    trace6 = go.Scatter(\n        x=train_sizes, \n        y=test_scores_mean, \n        showlegend=True,\n        name=\"Test score\",\n        line = dict(\n            color = colors[1],\n        ),\n    )\n    \n    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n    layout = go.Layout(\n        title=title,\n        autosize=True,\n        yaxis=dict(\n            title='F1 Score',\n        ),\n        xaxis=dict(\n            title=\"#Training samples\",\n        ),\n        legend=dict(\n            x=0.8,\n            y=0,\n        ),\n    )\n    fig = go.Figure(data=data, layout=layout)\n    return iplot(fig, filename=title)","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0],"trusted":true,"_uuid":"fe2e5292c052e31275edcaacad036e031596d5f1"},"cell_type":"code","source":"def plot_feature_importance(feature_importance, title):\n    trace1 = go.Bar(\n        x=feature_importance[:, 0],\n        y=feature_importance[:, 1],\n        marker = dict(color = colors[0]),\n        name='feature importance'\n    )\n    data = [trace1]\n    layout = go.Layout(\n        title=title,\n        autosize=True,\n        margin=go.layout.Margin(l=50, r=100, b=150),\n        xaxis=dict(\n            title='feature',\n            tickangle=30\n        ),\n        yaxis=dict(\n            title='feature importance',\n            automargin=True,\n        ),\n    )\n    fig = go.Figure(data=data, layout=layout)\n    return iplot(fig, filename=title)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"926f027e63fb951ab2e21a11d2420cb0f057b9f2"},"cell_type":"markdown","source":"### Logistic Regression ###"},{"metadata":{"_uuid":"f436653b8db05d28dcf573825ea241c0f737825b"},"cell_type":"markdown","source":"We define $x_i$ as the $n$-dimensional feature vector of a given sample and $\\beta_{0},\\,\\boldsymbol{\\beta} = (\\beta_{1}, ..., \\beta_{n})^T$ as the model parameters. Then the logistic regression model is defined as:\n\n$P(Y=1 \\vert x_i)= \\frac{\\text{exp}(\\beta_{0} + x_i^T\\boldsymbol{\\beta} )}{1+\\text{exp}(\\beta_{0} + x_i^T\\boldsymbol{\\beta} )} = \\frac{1}{1+\\text{exp}(-(\\beta_{0} + x_i^T\\boldsymbol{\\beta} ))}$"},{"metadata":{"_uuid":"1837aa94ce3407c404f88b1ddeab554e958f53cf"},"cell_type":"markdown","source":"The hyperparameters of a logistic regression include the following ones, which can be passed to the LogisticRegression of sklearn.linear_model:\n- penalty: the norm used for penalization (default='l2')\n- C: the inverse of the regularization strength (default=1.0)"},{"metadata":{"trusted":true,"_uuid":"9ff0b56fab5894f330b937482518c08fe35d42e1"},"cell_type":"code","source":"%%time\nclf_lr = LogisticRegression(random_state=SEED)\ngs_lr = do_gridsearch_with_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf, smote=sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c2bbf696dcc04d07dc9ac8c8e1f15bbbf59487e"},"cell_type":"code","source":"print_best_grid_search_scores_with_params(gs_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f15f33fd5dc130f7e776eb357f5e88f979dd2a7"},"cell_type":"code","source":"gs_lr_score = gs_lr.score(X_test, y_test)\ny_pred_lr = gs_lr.predict(X_test)\ncm_lr = confusion_matrix(y_test, y_pred_lr)\ncm_lr = cm_lr.astype('float') / cm_lr.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3254f5993bd82c439d7a7ba684c05620bb62f3e2"},"cell_type":"markdown","source":"In the following, the normalized confusion matrix is shown. It demonstrates the proportion of samples which are true churn and predicted as churn/no churn and the proportion of samples which are true no churn and predicted as churn/no churn."},{"metadata":{"trusted":true,"_uuid":"4af308088d49189a9e5ac28ffb793c17792acf54"},"cell_type":"code","source":"cm_df = pd.DataFrame(cm_lr.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\ncm_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc3f78f8de122ab214d1fd6eb36f7c212cd121f8"},"cell_type":"markdown","source":"The confusion matrix of the Logistic Regression shows that the probability to predict the correct class is with 0.76 and 0.79 similar for both classes. This means that the Logistic Regression has just a slight bias towards predicting a customer as churn."},{"metadata":{"trusted":true,"_uuid":"c92cf185ba30a1d737fb2fb20cc7f726d4ce3e41"},"cell_type":"code","source":"plot_learning_curve(gs_lr.best_estimator_, \"Learning Curve of Logistic Regression\", X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62ad0e2eb3aa167351efd60b08fe305a4f887e2a"},"cell_type":"markdown","source":"For the learning curve of the Logistic Regression on the train set we can not see a clear trend, but on the train set the score increases with the number of training examples. Trained on all training samples the f1-score on the train set and on the test set is very similar, so the Logistic Regression is not overfitting the train data."},{"metadata":{"_uuid":"e0f424417505e21303639da16cb31fd335836a51"},"cell_type":"markdown","source":"### K-Nearest Neighbors ###"},{"metadata":{"_uuid":"46800dc26786dc88b90d561afe349af1b3503e9b"},"cell_type":"markdown","source":"The K-Nearest Neighbors algorithm (KNN) is a non-parametric method, which considers the K closest training examples to the point of interest for predicting its class. This is done by a simple majority vote over the K closest points."},{"metadata":{"_uuid":"726950a9c5b8c88cfe3095eb2de159a440311660"},"cell_type":"markdown","source":"The hyperparameters of KNN include the following ones, which can be passed to the KNeighborsClassifier of sklearn.neighbors:\n- n_neighbors: corresponds to K, the number of nearest neighbors considered for the prediction (default=5) \n- weights: \n  - if uniform, then all neighbors have the same weight for the voting (default)\n  - if distance, then the votes of the neighbors are weighted by the inverse of the distance for the voting\n- p: the power parameter for the Minkowski metric (default=2)"},{"metadata":{"trusted":true,"_uuid":"b040a86af366101177f2df53bf641b2a286a5e6b"},"cell_type":"code","source":"%%time\nclf_knn = KNeighborsClassifier()\ngs_knn = do_gridsearch_with_cv(clf_knn, KNN_PARAMS, X_train, y_train, kf, smote=sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"309337c4daaf02cf293d8f242fb216f9652cf72f"},"cell_type":"code","source":"print_best_grid_search_scores_with_params(gs_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ac2c815232f8260617f4a09f6d712b2e3bec758"},"cell_type":"code","source":"gs_knn_score = gs_knn.score(X_test, y_test)\ny_pred_knn = gs_knn.predict(X_test)\ncm_knn = confusion_matrix(y_test, y_pred_knn)\ncm_knn = cm_knn.astype('float') / cm_knn.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f2cfb6845e3c370616fbfc88714b3a4edb1981a"},"cell_type":"markdown","source":"In the following, the normalized confusion matrix is shown:"},{"metadata":{"trusted":true,"_uuid":"5d188f594bad04879084164455c515314cf61b87"},"cell_type":"code","source":"cm_df = pd.DataFrame(cm_knn.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\ncm_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4567da4b4c5563d433aa4bea5aa7593648f89f19"},"cell_type":"markdown","source":"The KNN classifier shows a small bias towards predicting no churn."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"940cff4f72bafcfa7546da184076914d1b51efdd"},"cell_type":"code","source":"plot_learning_curve(gs_knn.best_estimator_, \"Learning Curve of KNN\", X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7094214580e0b1fc108ad808817a9c0ffc3e4f2c"},"cell_type":"markdown","source":"The f1-score of 1 on the train set is a bit strange on the first sight. This behavior is caused by using the \"distance\" weights for the vote of the neighbors. This means that in the majority vote every point has a vote weight equal to the inverse of its distance to the point of interest. Because the point of interest is in the train set the distance to the nearest point is 0. So the weight of the vote of this point is infinite and the prediction is therefore equal to the class of the point itself. This applies to all training samples which results in a f1-score of 1.\n\nTo show a more interesting learning curve on the train set, we train another KNN classifier using uniform weights instead of the distance weights:"},{"metadata":{"trusted":true,"_uuid":"39b905beade6c0d2caf5a3902b39fc2d12579498"},"cell_type":"code","source":"clf_knn_uni = KNeighborsClassifier()\ngs_knn_uniform = do_gridsearch_with_cv(clf_knn_uni, KNN_PARAMS_UNIFORM, X_train, y_train, kf, smote=sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46c7d6f3252ea752135554bc3ddcb6c631b9533e"},"cell_type":"code","source":"print_best_grid_search_scores_with_params(gs_knn_uniform, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aa8168b8a8adca98d5ca59e90e49718d7a62946"},"cell_type":"code","source":"plot_learning_curve(gs_knn_uniform.best_estimator_, \"Learning Curve of KNN with uniform weights\", X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47cb7a1cdbdf21cdf27e4ce26bdfa4ae0f02b6a1"},"cell_type":"markdown","source":"The learning curve shows that the f1-score on the test set constantly increases, which makes sense because the KNN can consider more training examples and therefore it can do a more accurate prediction on the test points. The train score has also an increasing trend but does not improve between 692 and 1652 training samples."},{"metadata":{"_uuid":"3eb5002abec4982edb87035fe40a3b3d1fa64aaa"},"cell_type":"markdown","source":"### Support-Vector Machine ###"},{"metadata":{"_uuid":"bf6daa830ffe9050f3b24e23a0ddea23dcd1cb6f"},"cell_type":"markdown","source":"A linear Support-Vector Machine (SVM) finds the optimal hyperplane between the points of two classes such that the distance of the nearest points to the decision boundary is maximized. This distance is called margin.\n\nIf the data set is not linearly separable, we can map the samples ${\\bf x}$ into a feature space of higher dimensions:\n${\\bf x} \\longrightarrow \\phi({\\bf x})$ in which the classes can be linearly separated. This results in a non-linear decision boundary in the original dimensions.\n\nAs the vectors ${\\bf x}_i$ appear only in inner products in both the decision\nfunction and the learning law, the mapping function $\\phi({\\bf x})$ does not \nneed to be explicitly specified. Instead, we define a so-called kernel function:\n\n$ K({\\bf x}_1,{\\bf x}_2)=\\phi({\\bf x}_1)^T\\phi({\\bf x}_2)$.\n\nIn the gridsearch we consider the following two kernels:\n- linear kernel: $ K({\\bf x}_1,{\\bf x}_2) = {\\bf x}_1 \\cdot {\\bf x}_2$\n- radial basis function: $ K({\\bf x}_1,{\\bf x}_2) = exp(-\\gamma({\\Vert {\\bf x}_1 - {\\bf x}_2 \\Vert}^2))$"},{"metadata":{"_uuid":"13e7372b33bd288ff49a68ba8bdda118cf9f8c3b"},"cell_type":"markdown","source":"The hyperparameters of a SVM include the following ones, which can be passed to the SVC of sklearn.svm:\n- C: the inverse of the regularization strength (default=1.0)\n- kernel: the kernel used (default='rbf')\n- gamma: The higher the gamma value it tries to exactly fit the training data set (default='auto_deprecated')"},{"metadata":{"trusted":true,"_uuid":"14cb05df3a33b458e27c5b7668d857c33d522ef2"},"cell_type":"code","source":"%%time\nclf_svm = svm.SVC(random_state=SEED, probability=True)\ngs_svm = do_gridsearch_with_cv(clf_svm, SVM_PARAMS, X_train, y_train, kf, smote=sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb0b7559931ac3fd7972333f574654cba8d74402"},"cell_type":"code","source":"print_best_grid_search_scores_with_params(gs_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c63abaa9a669c18beb93c38584bed21de6f35b3"},"cell_type":"code","source":"gs_svm_score = gs_svm.score(X_test, y_test)\ny_pred_svm = gs_svm.predict(X_test)\ncm_svm = confusion_matrix(y_test, y_pred_svm)\ncm_svm = cm_svm.astype('float') / cm_svm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"656f9139159b4471ad35665ece0dd10549e6085c"},"cell_type":"markdown","source":"In the following, the normalized confusion matrix is shown:"},{"metadata":{"trusted":true,"_uuid":"b3bfaffd5a4799e68fcd007361246abf412839d0"},"cell_type":"code","source":"pd.DataFrame(cm_svm.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c454f510d67fcbe132da2c26269689780c473564"},"cell_type":"markdown","source":"The confusion matrix shows that the SVM classifier has a clear bias towards predicting no churn. This is not desirable in our case, because we do not want to miss out on churn-customers."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"16dc5644573b1888781f7d9d112d437dc1d461ce"},"cell_type":"code","source":"plot_learning_curve(gs_svm.best_estimator_, \"Learning Curve of SVM\", X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76ea6e0e714c118383fd14fbf8a450f949663983"},"cell_type":"markdown","source":"The train score of the SVM does not improve with the number of training samples. The test score, however, improves for the first 692 samples but stays the same with more samples. This could be due to the fact that the decision boundary of the SVM depends just on the support vectors and therefore changes just in the case that the additional training samples are support vectors. Furthermore, the train score is always by more than 0.1 higher than the test score which means that the SVM is overfitting on the train data."},{"metadata":{"_uuid":"c40e8e34a626c424647adfff5c28867e5f223b10"},"cell_type":"markdown","source":"### Decision Tree ###"},{"metadata":{"_uuid":"27ace62a7dd7d8ddbeaa9fdc92ef0109d4904897"},"cell_type":"markdown","source":"A decision tree for classification consists of several splits, which determine for a input sample, the predicted class, which is a leaf node in the tree. The construction of the decision trees is done with a greedy algorithm, because the theoretical minimum of function exists but it is NP-hard to determine it, because number of partitions has a factorial growth.\nSpecifically, a greedy top-down approach is used which chooses a variable at each step that best splits the set of items. For measuring the \"best\" different metrics can be used, which generally measure the homogeneity of the target variable within the subsets. For this analysis we consider the following two metrics:"},{"metadata":{"_uuid":"e501e6f51e2a9ecad2bd3d158d7d482d7d607b01"},"cell_type":"markdown","source":"Gini impurity: Let $j$ be the number of classes and $p_i$ the fraction of items of class $i$ in a subset $p$, for $i \\in \\{1,2,..., j\\}$. Then the gini impurity is defined as follows: $\\;I_G(p) = 1- \\sum_{i=1}^j {p_i}^2$.\n\n\nInformation gain: It measures the reduction in entropy when applying the split. The entropy is defined as $H(t) = - \\sum_{i=1}^j p_i\\, \\text{log}_2\\,p_i$. Then we define the information gain to split $n$ samples in parent node $p$ into $k$ partitions, where $n_i$ is the number of samples in partition $i$ as $IG = H(p) - \\sum_{i = 1}^k \\frac{n_i}{n} H(i)$."},{"metadata":{"_uuid":"79d382b7dd11571d247672df5da5d1170c8405c6"},"cell_type":"markdown","source":"The hyperparameters of a Decision Tree include the following ones, which can be passed to the DecisionTreeClassifier of sklearn.tree:\n- criterion: the criterion which decides the feature and the value at the split (default='gini')\n- max_depth: the maximum depth of each tree (default=None)\n- min_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)"},{"metadata":{"trusted":true,"_uuid":"bc7033e8fb07c15d5560fcac9f1f3fc93530c8a1"},"cell_type":"code","source":"%%time\nclf_dt = DecisionTreeClassifier(random_state=SEED)\ngs_dt = do_gridsearch_with_cv(clf_dt, DECISION_TREE_PARAMS, X_train, y_train, kf, smote=sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a62dd5a06d2ad853c11a47503ef8c18e664a697"},"cell_type":"code","source":"print_best_grid_search_scores_with_params(gs_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8592569a12bc5b926a48a2c12f7f18c8817e517"},"cell_type":"code","source":"gs_dt_score = gs_dt.score(X_test, y_test)\ny_pred_dt = gs_dt.predict(X_test)\ncm_dt = confusion_matrix(y_test, y_pred_dt)\ncm_dt = cm_dt.astype('float') / cm_dt.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af4adf139265d0560cfbb23bb4742db7c8adc2bc"},"cell_type":"markdown","source":"In the following, the normalized confusion matrix is shown:"},{"metadata":{"trusted":true,"_uuid":"4be1f745247d18e00a931318d8e2e75df5fae30d"},"cell_type":"code","source":"cm_df = pd.DataFrame(cm_dt.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\ncm_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b805136b611a36af7e1d2bc6d257c4f4517c9fc"},"cell_type":"markdown","source":"Similar to the SVM, also the Decision Tree has some bias towards predicting no churn, resulting in a poor performance in classifying the true churn clients."},{"metadata":{"_uuid":"69dbca282cc7fc8681434d91ffbecaec55756fbf"},"cell_type":"markdown","source":"The Decision Tree class of sklearn also computes an importance value for each feature. This is done by weighting the decrease of impurity at each split by the probability of reaching this node for each node which split involves the feature of interest. Then these weighted decreases of impurity are summed up for each feature and this gives the feature importance."},{"metadata":{"trusted":true,"_uuid":"8e48963c85d12bec8a910ecf9acca77f09f58524"},"cell_type":"code","source":"feature_importance = np.array(sorted(zip(X_train.columns, gs_dt.best_estimator_.named_steps['clf'].feature_importances_), key=lambda x: x[1], reverse=True))\nplot_feature_importance(feature_importance, \"Feature importance in the decision tree\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"731ce1b1d2eee17534b13b5f00b12ed25ccc27ca"},"cell_type":"markdown","source":"We can see that the most important features for the Decision Tree are the customer service calls and the total day minutes. These were also found to be correlated with the target variable in the data exploration. Interestingly, the area code is the third most important feature, but the area code has near zero correlation with the churn column."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"c1fc64938a439bae20f3f46f25b1be7af533e79e"},"cell_type":"code","source":"plot_learning_curve(gs_dt.best_estimator_, \"Learning Curve of the Decision Tree\", X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e108bfd04859af076cb41ef4f1720faaf7d33a77"},"cell_type":"markdown","source":"The Decision Tree is highly overfitted as the train score is about 0.3 higher than the test score. Both the train and especially the test score have an increasing trend with the number of samples."},{"metadata":{"_uuid":"92aa4eac1dc601ca5a8eed3e3ac568d36cc19ab5"},"cell_type":"markdown","source":"### Random Forest ###"},{"metadata":{"_uuid":"b128923f1e56de1e3dab76300df7cc021b96d673"},"cell_type":"markdown","source":"A random forest is an ensemble model that fits a number of decision tree classifiers on various sub-samples of the dataset which are created by the use of bootstrapping. In the inference stage it uses a majority vote over all trees to obtain the prediction. This improves the predictive accuracy and controls over-fitting. "},{"metadata":{"_uuid":"a61eb63e0ade2d8779280ffb23a0932fd389aad2"},"cell_type":"markdown","source":"The hyperparameters of a random forest include the following ones, which can be passed to the RandomForestClassifier of sklearn.ensemble:\n- n_estimators: the number of trees \n- criterion: the criterion which decides the feature and the value at the split (default='gini')\n- max_depth: the maximum depth of each tree (default=None)\n- min_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)\n- max_features: the number of features which are considered for a split (default='sqrt')"},{"metadata":{"trusted":true,"_uuid":"edfe9d7cdb40a8489a4e051bafaebc4dacc498b8"},"cell_type":"code","source":"%%time\nclf_rf = RandomForestClassifier(random_state=SEED)\ngs_rf = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69fde07ccebcfb99495b7f3585171b64896f7d0d"},"cell_type":"code","source":"print_best_grid_search_scores_with_params(gs_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4a596f4832157a50a304b379d8bf0b7ef97094c"},"cell_type":"code","source":"gs_rf_score = gs_rf.score(X_test, y_test)\ny_pred_rf = gs_rf.predict(X_test)\ncm_rf = confusion_matrix(y_test, y_pred_rf)\ncm_rf = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adbb45178c5111c29b39c9ed7a47e2394205180b"},"cell_type":"markdown","source":"In the following, the normalized confusion matrix is shown:"},{"metadata":{"trusted":true,"_uuid":"9e5fb02739b348b69b1e37e15146985d4419e92f"},"cell_type":"code","source":"cm_df = pd.DataFrame(cm_rf.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\ncm_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fe6baa1cc4c66587b6bb856fd7205a330f1b60a"},"cell_type":"markdown","source":"The Random Forest classifier shows also some bias towards predicting no churn, but not as much as the Decision Tree. Furthermore the results are very good by misclassifying just very few test samples."},{"metadata":{"_uuid":"c9d2a1db1aab0491debc413f72714f2e30a7ddb7"},"cell_type":"markdown","source":"In the Random Forest the feature importances are obtained by computing it for all trees and taking the average."},{"metadata":{"trusted":true,"_uuid":"f7815dfb69ef2d734e7d05f9d880245d4e25c8a0"},"cell_type":"code","source":"feature_importance_rf = np.array(sorted(zip(X_train.columns, gs_rf.best_estimator_.named_steps['clf'].feature_importances_), key=lambda x: x[1], reverse=True))\nplot_feature_importance(feature_importance_rf, \"Feature importance in the Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d51d9bf322af1a63d0afe06151f717f925482779"},"cell_type":"markdown","source":"The most important feature for the Random Forest are the number of customer service calls, the total day minutes and the total day charge. The latter two basically contain the same information as they are highly correlated. Other important features are if the user has an international plan and the total international calls. The area code was not found as important as in the Decision Tree."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"7952c3645719c6b423d0fa3618c8212e5271dfbd"},"cell_type":"code","source":"plot_learning_curve(gs_dt.best_estimator_, \"Learning Curve of the Random Forest\", X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79ccc4adb5110eaba2a7cbc15d3a8af189463c40"},"cell_type":"markdown","source":"It can be seen that the test score of the Random Forest constantly increases with the number of samples. The train score is with over 0.9 always very high and shows that also the Random Forest overfits on the data."},{"metadata":{"_uuid":"e4e302599fa1bced7045106369ed454ade6d170f"},"cell_type":"markdown","source":"### Comparison ###"},{"metadata":{"_uuid":"bf1f60d9daf0ab5cecfd355ce135772054ef415c"},"cell_type":"markdown","source":"To compare the performance of the different classification model, we consider several evalutation metric in addition to the in chapter 4 defined precision, recall and f1-score. Further, let TP, FP, TN, FN be defined as in chapter 4.\n\n**Accuracy**<br/>\nThe accuracy is the percentage of samples classified correctly: $\\text{accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}$.\n\n**Area Under the Receiver Operating Characteristic curve (AUC)**<br/>\nTo introduce this concept, we define the following two metrics:\n- True positive rate (TPR): this is the same as the recall: $FPR = \\text{recall} = \\frac{TP}{FN + TP}$\n- False positive rate (FPR): this corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points: $FPR = \\frac{FP}{TN + FP}$\n\nTo plot the Receiver Operating Characteristic (ROC) curve we choose a number of different classification thresholds and compute the TPR and the FPR. So the curve shows the trade-off between these two. To combine the TPR and the FPR into one evaluation metric the area under the ROC curve (AUC) is computed."},{"metadata":{"_uuid":"ea111f32343f9ea5ba027f566843ac94b6029b77"},"cell_type":"markdown","source":"The following plot shows the ROC curves of the classifiers trained in the previous chapters:"},{"metadata":{"code_folding":[1],"trusted":true,"_uuid":"2d9fa9dda39cd8668c1118efa306f98faeff675f"},"cell_type":"code","source":"# code partially from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\ndef plot_roc_curve(classifiers, legend, title, X_test, y_test):\n    trace1 = go.Scatter(\n        x=[0, 1], \n        y=[0, 1], \n        showlegend=False,\n        mode=\"lines\",\n        name=\"\",\n        line = dict(\n            color = colors[0],\n        ),\n    )\n    \n    data = [trace1]\n    aucs = []\n    for clf, string, c in zip(classifiers, legend, colors[1:]):\n        y_test_roc = np.array([([0, 1] if y else [1, 0]) for y in y_test])\n        y_score = clf.predict_proba(X_test)\n        \n        # Compute ROC curve and ROC area for each class\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        for i in range(2):\n            fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n        aucs.append(roc_auc['micro'])\n\n        trace = go.Scatter(\n            x=fpr['micro'], \n            y=tpr['micro'], \n            showlegend=True,\n            mode=\"lines\",\n            name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n            hoverlabel = dict(\n                namelength=30\n            ),\n            line = dict(\n                color = c,\n            ),\n        )\n        data.append(trace)\n\n    layout = go.Layout(\n        title=title,\n        autosize=False,\n        width=550,\n        height=550,\n        yaxis=dict(\n            title='True Positive Rate',\n        ),\n        xaxis=dict(\n            title=\"False Positive Rate\",\n        ),\n        legend=dict(\n            x=0.4,\n            y=0.06,\n        ),\n    )\n    fig = go.Figure(data=data, layout=layout)\n    return aucs, iplot(fig, filename=title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95c1030e252e7d24e7b62a75b77584b452d60ca9"},"cell_type":"code","source":"classifiers = [gs_lr, gs_knn, gs_svm, gs_dt, gs_rf]\nclassifier_names = [\"Logistic Regression\", \"KNN\", \"SVM\", \"Decision Tree\", \"Random Forest\"]\nauc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", X_test, y_test)\nroc_plot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"744e5dee5d8ab85fd32bb55e403461172dfa9ce8"},"cell_type":"markdown","source":"We can see that the Random Forest classifier has with 0.98 the highest AUC value, followed by the SVM, the KNN and the Decision Tree with 0.96, 0.92 and 0.91, respectively. The Logistic Regression performs worse with an AUC of just 0.81."},{"metadata":{"_uuid":"f4010251ecfc8e42a21b9d42be38b21a11924500"},"cell_type":"markdown","source":"Finally, we compute the accuracy, precision, recall and f1-score on the test set for every classifier. The following table shows the results:"},{"metadata":{"trusted":true,"_uuid":"f0451313412f7b84fe8e12a6a594b16a01525d44"},"cell_type":"code","source":"accs = []\nrecalls = []\nprecision = []\nresults_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\nfor (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n    y_pred = clf.predict(X_test)\n    row = []\n    row.append(accuracy_score(y_test, y_pred))\n    row.append(precision_score(y_test, y_pred))\n    row.append(recall_score(y_test, y_pred))\n    row.append(f1_score(y_test, y_pred))\n    row.append(auc)\n    row = [\"%.3f\" % r for r in row]\n    results_table.loc[name] = row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea475db5639b323abb9aae8b290b96d360e98563"},"cell_type":"code","source":"results_table","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c8d9def05eef6d884fc45ced9c464d592dda9ef"},"cell_type":"markdown","source":"The results show that the Logistic Regression has with about 0.32 a very low precision value, which means that when it predicts a customer to churn, it is just in 32% of the cases correct.<br/>\nMoreover, in accuracy the Decision Tree performs with 0.89 better than the KNN, which achieves 0.86. But in the AUC measure the KNN outperforms with 0.92 the Decision Tree with 0.91.<br/>\nThe KNN even has the highest recall value, but it achieves poor results for the precision, which also makes its f1-score the second worst of all classifiers.<br/>\nThe SVM has with 0.58 the second highest precision score but is far behind the best precision score of 0.83 of the Random Forest. This also impacts the f1-score for which the SVM also achieves the second highest with an value of 0.67, but again being worse than the Random Forest, which achieves a f1-score of 0.82."},{"metadata":{"_uuid":"2849491107ba6dfbb73d903689ac25aa9b13d09d"},"cell_type":"markdown","source":"## Conclusion ##"},{"metadata":{"_uuid":"2724a3dbb3a7f289928065a8f077ac9f9f193408"},"cell_type":"markdown","source":"Our goal was to identify clients which are likely to churn, so we can do special-purpose marketing strategies to avoid the churn event. For this we evaluated differently preprocessed datasets and different classifiers. The analysis has shown that the PCA transformation was not found to be useful. Instead, we suggest to use the whole dataset and apply a oversampling technique in order to deal with the unbalanced target variable. <br/>\nIn the classification chapter we have trained several different classifiers, including a Logistic Regression, a K-Nearest Neighbors Classifier, a Support-Vector Machine, a Decision Tree and a Random Forest. It was found that the best performance in accuracy, as well as f1-score and AUC is achieved by the Random Forest. One of the most important predictors for the Random Forest is the number of customer service calls. This might imply that the company should improve its customer service. Another important feature is the total day minutes and the total day charge, which basically hold the same information. So the company could try to either lower its charge per minute for clients, which have many day minutes or it could offer flat rates for calls.<br/>\nConcluding, we suggest the Telecom company to use the Random Forest model to identify potential churn customers and according to the customers life-time value present them special offers."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"206px"},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}