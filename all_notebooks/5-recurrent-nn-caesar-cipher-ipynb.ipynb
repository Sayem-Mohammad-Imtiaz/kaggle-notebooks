{"cells":[{"metadata":{},"cell_type":"markdown","source":"The task is to construct a recurrent neural network to decrypt the text encrypted by [Caesars cipher](https://en.wikipedia.org/wiki/Caesar_cipher)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom random import shuffle\nimport time\n\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 10\nSTRING_SIZE = 60\nNUM_EPOCHS = 20\nLEARNING_RATE = 0.05\nFILE_NAME = \"/kaggle/input/frank-herbert-dune/Dune.txt\"\nDEVICE = \"cpu\"\nCAESAR_OFFSET = 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because Caesar cipher operates the concept of alphabet, it is necessary to make a similar alphabet from the symbols available in the text.\n\nAs a sample for the preparation of the alphabet and dataset for learning a model, we will take the text of the book of Frank Herbert - Dune, translated into Russian."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Alphabet(object):\n\n    def __init__(self):\n        self.letters = \"\"\n\n    def __len__(self):\n        return len(self.letters)\n\n    def __contains__(self, item):\n        return item in self.letters\n\n    def __getitem__(self, item):\n        if isinstance(item, int):\n            return self.letters[item % len(self.letters)]\n        elif isinstance(item, str):\n            return self.letters.find(item)\n\n    def __str__(self):\n        letters = \" \".join(self.letters)\n        return f\"Alphabet is:\\n {letters}\\n {len(self)} chars\"\n\n    def load_from_file(self, file_path):\n        with open(file_path) as file:\n            while True:\n                text = file.read(STRING_SIZE)\n                if not text:\n                    break\n                for ch in text:\n                    if ch not in self.letters:\n                        self.letters += ch\n        return self\n\n\nALPHABET = Alphabet().load_from_file(FILE_NAME)\nprint(ALPHABET)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create objects of training, test and validation data sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentenceDataset(torch.utils.data.Dataset):\n\n    def __init__(self, raw_data, alphabet):\n        super().__init__()\n        self._len = len(raw_data)\n        self.y = torch.tensor(\n            [[alphabet[ch] for ch in line] for line in raw_data]\n        ).to(DEVICE)\n        self.x = torch.tensor(\n            [[i + CAESAR_OFFSET for i in line] for line in self.y]\n        ).to(DEVICE)\n    \n    def __len__(self):\n        return self._len\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_text_array(file_path, step):\n    text_array = []\n    with open(file_path) as file:\n        while True:\n            text = file.read(STRING_SIZE)\n            if not text:\n                break\n            text_array.append(text)\n    del text_array[-1]\n    return text_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = get_text_array(FILE_NAME, STRING_SIZE)\nshuffle(raw_data)\n_10_percent = math.ceil(len(raw_data) * 0.1)\nval_data = raw_data[:_10_percent]\nraw_data = raw_data[_10_percent:]\n_20_percent = math.ceil(len(raw_data) * 0.2)\ntest_data = raw_data[:_20_percent]\ntrain_data = raw_data[_20_percent:]\n\nY_val = torch.tensor([[ALPHABET[ch] for ch in line] for line in val_data])\nX_val = torch.tensor([[i + CAESAR_OFFSET for i in line] for line in Y_val])\n\ntrain_dl = torch.utils.data.DataLoader(\n    SentenceDataset(\n        train_data, ALPHABET\n    ),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=True\n)\ntest_dl = torch.utils.data.DataLoader(\n    SentenceDataset(\n        test_data, ALPHABET\n    ),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our RNN network will represent a fairly simple model with a layer embeding, then there will be RNN cell and output linear layer.\n\nIt is worth noting that to the size of the input values in the Embeding layer and the size of the output values of the linear layer must be added to the number that we will shift the alphabet to encrypt text."},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNNModel(torch.nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.embed = torch.nn.Embedding(len(ALPHABET) + CAESAR_OFFSET, 32)\n        self.rnn = torch.nn.RNN(32, 128, batch_first=True)\n        self.linear = torch.nn.Linear(128, len(ALPHABET) + CAESAR_OFFSET)\n\n    def forward(self, sentence, state=None):\n        embed = self.embed(sentence)\n        o, h = self.rnn(embed)\n        return self.linear(o)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RNNModel().to(DEVICE)\nloss = torch.nn.CrossEntropyLoss().to(DEVICE)\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(NUM_EPOCHS):\n    train_loss, train_acc, iter_num = .0, .0, .0\n    start_epoch_time = time.time()\n    model.train()\n    for x_in, y_in in train_dl:\n        x_in = x_in\n        y_in = y_in.view(1, -1).squeeze()\n        optimizer.zero_grad()\n        out = model.forward(x_in).view(-1, len(ALPHABET) + CAESAR_OFFSET)\n        l = loss(out, y_in)\n        train_loss += l.item()\n        batch_acc = (out.argmax(dim=1) == y_in)\n        train_acc += batch_acc.sum().item() / batch_acc.shape[0]\n        l.backward()\n        optimizer.step()\n        iter_num += 1\n    print(\n        f\"Epoch: {epoch}, loss: {train_loss:.4f}, acc: \"\n        f\"{train_acc / iter_num:.4f}\",\n        end=\" | \"\n    )\n    test_loss, test_acc, iter_num = .0, .0, .0\n    model.eval()\n    for x_in, y_in in test_dl:\n        x_in = x_in\n        y_in = y_in.view(1, -1).squeeze()\n        out = model.forward(x_in).view(-1, len(ALPHABET) + CAESAR_OFFSET)\n        l = loss(out, y_in)\n        test_loss += l.item()\n        batch_acc = (out.argmax(dim=1) == y_in)\n        test_acc += batch_acc.sum().item() / batch_acc.shape[0]\n        iter_num += 1\n    print(\n        f\"test loss: {test_loss:.4f}, test acc: {test_acc / iter_num:.4f} | \"\n        f\"{time.time() - start_epoch_time:.2f} sec.\"\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 256\nval_results = model(X_val.to(DEVICE)).argmax(dim=2)\nval_acc = (val_results == Y_val.to(DEVICE)).flatten()\nval_acc = (val_acc.sum() / val_acc.shape[0]).item()\nout_sentence = \"\".join([ALPHABET[i.item()] for i in val_results[idx]])\ntrue_sentence = \"\".join([ALPHABET[i.item()] for i in Y_val[idx]])\nprint(f\"Validation accuracy is : {val_acc:.4f}\")\nprint(\"-\" * 20)\nprint(f\"Validation sentence is: \\\"{out_sentence}\\\"\")\nprint(\"-\" * 20)\nprint(f\"True sentence is:       \\\"{true_sentence}\\\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because Caesar's cipher is a fairly primitive way to encrypt text, our model is quickly learning and shows excellent accuracy on test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"\"\"Барбадос – островное государство в восточной части Карибского \nморя, входящее в Британское Содружество наций. Столицу, город-порт Бриджтаун, \nотличает колониальная архитектура. Одна из достопримечательностей – синагога, \nпостроенная в 1654 году.\"\"\"\nsentence_idx = [ALPHABET[i] for i in sentence]\nencrypted_sentence_idx = [i + CAESAR_OFFSET for i in sentence_idx]\nencrypted_sentence = \"\".join([ALPHABET[i] for i in encrypted_sentence_idx])\nresult = model(torch.tensor([encrypted_sentence_idx]).to(DEVICE)).argmax(dim=2)\ndeencrypted_sentence = \"\".join([ALPHABET[i.item()] for i in result.flatten()])\nprint(f\"Encrypted sentence is : {encrypted_sentence}\")\nprint(\"-\" * 20)\nprint(deencrypted_sentence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, if we take the text that contains the characters that are not included in the training sample, we can get text decryption errors."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}