{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Topic Modeling - LDA- tf-idf.\n\n\n# Task :\n\n# Given the abstract and title for a set of research articles, predict the topics for each article included in the test set.\n\n\n","metadata":{"id":"O0ksZm7qm-9W"}},{"cell_type":"markdown","source":"## Steps:\n1. Read the data from csv file.    \n[Pandas](https://pandas.pydata.org/docs/user_guide/io.html)\n2. Data Preparation\n3. Extract the required information from the data frame.\n4. Word Embedding.  \n* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n5. PreProcessing Data\n* [WordNetLemmatizer](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)\n6. Latent Dirichlet Allocation\n* [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n\n","metadata":{"id":"8fDiMTek-uZs"}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"id":"kA0X0Br66Mhx"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:09:02.629859Z","iopub.execute_input":"2021-07-22T17:09:02.630634Z","iopub.status.idle":"2021-07-22T17:09:02.649091Z","shell.execute_reply.started":"2021-07-22T17:09:02.630519Z","shell.execute_reply":"2021-07-22T17:09:02.647991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pandas\nimport pandas as pd\n\n# sklearn - count vectorizers\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# LDA\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Numpy\nimport numpy as np\n\n# matplotlib\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n# seaborn\nimport seaborn as sns\n\n# nltk\nimport nltk\n\n","metadata":{"id":"WfPZQQW76QZ_","execution":{"iopub.status.busy":"2021-07-22T17:09:02.650712Z","iopub.execute_input":"2021-07-22T17:09:02.65123Z","iopub.status.idle":"2021-07-22T17:09:04.484284Z","shell.execute_reply.started":"2021-07-22T17:09:02.651186Z","shell.execute_reply":"2021-07-22T17:09:04.483227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# step 1 : Read Data from csv file","metadata":{"id":"69Lr99fD65Bc"}},{"cell_type":"markdown","source":"\n[df.size, df.shape and df.ndim](https://www.geeksforgeeks.org/python-pandas-df-size-df-shape-and-df-ndim/)\n\n[pandas.DataFrame.size](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.size.html)\n\n[pandas.DataFrame.shape](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.shape.html)","metadata":{"id":"Fy51oi-m3gbY"}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/topic-modeling-for-research-articles/train.csv')\n# print(df_train)","metadata":{"id":"mkhIOJRA7wYr","execution":{"iopub.status.busy":"2021-07-22T17:09:04.486107Z","iopub.execute_input":"2021-07-22T17:09:04.48644Z","iopub.status.idle":"2021-07-22T17:09:05.01888Z","shell.execute_reply.started":"2021-07-22T17:09:04.486407Z","shell.execute_reply":"2021-07-22T17:09:05.01789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"id":"PhGsTkzx27Rk","outputId":"fc578e14-1b48-43ca-84a8-fc4995ee3d49","execution":{"iopub.status.busy":"2021-07-22T17:09:05.020539Z","iopub.execute_input":"2021-07-22T17:09:05.020863Z","iopub.status.idle":"2021-07-22T17:09:05.056884Z","shell.execute_reply.started":"2021-07-22T17:09:05.02083Z","shell.execute_reply":"2021-07-22T17:09:05.055553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"id":"x0WWOSbN2I5E","outputId":"d20fd8fd-9e92-45c9-8b94-a0323c9e456b","execution":{"iopub.status.busy":"2021-07-22T17:09:05.058572Z","iopub.execute_input":"2021-07-22T17:09:05.059029Z","iopub.status.idle":"2021-07-22T17:09:05.06906Z","shell.execute_reply.started":"2021-07-22T17:09:05.058961Z","shell.execute_reply":"2021-07-22T17:09:05.067657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.size","metadata":{"id":"PrgmH05Z3Pdv","outputId":"f6a0ded3-83ce-4b37-d762-a126d053fd0a","execution":{"iopub.status.busy":"2021-07-22T17:09:05.071168Z","iopub.execute_input":"2021-07-22T17:09:05.07162Z","iopub.status.idle":"2021-07-22T17:09:05.083447Z","shell.execute_reply.started":"2021-07-22T17:09:05.071572Z","shell.execute_reply":"2021-07-22T17:09:05.082024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test data","metadata":{"id":"YFHDWB7h4iPS"}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/topic-modeling-for-research-articles/test.csv')\n# print(df_test)","metadata":{"id":"aMhcvy4p629Q","execution":{"iopub.status.busy":"2021-07-22T17:09:05.086151Z","iopub.execute_input":"2021-07-22T17:09:05.086537Z","iopub.status.idle":"2021-07-22T17:09:05.347639Z","shell.execute_reply.started":"2021-07-22T17:09:05.086498Z","shell.execute_reply":"2021-07-22T17:09:05.34659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"id":"sGn-YE_53-0J","outputId":"5f8bc469-a895-4716-a32c-d8b3ff72841a","execution":{"iopub.status.busy":"2021-07-22T17:09:05.349453Z","iopub.execute_input":"2021-07-22T17:09:05.349772Z","iopub.status.idle":"2021-07-22T17:09:05.372183Z","shell.execute_reply.started":"2021-07-22T17:09:05.349741Z","shell.execute_reply":"2021-07-22T17:09:05.369043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape","metadata":{"id":"hAq_y1zi3-tE","outputId":"ae200d6d-c7d3-426e-922d-3658267db0df","execution":{"iopub.status.busy":"2021-07-22T17:09:05.374106Z","iopub.execute_input":"2021-07-22T17:09:05.374516Z","iopub.status.idle":"2021-07-22T17:09:05.383861Z","shell.execute_reply.started":"2021-07-22T17:09:05.374481Z","shell.execute_reply":"2021-07-22T17:09:05.382682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.size","metadata":{"id":"DAnWXcOA3-fL","outputId":"316a5d81-f1d2-4cbd-b767-1f14c499e855","execution":{"iopub.status.busy":"2021-07-22T17:09:05.38564Z","iopub.execute_input":"2021-07-22T17:09:05.38596Z","iopub.status.idle":"2021-07-22T17:09:05.398686Z","shell.execute_reply.started":"2021-07-22T17:09:05.385925Z","shell.execute_reply":"2021-07-22T17:09:05.397596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Data Preparation","metadata":{"id":"6t-JKTWcX_nV"}},{"cell_type":"markdown","source":"# **df_train**\n\n\n1.   ID\n2.   Title\n3.   Abstract\n4.   Subject it may belong to :\n        *   Computer Science\n        *   Physics\n        *   Maths\n        *   Stats\n        *   Quantitative Biology\t\n        *   Quantitative Finance\n\n\n\n","metadata":{"id":"LTVDseaW8VPU"}},{"cell_type":"code","source":"df_train.head()","metadata":{"id":"jSTNx4XL7oeV","outputId":"6c2d3d9b-a9e4-453e-bd94-57f3bc3072f4","execution":{"iopub.status.busy":"2021-07-22T17:09:05.399987Z","iopub.execute_input":"2021-07-22T17:09:05.400278Z","iopub.status.idle":"2021-07-22T17:09:05.426269Z","shell.execute_reply.started":"2021-07-22T17:09:05.400248Z","shell.execute_reply":"2021-07-22T17:09:05.424899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **df_test**\n\n1.   ID\n2.   Title\n3.   Abstract\n\n","metadata":{"id":"kfuMbAiH9KCf"}},{"cell_type":"code","source":"df_test.head()","metadata":{"id":"UbgJ_ubd8CQS","outputId":"9e516386-bc92-4034-f91e-005cf69773ff","execution":{"iopub.status.busy":"2021-07-22T17:09:05.428294Z","iopub.execute_input":"2021-07-22T17:09:05.428715Z","iopub.status.idle":"2021-07-22T17:09:05.443522Z","shell.execute_reply.started":"2021-07-22T17:09:05.428671Z","shell.execute_reply":"2021-07-22T17:09:05.442149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### select everything \n\n*   X = df_train.iloc[:]\n\n### select everything from row 3 to end\n\n\n*   x = df_train.iloc[3:]\n\n\n[pandas.DataFrame.iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)","metadata":{"id":"EnYI56Xh6lhL"}},{"cell_type":"markdown","source":"### Check: \n* Total number of articles. \n* Total number of articles without label. \n* Total labels.","metadata":{"id":"y4ya0sq-alff"}},{"cell_type":"code","source":"x = df_train.iloc[:,3:].sum()\nrowsum = df_train.iloc[:,2:].sum(axis = 1)\nno_of_label_count = 0\n\nfor sum in rowsum.items():\n  if sum == 0:\n    no_of_label_count += 1\n\nprint(\"Total number of articles = \",len(df_train))\nprint(\"Total number of articles without label = \",no_of_label_count)\nprint(\"total labels = \",x.sum())","metadata":{"id":"mB5_v_vJYTym","outputId":"8e9f2a68-c24b-424f-9b4f-e028df3acb35","execution":{"iopub.status.busy":"2021-07-22T17:09:05.445637Z","iopub.execute_input":"2021-07-22T17:09:05.446179Z","iopub.status.idle":"2021-07-22T17:09:05.498841Z","shell.execute_reply.started":"2021-07-22T17:09:05.446121Z","shell.execute_reply":"2021-07-22T17:09:05.497748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for missing value","metadata":{"id":"RXu6RKwNbsHK"}},{"cell_type":"code","source":"train_null = df_train.isnull().sum()\nprint(\"Number of missing value in train data: \",train_null)\n\ntest_null = df_test.isnull().sum()\nprint(\"Number of missing value in test data: \", test_null)\n\n\nprint(\"\\n\")\nprint(\"train null data: \",train_null.sum())\nprint(\"test null data: \",test_null.sum())","metadata":{"id":"ISY6gd6xa0Af","outputId":"a0401d29-73c7-4b95-ebba-da306de32be5","execution":{"iopub.status.busy":"2021-07-22T17:09:05.501576Z","iopub.execute_input":"2021-07-22T17:09:05.502041Z","iopub.status.idle":"2021-07-22T17:09:05.638015Z","shell.execute_reply.started":"2021-07-22T17:09:05.501966Z","shell.execute_reply":"2021-07-22T17:09:05.637014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for every row select from column 3 to end\n# X = df_train.iloc[:,3:]\n# X","metadata":{"id":"iq6Sb4OC56zP","execution":{"iopub.status.busy":"2021-07-22T17:09:05.639268Z","iopub.execute_input":"2021-07-22T17:09:05.63968Z","iopub.status.idle":"2021-07-22T17:09:05.645159Z","shell.execute_reply.started":"2021-07-22T17:09:05.639644Z","shell.execute_reply":"2021-07-22T17:09:05.644002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = df_train.iloc[:,3:].sum() # column wise sum\n# X","metadata":{"id":"By4BKtMN7ZMv","execution":{"iopub.status.busy":"2021-07-22T17:09:05.64639Z","iopub.execute_input":"2021-07-22T17:09:05.646692Z","iopub.status.idle":"2021-07-22T17:09:05.657645Z","shell.execute_reply.started":"2021-07-22T17:09:05.646662Z","shell.execute_reply":"2021-07-22T17:09:05.656465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot a Graph to look at he distribution\n\n*  [Plotting with categorical variables](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)\n\n*  [How to use Seaborn Data Visualization for Machine Learning](https://machinelearningmastery.com/seaborn-data-visualization-for-machine-learning/)\n\n\n*  [matplotlib.patches.Rectangle\n](https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html)\n\n*  [seaborn](https://seaborn.pydata.org/tutorial.html)\n\n*  [How can I display text over columns in a bar chart in matplotlib\n](https://stackoverflow.com/questions/7423445/how-can-i-display-text-over-columns-in-a-bar-chart-in-matplotlib)","metadata":{"id":"J6Ci6HlI7oIl"}},{"cell_type":"code","source":"'''\nx.index =  Index(['Computer Science', 'Physics', 'Mathematics', 'Statistics',\n       'Quantitative Biology', 'Quantitative Finance'],\n      dtype='object')\n\n\nx.values = array([8594, 6013, 5618, 5206,  587,  249])\n\n\ntype(x.values) = numpy.\n\n# create patch/rectangle for CS, Physics, Math, Stat, Quantitaive Bio, Quantitative Finacne\nax.patches = \n[<matplotlib.patches.Rectangle at 0x7f6b3bc080d0>,\n <matplotlib.patches.Rectangle at 0x7f6b3bbee110>,\n <matplotlib.patches.Rectangle at 0x7f6b3bbeec10>,\n <matplotlib.patches.Rectangle at 0x7f6b3bc08b50>,\n <matplotlib.patches.Rectangle at 0x7f6b3bb8f0d0>,\n <matplotlib.patches.Rectangle at 0x7f6b3bb8f510>]\n\n\n\nrects = ax.patches\nlabels = x.values\nfor rect,label in zip(rects,labels):\n  print(rect , label)\n\n\nop:\nRectangle(xy=(-0.4, 0), width=0.8, height=8594, angle=0) 8594\nRectangle(xy=(0.6, 0), width=0.8, height=6013, angle=0) 6013\nRectangle(xy=(1.6, 0), width=0.8, height=5618, angle=0) 5618\nRectangle(xy=(2.6, 0), width=0.8, height=5206, angle=0) 5206\nRectangle(xy=(3.6, 0), width=0.8, height=587, angle=0) 587\nRectangle(xy=(4.6, 0), width=0.8, height=249, angle=0) 249\n\n'''","metadata":{"id":"FfTd78Yz_Wcr","outputId":"acbc8a82-f157-49ad-ba8e-951eed8fc427","execution":{"iopub.status.busy":"2021-07-22T17:09:05.658957Z","iopub.execute_input":"2021-07-22T17:09:05.659291Z","iopub.status.idle":"2021-07-22T17:09:05.671989Z","shell.execute_reply.started":"2021-07-22T17:09:05.65926Z","shell.execute_reply":"2021-07-22T17:09:05.671122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  display text over columns in a bar chart in matplotlib","metadata":{"id":"eHgabcB5YFGf"}},{"cell_type":"code","source":"def autolabel(rects,labels):\n# attach some text labels\n  for rect,label in zip(rects,labels):\n      height = rect.get_height()\n      plt.text(rect.get_x()+rect.get_width()/2., 1.02*height, label,\n              ha='center', va='bottom')\n  return","metadata":{"id":"8qMx-4bsCvUn","execution":{"iopub.status.busy":"2021-07-22T17:09:05.673462Z","iopub.execute_input":"2021-07-22T17:09:05.673792Z","iopub.status.idle":"2021-07-22T17:09:05.685133Z","shell.execute_reply.started":"2021-07-22T17:09:05.673759Z","shell.execute_reply":"2021-07-22T17:09:05.683767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def graph_viz(x,title,label):\n  sns.set_theme(style=\"whitegrid\")\n  plt.figure(figsize=(12,12))\n  ax = sns.barplot(x.index, x.values, alpha = 0.8) # adjust transparency (alpha) \n  plt.title(title,fontsize = 12)\n  plt.ylabel(label,fontsize = 12)\n  plt.xlabel('Label',fontsize=12)\n\n  # Display count in each class over columns in a bar chart\n  rects = ax.patches\n  labels = x.values\n  autolabel(rects,labels)\n\n  plt.show()\n","metadata":{"id":"lSQJnEvUeKR_","execution":{"iopub.status.busy":"2021-07-22T17:09:05.690014Z","iopub.execute_input":"2021-07-22T17:09:05.690421Z","iopub.status.idle":"2021-07-22T17:09:05.698815Z","shell.execute_reply.started":"2021-07-22T17:09:05.690384Z","shell.execute_reply":"2021-07-22T17:09:05.697701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dataset is highly imbalanced","metadata":{"id":"kKfN1Tg9eVYy"}},{"cell_type":"markdown","source":"# Number of occurance of each document","metadata":{"id":"sQK4wfrSf4jx"}},{"cell_type":"code","source":"x = df_train.iloc[:,3:].sum()\ngraph_viz(x,title = 'Class Count',label='# of occurance')","metadata":{"id":"bZ4bKlv87lOT","outputId":"52e4d3ad-7b45-4083-ed12-0e5fc9929903","execution":{"iopub.status.busy":"2021-07-22T17:09:05.701473Z","iopub.execute_input":"2021-07-22T17:09:05.701845Z","iopub.status.idle":"2021-07-22T17:09:05.975159Z","shell.execute_reply.started":"2021-07-22T17:09:05.701808Z","shell.execute_reply":"2021-07-22T17:09:05.974085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# No of Multiple tags per article","metadata":{"id":"ipzyeoO0byol"}},{"cell_type":"code","source":"rowsum = df_train.iloc[:,2:].sum(axis = 1)\nx = rowsum.value_counts() # Return a Series containing counts of unique values.\ngraph_viz(x,title = 'Multiple tags per article', label = '# of occurances')\n","metadata":{"id":"2teRYXC6cDOJ","outputId":"8696bb86-9986-4b8b-9f3f-044e12d953d4","execution":{"iopub.status.busy":"2021-07-22T17:09:05.97805Z","iopub.execute_input":"2021-07-22T17:09:05.978523Z","iopub.status.idle":"2021-07-22T17:09:06.206491Z","shell.execute_reply.started":"2021-07-22T17:09:05.97847Z","shell.execute_reply":"2021-07-22T17:09:06.205229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Getting out only the abstract field","metadata":{"id":"_JhGWd1q9255"}},{"cell_type":"code","source":"#df_test['ABSTRACT']\n\ntrain_text = df_train['ABSTRACT']\nprint(train_text.head())\nprint('--------------------------')\ntest_text = df_test['ABSTRACT']\nprint(test_text.head())","metadata":{"id":"T0HOgXMY8McJ","outputId":"651b529c-3888-4772-96ca-8e12c8ef467c","execution":{"iopub.status.busy":"2021-07-22T17:09:06.207791Z","iopub.execute_input":"2021-07-22T17:09:06.208114Z","iopub.status.idle":"2021-07-22T17:09:06.216298Z","shell.execute_reply.started":"2021-07-22T17:09:06.208082Z","shell.execute_reply":"2021-07-22T17:09:06.215173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [df.value](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.values.html)\ntest_text.values","metadata":{"id":"BtbLuYSUbngE","outputId":"32f420b4-924f-47fd-866f-939d1d32dcb5","execution":{"iopub.status.busy":"2021-07-22T17:09:06.217908Z","iopub.execute_input":"2021-07-22T17:09:06.218361Z","iopub.status.idle":"2021-07-22T17:09:06.229564Z","shell.execute_reply.started":"2021-07-22T17:09:06.218312Z","shell.execute_reply":"2021-07-22T17:09:06.228759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4:  Word Embedding","metadata":{"id":"uswsEsLFDy-g"}},{"cell_type":"markdown","source":"### **Function to get top n words**\n[np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\n\n[numpy.flip()](https://www.w3resource.com/numpy/manipulation/flip.php)\n\n[numpy.argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)\n\n[numpy.sort](https://numpy.org/doc/stable/reference/generated/numpy.sort.html)\n\n[numpy.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html)\n\n[pandas.DataFrame.values](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.values.html)\n\n[pandas.DataFrame.iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)\n\n[Matplotlib.axes.Axes.bar()](https://www.geeksforgeeks.org/matplotlib-axes-axes-bar-in-python/)\n\n\n[How vectorizer fit_transform work in sklearn](https://stackoverflow.com/questions/47898326/how-vectorizer-fit-transform-work-in-sklearn/54350840)\n","metadata":{"id":"OwD9jW-3ZaFF"}},{"cell_type":"markdown","source":"**Explanation to get the top n words from the test data:**\n```\nOn test data perform vectorization by removing stop words (vectorization is converting text to numbers)\n\nThen count the number of times each word has appeared(np.sum) since it is a vector. we count column wise. \n\n  a | b | c | d\n0\n1\n2\n3\n------------------\nsum \n--------------------\n\nCapture the sum and index. Egsum value 4547 might be present at index 147.\n\nsort (np.argsort) the index where the value are present and store them in descending order.\nsort(np.sort) the sum in descending(np.flip) order and store.\n\n\nnow create a 2d matrix and fill it with zero\n\nNow replace zero with 1 in the place(row, col).\ncolumn value is obtained from the index above.\n\n\nonce you replace the 0 with 1 convert the vector back into text\n\n```","metadata":{"id":"CRnirXMXx73O"}},{"cell_type":"code","source":"# def get_top_n_words(n_top_word ,test_data):\n#   vectorizer = CountVectorizer(stop_words = 'english') #initialize the vectorizer to remove stop words\n#   transformed_vectorizer = vectorizer.fit_transform(test_data)\n#   # print(transformed_vectorizer)\n#   # print('-------------------------------')\n#   # print('\\n')\n#   print(vectorizer.get_feature_names())\n#   # print('-------------------------------')\n#   # print('\\n')\n#   print(transformed_vectorizer.toarray())\n\n#   vectorized_total = np.sum(transformed_vectorizer, axis = 0) # column wise addition\n#   print(\"Vectorized total is: \",vectorized_total)\n#   # print(np.argsort(vectorized_total))\n#   # print(np.argsort(vectorized_total)[0,:])\n#   print(np.flip(np.argsort(vectorized_total)[0,:]))\n#   # print(np.flip(np.argsort(vectorized_total)[0,:],1))\n#   # print(np.flip(np.sort(vectorized_total)[0,:],1))\n#   print(np.flip(np.sort(vectorized_total)[0,:]))\n#   # print(vectorized_total.shape[1]) # column\n#   # print(vectorized_total.shape)\n\n#   word_indices = np.flip(np.argsort(vectorized_total)[0,:],1)\n#   print(\"word index is : \",word_indices)\n#   word_value = np.flip(np.sort(vectorized_total)[0,:],1)\n#   # print(\"Word Value is = \",word_value)\n#   print(\"Word Value is = \",word_value[0,:n_top_word].tolist()[0])\n  \n#   # create a 2d array with 15 row and column and fill them with zero\n\n#   word_vector = np.zeros((n_top_word,vectorized_total.shape[1]))\n\n#   for i in range(n_top_word):\n#     # print(i,word_indices[0,i])\n#     word_vector[i,word_indices[0,i]] = 1\n\n#   # print(word_vector)\n#   # print(vectorizer.inverse_transform(word_vector))\n\n#   inverse_vectorizer = vectorizer.inverse_transform(word_vector)\n#   print(\"inverse of vector is : \",inverse_vectorizer)\n\n#   # for word in inverse_vectorizer:\n#   #   print(word)\n\n#   words = [word[0] for word in inverse_vectorizer]\n#   print(words)\n\n#   return(words, word_value[0,:n_top_word].tolist()[0])\n","metadata":{"id":"DPKX3xSjZoF3","execution":{"iopub.status.busy":"2021-07-22T17:09:06.230651Z","iopub.execute_input":"2021-07-22T17:09:06.231114Z","iopub.status.idle":"2021-07-22T17:09:06.241427Z","shell.execute_reply.started":"2021-07-22T17:09:06.231067Z","shell.execute_reply":"2021-07-22T17:09:06.240028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef get_top_n_words(n_top_word ,test_data):\n  vectorizer = TfidfVectorizer(stop_words = 'english') #initialize the vectorizer to remove stop words\n  transformed_vectorizer = vectorizer.fit_transform(test_data)\n  # print(transformed_vectorizer)\n  # print('-------------------------------')\n  # print('\\n')\n  print(vectorizer.get_feature_names())\n  # print('-------------------------------')\n  # print('\\n')\n  print(transformed_vectorizer.toarray())\n\n  vectorized_total = np.sum(transformed_vectorizer, axis = 0) # column wise addition\n  print(\"Vectorized total is: \",vectorized_total)\n  # print(np.argsort(vectorized_total))\n  # print(np.argsort(vectorized_total)[0,:])\n  print(np.flip(np.argsort(vectorized_total)[0,:]))\n  # print(np.flip(np.argsort(vectorized_total)[0,:],1))\n  # print(np.flip(np.sort(vectorized_total)[0,:],1))\n  print(np.flip(np.sort(vectorized_total)[0,:]))\n  # print(vectorized_total.shape[1]) # column\n  # print(vectorized_total.shape)\n\n  word_indices = np.flip(np.argsort(vectorized_total)[0,:],1)\n  print(\"word index is : \",word_indices)\n  word_value = np.flip(np.sort(vectorized_total)[0,:],1)\n  # print(\"Word Value is = \",word_value)\n  print(\"Word Value is = \",word_value[0,:n_top_word].tolist()[0])\n  \n  # create a 2d array with 15 row and column and fill them with zero\n\n  word_vector = np.zeros((n_top_word,vectorized_total.shape[1]))\n\n  for i in range(n_top_word):\n    # print(i,word_indices[0,i])\n    word_vector[i,word_indices[0,i]] = 1\n\n  # print(word_vector)\n  # print(vectorizer.inverse_transform(word_vector))\n\n  inverse_vectorizer = vectorizer.inverse_transform(word_vector)\n  print(\"inverse of vector is : \",inverse_vectorizer)\n\n  # for word in inverse_vectorizer:\n  #   print(word)\n\n  words = [word[0] for word in inverse_vectorizer]\n  print(words)\n\n  return(words, word_value[0,:n_top_word].tolist()[0])","metadata":{"id":"QqXxSBq4tPUR","execution":{"iopub.status.busy":"2021-07-22T17:09:06.242906Z","iopub.execute_input":"2021-07-22T17:09:06.243264Z","iopub.status.idle":"2021-07-22T17:09:06.258438Z","shell.execute_reply.started":"2021-07-22T17:09:06.243231Z","shell.execute_reply":"2021-07-22T17:09:06.257413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the top n words from the text by converting them into vector\n'''\nfunction gets a tuple of top n words and their count.\n'''\nprint(test_text)\n# get_top_n_words(n_top_word = 15,test_data = test_text)\nwords, words_val = get_top_n_words(n_top_word = 15,test_data = test_text.values)\n","metadata":{"id":"fP4hI9ML-HjF","outputId":"ac4dd2e9-bce3-4c02-b51e-c3dca23393be","execution":{"iopub.status.busy":"2021-07-22T17:09:06.260073Z","iopub.execute_input":"2021-07-22T17:09:06.260452Z","iopub.status.idle":"2021-07-22T17:09:09.828502Z","shell.execute_reply.started":"2021-07-22T17:09:06.260421Z","shell.execute_reply":"2021-07-22T17:09:09.827168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(style=\"whitegrid\")\nfig,ax = plt.subplots(figsize=(12,12)) # this is needed when you are using axis.bar\nax.bar(words,words_val) \nax.set_xticklabels(words, rotation='vertical') # makes the word vertical\nplt.title('Top words excluding stop words',fontsize = 12)\nplt.ylabel('# of occurance',fontsize = 12)\nplt.xlabel('Words',fontsize=12)\nplt.show()","metadata":{"id":"6mTQzhuVulfp","outputId":"4df53f04-0acb-4f00-ec2f-00a2f4822c0d","execution":{"iopub.status.busy":"2021-07-22T17:09:09.829718Z","iopub.execute_input":"2021-07-22T17:09:09.830045Z","iopub.status.idle":"2021-07-22T17:09:10.147509Z","shell.execute_reply.started":"2021-07-22T17:09:09.830011Z","shell.execute_reply":"2021-07-22T17:09:10.14635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# step 5: Topic Modelling","metadata":{"id":"Oj3JSTkcD-n4"}},{"cell_type":"markdown","source":"### Preprocessing Data - lemmatization","metadata":{"id":"XxmEcF7HQw0F"}},{"cell_type":"markdown","source":"* [what does the min/max document frequency exactly means](https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer)\n\n* [use build analyzer](https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn)","metadata":{"id":"Ff3SXZa5G289"}},{"cell_type":"markdown","source":"add lemmatize support to CountVectorizer (sklearn)","metadata":{"id":"4B13j3QQNpsE"}},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"id":"9A6_qenuQQBP","outputId":"f3d9c1ec-0b22-4749-e07b-ce17bf9e11d6","execution":{"iopub.status.busy":"2021-07-22T17:09:10.149156Z","iopub.execute_input":"2021-07-22T17:09:10.149594Z","iopub.status.idle":"2021-07-22T17:09:10.338141Z","shell.execute_reply.started":"2021-07-22T17:09:10.149546Z","shell.execute_reply":"2021-07-22T17:09:10.337218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import these modules\nfrom nltk.stem import WordNetLemmatizer\n  \nlemmatizer = nltk.WordNetLemmatizer()\nclass CountVectorizer_lemmatizer(CountVectorizer):\n  def build_analyzer(self):\n    analyzer = super(CountVectorizer_lemmatizer, self).build_analyzer()\n    return lambda doc:(lemmatizer.lemmatize(w) for w in analyzer(doc))\n","metadata":{"id":"8YXpjb41MBsI","execution":{"iopub.status.busy":"2021-07-22T17:09:10.34175Z","iopub.execute_input":"2021-07-22T17:09:10.342098Z","iopub.status.idle":"2021-07-22T17:09:10.351766Z","shell.execute_reply.started":"2021-07-22T17:09:10.342067Z","shell.execute_reply":"2021-07-22T17:09:10.35042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_text is  df_train['ABSTRACT']. all the characters from abstract.\n# train_text\n# print(list(train_text))\ntext = list(train_text)\n# print(text)\n# igonre a term that apperas in more than 95% of the document and ignore a term that appers in less than 2 document\ntf_vectorizer = CountVectorizer_lemmatizer(decode_error='ignore', max_df = 0.95,min_df=2,stop_words='english') # applying count vectorizer\n# print(tf_vectorizer)\ntf = tf_vectorizer.fit_transform(text)\n# print(tf)\n","metadata":{"id":"WMWfx1zlAby0","execution":{"iopub.status.busy":"2021-07-22T17:09:10.353107Z","iopub.execute_input":"2021-07-22T17:09:10.353652Z","iopub.status.idle":"2021-07-22T17:09:26.943621Z","shell.execute_reply.started":"2021-07-22T17:09:10.353614Z","shell.execute_reply":"2021-07-22T17:09:26.942452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Latent Dirichlet Allocation\n\n[LSA Topic Modelling](https://gist.github.com/susanli2016/3f88f5aab3f844cc53a44817386d06ce)","metadata":{"id":"h3eWZ3T-tQb-"}},{"cell_type":"markdown","source":"* **Fit ()**\n\ncall fit() to train the model using the input training and data.\n\n[fit() vs predict() vs fit_predict() in Python scikit-learn](https://towardsdatascience.com/fit-vs-predict-vs-fit-predict-in-python-scikit-learn-f15a34a8d39f)\n\n\nIn a nutshell: fitting is equal to training. Then, after it is trained, the model can be used to make predictions, usually with a .predict() method call.\n\n[What does the “fit” method in scikit-learn do?](https://stackoverflow.com/questions/45704226/what-does-the-fit-method-in-scikit-learn-do)\n\nLatentDirichletAllocation\n\n[sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n\n[How to interpret LDA components (using sklearn)](https://stackoverflow.com/questions/35140117/how-to-interpret-lda-components-using-sklearn)","metadata":{"id":"hwmrd7TerJ6R"}},{"cell_type":"code","source":"# play around with this\nn_topics = 10","metadata":{"id":"wSZHteHLweKk","execution":{"iopub.status.busy":"2021-07-22T17:09:26.945355Z","iopub.execute_input":"2021-07-22T17:09:26.945812Z","iopub.status.idle":"2021-07-22T17:09:26.952108Z","shell.execute_reply.started":"2021-07-22T17:09:26.945761Z","shell.execute_reply":"2021-07-22T17:09:26.950728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components = n_topics, max_iter=5, learning_method='online',learning_offset=50., random_state=0)\n\nlda.fit(tf)\n\n# making LDA TOP MATRIX USING CORPUS TF\nlda_topic_modelling = lda.fit_transform(tf)","metadata":{"id":"Z2YZFPMfQH5m","execution":{"iopub.status.busy":"2021-07-22T17:09:26.953505Z","iopub.execute_input":"2021-07-22T17:09:26.953807Z","iopub.status.idle":"2021-07-22T17:11:59.616119Z","shell.execute_reply.started":"2021-07-22T17:09:26.953777Z","shell.execute_reply":"2021-07-22T17:11:59.615073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_topic_modelling","metadata":{"id":"sWR20PHS1aMr","outputId":"1e6e70d7-d436-48ba-e121-8f66bc4c76ce","execution":{"iopub.status.busy":"2021-07-22T17:11:59.619346Z","iopub.execute_input":"2021-07-22T17:11:59.619802Z","iopub.status.idle":"2021-07-22T17:11:59.627472Z","shell.execute_reply.started":"2021-07-22T17:11:59.61976Z","shell.execute_reply":"2021-07-22T17:11:59.626342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### return an integer list of predicted topic catergories for a given topic matrix\n\n* [numpy.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)","metadata":{"id":"xCrL53sU1rO1"}},{"cell_type":"code","source":"def get_keys(topic_matrix):\n  # print(topic_matrix.argmax(axis = 1)) # axis = 1, will return maximum index in that array \n  keys = topic_matrix.argmax(axis = 1).tolist()\n  print(\"length of the keys is: \",len(keys))\n  return keys","metadata":{"id":"jHOm9v0D12RK","execution":{"iopub.status.busy":"2021-07-22T17:11:59.629307Z","iopub.execute_input":"2021-07-22T17:11:59.629756Z","iopub.status.idle":"2021-07-22T17:11:59.639565Z","shell.execute_reply.started":"2021-07-22T17:11:59.629708Z","shell.execute_reply":"2021-07-22T17:11:59.638437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Return a tuple of topic categories and their accompanying magnitude for a given list of keys\n\n[Counter](https://pymotw.com/2/collections/counter.html)","metadata":{"id":"Kl7jfGAT3wX9"}},{"cell_type":"code","source":"from collections import Counter\ndef key_to_count(keys):\n  count_pairs = Counter(keys).items()\n  # print(\"Count_pairs\",count_pairs)\n  categories = [pair[0] for pair in count_pairs]\n  # print(\"categories\",categories)\n  counts = [pair[1] for pair in count_pairs]\n  # print(\"Counts: \",counts)\n  return (categories, counts)","metadata":{"id":"81_sFaHx4BbH","execution":{"iopub.status.busy":"2021-07-22T17:11:59.641308Z","iopub.execute_input":"2021-07-22T17:11:59.64195Z","iopub.status.idle":"2021-07-22T17:11:59.6515Z","shell.execute_reply.started":"2021-07-22T17:11:59.641902Z","shell.execute_reply":"2021-07-22T17:11:59.649985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Getting category and count from LDA Model**","metadata":{"id":"5zX4dukC59zD"}},{"cell_type":"code","source":"lda_keys = get_keys(lda_topic_modelling)\nprint(\"keys: \",lda_keys)\n# key_to_count(lda_keys)\n\nlda_categories, lda_count = key_to_count(lda_keys)\n","metadata":{"id":"7t1VtDaC1bs0","outputId":"58440ba3-e309-4368-b26d-46188a83b339","execution":{"iopub.status.busy":"2021-07-22T17:11:59.653178Z","iopub.execute_input":"2021-07-22T17:11:59.653606Z","iopub.status.idle":"2021-07-22T17:11:59.679041Z","shell.execute_reply.started":"2021-07-22T17:11:59.653565Z","shell.execute_reply":"2021-07-22T17:11:59.677619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Return A list of n_topic strings, where each string contains the n most common words in a predicted category in order.**","metadata":{"id":"B_so9PMRCQlg"}},{"cell_type":"markdown","source":"[Code to returns a list of n_topic strings, where each string contains the n most common words in a predicted category, in order](https://gist.github.com/susanli2016/3f88f5aab3f844cc53a44817386d06ce)","metadata":{"id":"VitM70IkUBhK"}},{"cell_type":"code","source":"def get_top_n_words(n, keys, document_term_matrix, cv):\n    top_word_indices = []\n\n    for topic in range(n_topics): # go from 0 to 24\n        temp_vector_sum = 0\n        # print(len(keys))  # 20972\n        # print(keys)\n        for i in range(len(keys)): # go from 0 to 20971\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        \n        \n        temp_vector_sum = temp_vector_sum.toarray()\n        # print(temp_vector_sum)\n\n        # print(np.sort(temp_vector_sum))\n        # print(\"\\n\")\n        # print(np.argsort(temp_vector_sum))\n        # print(\"\\n\")\n        # print(np.argsort(temp_vector_sum)[0])\n        # print(\"\\n\")\n        # # last 15 col\n        # print(np.argsort(temp_vector_sum)[0][-n:]) # from -15 th col to end for row 0\n        # print(\"\\n\")\n        # print(np.flip(np.argsort(temp_vector_sum)[0][-n:],0))\n        # print(\"\\n\")\n        # print(\"\\n\")\n\n\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)  \n\n    # print(document_term_matrix)\n    # print(document_term_matrix.shape) #(20972, 24648)\n    # print(document_term_matrix.shape[1]) #24648\n    # print(np.zeros((1,document_term_matrix.shape[1])))\n    # create one row with 24648 column\n    # print(len(np.zeros((1,document_term_matrix.shape[1]))[0])) #24648\n\n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            # print(temp_word_vector)\n            \n            the_word = cv.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","metadata":{"id":"fiAE37jITHZm","execution":{"iopub.status.busy":"2021-07-22T17:11:59.680881Z","iopub.execute_input":"2021-07-22T17:11:59.681332Z","iopub.status.idle":"2021-07-22T17:11:59.693645Z","shell.execute_reply.started":"2021-07-22T17:11:59.681288Z","shell.execute_reply":"2021-07-22T17:11:59.692857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_top_words = 25\ntop_n_words = get_top_n_words(n_top_words, lda_keys, tf, tf_vectorizer)","metadata":{"id":"xiho7b8c2D9n","execution":{"iopub.status.busy":"2021-07-22T17:11:59.694747Z","iopub.execute_input":"2021-07-22T17:11:59.695235Z","iopub.status.idle":"2021-07-22T17:12:12.213017Z","shell.execute_reply.started":"2021-07-22T17:11:59.695193Z","shell.execute_reply":"2021-07-22T17:12:12.211999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Printing word from each topic**","metadata":{"id":"U8miz4j4Zt8f"}},{"cell_type":"code","source":"for i in range(len(top_n_words)):\n    print(\"Topic {}: \\n\".format(i+1), top_n_words[i]+\"\\n\")","metadata":{"id":"PgB1bLWPUnGG","outputId":"f130a9e4-adb6-406a-b4b9-565934b0addd","execution":{"iopub.status.busy":"2021-07-22T17:12:12.214281Z","iopub.execute_input":"2021-07-22T17:12:12.214601Z","iopub.status.idle":"2021-07-22T17:12:12.224057Z","shell.execute_reply.started":"2021-07-22T17:12:12.214568Z","shell.execute_reply":"2021-07-22T17:12:12.223051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data.\n\n[sklearn.manifold.TSNE](https://scikit-learn.org/0.15/modules/generated/sklearn.manifold.TSNE.html)\n\n[numpy.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html)\n\n[numpy.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)","metadata":{"id":"VxnS1GLBarrA"}},{"cell_type":"code","source":"# this will take time\nfrom sklearn.manifold import TSNE\ntnse_lda_model = TSNE(n_components=2, perplexity=50, early_exaggeration=4.0, learning_rate=100, n_iter=2000, metric='euclidean', init='random', verbose=0, random_state=0)\ntnse_lda_vector = tnse_lda_model.fit_transform(lda_topic_modelling)","metadata":{"id":"G7bIa7YVZmQB","execution":{"iopub.status.busy":"2021-07-22T17:12:12.226326Z","iopub.execute_input":"2021-07-22T17:12:12.226755Z","iopub.status.idle":"2021-07-22T17:16:53.099074Z","shell.execute_reply.started":"2021-07-22T17:12:12.226716Z","shell.execute_reply":"2021-07-22T17:16:53.09812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**# get top 5 words from each topic to use in plot**","metadata":{"id":"H86yRGUKlwZU"}},{"cell_type":"code","source":"k = 3\ntop_k_words_lda = get_top_n_words(k,lda_keys,tf,tf_vectorizer)\ntop_k_words_lda","metadata":{"id":"l0W04e-Cfryt","outputId":"5d652eb6-09ef-41d9-86e9-57a41afbea8f","execution":{"iopub.status.busy":"2021-07-22T17:16:53.100753Z","iopub.execute_input":"2021-07-22T17:16:53.10145Z","iopub.status.idle":"2021-07-22T17:17:01.837075Z","shell.execute_reply.started":"2021-07-22T17:16:53.101402Z","shell.execute_reply":"2021-07-22T17:17:01.836025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**# returns a list of centroid vectors from each predicted topic category**","metadata":{"id":"AZXQ64phl0F2"}},{"cell_type":"code","source":"\ndef get_mean_topic_vectors(keys, two_dim_vectors):\n  mean_topic_vectors = []\n\n  # n_topics = 10\n  for i in range(n_topics):\n    articles_in_the_topic = []\n\n    for j in range(len(keys)):\n      # print(i)\n      # print(keys[j])\n      if keys[j] == i:\n        # print(\"two_dim_vectors[j]\",two_dim_vectors[j])\n        articles_in_the_topic.append(two_dim_vectors[j])\n\n    articles_in_the_topic = np.vstack(articles_in_the_topic)\n    # print(\"articles_in_the_topic\",articles_in_the_topic)\n    mean_article_in_that_topic = np.mean(articles_in_the_topic,axis = 0) # column wise\n    # print(\"mean_article_in_that_topic\",mean_article_in_that_topic)\n    mean_topic_vectors.append(mean_article_in_that_topic)\n\n  return mean_topic_vectors\n","metadata":{"id":"qDzoRTpaiggM","execution":{"iopub.status.busy":"2021-07-22T17:17:01.838375Z","iopub.execute_input":"2021-07-22T17:17:01.838717Z","iopub.status.idle":"2021-07-22T17:17:01.845741Z","shell.execute_reply.started":"2021-07-22T17:17:01.838683Z","shell.execute_reply":"2021-07-22T17:17:01.844715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**# Getting the mean of the topic vector for the visualization**","metadata":{"id":"W5okSGM4l3jG"}},{"cell_type":"code","source":"lda_mean_topic_vectors = get_mean_topic_vectors(lda_keys,tnse_lda_vector)","metadata":{"id":"yfT581x-djY3","execution":{"iopub.status.busy":"2021-07-22T17:17:01.84717Z","iopub.execute_input":"2021-07-22T17:17:01.847547Z","iopub.status.idle":"2021-07-22T17:17:01.942041Z","shell.execute_reply.started":"2021-07-22T17:17:01.847514Z","shell.execute_reply":"2021-07-22T17:17:01.941193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Top word from each topic to use in plot**\n\n[Bokeh](https://docs.bokeh.org/en/latest/docs/user_guide/annotations.html)\n\n[Bokeh color_scatter](https://docs.bokeh.org/en/latest/docs/gallery/color_scatter.html)","metadata":{"id":"sXDxbXrJl-eZ"}},{"cell_type":"code","source":"# Colourmap for the visualization\ncolormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\" ])\ncolormap = colormap[:n_topics]\nprint(colormap)","metadata":{"id":"Yl7m4_VcrQ5Q","outputId":"e89f25af-4c48-4c9b-e0a1-b8f5009df3d6","execution":{"iopub.status.busy":"2021-07-22T17:17:01.943204Z","iopub.execute_input":"2021-07-22T17:17:01.943711Z","iopub.status.idle":"2021-07-22T17:17:01.950081Z","shell.execute_reply.started":"2021-07-22T17:17:01.943641Z","shell.execute_reply":"2021-07-22T17:17:01.949075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\n\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n# print(lda_mean_topic_vectors)\n# print(tnse_lda_vector) # there are 2 column\n# print(\"\\n\")\n# print(tnse_lda_vector[:,0]) # first column\n# print(\"\\n\")\n# print(tnse_lda_vector[:,1]) # second Column\n# print(\"\\n\")\n\nplot = figure(title=\"t-SNE clustring of {} LDA topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x = tnse_lda_vector[:,0], y = tnse_lda_vector[:,1], color = colormap[lda_keys])\n\n\nfor i in range(n_topics):\n  label = Label(x = lda_mean_topic_vectors[i][0], y = lda_mean_topic_vectors[i][1], text = top_k_words_lda[i], text_color = colormap[i])\n  plot.add_layout(label)\n\n\n\nshow(plot)\n","metadata":{"id":"6WO127t2jbxL","outputId":"c9b6cc93-e601-4f7d-9451-8249b76f86f5","execution":{"iopub.status.busy":"2021-07-22T17:17:01.951447Z","iopub.execute_input":"2021-07-22T17:17:01.952045Z","iopub.status.idle":"2021-07-22T17:17:02.560208Z","shell.execute_reply.started":"2021-07-22T17:17:01.951997Z","shell.execute_reply":"2021-07-22T17:17:02.559166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Assigning Topics to document**","metadata":{"id":"aZTWz6uVwp7W"}},{"cell_type":"code","source":"list(train_text)","metadata":{"id":"RAIV8ssrygMU","outputId":"56a1bbc9-bd49-42fb-c633-428814d91f6f","execution":{"iopub.status.busy":"2021-07-22T17:17:02.561437Z","iopub.execute_input":"2021-07-22T17:17:02.561724Z","iopub.status.idle":"2021-07-22T17:17:02.636313Z","shell.execute_reply.started":"2021-07-22T17:17:02.561696Z","shell.execute_reply":"2021-07-22T17:17:02.635177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc_topic = lda.transform(tf)\n\n# check for 20 documents\nfor n in range(20):\n  # print(doc_topic[n])\n  topic_most_pr = doc_topic[n].argmax()\n  # print(topic_most_pr)\n  print(\"Document #{} - topic: {}\\n\".format(n,topic_most_pr))\n  # this means document 1 belongs to topic 5 which seems to be like Quantitative Biology  ","metadata":{"id":"naxGhiyXureJ","outputId":"c3152951-9bef-4f35-9508-f2d6943f6fc3","execution":{"iopub.status.busy":"2021-07-22T17:17:02.637909Z","iopub.execute_input":"2021-07-22T17:17:02.638341Z","iopub.status.idle":"2021-07-22T17:17:10.527166Z","shell.execute_reply.started":"2021-07-22T17:17:02.638302Z","shell.execute_reply":"2021-07-22T17:17:10.526171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Advanced Analysis of LDA\nMain goal is to find the most relevant articles for each topic so they can be used as the links for further research","metadata":{"id":"JFuUxoFe0iob"}},{"cell_type":"code","source":"# making a dataframe from the document-topic matrix\ndoc_topic_df = pd.DataFrame(data=doc_topic)\ndoc_topic_df","metadata":{"id":"ls74XXlrvQCa","outputId":"daef7b91-8392-48bc-c444-b7fed732ba60","execution":{"iopub.status.busy":"2021-07-22T17:17:10.528502Z","iopub.execute_input":"2021-07-22T17:17:10.528809Z","iopub.status.idle":"2021-07-22T17:17:10.551473Z","shell.execute_reply.started":"2021-07-22T17:17:10.528776Z","shell.execute_reply":"2021-07-22T17:17:10.550704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# printing the top 'n' articles for each topic","metadata":{"id":"8u6l3SoM1lAD"}},{"cell_type":"code","source":"for (columnName, columnData) in doc_topic_df.iteritems():\n  print(columnName, columnData)","metadata":{"id":"qc2U7Vaj1sm3","outputId":"16b22153-141d-4122-86cd-1de08eb42f58","execution":{"iopub.status.busy":"2021-07-22T17:17:10.552721Z","iopub.execute_input":"2021-07-22T17:17:10.553168Z","iopub.status.idle":"2021-07-22T17:17:10.581168Z","shell.execute_reply.started":"2021-07-22T17:17:10.553122Z","shell.execute_reply":"2021-07-22T17:17:10.580185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (columnName, columnData) in doc_topic_df.iteritems():\n  print(columnData.values) # first column, second column and so on.\n  print(\"\\n\")\n  print(pd.DataFrame(data=columnData.values).sort_values(by=0, ascending=False)) # sort row wise. in descending order","metadata":{"id":"SpU1xlij2IfO","outputId":"fdaa0d60-9bae-41be-ca69-55788310ede0","execution":{"iopub.status.busy":"2021-07-22T17:17:10.58235Z","iopub.execute_input":"2021-07-22T17:17:10.582649Z","iopub.status.idle":"2021-07-22T17:17:10.658313Z","shell.execute_reply.started":"2021-07-22T17:17:10.582618Z","shell.execute_reply":"2021-07-22T17:17:10.657221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (columnName, columnData) in doc_topic_df.iteritems():\n    n = 5\n    print('Topic #', columnName)\n    sorted_topic = pd.DataFrame(data=columnData.values).sort_values(by=0, ascending=False)\n    sorted_topic.columns = [columnName]\n    print(sorted_topic[:n])\n    \n    # store IDs and titles of top articles in a dataframe\n    ids = sorted_topic[:n].index\n    print(\"\\n\")","metadata":{"id":"3mOWLYGy0uKD","outputId":"994b04e7-fc9a-4e3f-acd5-4d6cfd74c702","execution":{"iopub.status.busy":"2021-07-22T17:17:10.65984Z","iopub.execute_input":"2021-07-22T17:17:10.660319Z","iopub.status.idle":"2021-07-22T17:17:10.728891Z","shell.execute_reply.started":"2021-07-22T17:17:10.660263Z","shell.execute_reply":"2021-07-22T17:17:10.727776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the distribution of documents over each topic\nsns.set(rc={'figure.figsize':(10,5)})\ndoc_topic_df.idxmax(axis=1).value_counts().plot.bar(color='lightblue')\n\n#store the distributions in a dataframe\ndistribution = doc_topic_df.idxmax(axis=1).value_counts()\ndistribution","metadata":{"id":"a7Scrhj33Lvs","outputId":"2608589f-3cc8-4aa7-8e41-81622bd836d2","execution":{"iopub.status.busy":"2021-07-22T17:17:10.730445Z","iopub.execute_input":"2021-07-22T17:17:10.730848Z","iopub.status.idle":"2021-07-22T17:17:11.130021Z","shell.execute_reply.started":"2021-07-22T17:17:10.730804Z","shell.execute_reply":"2021-07-22T17:17:11.129082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nReference: https://www.kaggle.com/rababazeem/topic-modeling-lda","metadata":{"id":"lyyn_R2AKp3F"}}]}