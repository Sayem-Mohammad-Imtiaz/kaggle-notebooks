{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing dependencies and setting file paths"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\nimport random\nimport os\nimport glob\nimport cv2 \nfrom fastai.vision import *\nfrom fastai import *\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import plot_confusion_matrix\n\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set seed fol all\ndef seed_everything(seed=1358):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = Path('../input/planets-dataset/planet/planet/')\ntrain_img = PATH/'train-jpg'\ntrain_folder = 'train-jpg'\ntest_img = PATH/'test-jpg'\nmodel_dir = Path('/kaggle/working/')\nbs = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH.ls()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Little bit of EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(PATH, 'train_classes.csv'))\n# adding path to the image in our dataframe. \ntrain_df['image_name'] = train_df['image_name'].apply(lambda x: f'{train_folder}/{x}.jpg')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We have 17 unique labels for this data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since this is a multi lable task and the labels are given as tags in a single dataframe series\nbiner = MultiLabelBinarizer()\ntags = train_df['tags'].str.split()\ny = biner.fit_transform(tags)\n\nlabels = biner.classes_\nprint('Number of labels: ', len(labels))\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the labels into one hot encoded form for EDA ease. \nfor label in labels:\n    train_df[label] = train_df['tags'].apply(lambda x: 1 if label in x.split()  else 0)\n    \ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The label primary appears the most in our dataset followed by clear and agriculture. \nAs stated in the data description, primary refers to primary rainforest.\n> Generally speaking, the \"primary\" label was used for any area that exhibited dense tree cover. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[labels].sum().sort_values(ascending=False).plot(kind='barh', figsize=(8,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the co-ocurrance for these labels. \n> The combination (primary, clear) has the highest co-ocurrance. Followed by (primary, agriculture)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_asint = train_df.drop(train_df.columns[[0,1]], axis=1).astype(int)\ncoocc_df = df_asint.T.dot(df_asint)\n\ncoocc_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a few random images with there labels to see how the data looks. \nChoose 10 random images from the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading images\n\nrandom_imgs = train_df.ix[random.sample(list(train_df.index), 10)][['image_name', 'tags']]\n\nto_read = random_imgs.loc[:, 'image_name'].values\ntags = random_imgs.loc[:, 'tags'].values\n\nimages = [cv2.imread(os.path.join(PATH/file)) for file in to_read]\nprint(\"Number of images: \", len(images))\nprint(\"Size of an image: \", images[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\ncolumns = 5\nfor i, image in enumerate(images):\n    plt.subplot(len(images) / columns + 1, columns, i + 1)\n    plt.imshow(image)\n    plt.grid(False)\n    plt.title(tags[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training "},{"metadata":{},"cell_type":"markdown","source":"Using the usual fast ai to train and evaluate models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Size of Training set images: {len(list(train_img.glob('*.jpg')))}\")\nprint(f\"Size of Test set images: {len(list(test_img.glob('*.jpg')))}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Starting with an image size of 128*128 with a few transformations. \n+ Flipping the image vertically and horizontaly. \n+ Changing lighting and contrast. \n+ Rotations. \n+ Zooming. "},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 128\n\ntfms = get_transforms(do_flip=True,flip_vert=True,p_lighting=0.4,\n                      max_lighting=0.3, max_zoom=1.05, max_rotate=360, xtra_tfms=[flip_lr()])\n\n\n# The datablock API makes things very easy. \n# Im using 1% of the training data to validate the models. \n\nsrc = (ImageList.from_df(train_df, PATH, cols='image_name')\n        .split_by_rand_pct(valid_pct=0.1)\n        .label_from_df(label_delim=' '))\n\n\ndata = (src.transform(tfms,size=img_size,resize_method=ResizeMethod.CROP)\n        .databunch(bs=bs,num_workers=4) \n        .normalize(imagenet_stats)      \n       )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"36432 images for training and 4047 for validating. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train data has 36432 images of size 128x128x3. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.train_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.valid_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.train_ds[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A random point from the databunch. \nThis time i select the labels from **data.train_ds.y** and image from **data.train_ds.x**\n\nThe databunch containing the train dataset has both x and y components and we can index into them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.train_ds.y[200])\ndata.train_ds.x[200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(rows=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Every experiment would have two stages: \n+ 1st stage: Freezing early layers and only fine-tuning the last few newly added layers.\n+ 2nd stage: Unfreezing all the layers and fine-tuning them. \n"},{"metadata":{},"cell_type":"markdown","source":"## Train Kernel 1\n\n> For the first kernel: \n> + Using resnet50 (pretrained on imagenet)\n> + fbeta with 0.2 threshold and accuracy as metrics. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = models.resnet50\nacc_02 = partial(accuracy_thresh, thresh=0.2)\nf_score = partial(fbeta, thresh=0.2)\n\nlearn = create_cnn(data, model_1, metrics=[acc_02, f_score], model_dir='/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With 5 epochs able to get upto 92% fbeta. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.model_dir = '/kaggle/working'\nlearn.save('resnet50-stage1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Kernel 2\n\n> For the second train kernel:\n+ Using DenseNet121 (pretrained on imagenet)\n+ Similar metrics as first experiment. "},{"metadata":{},"cell_type":"markdown","source":"Fine-tuning last layers in Stage 1 "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model_2 = models.densenet121\nlearn_dense = create_cnn(data, model_2, metrics=[acc_02, f_score], model_dir='/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense.fit_one_cycle(5, 0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DenseNet201 performs slighly better than resnet50. \nI ll use this for fine tuning the entire model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense.save('DenseNet121-stage1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine-tuning the entire model in stage 2. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense.unfreeze()\nlearn_dense.lr_find()\nlearn_dense.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense.fit_one_cycle(5, slice(1e-5, 1e-4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense.save('DenseNet121-stage2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Kernel 3\n\nFine tuning the network further with larger image size. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2 = (src.transform(tfms,size=256,resize_method=ResizeMethod.CROP)\n        .databunch(bs=bs,num_workers=4) \n        .normalize(imagenet_stats)      \n       )\n\ndata_2","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"model_2 = models.densenet121\nlearn_dense_2 = create_cnn(data_2, model_2, metrics=[acc_02, f_score], model_dir='/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"learn_dense_2.load('DenseNet121-stage2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense_2.lr_find()\nlearn_dense_2.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense_2.fit_one_cycle(10, 0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense_2.unfreeze()\nlearn_dense_2.lr_find()\nlearn_dense_2.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_dense_2.fit_one_cycle(10, slice(1e-5,1e-4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}