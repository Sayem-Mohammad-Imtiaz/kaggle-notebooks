{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom nltk import word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve, confusion_matrix, classification_report\n\nfrom keras.models import Model, load_model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical, plot_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n# for dirname, _, filenames in os.walk('/kaggle/working'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.countplot(df.v1, ax=ax)\nax.set_xlabel('Label')\nax.set_title('Number of ham and spam messages')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.loc[:, 'v2']\ny = df.loc[:, 'v1']\n\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_data, X_test_data, y_train_labels, y_test_labels = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train_data.shape)\nprint(X_test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find length of each sentence after tokenization\nsent_lens = []\nfor sent in X_train_data:\n    sent_lens.append(len(word_tokenize(sent)))\n    \nprint(max(sent_lens))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sent_lens, bins=10, kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the length of 95% of review text to help in finding max. sequence length.\nnp.quantile(sent_lens, 0.95)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's select maximum sequence length as 38. We see that 95 % of review text are of length less than or equal to 38, so will keep the max length as 38 (38 words more than to tell the sentiment)"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sequence_length = 38\n\ntok = Tokenizer()\ntok.fit_on_texts(X_train_data.values)\n\nvocab_length = len(tok.word_index) #len(tok.word_counts) or len(tok.index_word.keys()) will also give same results\nprint('No. of unique tokens(vocab_size): ', vocab_length)\n\nX_train_sequences = tok.texts_to_sequences(X_train_data.values)\nX_test_sequences = tok.texts_to_sequences(X_test_data.values)\nprint('No of sequences:', len(X_train_sequences)) #No of sequences will be same as the number of training samples\nprint(X_train_sequences[:2])\n\n#make all sequences of equal length\nX_train = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\nX_test = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\nX_train[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_labels.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny_train = le.fit_transform(y_train_labels)\ny_test = le.fit_transform(y_test_labels)\nprint(y_train)\n\n# y_train_le  = y_train_le.reshape(-1, 1)\n# y_test_le  = y_test_le.reshape(-1, 1)\n# print(y_train_le)\n\n# y_train = np.asarray(y_train_le).astype('float32')\n# y_test = np.asarray(y_test_le).astype('float32')\n# print(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(vocab_len, max_seq_len):\n    inputs = Input(name='inputs', shape=[max_seq_len])   #None, 150\n    layer = Embedding(vocab_length + 1, 50, input_length=max_seq_len)(inputs) #None, 150, 50\n    layer = LSTM(64)(layer)  #None, 64\n    layer = Dense(256,name='FC1')(layer) #None, 256\n    layer = Activation('relu')(layer) #None, 256\n    layer = Dropout(0.5)(layer) #None, 256\n    layer = Dense(1,name='out_layer')(layer) #None, 1\n    layer = Activation('sigmoid')(layer) #None, 1\n    model = Model(inputs=inputs,outputs=layer)\n    model.compile(loss='binary_crossentropy',optimizer=RMSprop(), metrics=['acc'])\n    return model\n\nmodel = create_model(vocab_length, max_sequence_length)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the extension and start TensorBoard\n# %load_ext tensorboard\n# %tensorboard --logdir logs\n\nfilepath='model_with_best_weights.h5' #fixed path to save the best model\n# filepath=\"weights-improvement-{epoch:02d}-{val_loss:.4f}.hdf5\" #file path will change based on epoch and loss\n# Checkpointing is setup to save the network weights only when there is an improvement in classification accuracy on the validation dataset (monitor=’val_accuracy’ and mode=’max’). \n# The weights are stored in a file that includes the score in the filename (weights-improvement-{val_accuracy=.2f}.hdf5).\ncallbacks = [EarlyStopping(monitor='val_loss', patience=5, verbose=1),  #EarlyStopping(monitor='val_loss',min_delta=0.0001, patience=5),\n             ModelCheckpoint(filepath=filepath, monitor='val_loss', save_best_only=True, verbose=1),\n#              TensorBoard(log_dir='logs', histogram_freq=1, embeddings_freq=1)             \n            ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\n\n# list all data in history\nprint(history_dict.keys())\n\n# summarize history for loss\nplt.plot(history_dict['loss'])\nplt.plot(history_dict['val_loss'])\nplt.title('Training and Validation Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for accuracy\nplt.plot(history_dict['acc'])\nplt.plot(history_dict['val_acc'])\nplt.title('Training and Validation Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load best model and evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_model = load_model('model_with_best_weights.h5')\ntest_loss, test_acc = accr = loaded_model.evaluate(X_test, y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(test_loss, test_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make class predictions with the model on new data\ny_pred_proba = loaded_model.predict(X_test)\n\n# y_pred = loaded_model.predict_classes(X_test)  #we can't use it on Model object. Can be used on Sequential object\nprint(np.round(y_pred_proba, 3))\ny_pred = y_pred_proba > 0.5\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the first few cases\nfor i in range(5):\n    print('%s => %d (expected %d)' % (X_test[i].tolist(), y_pred[i], y_test[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate the roc auc score\nauc = roc_auc_score(y_test, y_pred_proba)\nprint('AUC: %.3f' % auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the roc curve\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_proba)\n\ndef plot_roc_curve(fpr,tpr): \n  import matplotlib.pyplot as plt\n  plt.plot(fpr,tpr) \n  plt.axis([0,1,0,1]) \n  plt.xlabel('False Positive Rate') \n  plt.ylabel('True Positive Rate') \n  plt.show()    \n  \nplot_roc_curve (fpr_keras, tpr_keras)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References:\n* https://www.kaggle.com/kredy10/simple-lstm-for-text-classification\n* https://www.kaggle.com/nihalbey/spam-detection-and-deep-nlp\n* https://www.kaggle.com/uday44/simple-text-classification-tokenization-embedding"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}