{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install plotly\n!pip install chainer","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:33:40.536913Z","iopub.execute_input":"2021-09-07T17:33:40.537636Z","iopub.status.idle":"2021-09-07T17:34:02.977722Z","shell.execute_reply.started":"2021-09-07T17:33:40.537458Z","shell.execute_reply":"2021-09-07T17:34:02.976565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport copy\nimport numpy as np\nimport pandas as pd\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nfrom plotly import tools, subplots\nfrom plotly.graph_objs import *\nfrom plotly.offline import init_notebook_mode, iplot, iplot_mpl\ninit_notebook_mode()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:02.980983Z","iopub.execute_input":"2021-09-07T17:34:02.981314Z","iopub.status.idle":"2021-09-07T17:34:06.792905Z","shell.execute_reply.started":"2021-09-07T17:34:02.981283Z","shell.execute_reply":"2021-09-07T17:34:06.791615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/stock-prices/all_stocks_5yr.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:06.79545Z","iopub.execute_input":"2021-09-07T17:34:06.796194Z","iopub.status.idle":"2021-09-07T17:34:07.64198Z","shell.execute_reply.started":"2021-09-07T17:34:06.796138Z","shell.execute_reply":"2021-09-07T17:34:07.640869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:07.6436Z","iopub.execute_input":"2021-09-07T17:34:07.643911Z","iopub.status.idle":"2021-09-07T17:34:07.677153Z","shell.execute_reply.started":"2021-09-07T17:34:07.643882Z","shell.execute_reply":"2021-09-07T17:34:07.676081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:07.67847Z","iopub.execute_input":"2021-09-07T17:34:07.678829Z","iopub.status.idle":"2021-09-07T17:34:07.695173Z","shell.execute_reply.started":"2021-09-07T17:34:07.678795Z","shell.execute_reply":"2021-09-07T17:34:07.694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Name'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:07.697031Z","iopub.execute_input":"2021-09-07T17:34:07.697356Z","iopub.status.idle":"2021-09-07T17:34:07.848387Z","shell.execute_reply.started":"2021-09-07T17:34:07.697323Z","shell.execute_reply":"2021-09-07T17:34:07.847611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Name']","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:07.849583Z","iopub.execute_input":"2021-09-07T17:34:07.849998Z","iopub.status.idle":"2021-09-07T17:34:07.860405Z","shell.execute_reply.started":"2021-09-07T17:34:07.849954Z","shell.execute_reply":"2021-09-07T17:34:07.859236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(data[data[\"Name\"]==\"AAL\"][\"open\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:07.864435Z","iopub.execute_input":"2021-09-07T17:34:07.86493Z","iopub.status.idle":"2021-09-07T17:34:08.150785Z","shell.execute_reply.started":"2021-09-07T17:34:07.864883Z","shell.execute_reply":"2021-09-07T17:34:08.149802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(data[data[\"Name\"]==\"AAL\"][\"high\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:08.152709Z","iopub.execute_input":"2021-09-07T17:34:08.153021Z","iopub.status.idle":"2021-09-07T17:34:08.406991Z","shell.execute_reply.started":"2021-09-07T17:34:08.152989Z","shell.execute_reply":"2021-09-07T17:34:08.405648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(data[data[\"Name\"]==\"AAL\"][\"close\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:08.408593Z","iopub.execute_input":"2021-09-07T17:34:08.409047Z","iopub.status.idle":"2021-09-07T17:34:08.658258Z","shell.execute_reply.started":"2021-09-07T17:34:08.409002Z","shell.execute_reply":"2021-09-07T17:34:08.657229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(data[data[\"Name\"]==\"AAL\"][\"volume\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:08.659751Z","iopub.execute_input":"2021-09-07T17:34:08.660047Z","iopub.status.idle":"2021-09-07T17:34:08.925173Z","shell.execute_reply.started":"2021-09-07T17:34:08.660018Z","shell.execute_reply":"2021-09-07T17:34:08.923994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.set_index('date')\ndata","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:08.926791Z","iopub.execute_input":"2021-09-07T17:34:08.927225Z","iopub.status.idle":"2021-09-07T17:34:08.991286Z","shell.execute_reply.started":"2021-09-07T17:34:08.927178Z","shell.execute_reply":"2021-09-07T17:34:08.989963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data[data['Name']=='AAL']\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:08.992929Z","iopub.execute_input":"2021-09-07T17:34:08.993254Z","iopub.status.idle":"2021-09-07T17:34:09.113353Z","shell.execute_reply.started":"2021-09-07T17:34:08.993224Z","shell.execute_reply":"2021-09-07T17:34:09.11223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:09.114811Z","iopub.execute_input":"2021-09-07T17:34:09.115153Z","iopub.status.idle":"2021-09-07T17:34:09.850972Z","shell.execute_reply.started":"2021-09-07T17:34:09.115113Z","shell.execute_reply":"2021-09-07T17:34:09.849995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var = input(\"Enter Stock Name: \")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:09.854735Z","iopub.execute_input":"2021-09-07T17:34:09.855058Z","iopub.status.idle":"2021-09-07T17:34:35.643852Z","shell.execute_reply.started":"2021-09-07T17:34:09.855029Z","shell.execute_reply":"2021-09-07T17:34:35.6429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data[data['Name']==var] # here you can change the name of the stock, instead of AAL, you can put ZTS for eg\nX = df[[\"open\",\"high\",\"low\",\"close\"]]\nY = df[\"Name\"]\ntrain,test,y_train,y_test= train_test_split(X,Y)\ndate_split = '2016-01-01'","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:35.645146Z","iopub.execute_input":"2021-09-07T17:34:35.645434Z","iopub.status.idle":"2021-09-07T17:34:35.746421Z","shell.execute_reply.started":"2021-09-07T17:34:35.645407Z","shell.execute_reply":"2021-09-07T17:34:35.745137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Environment1:\n    \n    def __init__(self, df, history_t=90):\n        self.data = df\n        self.history_t = history_t\n        self.reset()\n        \n    def reset(self):\n        self.t = 0\n        self.done = False\n        self.profits = 0\n        self.positions = []\n        self.position_value = 0\n        self.history = [0 for _ in range(self.history_t)]\n        return [self.position_value] + self.history # obs\n    \n    def step(self, act):\n        reward = 0\n        \n        # act = 0: stay, 1: buy, 2: sell\n        if act == 1:\n            self.positions.append(self.data.iloc[self.t, :]['close'])\n        elif act == 2: # sell\n            if len(self.positions) == 0:\n                reward = -1\n            else:\n                profits = 0\n                for p in self.positions:\n                    profits += (self.data.iloc[self.t, :]['close'] - p)\n                reward += profits\n                self.profits += profits\n                self.positions = []\n        \n        # set next time\n        self.t += 1\n        self.position_value = 0\n        for p in self.positions:\n            self.position_value += (self.data.iloc[self.t, :]['close'] - p)\n        self.history.pop(0)\n        self.history.append(self.data.iloc[self.t, :]['close'] - self.data.iloc[(self.t-1), :]['close'])\n        \n        # clipping reward\n        if reward > 0:\n            reward = 1\n        elif reward < 0:\n            reward = -1\n        \n        return [self.position_value] + self.history, reward, self.done # obs, reward, done","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:35.747945Z","iopub.execute_input":"2021-09-07T17:34:35.748244Z","iopub.status.idle":"2021-09-07T17:34:35.762908Z","shell.execute_reply.started":"2021-09-07T17:34:35.748215Z","shell.execute_reply":"2021-09-07T17:34:35.761565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = Environment1(train)\nprint(env.reset())\nfor _ in range(3):\n    pact = np.random.randint(3)\n    print(env.step(pact))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:35.764496Z","iopub.execute_input":"2021-09-07T17:34:35.764974Z","iopub.status.idle":"2021-09-07T17:34:35.782289Z","shell.execute_reply.started":"2021-09-07T17:34:35.764927Z","shell.execute_reply":"2021-09-07T17:34:35.78098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DQN\n\ndef train_dqn(env):\n\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, output_size)\n            )\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            y = self.fc3(h)\n            return y\n\n        def reset(self):\n            self.zerograds()\n\n    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n    Q_ast = copy.deepcopy(Q)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(Q)\n\n    epoch_num = 60\n    step_max = len(env.data)-1\n    memory_size = 200\n    batch_size = 20\n    epsilon = 1.0\n    epsilon_decrease = 1e-3\n    epsilon_min = 0.1\n    start_reduce_epsilon = 200\n    train_freq = 10\n    update_q_freq = 20\n    gamma = 0.97\n    show_log_freq = 5\n\n    memory = []\n    total_step = 0\n    total_rewards = []\n    total_losses = []\n\n    start = time.time()\n    for epoch in range(epoch_num):\n\n        pobs = env.reset()\n        step = 0\n        done = False\n        total_reward = 0\n        total_loss = 0\n\n        while not done and step < step_max:\n\n            # select act\n            pact = np.random.randint(3)\n            if np.random.rand() > epsilon:\n                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n                pact = np.argmax(pact.data)\n\n            # act\n            obs, reward, done = env.step(pact)\n\n            # add memory\n            memory.append((pobs, pact, reward, obs, done))\n            if len(memory) > memory_size:\n                memory.pop(0)\n\n            # train or update q\n            if len(memory) == memory_size:\n                if total_step % train_freq == 0:\n                    shuffled_memory = np.random.permutation(memory)\n                    memory_idx = range(len(shuffled_memory))\n                    for i in memory_idx[::batch_size]:\n                        batch = np.array(shuffled_memory[i:i+batch_size])\n                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_done = np.array(batch[:, 4].tolist(), dtype=bool)\n\n                        q = Q(b_pobs)\n                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n                        target = copy.deepcopy(q.data)\n                        for j in range(batch_size):\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n                        Q.reset()\n                        loss = F.mean_squared_error(q, target)\n                        total_loss += loss.data\n                        loss.backward()\n                        optimizer.update()\n\n                if total_step % update_q_freq == 0:\n                    Q_ast = copy.deepcopy(Q)\n\n            # epsilon\n            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n                epsilon -= epsilon_decrease\n\n            # next step\n            total_reward += reward\n            pobs = obs\n            step += 1\n            total_step += 1\n\n        total_rewards.append(total_reward)\n        total_losses.append(total_loss)\n\n        if (epoch+1) % show_log_freq == 0:\n            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n            elapsed_time = time.time()-start\n            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n            start = time.time()\n            \n    return Q, total_losses, total_rewards\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:35.78382Z","iopub.execute_input":"2021-09-07T17:34:35.784292Z","iopub.status.idle":"2021-09-07T17:34:35.936936Z","shell.execute_reply.started":"2021-09-07T17:34:35.784162Z","shell.execute_reply":"2021-09-07T17:34:35.935838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Q, total_losses, total_rewards = train_dqn(Environment1(train))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:34:35.93833Z","iopub.execute_input":"2021-09-07T17:34:35.938674Z","iopub.status.idle":"2021-09-07T17:47:29.401624Z","shell.execute_reply.started":"2021-09-07T17:34:35.938624Z","shell.execute_reply":"2021-09-07T17:47:29.400519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_train_test_by_q(train_env, test_env, Q, algorithm_name):\n    \n    # train\n    pobs = train_env.reset()\n    train_acts = []\n    train_rewards = []\n\n    for _ in range(len(train_env.data)-1):\n        \n        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n        pact = np.argmax(pact.data)\n        train_acts.append(pact)\n            \n        obs, reward, done = train_env.step(pact)\n        train_rewards.append(reward)\n\n        pobs = obs\n        \n    train_profits = train_env.profits\n    \n    # test\n    pobs = test_env.reset()\n    test_acts = []\n    test_rewards = []\n\n    for _ in range(len(test_env.data)-1):\n    \n        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n        pact = np.argmax(pact.data)\n        test_acts.append(pact)\n            \n        obs, reward, done = test_env.step(pact)\n        test_rewards.append(reward)\n\n        pobs = obs\n        \n    test_profits = test_env.profits\n    \n    # plot\n    train_copy = train_env.data.copy()\n    test_copy = test_env.data.copy()\n    train_copy['act'] = train_acts + [np.nan]\n    train_copy['reward'] = train_rewards + [np.nan]\n    test_copy['act'] = test_acts + [np.nan]\n    test_copy['reward'] = test_rewards + [np.nan]\n    train0 = train_copy[train_copy['act'] == 0]\n    train1 = train_copy[train_copy['act'] == 1]\n    train2 = train_copy[train_copy['act'] == 2]\n    test0 = test_copy[test_copy['act'] == 0]\n    test1 = test_copy[test_copy['act'] == 1]\n    test2 = test_copy[test_copy['act'] == 2]\n    act_color0, act_color1, act_color2 = 'gray', 'cyan', 'magenta'\n\n    data = [\n        Candlestick(x=train0.index, open=train0['open'], high=train0['high'], low=train0['low'], close=train0['close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=train1.index, open=train1['open'], high=train1['high'], low=train1['low'], close=train1['close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=train2.index, open=train2['open'], high=train2['high'], low=train2['low'], close=train2['close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2))),\n        Candlestick(x=test0.index, open=test0['open'], high=test0['high'], low=test0['low'], close=test0['close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=test1.index, open=test1['open'], high=test1['high'], low=test1['low'], close=test1['close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=test2.index, open=test2['open'], high=test2['high'], low=test2['low'], close=test2['close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n    ]\n    title = '{}: train s-reward {}, profits {}, test s-reward {}, profits {}'.format(\n        algorithm_name,\n        int(sum(train_rewards)),\n        int(train_profits),\n        int(sum(test_rewards)),\n        int(test_profits)\n    )\n    layout = {\n        'title': title,\n        'showlegend': False,\n         'shapes': [\n             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n         ],\n        'annotations': [\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n        ]\n    }\n    figure = Figure(data=data, layout=layout)\n    iplot(figure)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:47:29.40341Z","iopub.execute_input":"2021-09-07T17:47:29.404128Z","iopub.status.idle":"2021-09-07T17:47:29.444589Z","shell.execute_reply.started":"2021-09-07T17:47:29.404081Z","shell.execute_reply":"2021-09-07T17:47:29.443345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_train_test_by_q(Environment1(train), Environment1(test), Q, 'DQN')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:47:29.446408Z","iopub.execute_input":"2021-09-07T17:47:29.447201Z","iopub.status.idle":"2021-09-07T17:47:33.7292Z","shell.execute_reply.started":"2021-09-07T17:47:29.44715Z","shell.execute_reply":"2021-09-07T17:47:33.728064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('stock_price_prediction_model.pickle','wb') as f:\n    pickle.dump(train_dqn,f)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:47:33.732075Z","iopub.execute_input":"2021-09-07T17:47:33.732419Z","iopub.status.idle":"2021-09-07T17:47:33.737269Z","shell.execute_reply.started":"2021-09-07T17:47:33.732379Z","shell.execute_reply":"2021-09-07T17:47:33.7362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\ncolumns = {\n    'data_columns' : [col.lower() for col in Y.columns]\n}\nwith open(\"columns.json\",\"w\") as f:\n    f.write(json.dumps(columns))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T17:47:33.738614Z","iopub.execute_input":"2021-09-07T17:47:33.738921Z","iopub.status.idle":"2021-09-07T17:47:33.960572Z","shell.execute_reply.started":"2021-09-07T17:47:33.738891Z","shell.execute_reply":"2021-09-07T17:47:33.95893Z"},"trusted":true},"execution_count":null,"outputs":[]}]}