{"cells":[{"metadata":{},"cell_type":"markdown","source":"\nhttps://www.kaggle.com/datatattle/covid-19-nlp-text-classification\n\nColumns:\n\n1) Location\n\n2) Tweet At\n\n3) Original Tweet\n\n4) Label"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport matplotlib.pyplot as plt # plotting\nimport matplotlib.image as mpimg # images\nimport numpy as np #numpy\nimport seaborn as sns\nimport tensorflow.compat.v2 as tf #use tensorflow v2 as a main \nimport tensorflow.keras as keras # required for high level applications\nfrom sklearn.model_selection import train_test_split # split for validation sets\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import normalize # normalization of the matrix\nimport scipy\nimport pandas as pd\nimport unicodedata, re, string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(r'/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='latin_1')\ntest_df = pd.read_csv(r'/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding='latin_1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_history(history):\n    plt.figure()\n    for key in history.history.keys():\n        plt.plot(history.epoch, history.history[key], label=key)\n    plt.legend()\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mish(keras.layers.Activation):\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\nkeras.utils.get_custom_objects().update({'mish': Mish(mish)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pro řešení problemu jsem se rozhodl využít pouze text a proto Dále již nepracuji s datem lokací nebo se jmény.\nDále jsem si upravil třidy kde jsem původních 5 třid rozdělil na 2 pozitivní a negativní s tím že neutrálni jsem zařadil do skupiny s pozitivními"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\ntest_df = test_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\ntrain_df[\"Sentiment\"].replace({\"Neutral\": 1, \"Positive\": 1, \"Extremely Negative\": 0, \"Negative\": 0, \"Extremely Positive\": 1}, inplace=True)\ntest_df[\"Sentiment\"].replace({\"Neutral\": 1, \"Positive\": 1, \"Extremely Negative\": 0, \"Negative\": 0, \"Extremely Positive\": 1}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Sentiment', data=train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Počet pozitivních a počet a negativních zpráv"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['length'] = train_df.OriginalTweet.apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='Sentiment', y='length', data = train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ukazka originálních zpráv"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in train_df.loc[:10, 'OriginalTweet']:\n    print(x)\n    print('---------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nMetody a funkce pro upravení textu v tweetu"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_users_http(word):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    word = re.sub(r'http\\S+', '', word)\n    word = re.sub(r'@\\w+', '', word)\n    word = re.sub(r'#\\w+', '', word)\n    return word\n\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_numbers(words):\n    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(\"\\d+\", \"\", word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = remove_numbers(words)\n    words = remove_stopwords(words)\n    return words\n\ndef form_sentence(tweet):\n    tweet = remove_users_http(tweet)\n    tweet_blob = TextBlob(tweet)\n    return tweet_blob.words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Clean_text'] = train_df['OriginalTweet'].apply(form_sentence)\ntest_df['Clean_text'] = test_df['OriginalTweet'].apply(form_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Clean_text'] = train_df['Clean_text'].apply(normalize)\ntest_df['Clean_text'] = test_df['Clean_text'].apply(normalize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_nt(words):\n    st_res = []\n    for i in range(0, len(words) - 1):\n        if words[i+1] == \"n't\" or words[i+1] == \"nt\":\n            st_res.append(words[i]+(\"n't\"))\n        else:\n            if words[i] != \"n't\" and words[i] != \"nt\":\n                st_res.append(words[i])\n    return st_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Clean_text'] = train_df['Clean_text'].apply(fix_nt)\ntest_df['Clean_text'] = test_df['Clean_text'].apply(fix_nt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Clean_text'] = train_df['Clean_text'].apply(lambda x: \" \".join(x))\ntest_df['Clean_text'] = test_df['Clean_text'].apply(lambda x: \" \".join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = list(itertools.chain(*train_df.Clean_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist = nltk.FreqDist(all_words)\ndist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(train_df.Clean_text.apply(len))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import string as tf_string\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 128 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\nvocab_size = 10000 # Number of unique tokens in vocabulary\nsequence_length = 30 # Output dimension after vectorizing - words in vectorited representation are independent\n\nvect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\nvect_layer.adapt(train_df.Clean_text.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rozdělení trenovací sady sady na trenovaci a validační"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train_df.Clean_text, train_df.Sentiment, test_size=0.1, random_state=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"připravení testovací sady"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df.Clean_text\ny_test = test_df.Sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Vocabulary example: ', vect_layer.get_vocabulary()[:10])\nprint('Vocabulary shape: ', len(vect_layer.get_vocabulary()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.compat.v1.keras.layers import CuDNNGRU, CuDNNLSTM\nfrom tensorflow.keras.layers import LSTM, GRU, Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\nx_v = vect_layer(input_layer)\nemb = keras.layers.Embedding(vocab_size, embedding_dim)(x_v)\nx = LSTM(64, activation='mish', return_sequences=True)(emb)\nx = GRU(64, activation='mish', return_sequences=True)(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(128, 'mish')(x)\nx = keras.layers.Dense(64, 'mish')(x)\nx = keras.layers.Dense(32, 'mish')(x)\nx = keras.layers.Dropout(0.2)(x)\noutput_layer = keras.layers.Dense(1, 'sigmoid')(x)\n\nmodel = keras.Model(input_layer, output_layer)\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop', loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n\nbatch_size = 128\nepochs = 7\nhistory = model.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), callbacks=[es], epochs=epochs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_loss, accuracy = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = [1 if x >= 0.5 else 0 for x in y_pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_true=y_test, y_pred=y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_true=y_test, y_pred=y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_true=y_test, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_true=y_test, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}