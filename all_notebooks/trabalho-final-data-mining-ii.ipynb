{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img style=\"float: left;\" src=\"http://sindser.org.br/s/wp-content/uploads/2013/09/iesb1.jpg\"  width=\"400\" height=\"400\">\n\n## Instituto de Educação Superior de Brasília\n## Pós Graduação em Ciência de Dados\n## Data Mining e Machine Learning II\n## Leandro Alencar – 1931133007"},{"metadata":{},"cell_type":"markdown","source":"----\n# Modelos de Classificação por Árvore de Decisão\n## Table of contents\n* [Introdução](#introducao)\n* [Dados](#data)\n* [Análises Descritivas](#analises)\n* [Modelo](#model)\n* [Conclusão](#conclusao)"},{"metadata":{},"cell_type":"markdown","source":"# 1. Introdução <a name=\"introducao\"></a>\nEste trabalho consiste na análise da base de dados HMEQ (Home Equity), que é composta por informações de quase 6 mil operações de crédito, e desenvolvimento de um modelo que será capaz de indicar se um indivíduo irá honrar o compromisso até o final ou não, de forma a subsidiar a tomada de decisão no processo de aprovação de linha de crédito para um novo cliente de uma entidade financeira ou afim. Os modelos baseados em árvore de decisão escolhidos são: \n> Random Forest Classifier  \nExtra Trees Classifier"},{"metadata":{},"cell_type":"markdown","source":"# 2. Dados <a name=\"data\"></a>\nA base de dados está disponível em https://www.kaggle.com/ajay1735/hmeq-data, e contém 5.960 regitros de empréstimos com apenas 13 categorias de dados, sendo elas:\n\n* BAD : se o cliente faltou com o pagamento do empréstimo (1 = Sim)\n* LOAN : Valor solicitado de empréstimo\n* MORTDUE : Amount due on existing mortgage\n* VALUE : Valor atual da propriedade\n* REASON : Razão da solicitação (DebtCon: consolidação de débitos | HomeImp: melhoria na casa)\n* JOB : Seis categoria ocupacionais\n* YOJ : Tempo no trabalho atual em anos\n* DEROG : Quantidade de principais relatórios depreciativos\n* DELINQ : Quantidade de linhas de crédito inadimplentes\n* CLAGE : Idade da linha de negociaação mais antiga em meses\n* NINQ : Quantidade de linhas de crédito recentes\n* CLNO : Quantidade de linhas de crédito\n* DEBTINC : razão débito sobre receita \n----\n"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Bibliotecas e dependências do Python:\n* Pandas – Manipulação e análise de dados \n* Matplotlib – Visualização gráfica\n* Seaborn - Visualização gráfica         \n* Scikit-learn - Aprendizado de máquina"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.display.max_columns = 200\npd.options.display.max_rows = 100\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nimport  warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Análises Exploratórias<a name=\"analises\"></a>\nPrimeiro realizou-se a visualização rápida das principais informações (missings, formato de dados e balanceamento) para levantamento das estratégias de preprocessamento. As colunas foram importadas corretamente quanto ao seu formato, porém foi verificada uma grande quantidade de missings, inclusive registros com apenas 1 valor preenchido e desbalanceamento das classes da variável resposta \"BAD\".\n\nEssas inconsistências serao tratadas em etapa posterior.\n\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file = '/kaggle/input/hmeq-data/hmeq.csv'\ndf = pd.read_csv(file)\n# Visualização de informações gerais e missings\ndf.info()\n\n# Visualização da distribuição de classes\nprint('\\nDistribuição da Categoria BAD:\\n', df['BAD'].value_counts(normalize=True))\n\n# Visualização das 5 primeiras linhas do dataset\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Visualização Gráfica\nA fim de verificar presença de viés, optou-se pela ilustração gráfica das variáveis numéricas e categórias distribuídas pela variável resposta, \"BAD\", e ficou demonstrado que no caso das numéricas, e com exceção de \"DEROG\" e \"DELINQ\", estão bem distribuídas. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,5, figsize=(30,15))\nrow_1 = ['LOAN','MORTDUE','VALUE','YOJ','DEROG']\nrow_2 = ['DELINQ','CLAGE','NINQ','CLNO','DEBTINC']\nrow_3 = []\nfor i, col in enumerate(row_1):\n    sns.boxplot(x=df['BAD'], y=df[col], ax=ax[0,i])\nfor i, col in enumerate(row_2):\n    sns.boxplot(x=df['BAD'], y=df[col], ax=ax[1,i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No caso da variável categórica \"REASON\", para cada classe existente a distribuição entre \"BAD = 1\" e \"BAD = 0\" é semelhante, em torno de 20% das observações são do tipo que não honram com o pagamento do empréstimo."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,7))\nax = sns.barplot(x=\"REASON\", y=\"BAD\", hue='BAD', data=df, estimator=lambda x: len(x))\nax.annotate(df['REASON'][(df['REASON'] == 'HomeImp') & (df['BAD'] == 0)].shape[0], xy=(0.11, 0.42), xycoords=\"axes fraction\")\nax.annotate(df['REASON'][(df['REASON'] == 'HomeImp') & (df['BAD'] == 1)].shape[0], xy=(0.32, 0.12), xycoords=\"axes fraction\")\nax.annotate(df['REASON'][(df['REASON'] == 'DebtCon') & (df['BAD'] == 0)].shape[0], xy=(0.61, 0.97), xycoords=\"axes fraction\")\nax.annotate(df['REASON'][(df['REASON'] == 'DebtCon') & (df['BAD'] == 1)].shape[0], xy=(0.82, 0.23), xycoords=\"axes fraction\")\nplt.ylabel('Count')\nplt.tick_params(\n    axis='y',          \n    left=False,\n    labelleft=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Valores missing\nA primeira estratégia adotada foi de eliminar as linhas que apresentavam mais da metade das colunas com campo missing, uma vez que esses registros não correponderiam à verdade caso realizássemos a introdução de novos valores baseado em medidas de agrupamento (média, moda, etc). Esse passo resultou na redução de 126 linhas. \n\nPosteriormente o tipo de tratamento foi realizado com base nos dois tipos de dados: substituição dos missings pela moda, para os categóricos; preencimento dos missings por \"-1\", para os numéricos. A moda foi escolhida por uma questão estatística, imaginando que por se refletir um comportamento da sociedade, seria mais provavel que aquele indivíduo sem informação pertencesse ao grupo de maior predominância, e também não resultaria em um enviesamento dos dados, pois a quantidade desses registros nulos era bem pequena. Para os dados numéricos, o método foi pensado tendo em mente o algorítimo escolhido para solucionar o problema, um algorítimo de arvore de decisão. Optou-se, então, pelo preenchimento por um valor que não influenciasse o resultado deste algorítmo, por isso a escolha do \"-1\".\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(thresh=df.shape[1]/2, axis=0)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Tratando colunas categóricas\n# =============================================================================\nfor col in df.select_dtypes(include='object').columns:\n    if df[col].isna().sum() > 0:\n         df[col].fillna(df[col].mode()[0], inplace=True)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Tratando colunas numéricas\n# =============================================================================\nfor col in df.select_dtypes(exclude='object').columns:\n    if df[col].isna().sum() > 0:\n        df[col].fillna(-1, inplace=True)      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Resultado\n# =============================================================================\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Balanceamento de Classes\nO balanceamento de classes é extremamente importante para a precisão do nosso modelo, e para o método escolhido foi o de \"upsampling\" da classe de menor frequência por meio de uma função do pacote Scikit-Learn.\n\nAbaixo uma função que irá mostrar a distribuição das classes em todas as variáveis categóricas, mas só iremos balancear a variável resposta, \"BAD\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"def showBalance(df, col):\n    for c in col:\n        print('Distribuição da Coluna: ', c,'\\n',df[c].value_counts(normalize=True),'\\n')\n    else:\n       pass\n        \nshowBalance(df, col=['REASON','JOB','BAD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Função para realizar balanceamento\n# =============================================================================\ndef balance(df, col):\n    df_maior = df[df[col] == df[col].mode()[0]]\n    df_menor = df[df[col] != df[col].mode()[0]]\n \n    # Upsample da menor classe\n    df_menor_upsampled = resample(df_menor, \n                                  replace=True,     \n                                  n_samples=df_maior.shape[0],\n                                  random_state=42) \n \n    # Combinar as classe predominante com a menor classe aumentada\n    df_upsample = pd.concat([df_maior, df_menor_upsampled])\n\n    # Display new class counts\n    print('Contagem de registros')\n    print(df_upsample['BAD'].value_counts())\n    print('\\nDistribuição dos registros')\n    print(df_upsample['BAD'].value_counts(normalize=True))\n\n    return df_upsample\n    \ndf_upsample = balance(df, 'BAD')\nprint('\\n')\nshowBalance(df_upsample, col=['REASON','JOB','BAD'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualização da distribuição de frequência\nAbaixo podemos ver como ficou a distribuição de frequência de todas as variáveis após o processo de tratamento de dados."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_upsample.hist(figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Dummies\nOutra etapa realizada foi a de transoformar as variáveis categóricas em variáveis \"Dummy\", para que o modelo possa entender os pesos de cada classes dentro das categorias e ser capaz de prever corretamente a resposta."},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = ['REASON', 'JOB']\ndf_upsample = pd.get_dummies(df_upsample, columns=dummies, drop_first=True, dtype='int64')\n\ncols = df_upsample.columns.tolist()\ncols = cols[1:] + cols[:1]\ndf_upsample = df_upsample[cols]\ndel cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O dataset ficou, então, com um total de 9.346 observações e 17 colunas após todos os tratamentos realizados."},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Observação sobre o dataset rebalanceado\n# =============================================================================\ndf_upsample.info()\ndf_upsample.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modelo <a name=\"model\"></a>\nPara realozação da tarefa de previsão serão utilizados um algorítimo de Random Forest e um de Extra Trees, ambos baseados em modelos de árvore de decisão.\nRandom Forest consiste em um método que utiliza diversas árvores de decisão durante o treinamento e retorna a média dos resultados individuais dessas árvores, uma grande vantagem desse modelo é a diminuição da possibilidade de overfitting.\n\nExtra trees, de Extremely Randomized Trees ou Árvores Extremamente Aleatóreas, tem como principal diferencial a aleatoriadade dos parâmetros do algorítimo. Esse fator resulta em um modelo com baixa variância e é indicado principalmente para problemas com alta dimensionalidade."},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Divisão Treino e Teste\n# =============================================================================\ntrain, test = train_test_split(df_upsample, test_size=0.1, random_state=0)\n# =============================================================================\n# X e Y dos dados de treino\n# =============================================================================\nX = train.iloc[:,:-1].values\ny = train.iloc[:,-1].values\n# =============================================================================\n# Divisão Treino e Validação\n# =============================================================================\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Random Forest Classifier\n# =============================================================================\n''' Scikit-Learn Definition\nA random forest is a meta estimator that fits a number of decision tree classifiers \non various sub-samples of the dataset and uses averaging to improve the predictive \naccuracy and control over-fitting. The sub-sample size is always the same as the \noriginal input sample size but the samples are drawn with replacement if bootstrap=True (default).\n'''\nrf = RandomForestClassifier(n_estimators=700, criterion='entropy', random_state=42)\n\n# =============================================================================\n# Extra Trees Classifier\n# =============================================================================\n''' Scikit-Learn Definition\nThis class implements a meta estimator that fits a number of randomized \ndecision trees (a.k.a. extra-trees) on various sub-samples of the dataset \nand uses averaging to improve the predictive accuracy and control over-fitting.\n'''\nextc = ExtraTreesClassifier(n_estimators=700,\n                            criterion='entropy',\n                            min_samples_split=5,\n                            max_depth=50,\n                            min_samples_leaf=5,\n                            random_state=42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Random Forest\n# =============================================================================\nrf.fit(X_train, y_train)\ny_rfpred = rf.predict(X_test)\n\n# =============================================================================\n# Extra Trees\n# =============================================================================\nextc.fit(X_train, y_train)\ny_extcpred = extc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Dados de Validação\n# =============================================================================\nprint('Validação Random Forest....Acurácia de: ', \"{0:.1f}\".format(accuracy_score(y_test, y_rfpred)*100),'%')\nprint('Validação Extra Trees......Acurácia de: ', \"{0:.1f}\".format(accuracy_score(y_test, y_extcpred)*100),'%')\n\n# =============================================================================\n# Dados de Teste\n# =============================================================================\ny2_rf = rf.predict(test.iloc[:,:-1].values)\ny2_extc = extc.predict(test.iloc[:,:-1].values)\nprint('\\nTeste Random Forest....Acurácia de: ', \"{0:.1f}\".format(accuracy_score(test.iloc[:,-1].values, y2_rf)*100),'%')\nprint('Teste Extra Trees......Acurácia de: ', \"{0:.1f}\".format(accuracy_score(test.iloc[:,-1].values, y2_extc)*100),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validação Cruzada\nPara confirmação do desempenho do modelo, iremos utilizar a validação cruzada em 5 agrupamentos de dados aleatórios a fim de verificar o comportamento do mesmo modelo ao lidar com observaçoes diferentes."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(rf, X_train, y_train, n_jobs=-1, cv=5)\nprint('Scores: ', [round(x,2) for x in scores])\nprint('Score médio: ', round(scores.mean(),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(extc, X_train, y_train, n_jobs=-1, cv=5)\nprint('Scores: ', [round(x,2) for x in scores])\nprint('Score médio: ', round(scores.mean(),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Desempenho\nO desempenho de ambos os modelos foi muito semelhante e embora, inicialmente, tenha parecido que o modelo Extra Trees havia levado pequena vantagem sobre Random Forest, a validação cruzada mostrou que este fato pode ter ocorrido por uma casualidade da amostra selecionada, visto que em 5 testes aleatórios posteriores o Random Forest se mostrou superior. É interessante também olhar o nível de importância que cada modelo definiu para as variáveis do dataset por meio do gráfico abaixo."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(16,10), sharex=True)\nrf_feat_importances = pd.Series(rf.feature_importances_, index=df_upsample.iloc[:,:-1].columns)\nex_feat_importances = pd.Series(extc.feature_importances_, index=df_upsample.iloc[:,:-1].columns)\n\nrf_feat_importances.plot(kind='barh', ax=ax[0], title='Random Forest')\nex_feat_importances.plot(kind='barh', ax=ax[1], title='Extra Trees')\n\nplt.xlim((0.0, 0.35))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por fim, a Matriz de Confusão nos mostra que o modelo de Random Forest desempenhou de maneira exímia, com altíssimos índices de Sensibilidade e Especificidade."},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_rfpred, labels=[1, 0])\n'''\ndefault:\n[[TN, FP,\n  FN, TP]]\n  \nlabels = [1,0]:\n[[TP, FP,\n  FN, TN]]\n'''\n\nfig, ax = plt.subplots(figsize=(7,6))\nsns.heatmap(cm, annot=True, ax=ax, fmt='.0f'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('Real labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['1', '0'])\nax.yaxis.set_ticklabels(['1', '0']);\nplt.show()\n\nsens = cm[0,0] / (cm[0,0] + cm[1,0])\nesp = cm[1,1] / (cm[0,1] + cm[1,1])\n\nprint('Sensibilidade: ', round(sens,2))\nprint('Especificidade: ', round(esp,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Conclusão <a name=\"conclusao\"></a>\n\nEmbora os dados iniciais possuíssem muitos valores missings, o tratamento de dados não apresentou grandes dificuldades e, passada essa etapa, foram bem tranquilas as etapas seguintes de transformação de variáveis dummies e treinamento do modelo. Também foi muito interessante perceber que a aleatoriedade acrescentada pela validação cruzada enriquece bastante a avaliação do melhor modelo, pois a seleção inicial dos dados pode acarretar em um viés escondido. Por fim, o modelo de Random Forest demmonstrou ótimo desempenho, ratificado pela validação cruzada e pela matriz de confusão."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}