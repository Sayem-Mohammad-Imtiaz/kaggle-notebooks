{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom sklearn import preprocessing\n#from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans \nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/CreditCardUsage.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"MINIMUM_PAYMENTS\"] = df[\"MINIMUM_PAYMENTS\"].fillna(df[\"MINIMUM_PAYMENTS\"].median())\ndf[\"CREDIT_LIMIT\"] = df[\"CREDIT_LIMIT\"].fillna(df[\"CREDIT_LIMIT\"].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 6))\ncorr = df.corr()\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n        annot=True,\n        linewidths=0.5,\n        cmap=\"YlGnBu\")\nplt.title('CreditCardUsage Correlation Heatmap')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df2 = df.drop(columns=\"CUST_ID\")\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Scaling: \"WHY\"**\n* Features with different scales and outliers are two characteristics lead to difficulties to visualize the data.\n* Majority of classifiers calculate the distance between two points by the Euclidean distance. \n* If one of the features has a broad range of values, the distance will be influenced by broad range feature.\n* Therefore, its absolutely necessary to range all features should be scaled. so that each feature contributes approximately proportionately to the final distance.\n\n**\"WHEN\"**\n* Any algorithm that computes distance or assumes normality, scale your features.\n\n**HOW**\n* StandardScaler\n* Normalize\n* MinMaxScaler\n* RobustScaler\n* PowerTransformer\n* QuantileTransformer\n \nThere are other scalers out there as well. I selected the above six to does it scale the data and visulize it.\nFeatures of credit card usage dataset are individually scaled by each of the above scaler.\n\n"},{"metadata":{},"cell_type":"markdown","source":"**StandardScaler**\n\nStandardScaler will scale the features, in a way each feature will have mean = 0 and standard deviation = 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the Scaler object\nss = preprocessing.StandardScaler()\n# Fit your data on the scaler object\nss_df = ss.fit_transform(x)\nss_df = pd.DataFrame(ss_df, columns=x.columns)\nss_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normalization:**\n\nNormalization is the process of scaling individual samples to have unit norm.\nNormalization is a row-wise operation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize feature data\nnorm_df = preprocessing.normalize(x)\nnorm_df = pd.DataFrame(norm_df, columns=x.columns)\nnorm_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MinMaxScaler:**\n\nMinMaxScaler transforms features by scaling each feature to a given range (default at (0,1)) \nit is sensitive to outliers, so if there are outliers in the data, you might want to consider another scaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the MinMaxscaler object\nmms = preprocessing.MinMaxScaler()\n# Fit your data on the Minmaxscaler object\nmms_df = mms.fit_transform(x)\nmms_df = pd.DataFrame(mms_df, columns=x.columns)\nmms_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RobustScaler:**\n\nRobustScaler removes the median and scales the data according to the quantile range. \nIf your data contains many outliers, you can use the RobustScaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the Robust scaler object\nrs = preprocessing.RobustScaler()\n# Fit your data on the Robust Scaler object\nrs_df = rs.fit_transform(x)\nrs_df = pd.DataFrame(rs_df, columns=x.columns)\nrs_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PowerTransformer:**\n\nPowerTransformer also applies zero-mean, unit variance normalization to the transformed output"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the power Transformer object\npt = preprocessing.PowerTransformer()\npt_df = pt.fit_transform(x)\npt_df = pd.DataFrame(pt_df, columns=x.columns)\npt_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**QuantileTransformer**\n\nQuantileTransformer maps data to the range of 0 to 1. even the outliers which cannot be distinguished anymore from the inliers.\n\nAs RobustScaler, QuantileTransformer is robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation on held out data. But contrary to RobustScaler, QuantileTransformer will also automatically collapse any outlier by setting them to the a priori defined range boundaries (0 and 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"qt = preprocessing.QuantileTransformer()\nqt_df = qt.fit_transform(x)\nqt_df = pd.DataFrame(qt_df, columns=x.columns)\nqt_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Column order of plots:**\n\n* Unscaled Features\n* Normalised features\n* MinMaxScaled Features\n* Standarscaled features\n* RobustScaled Features\n* PowerTransformed features\n* QuantileTransformed Features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nf = plt.figure(figsize=(40,100))\nfor feature_name in x.columns:\n    ax=f.add_subplot(x.shape[1],7,i)\n    sns.distplot(x[feature_name])\n    plt.xlabel(feature_name)\n    i=i+1  \n    ax=f.add_subplot(x.shape[1],7,i)\n    sns.distplot(norm_df[feature_name])\n    plt.xlabel(feature_name+\" After Normalization\")\n    i=i+1 \n    ax=f.add_subplot(x.shape[1],7,i)\n    sns.distplot(mms_df[feature_name])\n    plt.xlabel(feature_name+\" After MinMaxScaler\")\n    i=i+1 \n    ax=f.add_subplot(x.shape[1],7,i)\n    sns.distplot(ss_df[feature_name])\n    plt.xlabel(feature_name+\" After StandardScaler\")\n    i=i+1 \n    ax=f.add_subplot(x.shape[1],7,i)\n    sns.distplot(rs_df[feature_name])\n    plt.xlabel(feature_name+\" After RobustScaler\")\n    i=i+1\n    ax=f.add_subplot(x.shape[1],7,i)\n    sns.distplot(pt_df[feature_name])\n    plt.xlabel(feature_name+\" After PowerTransformer\")\n    i=i+1\n    ax=f.add_subplot(x.shape[1],7,i)\n    sns.distplot(qt_df[feature_name])\n    plt.xlabel(feature_name+\" After QuantileTransformer\")\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Fit score for unscaled and scaled features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"testrange = range(1,30)\nkmeans = [KMeans(n_clusters=i) for i in testrange]\nx_score = [kmeans[i].fit(x).score(x) for i in range(len(kmeans))]\nnorm_df_score = [kmeans[i].fit(norm_df).score(norm_df) for i in range(len(kmeans))]\nmms_df_score = [kmeans[i].fit(mms_df).score(mms_df) for i in range(len(kmeans))]\nss_df_score = [kmeans[i].fit(ss_df).score(ss_df) for i in range(len(kmeans))]\nrs_df_score = [kmeans[i].fit(rs_df).score(rs_df) for i in range(len(kmeans))]\npt_df_score = [kmeans[i].fit(pt_df).score(pt_df) for i in range(len(kmeans))]\nqt_df_score = [kmeans[i].fit(qt_df).score(qt_df) for i in range(len(kmeans))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Plot elbow score for unscaled and scaled features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nplt.subplot(431)\nplt.plot(testrange,x_score,'bx-')\nplt.xlabel('Number of Clusters X')\nplt.ylabel('Score')\nplt.subplot(432)\nplt.plot(testrange,norm_df_score,'bx-')\nplt.xlabel('Number of Clusters Normalization')\nplt.ylabel('Score')\nplt.subplot(433)\nplt.plot(testrange,mms_df_score,'bx-')\nplt.xlabel('Number of Clusters MinMaxScaler')\nplt.ylabel('Score')\nplt.subplot(434)\nplt.plot(testrange,ss_df_score,'bx-')\nplt.xlabel('Number of Clusters Standardscaler')\nplt.ylabel('Score')\nplt.subplot(435)\nplt.plot(testrange,rs_df_score,'bx-')\nplt.xlabel('Number of Clusters RobustScaler')\nplt.ylabel('Score')\nplt.subplot(436)\nplt.plot(testrange,pt_df_score,'bx-')\nplt.xlabel('Number of Clusters PowerTransformer')\nplt.ylabel('Score')\nplt.subplot(437)\nplt.plot(testrange,qt_df_score,'bx-')\nplt.xlabel('Number of Clusters QantileTransformer')\nplt.ylabel('Score')\nplt.suptitle('Elbow Curve - Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determine interia for unscaled and scaled features"},{"metadata":{"trusted":true},"cell_type":"code","source":"testrange = range(1,30)\nkmeans = [KMeans(n_clusters=i) for i in testrange]\nx_inertia = [kmeans[i].fit(x).inertia_ for i in range(len(kmeans))]\nnorm_df_inertia = [kmeans[i].fit(norm_df).inertia_ for i in range(len(kmeans))]\nmms_df_inertia = [kmeans[i].fit(mms_df).inertia_ for i in range(len(kmeans))]\nss_df_inertia = [kmeans[i].fit(ss_df).inertia_ for i in range(len(kmeans))]\nrs_df_inertia = [kmeans[i].fit(rs_df).inertia_ for i in range(len(kmeans))]\npt_df_inertia = [kmeans[i].fit(pt_df).inertia_ for i in range(len(kmeans))]\nqt_df_inertia = [kmeans[i].fit(qt_df).inertia_ for i in range(len(kmeans))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot interia for unscaled and scaled features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.subplot(431)\nplt.plot(testrange,x_inertia,'bx-')\nplt.xlabel('Number of Clusters X')\nplt.ylabel('SSD')\nplt.subplot(432)\nplt.plot(testrange,norm_df_inertia,'bx-')\nplt.xlabel('Number of Clusters Normalization')\nplt.ylabel('SSD')\nplt.subplot(433)\nplt.plot(testrange,mms_df_inertia,'bx-')\nplt.xlabel('Number of Clusters MinMaxScaler')\nplt.ylabel('SSD')\nplt.subplot(434)\nplt.plot(testrange,ss_df_inertia,'bx-')\nplt.xlabel('Number of Clusters Standardscaler')\nplt.ylabel('SSD')\nplt.subplot(435)\nplt.plot(testrange,rs_df_inertia,'bx-')\nplt.xlabel('Number of Clusters RobustScaler')\nplt.ylabel('SSD')\nplt.subplot(436)\nplt.plot(testrange,pt_df_inertia,'bx-')\nplt.xlabel('Number of Clusters PowerTransformer')\nplt.ylabel('SSD')\nplt.subplot(437)\nplt.plot(testrange,qt_df_inertia,'bx-')\nplt.xlabel('Number of Clusters QantileTransformer')\nplt.ylabel('SSD')\nplt.suptitle('Elbow Curve - SSD')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find optimal silhouette score for all scaled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_score = mms_score = ss_score = rs_score = pt_score = qt_score = 0.0\nsave_norm_score = save_mms_score = save_ss_score = save_rs_score = save_pt_score = save_qt_score = 0.0\nnorm_cluster = mms_cluster = ss_cluster = rs_cluster = pt_cluster = qt_cluster = 0.0\nfor n_clusters in range(2,15):\n    km = KMeans (n_clusters=n_clusters)\n    norm_preds = km.fit_predict(norm_df)\n    mms_preds = km.fit_predict(mms_df)\n    ss_preds = km.fit_predict(ss_df)\n    rs_preds = km.fit_predict(rs_df)\n    pt_preds = km.fit_predict(pt_df)\n    qt_preds = km.fit_predict(qt_df)\n    #centers = km.cluster_centers_\n    norm_score = silhouette_score(norm_df, norm_preds, metric='euclidean')\n    mms_score = silhouette_score(mms_df, mms_preds, metric='euclidean')\n    ss_score = silhouette_score(ss_df, ss_preds, metric='euclidean')\n    rs_score = silhouette_score(rs_df, rs_preds, metric='euclidean')\n    pt_score = silhouette_score(pt_df, pt_preds, metric='euclidean')\n    qt_score = silhouette_score(qt_df, qt_preds, metric='euclidean')\n    \n    if save_norm_score < norm_score:\n        save_norm_score = norm_score\n        norm_cluster = n_clusters\n    if save_mms_score < mms_score:\n        save_mms_score = mms_score\n        mms_cluster = n_clusters\n    if save_ss_score < ss_score:\n        save_ss_score = ss_score\n        ss_cluster = n_clusters\n    if save_rs_score < rs_score:\n        save_rs_score = rs_score\n        rs_cluster = n_clusters\n    if save_pt_score < pt_score:\n        save_pt_score = pt_score\n        pt_cluster = n_clusters\n    if save_qt_score < qt_score:\n        save_qt_score = qt_score\n        qt_cluster = n_clusters\n\nprint (\"For normalization     optimal cluster = {}, silhouette score is {}\".format(norm_cluster, norm_score))\nprint (\"For MinMaxScaler      optimal cluster = {}, silhouette score is {}\".format(mms_cluster, mms_score))\nprint (\"For StandardScaler    optimal cluster = {}, silhouette score is {}\".format(ss_cluster, ss_score))\nprint (\"For RobustScaler      optimal cluster = {}, silhouette score is {}\".format(rs_cluster, rs_score))\nprint (\"For PowerTransform    optimal cluster = {}, silhouette score is {}\".format(pt_cluster, pt_score))\nprint (\"For QuantileTransform optimal cluster = {}, silhouette score is {}\".format(qt_cluster, qt_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means = KMeans(n_clusters = norm_cluster, n_init = 12).fit(norm_df)\nlabels_norm = k_means.labels_\ncentroids_norm = k_means.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means = KMeans(n_clusters = mms_cluster, n_init = 12).fit(mms_df)\nlabels_mms = k_means.labels_\ncentroids_mms = k_means.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means = KMeans(n_clusters = ss_cluster, n_init = 12).fit(ss_df)\nlabels_ss = k_means.labels_\ncentroids_ss = k_means.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means = KMeans(n_clusters = rs_cluster, n_init = 12).fit(rs_df)\nlabels_rs = k_means.labels_\ncentroids_rs = k_means.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means = KMeans(n_clusters = pt_cluster, n_init = 12).fit(pt_df)\nlabels_pt = k_means.labels_\ncentroids_pt = k_means.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means = KMeans(n_clusters = qt_cluster, n_init = 12).fit(qt_df)\nlabels_qt = k_means.labels_\ncentroids_qt = k_means.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['labels_norm'] = labels_norm\nx['labels_mms'] = labels_mms\nx['labels_ss'] = labels_ss\nx['labels_rs'] = labels_rs\nx['labels_pt'] = labels_pt\nx['labels_qt'] = labels_qt\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.groupby('labels_norm').mean().sort_values(by='BALANCE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x['BALANCE'], x['PURCHASES'], c=labels_norm.astype(np.float), alpha=0.8,s=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"References:\n* https://scikit-learn.org/stable/modules/preprocessing.html\n* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler\n* https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}