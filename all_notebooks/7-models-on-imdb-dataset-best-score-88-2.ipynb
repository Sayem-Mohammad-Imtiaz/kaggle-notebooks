{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Sentiment Analysis of IMDB Movie Reviews**","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"markdown","source":"**Problem Statement:**\n\nIn this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models.","metadata":{}},{"cell_type":"code","source":"#Load the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.sparse import hstack\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\n# from bs4 import BeautifulSoup\nimport spacy\nimport re,string,unicodedata\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom textblob import TextBlob\nfrom textblob import Word\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-06-12T04:28:02.485502Z","iopub.execute_input":"2021-06-12T04:28:02.485841Z","iopub.status.idle":"2021-06-12T04:28:10.38851Z","shell.execute_reply.started":"2021-06-12T04:28:02.485809Z","shell.execute_reply":"2021-06-12T04:28:10.387674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing the training data\ndata=pd.read_csv(r'../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nprint(data.shape)\ndata.head(10)","metadata":{"_uuid":"4c593c17588723c0b0b0f19851cb70a8447ced76","scrolled":true,"execution":{"iopub.status.busy":"2021-06-12T04:28:10.390006Z","iopub.execute_input":"2021-06-12T04:28:10.390355Z","iopub.status.idle":"2021-06-12T04:28:11.588672Z","shell.execute_reply.started":"2021-06-12T04:28:10.390319Z","shell.execute_reply":"2021-06-12T04:28:11.587724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Summary of the dataset\ndata.describe()","metadata":{"_uuid":"7f11c83b1320c8982b36889145f7f770563674a8","execution":{"iopub.status.busy":"2021-06-12T04:28:11.590243Z","iopub.execute_input":"2021-06-12T04:28:11.590493Z","iopub.status.idle":"2021-06-12T04:28:11.704575Z","shell.execute_reply.started":"2021-06-12T04:28:11.590468Z","shell.execute_reply":"2021-06-12T04:28:11.703842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sentiment count**","metadata":{"_uuid":"453c3fd238f62ab8f649eb01771817e25bc0c77d"}},{"cell_type":"code","source":"#Class Distrubution\ndata['sentiment'].value_counts()","metadata":{"_uuid":"cb6bb97b0f851947dcf341a1de5708a1f2bc64c1","execution":{"iopub.status.busy":"2021-06-12T04:28:11.706096Z","iopub.execute_input":"2021-06-12T04:28:11.706433Z","iopub.status.idle":"2021-06-12T04:28:11.724527Z","shell.execute_reply.started":"2021-06-12T04:28:11.706398Z","shell.execute_reply":"2021-06-12T04:28:11.723736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the dataset is perfectly balanced.","metadata":{}},{"cell_type":"markdown","source":"### Change Target variable","metadata":{}},{"cell_type":"code","source":"## 0 as Negative and 1 as Positive\ndata.sentiment=data.sentiment.apply(lambda x: 0 if x=='negative' else 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:11.726459Z","iopub.execute_input":"2021-06-12T04:28:11.726992Z","iopub.status.idle":"2021-06-12T04:28:11.758034Z","shell.execute_reply.started":"2021-06-12T04:28:11.726957Z","shell.execute_reply":"2021-06-12T04:28:11.757293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's start with feature extraction.\n<a id = 3></a>\n<h1><font color = MidnightBlue>Feature Engineering</font></h1>\n<hr style=\"width:100%;height:1.2px;border-width:0;background-color:silver\">\n\n### Indirect features:\n\n- count of sentences\n- count of words\n- count of unique words\n- -count of letters\n- count of punctuations\n- count of uppercase words/letters\n- count of stop words\n- Avg length of each word","metadata":{}},{"cell_type":"code","source":"## Indirect features\neng_stopwords = set(stopwords.words(\"english\"))\n\ndata['count_sent']=data[\"review\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndata['count_word']=data[\"review\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndata['count_unique_word']=data[\"review\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndata['count_letters']=data[\"review\"].apply(lambda x: len(str(x)))\n#punctuation count\ndata[\"count_punctuations\"] =data[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndata[\"count_words_upper\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndata[\"count_words_title\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ndata[\"count_stopwords\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ndata[\"mean_word_len\"] = data[\"review\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n#Word count percent in each comment:\ndata['word_unique_percent']=data['count_unique_word']*100/data['count_word']\n#Punct percent in each comment:\ndata['punct_percent']=data['count_punctuations']*100/data['count_word']\n#derived features\n#Word count percent in each comment:\ndata['word_unique_percent']=data['count_unique_word']*100/data['count_word']\n#derived features\n#Punct percent in each comment:\ndata['punct_percent']=data['count_punctuations']*100/data['count_word']","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:11.759197Z","iopub.execute_input":"2021-06-12T04:28:11.759561Z","iopub.status.idle":"2021-06-12T04:28:26.883491Z","shell.execute_reply.started":"2021-06-12T04:28:11.759527Z","shell.execute_reply":"2021-06-12T04:28:26.882669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Reordering the columns \ndata = data[['review', 'count_sent', 'count_word', 'count_unique_word',\n       'count_letters', 'count_punctuations', 'count_words_upper',\n       'count_words_title', 'count_stopwords', 'mean_word_len',\n       'word_unique_percent', 'punct_percent','sentiment']]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:26.885482Z","iopub.execute_input":"2021-06-12T04:28:26.886106Z","iopub.status.idle":"2021-06-12T04:28:26.913056Z","shell.execute_reply.started":"2021-06-12T04:28:26.886067Z","shell.execute_reply":"2021-06-12T04:28:26.912307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization- Understand your data better","metadata":{}},{"cell_type":"markdown","source":"#### 1) Understanding Sentiment across Word count\n- This visualization shows us that count of words in positive and negative reviews have same pattern","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(20, 5))\ndata1=data[data.count_word<300]\nax = sns.countplot(x=\"count_word\",  hue='sentiment', data=data1, palette=\"pastel\")\nplt.title(\"Distribution of Words in review across Target var\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:26.915961Z","iopub.execute_input":"2021-06-12T04:28:26.916215Z","iopub.status.idle":"2021-06-12T04:28:30.946818Z","shell.execute_reply.started":"2021-06-12T04:28:26.91619Z","shell.execute_reply":"2021-06-12T04:28:30.94586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2) Understanding Sentiment across Punctuation count\n- This shows usage of punctuations is same in both sentiments. This negate general perception of higher usage of punctuation in negative reviews","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(20, 5))\ndata1=data[data.count_punctuations<300]\nax = sns.countplot(x=\"count_punctuations\",  hue='sentiment', data=data1, palette=\"pastel\")\nplt.title(\"Distribution of Punctuations in review across review sentiment\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:30.948902Z","iopub.execute_input":"2021-06-12T04:28:30.949249Z","iopub.status.idle":"2021-06-12T04:28:34.931704Z","shell.execute_reply.started":"2021-06-12T04:28:30.949213Z","shell.execute_reply":"2021-06-12T04:28:34.930737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2) Understanding Sentiment across Stopwords count\n- This graph don't highlight any specific feature across classes.","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(20, 5))\ndata1=data[data.count_stopwords<300]\nax = sns.countplot(x=\"count_stopwords\",  hue='sentiment', data=data1, palette=\"pastel\")\nplt.title(\"Distribution of Stopwords in review across review sentiment\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:34.933172Z","iopub.execute_input":"2021-06-12T04:28:34.933529Z","iopub.status.idle":"2021-06-12T04:28:39.24731Z","shell.execute_reply.started":"2021-06-12T04:28:34.93349Z","shell.execute_reply":"2021-06-12T04:28:39.246549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4) Mean values on Indirect features","metadata":{}},{"cell_type":"code","source":"data.groupby(['sentiment'],as_index=False).mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:39.248551Z","iopub.execute_input":"2021-06-12T04:28:39.248907Z","iopub.status.idle":"2021-06-12T04:28:39.275369Z","shell.execute_reply.started":"2021-06-12T04:28:39.24887Z","shell.execute_reply":"2021-06-12T04:28:39.27442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This shows mean value of each indirect feature is almost similar for both the sentiments. No starling differences were observed for any feature. This also indicate that model comprising these indirect features as explanatory variable will not yield good accuracy score. So to get good classification model, we have to develop machine learning or neural network model based on word vectorizer only.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5) Histogram for word count for both classes","metadata":{}},{"cell_type":"code","source":"plt.hist(data[data['sentiment']==0]['count_word'],range=(0,2000),color='SkyBlue')\nplt.title('Nagative Reviews')\nplt.show()\n\nplt.hist(data[data['sentiment']==1]['count_word'],range=(0,2000),color='PeachPuff')\nplt.title('Postive Reviews')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:39.276667Z","iopub.execute_input":"2021-06-12T04:28:39.277028Z","iopub.status.idle":"2021-06-12T04:28:39.575643Z","shell.execute_reply.started":"2021-06-12T04:28:39.276992Z","shell.execute_reply":"2021-06-12T04:28:39.574566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Preprocessing of Reviews","metadata":{}},{"cell_type":"code","source":"string.punctuation","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:39.577019Z","iopub.execute_input":"2021-06-12T04:28:39.577399Z","iopub.status.idle":"2021-06-12T04:28:39.583238Z","shell.execute_reply.started":"2021-06-12T04:28:39.577359Z","shell.execute_reply":"2021-06-12T04:28:39.582243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing all punctuations from Text\nmapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndef word_replace(text):\n    return text.replace('<br />','')\n\n\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\n\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\n\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ndef preprocess(text):\n    text=clean_contractions(text,mapping)\n    text=text.lower()\n    text=word_replace(text)\n    text=remove_urls(text)\n    text=remove_html(text)\n    text=remove_stopwords(text)\n    text=remove_punctuation(text)\n#     text=stem_words(text) ## Takes too much of time\n    text=lemmatize_words(text)\n    \n    return text\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:39.584714Z","iopub.execute_input":"2021-06-12T04:28:39.585102Z","iopub.status.idle":"2021-06-12T04:28:39.60868Z","shell.execute_reply.started":"2021-06-12T04:28:39.585064Z","shell.execute_reply":"2021-06-12T04:28:39.607927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> In machine learning task, cleaning or pre-processing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is of most importance. IMDB reviews are posted by users manually, so we observe high usage of contractions and chat words in it. Also, some reviews are collected from other sites, so we also observe usage of many HTML tags in dataset.</b>\n\n**a. Clean Contractions or Chat Words:**\nAs this is manually entered reviews, people do use a lot of abbreviated words in chat and so it is important for us to expand all such chat words and contractions used. I’ve used list of slangs and contractions from repo.\n\n**b. Lower Casing** Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way. This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n\n**c. Removal Of Stop Words**\nStopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language is,\n\n**d. Stemming** Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. For example, if there are two words in the corpus walks and walking, then stemming will stem the suffix to make them walk. In some cases, stemming results in shorting the world literals and we lose information in it. So Lemmatization is better process.\n\n**e. Lemmatization**\nLemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language. As a result, this one is generally slower than stemming process. I’m using standard WordNetLemmatizer for work.\n\n**f. Removal Of Emojis & Emoticons** With more and more usage of social media platforms, there is an explosion in the usage of emojis in our day to day life as well. As IMDB reviews are manually added, we found usage of Emojis & Emoticons, so we removed them.\n\n**g. Removal Of Urls & HTML Tags:**\nWe found large usage of HTML tags in dataset. To make sense of dataset, such tags to be removed.\n\n**h. Removal Of Punctuations** In this process, we remove the punctuations (!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~) from the text data. This is a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way. Note of caution- This process has to be performed after removal of HTML tags else some standard tags of HTML will partially get removed in this process and afterwards HTML removal process will not give suitable results.","metadata":{}},{"cell_type":"code","source":"data[\"reviews_p\"] = data[\"review\"].apply(lambda text: preprocess(text))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:28:39.611336Z","iopub.execute_input":"2021-06-12T04:28:39.611631Z","iopub.status.idle":"2021-06-12T04:29:16.593038Z","shell.execute_reply.started":"2021-06-12T04:28:39.611597Z","shell.execute_reply":"2021-06-12T04:29:16.592191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Cloud","metadata":{"papermill":{"duration":0.055579,"end_time":"2021-04-10T20:57:28.413509","exception":false,"start_time":"2021-04-10T20:57:28.35793","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Positive reviews\nfrom wordcloud import WordCloud,STOPWORDS\nplt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 200 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(data[data.sentiment == 1].reviews_p))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"papermill":{"duration":42.788516,"end_time":"2021-04-10T20:58:11.258414","exception":false,"start_time":"2021-04-10T20:57:28.469898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:29:16.594386Z","iopub.execute_input":"2021-06-12T04:29:16.594779Z","iopub.status.idle":"2021-06-12T04:29:43.070694Z","shell.execute_reply.started":"2021-06-12T04:29:16.594722Z","shell.execute_reply":"2021-06-12T04:29:43.069918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Negative Reviews.\nplt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 200 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(data[data.sentiment == 0].reviews_p))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"papermill":{"duration":47.109893,"end_time":"2021-04-10T20:58:58.583145","exception":false,"start_time":"2021-04-10T20:58:11.473252","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:29:43.071738Z","iopub.execute_input":"2021-06-12T04:29:43.072158Z","iopub.status.idle":"2021-06-12T04:30:08.093411Z","shell.execute_reply.started":"2021-06-12T04:29:43.072113Z","shell.execute_reply":"2021-06-12T04:30:08.092503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From these word clouds, we are not able to judge any starling differences in both the sentiments by looking at words. We don’t see usage of extreme negative connotation or abusive language used while writing negative reviews.","metadata":{}},{"cell_type":"markdown","source":"### Utility Function","metadata":{}},{"cell_type":"code","source":"def metrics(model,x,y):\n    y_pred = model.predict(x)\n    acc = accuracy_score(y, y_pred)\n    f1=f1_score(y, y_pred)\n    cm=confusion_matrix(y, y_pred)\n    report=classification_report(y,y_pred)\n    plt.figure(figsize=(4,4))\n    sns.heatmap(cm,annot=True,cmap='coolwarm',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n    plt.xlabel(\"Predicted\",fontsize=16)\n    plt.ylabel(\"Actual\",fontsize=16)\n    plt.show()\n    print(\"\\nAccuracy: \",round(acc,2))\n    print(\"\\nF1 Score: \",round(f1,2))\n#     print(\"\\nConfusion Matrix: \\n\",cm)\n    print(\"\\nReport:\",report)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:08.094725Z","iopub.execute_input":"2021-06-12T04:30:08.095339Z","iopub.status.idle":"2021-06-12T04:30:08.103847Z","shell.execute_reply.started":"2021-06-12T04:30:08.095296Z","shell.execute_reply":"2021-06-12T04:30:08.102653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model based on Indirect Features","metadata":{}},{"cell_type":"code","source":"X=data[['count_sent', 'count_word', 'count_unique_word',\n       'count_letters', 'count_punctuations', 'count_words_upper',\n       'count_words_title', 'count_stopwords', 'mean_word_len',\n       'word_unique_percent', 'punct_percent']]\ny=data['sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:08.105301Z","iopub.execute_input":"2021-06-12T04:30:08.105917Z","iopub.status.idle":"2021-06-12T04:30:08.118976Z","shell.execute_reply.started":"2021-06-12T04:30:08.105881Z","shell.execute_reply":"2021-06-12T04:30:08.118137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Selecting numerical features for training model","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:08.120375Z","iopub.execute_input":"2021-06-12T04:30:08.120796Z","iopub.status.idle":"2021-06-12T04:30:08.135975Z","shell.execute_reply.started":"2021-06-12T04:30:08.120762Z","shell.execute_reply":"2021-06-12T04:30:08.135052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[i.shape for i in [X_train, X_test, y_train, y_test] ]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:08.13839Z","iopub.execute_input":"2021-06-12T04:30:08.138644Z","iopub.status.idle":"2021-06-12T04:30:08.144023Z","shell.execute_reply.started":"2021-06-12T04:30:08.13862Z","shell.execute_reply":"2021-06-12T04:30:08.142969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression with Indirect features\nclassifier = LogisticRegression(C=0.1, solver='sag')\nclassifier.fit(X_train, y_train)\nmetrics(classifier,X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:08.145613Z","iopub.execute_input":"2021-06-12T04:30:08.14625Z","iopub.status.idle":"2021-06-12T04:30:09.287122Z","shell.execute_reply.started":"2021-06-12T04:30:08.146214Z","shell.execute_reply":"2021-06-12T04:30:09.286075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As expected, this model is giving us poor accuracy of 58% as we depicted in EDA. Indirect features have very similar trends and patterns across both the classes, we have seen in EDA portion.","metadata":{}},{"cell_type":"markdown","source":"## Unsupervised- Pre-trained model- TextBlob \n- TextBlob is a python library for Natural Language Processing (NLP). TextBlob actively used Natural Language ToolKit (NLTK) to achieve tasks related to sentiment analysis. NLTK is a library which gives an easy access to a lot of lexical resources and allows users to work with categorization, classification and many other tasks. TextBlob is a simple library which supports complex analysis and operations on textual data. For lexicon-based approaches, a sentiment is defined by its semantic orientation and the intensity of each word in the sentence. This requires a pre-defined dictionary classifying negative and positive words. Generally, a text message will be represented by bag of words. After assigning individual scores to all the words, final sentiment is calculated by some pooling operation like taking an average of all the sentiments.","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:09.29232Z","iopub.execute_input":"2021-06-12T04:30:09.292571Z","iopub.status.idle":"2021-06-12T04:30:09.296478Z","shell.execute_reply.started":"2021-06-12T04:30:09.292545Z","shell.execute_reply":"2021-06-12T04:30:09.295676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sentiment( tweet):\n        analysis = TextBlob(tweet)\n      \n        if analysis.sentiment.polarity > 0:\n            return 'positive'\n        else:\n            return 'negative'","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:09.299612Z","iopub.execute_input":"2021-06-12T04:30:09.300179Z","iopub.status.idle":"2021-06-12T04:30:09.306448Z","shell.execute_reply.started":"2021-06-12T04:30:09.300141Z","shell.execute_reply":"2021-06-12T04:30:09.30564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['TextBlob']=data['reviews_p'].apply(lambda x: get_sentiment(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:09.309361Z","iopub.execute_input":"2021-06-12T04:30:09.309609Z","iopub.status.idle":"2021-06-12T04:30:53.943527Z","shell.execute_reply.started":"2021-06-12T04:30:09.309585Z","shell.execute_reply":"2021-06-12T04:30:53.942395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.TextBlob.value_counts())\nprint(\"\\n\",data.sentiment.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:53.944909Z","iopub.execute_input":"2021-06-12T04:30:53.945258Z","iopub.status.idle":"2021-06-12T04:30:53.96211Z","shell.execute_reply.started":"2021-06-12T04:30:53.945221Z","shell.execute_reply":"2021-06-12T04:30:53.961133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['TextBlob1']=data.TextBlob.apply(lambda x: 0 if x=='negative' else 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:53.963408Z","iopub.execute_input":"2021-06-12T04:30:53.963769Z","iopub.status.idle":"2021-06-12T04:30:53.997127Z","shell.execute_reply.started":"2021-06-12T04:30:53.963713Z","shell.execute_reply":"2021-06-12T04:30:53.99642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(data.sentiment,data.TextBlob1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:53.998289Z","iopub.execute_input":"2021-06-12T04:30:53.998636Z","iopub.status.idle":"2021-06-12T04:30:54.009761Z","shell.execute_reply.started":"2021-06-12T04:30:53.998602Z","shell.execute_reply":"2021-06-12T04:30:54.008856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(4,4))\nsns.heatmap(confusion_matrix(data.sentiment,data.TextBlob1),annot=True,cmap='coolwarm',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\nplt.xlabel(\"Predicted\",fontsize=16)\nplt.ylabel(\"Actual\",fontsize=16)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:30:54.010997Z","iopub.execute_input":"2021-06-12T04:30:54.011344Z","iopub.status.idle":"2021-06-12T04:30:54.282377Z","shell.execute_reply.started":"2021-06-12T04:30:54.011308Z","shell.execute_reply":"2021-06-12T04:30:54.281615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This model gives us accuracy of 69%. This model is not able to perform the best way possible as negative reviews contain any negative words. We know, after assigning individual scores to all the words, final sentiment is calculated by some pooling operation like taking an average of all the sentiments","metadata":{}},{"cell_type":"markdown","source":"## N-gram Analysis\n- The order that words are used in text is not random. In English, for example, you can say \"the red apple\" but not \"apple red the\". The general idea is that you can look at each pair (or double, triple etc.) of words that occur next to each other. In a sufficently-large corpus, you're likely to see \"the red\" and \"red apple\" several times, but less likely to see \"apple red\" and \"red the\". This is useful to know if, for example, you're trying to figure out what someone is more likely to say to help decide between possible output for an automatic speech recognition system. These co-occuring words are known as \"n-grams\", where \"n\" is a number saying how long a string of words you considered.","metadata":{}},{"cell_type":"code","source":"texts = ' '.join(data['reviews_p'])","metadata":{"papermill":{"duration":0.16635,"end_time":"2021-04-10T20:59:01.463107","exception":false,"start_time":"2021-04-10T20:59:01.296757","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:30:54.28361Z","iopub.execute_input":"2021-06-12T04:30:54.283958Z","iopub.status.idle":"2021-06-12T04:30:54.359882Z","shell.execute_reply.started":"2021-06-12T04:30:54.283913Z","shell.execute_reply":"2021-06-12T04:30:54.358939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = texts.split(\" \")","metadata":{"papermill":{"duration":0.997216,"end_time":"2021-04-10T20:59:02.547063","exception":false,"start_time":"2021-04-10T20:59:01.549847","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:30:54.361304Z","iopub.execute_input":"2021-06-12T04:30:54.361873Z","iopub.status.idle":"2021-06-12T04:30:54.799868Z","shell.execute_reply.started":"2021-06-12T04:30:54.361833Z","shell.execute_reply":"2021-06-12T04:30:54.798991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_n_gram(string,i):\n    n_gram = (pd.Series(nltk.ngrams(string, i)).value_counts())[:7]\n    n_gram_df=pd.DataFrame(n_gram)\n    n_gram_df = n_gram_df.reset_index()\n    n_gram_df = n_gram_df.rename(columns={\"index\": \"word\", 0: \"count\"})\n    print(n_gram_df.head())\n    plt.figure(figsize = (10,5))\n    return sns.barplot(x='count',y='word', data=n_gram_df)","metadata":{"papermill":{"duration":0.096397,"end_time":"2021-04-10T20:59:02.730728","exception":false,"start_time":"2021-04-10T20:59:02.634331","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:30:54.802556Z","iopub.execute_input":"2021-06-12T04:30:54.803361Z","iopub.status.idle":"2021-06-12T04:30:54.810263Z","shell.execute_reply.started":"2021-06-12T04:30:54.803318Z","shell.execute_reply":"2021-06-12T04:30:54.809565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 11></a>\n<h2><font color = MidnightBlue>Unigram Analysis</font></h2>","metadata":{"papermill":{"duration":0.092412,"end_time":"2021-04-10T20:59:02.910412","exception":false,"start_time":"2021-04-10T20:59:02.818","status":"completed"},"tags":[]}},{"cell_type":"code","source":"draw_n_gram(string,1)","metadata":{"papermill":{"duration":15.687652,"end_time":"2021-04-10T20:59:18.704219","exception":false,"start_time":"2021-04-10T20:59:03.016567","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:30:54.811449Z","iopub.execute_input":"2021-06-12T04:30:54.811821Z","iopub.status.idle":"2021-06-12T04:31:02.21955Z","shell.execute_reply.started":"2021-06-12T04:30:54.811783Z","shell.execute_reply":"2021-06-12T04:31:02.218806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 12></a>\n<h2><font color = MidnightBlue>Bigram Analysis</font></h2>","metadata":{"papermill":{"duration":0.100454,"end_time":"2021-04-10T20:59:18.893764","exception":false,"start_time":"2021-04-10T20:59:18.79331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"draw_n_gram(string,2)","metadata":{"papermill":{"duration":28.070505,"end_time":"2021-04-10T20:59:47.060866","exception":false,"start_time":"2021-04-10T20:59:18.990361","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:31:02.22085Z","iopub.execute_input":"2021-06-12T04:31:02.221196Z","iopub.status.idle":"2021-06-12T04:31:16.043376Z","shell.execute_reply.started":"2021-06-12T04:31:02.22116Z","shell.execute_reply":"2021-06-12T04:31:16.042614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = 13></a>\n<h2><font color = MidnightBlue>Trigram Analysis</font></h2>","metadata":{"papermill":{"duration":0.090379,"end_time":"2021-04-10T20:59:47.243566","exception":false,"start_time":"2021-04-10T20:59:47.153187","status":"completed"},"tags":[]}},{"cell_type":"code","source":"draw_n_gram(string,3)","metadata":{"papermill":{"duration":34.202262,"end_time":"2021-04-10T21:00:21.536085","exception":false,"start_time":"2021-04-10T20:59:47.333823","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T04:31:16.04465Z","iopub.execute_input":"2021-06-12T04:31:16.045014Z","iopub.status.idle":"2021-06-12T04:31:32.356644Z","shell.execute_reply.started":"2021-06-12T04:31:16.044977Z","shell.execute_reply":"2021-06-12T04:31:32.355882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This N\ngram analysis showcase words which are\noccurring together in IMDB data base.\nFrom Uni\ngram we see that ‘Movie’ word occurs more\nthan 1L times in dataset, which is quite obvious.\nFrom Bi\ngram, we see that ‘look like’ and ‘ever seen’\nwords have occurred more than 2500 times together\nFrom Tri\ngram, we see that ‘movie ever seen ’ has\nobserved maximum number of times together.","metadata":{}},{"cell_type":"markdown","source":"# Word Embedding: \n- Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. A Word Embedding format generally tries to map a word using a dictionary to a vector.\n\n## Frequency Based Vectorization","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['reviews_p'], data['sentiment'], test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:31:32.359302Z","iopub.execute_input":"2021-06-12T04:31:32.35956Z","iopub.status.idle":"2021-06-12T04:31:32.372145Z","shell.execute_reply.started":"2021-06-12T04:31:32.359535Z","shell.execute_reply":"2021-06-12T04:31:32.371346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[i.shape for i in [X_train, X_test, y_train, y_test] ]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:31:32.373316Z","iopub.execute_input":"2021-06-12T04:31:32.373809Z","iopub.status.idle":"2021-06-12T04:31:32.378821Z","shell.execute_reply.started":"2021-06-12T04:31:32.373775Z","shell.execute_reply":"2021-06-12T04:31:32.377965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1) TF-IDF-** \n- In TF-IDF which based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. Common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document. Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents. TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.","metadata":{}},{"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 3),\n    max_features=10000\n)\n\nword_vectorizer.fit(data['reviews_p'])\n\ntfidf_train = word_vectorizer.transform(X_train)\ntfidf_test = word_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:31:32.379962Z","iopub.execute_input":"2021-06-12T04:31:32.38057Z","iopub.status.idle":"2021-06-12T04:32:50.47798Z","shell.execute_reply.started":"2021-06-12T04:31:32.380533Z","shell.execute_reply":"2021-06-12T04:32:50.477111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_vectorizer.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:32:50.479287Z","iopub.execute_input":"2021-06-12T04:32:50.479638Z","iopub.status.idle":"2021-06-12T04:32:50.483883Z","shell.execute_reply.started":"2021-06-12T04:32:50.479602Z","shell.execute_reply":"2021-06-12T04:32:50.48298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of tfidf_train:',tfidf_train.shape)\nprint('Shape of tfidf_test:',tfidf_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:32:50.48523Z","iopub.execute_input":"2021-06-12T04:32:50.485593Z","iopub.status.idle":"2021-06-12T04:32:50.496528Z","shell.execute_reply.started":"2021-06-12T04:32:50.485557Z","shell.execute_reply":"2021-06-12T04:32:50.495329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2) Count Vectorizer-** \n- It is tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample.","metadata":{}},{"cell_type":"code","source":"cv=CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}',ngram_range=(1,3),max_features=10000)\ncv.fit(data['reviews_p'])\ncv_train=cv.transform(X_train)\ncv_test=cv.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:32:50.4978Z","iopub.execute_input":"2021-06-12T04:32:50.49814Z","iopub.status.idle":"2021-06-12T04:34:15.18978Z","shell.execute_reply.started":"2021-06-12T04:32:50.498106Z","shell.execute_reply":"2021-06-12T04:34:15.18884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of cv_train:',cv_train.shape)\nprint('Shape of cv_test:',cv_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:34:15.191073Z","iopub.execute_input":"2021-06-12T04:34:15.191438Z","iopub.status.idle":"2021-06-12T04:34:15.198309Z","shell.execute_reply.started":"2021-06-12T04:34:15.191401Z","shell.execute_reply":"2021-06-12T04:34:15.197282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine Learning Models","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1) Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic Regression with TF-IDF\nclassifier = LogisticRegression(C=0.1, solver='sag')\nclassifier.fit(tfidf_train, y_train)\nmetrics(classifier,tfidf_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:34:15.20003Z","iopub.execute_input":"2021-06-12T04:34:15.20038Z","iopub.status.idle":"2021-06-12T04:34:15.945945Z","shell.execute_reply.started":"2021-06-12T04:34:15.200345Z","shell.execute_reply":"2021-06-12T04:34:15.944825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression with Count Vectoriser\nclassifier1 = LogisticRegression(penalty='l2',C=10)\nclassifier1.fit(cv_train, y_train)\n\nmetrics(classifier1,cv_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:34:15.947242Z","iopub.execute_input":"2021-06-12T04:34:15.947596Z","iopub.status.idle":"2021-06-12T04:34:19.571716Z","shell.execute_reply.started":"2021-06-12T04:34:15.947559Z","shell.execute_reply":"2021-06-12T04:34:19.570761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2) Naive Bayes","metadata":{}},{"cell_type":"code","source":"#Naive Bayes with tf_idf\nmodel= MultinomialNB()\n\nmodel.fit(tfidf_train, y_train)\nmetrics(model,tfidf_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:34:19.573581Z","iopub.execute_input":"2021-06-12T04:34:19.574175Z","iopub.status.idle":"2021-06-12T04:34:19.838674Z","shell.execute_reply.started":"2021-06-12T04:34:19.574134Z","shell.execute_reply":"2021-06-12T04:34:19.837922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Naive Bayes with Count Vectorizer\nmodela= MultinomialNB()\n\nmodela.fit(cv_train, y_train)\nmetrics(modela,cv_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:34:19.840593Z","iopub.execute_input":"2021-06-12T04:34:19.841162Z","iopub.status.idle":"2021-06-12T04:34:20.10491Z","shell.execute_reply.started":"2021-06-12T04:34:19.841122Z","shell.execute_reply":"2021-06-12T04:34:20.103838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3) XGBoost","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier() #Default XGBoost Model with TF_IDF\nmodel.fit(tfidf_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:34:20.106104Z","iopub.execute_input":"2021-06-12T04:34:20.106438Z","iopub.status.idle":"2021-06-12T04:36:14.041411Z","shell.execute_reply.started":"2021-06-12T04:34:20.106402Z","shell.execute_reply":"2021-06-12T04:36:14.040606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(model,tfidf_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:14.042696Z","iopub.execute_input":"2021-06-12T04:36:14.043061Z","iopub.status.idle":"2021-06-12T04:36:14.320725Z","shell.execute_reply.started":"2021-06-12T04:36:14.043026Z","shell.execute_reply":"2021-06-12T04:36:14.319881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelcv = XGBClassifier() #Default XGBoost Model with count Vectorizer\nmodelcv.fit(cv_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:14.322173Z","iopub.execute_input":"2021-06-12T04:36:14.322787Z","iopub.status.idle":"2021-06-12T04:36:37.46885Z","shell.execute_reply.started":"2021-06-12T04:36:14.322728Z","shell.execute_reply":"2021-06-12T04:36:37.468043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(modelcv,cv_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:37.470136Z","iopub.execute_input":"2021-06-12T04:36:37.470475Z","iopub.status.idle":"2021-06-12T04:36:37.746865Z","shell.execute_reply.started":"2021-06-12T04:36:37.470438Z","shell.execute_reply":"2021-06-12T04:36:37.745809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4) SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(penalty='l2')\nlinear_svc.fit(tfidf_train, y_train)\nmetrics(linear_svc,tfidf_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:37.748053Z","iopub.execute_input":"2021-06-12T04:36:37.748386Z","iopub.status.idle":"2021-06-12T04:36:38.414375Z","shell.execute_reply.started":"2021-06-12T04:36:37.74835Z","shell.execute_reply":"2021-06-12T04:36:38.413594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#on Count Vectorizer\nlinear_svc1 = LinearSVC(C=0.5, random_state=42)\nlinear_svc1.fit(cv_train, y_train)\n\nmetrics(linear_svc1,cv_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:38.415531Z","iopub.execute_input":"2021-06-12T04:36:38.415877Z","iopub.status.idle":"2021-06-12T04:36:42.666971Z","shell.execute_reply.started":"2021-06-12T04:36:38.415841Z","shell.execute_reply":"2021-06-12T04:36:42.66601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network Models","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import LSTM,Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\nfrom keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:42.668234Z","iopub.execute_input":"2021-06-12T04:36:42.668582Z","iopub.status.idle":"2021-06-12T04:36:42.676191Z","shell.execute_reply.started":"2021-06-12T04:36:42.668547Z","shell.execute_reply":"2021-06-12T04:36:42.675418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:42.677395Z","iopub.execute_input":"2021-06-12T04:36:42.677819Z","iopub.status.idle":"2021-06-12T04:36:42.686086Z","shell.execute_reply.started":"2021-06-12T04:36:42.67778Z","shell.execute_reply":"2021-06-12T04:36:42.685166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Out of 50k dataset, 36k for training, 4k for Validationa and 10k for testing\n\nX_train, X_test, y_train, y_test = train_test_split(data['reviews_p'], data['sentiment'],test_size=0.2, random_state=0)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,test_size=0.1, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:42.687622Z","iopub.execute_input":"2021-06-12T04:36:42.687981Z","iopub.status.idle":"2021-06-12T04:36:42.70766Z","shell.execute_reply.started":"2021-06-12T04:36:42.687947Z","shell.execute_reply":"2021-06-12T04:36:42.70695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[x.shape for x in [X_train,X_valid,X_test]]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:42.708888Z","iopub.execute_input":"2021-06-12T04:36:42.70922Z","iopub.status.idle":"2021-06-12T04:36:42.714667Z","shell.execute_reply.started":"2021-06-12T04:36:42.709187Z","shell.execute_reply":"2021-06-12T04:36:42.713767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(data.reviews_p)\n\nX_train1 = tokenizer.texts_to_sequences(X_train)\nX_valid1 = tokenizer.texts_to_sequences(X_valid)\nX_test1 = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(X_train[2])\nprint(X_train1[2])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:42.716008Z","iopub.execute_input":"2021-06-12T04:36:42.716693Z","iopub.status.idle":"2021-06-12T04:36:53.707444Z","shell.execute_reply.started":"2021-06-12T04:36:42.716599Z","shell.execute_reply":"2021-06-12T04:36:53.706492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train[2])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T04:36:53.708749Z","iopub.execute_input":"2021-06-12T04:36:53.709109Z","iopub.status.idle":"2021-06-12T04:36:53.714323Z","shell.execute_reply.started":"2021-06-12T04:36:53.70907Z","shell.execute_reply":"2021-06-12T04:36:53.713458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_lens = [len(s) for s in X_train1]\nprint(\"average length: %0.1f\" % np.mean(seq_lens))\nprint(\"max length: %d\" % max(seq_lens))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:11:44.95249Z","iopub.execute_input":"2021-06-12T05:11:44.952902Z","iopub.status.idle":"2021-06-12T05:11:44.970556Z","shell.execute_reply.started":"2021-06-12T05:11:44.952869Z","shell.execute_reply":"2021-06-12T05:11:44.967242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 150\n\nX_train1 = pad_sequences(X_train1, padding='post', maxlen=maxlen)\nX_valid1 = pad_sequences(X_valid1, padding='post', maxlen=maxlen)\nX_test1 = pad_sequences(X_test1, padding='post', maxlen=maxlen)\n\nprint(X_train1[2, :])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:11:51.94435Z","iopub.execute_input":"2021-06-12T05:11:51.944689Z","iopub.status.idle":"2021-06-12T05:11:53.124377Z","shell.execute_reply.started":"2021-06-12T05:11:51.944659Z","shell.execute_reply":"2021-06-12T05:11:53.123471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:11:53.125813Z","iopub.execute_input":"2021-06-12T05:11:53.126142Z","iopub.status.idle":"2021-06-12T05:11:53.133041Z","shell.execute_reply.started":"2021-06-12T05:11:53.126114Z","shell.execute_reply":"2021-06-12T05:11:53.132005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 50\ncallback = EarlyStopping(monitor='val_loss', patience=2)\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:11:53.135424Z","iopub.execute_input":"2021-06-12T05:11:53.13593Z","iopub.status.idle":"2021-06-12T05:11:57.076572Z","shell.execute_reply.started":"2021-06-12T05:11:53.135868Z","shell.execute_reply":"2021-06-12T05:11:57.0757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train1, y_train,epochs=10,verbose=True,validation_data=(X_valid1, y_valid),batch_size=1000,callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:11:57.07813Z","iopub.execute_input":"2021-06-12T05:11:57.078633Z","iopub.status.idle":"2021-06-12T05:12:36.732174Z","shell.execute_reply.started":"2021-06-12T05:11:57.078596Z","shell.execute_reply":"2021-06-12T05:12:36.731197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, model.predict_classes(X_test1))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:12:36.736216Z","iopub.execute_input":"2021-06-12T05:12:36.738018Z","iopub.status.idle":"2021-06-12T05:12:37.337508Z","shell.execute_reply.started":"2021-06-12T05:12:36.737897Z","shell.execute_reply":"2021-06-12T05:12:37.336709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:12:37.338815Z","iopub.execute_input":"2021-06-12T05:12:37.339175Z","iopub.status.idle":"2021-06-12T05:12:37.606401Z","shell.execute_reply.started":"2021-06-12T05:12:37.339138Z","shell.execute_reply":"2021-06-12T05:12:37.605453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(4,4))\nsns.heatmap(confusion_matrix(y_test, model.predict_classes(X_test1)),annot=True,cmap='coolwarm',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\nplt.xlabel(\"Predicted\",fontsize=16)\nplt.ylabel(\"Actual\",fontsize=16)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:12:37.607742Z","iopub.execute_input":"2021-06-12T05:12:37.608115Z","iopub.status.idle":"2021-06-12T05:12:38.238297Z","shell.execute_reply.started":"2021-06-12T05:12:37.608079Z","shell.execute_reply":"2021-06-12T05:12:38.237525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test1.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:12:38.240916Z","iopub.execute_input":"2021-06-12T05:12:38.241242Z","iopub.status.idle":"2021-06-12T05:12:38.24644Z","shell.execute_reply.started":"2021-06-12T05:12:38.241214Z","shell.execute_reply":"2021-06-12T05:12:38.245477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM Model","metadata":{}},{"cell_type":"code","source":"embedding_vecor_length = 32\ncallback = EarlyStopping(monitor='val_loss', patience=2)\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:12:38.248284Z","iopub.execute_input":"2021-06-12T05:12:38.248675Z","iopub.status.idle":"2021-06-12T05:12:38.503709Z","shell.execute_reply.started":"2021-06-12T05:12:38.248636Z","shell.execute_reply":"2021-06-12T05:12:38.502882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train1, y_train, epochs=10, batch_size=256,verbose = 1,validation_data=(X_valid1,y_valid),callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:12:38.506526Z","iopub.execute_input":"2021-06-12T05:12:38.506807Z","iopub.status.idle":"2021-06-12T05:21:31.859531Z","shell.execute_reply.started":"2021-06-12T05:12:38.506779Z","shell.execute_reply":"2021-06-12T05:21:31.858779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, model.predict_classes(X_test1))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:21:31.860863Z","iopub.execute_input":"2021-06-12T05:21:31.861215Z","iopub.status.idle":"2021-06-12T05:21:42.593216Z","shell.execute_reply.started":"2021-06-12T05:21:31.861178Z","shell.execute_reply":"2021-06-12T05:21:42.592356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:21:42.595965Z","iopub.execute_input":"2021-06-12T05:21:42.596224Z","iopub.status.idle":"2021-06-12T05:21:42.605731Z","shell.execute_reply.started":"2021-06-12T05:21:42.596198Z","shell.execute_reply":"2021-06-12T05:21:42.604798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:21:42.608811Z","iopub.execute_input":"2021-06-12T05:21:42.609063Z","iopub.status.idle":"2021-06-12T05:21:42.88473Z","shell.execute_reply.started":"2021-06-12T05:21:42.60904Z","shell.execute_reply":"2021-06-12T05:21:42.883888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(4,4))\nsns.heatmap(confusion_matrix(y_test, model.predict_classes(X_test1)),annot=True,cmap='coolwarm',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\nplt.xlabel(\"Predicted\",fontsize=16)\nplt.ylabel(\"Actual\",fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:21:42.886026Z","iopub.execute_input":"2021-06-12T05:21:42.886357Z","iopub.status.idle":"2021-06-12T05:21:54.21176Z","shell.execute_reply.started":"2021-06-12T05:21:42.88632Z","shell.execute_reply":"2021-06-12T05:21:54.210923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# CNN Model\nhttps://github.com/mrunal46/Text-Classification-using-LSTM-and-CNN/blob/master/LSTM%20and%20CNN%20on%20imdb.ipynb","metadata":{}},{"cell_type":"code","source":"embedding_vecor_length = 32\ncallback = EarlyStopping(monitor='val_loss', patience=2)\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:21:54.213282Z","iopub.execute_input":"2021-06-12T05:21:54.213633Z","iopub.status.idle":"2021-06-12T05:21:54.557788Z","shell.execute_reply.started":"2021-06-12T05:21:54.213596Z","shell.execute_reply":"2021-06-12T05:21:54.556964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train1, y_train, epochs=10, batch_size=256,verbose = 1,validation_data=(X_valid1,y_valid),callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:21:54.559047Z","iopub.execute_input":"2021-06-12T05:21:54.559376Z","iopub.status.idle":"2021-06-12T05:23:58.819938Z","shell.execute_reply.started":"2021-06-12T05:21:54.559339Z","shell.execute_reply":"2021-06-12T05:23:58.81905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, model.predict_classes(X_test1))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:23:58.823808Z","iopub.execute_input":"2021-06-12T05:23:58.82577Z","iopub.status.idle":"2021-06-12T05:24:00.450836Z","shell.execute_reply.started":"2021-06-12T05:23:58.825718Z","shell.execute_reply":"2021-06-12T05:24:00.450038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:24:00.452146Z","iopub.execute_input":"2021-06-12T05:24:00.452487Z","iopub.status.idle":"2021-06-12T05:24:00.732693Z","shell.execute_reply.started":"2021-06-12T05:24:00.45245Z","shell.execute_reply":"2021-06-12T05:24:00.73189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}