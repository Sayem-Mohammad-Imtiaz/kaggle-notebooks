{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install tensorflow==1.13.1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install tensorflow==1.13.1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# ****Novel approach","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:12:56.117472Z","iopub.execute_input":"2021-06-08T13:12:56.117991Z","iopub.status.idle":"2021-06-08T13:13:01.585502Z","shell.execute_reply.started":"2021-06-08T13:12:56.117848Z","shell.execute_reply":"2021-06-08T13:13:01.584554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport scipy.sparse as sp\nimport numpy as np\nfrom tensorflow.keras.layers import SimpleRNNCell\nfrom tensorflow.compat.v1.nn.rnn_cell import BasicRNNCell\nimport tensorflow as tf\nfrom tensorflow.python.platform import tf_logging as logging\nimport pickle as pkl\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport math\nimport os\nimport numpy.linalg as la\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\n#import matplotlib.pyplot as plt\nimport time\ntime_start=time.time()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:14:23.056004Z","iopub.execute_input":"2021-06-08T13:14:23.056382Z","iopub.status.idle":"2021-06-08T13:14:23.064138Z","shell.execute_reply.started":"2021-06-08T13:14:23.056351Z","shell.execute_reply":"2021-06-08T13:14:23.062894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading of Dataset-SZ Traffic\ndef load_sz_data():\n    sz_adj = pd.read_csv('../input/traffic/sz_adj.csv',header=None)\n    adj = np.mat(sz_adj)\n    sz_tf = pd.read_csv('../input/traffic/sz_speed.csv')\n    return sz_tf, adj\n\ndata, adj = load_sz_data()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:14:23.624429Z","iopub.execute_input":"2021-06-08T13:14:23.624832Z","iopub.status.idle":"2021-06-08T13:14:23.746765Z","shell.execute_reply.started":"2021-06-08T13:14:23.624799Z","shell.execute_reply":"2021-06-08T13:14:23.745775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\ntime_len=data.shape[0] # Time sequence length\nnum_nodes=data.shape[1] #Number of Roads","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:14:24.204729Z","iopub.execute_input":"2021-06-08T13:14:24.205117Z","iopub.status.idle":"2021-06-08T13:14:24.216036Z","shell.execute_reply.started":"2021-06-08T13:14:24.205084Z","shell.execute_reply":"2021-06-08T13:14:24.214545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the variables\noutput_dim=pre_len=2\nseq_len=4\nnum_units=100\ntrain_rate=0.8\nbatch_size=32","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:14:24.625302Z","iopub.execute_input":"2021-06-08T13:14:24.625668Z","iopub.status.idle":"2021-06-08T13:14:24.631278Z","shell.execute_reply.started":"2021-06-08T13:14:24.625636Z","shell.execute_reply":"2021-06-08T13:14:24.629928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalization : Traffic Speed Data\n\ndata1 =np.mat(data,dtype=np.float32)\n\nmax_value = np.max(data1)\ndata1  = data1/max_value\n#print(data1)\n\n   ","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:14:25.051757Z","iopub.execute_input":"2021-06-08T13:14:25.052141Z","iopub.status.idle":"2021-06-08T13:14:25.060365Z","shell.execute_reply.started":"2021-06-08T13:14:25.052088Z","shell.execute_reply":"2021-06-08T13:14:25.059059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(data, time_len, rate, seq_len, pre_len):\n    train_size = int(time_len * rate) #2976 *0.8 =2380\n    train_data = data[0:train_size] #  [0:2380]\n    test_data = data[train_size:time_len] #[2380:2976]\n    print(train_data.shape,'----->',test_data.shape)\n\n    trainX, trainY, testX, testY = [], [], [], []\n    for i in range(len(train_data) - seq_len - pre_len): #(2380-4-1)=2375\n        a = train_data[i: i + seq_len + pre_len] #[0:0+4+1] \n        trainX.append(a[0 : seq_len]) #a[0:4] 4 time * 156 roads\n        trainY.append(a[seq_len : seq_len + pre_len]) #a[4:4+1] 1 time*156 \n    for i in range(len(test_data) - seq_len -pre_len):\n        b = test_data[i: i + seq_len + pre_len]\n        testX.append(b[0 : seq_len])\n        testY.append(b[seq_len : seq_len + pre_len])\n      \n    trainX1 = np.array(trainX) \n    trainY1 = np.array(trainY)\n    testX1 = np.array(testX)\n    testY1 = np.array(testY)\n    return trainX1, trainY1, testX1, testY1\n\ntrainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)\n\ntotalbatch = int(trainX.shape[0]/batch_size)\ntraining_data_count = len(trainX)  \nprint('Train Test Split Details :')\nprint('Train x ----> ',len(trainX))\nprint('Train y ----> ',len(trainY))\nprint(trainX.shape)\nprint('Test x ----> ',len(testX))\nprint('Test y ----> ',len(testY))\nprint('Total Batch ----> ',totalbatch)\n#print('\\nTrain Sample Details :')\n#print(trainX[0],'--->',trainY[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:14:25.458324Z","iopub.execute_input":"2021-06-08T13:14:25.458677Z","iopub.status.idle":"2021-06-08T13:14:25.510422Z","shell.execute_reply.started":"2021-06-08T13:14:25.458645Z","shell.execute_reply":"2021-06-08T13:14:25.508998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n        init_range = np.sqrt(1 / (input_dim))\n        initial = tf.random.uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n        return tf.Variable(initial,name=name) \n#weights =weight_variable_glorot(4, 64, name='weights')\n#weights","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:14:25.917048Z","iopub.execute_input":"2021-06-08T13:14:25.917399Z","iopub.status.idle":"2021-06-08T13:14:25.923121Z","shell.execute_reply.started":"2021-06-08T13:14:25.917368Z","shell.execute_reply":"2021-06-08T13:14:25.921898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class stgcnCell(BasicRNNCell):\n    \"\"\"Temporal Graph Convolutional Network \"\"\"\n\n    def call(self, inputs, **kwargs):\n        pass\n\n    def __init__(self, num_units, adj, num_nodes, input_size=None,\n                 act=tf.nn.tanh, reuse=None):\n\n        super(stgcnCell, self).__init__(num_units=num_units,reuse=reuse)\n        self._act = act\n        self._nodes = num_nodes\n        self._units = num_units\n        self._adj = []\n        self._adj.append(self.calculate_laplacian(adj))\n\n    @staticmethod\n    def _build_sparse_matrix(L):\n        L = L.tocoo()\n        indices = np.column_stack((L.row, L.col))\n        L = tf.SparseTensor(indices, L.data, L.shape)\n        return tf.sparse_reorder(L)\n\n    def calculate_laplacian(self,adj, lambda_max=1):  \n        adj = self.normalized_adj(adj + sp.eye(adj.shape[0])) # normalisation(self identity matrix + adj)\n        adj = sp.csr_matrix(adj) #compressed sparse matrix\n        adj = adj.astype(np.float32)\n        return self.sparse_to_tuple(adj)\n    \n    def normalized_adj(self,adj):\n        adj = sp.coo_matrix(adj)\n        degree = np.array(adj.sum(1)) # Degree Matrix row wise sum\n        d_inv_sqrt = np.power(degree, -0.5).flatten() # D inv = Degree ^-0.5 \n        d_mat_inv_sqrt = sp.diags(d_inv_sqrt) #substitution of the 1D array degree in a 2D matrix diagonals\n        normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # norm= D^-0.5 * adj * D^-0.5\n        normalized_adj = normalized_adj.astype(np.float32) \n        return normalized_adj\n     \n    ''' def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n        return tf.Variable(initial,name=name) '''\n    \n    def sparse_to_tuple(self,mx):\n        mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose() #coordinate stacking row and column wise and transpose\n        L = tf.SparseTensor(coords, mx.data, mx.shape) # mx.shape= (156,156)\n        #print('shape ---->',mx.shape)\n        return tf.sparse.reorder(L) #row major ordering\n        \n    def init_state(self,batch_size):       \n        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n        return state  \n               \n    @staticmethod\n    def _concat(x, x_):\n        x_ = tf.expand_dims(x_, 0)\n        return tf.concat([x, x_], axis=0)   \n    @property\n    def state_size(self):\n        return self._nodes * self._units\n\n    @property\n    def output_size(self):\n        return self._units\n\n    def __call__(self, inputs, state, scope=None):\n\n        with tf.compat.v1.variable_scope(scope or \"tgcn\",reuse=tf.compat.v1.AUTO_REUSE):\n            with tf.compat.v1.variable_scope(\"gates\",reuse=tf.compat.v1.AUTO_REUSE):  \n                value = tf.nn.sigmoid(\n                    self._gc(inputs, state, 2 * self._units, bias=1.0, scope=scope))\n                r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n            with tf.compat.v1.variable_scope(\"candidate\",reuse=tf.compat.v1.AUTO_REUSE):\n                r_state = r * state\n                c = self._act(self._gc(inputs, r_state, self._units, scope=scope))\n            new_h = u * state + (1 - u) * c\n        return new_h, new_h\n\n\n    def _gc(self, inputs, state, output_size, bias=0.0, scope=None):\n        ## inputs:(-1,num_nodes)\n        inputs = tf.expand_dims(inputs, 2)\n        ## state:(batch,num_node,gru_units)\n        state = tf.reshape(state, (-1, self._nodes, self._units))#(-1,156,64)\n        ## concat\n        x_s = tf.concat([inputs, state], axis=2)\n        input_size = x_s.get_shape()[2]\n        ## (num_node,input_size,-1)\n        x0 = tf.transpose(x_s, perm=[1, 2, 0])  \n        x0 = tf.reshape(x0, shape=[self._nodes, -1])\n        \n        scope = tf.compat.v1.get_variable_scope()\n        with tf.compat.v1.variable_scope(scope):\n            for m in self._adj:\n                x1 = tf.compat.v1.sparse_tensor_dense_matmul(m, x0)\n#                print(x1)\n            x = tf.reshape(x1, shape=[self._nodes, input_size,-1])\n            x = tf.transpose(x,perm=[2,0,1])\n            x = tf.reshape(x, shape=[-1, input_size])\n            weights = weight_variable_glorot(input_size, output_size, name='weights')\n            x = tf.matmul(x, weights)  # (batch_size * self._nodes, output_size)\n            biases = tf.compat.v1.get_variable(\n                \"biases\", [output_size], initializer=tf.constant_initializer(bias))\n            x = tf.nn.bias_add(x, biases)\n            x = tf.reshape(x, shape=[-1, self._nodes, output_size])\n            x = tf.reshape(x, shape=[-1, self._nodes * output_size])\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:01.075253Z","iopub.execute_input":"2021-06-08T13:34:01.075621Z","iopub.status.idle":"2021-06-08T13:34:01.106738Z","shell.execute_reply.started":"2021-06-08T13:34:01.07559Z","shell.execute_reply":"2021-06-08T13:34:01.105349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def STGCN(_X, _weights, _biases):\n    ###\n    cell_1 = stgcnCell(num_units, adj, num_nodes=num_nodes)\n    cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n    _X = tf.unstack(_X, axis=1)\n    #rnncell = tf.keras.layers.RNN(cell, return_state=True)\n    #outputs, states= rnncell(_X)\n    outputs, states=tf.compat.v1.nn.static_rnn(cell, _X, dtype=tf.float32)\n    m = []\n    for i in outputs:\n        o = tf.reshape(i,shape=[-1,num_nodes,num_units])\n        o = tf.reshape(o,shape=[-1,num_units])\n        m.append(o)\n    last_output = m[-1]\n    output = tf.matmul(last_output, _weights['out']) + _biases['out']\n    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n    output = tf.transpose(output, perm=[0,2,1])\n    output = tf.reshape(output, shape=[-1,num_nodes])\n    return output, m, states","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:01.826051Z","iopub.execute_input":"2021-06-08T13:34:01.826442Z","iopub.status.idle":"2021-06-08T13:34:01.83615Z","shell.execute_reply.started":"2021-06-08T13:34:01.826396Z","shell.execute_reply":"2021-06-08T13:34:01.834733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.disable_eager_execution()\ninputs = tf.compat.v1.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\nlabels = tf.compat.v1.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\nweights = {\n    'out': tf.Variable(tf.random.normal([num_units, pre_len], mean=1.0), name='weight_o')}\nbiases = {\n    'out': tf.Variable(tf.random.normal([pre_len]),name='bias_o')}\nprint(type(inputs))\npred,ttts,ttto = STGCN(inputs, weights, biases)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:03.352998Z","iopub.execute_input":"2021-06-08T13:34:03.353358Z","iopub.status.idle":"2021-06-08T13:34:03.594612Z","shell.execute_reply.started":"2021-06-08T13:34:03.353325Z","shell.execute_reply":"2021-06-08T13:34:03.593521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pred","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:06.625016Z","iopub.execute_input":"2021-06-08T13:34:06.625382Z","iopub.status.idle":"2021-06-08T13:34:06.63062Z","shell.execute_reply.started":"2021-06-08T13:34:06.625352Z","shell.execute_reply":"2021-06-08T13:34:06.629068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lambda_loss = 0.0015\nLreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())\nlabel = tf.reshape(labels, [-1,num_nodes])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:10.692424Z","iopub.execute_input":"2021-06-08T13:34:10.692798Z","iopub.status.idle":"2021-06-08T13:34:10.765483Z","shell.execute_reply.started":"2021-06-08T13:34:10.692765Z","shell.execute_reply":"2021-06-08T13:34:10.764376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:12.742416Z","iopub.execute_input":"2021-06-08T13:34:12.742798Z","iopub.status.idle":"2021-06-08T13:34:12.764308Z","shell.execute_reply.started":"2021-06-08T13:34:12.742766Z","shell.execute_reply":"2021-06-08T13:34:12.763443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:15.280393Z","iopub.execute_input":"2021-06-08T13:34:15.280754Z","iopub.status.idle":"2021-06-08T13:34:15.291972Z","shell.execute_reply.started":"2021-06-08T13:34:15.280722Z","shell.execute_reply":"2021-06-08T13:34:15.290259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=0.001\nopt = tf.compat.v1.train.AdamOptimizer(lr)\noptimizer=opt.minimize(loss)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:34:18.108137Z","iopub.execute_input":"2021-06-08T13:34:18.108511Z","iopub.status.idle":"2021-06-08T13:34:19.633345Z","shell.execute_reply.started":"2021-06-08T13:34:18.108479Z","shell.execute_reply":"2021-06-08T13:34:19.632083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables = tf.compat.v1.global_variables()\ntraining_epoch=1000","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:42:39.69192Z","iopub.execute_input":"2021-06-08T12:42:39.692264Z","iopub.status.idle":"2021-06-08T12:42:39.697211Z","shell.execute_reply.started":"2021-06-08T12:42:39.692224Z","shell.execute_reply":"2021-06-08T12:42:39.69633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sess = tf.compat.v1.Session()\nsess.run(tf.compat.v1.global_variables_initializer())","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:43:38.943955Z","iopub.execute_input":"2021-06-08T12:43:38.944311Z","iopub.status.idle":"2021-06-08T12:43:39.46343Z","shell.execute_reply.started":"2021-06-08T12:43:38.944278Z","shell.execute_reply":"2021-06-08T12:43:39.46259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(a,b):\n    rmse = math.sqrt(mean_squared_error(a,b))\n    mae = mean_absolute_error(a, b)\n    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n    var = 1-(np.var(a-b))/np.var(a)\n    return rmse, mae, 1-F_norm, r2, var","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:43:44.709518Z","iopub.execute_input":"2021-06-08T12:43:44.709831Z","iopub.status.idle":"2021-06-08T12:43:44.715358Z","shell.execute_reply.started":"2021-06-08T12:43:44.709804Z","shell.execute_reply":"2021-06-08T12:43:44.714464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\ntest_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:43:47.438272Z","iopub.execute_input":"2021-06-08T12:43:47.438596Z","iopub.status.idle":"2021-06-08T12:43:47.442852Z","shell.execute_reply.started":"2021-06-08T12:43:47.438567Z","shell.execute_reply":"2021-06-08T12:43:47.442018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def acc(a,b):  \n    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n    train_acc=1-F_norm\n    return train_acc","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:43:48.271338Z","iopub.execute_input":"2021-06-08T12:43:48.271666Z","iopub.status.idle":"2021-06-08T12:43:48.275986Z","shell.execute_reply.started":"2021-06-08T12:43:48.271636Z","shell.execute_reply":"2021-06-08T12:43:48.27503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(training_epoch):\n    print(\"Epoch \", epoch)\n    for m in range(totalbatch):\n        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n        batch_loss.append(loss1)\n        batch_rmse.append(rmse1 * max_value)\n        train_label=np.reshape(mini_label,[-1,num_nodes])\n        \n\n     # Test completely at every epoch\n    print(\"Accuracy ----> \", evaluation(train_label,train_output)[2])\n    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n                                         feed_dict = {inputs:testX, labels:testY})\n    #train_label=np.reshape(trainY,[-1,num_nodes])\n    #train_acc=acc(train_label,train_output)\n    test_label = np.reshape(testY,[-1,num_nodes])\n    rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output)\n    test_label1 = test_label * max_value#Inverse normalization\n    test_output1 = test_output * max_value\n    test_loss.append(loss2)\n    test_rmse.append(rmse * max_value)\n    test_mae.append(mae * max_value)\n    test_acc.append(acc)\n    test_r2.append(r2_score)\n    test_var.append(var_score)\n    test_pred.append(test_output1)\n    print(mini_label.shape,train_output.shape)\n    print('Iter:{}'.format(epoch),\n          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n          'test_loss:{:.4}'.format(loss2),\n          'test_rmse:{:.4}'.format(rmse),\n          'test_acc:{:.4}'.format(acc))\n    \n        \ntime_end = time.time()\nprint(time_end-time_start,'s')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T12:43:49.208874Z","iopub.execute_input":"2021-06-08T12:43:49.209191Z","iopub.status.idle":"2021-06-08T12:44:15.958921Z","shell.execute_reply.started":"2021-06-08T12:43:49.209161Z","shell.execute_reply":"2021-06-08T12:44:15.957424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = int(len(batch_rmse)/totalbatch)\nbatch_rmse1 = [i for i in batch_rmse]\ntrain_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\nbatch_loss1 = [i for i in batch_loss]\ntrain_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n\nindex = test_rmse.index(np.min(test_rmse))\ntest_result = test_pred[index]\nvar = pd.DataFrame(test_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Testing:\")\nprint('min_rmse:%r'%(np.min(test_rmse)),\n      'min_mae:%r'%(test_mae[index]),\n      'max_acc:%r'%(test_acc[index]),\n      'r2:%r'%(test_r2[index]),\n      'var:%r'%test_var[index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training:\")\nrmse, mae, acc, r2_score, var_score = evaluation(train_label,train_output)\nprint('min_rmse:%r'%(rmse),\n      'min_mae:%r'%(mae),\n      'max_acc:%r'%(acc),\n      'r2:%r'%(r2_score),\n      'var:%r'%(var_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inverse normalisation\nprint('min_rmse:%r'%(rmse*max_value),\n      'min_mae:%r'%(mae*max_value),\n      'max_acc:%r'%(acc),\n      'r2:%r'%(r2_score),\n      'var:%r'%(var_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##visualisation\n##When hidden units = 8,16,32,64,128\nimport matplotlib.pyplot as plt \nhidden=[8,16,32,64,100,128]\nmae=[2.96,2.82,2.74,2.72,2.70,2.72]\nrmse=[4.15,4.04,3.96,3.95,3.91,3.93]\nfig,ax=plt.subplots()\nax.plot(hidden,mae,color=\"red\",label=\"mae\",marker=\"o\")\nax.set_xlabel(\"Hidden Units\",fontsize=14)\nax.set_ylabel(\"MAE\",color=\"red\",fontsize=14)\nax2=ax.twinx()\n# make a plot with different y-axis using second axis object\nax2.plot(hidden,rmse,color=\"blue\",label=\"rmse\",marker=\"o\")\n#ax.set_xlabel(\"Hidden Units\",fontsize=14)\nax2.set_ylabel(\"RMSE\",color=\"blue\",fontsize=14)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc=[7.20,7.22,7.27,7.29,7.31]\nr2=[0.8528,0.8531,0.8532,0.8534,0.8541,0.8538]\nvar=[0.852,0.855,0.8525,0.8575,0.8625,0.8575]\n##visualisation\n##When hidden units = 8,16,32,64,128\nhidden=[8,16,32,64,100,128]\nfig,ax=plt.subplots()\nax.plot(hidden,r2,color=\"red\",label=\"mae\",marker=\"o\")\nax.set_xlabel(\"Hidden Units\",fontsize=14)\nax.set_ylabel(\"R2\",color=\"red\",fontsize=14)\nax2=ax.twinx()\n# make a plot with different y-axis using second axis object\nax2.plot(hidden,var,color=\"blue\",label=\"rmse\",marker=\"o\")\n#ax.set_xlabel(\"Hidden Units\",fontsize=14)\nax2.set_ylabel(\"Variance\",color=\"blue\",fontsize=14)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden=[8,16,32,64,100,128]\nacc=[72.05,72.2,72.7,72.9,73.1,73]\nplt.plot(hidden,acc,label=\"accuracy\",marker=\"o\")\nplt.xlabel(\"Hidden Units\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **STGCN Model**","metadata":{}},{"cell_type":"code","source":"time_start=time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tstgcnCell(BasicRNNCell):\n    \"\"\"Temporal Graph Convolutional Network \"\"\"\n\n    def call(self, inputs, **kwargs):\n        pass\n\n    def __init__(self, num_units, adj, num_nodes, input_size=None,\n                 act=tf.nn.tanh, reuse=None):\n\n        super(tstgcnCell, self).__init__(num_units=num_units,reuse=reuse)\n        self._act = act\n        self._nodes = num_nodes\n        self._units = num_units\n        self._adj = []\n        self._adj.append(self.calculate_laplacian(adj))\n\n    @staticmethod\n    def _build_sparse_matrix(L):\n        L = L.tocoo()\n        indices = np.column_stack((L.row, L.col))\n        L = tf.SparseTensor(indices, L.data, L.shape)\n        return tf.sparse_reorder(L)\n\n    def calculate_laplacian(self,adj, lambda_max=1):  \n        adj = self.normalized_adj(adj + sp.eye(adj.shape[0])) # normalisation(self identity matrix + adj)\n        adj = sp.csr_matrix(adj) #compressed sparse matrix\n        adj = adj.astype(np.float32)\n        return self.sparse_to_tuple(adj)\n    \n    def normalized_adj(self,adj):\n        adj = sp.coo_matrix(adj)\n        degree = np.array(adj.sum(1)) # Degree Matrix row wise sum\n        d_inv_sqrt = np.power(degree, -0.5).flatten() # D inv = Degree ^-0.5 \n        d_mat_inv_sqrt = sp.diags(d_inv_sqrt) #substitution of the 1D array degree in a 2D matrix diagonals\n        normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # norm= D^-0.5 * adj * D^-0.5\n        normalized_adj = normalized_adj.astype(np.float32) \n        return normalized_adj\n     \n    ''' def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n        return tf.Variable(initial,name=name) '''\n    \n    def sparse_to_tuple(self,mx):\n        mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose() #coordinate stacking row and column wise and transpose\n        L = tf.SparseTensor(coords, mx.data, mx.shape) # mx.shape= (156,156)\n        #print('shape ---->',mx.shape)\n        return tf.sparse.reorder(L) #row major ordering\n        \n    def init_state(self,batch_size):       \n        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n        return state  \n               \n    @staticmethod\n    def _concat(x, x_):\n        x_ = tf.expand_dims(x_, 0)\n        return tf.concat([x, x_], axis=0)   \n    @property\n    def state_size(self):\n        return self._nodes * self._units\n\n    @property\n    def output_size(self):\n        return self._units\n\n    def __call__(self, inputs, state, scope=None):\n\n        with tf.compat.v1.variable_scope(scope or \"tgcn\",reuse=tf.compat.v1.AUTO_REUSE):\n            with tf.compat.v1.variable_scope(\"gates\",reuse=tf.compat.v1.AUTO_REUSE):  \n                value = tf.nn.sigmoid(\n                    self._gc(inputs, state, 2 * self._units, bias=1.0, scope=scope))\n                r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n            with tf.compat.v1.variable_scope(\"candidate\",reuse=tf.compat.v1.AUTO_REUSE):\n                r_state = r * state\n                c = self._act(self._gc(inputs, r_state, self._units, scope=scope))\n            new_h = u * state + (1 - u) * c\n        return new_h, new_h\n\n\n    def _gc(self, inputs, state, output_size, bias=0.0, scope=None):\n        ## inputs:(-1,num_nodes)\n        inputs = tf.expand_dims(inputs, 2)\n        ## state:(batch,num_node,gru_units)\n        state = tf.reshape(state, (-1, self._nodes, self._units))#(-1,156,64)\n        ## concat\n        x_s = tf.concat([inputs, state], axis=2)\n        input_size = x_s.get_shape()[2]\n        ## (num_node,input_size,-1)\n        x0 = tf.transpose(x_s, perm=[1, 2, 0])  \n        x0 = tf.reshape(x0, shape=[self._nodes, -1])\n        \n        scope = tf.compat.v1.get_variable_scope()\n        with tf.compat.v1.variable_scope(scope):\n            for m in self._adj:\n                x1 = tf.compat.v1.sparse_tensor_dense_matmul(m, x0)\n#                print(x1)\n            x = tf.reshape(x1, shape=[self._nodes, input_size,-1])\n            x = tf.transpose(x,perm=[2,0,1])\n            x = tf.reshape(x, shape=[-1, input_size])\n            initializer = tf.initializers.GlorotUniform()\n            weights = tf.Variable(initializer(shape=(input_size, output_size)))\n            x = tf.matmul(x, weights)  # (batch_size * self._nodes, output_size)\n            biases = tf.compat.v1.get_variable(\n                \"t_biases\", [output_size], initializer=tf.constant_initializer(bias))\n            x = tf.nn.bias_add(x, biases)\n            x = tf.reshape(x, shape=[-1, self._nodes, output_size])\n            x = tf.reshape(x, shape=[-1, self._nodes * output_size])\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:15.746336Z","iopub.execute_input":"2021-06-08T13:35:15.746822Z","iopub.status.idle":"2021-06-08T13:35:15.786205Z","shell.execute_reply.started":"2021-06-08T13:35:15.746765Z","shell.execute_reply":"2021-06-08T13:35:15.78502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TSTGCN(_X, _weights, _biases):\n    ###\n    cell_1 = tstgcnCell(num_units, adj, num_nodes=num_nodes)\n    cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n    _X = tf.unstack(_X, axis=1)\n    #rnncell = tf.keras.layers.RNN(cell, return_state=True)\n    #outputs, states= rnncell(_X)\n    outputs, states=tf.compat.v1.nn.static_rnn(cell, _X, dtype=tf.float32)\n    m = []\n    for i in outputs:\n        o = tf.reshape(i,shape=[-1,num_nodes,num_units])\n        o = tf.reshape(o,shape=[-1,num_units])\n        m.append(o)\n    last_output = m[-1]\n    output = tf.matmul(last_output, _weights['out']) + _biases['out']\n    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n    output = tf.transpose(output, perm=[0,2,1])\n    output = tf.reshape(output, shape=[-1,num_nodes])\n    return output, m, states","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:17.074745Z","iopub.execute_input":"2021-06-08T13:35:17.075237Z","iopub.status.idle":"2021-06-08T13:35:17.084747Z","shell.execute_reply.started":"2021-06-08T13:35:17.075189Z","shell.execute_reply":"2021-06-08T13:35:17.083483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.disable_eager_execution()\nt_inputs = tf.compat.v1.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\nt_labels = tf.compat.v1.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\nt_weights = {\n    'out': tf.Variable(tf.random.normal([num_units, pre_len], mean=1.0), name='weight_o')}\nt_biases = {\n    'out': tf.Variable(tf.random.normal([pre_len]),name='bias_o')}\nprint(type(inputs))\ntpred,ttts,ttto = TSTGCN(t_inputs, t_weights, t_biases)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:18.087013Z","iopub.execute_input":"2021-06-08T13:35:18.087413Z","iopub.status.idle":"2021-06-08T13:35:18.351083Z","shell.execute_reply.started":"2021-06-08T13:35:18.087381Z","shell.execute_reply":"2021-06-08T13:35:18.34998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ty_pred = tpred","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:20.850267Z","iopub.execute_input":"2021-06-08T13:35:20.850632Z","iopub.status.idle":"2021-06-08T13:35:20.856709Z","shell.execute_reply.started":"2021-06-08T13:35:20.850599Z","shell.execute_reply":"2021-06-08T13:35:20.855299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lambda_loss = 0.0015\nLreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())\nt_label = tf.reshape(t_labels, [-1,num_nodes])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:21.937675Z","iopub.execute_input":"2021-06-08T13:35:21.938057Z","iopub.status.idle":"2021-06-08T13:35:22.025809Z","shell.execute_reply.started":"2021-06-08T13:35:21.938024Z","shell.execute_reply":"2021-06-08T13:35:22.024955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_loss = tf.reduce_mean(tf.nn.l2_loss(ty_pred-t_label) + Lreg)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:22.782694Z","iopub.execute_input":"2021-06-08T13:35:22.783066Z","iopub.status.idle":"2021-06-08T13:35:22.796852Z","shell.execute_reply.started":"2021-06-08T13:35:22.783033Z","shell.execute_reply":"2021-06-08T13:35:22.795902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_error = tf.sqrt(tf.reduce_mean(tf.square(ty_pred-t_label)))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:23.813521Z","iopub.execute_input":"2021-06-08T13:35:23.813985Z","iopub.status.idle":"2021-06-08T13:35:23.824603Z","shell.execute_reply.started":"2021-06-08T13:35:23.813938Z","shell.execute_reply":"2021-06-08T13:35:23.823485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=0.001\nt_optimizer=opt.minimize(t_loss)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:25.583182Z","iopub.execute_input":"2021-06-08T13:35:25.583537Z","iopub.status.idle":"2021-06-08T13:35:26.507505Z","shell.execute_reply.started":"2021-06-08T13:35:25.583505Z","shell.execute_reply":"2021-06-08T13:35:26.506065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables = tf.compat.v1.global_variables()\ntraining_epoch=1000\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:27.594004Z","iopub.execute_input":"2021-06-08T13:35:27.594394Z","iopub.status.idle":"2021-06-08T13:35:27.599582Z","shell.execute_reply.started":"2021-06-08T13:35:27.594362Z","shell.execute_reply":"2021-06-08T13:35:27.598244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sess = tf.compat.v1.Session()\nsess.run(tf.compat.v1.global_variables_initializer())","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:30.37126Z","iopub.execute_input":"2021-06-08T13:35:30.37162Z","iopub.status.idle":"2021-06-08T13:35:33.602373Z","shell.execute_reply.started":"2021-06-08T13:35:30.37159Z","shell.execute_reply":"2021-06-08T13:35:33.601375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(a,b):\n    rmse = math.sqrt(mean_squared_error(a,b))\n    mae = mean_absolute_error(a, b)\n    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n    var = 1-(np.var(a-b))/np.var(a)\n    return rmse, mae, 1-F_norm, r2, var","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:35.156856Z","iopub.execute_input":"2021-06-08T13:35:35.157266Z","iopub.status.idle":"2021-06-08T13:35:35.166513Z","shell.execute_reply.started":"2021-06-08T13:35:35.157234Z","shell.execute_reply":"2021-06-08T13:35:35.165042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\ntest_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:36.16246Z","iopub.execute_input":"2021-06-08T13:35:36.163116Z","iopub.status.idle":"2021-06-08T13:35:36.174585Z","shell.execute_reply.started":"2021-06-08T13:35:36.162899Z","shell.execute_reply":"2021-06-08T13:35:36.17329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def acc(a,b):  \n    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n    train_acc=1-F_norm\n    return train_acc","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:35:37.302182Z","iopub.execute_input":"2021-06-08T13:35:37.302566Z","iopub.status.idle":"2021-06-08T13:35:37.309064Z","shell.execute_reply.started":"2021-06-08T13:35:37.302532Z","shell.execute_reply":"2021-06-08T13:35:37.308102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(training_epoch):\n    print(\"Epoch \", epoch)\n    for m in range(totalbatch):\n        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n        batch_loss.append(loss1)\n        batch_rmse.append(rmse1 * max_value)\n        train_label=np.reshape(mini_label,[-1,num_nodes])\n     #print(mini_label.shape,train_output.shape) (32, 1, 156) (32, 156)\n     # Test completely at every epoch\n    print(\"Accuracy ----> \", evaluation(train_label,train_output)[2])\n    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n                                         feed_dict = {inputs:testX, labels:testY})\n    #train_label=np.reshape(trainY,[-1,num_nodes])\n    #train_acc=acc(train_label,train_output)\n    test_label = np.reshape(testY,[-1,num_nodes])\n    rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output)\n    test_label1 = test_label * max_value#Inverse normalization\n    test_output1 = test_output * max_value\n    test_loss.append(loss2)\n    test_rmse.append(rmse * max_value)\n    test_mae.append(mae * max_value)\n    test_acc.append(acc)\n    test_r2.append(r2_score)\n    test_var.append(var_score)\n    test_pred.append(test_output1)\n    #print(mini_label.shape,train_output.shape)\n    print('Iter:{}'.format(epoch),\n          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n          'test_loss:{:.4}'.format(loss2),\n          'test_rmse:{:.4}'.format(rmse),\n          'test_acc:{:.4}'.format(acc))\n    \n        \ntime_end = time.time()\nprint('Time taken : ',time_end-time_start,'s')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = int(len(batch_rmse)/totalbatch)\nbatch_rmse1 = [i for i in batch_rmse]\ntrain_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\nbatch_loss1 = [i for i in batch_loss]\ntrain_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\nindex = test_rmse.index(np.min(test_rmse))\ntest_result = test_pred[index]\nvar = pd.DataFrame(test_result)\n#plot_result(test_result,test_label1,path)\n#plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Testing:\")\nprint('min_rmse:%r'%(np.min(test_rmse)),\n      'min_mae:%r'%(test_mae[index]),\n      'max_acc:%r'%(test_acc[index]),\n      'r2:%r'%(test_r2[index]),\n      'var:%r'%test_var[index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training:\")\nrmse, mae, acc, r2_score, var_score = evaluation(train_label,train_output)\nprint('min_rmse:%r'%(rmse),\n      'min_mae:%r'%(mae),\n      'max_acc:%r'%(acc),\n      'r2:%r'%(r2_score),\n      'var:%r'%(var_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inverse normalisation\nprint('min_rmse:%r'%(rmse*max_value),\n      'min_mae:%r'%(mae*max_value),\n      'max_acc:%r'%(acc),\n      'r2:%r'%(r2_score),\n      'var:%r'%(var_score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}