{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# configure matplotlib for charts \n%matplotlib inline \nimport matplotlib.pyplot as plt\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re # regex \n\nimport nltk # \nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords # stop words\nfrom nltk.stem import WordNetLemmatizer # lemmatizer \nfrom sklearn.decomposition import TruncatedSVD # dimensionality reduction for charts\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # text vectorization algorithms \nfrom sklearn.linear_model import LogisticRegression # classification algorithm \nfrom sklearn.metrics import classification_report, confusion_matrix #colculation metrix and show results\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d9b29560c5b151be701f333be04fe9ee85ccb9f"},"cell_type":"code","source":"# Load IMDB dataset\ndataset = pd.read_csv('../input/imdb_master.csv', encoding='windows-1252')\ndataset.head()\n\ntrain = dataset[dataset.type == 'train']\ntest = dataset[dataset.type == 'test']\n(train_texts, train_labels),(test_texts, test_labels) = (train.review, train.label), (test.review, test.label)\n\ntrain_texts = train_texts[train_labels != 'unsup']\ntrain_labels = train_labels[train_labels != 'unsup']\ntest_texts = test_texts[test_labels != 'unsup']\ntest_labels = test_labels[test_labels != 'unsup']\n\ntrain_labels[train_labels=='pos'] = 1\ntrain_labels[train_labels=='neg'] = 0\ntrain_labels = train_labels.astype(np.int)\ntest_labels[test_labels=='pos'] = 1\ntest_labels[test_labels=='neg'] = 0\ntest_labels = test_labels.astype(np.int)\n\nprint(\"train labels\", set(train_labels))\nprint(\"test labels\", set(test_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86115b5c47bd131d26097071e91fb9b166d541bd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29eb726a34de46b15ff7f6140cedb9cad02cf629"},"cell_type":"code","source":"# Show some examples\ntrain_texts[25000],'', train_texts[25234],'', train_texts[29366]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8ba3ad5408cd6cae9c9eb48bf3467f0f3559d9a"},"cell_type":"markdown","source":"## Examples and defining cleaning and tokenization functions"},{"metadata":{"trusted":true,"_uuid":"2843d1cd84314884a86f7943cea7bff31dae3ea5"},"cell_type":"code","source":"# print with default NLTK tokenization\nprint(nltk.word_tokenize(train_texts[25000]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f14cfa57cfe8948b99c7c091cd1bd313a9597db7"},"cell_type":"code","source":"# Define cleaning function \n# NOTE: try use different clearning techniques\npattern = re.compile(r\"[^a-zA-Z ]+\")\n\ndef preproc(text):\n    return pattern.sub('', text.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40f98447bf2b528a99f4188c7bdfd033b9c4ad7e"},"cell_type":"code","source":"print(nltk.word_tokenize(preproc(train_texts[25000])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"027e2013e8136204995e519b0181185a4b8fca58"},"cell_type":"code","source":"# Defining normalization functions  \n\n# lemmatizator WordNet\nstemmer = WordNetLemmatizer()\n\n# stop words for english \nstop_words = stopwords.words('english')\n\ndef nornalize(words, use_lemma = False):\n    filtered_sentence = [w for w in words if not w in stop_words] \n    \n    if use_lemma:\n        filtered_sentence = [ stemmer.lemmatize(w) for w in filtered_sentence]\n        \n    return filtered_sentence\n\ndef tokenize(text, use_lemma=False):\n    words = nltk.word_tokenize(preproc(text))\n    return nornalize(words, use_lemma)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f172080b016e3a6c8eb741d38d6dd12831a34b4"},"cell_type":"code","source":"# Show examples of tokenization \n\n# without lemmatization\nprint(tokenize(train_texts[25000]))\n\n# with lemmatization \nprint(tokenize(train_texts[25000], use_lemma=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"342aec0375b121839af138284fbc54ef0e2a3a41"},"cell_type":"code","source":"# Define function to plot chart \n\ndef plot_vectors(vectors, labels):\n    y_chart_neg = labels == 0\n    y_chart_pos = labels == 1\n    \n    # make dimensionality reduction to whow on 2D chart\n    svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n    X_chart = svd.fit_transform(vectors)  \n    \n    plt.figure(figsize=(10, 10))\n    plt.scatter(X_chart[y_chart_pos,0], X_chart[y_chart_pos,1], marker='o', c='b')\n    plt.scatter(X_chart[y_chart_neg,0], X_chart[y_chart_neg,1], marker='x', c='r')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f701f1f21171c6d9aa08aa65b3ffc96306aec19c"},"cell_type":"markdown","source":"## Sentiment classification with Baf-of-words features and LogRegression \n\n> try tune hyperparameters, for example select different regularization C\n\nUsed CountVectorizer with custom preprocessor and tokenizer, used only unigrams and selected 60000 more important features \n\n> try different parameters"},{"metadata":{"trusted":true,"_uuid":"26654ab82374f8c511dee49be475ad228fe96e93"},"cell_type":"code","source":"%%time\nbow_vectorizer = CountVectorizer(preprocessor=preproc, tokenizer=(lambda t: tokenize(t)), ngram_range=(1, 1), max_features = 60000)\nbow_train = bow_vectorizer.fit_transform(train_texts)\nbow_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe41262dc266f48d3e611d171ed10efbabdaf030"},"cell_type":"code","source":"plot_vectors(bow_train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a82b10e02cc36603ca00b963317b78af9f2f64f2"},"cell_type":"code","source":"# Train classification algorithm with default values \n\nlr = LogisticRegression()\nlr.fit(bow_train, train_labels)\n\n# predict values\ny_pred = lr.predict(bow_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1887906d025122440ad1f63274f63f36f4a3beb"},"cell_type":"code","source":"# analyse results with training set \n\nprint(classification_report(train_labels, y_pred))\nprint(confusion_matrix(train_labels, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3ad247ef5fcb92d5d6c0a9afb90ab9c6b8978fc"},"cell_type":"code","source":"# validate on test set ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"scrolled":true,"_uuid":"e82c1a06c915b5887455fddf00dd24f65a9c59f0"},"cell_type":"code","source":"%%time\nbow_test = bow_vectorizer.transform(test_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79ae4b093bc1cc06e667d84113cc2753a5c7a0e9"},"cell_type":"code","source":"y_pred_test = lr.predict(bow_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"301d69d312135d8e71be8cd48d581c4b03ee6733"},"cell_type":"code","source":"print(classification_report(test_labels, y_pred_test))\nprint(confusion_matrix(test_labels, y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e462eccc00b7551f7139fc9ca9215a3fca6edf"},"cell_type":"markdown","source":"### Analyse results, what can you say about this model? "},{"metadata":{"trusted":true,"_uuid":"41c32550fc58935a68fe7bdaa5a2692ea2582c1c","scrolled":false},"cell_type":"markdown","source":"\n\n\n\n"},{"metadata":{"trusted":true,"_uuid":"3b60d26e4e39ac47e675d63adf387eea99febc25"},"cell_type":"markdown","source":"## Sentiment classification with TF-IDF features and LogRegression \n\n> try tune hyperparameters, for example select different regularization C\n\nUsed TfidfVectorizer with custom preprocessor and tokenizer, used only unigrams and selected 60000 more important features \n\n> try different parameters"},{"metadata":{"trusted":true,"_uuid":"9ffd817bd868b75f455bf78d0de2b9a027522da0"},"cell_type":"code","source":"%%time\ntfidf_vectorizer = TfidfVectorizer(preprocessor=preproc, tokenizer=(lambda t: tokenize(t)), ngram_range=(1, 2), max_features = 60000 )\ntfidf_train = tfidf_vectorizer.fit_transform(train_texts)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"102ee910c1c407d63a01d6444102763c488d0306"},"cell_type":"code","source":"plot_vectors(tfidf_train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3f90137123d4f8ebc44a797639d652de6f8c5b7"},"cell_type":"code","source":"# Train classification algorithm with default values \nlr = LogisticRegression()\nlr.fit(tfidf_train, train_labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db6b6c853c658d7eeeb79059e3311e9e96482672"},"cell_type":"code","source":"# analyse results with training set \ny_pred = lr.predict(tfidf_train)\nprint(classification_report(train_labels, y_pred))\nprint(confusion_matrix(train_labels, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c1b2fd02654ec42934ae8465004ed57218db85"},"cell_type":"code","source":"# validate on test set \ntfidf_test = tfidf_vectorizer.transform(test_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"148d4ebec2c2b8768ed01b279e0a293be8639c69"},"cell_type":"code","source":"y_test_pred = lr.predict(tfidf_test)\n\nprint(classification_report(test_labels, y_test_pred))\nprint(confusion_matrix(test_labels, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a2232425fe8053e5f487c9fe9c21e09e1374603"},"cell_type":"markdown","source":"# Task\n\nTry to improve result on test. Use lemmatization, tune hyper parameters in classification algorithm, try other algorithms and way to extract features from text.****"},{"metadata":{"trusted":true,"_uuid":"a2d46c92f2b8dafa381a0a3149cece872ad3addf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}