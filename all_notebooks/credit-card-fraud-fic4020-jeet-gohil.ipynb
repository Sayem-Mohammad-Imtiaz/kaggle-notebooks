{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ***FIC4020 assignment by Jeet Gohil - 657076***\n## In your Notebook do the following:\n1. Import the credit card fraud data set\n1. Plot histograms for the frequency/number of fraudulent and non-fraudulent transactions against Amount\n1. Draw boxplots showing summary statistics for the Amount column\n1. Generate a correlation matrix illustrating using a heatmap the relationship between the different variables\n1. Generate a scatterplot for Amount and V2 showing a line of best fit using the equation of a straight line is y = mx + c, where m is the slope of the line and c is the y intercept\n1. Build an outlier detection model for your data using the Isolation Forest and the Local Outlier Factor\n1. Analyze the models using Errors, Confusion Matrix, Accuracy Score and Classification Report to identify the strengths and weaknesses of the models\n1. Discuss as a conclusion the best model and how to use it in the future in identifying fraudulent credit card transactions"},{"metadata":{},"cell_type":"markdown","source":"# Content of the dataset"},{"metadata":{},"cell_type":"markdown","source":"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."},{"metadata":{},"cell_type":"markdown","source":"# Importing the credit card fraud data set"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/credit-card-fraud-detection/creditcard.csv',sep=',')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Exploring the data we imported"},{"metadata":{},"cell_type":"markdown","source":"Firstly, we need to check if the dataset have any missing values. Pandas can only check for standard missing values which is null. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{},"cell_type":"markdown","source":"A graph/chart is one of the best methods to understand the data that we have.\n\nWe will start analyzing how many of the cases in this dataset are fraudulent and which are not."},{"metadata":{},"cell_type":"markdown","source":"## Plot histograms for the frequency/number of fraudulent and non-fraudulent transactions against Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_classes = pd.value_counts(data['Class'], sort = True)\n\ncount_classes.plot(kind = 'bar', rot=0)\n\nplt.title(\"Transaction Class Distribution\")\n\nplt.xticks(range(2), LABELS)\n\nplt.xlabel(\"Class\")\n\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the histogram above, we can easily notice that the number of fraud cases were very few compared to the enormous number of non-fraudulent cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud = data[data['Class']==1]\n\nnormal = data[data['Class']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fraud.shape,normal.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to analyze more amount of information from the transaction data\nHow different are the amount of money used in different transaction classes?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud.Amount.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normal.Amount.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Draw boxplots showing summary statistics for the Amount column"},{"metadata":{},"cell_type":"markdown","source":"A Box Plot is also known as **Whisker plot** is created to display the summary of the set of data values having properties like minimum, first quartile, median, third quartile and maximum. In the box plot, a box is created from the first quartile to the third quartile, a verticle line is also there which goes through the box at the median. Here x-axis denotes the data to be plotted while the y-axis shows the frequency distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data, palette=\"PRGn\",showfliers=True, showmeans=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data, palette=\"PRGn\",showfliers=False, showmeans=True)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate a correlation matrix illustrating using a heatmap the relationship between the different variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Take some sample of the data\n\ndata1= data.sample(frac = 0.1,random_state=1)\n\ndata1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Determine the number of fraud and valid transactions in the dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Fraud = data1[data1['Class']==1]\n\nValid = data1[data1['Class']==0]\n\noutlier_fraction = len(Fraud)/float(len(Valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(outlier_fraction)\n\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\n\nprint(\"Valid Cases : {}\".format(len(Valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = data1.corr()\ntop_corr_features = corrmat.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating a scatterplot for Amount and V2 showing a line of best fit using the equation of a straight line is y = mx + c, where m is the slope of the line and c is the y intercept"},{"metadata":{},"cell_type":"markdown","source":"## Get the Fraud and the normal dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud = data[data['Class']==1]\n\nnormal = data[data['Class']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fraud.shape,normal.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normal.Amount.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m, b = np.polyfit(fraud.V2, fraud.Amount, 1)\nplt.plot(fraud.V2, fraud.Amount, 'o')\nplt.plot(fraud.V2, m*fraud.V2 + b, color=\"red\")\nplt.xlabel(\"V2\")\nplt.ylabel(\"Amount\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m, b = np.polyfit(normal.V2, normal.Amount, 1)\nplt.plot(normal.V2, normal.Amount, 'o')\nplt.plot(normal.V2, m*normal.V2 + b, color=\"red\")\nplt.xlabel(\"V2\")\nplt.ylabel(\"Amount\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building an outlier detection model for the data using the Isolation Forest and the Local Outlier Factor classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create independent and Dependent Features\ncolumns = data1.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = data1[columns]\nY = data1[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=state, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(classifiers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing the models using Errors, Confusion Matrix, Accuracy Score and Classification Report to identify the strengths and weaknesses of the models"},{"metadata":{},"cell_type":"markdown","source":"## Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nn_outliers = len(Fraud)\nLABELS = [\"Nonfraudulent\", \"Fraudulent\"]\n\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Isolation Forest\":\n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    else:    \n       print(\"No other model\")\n    \n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))\n    conf_matrix = confusion_matrix(Y, y_pred)\n    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='d');\n    plt.title('Confusion matrix for ' + clf_name)\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"1. Isolation Forest detected 73 errors while Local Outlier Factor model detected 97 errors. This shows us that the Isolation Forest Method is the best one between the two. Isolation Forest Method also had a higher accuracy of 99.74% while Local outlier factor had an accuracy of 99.65%. \n1. When comparing error precision & recall for 3 models , the Isolation Forest performed much better than the LOF as we can see that the detection of fraud cases is around 27 % versus LOF detection rate of just 2%. Isolation Forest correctly detected 13 fraudulent transactions when LOF only detected 1.  So,Isolation Forest Method performed much better in determining the fraud cases which is around 30%.\n1. We can improve on this accuracy by increasing the sample size or use deep learning algorithms however at the cost of computational expense. We can also use complex anomaly detection models to get better accuracy in determining more fraudulent cases."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}