{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Python Deep Learning\n\nNotebook内容全部来源于《Python深度学习》，涵盖书中全部内容，以练习为主，理论知识较少，该书作者：Francois Chollet，即Keras之父，该书译者：张亮；","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"import os,sys,random,gc\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras import Input,models,layers,optimizers,preprocessing\nfrom keras.datasets import mnist,imdb,reuters,boston_housing\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG16\nfrom keras.models import load_model\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n%matplotlib inline","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-03T08:47:10.09259Z","iopub.execute_input":"2021-08-03T08:47:10.092909Z","iopub.status.idle":"2021-08-03T08:47:15.396837Z","shell.execute_reply.started":"2021-08-03T08:47:10.092878Z","shell.execute_reply":"2021-08-03T08:47:15.396074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n\nprint(train_images.shape)\nprint(train_labels.shape)\nprint(test_images.shape)\nprint(test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:15.399249Z","iopub.execute_input":"2021-08-03T08:47:15.399607Z","iopub.status.idle":"2021-08-03T08:47:15.80755Z","shell.execute_reply.started":"2021-08-03T08:47:15.39957Z","shell.execute_reply":"2021-08-03T08:47:15.806629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 神经网络数学基础","metadata":{}},{"cell_type":"code","source":"# reshape 3-ndim tensor(60000,28,28) to 2-nidm tensor(60000,512)\ntrain_x = train_images.reshape((60000,28*28))\ntest_x = test_images.reshape((10000,28*28))\n# scale 0~255 to 0~1\ntrain_x = train_x.astype(\"float32\")/255\ntest_x = test_x.astype(\"float32\")/255\n# labels transform\ntrain_y = to_categorical(train_labels)\ntest_y = to_categorical(test_labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:15.809466Z","iopub.execute_input":"2021-08-03T08:47:15.809833Z","iopub.status.idle":"2021-08-03T08:47:16.007839Z","shell.execute_reply.started":"2021-08-03T08:47:15.809795Z","shell.execute_reply":"2021-08-03T08:47:16.007049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import models,layers\n\n# build network framework\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512,activation=\"relu\",input_shape=(28*28,))) # 数据蒸馏，稠密连接（全连接）神经层，一阶张量，向量长度为28*28\nnetwork.add(layers.Dense(10,activation=\"softmax\")) # 输出层，10个标签，激活函数为softmax，一般用于多分类\n\n# compile：loss function, optimizer, metric\nnetwork.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\n# train model\nnetwork.fit(train_x,train_y,epochs=5,batch_size=128)\n\n# evaluate model\ntest_loss,test_acc = network.evaluate(test_x,test_y)\n\nprint(\"test loss:\",test_loss)\nprint(\"test accuracy:\",test_acc)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:16.00988Z","iopub.execute_input":"2021-08-03T08:47:16.010472Z","iopub.status.idle":"2021-08-03T08:47:25.801639Z","shell.execute_reply.started":"2021-08-03T08:47:16.01043Z","shell.execute_reply":"2021-08-03T08:47:25.800873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"张量阶：\",train_images.ndim)\nprint(\"张量形状：\",train_images.shape)\nprint(\"张量数据类型：\",train_images.dtype)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:25.805299Z","iopub.execute_input":"2021-08-03T08:47:25.805573Z","iopub.status.idle":"2021-08-03T08:47:25.813506Z","shell.execute_reply.started":"2021-08-03T08:47:25.805548Z","shell.execute_reply":"2021-08-03T08:47:25.811898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(train_images[4],cmap=plt.cm.binary)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:25.817122Z","iopub.execute_input":"2021-08-03T08:47:25.817481Z","iopub.status.idle":"2021-08-03T08:47:25.980569Z","shell.execute_reply.started":"2021-08-03T08:47:25.817446Z","shell.execute_reply":"2021-08-03T08:47:25.979859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 电影评论情感分类\n\n- 数据源：IMDB；\n- 特点：评论向量长度不一，这个长度对应的是评论文本的长度；\n\n整数序列需要处理后再送入网络：\n- 方法1：填充序列，使其具有相等的长度，然后送入Embedding层；\n- 方法2：one-hot处理，每个序列转为长度为10000（num_words）的向量，然后送入Dense层处理；","metadata":{}},{"cell_type":"code","source":"(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=10000) # 只保留出现此处最多的10000个单词\n\nprint(train_data.shape)\nprint(train_labels.shape)\nprint(test_data.shape)\nprint(test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:25.982074Z","iopub.execute_input":"2021-08-03T08:47:25.982431Z","iopub.status.idle":"2021-08-03T08:47:34.065701Z","shell.execute_reply.started":"2021-08-03T08:47:25.982393Z","shell.execute_reply":"2021-08-03T08:47:34.064695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def review(vec,dict_):\n    dict_ = {v:k for k,v in dict_.items()}\n    return \" \".join([dict_.get(int(v)-3,\"?\") for v in vec])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:34.067079Z","iopub.execute_input":"2021-08-03T08:47:34.067424Z","iopub.status.idle":"2021-08-03T08:47:34.073141Z","shell.execute_reply.started":"2021-08-03T08:47:34.067386Z","shell.execute_reply":"2021-08-03T08:47:34.072293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_ = imdb.get_word_index()\n\nprint(train_data[0])\nprint(review(train_data[0],dict_))\nprint(train_labels[0])\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:34.074482Z","iopub.execute_input":"2021-08-03T08:47:34.074924Z","iopub.status.idle":"2021-08-03T08:47:34.449991Z","shell.execute_reply.started":"2021-08-03T08:47:34.074886Z","shell.execute_reply":"2021-08-03T08:47:34.449019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences),dimension))\n    for i,sequence in enumerate(sequences):\n        results[i, sequence] = 1\n    return results\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nprint(x_train.shape)\nprint(x_train[0])\nprint(x_test.shape)\nprint(x_test[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:34.451754Z","iopub.execute_input":"2021-08-03T08:47:34.452257Z","iopub.status.idle":"2021-08-03T08:47:38.325063Z","shell.execute_reply.started":"2021-08-03T08:47:34.452217Z","shell.execute_reply":"2021-08-03T08:47:38.323896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"relu\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"binary_crossentropy\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# train and validation\nx_val = x_train[:10000]\nx_train_partial = x_train[10000:]\ny_val = train_labels[:10000]\ny_train_partial = train_labels[10000:]\n\n# W迭代更新次数=(15000/512)*20\nhistory = network.fit(x_train_partial,y_train_partial,epochs=20,batch_size=512,validation_data=(x_val,y_val))\n\nhistory.history\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:38.326873Z","iopub.execute_input":"2021-08-03T08:47:38.32734Z","iopub.status.idle":"2021-08-03T08:47:52.951645Z","shell.execute_reply.started":"2021-08-03T08:47:38.327291Z","shell.execute_reply":"2021-08-03T08:47:52.950745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:52.953004Z","iopub.execute_input":"2021-08-03T08:47:52.953544Z","iopub.status.idle":"2021-08-03T08:47:53.21625Z","shell.execute_reply.started":"2021-08-03T08:47:52.953506Z","shell.execute_reply":"2021-08-03T08:47:53.215211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df[[\"accuracy\",\"val_accuracy\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:53.217787Z","iopub.execute_input":"2021-08-03T08:47:53.218264Z","iopub.status.idle":"2021-08-03T08:47:53.38701Z","shell.execute_reply.started":"2021-08-03T08:47:53.218212Z","shell.execute_reply":"2021-08-03T08:47:53.38627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"relu\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"binary_crossentropy\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# W迭代更新次数=(25000/512)*20\nnetwork.fit(x_train,train_labels,epochs=4,batch_size=512)\n\nresult = network.evaluate(x_test,test_labels)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:53.388356Z","iopub.execute_input":"2021-08-03T08:47:53.38873Z","iopub.status.idle":"2021-08-03T08:48:00.843386Z","shell.execute_reply.started":"2021-08-03T08:47:53.3887Z","shell.execute_reply":"2021-08-03T08:48:00.842488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"relu\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"binary_crossentropy\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# W迭代更新次数=(25000/512)*20\nnetwork.fit(x_train,train_labels,epochs=4,batch_size=512)\n\nresult = network.evaluate(x_test,test_labels)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:00.846389Z","iopub.execute_input":"2021-08-03T08:48:00.846648Z","iopub.status.idle":"2021-08-03T08:48:07.951813Z","shell.execute_reply.started":"2021-08-03T08:48:00.846623Z","shell.execute_reply":"2021-08-03T08:48:07.950851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"relu\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"binary_crossentropy\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# W迭代更新次数=(25000/512)*20\nnetwork.fit(x_train,train_labels,epochs=4,batch_size=128)\n\nresult = network.evaluate(x_test,test_labels)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:07.953458Z","iopub.execute_input":"2021-08-03T08:48:07.95387Z","iopub.status.idle":"2021-08-03T08:48:16.73856Z","shell.execute_reply.started":"2021-08-03T08:48:07.953816Z","shell.execute_reply":"2021-08-03T08:48:16.737442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"relu\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"binary_crossentropy\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# W迭代更新次数=(25000/512)*20\nnetwork.fit(x_train,train_labels,epochs=4,batch_size=512)\n\nresult = network.evaluate(x_test,test_labels)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:16.741939Z","iopub.execute_input":"2021-08-03T08:48:16.742247Z","iopub.status.idle":"2021-08-03T08:48:24.104399Z","shell.execute_reply.started":"2021-08-03T08:48:16.742217Z","shell.execute_reply":"2021-08-03T08:48:24.103675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"relu\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"mse\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# W迭代更新次数=(25000/512)*20\nnetwork.fit(x_train,train_labels,epochs=4,batch_size=512)\n\nresult = network.evaluate(x_test,test_labels)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:24.106508Z","iopub.execute_input":"2021-08-03T08:48:24.106861Z","iopub.status.idle":"2021-08-03T08:48:31.478572Z","shell.execute_reply.started":"2021-08-03T08:48:24.106823Z","shell.execute_reply":"2021-08-03T08:48:31.477492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"tanh\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(16,activation=\"tanh\"))\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"binary_crossentropy\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# W迭代更新次数=(25000/512)*20\nnetwork.fit(x_train,train_labels,epochs=4,batch_size=512)\n\nresult = network.evaluate(x_test,test_labels)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:31.480325Z","iopub.execute_input":"2021-08-03T08:48:31.480908Z","iopub.status.idle":"2021-08-03T08:48:38.630835Z","shell.execute_reply.started":"2021-08-03T08:48:31.48086Z","shell.execute_reply":"2021-08-03T08:48:38.629779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(32,activation=\"relu\",input_shape=(10000,))) # relu整流激活函数实现表示空间非线性\nnetwork.add(layers.Dense(64,activation=\"relu\"))\nnetwork.add(layers.Dense(1,activation=\"sigmoid\")) # sigmoid将输出压缩到0~1之间作为二分类的类别概率值\n\nnetwork.compile(loss=\"binary_crossentropy\", # 适用于输出概率值的二分类模型\n               optimizer=\"rmsprop\", # SGD的变种\n               metrics=[\"accuracy\"])\n\n# W迭代更新次数=(25000/512)*20\nnetwork.fit(x_train,train_labels,epochs=4,batch_size=512)\n\nresult = network.evaluate(x_test,test_labels)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:38.632597Z","iopub.execute_input":"2021-08-03T08:48:38.632934Z","iopub.status.idle":"2021-08-03T08:48:45.845662Z","shell.execute_reply.started":"2021-08-03T08:48:38.632896Z","shell.execute_reply":"2021-08-03T08:48:45.845004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 新闻多分类\n\n单标签、多分类问题；","metadata":{}},{"cell_type":"code","source":"(train_data,train_labels),(test_data,test_labels) = reuters.load_data(num_words=10000)\n\nprint(train_data.shape)\nprint(train_labels.shape)\nprint(test_data.shape)\nprint(test_labels.shape)\n\ndict_ = reuters.get_word_index()\nprint(review(train_data[0],dict_))\nprint(review(test_data[0],dict_))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:45.84861Z","iopub.execute_input":"2021-08-03T08:48:45.848892Z","iopub.status.idle":"2021-08-03T08:48:46.9784Z","shell.execute_reply.started":"2021-08-03T08:48:45.848865Z","shell.execute_reply":"2021-08-03T08:48:46.976676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:46.979795Z","iopub.execute_input":"2021-08-03T08:48:46.980341Z","iopub.status.idle":"2021-08-03T08:48:47.60618Z","shell.execute_reply.started":"2021-08-03T08:48:46.980299Z","shell.execute_reply":"2021-08-03T08:48:47.605242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 功能等价于to_categorical\ndef to_one_hot(seqs,dimension=46):\n    results = np.zeros((len(seqs),dimension))\n    for i,seq in enumerate(seqs):\n        results[i,seq] = 1\n    return results\n\ny_train = to_one_hot(train_labels)\ny_test = to_one_hot(test_labels)\n\nprint(y_train[0])\nprint(y_test[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:47.607707Z","iopub.execute_input":"2021-08-03T08:48:47.608118Z","iopub.status.idle":"2021-08-03T08:48:47.643233Z","shell.execute_reply.started":"2021-08-03T08:48:47.608079Z","shell.execute_reply":"2021-08-03T08:48:47.642282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(64,activation=\"relu\",input_shape=(10000,))) # 隐藏单元数设置为64，用于构建更复杂的表示空间去识别复杂的46个类别的表示\nnetwork.add(layers.Dense(64,activation=\"relu\"))\nnetwork.add(layers.Dense(46,activation=\"softmax\")) # softmax用于多分类的激活函数，输出46个类别对应的概率，概率和为1\n\nnetwork.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nx_val = x_train[:1000]\nx_train_part = x_train[1000:]\ny_val = y_train[:1000]\ny_train_part = y_train[1000:]\n\nhistory = network.fit(x_train_part,y_train_part,epochs=20,batch_size=512,validation_data=(x_val,y_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:47.644688Z","iopub.execute_input":"2021-08-03T08:48:47.645245Z","iopub.status.idle":"2021-08-03T08:48:53.481159Z","shell.execute_reply.started":"2021-08-03T08:48:47.6452Z","shell.execute_reply":"2021-08-03T08:48:53.4804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\n\nhistory_df[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:53.483469Z","iopub.execute_input":"2021-08-03T08:48:53.483731Z","iopub.status.idle":"2021-08-03T08:48:53.652806Z","shell.execute_reply.started":"2021-08-03T08:48:53.483705Z","shell.execute_reply":"2021-08-03T08:48:53.651918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df[[\"accuracy\",\"val_accuracy\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:53.654248Z","iopub.execute_input":"2021-08-03T08:48:53.654577Z","iopub.status.idle":"2021-08-03T08:48:53.802795Z","shell.execute_reply.started":"2021-08-03T08:48:53.654549Z","shell.execute_reply":"2021-08-03T08:48:53.801746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(64,activation=\"relu\",input_shape=(10000,))) # 隐藏单元数设置为64，用于构建更复杂的表示空间去识别复杂的46个类别的表示\nnetwork.add(layers.Dense(64,activation=\"relu\"))\nnetwork.add(layers.Dense(46,activation=\"softmax\")) # softmax用于多分类的激活函数，输出46个类别对应的概率，概率和为1\n\nnetwork.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nnetwork.fit(x_train,y_train,epochs=7,batch_size=512)\n\nresult = network.evaluate(x_test,y_test)\n\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:53.804371Z","iopub.execute_input":"2021-08-03T08:48:53.804758Z","iopub.status.idle":"2021-08-03T08:48:56.680724Z","shell.execute_reply.started":"2021-08-03T08:48:53.804718Z","shell.execute_reply":"2021-08-03T08:48:56.679264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(64,activation=\"relu\",input_shape=(10000,))) # 隐藏单元数设置为64，用于构建更复杂的表示空间去识别复杂的46个类别的表示\nnetwork.add(layers.Dense(4,activation=\"relu\"))\nnetwork.add(layers.Dense(46,activation=\"softmax\")) # softmax用于多分类的激活函数，输出46个类别对应的概率，概率和为1\n\nnetwork.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nx_val = x_train[:1000]\nx_train_part = x_train[1000:]\ny_val = y_train[:1000]\ny_train_part = y_train[1000:]\n\nhistory = network.fit(x_train_part,y_train_part,epochs=20,batch_size=512,validation_data=(x_val,y_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:48:56.682203Z","iopub.execute_input":"2021-08-03T08:48:56.682594Z","iopub.status.idle":"2021-08-03T08:49:02.418439Z","shell.execute_reply.started":"2021-08-03T08:48:56.682554Z","shell.execute_reply":"2021-08-03T08:49:02.417661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(64,activation=\"relu\",input_shape=(10000,))) # 隐藏单元数设置为64，用于构建更复杂的表示空间去识别复杂的46个类别的表示\nnetwork.add(layers.Dense(128,activation=\"relu\"))\nnetwork.add(layers.Dense(46,activation=\"softmax\")) # softmax用于多分类的激活函数，输出46个类别对应的概率，概率和为1\n\nnetwork.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nx_val = x_train[:1000]\nx_train_part = x_train[1000:]\ny_val = y_train[:1000]\ny_train_part = y_train[1000:]\n\nhistory = network.fit(x_train_part,y_train_part,epochs=20,batch_size=512,validation_data=(x_val,y_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:49:02.421177Z","iopub.execute_input":"2021-08-03T08:49:02.421534Z","iopub.status.idle":"2021-08-03T08:49:08.340104Z","shell.execute_reply.started":"2021-08-03T08:49:02.4215Z","shell.execute_reply":"2021-08-03T08:49:08.339341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(64,activation=\"relu\",input_shape=(10000,))) # 隐藏单元数设置为64，用于构建更复杂的表示空间去识别复杂的46个类别的表示\nnetwork.add(layers.Dense(64,activation=\"relu\"))\nnetwork.add(layers.Dense(64,activation=\"relu\"))\nnetwork.add(layers.Dense(64,activation=\"relu\"))\nnetwork.add(layers.Dense(46,activation=\"softmax\")) # softmax用于多分类的激活函数，输出46个类别对应的概率，概率和为1\n\nnetwork.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nx_val = x_train[:1000]\nx_train_part = x_train[1000:]\ny_val = y_train[:1000]\ny_train_part = y_train[1000:]\n\nhistory = network.fit(x_train_part,y_train_part,epochs=20,batch_size=512,validation_data=(x_val,y_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:49:08.34374Z","iopub.execute_input":"2021-08-03T08:49:08.344034Z","iopub.status.idle":"2021-08-03T08:49:14.564819Z","shell.execute_reply.started":"2021-08-03T08:49:08.344006Z","shell.execute_reply":"2021-08-03T08:49:14.563965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(16,activation=\"relu\",input_shape=(10000,))) # 隐藏单元数设置为64，用于构建更复杂的表示空间去识别复杂的46个类别的表示\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(16,activation=\"relu\"))\nnetwork.add(layers.Dense(46,activation=\"softmax\")) # softmax用于多分类的激活函数，输出46个类别对应的概率，概率和为1\n\nnetwork.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nx_val = x_train[:1000]\nx_train_part = x_train[1000:]\ny_val = y_train[:1000]\ny_train_part = y_train[1000:]\n\nhistory = network.fit(x_train_part,y_train_part,epochs=50,batch_size=512,validation_data=(x_val,y_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:49:14.568537Z","iopub.execute_input":"2021-08-03T08:49:14.568822Z","iopub.status.idle":"2021-08-03T08:49:27.365443Z","shell.execute_reply.started":"2021-08-03T08:49:14.568795Z","shell.execute_reply":"2021-08-03T08:49:27.364528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 波士顿房价回归预测","metadata":{}},{"cell_type":"code","source":"(train_data,train_targets),(test_data,test_targets) = boston_housing.load_data()\n\nprint(train_data.shape)\nprint(train_data[0])\nprint(train_targets.shape)\nprint(test_data.shape)\nprint(test_data[0])\nprint(test_targets.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:49:27.377032Z","iopub.execute_input":"2021-08-03T08:49:27.377328Z","iopub.status.idle":"2021-08-03T08:49:27.444775Z","shell.execute_reply.started":"2021-08-03T08:49:27.377298Z","shell.execute_reply":"2021-08-03T08:49:27.443327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_ = train_data.mean(axis=0)\nstd_ = train_data.std(axis=0)\ntrain_data -= mean_\ntrain_data /= std_\ntest_data -= mean_\ntest_data /= std_","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:49:27.447805Z","iopub.execute_input":"2021-08-03T08:49:27.448475Z","iopub.status.idle":"2021-08-03T08:49:27.455428Z","shell.execute_reply.started":"2021-08-03T08:49:27.448427Z","shell.execute_reply":"2021-08-03T08:49:27.454187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64,activation=\"relu\",input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64,activation=\"relu\"))\n    model.add(layers.Dense(1))\n    model.compile(optimizer=\"rmsprop\",loss=\"mse\",metrics=[\"mae\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:49:27.457602Z","iopub.execute_input":"2021-08-03T08:49:27.458087Z","iopub.status.idle":"2021-08-03T08:49:27.469902Z","shell.execute_reply.started":"2021-08-03T08:49:27.458042Z","shell.execute_reply":"2021-08-03T08:49:27.468531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nk=5\nfold_size = train_data.shape[0]//k\nall_scores = []\nfor i in range(k):\n    x_train = np.concatenate([\n        train_data[:i*fold_size],\n        train_data[(i+1)*fold_size:]\n    ],axis=0)\n    y_train = np.concatenate([\n        train_targets[:i*fold_size],\n        train_targets[(i+1)*fold_size:]\n    ],axis=0)\n    x_val = train_data[i*fold_size:(i+1)*fold_size]\n    y_val = train_targets[i*fold_size:(i+1)*fold_size]\n    \n    model = build_model()\n    model.fit(x_train,y_train,epochs=100,batch_size=1,verbose=0)\n    val_mse,val_mae = model.evaluate(x_val,y_val,verbose=0)\n    all_scores.append(val_mae)\n\nprint(np.mean(all_scores),all_scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:49:27.471645Z","iopub.execute_input":"2021-08-03T08:49:27.472268Z","iopub.status.idle":"2021-08-03T08:52:52.453518Z","shell.execute_reply.started":"2021-08-03T08:49:27.472222Z","shell.execute_reply":"2021-08-03T08:52:52.452473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nk=5\nfold_size = train_data.shape[0]//k\nall_scores = []\nfor i in range(k):\n    x_train = np.concatenate([\n        train_data[:i*fold_size],\n        train_data[(i+1)*fold_size:]\n    ],axis=0)\n    y_train = np.concatenate([\n        train_targets[:i*fold_size],\n        train_targets[(i+1)*fold_size:]\n    ],axis=0)\n    x_val = train_data[i*fold_size:(i+1)*fold_size]\n    y_val = train_targets[i*fold_size:(i+1)*fold_size]\n    \n    model = build_model()\n    history = model.fit(x_train,y_train,epochs=500,batch_size=1,verbose=0,validation_data=(x_val,y_val))\n    all_scores.append(history.history[\"val_mae\"])\n\nall_scores = [np.mean([score[i] for score in all_scores]) for i in range(500)]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:52:52.455097Z","iopub.execute_input":"2021-08-03T08:52:52.455665Z","iopub.status.idle":"2021-08-03T09:13:05.744981Z","shell.execute_reply.started":"2021-08-03T08:52:52.455619Z","shell.execute_reply":"2021-08-03T09:13:05.744195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\"VAL-MAE\":all_scores}).plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:05.746481Z","iopub.execute_input":"2021-08-03T09:13:05.746818Z","iopub.status.idle":"2021-08-03T09:13:05.907391Z","shell.execute_reply.started":"2021-08-03T09:13:05.746782Z","shell.execute_reply":"2021-08-03T09:13:05.90651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\"VAL-MAE\":all_scores[10:100]}).plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:05.909087Z","iopub.execute_input":"2021-08-03T09:13:05.909468Z","iopub.status.idle":"2021-08-03T09:13:06.047453Z","shell.execute_reply.started":"2021-08-03T09:13:05.909429Z","shell.execute_reply":"2021-08-03T09:13:06.04654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\"VAL-MAE\":np.array(all_scores[10:-1])*.9+np.array(all_scores[11:])*.1}).plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:06.049058Z","iopub.execute_input":"2021-08-03T09:13:06.04942Z","iopub.status.idle":"2021-08-03T09:13:06.201088Z","shell.execute_reply.started":"2021-08-03T09:13:06.049383Z","shell.execute_reply":"2021-08-03T09:13:06.200177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel.fit(x_train,y_train,epochs=65,batch_size=1,verbose=0)\nresult = model.evaluate(test_data,test_targets)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:06.202714Z","iopub.execute_input":"2021-08-03T09:13:06.203118Z","iopub.status.idle":"2021-08-03T09:13:32.911692Z","shell.execute_reply.started":"2021-08-03T09:13:06.203082Z","shell.execute_reply":"2021-08-03T09:13:32.910828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel.fit(x_train,y_train,epochs=65,batch_size=16,verbose=0)\nresult = model.evaluate(test_data,test_targets)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:32.916175Z","iopub.execute_input":"2021-08-03T09:13:32.918377Z","iopub.status.idle":"2021-08-03T09:13:36.957807Z","shell.execute_reply.started":"2021-08-03T09:13:32.918326Z","shell.execute_reply":"2021-08-03T09:13:36.956806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 评估模型\n\n1. 数据代表性：不平衡类别的按比例划分训练集和验证集；\n2. 时间箭头：时序数据按时间前后划分；\n3. 数据冗余：避免训练集与验证集中出现重复数据，这个结果类似数据泄露；\n\n## 神经网络的数据预处理\n\n1. 向量化\n2. 标准化\n3. 缺失、异常处理\n\n## 神经网络的特征工程\n\n1. 提高模型运行速度；\n2. 减少模型依赖的数据量；\n\n因此对于DL，特征工程依然是有用的；\n\n## 过欠拟合\n\n模型对于数据中的表示的学习是不足还是过多；\n\n正则化：限制模型存储的信息量，或对其加以约束；\n\n手段：\n1. 减少网络大小，也就减少了网络中的参数个数，也就减少了存储的信息量，限制了表示空间；\n2. 添加权重正则化；\n3. 增加dropout正则化：训练中，每层的输出中有一定比例的特征被丢弃，即设置为0，一般0.2到0.5之间，注意测试时，则不会进行丢弃，因此此时的输出需要按drop比例缩小，因此测试中比训练中有更多的单元被激活，需要缩小平衡处理（PS：这两个计算过程如果都放到训练中做，则测试时不需要缩小处理，当然在训练中则是按比例放大）；\n\nDropout思想：在层的输出中人工引入噪声，以打破那些不显著的、偶然发现的表示/模式，以此实现降低过拟合，重要的表示理应是有更强的鲁棒性的；\n\n## DL用于计算机视觉\n\n针对不用的场景业务，不同类型（不同阶的张量）数据，需要使用不同的网络拓扑架构+神经层来构建模型，例如普遍用于CV领域的卷积神经网络；","metadata":{}},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\n# output shape=26 26 32, 26 26由(3,3)，也就是窗口大小决定，通道数32由入参32决定\nmodel.add(layers.Conv2D(32,(3,3),activation=\"relu\",input_shape=(28,28,1,)))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\n\n# 下面这三层用于多类别概率输出，不属于卷积神经网络拓扑结构的核心层\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64,activation=\"relu\"))\nmodel.add(layers.Dense(10,activation=\"softmax\"))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:36.959283Z","iopub.execute_input":"2021-08-03T09:13:36.959672Z","iopub.status.idle":"2021-08-03T09:13:37.165879Z","shell.execute_reply.started":"2021-08-03T09:13:36.95963Z","shell.execute_reply":"2021-08-03T09:13:37.164791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n\ntrain_x = train_images.reshape((60000,28,28,1))\ntest_x = test_images.reshape((10000,28,28,1))\n\ntrain_x = train_x.astype(\"float32\")/255\ntest_x = test_x.astype(\"float32\")/255\n\ntrain_y = to_categorical(train_labels)\ntest_y = to_categorical(test_labels)\n\nmodel.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nmodel.fit(train_x,train_y,epochs=5,batch_size=128)\n\ntest_loss,test_acc = model.evaluate(test_x,test_y)\n\nprint(\"test loss:\",test_loss)\nprint(\"test accuracy:\",test_acc)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:37.167078Z","iopub.execute_input":"2021-08-03T09:13:37.167558Z","iopub.status.idle":"2021-08-03T09:13:50.752533Z","shell.execute_reply.started":"2021-08-03T09:13:37.167521Z","shell.execute_reply":"2021-08-03T09:13:50.751759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32,(3,3),activation=\"relu\",input_shape=(28,28,1,)))\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64,activation=\"relu\"))\nmodel.add(layers.Dense(10,activation=\"softmax\"))\n\nmodel.compile(optimizer=\"rmsprop\",\n               loss=\"categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n\nmodel.fit(train_x,train_y,epochs=5,batch_size=128)\n\ntest_loss,test_acc = model.evaluate(test_x,test_y)\n\nprint(\"test loss:\",test_loss)\nprint(\"test accuracy:\",test_acc)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:13:50.755541Z","iopub.execute_input":"2021-08-03T09:13:50.755809Z","iopub.status.idle":"2021-08-03T09:14:10.46238Z","shell.execute_reply.started":"2021-08-03T09:13:50.75578Z","shell.execute_reply":"2021-08-03T09:14:10.460853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 卷积神经网络 - 猫狗分类\n\n- 小型卷积神经网络：0.74；\n- 数据增强+dropout：0.8；\n- 预训练网络提取特征：0.9；\n- 微调预训练网络：0.9；","metadata":{}},{"cell_type":"code","source":"!mkdir ../working/train\n!mkdir ../working/test\n!mkdir ../working/validation\n!mkdir ../working/train/dogs\n!mkdir ../working/validation/dogs\n!mkdir ../working/test/dogs\n!mkdir ../working/train/cats\n!mkdir ../working/validation/cats\n!mkdir ../working/test/cats\n!mkdir ../working/models\n\nimport os\ncats = []\ndogs = []\nfor f in os.listdir(\"../input/dogs-vs-cats/train/train\"):\n    if f.startswith(\"cat\") and f.endswith(\"jpg\") and len(cats)<2000:\n        cats.append(f)\n    elif f.startswith(\"dog\") and f.endswith(\"jpg\") and len(dogs)<2000:\n        dogs.append(f)\n\nfor cat in cats[:1000]:\n    os.system(\"cp ../input/dogs-vs-cats/train/train/\"+cat+\" ../working/train/cats/\")\nfor cat in cats[1000:1500]:\n    os.system(\"cp ../input/dogs-vs-cats/train/train/\"+cat+\" ../working/validation/cats/\")\nfor cat in cats[1500:]:\n    os.system(\"cp ../input/dogs-vs-cats/train/train/\"+cat+\" ../working/test/cats/\")\nfor dog in dogs[:1000]:\n    os.system(\"cp ../input/dogs-vs-cats/train/train/\"+dog+\" ../working/train/dogs/\")\nfor dog in dogs[1000:1500]:\n    os.system(\"cp ../input/dogs-vs-cats/train/train/\"+dog+\" ../working/validation/dogs/\")\nfor dog in dogs[1500:]:\n    os.system(\"cp ../input/dogs-vs-cats/train/train/\"+dog+\" ../working/test/dogs/\")\n\ntrain_dir = \"../working/train\"\ntest_dir = \"../working/test\"\nvalidation_dir = \"../working/validation\"\nprint(len(os.listdir(train_dir)))\nprint(len(os.listdir(validation_dir)))\nprint(len(os.listdir(test_dir)))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:14:10.464086Z","iopub.execute_input":"2021-08-03T09:14:10.464431Z","iopub.status.idle":"2021-08-03T09:19:44.434875Z","shell.execute_reply.started":"2021-08-03T09:14:10.464394Z","shell.execute_reply":"2021-08-03T09:19:44.43379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32,(3,3),activation=\"relu\",input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512,activation=\"relu\"))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:19:44.436713Z","iopub.execute_input":"2021-08-03T09:19:44.437167Z","iopub.status.idle":"2021-08-03T09:19:44.753112Z","shell.execute_reply.started":"2021-08-03T09:19:44.437117Z","shell.execute_reply":"2021-08-03T09:19:44.752133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"acc\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:19:44.755881Z","iopub.execute_input":"2021-08-03T09:19:44.756193Z","iopub.status.idle":"2021-08-03T09:19:44.770563Z","shell.execute_reply.started":"2021-08-03T09:19:44.756162Z","shell.execute_reply":"2021-08-03T09:19:44.769574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntrain_generator = ImageDataGenerator(rescale=1./255)\ntest_generator = ImageDataGenerator(rescale=1./255)\n\ntrain_iter = train_generator.flow_from_directory(train_dir,target_size=(150,150),batch_size=20,class_mode=\"binary\")\nvalidation_iter = test_generator.flow_from_directory(validation_dir,target_size=(150,150),batch_size=20,class_mode=\"binary\")\n\nhistory = model.fit_generator(\n    train_iter,steps_per_epoch=100, # 该参数表示每次epoch迭代次数，次数为(1000+1000)/20\n    epochs=30,\n    validation_data=validation_iter,validation_steps=50 # 与上述steps一致，次数为(500+500)/20\n)\n\nmodel.save(\"../working/models/cats_dogs_1.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:19:44.772651Z","iopub.execute_input":"2021-08-03T09:19:44.772979Z","iopub.status.idle":"2021-08-03T09:23:46.454172Z","shell.execute_reply.started":"2021-08-03T09:19:44.772929Z","shell.execute_reply":"2021-08-03T09:23:46.45338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_1 = pd.DataFrame(history.history)\nhistory_1[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:23:46.457926Z","iopub.execute_input":"2021-08-03T09:23:46.458237Z","iopub.status.idle":"2021-08-03T09:23:46.625563Z","shell.execute_reply.started":"2021-08-03T09:23:46.458208Z","shell.execute_reply":"2021-08-03T09:23:46.624578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_1[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:23:46.627114Z","iopub.execute_input":"2021-08-03T09:23:46.627591Z","iopub.status.idle":"2021-08-03T09:23:46.784904Z","shell.execute_reply.started":"2021-08-03T09:23:46.62755Z","shell.execute_reply":"2021-08-03T09:23:46.783979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ndata_gen = ImageDataGenerator(rotation_range=40,width_shift_range=.2,height_shift_range=.2,\n                              shear_range=.2,zoom_range=.2,horizontal_flip=True,fill_mode=\"nearest\")\nimg = image.load_img(\"../working/train/dogs/\"+os.listdir(\"../working/train/dogs\")[0], target_size=(150,150))\nimg_arr = image.img_to_array(img)\nimg_arr = img_arr.reshape((1,)+img_arr.shape) # reshape to 1 150 150 3\ni=0\nplt.imshow(image.img_to_array(img))\nfor batch in data_gen.flow(img_arr,batch_size=1):\n    plt.figure(i+1)\n    plt.imshow(image.array_to_img(batch[0]))\n    i+=1\n    if i%3==0:\n        break\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:23:46.786748Z","iopub.execute_input":"2021-08-03T09:23:46.787165Z","iopub.status.idle":"2021-08-03T09:23:47.476838Z","shell.execute_reply.started":"2021-08-03T09:23:46.787123Z","shell.execute_reply":"2021-08-03T09:23:47.475912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntrain_generator = ImageDataGenerator(rescale=1./255,rotation_range=40,\n                                     width_shift_range=.2,height_shift_range=.2,\n                                    shear_range=.2,zoom_range=.2,horizontal_flip=True)\ntest_generator = ImageDataGenerator(rescale=1./255)\ntrain_iter = train_generator.flow_from_directory(train_dir,target_size=(150,150),batch_size=20,class_mode=\"binary\")\nvalidation_iter = test_generator.flow_from_directory(validation_dir,target_size=(150,150),batch_size=20,class_mode=\"binary\")\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32,(3,3),activation=\"relu\",input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(.5)) # drop比例，处理过拟合问题\nmodel.add(layers.Dense(512,activation=\"relu\"))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"acc\"])\n\nhistory = model.fit_generator(\n    train_iter,steps_per_epoch=100, # 该参数表示每次epoch迭代次数，次数为(1000+1000)/20\n    epochs=30,\n    validation_data=validation_iter,validation_steps=50 # 与上述steps一致，次数为(500+500)/20\n)\n\nmodel.save(\"../working/models/cats_dogs_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:23:47.478367Z","iopub.execute_input":"2021-08-03T09:23:47.478831Z","iopub.status.idle":"2021-08-03T09:32:25.110624Z","shell.execute_reply.started":"2021-08-03T09:23:47.47879Z","shell.execute_reply":"2021-08-03T09:32:25.109754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_2 = pd.DataFrame(history.history)\nhistory_2[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:32:25.112225Z","iopub.execute_input":"2021-08-03T09:32:25.112586Z","iopub.status.idle":"2021-08-03T09:32:25.27288Z","shell.execute_reply.started":"2021-08-03T09:32:25.112549Z","shell.execute_reply":"2021-08-03T09:32:25.271938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_2[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:32:25.274589Z","iopub.execute_input":"2021-08-03T09:32:25.275024Z","iopub.status.idle":"2021-08-03T09:32:25.442803Z","shell.execute_reply.started":"2021-08-03T09:32:25.274982Z","shell.execute_reply":"2021-08-03T09:32:25.442008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv_base = VGG16(weights=\"imagenet\",include_top=False,input_shape=(150,150,3))\nconv_base.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:32:25.444399Z","iopub.execute_input":"2021-08-03T09:32:25.444751Z","iopub.status.idle":"2021-08-03T09:32:26.27376Z","shell.execute_reply.started":"2021-08-03T09:32:25.444715Z","shell.execute_reply":"2021-08-03T09:32:26.272958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n# VGG16做特征提取输出feature，即不使用数据增强\ndata_gen = ImageDataGenerator(rescale=1./255)\nbatch_size = 20\ndef extract_feature(path,sample_count):\n    features = np.zeros(shape=(sample_count,4,4,512))\n    labels = np.zeros(shape=(sample_count))\n    generator = data_gen.flow_from_directory(\n        path,target_size=(150,150),batch_size=batch_size,class_mode=\"binary\"\n    )\n    i=0\n    for input_batch,label_batch in generator:\n        features_batch = conv_base.predict(input_batch)\n        features[i*batch_size:(i+1)*batch_size]=features_batch\n        labels[i*batch_size:(i+1)*batch_size]=label_batch\n        i+=1\n        if i*batch_size >= sample_count:\n            break\n    return features,labels\n\ntrain_features,train_labels = extract_feature(train_dir,2000)\ntest_features,test_labels = extract_feature(test_dir,1000)\nvalidation_features,validation_labels = extract_feature(validation_dir,1000)\n\n# 手动展开\ntrain_features = np.reshape(train_features,(2000,4*4*512))\ntest_features = np.reshape(test_features,(1000,4*4*512))\nvalidation_features = np.reshape(validation_features,(1000,4*4*512))\n\n# 结构从原始的卷积神经网络变为只有两个隐含层的线性叠加结构，相当于之前的卷积层已经被卷积基代替了\nmodel = models.Sequential()\nmodel.add(layers.Dense(256,activation=\"relu\",input_shape=(4*4*512,)))\nmodel.add(layers.Dropout(.5))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-5),loss=\"binary_crossentropy\",metrics=[\"acc\"])\n\nhistory = model.fit(train_features,train_labels,epochs=30,batch_size=20,validation_data=(validation_features,validation_labels))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:32:26.27533Z","iopub.execute_input":"2021-08-03T09:32:26.275722Z","iopub.status.idle":"2021-08-03T09:33:00.24849Z","shell.execute_reply.started":"2021-08-03T09:32:26.275679Z","shell.execute_reply":"2021-08-03T09:33:00.247764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_3 = pd.DataFrame(history.history)\nhistory_3[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:33:00.249991Z","iopub.execute_input":"2021-08-03T09:33:00.250254Z","iopub.status.idle":"2021-08-03T09:33:00.396978Z","shell.execute_reply.started":"2021-08-03T09:33:00.250228Z","shell.execute_reply":"2021-08-03T09:33:00.396128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_3[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:33:00.398517Z","iopub.execute_input":"2021-08-03T09:33:00.398907Z","iopub.status.idle":"2021-08-03T09:33:00.539888Z","shell.execute_reply.started":"2021-08-03T09:33:00.398869Z","shell.execute_reply":"2021-08-03T09:33:00.539081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n# 将卷积基作为一个网络层添加到拓扑结构中，应用数据增强\ntrain_generator = ImageDataGenerator(rescale=1./255,rotation_range=20,\n                                     width_shift_range=.1,height_shift_range=.1,\n                                    shear_range=.2,zoom_range=.2,horizontal_flip=True)\ntest_generator = ImageDataGenerator(rescale=1./255)\ntrain_iter = train_generator.flow_from_directory(train_dir,target_size=(150,150),batch_size=20,class_mode=\"binary\")\nvalidation_iter = test_generator.flow_from_directory(validation_dir,target_size=(150,150),batch_size=20,class_mode=\"binary\")\n\nconv_base = VGG16(weights=\"imagenet\",include_top=False,input_shape=(150,150,3))\nconv_base.trainable = False\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256,activation=\"relu\"))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:33:00.541473Z","iopub.execute_input":"2021-08-03T09:33:00.541827Z","iopub.status.idle":"2021-08-03T09:33:01.29896Z","shell.execute_reply.started":"2021-08-03T09:33:00.541791Z","shell.execute_reply":"2021-08-03T09:33:01.297427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-5),loss=\"binary_crossentropy\",metrics=[\"acc\"])\n\nhistory = model.fit_generator(\n    train_iter,steps_per_epoch=100, # 该参数表示每次epoch迭代次数，次数为(1000+1000)/20\n    epochs=30,\n    validation_data=validation_iter,validation_steps=50 # 与上述steps一致，次数为(500+500)/20\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:33:01.3002Z","iopub.execute_input":"2021-08-03T09:33:01.300553Z","iopub.status.idle":"2021-08-03T09:42:07.349936Z","shell.execute_reply.started":"2021-08-03T09:33:01.300515Z","shell.execute_reply":"2021-08-03T09:42:07.349229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_4 = pd.DataFrame(history.history)\nhistory_4[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:42:07.353197Z","iopub.execute_input":"2021-08-03T09:42:07.353461Z","iopub.status.idle":"2021-08-03T09:42:07.507148Z","shell.execute_reply.started":"2021-08-03T09:42:07.353435Z","shell.execute_reply":"2021-08-03T09:42:07.506297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_4[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:42:07.508863Z","iopub.execute_input":"2021-08-03T09:42:07.509286Z","iopub.status.idle":"2021-08-03T09:42:07.703049Z","shell.execute_reply.started":"2021-08-03T09:42:07.509245Z","shell.execute_reply":"2021-08-03T09:42:07.701981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n# Fine tuning\nrefreeze = False\nfor layer in conv_base.layers:\n    if layer == \"block5_conv1\":\n        refreeze = True\n    if refreeze:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\n# 取更小的学习率，保证对于原网络表示的更新是微小的\nmodel.compile(optimizer=optimizers.RMSprop(lr=3e-6),loss=\"binary_crossentropy\",metrics=[\"acc\"])\n\n# 解冻卷积基中部分层，基于之前训练好的分类层做联合训练\nhistory = model.fit_generator(\n    train_iter,steps_per_epoch=100, # 该参数表示每次epoch迭代次数，次数为(1000+1000)/20\n    epochs=50,\n    validation_data=validation_iter,validation_steps=50 # 与上述steps一致，次数为(500+500)/20\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:42:07.705121Z","iopub.execute_input":"2021-08-03T09:42:07.705582Z","iopub.status.idle":"2021-08-03T09:57:25.844762Z","shell.execute_reply.started":"2021-08-03T09:42:07.705534Z","shell.execute_reply":"2021-08-03T09:57:25.844041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_5 = pd.DataFrame(history.history)\nhistory_5[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:25.847852Z","iopub.execute_input":"2021-08-03T09:57:25.848129Z","iopub.status.idle":"2021-08-03T09:57:25.997414Z","shell.execute_reply.started":"2021-08-03T09:57:25.848101Z","shell.execute_reply":"2021-08-03T09:57:25.996653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_5[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:25.998581Z","iopub.execute_input":"2021-08-03T09:57:25.998969Z","iopub.status.idle":"2021-08-03T09:57:26.175236Z","shell.execute_reply.started":"2021-08-03T09:57:25.998924Z","shell.execute_reply":"2021-08-03T09:57:26.174468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 深度神经网络可视化\n\n对于视觉世界任务，相关可视化方法甚至要好于人工特征工程的可视化效果，这与视觉世界的平移不变、层次空间等特性与卷积神经网络提取表示特征想符合有很大关系，同样的结构用于其他任务，可视化效果可能就没这么理想；\n\n- 卷积核输出结果可视化（深度为N个过滤器）；\n","metadata":{}},{"cell_type":"code","source":"model = load_model(\"../working/models/cats_dogs_2.h5\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:26.176793Z","iopub.execute_input":"2021-08-03T09:57:26.1772Z","iopub.status.idle":"2021-08-03T09:57:26.337318Z","shell.execute_reply.started":"2021-08-03T09:57:26.177157Z","shell.execute_reply":"2021-08-03T09:57:26.335561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nimg_path = \"../working/test/dogs/\"+random.choice(os.listdir(\"../working/test/dogs\"))\nimg = image.load_img(img_path,target_size=(150,150))\nimg_tensor = image.img_to_array(img)\nprint(img_tensor.shape)\nimg_tensor = np.expand_dims(img_tensor,axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n\nlayer_outputs = [layer.output for layer in model.layers[:8]] # 只获取前8层，也就是卷积层和最大池层的输出 - 中间输出\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(img_tensor)\nfor i in range(len(activations)):\n    print(activations[i].shape) # 输出的shape\n\n\n# 第一个卷积层的第八个过滤器\nplt.figure(figsize=(15,20))\nplt.subplot(5,1,1)\nplt.imshow(img_tensor[0])\n\nidx = random.choice(list(range(activations[0].shape[3])))\nvec = activations[0][0,:,:,idx]\nplt.subplot(5,4,5)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,9)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[1][0,:,:,idx]\nplt.subplot(5,4,13)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,17)\nplt.imshow(vec,cmap=\"viridis\")\n\nidx = random.choice(list(range(activations[2].shape[3])))\nvec = activations[2][0,:,:,idx]\nplt.subplot(5,4,6)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,10)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[3][0,:,:,idx]\nplt.subplot(5,4,14)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,18)\nplt.imshow(vec,cmap=\"viridis\")\n\nidx = random.choice(list(range(activations[4].shape[3])))\nvec = activations[4][0,:,:,idx]\nplt.subplot(5,4,7)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,11)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[5][0,:,:,idx]\nplt.subplot(5,4,15)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,19)\nplt.imshow(vec,cmap=\"viridis\")\n\nidx = random.choice(list(range(activations[6].shape[3])))\nvec = activations[6][0,:,:,idx]\nplt.subplot(5,4,8)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,12)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[7][0,:,:,idx]\nplt.subplot(5,4,16)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,20)\nplt.imshow(vec,cmap=\"viridis\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:26.339388Z","iopub.execute_input":"2021-08-03T09:57:26.339723Z","iopub.status.idle":"2021-08-03T09:57:28.808798Z","shell.execute_reply.started":"2021-08-03T09:57:26.339686Z","shell.execute_reply":"2021-08-03T09:57:28.808003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nlayer_names = [layer.name for layer in model.layers]\nimages_per_row = 16\nfor layer_name, layer_activation in zip(layer_names,activations):\n    n_features = layer_activation.shape[-1]\n    size = layer_activation.shape[1]\n    n_cols = n_features // images_per_row\n    display_grid = np.zeros((size*n_cols,images_per_row*size))\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,:,:,col*images_per_row+row]\n            channel_image = ((channel_image - channel_image.mean())/channel_image.std())*64+128\n            channel_image = np.clip(channel_image,0,255).astype(\"uint8\")\n            display_grid[col*size:(col+1)*size,row*size:(row+1)*size] = channel_image\n    scale = 1./size\n    plt.figure(figsize=(scale*display_grid.shape[1],scale*display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid,aspect=\"auto\",cmap=\"viridis\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:28.812066Z","iopub.execute_input":"2021-08-03T09:57:28.812401Z","iopub.status.idle":"2021-08-03T09:57:30.663385Z","shell.execute_reply.started":"2021-08-03T09:57:28.812354Z","shell.execute_reply":"2021-08-03T09:57:30.662363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nimg_path = \"../working/test/cats/\"+random.choice(os.listdir(\"../working/test/cats\"))\nimg = image.load_img(img_path,target_size=(150,150))\nimg_tensor = image.img_to_array(img)\nprint(img_tensor.shape)\nimg_tensor = np.expand_dims(img_tensor,axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n\nlayer_outputs = [layer.output for layer in model.layers[:8]] # 只获取前8层，也就是卷积层和最大池层的输出 - 中间输出\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(img_tensor)\nfor i in range(len(activations)):\n    print(activations[i].shape) # 输出的shape\n\n\n# 第一个卷积层的第八个过滤器\nplt.figure(figsize=(15,20))\nplt.subplot(5,1,1)\nplt.imshow(img_tensor[0])\n\nidx = random.choice(list(range(activations[0].shape[3])))\nvec = activations[0][0,:,:,idx]\nplt.subplot(5,4,5)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,9)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[1][0,:,:,idx]\nplt.subplot(5,4,13)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,17)\nplt.imshow(vec,cmap=\"viridis\")\n\nidx = random.choice(list(range(activations[2].shape[3])))\nvec = activations[2][0,:,:,idx]\nplt.subplot(5,4,6)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,10)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[3][0,:,:,idx]\nplt.subplot(5,4,14)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,18)\nplt.imshow(vec,cmap=\"viridis\")\n\nidx = random.choice(list(range(activations[4].shape[3])))\nvec = activations[4][0,:,:,idx]\nplt.subplot(5,4,7)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,11)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[5][0,:,:,idx]\nplt.subplot(5,4,15)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,19)\nplt.imshow(vec,cmap=\"viridis\")\n\nidx = random.choice(list(range(activations[6].shape[3])))\nvec = activations[6][0,:,:,idx]\nplt.subplot(5,4,8)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,12)\nplt.imshow(vec,cmap=\"viridis\")\nvec = activations[7][0,:,:,idx]\nplt.subplot(5,4,16)\nplt.imshow(np.clip(((vec-vec.mean())/vec.std())*64+128,0,255).astype(\"uint8\"),cmap=\"viridis\")\nplt.subplot(5,4,20)\nplt.imshow(vec,cmap=\"viridis\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:30.665111Z","iopub.execute_input":"2021-08-03T09:57:30.665578Z","iopub.status.idle":"2021-08-03T09:57:33.312229Z","shell.execute_reply.started":"2021-08-03T09:57:30.665532Z","shell.execute_reply":"2021-08-03T09:57:33.311373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nlayer_names = [layer.name for layer in model.layers]\nimages_per_row = 16\nfor layer_name, layer_activation in zip(layer_names,activations):\n    n_features = layer_activation.shape[-1]\n    size = layer_activation.shape[1]\n    n_cols = n_features // images_per_row\n    display_grid = np.zeros((size*n_cols,images_per_row*size))\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,:,:,col*images_per_row+row]\n            channel_image = ((channel_image - channel_image.mean())/channel_image.std())*64+128\n            channel_image = np.clip(channel_image,0,255).astype(\"uint8\")\n            display_grid[col*size:(col+1)*size,row*size:(row+1)*size] = channel_image\n    scale = 1./size\n    plt.figure(figsize=(scale*display_grid.shape[1],scale*display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid,aspect=\"auto\",cmap=\"viridis\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:33.313698Z","iopub.execute_input":"2021-08-03T09:57:33.314128Z","iopub.status.idle":"2021-08-03T09:57:35.211795Z","shell.execute_reply.started":"2021-08-03T09:57:33.314083Z","shell.execute_reply":"2021-08-03T09:57:35.210623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat_output = model.output[:,0]\n# last_conv_layer = model.get_layer(\"conv2d_3\")\n# grads = K.gradients(cat_output,last_conv_layer.output)[0]\n# pooled_grads = K.mean(grads,axis=(0,1,2))\n# iterate = K.function([model.input],[pooled_grads,last_conv_layer.output[0]])\n# pooled_grads_value,conv_layer_output_value = iterate(img_tensor)\n# for i in range(512):\n#     conv_layer_output_value[:,:,i]*=pooled_grads_value[i]\n# heatmap = np.mean(conv_layer_output,axis=-1)\n# heatmap = np.maximum(heatmap,0)\n# heatmap /= np.max(heatmap)\n# plt.matshow(heatmap)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:35.213725Z","iopub.execute_input":"2021-08-03T09:57:35.214215Z","iopub.status.idle":"2021-08-03T09:57:35.218904Z","shell.execute_reply.started":"2021-08-03T09:57:35.21417Z","shell.execute_reply":"2021-08-03T09:57:35.217851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DL应用于序列问题\n\n- 文本序列；\n- 时间序列；\n- 其他序列；\n\n序列问题特点：序列元素前后关系、上下文、历史依赖性；\n\n对于文本序列组成单元：\n- 字；\n- 词；\n- n-grams，词组对；\n称为token，分词更一般的指的是将文本转为token集合，只不过一般值得是分为**词**的集合；\n\n文本数据向量化：\n- one-hot；\n- token嵌入（一般指词嵌入）；","metadata":{}},{"cell_type":"code","source":"gc.collect()\n(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=10000) # 获取出现次数最多的前10000个单词\nx_train = preprocessing.sequence.pad_sequences(x_train,maxlen=100) # 文本截取前20个单词\nx_test = preprocessing.sequence.pad_sequences(x_test,maxlen=100)\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:35.220764Z","iopub.execute_input":"2021-08-03T09:57:35.221225Z","iopub.status.idle":"2021-08-03T09:57:45.152874Z","shell.execute_reply.started":"2021-08-03T09:57:35.221183Z","shell.execute_reply":"2021-08-03T09:57:45.152082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 利用Embedding层学习词嵌入字典\n# 嵌入层后直接接全连接层做分类任务，缺陷在于模型无法全面的考虑token之间的关系以及上下文、语境等，而是一个一个的针对单个token做计算\n# you love i和i love you在这种情况下模型无法分辨其差异\nmodel = models.Sequential()\nmodel.add(layers.Embedding(10000,8,input_length=100)) # 10000为上述索引最大值+1，8为单个token向量化后的向量长度\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"acc\"])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:45.154235Z","iopub.execute_input":"2021-08-03T09:57:45.154572Z","iopub.status.idle":"2021-08-03T09:57:45.198963Z","shell.execute_reply.started":"2021-08-03T09:57:45.154544Z","shell.execute_reply":"2021-08-03T09:57:45.198184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nhistory = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=.2)\nhistory_df = pd.DataFrame(history.history)\nhistory_df[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:57:45.200157Z","iopub.execute_input":"2021-08-03T09:57:45.200543Z","iopub.status.idle":"2021-08-03T09:58:04.122874Z","shell.execute_reply.started":"2021-08-03T09:57:45.200511Z","shell.execute_reply":"2021-08-03T09:58:04.122125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:58:04.126417Z","iopub.execute_input":"2021-08-03T09:58:04.12671Z","iopub.status.idle":"2021-08-03T09:58:04.283439Z","shell.execute_reply.started":"2021-08-03T09:58:04.126682Z","shell.execute_reply":"2021-08-03T09:58:04.282703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 循环神经网络 - RNN\n\n循环的目的在于一步一步的计算时间步内结果并作为状态向后传递，循环作用于单个输入序列上（类似人眼扫过一大段话时，会一小段一小段的看，同时大脑中的内容模型会根据已经看过的内容实时更新，最后看完整句话，并理解了它的意思），在两个不同的序列之间，状态会被重置；\n\n- SimpleRNN：最简单循环神经网络，SimpleRNN的问题在于当处于时刻t，理论说，此时模型应该记住所有之前的时间步见过的信息，但是实际上它不可能学习到这种**长期依赖**，原因在于**梯度消失问题**；\n- LSTM：长短期记忆，是SimpleRNN的一种变体，简单地说它增加了一种可以跨越多个时间步传递信息的机制，以解决SimpleRNN的长期依赖捕获不到的问题；\n- GRU：\n\nRNN高级用法：\n- 循环dropout：\n- 堆叠循环层：\n- 双向循环：","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntimesteps = 10\ninput_features = 8\noutput_features = 16\n\ninput_seq = np.random.random((timesteps,input_features))\n\nW = np.random.random((output_features,input_features))\nU = np.random.random((output_features,output_features))\nB = np.random.random((output_features,))\n\noutput_seq = []\nstate_t = np.zeros((output_features,))\nfor input_t in input_seq:\n    output_t = np.tanh(np.dot(W,input_t)+np.dot(U,state_t)+B)\n    output_seq.append(output_t)\n    state_t = output_t\n    \nprint(input_seq)\nprint(output_seq)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:58:04.28601Z","iopub.execute_input":"2021-08-03T09:58:04.286415Z","iopub.status.idle":"2021-08-03T09:58:04.534711Z","shell.execute_reply.started":"2021-08-03T09:58:04.28637Z","shell.execute_reply":"2021-08-03T09:58:04.533822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\nmodel.add(layers.Embedding(10000,32))\nmodel.add(layers.SimpleRNN(32)) # 每个序列的处理只输出最后一个时间步对应的结果\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:58:04.53784Z","iopub.execute_input":"2021-08-03T09:58:04.539736Z","iopub.status.idle":"2021-08-03T09:58:04.808746Z","shell.execute_reply.started":"2021-08-03T09:58:04.539685Z","shell.execute_reply":"2021-08-03T09:58:04.807709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\nmodel.add(layers.Embedding(10000,32))\nmodel.add(layers.SimpleRNN(32,return_sequences=True)) # 输出所有时间步的结果\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:58:04.810336Z","iopub.execute_input":"2021-08-03T09:58:04.810681Z","iopub.status.idle":"2021-08-03T09:58:05.059719Z","shell.execute_reply.started":"2021-08-03T09:58:04.810644Z","shell.execute_reply":"2021-08-03T09:58:05.059009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\nmodel.add(layers.Embedding(10000,32))\nmodel.add(layers.SimpleRNN(32,return_sequences=True)) # 堆叠循环层\nmodel.add(layers.SimpleRNN(32,return_sequences=True))\nmodel.add(layers.SimpleRNN(32,return_sequences=True))\nmodel.add(layers.SimpleRNN(32))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:58:05.06183Z","iopub.execute_input":"2021-08-03T09:58:05.062373Z","iopub.status.idle":"2021-08-03T09:58:05.45067Z","shell.execute_reply.started":"2021-08-03T09:58:05.062324Z","shell.execute_reply":"2021-08-03T09:58:05.449766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmax_features,maxlen,batch_size = 10000,500,32\n(input_train,y_train),(input_test,y_test) = imdb.load_data(num_words=max_features)\ninput_train = preprocessing.sequence.pad_sequences(input_train,maxlen=maxlen)\ninput_test = preprocessing.sequence.pad_sequences(input_test,maxlen=maxlen)\nprint(input_train.shape)\nprint(y_train.shape)\nprint(input_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:58:05.452061Z","iopub.execute_input":"2021-08-03T09:58:05.452431Z","iopub.status.idle":"2021-08-03T09:58:16.013199Z","shell.execute_reply.started":"2021-08-03T09:58:05.452392Z","shell.execute_reply":"2021-08-03T09:58:16.012286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\nmodel.add(layers.Embedding(max_features,32))\nmodel.add(layers.SimpleRNN(32))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"acc\"])\n\nhistory = model.fit(input_train,y_train,epochs=10,batch_size=batch_size,validation_split=.2)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:58:16.014833Z","iopub.execute_input":"2021-08-03T09:58:16.01545Z","iopub.status.idle":"2021-08-03T10:24:14.788777Z","shell.execute_reply.started":"2021-08-03T09:58:16.015407Z","shell.execute_reply":"2021-08-03T10:24:14.787973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:24:14.790382Z","iopub.execute_input":"2021-08-03T10:24:14.79064Z","iopub.status.idle":"2021-08-03T10:24:14.936291Z","shell.execute_reply.started":"2021-08-03T10:24:14.790615Z","shell.execute_reply":"2021-08-03T10:24:14.935428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:24:14.938019Z","iopub.execute_input":"2021-08-03T10:24:14.938376Z","iopub.status.idle":"2021-08-03T10:24:15.071297Z","shell.execute_reply.started":"2021-08-03T10:24:14.938339Z","shell.execute_reply":"2021-08-03T10:24:15.070315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel = models.Sequential()\nmodel.add(layers.Embedding(max_features,32))\nmodel.add(layers.LSTM(32)) # 缓解SimpleRNN中由于梯度消失导致的无法识别长期依赖问题\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"acc\"])\n\nhistory = model.fit(input_train,y_train,epochs=10,batch_size=batch_size,validation_split=.2)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:24:15.072979Z","iopub.execute_input":"2021-08-03T10:24:15.073332Z","iopub.status.idle":"2021-08-03T10:26:26.061323Z","shell.execute_reply.started":"2021-08-03T10:24:15.073296Z","shell.execute_reply":"2021-08-03T10:26:26.060503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[[\"loss\",\"val_loss\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:26.063034Z","iopub.execute_input":"2021-08-03T10:26:26.063344Z","iopub.status.idle":"2021-08-03T10:26:26.228578Z","shell.execute_reply.started":"2021-08-03T10:26:26.063313Z","shell.execute_reply":"2021-08-03T10:26:26.227716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df[[\"acc\",\"val_acc\"]].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:26.230023Z","iopub.execute_input":"2021-08-03T10:26:26.230428Z","iopub.status.idle":"2021-08-03T10:26:26.40396Z","shell.execute_reply.started":"2021-08-03T10:26:26.230388Z","shell.execute_reply":"2021-08-03T10:26:26.403095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 循环神经网络应用于时序序列\n\n- 循环dropout；\n- 堆叠循环层；\n- 双向循环；\n\n时序序列问题抽象：\n1. 设timestep=10min，每个step采集一次数据；\n2. 给定lookback个step之内的数据；\n3. 能够预测delay个step之后的某个target；","metadata":{}},{"cell_type":"code","source":"jc_df = pd.read_csv(\"../input/jena-climate-2009-2016/jena_climate_2009_2016.csv\")\njc_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:26.407815Z","iopub.execute_input":"2021-08-03T10:26:26.408198Z","iopub.status.idle":"2021-08-03T10:26:27.646013Z","shell.execute_reply.started":"2021-08-03T10:26:26.408165Z","shell.execute_reply":"2021-08-03T10:26:27.645084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jc_df[\"T (degC)\"].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:27.647642Z","iopub.execute_input":"2021-08-03T10:26:27.648Z","iopub.status.idle":"2021-08-03T10:26:27.852965Z","shell.execute_reply.started":"2021-08-03T10:26:27.647963Z","shell.execute_reply":"2021-08-03T10:26:27.851855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jc_df.iloc[:1440][\"T (degC)\"].plot() # 每天144个点，前10天温度数据","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:27.855088Z","iopub.execute_input":"2021-08-03T10:26:27.855534Z","iopub.status.idle":"2021-08-03T10:26:28.007149Z","shell.execute_reply.started":"2021-08-03T10:26:27.855485Z","shell.execute_reply":"2021-08-03T10:26:28.00605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 标准化\nfloat_data = jc_df.drop(\"Date Time\",axis=1).values\nmean = float_data[:200000].mean(axis=0)\nfloat_data -= mean\nstd = float_data[:20000].std(axis=0)\nfloat_data /= std\nfloat_data","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:28.009219Z","iopub.execute_input":"2021-08-03T10:26:28.009675Z","iopub.status.idle":"2021-08-03T10:26:28.051896Z","shell.execute_reply.started":"2021-08-03T10:26:28.009627Z","shell.execute_reply":"2021-08-03T10:26:28.05089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 利用yield关键字构建时间序列样本及其target生成器\ndef generator(data,lookback,delay,min_index,max_index,shuffle=False,batch_size=128,step=6):\n    '''\n    data\n    lookback\n    delay\n    min_index,max_index\n    shuffle\n    batch_size\n    step\n    '''\n    max_index = len(data)-delay-1 if max_index is None else max_index\n    i = min_index + lookback\n    while True:\n        if shuffle:\n            rows = np.random.randint(min_index+lookback,max_index,size=batch_size)\n        else:\n            i = min_index + lookback if i + batch_size >= max_index else i\n            rows = np.arange(i,min(i+batch_size,max_index))\n            i += len(rows)\n        samples = np.zeros((len(rows),lookback//step,data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j,row in enumerate(rows):\n            indices = range(rows[j]-lookback,rows[j],step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j]+delay][1]\n        yield samples,targets","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:28.053421Z","iopub.execute_input":"2021-08-03T10:26:28.053781Z","iopub.status.idle":"2021-08-03T10:26:28.065798Z","shell.execute_reply.started":"2021-08-03T10:26:28.053744Z","shell.execute_reply":"2021-08-03T10:26:28.064657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lookback,step,delay,batch_size = 1440,6,144,128\ntrain_gen = generator(data=float_data,lookback=lookback,delay=delay,min_index=0,max_index=200000,shuffle=True,step=step,batch_size=batch_size)\nval_gen = generator(data=float_data,lookback=lookback,delay=delay,min_index=200001,max_index=300000,shuffle=True,step=step,batch_size=batch_size)\ntest_gen = generator(data=float_data,lookback=lookback,delay=delay,min_index=300001,max_index=None,shuffle=True,step=step,batch_size=batch_size)\n\nval_steps = (300000-200001-lookback) // batch_size\ntest_steps = (len(float_data)-300001-lookback) // batch_size","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:28.067747Z","iopub.execute_input":"2021-08-03T10:26:28.068599Z","iopub.status.idle":"2021-08-03T10:26:28.079179Z","shell.execute_reply.started":"2021-08-03T10:26:28.068472Z","shell.execute_reply":"2021-08-03T10:26:28.078194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# naive method\ndef evaluate_naive_method():\n    batch_maes = []\n    for step in range(val_steps):\n        samples,targets = next(val_gen)\n        preds = samples[:,-1,1]\n        mae = np.mean(np.abs(preds-targets))\n        batch_maes.append(mae)\n    print(\"naive method mae=\",np.mean(batch_maes))\nevaluate_naive_method()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:28.080804Z","iopub.execute_input":"2021-08-03T10:26:28.081189Z","iopub.status.idle":"2021-08-03T10:26:36.730286Z","shell.execute_reply.started":"2021-08-03T10:26:28.081129Z","shell.execute_reply":"2021-08-03T10:26:36.728781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 密集连接网络模型\nmodel = models.Sequential()\nmodel.add(layers.Flatten(input_shape=(lookback//step,float_data.shape[-1])))\nmodel.add(layers.Dense(32,activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=optimizers.RMSprop(),loss='mae')\nhistory = model.fit_generator(train_gen,steps_per_epoch=500,epochs=20,validation_data=val_gen,validation_steps=val_steps)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:26:36.73166Z","iopub.execute_input":"2021-08-03T10:26:36.73202Z","iopub.status.idle":"2021-08-03T10:32:16.811041Z","shell.execute_reply.started":"2021-08-03T10:26:36.73198Z","shell.execute_reply":"2021-08-03T10:32:16.810095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss','val_loss']].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:16.814232Z","iopub.execute_input":"2021-08-03T10:32:16.814502Z","iopub.status.idle":"2021-08-03T10:32:16.995387Z","shell.execute_reply.started":"2021-08-03T10:32:16.814474Z","shell.execute_reply":"2021-08-03T10:32:16.994253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RNN GRU基准模型\nmodel = models.Sequential()\nmodel.add(layers.GRU(32,input_shape=(None,float_data.shape[-1])))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=optimizers.RMSprop(),loss='mae')\nhistory = model.fit_generator(train_gen,steps_per_epoch=500,epochs=20,validation_data=val_gen,validation_steps=val_steps)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:34:32.924135Z","iopub.execute_input":"2021-08-03T10:34:32.924492Z","iopub.status.idle":"2021-08-03T10:41:16.570855Z","shell.execute_reply.started":"2021-08-03T10:34:32.92446Z","shell.execute_reply":"2021-08-03T10:41:16.570115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss','val_loss']].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:41:16.572985Z","iopub.execute_input":"2021-08-03T10:41:16.573325Z","iopub.status.idle":"2021-08-03T10:41:16.731506Z","shell.execute_reply.started":"2021-08-03T10:41:16.573286Z","shell.execute_reply":"2021-08-03T10:41:16.730652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RNN GRU+Dropout\nmodel = models.Sequential()\nmodel.add(layers.GRU(32,dropout=.2,recurrent_dropout=.2,input_shape=(None,float_data.shape[-1])))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=optimizers.RMSprop(),loss='mae')\nhistory = model.fit_generator(train_gen,steps_per_epoch=500,epochs=20,validation_data=val_gen,validation_steps=val_steps)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:41:16.733239Z","iopub.execute_input":"2021-08-03T10:41:16.733595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss','val_loss']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RNN GRU堆叠+Dropout\nmodel = models.Sequential()\nmodel.add(layers.GRU(32,dropout=.1,recurrent_dropout=.5,return_sequence=True,input_shape=(None,float_data.shape[-1])))\nmodel.add(layers.GRU(64,activation='relu',dropout=.1,recurrent_dropout=.5))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer=optimizers.RMSprop(),loss='mae')\nhistory = model.fit_generator(train_gen,steps_per_epoch=500,epochs=20,validation_data=val_gen,validation_steps=val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss','val_loss']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 高级深度学习最佳实践\n\n- Keras函数式API；\n- Keras回调函数；\n- TensorBoard可视化工具；\n- 开发先进模型的重要最佳实践；\n    - 高级架构模式：\n        1. 批标准化；\n        2. 深度可分离卷积；\n        3. 残差连接；\n    - 超参数优化；\n    - 集成模型；\n\n多模态数据输入：\n- 构建多个模型，每个模型处理一种输入，最后结果加权融合 - 缺点：多个模型学习的内容可能是互相冗余的，数据分散降低了假设空间的可能性；\n- 构建单个模型同时处理多个输入 - 优点：不需要额外处理结果，对数据的使用充分且不会冗余；\n\n多任务输出：\n- 构建多个模型 - 缺点：会有重复工作量，数据特征互相是相关的；\n- 构建单个模型输出多个任务输出 - 优点：任务之间的相关性使得模型浅层表示会对各个任务都有用；","metadata":{}},{"cell_type":"code","source":"# Sequential构建的都是线性堆叠层的模型，结构简单明了，单输入单输出；\n# Keras函数式API可以构建类图结构模型，且支持多输入、多输出；\n# 多模态输入：文本、结构化数据、图片共同服务于一个任务；\n\ninput_tensor = Input(shape=(64,))\nx = layers.Dense(32,activation=\"relu\",input_shape=(64,))(input_tensor)\nx = layers.Dense(32,activation=\"relu\")(x)\noutput_tensor = layers.Dense(10,activation=\"softmax\")(x)\n\nmodel = models.Model(input_tensor,output_tensor)\nmodel.summary()\n\nmodel.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\")\n\nx_train = np.random.random((1000,64))\ny_train = np.random.random((1000,10))\nmodel.fit(x_train,y_train,epochs=10,batch_size=128)\nmodel.evaluate(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.32156Z","iopub.status.idle":"2021-08-03T10:32:17.322288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 生成式深度学习\n\n顾名思义，不是用于被动性（目标识别）、反应性（驾驶汽车），而是创造性的任务；","metadata":{}},{"cell_type":"code","source":"def reweight_distribution(distribution, temperature=.5):\n    distribution = np.log(distribution)/temperature\n    distribution = np.exp(distribution)\n    return distribution/np.sum(distribution)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.323797Z","iopub.status.idle":"2021-08-03T10:32:17.324624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr = np.array([0.1,0.2,0.3,0.4])\nprint(arr)\nprint(reweight_distribution(arr,.01))\nprint(reweight_distribution(arr,.1))\nprint(reweight_distribution(arr,.5))\nprint(reweight_distribution(arr,.8))\nprint(reweight_distribution(arr,1.))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.325931Z","iopub.status.idle":"2021-08-03T10:32:17.326657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 变分自编码器VAE与生成式对抗网络GAN","metadata":{}},{"cell_type":"code","source":"# VAE编码器网络\nimg_shape = (28,28,1)\nbatch_size = 16\nlatent_dim = 2 # 潜在空间维度\n\ninput_img = Input(shape=img_shape)\nx = layers.Conv2D(32,3,padding=\"same\",activation=\"relu\")(input_img)\nx = layers.Conv2D(64,3,padding=\"same\",activation=\"relu\",strides=(2,2))(x)\nx = layers.Conv2D(64,3,padding=\"same\",activation=\"relu\")(x)\nx = layers.Conv2D(64,3,padding=\"same\",activation=\"relu\")(x)\nshape_before_flattening = K.int_shape(x)\nx = layers.Flatten()(x)\nx = layers.Dense(32,activation=\"relu\")(x)\nz_mean = layers.Dense(latent_dim)(x) # 输入图像最终被编码器编码为z_mean和z_log_var两个参数\nz_log_var = layers.Dense(latent_dim)(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.32809Z","iopub.status.idle":"2021-08-03T10:32:17.328787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # VAE编码器网络\n# img_shape = (500,1)\n# batch_size = 16\n# latent_dim = 2 # 潜在空间维度\n\n# input_img = Input(shape=img_shape)\n# x = layers.Conv1D(32,3,padding=\"same\",activation=\"relu\")(input_img)\n# x = layers.Conv1D(64,3,padding=\"same\",activation=\"relu\",strides=2)(x)\n# x = layers.Conv1D(64,3,padding=\"same\",activation=\"relu\")(x)\n# x = layers.Conv1D(64,3,padding=\"same\",activation=\"relu\")(x)\n# shape_before_flattening = K.int_shape(x)\n# x = layers.Flatten()(x)\n# x = layers.Dense(32,activation=\"relu\")(x)\n# z_mean = layers.Dense(latent_dim)(x) # 输入图像最终被编码器编码为z_mean和z_log_var两个参数\n# z_log_var = layers.Dense(latent_dim)(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.330334Z","iopub.status.idle":"2021-08-03T10:32:17.331053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 潜在空间的采样函数\n# 使用z_mean和z_log_var生成空间中的一个点\ndef sampling(args):\n    z_mean_,z_log_var_ = args\n    epsilon = K.random_normal(shape=(K.shape(z_mean_)[0],latent_dim),mean=0.,stddev=1.)\n    return z_mean_ + K.exp(.5*z_log_var_) * epsilon\nz = layers.Lambda(sampling)([z_mean,z_log_var])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.332481Z","iopub.status.idle":"2021-08-03T10:32:17.333195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 潜在空间的采样函数\n# # 使用z_mean和z_log_var生成空间中的一个点\n# def sampling(args):\n#     z_mean_,z_log_var_ = args\n#     epsilon = K.random_normal(shape=(K.shape(z_mean_)[0],latent_dim),mean=0.,stddev=1.)\n#     return z_mean_ + K.exp(.5*z_log_var_) * epsilon\n# z = layers.Lambda(sampling)([z_mean,z_log_var])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.334785Z","iopub.status.idle":"2021-08-03T10:32:17.335653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VAE解码器网络，将潜在空间的点映射为图像\ndecoder_input = Input(K.int_shape(z)[1:])\nx = layers.Dense(np.prod(shape_before_flattening[1:]),activation=\"relu\")(decoder_input)\nx = layers.Reshape(shape_before_flattening[1:])(x)\nx = layers.Conv2DTranspose(32,3,padding=\"same\",activation=\"relu\",strides=(2,2))(x)\nx = layers.Conv2D(1,3,padding=\"same\",activation=\"sigmoid\")(x)\ndecoder = models.Model(decoder_input,x)\nz_decoded = decoder(z) # 编码器与解码器组合","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.337029Z","iopub.status.idle":"2021-08-03T10:32:17.337734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # VAE解码器网络，将潜在空间的点映射为图像\n# decoder_input = Input(K.int_shape(z)[1:])\n# x = layers.Dense(np.prod(shape_before_flattening[1:]),activation=\"relu\")(decoder_input)\n# x = layers.Reshape(shape_before_flattening[1:])(x)\n# x = layers.Conv1DTranspose(32,3,padding=\"same\",activation=\"relu\",strides=2)(x)\n# x = layers.Conv1D(1,3,padding=\"same\",activation=\"sigmoid\")(x)\n# decoder = models.Model(decoder_input,x)\n# z_decoded = decoder(z) # 编码器与解码器组合","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.33934Z","iopub.status.idle":"2021-08-03T10:32:17.340108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 自定义损失函数层\nclass CustomVariationalLayer(keras.layers.Layer):\n    def vae_loss(self,x,z_decoded,z_mean,z_log_var):\n        x = K.flatten(x)\n        z_decoded = K.flatten(z_decoded)\n        xent_loss = keras.metrics.binary_crossentropy(x,z_decoded)\n        k1_loss = -5e-4 * K.mean(1+z_log_var-K.square(z_mean)-K.exp(z_log_var),axis=-1)\n        return K.mean(xent_loss+k1_loss)\n    def call(self,inputs):\n        x,z_decoded,z_mean,z_log_var = inputs\n        loss = self.vae_loss(x,z_decoded,z_mean,z_log_var)\n        self.add_loss(loss,inputs=inputs)\n        return x\n\ny = CustomVariationalLayer()([input_img,z_decoded,z_mean,z_log_var])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.341608Z","iopub.status.idle":"2021-08-03T10:32:17.342313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 自定义损失函数层\n# class CustomVariationalLayer(keras.layers.Layer):\n#     def vae_loss(self,x,z_decoded,z_mean,z_log_var):\n#         x = K.flatten(x)\n#         z_decoded = K.flatten(z_decoded)\n#         xent_loss = keras.metrics.binary_crossentropy(x,z_decoded)\n#         k1_loss = -5e-4 * K.mean(1+z_log_var-K.square(z_mean)-K.exp(z_log_var),axis=-1)\n#         return K.mean(xent_loss+k1_loss)\n#     def call(self,inputs):\n#         x,z_decoded,z_mean,z_log_var = inputs\n#         loss = self.vae_loss(x,z_decoded,z_mean,z_log_var)\n#         self.add_loss(loss,inputs=inputs)\n#         return x\n\n# y = CustomVariationalLayer()([input_img,z_decoded,z_mean,z_log_var])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.343867Z","iopub.status.idle":"2021-08-03T10:32:17.344641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 实例化模型并训练，不需要指定loss，也不需要y_train\nvae = models.Model(input_img,y)\nvae.compile(optimizer=\"RMSprop\",loss=None)\nvae.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.346061Z","iopub.status.idle":"2021-08-03T10:32:17.346807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n(x_train,_),(x_test,y_test) = mnist.load_data()\nx_train = x_train.astype(\"float32\") / 255.\nx_train = x_train.reshape(x_train.shape+(1,))\nx_test = x_test.astype(\"float32\") / 255.\nx_test = x_test.reshape(x_test.shape+(1,))\n\nvae.fit(x=x_train,y=None,shuffle=True,epochs=10,batch_size=batch_size,validation_data=(x_test,None))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.34831Z","iopub.status.idle":"2021-08-03T10:32:17.349053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gc.collect()\n# max_features,maxlen,batch_size = 10000,500,32\n# (x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n# x_train = preprocessing.sequence.pad_sequences(input_train,maxlen=maxlen)\n# x_test = preprocessing.sequence.pad_sequences(input_test,maxlen=maxlen)\n# print(x_train.shape)\n# print(y_train.shape)\n# print(x_test.shape)\n# print(y_test.shape)\n\n# vae.fit(x=x_train,y=None,shuffle=True,epochs=10,batch_size=batch_size,validation_data=(x_test,None))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.350726Z","iopub.status.idle":"2021-08-03T10:32:17.351676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n=15\n# print(np.linspace(0.05,0.95,n)) # 获取a到b的n等分集合\n# print(norm.ppf(np.linspace(0.05,0.95,n))) # ppf：累积分布函数的逆函数，返回传入x对应的正态分布的x轴坐标，0对应0.5，两侧依次类推","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.352995Z","iopub.status.idle":"2021-08-03T10:32:17.353694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 从潜在空间采样，解码成图像\nn = 15\ndigit_size = 28\nfigure = np.zeros((digit_size*n, digit_size*n))\ngrid_x = norm.ppf(np.linspace(0.05,0.95,n))\ngrid_y = norm.ppf(np.linspace(0.05,0.95,n))\nfor i,yi in enumerate(grid_x):\n    for j,xi in enumerate(grid_y):\n        z_sample = np.array([[xi,yi]])\n        z_sample = np.tile(z_sample,batch_size).reshape(batch_size,2)\n        x_decoded = decoder.predict(z_sample,batch_size=batch_size)\n        digit = x_decoded[0].reshape(digit_size,digit_size)\n        figure[i*digit_size:(i+1)*digit_size,j*digit_size:(j+1)*digit_size] = digit\n\nplt.figure(figsize=(10,10))\nplt.imshow(figure,cmap=\"Greys_r\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.355186Z","iopub.status.idle":"2021-08-03T10:32:17.355934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 从潜在空间采样，解码成文本\n# n = 15\n# digit_size = maxlen\n# figure = np.zeros((digit_size*n, digit_size*n))\n# grid_x = norm.ppf(np.linspace(0.05,0.95,n))\n# grid_y = norm.ppf(np.linspace(0.05,0.95,n))\n# for i,yi in enumerate(grid_x):\n#     for j,xi in enumerate(grid_y):\n#         z_sample = np.array([[xi,yi]])\n#         z_sample = np.tile(z_sample,batch_size).reshape(batch_size,2)\n#         x_decoded = decoder.predict(z_sample,batch_size=batch_size)\n#         digit = x_decoded[0].reshape(digit_size,)\n#         print(digit)\n#         break\n#     break","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.357318Z","iopub.status.idle":"2021-08-03T10:32:17.358055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xi,yi = norm.ppf([0.95,0.05])\nprint(xi,yi)\nz_sample = np.array([[xi,yi]])\nz_sample = np.tile(z_sample,batch_size).reshape(batch_size,2)\nx_decoded = decoder.predict(z_sample,batch_size=batch_size)\ndigit = x_decoded[0].reshape(digit_size,digit_size)\nplt.imshow(digit,cmap=\"Greys_r\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.359519Z","iopub.status.idle":"2021-08-03T10:32:17.360231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xi,yi = norm.ppf([0.85,0.15])\nprint(xi,yi)\nz_sample = np.array([[xi,yi]])\nz_sample = np.tile(z_sample,batch_size).reshape(batch_size,2)\nx_decoded = decoder.predict(z_sample,batch_size=batch_size)\ndigit = x_decoded[0].reshape(digit_size,digit_size)\nplt.imshow(digit,cmap=\"Greys_r\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.361648Z","iopub.status.idle":"2021-08-03T10:32:17.362369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xi,yi = norm.ppf([0.75,0.25])\nprint(xi,yi)\nz_sample = np.array([[xi,yi]])\nz_sample = np.tile(z_sample,batch_size).reshape(batch_size,2)\nx_decoded = decoder.predict(z_sample,batch_size=batch_size)\ndigit = x_decoded[0].reshape(digit_size,digit_size)\nplt.imshow(digit,cmap=\"Greys_r\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:32:17.363845Z","iopub.status.idle":"2021-08-03T10:32:17.364565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The end.\nFrom SIBAT.","metadata":{}}]}