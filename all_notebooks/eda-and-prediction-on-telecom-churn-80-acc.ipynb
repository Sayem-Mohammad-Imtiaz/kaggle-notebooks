{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intoduction","metadata":{}},{"cell_type":"markdown","source":"The development and success of a company not only depends on attracting new customers but also keeping the existing ones. Therefore, it is vital to investigate what motivates the customers to leave and make predictions accordingly, hence we can take action on those who are likely to drop out to prevent outflow. The telecom-users dataset contains around 6 thousand records of customers from a telecom company. The attributes include demographics of the customers, the services they subscribe to, the billing information, and most importantly, whether the contracts are renewed. The objectives of this notebook are to explore the relations between customers' features and churn and build models to predict whether a customer would leave. The notebook consists of five sections: \n* Import Data and Cleaning\n* Explore the Distribution of Target and Features\n* Explore the Effects of Features on Target\n* Preprocess\n* Build Models and Make Prediction","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nfrom math import floor\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import  plot_confusion_matrix, classification_report\n%matplotlib inline\nsns.set_theme()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data and Cleaning","metadata":{}},{"cell_type":"code","source":"# import data from csv file and show the head\ndf = pd.read_csv('../input/telecom-users-dataset/telecom_users.csv')\ndf.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first take a brief look at data types & non-null counts in each columns\ndf.info()\n# check duplicated entries\ndf.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that the first column is redundant. So I am dropping it and set the customerID as the index.\nAlso, the data type of TotalCharges and SeniorCitizen are changed to make further inspection easier. ","metadata":{}},{"cell_type":"code","source":"# drop Unnamed: 0 and set customerID as index\ndf.drop('Unnamed: 0',axis = 1, inplace = True)\ndf.set_index('customerID',inplace = True)\n# change the data types of TotalCharges and SeniorCitizen \ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['SeniorCitizen'] = df['SeniorCitizen'].astype('object')\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 10 entries with null TotalCharges, from the column names we can infer that TotalCharges could be tenure * MonthlyCharges. So let's make a scatter plot to see if it's the case. \n","metadata":{}},{"cell_type":"code","source":"eval_TotalCharges = df.tenure * df.MonthlyCharges\nax = sns.scatterplot(x = eval_TotalCharges,y = df.TotalCharges )\nax.set(xlabel = 'Evaluated TotalCharges',ylabel ='Actual TotalCharges' )\n#after seeing the scatter plort, i think it's now safe to fill null TotalCharges with tenure * MonthlyCharges\ndf['TotalCharges'].fillna(df.tenure * df.MonthlyCharges,inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore the Distribution of Target and Features","metadata":{}},{"cell_type":"markdown","source":"**First take a look at the target variable.**","metadata":{}},{"cell_type":"code","source":"# set the palette to colorblind-friendly style\nsns.set_palette('colorblind')\nsns.countplot(data = df, x = 'Churn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Distribution of numeric features**","metadata":{}},{"cell_type":"code","source":"# extract numeric and categorical columns\nnum_cols = df.columns[df.dtypes!='object']\nnon_num_cols = df.columns[(df.dtypes=='object') & (df.columns!='Churn')]\n\nfig, axes = plt.subplots(1,3,figsize=(18, 5))\nfor i,col in enumerate(num_cols):\n    sns.violinplot(ax = axes[i], y = df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Distribution of categorical features**","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(8,2,figsize=(15,60))\nfor i,col in enumerate(non_num_cols):\n    plt_col = (i+floor(i/8))%2\n    plt_row = i%8\n    counts = df[col].value_counts()\n    counts.plot.pie(ax = axes[plt_row,plt_col],explode=[0.03]*df[col].nunique(),autopct=\"%.1f%%\",labeldistance = 1.05,radius = 0.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the dataset is pretty clean already. There are no unreasonable outliers or categories that are irrelevant to the column names. So we are good to move on to the next section without further cleaning.","metadata":{}},{"cell_type":"markdown","source":"# Explore the Effects of Features on Target","metadata":{}},{"cell_type":"markdown","source":"**Effects of numeric features**","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df,vars =num_cols, hue = \"Churn\",kind = 'kde')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The first pattern observed is that clients are most likely to drop out within the first couple of months. \n* And this is particularly the case for those who received expensive bills right after signing up. We can find the densest churn where the monthly charges are above 70 and the tenure is close to 0.","metadata":{}},{"cell_type":"code","source":"#df.groupby('Churn').gender.value_counts().plot(kind = 'bar)\nfig, axes = plt.subplots(4,4,figsize=(20, 20),sharey = True)\n#df.groupby('Churn').tenure.hist(alpha = 0.7,legend = True,bins = 20)\nfor i,col in enumerate(non_num_cols):\n    plt_col = i%4\n    plt_row = floor(i/4)\n    chart = sns.countplot(ax =axes[plt_row,plt_col],x= col ,hue = 'Churn' ,data = df)\n    if df[col].astype(str).str.len().max()>20:\n        chart.set_xticklabels(chart.get_xticklabels(), rotation=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* demographics have little impact on target \n* Under the condition that internet service is subscribed, clients who subscribe to additional services are more likely to stay.\n* Month-to-Month contracts make it easier to change service providers.","metadata":{}},{"cell_type":"markdown","source":"# **Preprocess**","metadata":{}},{"cell_type":"markdown","source":"Before fitting data into the models, we need to transform categorical data into numeric. As most categories are not ordinal, one-hot encoding will be applied instead of integer encoding.","metadata":{}},{"cell_type":"code","source":"# keep drop_first argument false, and manually select redundant dummies to drop and keep the relevant ones\ndf_dummies = pd.get_dummies(df)\ndf_dummies.drop(df_dummies.columns[df_dummies.columns.str.endswith('No internet service')],axis=1,inplace = True)\ndf_dummies.drop(['gender_Male', 'SeniorCitizen_0', 'Partner_No', 'Dependents_No', 'PhoneService_No','PaperlessBilling_No','Churn_No'],axis=1,inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corMatrix = df_dummies.corr()\nplt.figure(figsize=(40,25))\nsns.heatmap(corMatrix,  annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corMatrix['Churn_Yes'].sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_dummies.drop(['Churn_Yes'],axis = 1)\nscaler = StandardScaler()\nscaler.fit(X)\nX1 = scaler.transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Models and Make Prediction","metadata":{}},{"cell_type":"code","source":"\ny = df_dummies.Churn_Yes\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42,stratify =y)\nlr = LogisticRegression(max_iter = 500,random_state = 42)\nlr.fit(X_train,y_train)\nplot_confusion_matrix(lr, X_test, y_test)\npredy = lr.predict(X_test)\nprint(classification_report(y_test,predy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.DataFrame(lr.coef_.transpose(),index = X.columns)\nfeatures.sort_values(by = 0, ascending = False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfclf = RandomForestClassifier(n_estimators = 500,criterion = 'entropy',random_state = 42)\nrfclf.fit(X_train,y_train)\nplot_confusion_matrix(rfclf, X_test, y_test)\npredy = rfclf.predict(X_test)\nprint(classification_report(y_test,predy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.DataFrame(rfclf.feature_importances_,index = X.columns)\nfeatures.sort_values(by = 0, ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbclf = GradientBoostingClassifier(n_estimators = 500,random_state = 42)\ngbclf.fit(X_train,y_train)\nplot_confusion_matrix(gbclf, X_test, y_test)\npredy = gbclf.predict(X_test)\nprint(classification_report(y_test,predy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.DataFrame(gbclf.feature_importances_,index = X.columns)\nfeatures.sort_values(by = 0, ascending = False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LogisticRegression scored the best in overall accuracy, but GradientBoostingClassifier performed slightly better in terms of churn recall, and the top features are more aligned with what we observed in the EDA.","metadata":{}}]}