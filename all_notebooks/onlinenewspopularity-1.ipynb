{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ------------------> Online News Popularity <-----------------"},{"metadata":{},"cell_type":"markdown","source":"<img src='http://globalorphanprevention.org/uploads/3/4/3/1/34317250/1434670_orig.jpg' width='1200px'/>"},{"metadata":{},"cell_type":"markdown","source":"## Contributions \n- **[Abhisar](https://www.kaggle.com/abhisarnarkhede)** : Data collection and cleaning\n- **[Deepak](https://www.kaggle.com/deepakshende) and [Amir](https://www.kaggle.com/aahaan007)** : Data exploration, analysis, reporting and dashboarding\n- **[Vishnu](https://www.kaggle.com/psvishnu)**: Data modelling and deployment\n- **[Sneharth](https://www.kaggle.com/sneharth03) and [Sharmistha](https://www.kaggle.com/sharmistha96)** : Assisting all the above steps however, most importantly will define business requirement, data validation part."},{"metadata":{},"cell_type":"markdown","source":"## Business understanding\n* This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks, i.e. how popular any given article is. The dataset is publicly available at University of California Irvine Machine Learning Repository."},{"metadata":{},"cell_type":"markdown","source":"## Attribute Description :\n\n#### Input variables : \n* url: URL of the article (non-predictive)\n* timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n* n_tokens_title: Number of words in the title\n* n_tokens_content: Number of words in the content\n* n_unique_tokens: Rate of unique words in the content\n* n_non_stop_words: Rate of non-stop words in the content\n* n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n* num_hrefs: Number of links\n* num_self_hrefs: Number of links to other articles published by Mashable\n* num_imgs: Number of images\n* num_videos: Number of videos\n* average_token_length: Average length of the words in the content\n* num_keywords: Number of keywords in the metadata\n* data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n* data_channel_is_entertainment: Is data channel 'Entertainment'?\n* data_channel_is_bus: Is data channel 'Business'?\n* data_channel_is_socmed: Is data channel 'Social Media'?\n* data_channel_is_tech: Is data channel 'Tech'?\n* data_channel_is_world: Is data channel 'World'?\n* kw_min_min: Worst keyword (min. shares)\n* kw_max_min: Worst keyword (max. shares)\n* kw_avg_min: Worst keyword (avg. shares)\n* kw_min_max: Best keyword (min. shares)\n* kw_max_max: Best keyword (max. shares)\n* kw_avg_max: Best keyword (avg. shares)\n* kw_min_avg: Avg. keyword (min. shares)\n* kw_max_avg: Avg. keyword (max. shares)\n* kw_avg_avg: Avg. keyword (avg. shares)\n* self_reference_min_shares: Min. shares of referenced articles in Mashable\n* self_reference_max_shares: Max. shares of referenced articles in Mashable\n* self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n* weekday_is_monday: Was the article published on a Monday?\n* weekday_is_tuesday: Was the article published on a Tuesday?\n* weekday_is_wednesday: Was the article published on a Wednesday?\n* weekday_is_thursday: Was the article published on a Thursday?\n* weekday_is_friday: Was the article published on a Friday?\n* weekday_is_saturday: Was the article published on a Saturday?\n* weekday_is_sunday: Was the article published on a Sunday?\n* is_weekend: Was the article published on the weekend?\n* LDA_00: Closeness to LDA topic 0\n* LDA_01: Closeness to LDA topic 1\n* LDA_02: Closeness to LDA topic 2\n* LDA_03: Closeness to LDA topic 3\n* LDA_04: Closeness to LDA topic 4\n* global_subjectivity: Text subjectivity\n* global_sentiment_polarity: Text sentiment polarity\n* global_rate_positive_words: Rate of positive words in the content\n* global_rate_negative_words: Rate of negative words in the content\n* rate_positive_words: Rate of positive words among non-neutral tokens\n* rate_negative_words: Rate of negative words among non-neutral tokens\n* avg_positive_polarity: Avg. polarity of positive words\n* min_positive_polarity: Min. polarity of positive words\n* max_positive_polarity: Max. polarity of positive words\n* avg_negative_polarity: Avg. polarity of negative words\n* min_negative_polarity: Min. polarity of negative words\n* max_negative_polarity: Max. polarity of negative words\n* title_subjectivity: Title subjectivity\n* title_sentiment_polarity: Title polarity\n* abs_title_subjectivity: Absolute subjectivity level\n* abs_title_sentiment_polarity: Absolute polarity level\n* shares: Number of shares (target)"},{"metadata":{},"cell_type":"markdown","source":"## Dependencies"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport os # accessing directory structure\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading and Describing data"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"nRowsRead = None\ndf_newspopularity = pd.read_csv('/kaggle/input/OnlineNewsPopularity.csv', delimiter=',', nrows = nRowsRead)\ndf_newspopularity.dataframeName = 'OnlineNewsPopularity.csv'\nnRow, nCol = df_newspopularity.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's take a quick look at what the data looks like (first 250 rows) :"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df_newspopularity.head(250)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Describing each column :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_newspopularity.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution graphs (histogram/bar graph) of each column :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df_newspopularity, 61, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation matrix :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_newspopularity.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(df_newspopularity, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scatter and Density plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df_newspopularity, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}