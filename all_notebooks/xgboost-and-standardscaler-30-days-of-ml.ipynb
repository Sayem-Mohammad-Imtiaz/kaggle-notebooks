{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,make_scorer\nfrom xgboost import XGBRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-23T02:00:17.906531Z","iopub.execute_input":"2021-08-23T02:00:17.906944Z","iopub.status.idle":"2021-08-23T02:00:19.085838Z","shell.execute_reply.started":"2021-08-23T02:00:17.906899Z","shell.execute_reply":"2021-08-23T02:00:19.084975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/train-folds-5/train_folds.csv\")\ndf_test=pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsubmission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:19.087609Z","iopub.execute_input":"2021-08-23T02:00:19.087975Z","iopub.status.idle":"2021-08-23T02:00:22.509602Z","shell.execute_reply.started":"2021-08-23T02:00:19.087936Z","shell.execute_reply":"2021-08-23T02:00:22.508751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PART I: EXPLORATORY DATA ANALYSIS","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:22.511359Z","iopub.execute_input":"2021-08-23T02:00:22.511743Z","iopub.status.idle":"2021-08-23T02:00:22.516891Z","shell.execute_reply.started":"2021-08-23T02:00:22.511705Z","shell.execute_reply":"2021-08-23T02:00:22.516034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:22.518972Z","iopub.execute_input":"2021-08-23T02:00:22.519363Z","iopub.status.idle":"2021-08-23T02:00:22.559837Z","shell.execute_reply.started":"2021-08-23T02:00:22.519327Z","shell.execute_reply":"2021-08-23T02:00:22.559052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:22.561144Z","iopub.execute_input":"2021-08-23T02:00:22.561551Z","iopub.status.idle":"2021-08-23T02:00:22.589191Z","shell.execute_reply.started":"2021-08-23T02:00:22.561509Z","shell.execute_reply":"2021-08-23T02:00:22.588171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return any column with missing values. No columns with missing values found\ndf.columns[df.isnull().any()]","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:22.590816Z","iopub.execute_input":"2021-08-23T02:00:22.591369Z","iopub.status.idle":"2021-08-23T02:00:22.854906Z","shell.execute_reply.started":"2021-08-23T02:00:22.591328Z","shell.execute_reply":"2021-08-23T02:00:22.854028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #Histograms for each var\ndf_hist = df.hist(bins=10,figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:22.856118Z","iopub.execute_input":"2021-08-23T02:00:22.856446Z","iopub.status.idle":"2021-08-23T02:00:24.987638Z","shell.execute_reply.started":"2021-08-23T02:00:22.856417Z","shell.execute_reply":"2021-08-23T02:00:24.986801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a heatmap to show correlation\nfig,axes = plt.subplots(1,1,figsize=(16,14))\nsns.heatmap(df.corr(),annot=True, cmap=\"RdYlGn\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:24.990039Z","iopub.execute_input":"2021-08-23T02:00:24.990412Z","iopub.status.idle":"2021-08-23T02:00:26.628738Z","shell.execute_reply.started":"2021-08-23T02:00:24.990373Z","shell.execute_reply":"2021-08-23T02:00:26.627893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select categorical vars only\n\ndf_cat = df.select_dtypes(include = 'object').copy()\n# counts of each var value\ndf_cat.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:26.630128Z","iopub.execute_input":"2021-08-23T02:00:26.630446Z","iopub.status.idle":"2021-08-23T02:00:27.138034Z","shell.execute_reply.started":"2021-08-23T02:00:26.630415Z","shell.execute_reply":"2021-08-23T02:00:27.137009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the numbers of each unique values for each categorical var using lambda expression\ndf_cat.apply(lambda x:x.value_counts()).T.stack()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:27.139627Z","iopub.execute_input":"2021-08-23T02:00:27.139979Z","iopub.status.idle":"2021-08-23T02:00:27.828475Z","shell.execute_reply.started":"2021-08-23T02:00:27.139943Z","shell.execute_reply":"2021-08-23T02:00:27.827453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a var for useful features which are not 'id', 'kfold' or 'target', extract the features we don't want to use\nkept_features = [useful_cols for useful_cols in df.columns if useful_cols not in ('id','kfold','target')]\n# creating a var for columns need to be encoded (object columns)\nobject_cols = [col for col in kept_features if 'cat' in col]\n# creating a var for numerical columns for feature engineering\nnumerical_cols = [col for col in kept_features if 'cont' in col]\n# removing 'id','target' and 'kfold' from df_test, convert our test df to useful features only \ndf_test=df_test[kept_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:27.829942Z","iopub.execute_input":"2021-08-23T02:00:27.830337Z","iopub.status.idle":"2021-08-23T02:00:27.859813Z","shell.execute_reply.started":"2021-08-23T02:00:27.830297Z","shell.execute_reply":"2021-08-23T02:00:27.859009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params={'colsample_bytree': 0.1,\n            'learning_rate': 0.0752111846201471,\n            'max_depth': 5,\n            'n_estimators': 1000,\n            'reg_alpha': 1e-09,\n            'reg_lambda': 100.0,\n            'subsample': 1.0,\n            'min_child_weight':6,\n            'booster' : 'gbtree',\n            'tree_method' :'gpu_hist',\n            'predictor': 'gpu_predictor',\n            'gpu_id' : 0,\n            'scale_pos_weight' :1,\n            'predictor': 'gpu_predictor',\n            'early_stopping_rounds' : 5,\n            'eval_metric':'rmse',\n            'gamma' : 0,\n            #Subsample ratio of the training instances, \n            #Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n            'subsample': 0.96,\n            \n           }","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:27.861074Z","iopub.execute_input":"2021-08-23T02:00:27.86146Z","iopub.status.idle":"2021-08-23T02:00:27.866824Z","shell.execute_reply.started":"2021-08-23T02:00:27.86142Z","shell.execute_reply":"2021-08-23T02:00:27.866057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PART II: FITTING THE MODEL AND MAKE PREDICTIONS","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:27.868027Z","iopub.execute_input":"2021-08-23T02:00:27.868546Z","iopub.status.idle":"2021-08-23T02:00:27.881376Z","shell.execute_reply.started":"2021-08-23T02:00:27.868512Z","shell.execute_reply":"2021-08-23T02:00:27.880451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating a list for final predictions\nfinal_predictions = []\nscores = []\n# creating a for loop to loop over fold, reserving -1 fold for training data\nfor fold in range(5):\n    # training data that is not at fold\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    #training data that is at fold\n    x_valid = df[df.kfold == fold ].reset_index(drop=True)\n    # making a copy of the test set to avoid errors\n    x_test = df_test.copy()\n    # creating the training dataset, validation dataset and test dataset\n    y_train= x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train[kept_features]\n    x_valid = x_valid[kept_features]\n     # ordinal-encode categorical columns\n    OE = preprocessing.OrdinalEncoder()\n    \n    # always fit_transform on the training data\n    x_train[object_cols] = OE.fit_transform(x_train[object_cols])\n    \n    #transform on the validation and test sets\n    x_valid[object_cols] = OE.transform(x_valid[object_cols])\n    x_test[object_cols] = OE.transform(x_test[object_cols])\n    \n    # Feature engineering for numerical var : standardisation\n    \n    scaler = preprocessing.StandardScaler()\n    # standardise training data, using .fit_transform()\n    x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n    #standardise validation and test data using .transform()\n    x_valid[numerical_cols] = scaler.transform(x_valid[numerical_cols])\n    x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n    #using XGBRegressor instead since rf takes a long time to run\n    model = XGBRegressor(**xgb_params)\n    model.fit(x_train, y_train)\n    preds_valid = model.predict(x_valid)\n    test_preds = model.predict(x_test)\n    final_predictions.append(test_preds)\n    RMSE = mean_squared_error(y_valid, preds_valid, squared=False)\n    print(fold,RMSE)\n    scores.append(RMSE)\n\n    print (np.mean(scores),np.std(scores))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:00:27.882606Z","iopub.execute_input":"2021-08-23T02:00:27.882956Z","iopub.status.idle":"2021-08-23T02:01:04.110372Z","shell.execute_reply.started":"2021-08-23T02:00:27.882918Z","shell.execute_reply":"2021-08-23T02:01:04.109463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making predictions, taking the mean of the predictions of all 5 models\npreds = np.mean(np.column_stack(final_predictions), axis=1)\npreds ","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:01:04.111528Z","iopub.execute_input":"2021-08-23T02:01:04.111839Z","iopub.status.idle":"2021-08-23T02:01:04.124547Z","shell.execute_reply.started":"2021-08-23T02:01:04.111805Z","shell.execute_reply":"2021-08-23T02:01:04.123431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a submission\nsubmission.target = preds\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T02:01:42.842203Z","iopub.execute_input":"2021-08-23T02:01:42.842539Z","iopub.status.idle":"2021-08-23T02:01:43.33155Z","shell.execute_reply.started":"2021-08-23T02:01:42.842509Z","shell.execute_reply":"2021-08-23T02:01:43.330441Z"},"trusted":true},"execution_count":null,"outputs":[]}]}