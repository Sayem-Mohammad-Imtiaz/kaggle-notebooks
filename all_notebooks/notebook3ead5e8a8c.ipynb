{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Board Game Rating Prediction (Term Project)\n## Name: Jay Nitin Chaphekar\n## UTA ID: 1001763932"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThis project is based on the NLP (Natural Language Processing) domain which includes the analysing text data and predicting sentiments based on the user reviews. This includes analyzing the data removing stopwords, creating vocabulary of meaningful words, finding the best classfier with suitable hyperarameters and based on that predicting the rating of the review "},{"metadata":{},"cell_type":"markdown","source":"## Project Description\nThe goal of this project is to predict ratings of the given game reviews by the users. After selecting a best model hosting it to the website for good user experience.\n"},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nimport pickle\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading Data\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"data=pd.read_csv('C:/Users/chaph/Desktop/DM/archive/bgg-15m-reviews.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the data we can see that the data has six columns. For the rating prediction we only need rating and the comment columns."},{"metadata":{"trusted":false},"cell_type":"code","source":"data=data.iloc[:,[2,3]]\ndata['comment'] = data['comment'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(data[\"comment\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the data we can see that our data has more positive data than the negative data and has more reviews with rating 7-8."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.hist(data[\"rating\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.rating.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing the data"},{"metadata":{},"cell_type":"markdown","source":"### Dropping null values "},{"metadata":{"trusted":false},"cell_type":"code","source":"data=data.dropna(subset=[\"comment\"])\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shuffling the data for the randomness in the predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"data=data.sample(frac=1)\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the rating values are like 8.6, 8.7, 8.8 we will round off the rating values to get 10 classes for the 10 classifications."},{"metadata":{"trusted":false},"cell_type":"code","source":"#X=data.iloc[:,1]\nX=text_counts\nY= data.iloc[:,0]\nY= np.rint(Y)\nY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For this project I will use various classifiers and then will select the best classifier among them\n1. Naive Bayes (MultinominalNB and BernouliNB)\n2. Decision Tree\n3. SVM\n4. Ridge Classifier\n\nFor this above mentioned classifiers I will try 3 different approaches for document and word processing to get best results\n\n1. CounterVectorization\n2. TfidfVectorizer\n3. HashingVectorizer"},{"metadata":{},"cell_type":"markdown","source":"### 1. CounterVectorization:\nThe count vectorizer is used to tokenize documents and build vocabulary of meaningful words and to encode new documents with vocabulary.. We will tokenize the data and build vocabulary from given review data and then provide that data to our various classifiers to analyze the results.\nWe will also use n_gram range for our countervectorizer, an n-gram is a contiguous sequence of n items from a given sample of text which will give us a list where n-words will occur frequently "},{"metadata":{},"cell_type":"markdown","source":"### Initializing CounterVectorizer"},{"metadata":{"trusted":false},"cell_type":"code","source":"token = RegexpTokenizer(r'[a-zA-Z0-9]+')\ncv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\ntext_counts = cv.fit_transform(data['comment'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the dataset into Training, Development and Testing sets"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set=[]\ntest_set=[]\ndev_set=[]\nxtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=1)\n\nxtrain, xdev, ytrain, ydev = train_test_split(xtrain, ytrain, test_size=0.25, random_state=1) \nxtrain\n#train_set, dev_set, test_set = np.split(data, [int(.6*len(data)), int(.8*len(data))])\n#print(len(train_set),len(dev_set),len(test_set))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implementing Naive Bayes Algorithm"},{"metadata":{},"cell_type":"markdown","source":"### MultinomialNB\nMultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification where the data are typically represented as word vector counts."},{"metadata":{"trusted":false},"cell_type":"code","source":"MNB = MultinomialNB()\nMNB.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted = MNB.predict(xdev)\naccuracy_score = metrics.accuracy_score(predicted, ydev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(str('{:04.2f}'.format(accuracy_score*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth naive Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(predicted)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mnb=accuracy_metric(list(ydev), np.round(predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BernoulliNB\nBernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. "},{"metadata":{"trusted":false},"cell_type":"code","source":"BNB = BernoulliNB()\nBNB.fit(xtrain, ytrain)\naccuracy_score_bnb = metrics.accuracy_score(BNB.predict(xdev),ydev)\nprint('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth BernNb Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(BNB.predict(xdev))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bnb=accuracy_metric(list(ydev), np.round(BNB.predict(xdev)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree\nDecision Trees are the popular way to to split data based on classes or conditions which are used for both regression and classification tasks. Decision Trees splits the data according to conditions and based on that assigns values to the each node calculating impurity of the classified solution. Here we will be using <b>gini index</b> as our impurity criterion.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=2, min_samples_split=20, min_samples_leaf=5).fit(xtrain, ytrain)\npredict_y = clf.predict(xdev)\nprint('Decision tree accuracy: ', metrics.accuracy_score(ydev, predict_y)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth Decision Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(predict_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dt=accuracy_metric(list(ydev), np.round(predict_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine\nSupport Vector Machine is a very famous machine learning classifier which divides the data into the hyperplanes by separating the classes by drawing different lines according to the data and then selects the best line which is most equidistant from the each class.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf = svm.LinearSVC(C=1.0, penalty='l2').fit(xtrain, ytrain)\npredict_y = clf.predict(xdev)\nprint('SVM accuracy: ', metrics.accuracy_score(ydev, predict_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth SVM Accuracy: ', accuracy_metric(list(ydev), np.round(predict_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svmv=accuracy_metric(list(ydev), np.round(predict_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Classifier\nRidge classifier is a classification algorithm that uses ridge regression to classify multi-nomial values. For multi-class classification, n_class classifiers are trained in a one-versus-all approach."},{"metadata":{"trusted":false},"cell_type":"code","source":"rc = RidgeClassifier()\nrc.fit(xtrain, ytrain)\nscore = rc.score(xtrain, ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"srv=rc.predict(xdev)\nprint('Ridge Classifier Accuracy: ', metrics.accuracy_score(ydev, srv))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print('Ridge Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(srv)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rcv=accuracy_metric(list(ydev), np.round(srv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"acc=[mnb,bnb,dt,svmv,rcv]\nval=['MultinomialNB','BernouliNB','Decision Tree','SVM','Ridge']\nplt.bar(val,acc, color='g',width=0.25)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Smoothing:\nSmoothing plays an important role in improving accuracy. In this data we can see that our models predicted values very poorly,that is because of the 10 different rating classes each being checked for the exact value. As part of smoothing we can consider rating one below and above is same as the actual rating this will improve our accuracy dramatically."},{"metadata":{"trusted":false},"cell_type":"code","source":"def accuracy_metric(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i] or actual[i] == predicted[i]+1 or actual[i] == predicted[i]-1:\n            correct += 1\n    return correct / float(len(actual)) * 100.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. TF- IDF:\nTF-IDF is a short for Term Frequency- Inverse Document Frequency which denotes how a word is important to the document which means that the word has more occurance in the vocabulary and inversely propotional to the occurance in the document gives the importance of the given word."},{"metadata":{"trusted":false},"cell_type":"code","source":"tfidf = TfidfVectorizer()\ntext_count_2 = tfidf.fit_transform(data['comment'])\n\nxtrain, xtest, ytrain, ytest = train_test_split(text_count_2, Y, test_size=0.2, random_state=1)\n\nxtrain, xdev, ytrain, ydev = train_test_split(xtrain, ytrain, test_size=0.25, random_state=1) \n\n\nMNB.fit(xtrain, ytrain)\naccuracy_score_mnb_tf = metrics.accuracy_score(MNB.predict(xdev), ydev)\nprint('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb_tf*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth naive Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(predicted)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mnbtf=accuracy_metric(list(ydev), np.round(predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BernoulliNB"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nBNB = BernoulliNB()\nBNB.fit(xtrain, ytrain)\naccuracy_score_bnb_tf = metrics.accuracy_score(BNB.predict(xdev),ydev)\nprint('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth BernNb Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(BNB.predict(xdev))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bnbtf=accuracy_metric(list(ydev), np.round(BNB.predict(xdev)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('...Decision_tree...')\nclf = DecisionTreeClassifier(criterion=\"gini\", max_depth=2, min_samples_split=20, min_samples_leaf=5).fit(xtrain, ytrain)\npredict_y = clf.predict(xdev)\ndec_tf=metrics.accuracy_score(ydev, predict_y)*100\nprint('Decision tree accuracy: ', metrics.accuracy_score(ydev, predict_y)*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth Decision Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(predict_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":" dttf=accuracy_metric(list(ydev), np.round(predict_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  SVM"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf = svm.LinearSVC(C=1.0, penalty='l2').fit(xtrain, ytrain)\npredict_y = clf.predict(xdev)\nsvm_tf=metrics.accuracy_score(ydev, predict_y)\nprint('SVM accuracy: ', metrics.accuracy_score(ydev, predict_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth Decision Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(predict_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":" svmtf=accuracy_metric(list(ydev), np.round(predict_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Ridge Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import RidgeClassifier\nrc = RidgeClassifier()\nrc.fit(xtrain, ytrain)\nscore_tf = rc.score(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"srv_tf=rc.predict(xdev)\nprint('Ridge Classifier Accuracy: ', metrics.accuracy_score(ydev, srv_tf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(' Smooth Ridge Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(srv_tf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rctf=accuracy_metric(list(ydev), np.round(srv_tf))\nsvmtf=65.38","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Accuracy Graph"},{"metadata":{"trusted":false},"cell_type":"code","source":"acc=[mnbtf,bnbtf,dttf,svmtf,rctf]\nval=['MultinomialNB','BernouliNB','Decision Tree','SVM','Ridge']\nplt.bar(val,acc, color='b',width=0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Hashing Vectorization\n\nCounts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n\nThis, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n\nA clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n\nThe HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed."},{"metadata":{"trusted":false},"cell_type":"code","source":"text = [\"The quick brown fox jumped over the lazy dog.\"]\n\nvectorizer = HashingVectorizer(n_features=20)\ntext_count_3 = vectorizer.fit_transform(data['comment'])\nxtrain, xtest, ytrain, ytest = train_test_split(text_count_3, Y, test_size=0.2, random_state=1)\n\nxtrain, xdev, ytrain, ydev = train_test_split(xtrain, ytrain, test_size=0.25, random_state=1) \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Multinomial NB"},{"metadata":{"trusted":false},"cell_type":"code","source":"MNB.fit(xtrain, ytrain)\naccuracy_score_mnb_hv = metrics.accuracy_score(MNB.predict(xtest), ytest)\nprint('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb_hv*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth Multinomial NB accuracy:', accuracy_metric(list(ydev), np.round(predicted)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Bernoulli NB"},{"metadata":{"trusted":false},"cell_type":"code","source":"BNB = BernoulliNB()\nBNB.fit(xtrain, ytrain)\naccuracy_score_bnb_hv = metrics.accuracy_score(BNB.predict(xdev),ydev)\nprint('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth BNB accuracy: ', accuracy_metric(list(ydev), np.round(BNB.predict(xdev))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bnbhs=accuracy_metric(list(ydev), np.round(BNB.predict(xdev)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=2, min_samples_split=20, min_samples_leaf=5).fit(xtrain, ytrain)\npredict_y = clf.predict(xdev)\ndec_hv=metrics.accuracy_score(ydev, predict_y)*100\nprint('Decision tree accuracy: ', metrics.accuracy_score(ydev, predict_y)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth Decision Decision Tree: ', accuracy_metric(list(ydev), np.round(predict_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dths=accuracy_metric(list(ydev), np.round(predict_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  SVM"},{"metadata":{"trusted":false},"cell_type":"code","source":"clf_hv = svm.LinearSVC(C=1.0, penalty='l2').fit(xtrain, ytrain)\npredict_y = clf.predict(xdev)\nsvm_hv=metrics.accuracy_score(ydev, predict_y)*100\nprint('SVM accuracy: ', metrics.accuracy_score(ydev, predict_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Smooth SVM accuracy: ',  accuracy_metric(list(ydev), np.round(predict_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svmhs=accuracy_metric(list(ydev), np.round(predict_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Ridge Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"rc = RidgeClassifier()\nrc.fit(xtrain, ytrain)\nscore_hv = rc.score(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"srv_hs=rc.predict(xdev)\nprint('Ridge Classifier Accuracy: ', metrics.accuracy_score(ydev, srv_hs)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Ridge Classifier Accuracy: ', accuracy_metric(list(ydev), np.round(srv_hs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rchs=accuracy_metric(list(ydev), np.round(srv_hs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"acc=[mnbhs,bnbhs,dths,svmhs,rchs]\nval=['MultinomialNB','BernouliNB','Decision Tree','SVM','Ridge']\nplt.bar(val,acc, color='r',width=0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Model\nAs we can see that the model with TF-IDF vectorization performs best among other models we will select this model as our final model."},{"metadata":{},"cell_type":"markdown","source":"### K-Fold Cross Validation\nTo get the accurate result we will run the 10-Fold cross validation on our selected model and run it on the test dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"cv_scores = cross_val_score(rc, xtest, ytest, cv=5)\nprint(\"CV average score: %.2f\" % (cv_scores.mean()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving the model\nNow we will save the model to deploy it on our cloud hosted website"},{"metadata":{"trusted":false},"cell_type":"code","source":"filename = 'tfrc.sav'\npickle.dump(rc, open(filename, 'wb'))\npickle.dump(cv, open('cv', 'wb'))\nprint(\"Model Saved\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Challanges:\nThe main challenge was that the data was large in size and needed cleaning and an efficient way to vectorizee the data so that it can process faster. \nin order to overcome that challege I implemented and tested various vectorization methods with different algorithms to get the best output along with that i had to convert data according to those methods as the data type was different for various methods."},{"metadata":{},"cell_type":"markdown","source":"### Difference from the references:\nMy main reference to this project was https://towardsdatascience.com/sentiment-analysis-introduction-to-naive-bayes-algorithm-96831d77ac91 which includes the brief implementation of naive bayes along with some other classifiers starting with that I have made several changes according to our data  first being different document vectorizing methods  with more algortihms such as LinearSVM and Ridge classifier along with that I trained models of each algortihm of different document vectorizing methods to get the best algorithm with good hyperparamters. to make sure of the accuracy I also ran the 5-Fold Cross validation of the test dataset to get the best results"},{"metadata":{},"cell_type":"markdown","source":"### References:\nhttps://towardsdatascience.com/sentiment-analysis-introduction-to-naive-bayes-algorithm-96831d77ac91\nhttps://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\nhttps://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes\nhttps://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\nhttps://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.linear_model.RidgeClassifier.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\nhttps://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\nhttps://towardsdatascience.com/decision-tree-in-machine-learning-e380942a4c96"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}