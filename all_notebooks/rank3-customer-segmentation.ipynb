{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Its an Analytics vidhya Competition \n### To get an idea about competition and use data set for your own Notebooks  \n* Dataset Link- https://www.kaggle.com/vetrirah/customer\n* Competition Link - https://datahack.analyticsvidhya.com/contest/janatahack-customer-segmentation/#ProblemStatement"},{"metadata":{},"cell_type":"markdown","source":"I personally Achieced Rank 8 on Public leaderboard and 25 on Private leaderboard,But some of my other solutions can give rank 3 and 6 on private leader board which i missed to select,One more important think about this competition there is small amount of data so making a right validation set is also important( rank 2 on public leader board recieved rank 52 on private leader board in this competition)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to see all the comands result in a single kernal \nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# to increase no. of rows and column visibility in outputs\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model_01\n# just found 1 date leak and get result as 92.47 with rank of 7 on public leader board\n# i have seen that around 89% of date IDs are repeated to merge test and train data sort them with ids and \n# given next outcome value to current one and fill missing value with D\n# Model_02\n# make an model and \n# if the same id is not present in test data set then fill its value by predicted value ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import data\ntrain = pd.read_csv(r'../input/customer/Train.csv')\ntest = pd.read_csv(r'../input/customer/Test.csv')\nsample_submmission = pd.read_csv(r'../input/customer/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Having a look at data\ntrain.head()\ntest.head()\nsample_submmission.head()\ntrain.shape ,test.shape, sample_submmission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to check type of variables and missing columns in dataset\ntrain.info()\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Apart from target column there are 6 other columns which are object type and there are also missing values in data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking missing values \n(100*train.isna().sum().sort_values(ascending=False)/len(train)).round(2)\n(100*test.isna().sum().sort_values(ascending=False)/len(test)).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6 columns has missing values and Work_Experience has around 10% values mising"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Distribution of Target variable\nsns.countplot(train['Segmentation'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's a balanced distribution "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ID'].nunique()\ntest['ID'].nunique()\ndf['ID'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging train and test data\ntrain['is_train']=1\ntest['is_train']=0\ndf=pd.concat([train,test],axis=0)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#In testset and train set all ids are unique as their no. of rows equal to no. of unique values of Id \n#but some of Train set Id's get repeated in Test as no. of unique id in df not equal to sum of test train unique ids\n#exact no.approx 89 %(2332 observations)\n#assuming outcome remain same if ID. is same\n#These below article may help in understanding this \n#Approach- https://www.kaggle.com/questions-and-answers/171939 \n#Approach in Detail in this competition- https://www.kaggle.com/questions-and-answers/171953"},{"metadata":{},"cell_type":"markdown","source":"# Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sort_values(by=['ID'],inplace=True)\ntest.sort_values(by=['ID'],inplace=True)\ndf.sort_values(by=['ID','is_train'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df['Segmentation_output'] = df['Segmentation'].shift(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoding = df.groupby('ID').size()\ndf['ID_Freq_encoded']= df.ID.map(Encoding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out=df[df['is_train']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out['Segmentation_output']=out['Segmentation_output'].fillna('D')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=['ID','ID_Freq_encoded','Segmentation_output']\nout_1=out[col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2nd Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Ever_Married']=df['Ever_Married'].replace({'Yes':1,'No':0})\ndf['Gender']=df['Gender'].replace({'Male':1,'Female':0})\ndf['Graduated']=df['Graduated'].replace({'Yes':1,'No':0})\ndf['Spending_Score']=df['Spending_Score'].replace({'Low':0,'Average':1,'High':2})\ndf['Var_1']=df['Var_1'].str[-1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visulization"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntrain.nunique()\ntrain['Segmentation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T\ntest.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values\ntrain.isna().sum()/len(train)\ntest.isna().sum()/len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Ever_Married']=train['Ever_Married'].replace({'Yes':1,'No':0})\ntrain['Gender']=train['Gender'].replace({'Male':1,'Female':0})\ntrain['Graduated']=train['Graduated'].replace({'Yes':1,'No':0})\ntrain['Spending_Score']=train['Spending_Score'].replace({'Low':0,'Average':1,'High':2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoding = df.groupby('Profession').size()\ndf['Profession_Freq_encoded']= df.Profession.map(Encoding)\nEncoding = df.groupby('Var_1').size()\ndf['Var_1_Freq_encoded']= df.Var_1.map(Encoding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_var=pd.get_dummies(df['Var_1'])\ndf_Profession=pd.get_dummies(df['Profession'])\ndf_Segmentation=pd.get_dummies(df['Segmentation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_var.shape, df_Profession.shape , df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=pd.concat([df,df_var,df_Profession,df_Segmentation],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1.head()\ndf_1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=df_1.drop(['Profession','Var_1'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1=df_1[df_1['is_train']==1]\ntest_1=df_1[df_1['is_train']==0]\ntest_id=test_1['ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1=train_1.drop('ID',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1=train_1.drop(['is_train','Segmentation_output','ID_Freq_encoded','A','B','C','D'],axis=1)\ntest_1=test_1.drop(['Segmentation','is_train','Segmentation_output','ID_Freq_encoded','A','B','C','D'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=train_1['Segmentation']\ntrain_1=train_1.drop('Segmentation',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_t, X_tt, y_t, y_tt = train_test_split(train_1, Y, test_size=.25, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nlgbcl = LGBMClassifier(n_estimators=60)\nlgbcl= lgbcl.fit(X_t, y_t,eval_metric='multi_error',eval_set=(X_tt , y_tt),verbose=10)\ny_predict = lgbcl.predict(X_tt)\nprint(lgbcl.score(X_t , y_t))\nprint(lgbcl.score(X_tt , y_tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(lgbcl.feature_importances_, index=X_t.columns)\n#feat_importances.nlargest(30).plot(kind='barh')\nfeat_importances.nsmallest(100).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ID=test_1['ID']\ntest_1=test_1.drop('ID',axis=1)\nlgbcl= lgbcl.fit(train_1, Y)\ny_predict = lgbcl.predict(test_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_2 = pd.DataFrame({ 'ID':test_ID,'Segmentation_1':y_predict}) \n#submission.to_csv(\"1st_try.csv\", index = False)\nout=pd.merge(out_1,out_2,how='inner', on='ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out['Segmentation']=out['Segmentation_output'][out['ID_Freq_encoded']==2]\nout['Segmentation']=out['Segmentation'].fillna(out['Segmentation_1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=['ID','Segmentation']\noutput=out[col]\noutput.to_csv(\"sol.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If you find thsi notebook helpful don't forget to upvote. It will Enourage me to make some more Notebooks like this."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}