{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Objective \n\n## Predictive HR Analytics\nThe objective of the notebook is to apply regression techniques to predict the monthly salary of the employee.\nAlgorithms of interest - Boosting Regressor, RandomForest.\nMAPE, MAE is preferred as an evaluation metric over MSE/RMSE.\n\nModel accuracy can be improved further by tuning the model hyperparams and feature engineering.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### Import Libraries and Inspect the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Libraries\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sb\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor #machine learning model 1\nfrom sklearn.ensemble import RandomForestRegressor #machine learning model 2\nfrom sklearn.metrics import mean_squared_error #regression evaluation\nfrom matplotlib import rcParams #Plotting params.\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the data.\ndata = pd.read_csv('/kaggle/input/hr-analytics-case-study/general_data.csv')\nemp_survey = pd.read_csv('/kaggle/input/hr-analytics-case-study/employee_survey_data.csv')\nman_survey = pd.read_csv('/kaggle/input/hr-analytics-case-study/manager_survey_data.csv')\n#in_time = pd.read_csv('/Users/kashs/Datasets/hr-analytics-case-study/in_time.csv')\n#out_time = pd.read_csv('/Users/kashs/Datasets/hr-analytics-case-study/out_time.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspect the dataframe.\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check the columns in the dataset.\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets combine the dataset from the employee survey, management survey and employee data.\n#Merge 1\n#Merging Employee Survey Data to main df.\ncombined = data.merge(emp_survey, on= 'EmployeeID')\n#Merge 2\n#Merging Management Survey Data to main df.\ndf = combined.merge(man_survey, on= 'EmployeeID')\ndel combined","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration\n\nExploring only some features that intuiteively make most sense. A lot more exploration and feature engineering will be done in future commits.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking Null Values\ndf.isnull().sum()\n\n#There are some null values in NumCompaniesWorked, TotalWorkingYears, EnvironmentSatisfaction, JobSatisfaction and\n#WorkLifeBalance.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop Na's. (Since they are <5% of total observations).\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking number of rows and columns.\ndf.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Data Types and Count.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Average salary across all departments.\nprint (\"Average Salary across all Departments: $\",df['MonthlyIncome'].mean())\n#Average salary across Education levels. (1-Below College, 5-Doctor)\nprint (\"Average Salary for Education Level (Below College):$ \",df[df['Education'] == 1].MonthlyIncome.mean() )\nprint (\"Average Salary for Education Level (Doctor):$ \",df[df['Education'] == 5].MonthlyIncome.mean())\n#There isnt much difference between average salaries of Doctor education level and college!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking monthly income by education level and distance from home.\nrcParams['figure.figsize']=12,10\nax1=df.plot.scatter(x='DistanceFromHome',\n                      y='MonthlyIncome', c='Education', colormap = 'viridis')\nplt.xlabel('Distance From Home')\nplt.ylabel('MonthlyIncome', fontsize = 12)\nplt.suptitle ('Income by Distance from Home and Education Level')\n\n#Education Levels (As given in data dictionary).\n# 1 'Below College'\n# 2 'College'\n# 3 'Bachelor'\n# 4 'Master'\n# 5 'Doctor'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking Outliers (Although the outliers have been removed here, the exploration will be done using the original 'df').\ndf1 = df.copy(deep = True) \n# In this technique, floor (e.g., the 10th percentile) the lower values and cap (e.g., the 90th percentile) the higher values. \n# Print the 10th and 90th percentiles which will be used for quantile-based flooring/capping.\nprint(\"10th Percentile: \",df1['MonthlyIncome'].quantile(0.10))\nprint(\"90th Percentile: \",df1['MonthlyIncome'].quantile(0.95))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"MonthlyIncome\"] = np.where(df1[\"MonthlyIncome\"] <23176, 23176,df1['MonthlyIncome']) #floor values lower than 10th pcntile.\ndf1[\"MonthlyIncome\"] = np.where(df1[\"MonthlyIncome\"] >137755, 137755,df1['MonthlyIncome']) #floor values higher than 90th pcntile.\n#Check the skew.\nprint(\"The skewness of capped data is: \", df1['MonthlyIncome'].skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Monthly Income by Department Type. \nplt.figure(figsize=(7,4))\nsb.boxplot(x='Department', y='MonthlyIncome', data=df, palette='hls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Monthly Income by EducationField.\nplt.figure(figsize=(12,5))\nsb.boxplot(x='EducationField', y='MonthlyIncome', data=df, palette='hls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Monthly Income By Job Role.\nplt.figure(figsize=(19,4))\nsb.boxplot(x='JobRole', y='MonthlyIncome', data=df, palette='hls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Monthly Income by Sex.\nplt.figure(figsize=(5,4))\nsb.boxplot(x='Gender', y='MonthlyIncome', data=df, palette='hls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Monthly Income by Attrition.\nplt.figure(figsize=(5,4))\nsb.boxplot(x='Attrition', y='MonthlyIncome', data=df, palette='hls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mapping numerical values to attrition.\ndf1['Attrition'] = df1['Attrition'].map({'No':0, 'Yes':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get a count of objects (categorical columns).\nobject_col = []\nfor column in df.columns:\n    if df[column].dtype == object and len(df[column].unique()) <= 30:\n        object_col.append(column)\n        print(f\"{column} : {df[column].unique()}\")\n        print(df[column].value_counts())\n        print(\"==================================\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing\n\nDropping some features, checking correlation to avoid overfitting and checknig the distribution of the target feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping Employee Count and StandardHours features (sd=0/have just one value in column).\ndf1.drop(['EmployeeCount','StandardHours','EmployeeID','Over18'],axis=1 , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check correlation.\nplt.figure(figsize=(15,10))\nsb.heatmap(df1.corr(), annot=True, cmap=\"YlGnBu\", annot_kws={\"size\":8})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Highly correlated features cause overfitting.\n#Will drop the features - TotalWorkingYear/Age, TotalWorkingYears/YearsAtCompany\ndf1.drop(['TotalWorkingYears','YearsAtCompany'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Correlation Again.\ndf1.corr().sort_values(by = 'MonthlyIncome' ,ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummifying DF.\ndf1 = pd.get_dummies(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking distribution of target feature (Monthly Income).\nplt.figure(figsize=(10,5))\nsb.distplot(df1['MonthlyIncome']) #The data is right skewed.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Modeling\n\nSince this is a regression problem and the dataset is small, I have chosen to go with some basic yet powerful algorithms to get somewhat ootb accurate results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove Target Feature: Monthly Income.\nX = df1.drop('MonthlyIncome',axis=1)\ny = df1['MonthlyIncome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Snappy Functions to check cross validation and print the result using best hyperparams. (Grid Search CV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to get cross validation scores\nfrom sklearn.model_selection import cross_val_score\ndef get_cv_scores(model):\n    scores = cross_val_score(model,\n                             X_train,\n                             y_train,\n                             cv=5,\n                             scoring='r2')\n    \n    print('CV Mean: ', np.mean(scores))\n    print('STD: ', np.std(scores))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to print best hyperparams.\ndef print_results(results):\n    print('BEST hyperparams: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 1 - Boosting Regression\n\nBoosting aggregates a lot of smaller, weak models to generate one strong model.The prediction time is fast (although training take a while) and it should not be used with noisy data - since it learns from mistakes, it tends to overfit while trying to fix to the noise and outliers in the data.\n\nThe data is floored already to ovoid any overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the dataset into 80-20.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\n#Applying Scaler.\nscaler = preprocessing.Normalizer()\nX_train = scaler.fit_transform(X_train) #notice how the target feature (y) is untouched.\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Boosting Params.\n - 'max_depth': how deep the trees are (usually lesser than random forest trees since they're shallower and weaker)\n - 'n_estimators': number of trees.\n  - 'learning_rate': How quickly and whether or not the algo will find the optimum solution. \n  \n      set it too high - never find the optimal solution.\n      \n      set it too low - still may not find it but if you do, it may take too long because each iteration will take a step too small.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Lets instantiate the model and use cross validation to find the best hyperparams.\n# from sklearn.model_selection import GridSearchCV\n# clf = GradientBoostingRegressor()\n# parameters = {'n_estimators': [500,750,1000], 'max_depth': [4,8,None], 'learning_rate':[.1, .01]} \n# #smaller value of c is stronger regularization. (it is the inverse of regularization strength)\n# cv = GridSearchCV(clf, parameters, cv=5)\n# cv.fit(X_train, y_train.ravel()) #fit it on the train data to find best hyperparams.\n# print_results(cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The best identified hyperparams are - 0.811 (+/-0.079) for {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 750}\n\n# Fit regression model using best hyperparams.\nparams = {'n_estimators': 750, 'max_depth': 8, 'min_samples_split': 20,\n          'learning_rate': 0.1, 'loss': 'ls'}\nclf = GradientBoostingRegressor(**params)\nclf.fit(X_train, y_train)\ny_pred_clf = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Evaluation\nfrom sklearn import metrics\nprint ('MAE:',metrics.mean_absolute_error(y_test,y_pred_clf))\nprint ('MSE:',metrics.mean_squared_error(y_test,y_pred_clf))\nprint ('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred_clf)))\nprint ('Mean Abs % Error:', mean_absolute_percentage_error(y_test, y_pred_clf)) #Under 4% error.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training deviance. Check Bias-Variance Trade-off.\n\n# Compute test set deviance\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i] = clf.loss_(y_test, y_pred)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the most important features. (This is the magic of boosting algorithm!).\nplt.figure(figsize=(15,10))\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2 - Random Forest Regressor\n\nRandom forest takes n samples with replacement, sample the features (columns) as well as the data (rows) completely independent from each other. Each tree generates a prediction, voting is done and then final prediction is done using the votes.\n\nWhen to Use it\n - Gives significance of predictors.\n - Quick for benchmarking - fast, flexible.\n - Great for messy data, with missing values, outliers etc.\n\nWhen to NOT use it\n- Not the best for extracting most value from data.\n- Not very transparent - when there are 100's of trees, hard to see details with what's happening.\n- Quick to train, not quick to make predictions.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the dataset.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=10)\n\nfrom sklearn import preprocessing\n#Applying Scaler.\nscaler = preprocessing.Normalizer()\nX_train = scaler.fit_transform(X_train) #notice how the target feature (y) is untouched.\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Lets instantiate the model and use cross validation to find the best hyperparams.\n# from sklearn.model_selection import GridSearchCV\n# rf = RandomForestRegressor()\n# parameters = {'n_estimators': [100, 250,500]} \n# cv = GridSearchCV(rf, parameters, cv=5)\n# cv.fit(X_train, y_train.ravel()) #fit it on the train data to find best hyperparams.\n# print_results(cv)\n\n#The reason the above code is commented to reduce the run time, since cross validation takes hold out sets, it take a while to run.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reapplying with best hyperparams.\nfrom sklearn.ensemble import RandomForestRegressor #Regressor\nrf = RandomForestRegressor(n_estimators = 500,\n                              criterion = 'mse',\n                              random_state = 1,\n                              n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit model\nrf.fit(X_train,y_train.values.ravel())\ny_pred_rf = rf.predict(X_train)\ny_pred_rf = rf.predict(X_test)\n\n#Another way to issue print.\nprint('Forest train score %.3f, Forest test score: %.3f' % (rf.score(X_train,y_train), rf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Evaluation - \nfrom sklearn import metrics\nprint ('MAE:',metrics.mean_absolute_error(y_test,y_pred_rf))\nprint ('MSE:',metrics.mean_squared_error(y_test,y_pred_rf))\nprint ('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred_rf)))\nprint ('Mean Abs % Error:', mean_absolute_percentage_error(y_test, y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample Prediction\nAs a sample, 5 values are taken from the dataset and checked against their predictions given by the ML model. Using Boosting Regressor as baseline since that's given the most accurate results for now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking a subet of the data to show actuals vs predictions.\nsample_data = df1.drop('MonthlyIncome',axis=1)[30:35]\nsample_salary = df1['MonthlyIncome'][30:35]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets predict and check the monthly income value for the observations.\n#Scale the data.\nscaler = preprocessing.Normalizer()\nsample_data = scaler.fit_transform(sample_data)\n\n#Predict on the new sample.\nsample_prediction = clf.predict(sample_data)\nprint ('\\n''--------------')\nprint (\"Predicted Monthly Income\" '\\n', sample_prediction.reshape(-1,1))\nprint ('\\n''--------------')\nprint (\"Actual Monthly Income\" '\\n' ,sample_salary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Thats all for now. Next steps feature engineering and hyperparams tuning. Always open for feedback.\n#Happy Learning!","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}