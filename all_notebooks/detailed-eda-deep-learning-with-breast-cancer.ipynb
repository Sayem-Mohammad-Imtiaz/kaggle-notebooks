{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nHello people, welcome to my kernel! In this kernel I am going to apply deep learning to breast cancer dataset. I am going to tell everything step by step. Before the start. Let's take a look at our schedule\n\n# Schedule\n1. Importing Libraries and Dataset\n1. Dataset Overview\n1. Simple Data Analyses\n    * Diagnosis Countplot\n    * Radius Mean Histogram\n    * Texture Mean Histogram\n    * Smoothness Mean Histogram\n1. Outlier Detection\n1. Detailed Data Analyses\n    * Correlation Heatmap\n    * Correlation Between Features\n        * Radius Mean - Texture Mean Scatter Plot\n        * Radius Mean - Smoothnes Mean Scatter Plot\n1. Preprocessing\n    * Dropping Unrelevant Features\n    * Converting Label Feature Into Int64\n    * Scaling (Normalizing)\n    * Train Test Split\n1. Modeling\n    * Creating Model Function\n    * Cross Validation\n    * Fitting Model\n    * Prediction and Result\n1. Conclusion\n    "},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries and Dataset\nIn this section I am going to import libraries and dataset that I will use. However I am not going to import deep learning libraries and scikit-learn in this section, I am going to import them when I use them. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\"\"\"\nData Manipulating\n\"\"\"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\"\"\"\nVisualization\n\"\"\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now I am going to import dataset. Our dataset's format is csv, so I will use pandas' read_csv method for importing."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Overview\n\nIn this section I am going to examine the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are two unrelevant features in the dataset: id, Unnamed: 32. We have to drop them\n* There are 31 features in the dataset except the unrelevants.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Good news! Our dataset does not have any nan values, so we will not fill any nan values."},{"metadata":{},"cell_type":"markdown","source":"# Simple Data Analyses\n\nIn this section I am going to do some Simple EDA. But I can not examine all the features because there are 31 features. So I am going to examine some features."},{"metadata":{},"cell_type":"markdown","source":"## Diagnosis Countplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(8,6))\nsns.countplot(data.diagnosis)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see there are two labels in our dataset. M and B\n* Most of the dataset are B labeled.\n"},{"metadata":{},"cell_type":"markdown","source":"## Radius Mean Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(8,6))\nsns.distplot(data[\"radius_mean\"],color=\"#FC2D2D\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see most of the dataset's radius_mean is between 10 and 20\n"},{"metadata":{},"cell_type":"markdown","source":"## Texture Mean Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(8,6))\nsns.distplot(data[\"texture_mean\"],color=\"#F739F7\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Texture mean plot is so similar with radius mean"},{"metadata":{},"cell_type":"markdown","source":"## Smoothness Mean Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(8,6))\nsns.distplot(data[\"smoothness_mean\"],color=\"#F5BA5D\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see our smoothness_mean values are between 1 and 0.\n* So it means that if we do not normalize our dataset, there will be problems.\n* Most of the values is between 0.06 and 0.14"},{"metadata":{},"cell_type":"markdown","source":"I think it is enough for having a bit more idea about the dataset. Let's move on to the next section."},{"metadata":{},"cell_type":"markdown","source":"# Outlier Detection\n\nIn this section I am going to drop outlier values. One day I've heard something about outlier values from a data scientist, he said outliers are silent killers, so you have to drop them from dataset. Yes, he is definitely right. \n\nBut I am not going to drop the rows that only have one outlier value. I am going to drop the rows that have outlier values more than five. I have to do this because I do not want to drop so many rows.\n\nLet's go!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_index_detector(df,features):\n    indexes = []\n    result = []\n    for ftr in features:\n        \n        Q1 = df.describe()[ftr][\"25%\"] # Lower quartile\n        Q3 = df.describe()[ftr][\"75%\"] # Upper quartile\n        IQR = Q3 - Q1 # IQR\n        STEP = IQR*1.5 # Outlier Step\n        \n        ind = data[(data[ftr]<Q1-STEP) | (data[ftr]>Q3+STEP)].index.values\n        for i in ind:\n            indexes.append(i)\n    \n    for index in indexes:\n        \n        indexes.remove(index) \n        if index in indexes: # More than 2\n            indexes.remove(index)\n            \n            if index in indexes: # More than 3\n                indexes.remove(index)\n                \n                if index in indexes: # More than 4\n                    indexes.remove(index)\n                    \n                    if index in indexes: # Append Final Result\n                        result.append(index)\n            \n    \n    return result\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What Did I Do In This Function\n1. I've started a for loop that each iteration is a different feature.\n1. I've compute outlier step using outlier formula\n1. I've find the indexes for each feature\n    * For Instance:\n        * Feature 1 Outlier Index : 32,12,42\n        * Feature 2 Outlier Index : 32,16,89\n    \n    So we can say there are two outliers in the first row\n1. I've append indexes into my list.\n1. I've created a filter.\n1. I've filtered rows that have outliers more than four.\n1. I've append them into my result list."},{"metadata":{},"cell_type":"markdown","source":"* And now I am going to drop them for improving my future model."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = (list(data))\nfeature_names.remove(\"id\")\nfeature_names.remove(\"diagnosis\")\nfeature_names.remove(\"Unnamed: 32\")\nprint(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = outlier_index_detector(data,feature_names)\nprint(outliers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there are same values in list. So I am going to drop them.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = list(np.unique(outliers))\nprint(\"There are {} outlier rows \\n\".format(len(outliers)))\nprint(outliers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we are ready to drop them\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Len of the dataset before dropping outliers\",len(data))\ndata.drop(outliers,inplace=True)\nprint(\"Len of the dataset after dropping outliers\",len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We completed this section. Let's move on!"},{"metadata":{},"cell_type":"markdown","source":"# Detailed Data Analyses\n\nIn this section I am going to examine correlations between features. But I can not examine all correlations between features because you know there are 31 features in the dataset."},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heatmap\n\nFirst I am going to use correlation heatmap for diagnosing all the relations between the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(data.corr(),annot=True,linewidths=1.5,fmt=\"0.1f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* What a confusing heatmap. \n* There are too many strong positive correlations between the features\n* Unlike the positive correlations, there is not any strong negative correlation between the features."},{"metadata":{},"cell_type":"markdown","source":"## Correlation Between Features\n\nIn this sub-section I am going to use features that I've used in simple data analyses. Let's begin.\n"},{"metadata":{},"cell_type":"markdown","source":"### Radius Mean - Texture Mean Scatter Plot\nWe've said that these two feature's histograms are similar. But It does not means they have strong correlation. Let's take a look at our scatter plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(x=\"radius_mean\",y=\"texture_mean\",data=data,color=\"#670F91\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see there is a little correlation between them.\n"},{"metadata":{},"cell_type":"markdown","source":"### Radius Mean - Smoothness Mean Scatter Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(x=\"radius_mean\",y=\"smoothness_mean\",data=data,color=\"#BD6F4B\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no correlation between these features. "},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nIn this section I am going to prepare the dataset for modeling. I am going to follow these steps:\n* Dropping Unrelevant Features\n* Converting Label Feature Into Int64\n* Scaling (Normalizing)\n* Train Test Split\n\nLet's start with dropping unrelevant features.\n"},{"metadata":{},"cell_type":"markdown","source":"## Dropping Unrelevant Features\nI know, this is easy, however I've said this in the beginning of the kernel. I am going to explain everything as much as I can step by step. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Axis 1 is columns (features) and axis 0 is rows (entries)"},{"metadata":{},"cell_type":"markdown","source":"## Converting Label Feature Into Int64\nIn this section I am going to convert label (diagnosis) into int64. In order to do this I am going to use list comprehension from vanilla python.\n\n     We love python <3"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"First 5 entries\",data.diagnosis[:5])\ndata.diagnosis = [0 if each == \"M\" else 1 for each in data.diagnosis]\nprint(data.diagnosis[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Yea, our first five diagnosis is 0 but do not worry there are 1 values in the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling\nWe are approaching to the most exciting section. In this section I am going to normalize dataset. In order to do this I am going to use this formula\n\n     (value - min(data)) /( max(data) - min(data))"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (data-np.min(data)) / (np.max(data)-np.min(data))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split\n\nIn this section I am going to split the dataset into train and test. In order to do this I am going to use sklearn library's train_test_split function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = data.drop(\"diagnosis\",axis=1) \ny = data.diagnosis\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our arrays are ready. Let's check the lenghts of our arrays.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Len of the x_train\",len(x_train))\nprint(\"Len of the x_test \",len(x_test))\nprint(\"Len of the y_train\",len(y_train))\nprint(\"Len of the y_test \",len(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\nFinally we came! In this section I am going to create our deep learning model. I am going to follow these steps.\n* Creating Model Function\n* Cross Validation\n* Fitting Model\n* Prediction and Result"},{"metadata":{},"cell_type":"markdown","source":"## Creating Model Function\nIn this section I am going to define model function. You know, I am going to use keras library and I have to define my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units=12,kernel_initializer=\"uniform\",activation=\"tanh\",input_dim=30))\n    classifier.add(Dense(units=6,kernel_initializer=\"uniform\",activation=\"tanh\"))\n    classifier.add(Dense(units=1,kernel_initializer=\"uniform\",activation=\"sigmoid\")) # Output Layer\n    classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n    return classifier\n                   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What Did I Do In This Function\n1. I've created sequential object (you can think it is empty model)\n1. I've add my layers\n    * Units = How many nodes in the layer\n    * kernel_initializer= How do algorithm initalize weights and bias.\n    * Activation = activation function like tanh,relu and sigmoid\n1. I've compile my model.\n\n### Why did I use Sigmoid Function In Output Layer?\nBecause in this kernel we will do binary classification (0 and 1) and we use sigmoid activation function for this.\n\nI know there are questions in your mind, I would want to explain everything with more detail, but I am a beginner in deep learning and I am not very good at English, but if you want to learn details, you can check my teacher's kernel:\n\n*https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners*"},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation\n\nIn this section I am going to do cross validation and check the score. I am going to use sklearn library's cross_validation_score. But before this I am going to create my keras classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = KerasClassifier(build_fn=build_classifier,epochs=100)\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator=classifier,X=x_train,y=y_train,cv=3)\n\nprint(\"Mean of CV scores\",accuracies.mean())\nprint(\"Variance of CV scores\",accuracies.std())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our cross validation score mean is %97.6\n* Our cross validation variance is 0.01"},{"metadata":{},"cell_type":"markdown","source":"## Fitting Model\nIn this section I am going to fit my model using my x_train and y_train arrays."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction and Result\nIn this section I am going to predict my test values and compute accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Our train score is\",classifier.score(x_train,y_train))\nprint(\"Our test score is \",classifier.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nThanks for your attention, if there are questions in your mind, you can ask them in comment section I will definitely answer them as much as I can. \n\nIf you see any mistakes or problems in my kernel, please contact with me."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}