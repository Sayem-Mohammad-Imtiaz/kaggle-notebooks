{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data loading\n\n_This part was adapted from a notebook of @Xiao Song_"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd    \nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\ncredit = pd.read_csv('../input/credit-card-approval-prediction/credit_record.csv')  \napplication = pd.read_csv('../input/credit-card-approval-prediction/application_record.csv') \ncredit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"application","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(f\"how many unique ID in application record? {len(set(application['ID']))}\")\nprint(f\"how many unique ID in credit record? {len(set(credit['ID']))}\")\nprint(f\"how many IDs do two tables share? {len(set(application['ID']).intersection(set(credit['ID'])))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Definition of \"Bad\" client\n\nDetailed explanation could be seen [here](https://www.listendata.com/2019/09/credit-risk-vintage-analysis.html). \n\n\n_This part was adapted from a notebook of @Xiao Song_"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = credit.groupby('ID')\n### convert credit data to wide format which every ID is a row\npivot_tb = credit.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\npivot_tb['open_month'] = grouped['MONTHS_BALANCE'].min() # smallest value of MONTHS_BALANCE, is the month when loan was granted\npivot_tb['end_month'] = grouped['MONTHS_BALANCE'].max() # biggest value of MONTHS_BALANCE, might be observe over or canceling account\npivot_tb['ID'] = pivot_tb.index\npivot_tb = pivot_tb[['ID', 'open_month', 'end_month']]\npivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month'] # calculate observe window\npivot_tb.reset_index(drop = True, inplace = True)\ncredit = pd.merge(credit, pivot_tb, on = 'ID', how = 'left') # join calculated information\ncredit0 = credit.copy()\ncredit = credit[credit['window'] > 20] # delete users whose observe window less than 20\ncredit['status'] = np.where((credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 1, 0) # analyze > 60 days past due \ncredit['status'] = credit['status'].astype(np.int8) # 1: overdue 0: not\ncredit['month_on_book'] = credit['MONTHS_BALANCE'] - credit['open_month'] # calculate month on book: how many months after opening account\ncredit.sort_values(by = ['ID','month_on_book'], inplace = True)\n\n##### denominator\ndenominator = pivot_tb.groupby(['open_month']).agg({'ID': ['count']}) # count how many users in every month the account was opened\ndenominator.reset_index(inplace = True)\ndenominator.columns = ['open_month','sta_sum']\n\n##### ventage table\nvintage = credit.groupby(['open_month','month_on_book']).agg({'ID': ['count']}) \nvintage.reset_index(inplace = True)\nvintage.columns = ['open_month','month_on_book','sta_sum'] \nvintage['due_count'] = np.nan\nvintage = vintage[['open_month','month_on_book','due_count']] # delete aggerate column\nvintage = pd.merge(vintage, denominator, on = ['open_month'], how = 'left') # join sta_sum colun to vintage table\nvintage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"larger_window = abs(vintage['open_month'].min())\nfor j in range(-larger_window,1): # outer loop: month in which account was opened\n    ls = []\n    for i in range(0,larger_window+1): # inner loop time after the credit card was granted\n        due = list(credit[(credit['status'] == 1) & (credit['month_on_book'] == i) & (credit['open_month'] == j)]['ID']) # get ID which satisfy the condition\n        ls.extend(due) # As time goes, add bad customers\n        vintage.loc[(vintage['month_on_book'] == i) & (vintage['open_month'] == j), 'due_count'] = len(set(ls)) # calculate non-duplicate ID numbers using set()\n        \nvintage['sta_rate']  = vintage['due_count'] / vintage['sta_sum'] # calculate cumulative % of bad customers\nvintage        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using `pivot` to convert long data to wide data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Vintage wide table\nvintage_wide = vintage.pivot(index = 'open_month',\n                             columns = 'month_on_book',\n                             values = 'sta_rate')\n\n# plot vintage line chart\nvintage0 = vintage_wide.replace(0,np.nan)\nlst = [i for i in range(0,larger_window + 1)]\nvintage_wide[lst].T.plot(legend = False, grid = True, title = 'Cumulative % of Bad Customers (> 60 Days Past Due)')\n#plt.axvline(30)\n#plt.axvline(25)\n#plt.axvline(20)\nplt.xlabel('Months on Books')\nplt.ylabel('Cumulative % > 60 Days Past Due')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observe Window Analysis\n\nBecause of two reasons, account cancellation and observe over, our observe on accounts will be truncated. Observe window is a significant parameter to be considered. If observe window is too short, users' behavior will not fully show off, which will bring unnecessary noise to our data.\n\nIn order to observe how many accounts increase as observe window extend, we plot this. "},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = []\nfor i in range(0,larger_window + 1):\n    ratio = len(pivot_tb[pivot_tb['window'] < i]) / len(set(pivot_tb['ID']))\n    lst.append(ratio)\n    \npd.Series(lst).plot(legend = False, grid = True, title = ' ')\nplt.xlabel('Observe Window')\nplt.ylabel('account ratio')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see that a 60 months observe window covers all appliers, while 20 months window contains about 52% records."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_observe(credit, command):\n    '''calculate observe window\n    '''\n    larger_window = abs(credit['MONTHS_BALANCE'].min())\n    id_sum = len(set(pivot_tb['ID']))\n    credit['status'] = 0\n    exec(command)\n    #credit.loc[(credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\n    credit['month_on_book'] = credit['MONTHS_BALANCE'] - credit['open_month']\n    minagg = credit[credit['status'] == 1].groupby('ID')['month_on_book'].min()\n    minagg = pd.DataFrame(minagg)\n    minagg['ID'] = minagg.index\n    obslst = pd.DataFrame({'month_on_book':range(0,larger_window + 1), 'rate': None})\n    lst = []\n    for i in range(0,larger_window + 1):\n        due = list(minagg[minagg['month_on_book']  == i]['ID'])\n        lst.extend(due)\n        obslst.loc[obslst['month_on_book'] == i, 'rate'] = len(set(lst)) / id_sum \n    return obslst['rate']\n\ncommand = \"credit.loc[(credit['STATUS'] == '0') | (credit['STATUS'] == '1') | (credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"   \nmorethan1 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '1') | (credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"   \nmorethan30 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan60 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan90 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan120 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan150 = calculate_observe(credit, command)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"obslst = pd.DataFrame({'past due more than 30 days': morethan30,\n                       'past due more than 60 days': morethan60,\n                       'past due more than 90 days': morethan90,\n                       'past due more than 120 days': morethan120,\n                       'past due more than 150 days': morethan150\n                        })\nobslst.plot(grid = True, title = 'Cumulative % of Bad Customers Analysis')\nplt.xlabel('Months on Books')\nplt.ylabel('Cumulative %')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot could be seen as a average (across open-month) version of vintage plot.\nFor longer past due date, it needs longer observe window. For example, more than 150 days past due needs at least 5 months until first *bad customer* appears. For most situation, a 20-months observe window could cover most *bad customer*. However, For 30 days past due, we could see that after 30 months on books, there still are new *bad customer* join in the list. So a 20 MOB observe window will be appropriate. Those who exists shorter than the observe window should be excluded from our analysis, thus you could see I deleted users whose observe window less than 20 on last section (the window could be changed)."},{"metadata":{},"cell_type":"markdown","source":"## Overall Past-due Ratio\n\nCalculating overall past-due rate. Respectively, we analyze 1 day past due, 20 days past due, 60 days past due, 90 days past due, 120 days past due, 150 days past due. This analysis could help us to define who are *bad customers*. We could see that almost 87% users have past due more than 1 day, which is too common, thus it's inappropriate to be a standard. What about 150 days overdue? Only 0.4% of accounts appear to past due that long. If we use that, we will left many *bad customers* in our scrutiny. A table like that will help you to determine what  will be the most suitable standard of *bad customers*."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_rate(pivot_tb, command): \n    '''calculate bad customer rate\n    '''\n    credit0['status'] = None\n    exec(command) # excuate input code\n    sumagg = credit0.groupby('ID')['status'].agg(sum)\n    pivot_tb = pd.merge(pivot_tb, sumagg, on = 'ID', how = 'left')\n    pivot_tb.loc[pivot_tb['status'] > 1, 'status'] = 1\n    rate = pivot_tb['status'].sum() / len(pivot_tb)\n    return round(rate, 5)\n\ncommand = \"credit0.loc[(credit0['STATUS'] == '0') | (credit0['STATUS'] == '1') | (credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"   \nmorethan1 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '1') | (credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"   \nmorethan30 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan60 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan90 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan120 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan150 = calculate_rate(pivot_tb, command)\n\nsummary_dt = pd.DataFrame({'situation':['past due more than 1 day',\n                               'past due more than 30 days',\n                               'past due more than 60 days',\n                               'past due more than 90 days',\n                               'past due more than 120 days',\n                               'past due more than 150 days'],\n                      'bad customer ratio':[morethan1,\n                               morethan30,\n                               morethan60,\n                               morethan90, \n                               morethan120,\n                               morethan150, \n                      ]})\nsummary_dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Definition of targets"},{"metadata":{},"cell_type":"markdown","source":"Possibilities : \n1. Past due more than X days\n2. Past more than Y% of dues"},{"metadata":{"trusted":true},"cell_type":"code","source":"#\"Bad\" client are identified as client that past due more than 30 days\ny = credit0[['ID','STATUS','status']]\ny['status'] = 0 #0 is the label for a \"good\" client\nexec(\"y.loc[(y['STATUS'] == '1') | (y['STATUS'] == '2') | (y['STATUS'] == '3' )| (y['STATUS'] == '4' )| (y['STATUS'] == '5'), 'status'] = 1\") #1 is the label for a \"Bad\" client\ny = y[['ID','status']].rename(columns={\"ID\": \"ID\", \"status\": \"target\"})\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"application = pd.read_csv('../input/credit-card-approval-prediction/application_record.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Numerical features\napplication = application.replace(['N','Y'],[0,1]) #Converts Yes/No in 1/0\napplication = application.rename(columns={\"CODE_GENDER\":\"F\", \"NAME_EDUCATION_TYPE\":\"EDUCATION\"})\napplication = application.replace(['F','M'],[1,0]) #Converts Female/Male in 1/0\napplication = application.replace(['Academic degree', 'Higher education', 'Incomplete higher', \n                                   'Secondary / secondary special', 'Lower secondary'],\n                                  [4,3,2,1,0]) #Converts education level into numerical features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert categorical variables\napplication['OCCUPATION_TYPE'] = application['OCCUPATION_TYPE'].apply(lambda x : 'Unknown' if pd.isnull(x) else x)\napplication = pd.get_dummies(application)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merge both datasets\ndf = application\ndf = df.merge(y, on=\"ID\", how=\"inner\").drop(columns=[\"ID\"])\n#df = df[:10000] #Test only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\nfrom sklearn.model_selection import cross_validate\n\nn_tree = 200\n\n#Create a random forest classifier\nRandomForest = RandomForestClassifier(n_estimators=n_tree,\n                              max_depth=15,\n                              min_samples_leaf=5, \n                              max_samples=0.10,\n                              bootstrap=False, #Dataset is large enought (Bootstrap add some variability)\n                              class_weight=\"balanced\" #Give a larger weight to unrepresented class\n                              )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split into train and test set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,balanced_accuracy_score,confusion_matrix\n\ntrain, test = train_test_split(df, shuffle=True, test_size=0.2)\n\nX_train, y_train = train.drop(columns=['target']), train.target\nX_test, y_test = test.drop(columns=['target']), test.target\n\nRandomForest.fit(X_train, y_train)\ny_predict_train = RandomForest.predict(X_train)\ny_predict = RandomForest.predict(X_test)\n\nprint('Training Score is {:.2%}'.format(accuracy_score(y_train, y_predict_train)))\nprint('Balanced training Score is {:.2%}'.format(balanced_accuracy_score(y_train, y_predict_train)))\ndisplay(pd.DataFrame(confusion_matrix(y_train,y_predict_train,normalize=\"all\")).style.format('{:.2%}'))\n\nprint('Accuracy Score is {:.2%}'.format(accuracy_score(y_test, y_predict)))\nprint('Balanced accuracy Score is {:.2%}'.format(balanced_accuracy_score(y_test, y_predict)))\ndisplay(pd.DataFrame(confusion_matrix(y_test,y_predict,normalize=\"all\")).style.format('{:.2%}'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"markdown","source":"_Extra trees are less performant. The following code is commented_\n\n<code>\nExtraTree = ExtraTreesClassifier(n_estimators=200,\n                              max_depth=15,\n                              min_samples_leaf=16, \n                              class_weight=\"balanced\" #Give a larger weight to unrepresented class\n                              )\n</code><code>\n#Split into train and test set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,balanced_accuracy_score,confusion_matrix\n</code><code>\ntrain, test = train_test_split(df, shuffle=True, test_size=0.1)\n</code><code>\nX_train, y_train = train.drop(columns=['target']), train.target\nX_test, y_test = test.drop(columns=['target']), test.target\n</code><code>\nExtraTree.fit(X_train, y_train)\ny_predict_train = ExtraTree.predict(X_train)\ny_predict = ExtraTree.predict(X_test)\n</code><code>\nprint('Training Score is {:.2%}'.format(accuracy_score(y_train, y_predict_train)))\nprint('Balanced training Score is {:.2%}'.format(balanced_accuracy_score(y_train, y_predict_train)))\ndisplay(pd.DataFrame(confusion_matrix(y_train,y_predict_train,normalize=\"all\")).style.format('{:.2%}'))\n</code><code>\nprint('Accuracy Score is {:.2%}'.format(accuracy_score(y_test, y_predict)))\nprint('Balanced accuracy Score is {:.2%}'.format(balanced_accuracy_score(y_test, y_predict)))\ndisplay(pd.DataFrame(confusion_matrix(y_test,y_predict,normalize=\"all\")).style.format('{:.2%}'))\n</code>"},{"metadata":{},"cell_type":"markdown","source":"# Explicability"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parameters\nfeatures = X_train.columns\nnb_features = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### General information"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot one complete decision tree\n\nestimator = RandomForest.estimators_[5] #5 is an arbitrary choice\n\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = X_train.columns,\n                class_names = ['0','1'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return the top 10 features in importance order (%)\ndisp_df = pd.DataFrame(RandomForest.feature_importances_.T, columns=['gini'], index=features)\ndisp_df['gini'] = disp_df.gini.apply(lambda x : round(x*100,2))\n\nprint('Top %s most important features' % nb_features)\ndisp_tab = disp_df.gini.nlargest(nb_features)\nprint(tabulate(np.array([disp_tab.index, disp_tab.values]).T, headers=['Features','%']))\n\nprint('\\n%s least important features' % nb_features)\ndisp_tab = disp_df.gini.nsmallest(nb_features)\nprint(tabulate(np.array([disp_tab.index, disp_tab.values]).T, headers=['Features','%']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Focusing on specific candidates"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select samples\nnb_samples = 5\ntarget = 1 #0 loan accepted, 1 refused\n\ny_predict_df = pd.DataFrame(y_predict, columns=['target'])\nindex = y_predict_df[y_predict_df.target==target].sample(n=nb_samples).index\ndf_accepted = X_test.iloc[index]\nsamples = df_accepted.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print the confidence across samples\nimport matplotlib.pyplot as plt\n\nconfidence = RandomForest.predict_proba(df_accepted).T\nconfigs = confidence[0]\nN = len(configs)\nind = np.arange(N)\n\nwidth = 0.4\n\np1 = plt.bar(ind, confidence[0]*n_tree, width, color='g')\np2 = plt.bar(ind, confidence[1]*n_tree, width, bottom=confidence[0]*n_tree, color='r')\n\nplt.ylim([0,1.2*n_tree])\nplt.ylabel('Number of estimators', fontsize=12)\nplt.xlabel('Samples', fontsize=12)\nplt.legend((p1[0], p2[0]), ('Accepted', 'Refused'), fontsize=12, ncol=2, framealpha=0, fancybox=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n#Code to display the most important features for each samples \ndict_feat = dict()\nfor sample_id in range(len(samples)) : \n    dict_feat[sample_id] = Counter()\n    \n\nfor estimator in RandomForest.estimators_ :\n    \n    # Using those arrays, we can parse the tree structure:\n    node_indicator = estimator.decision_path(samples)\n    feature = estimator.tree_.feature\n    threshold = estimator.tree_.threshold\n    impurity = estimator.tree_.impurity\n\n    # Similarly, we can also have the leaves ids reached by each sample.\n    leave_id = estimator.apply(samples)\n    \n    for sample_id in range(len(samples)) : \n        node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                        node_indicator.indptr[sample_id + 1]]\n\n        for node_id in node_index:\n            if leave_id[sample_id] == node_id:\n                continue\n\n            if (samples[sample_id, feature[node_id]] <= threshold[node_id]):\n                threshold_sign = \" - lower\"\n            else:\n                threshold_sign = \" - larger\"\n\n            key = features[feature[node_id]] + threshold_sign + \":\" + str(samples[sample_id, feature[node_id]])\n\n            #dict_feat[sample_id][key] += 1\n            dict_feat[sample_id][key] += (0.5-impurity[node_id]) #the lesser gini is, the more determinant is the feature\n\n\n#Print rules\nfor sample_id in range(len(samples)) : \n    print('\\nDeterminant rules to predict sample %s: ' % sample_id)\n    disp_tab = []\n    for k, v in dict_feat[sample_id].most_common(nb_features):\n        k_tab = k.split(':')\n        #disp_tab.append([k_tab[0],k_tab[1],'{0:.0%}'.format(v/n_tree)])\n        disp_tab.append([k_tab[0],k_tab[1],'{:.2}'.format(v/n_tree)])\n    print(tabulate(disp_tab, headers=['Reasons','Values', 'Gini metrics'])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}