{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport datetime\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport yfinance as yf\nimport holidays\nimport pickle\n\nmax_features = 20000\nmaxlen = 100\n# loading tokenizer\nwith open('../input/lstmmodel/tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install holidays","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model('../input/lstmmodel/Model1-LSTM.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = pd.read_csv('/kaggle/input/massive-stock-news-analysis-db-for-nlpbacktests/analyst_ratings_processed.csv', index_col=0)\ndata1.dropna(inplace = True)\ndata1.rename(columns={'stock':'ticker'}, inplace=True)\ndata1['date'] = data1['date'].apply(lambda x : x.split()[0])\ndata2 = pd.read_csv('/kaggle/input/us-equities-news-data/us_equities_news_dataset.csv', index_col=0)\ndata2.dropna(inplace = True)\ndata2.reset_index(drop=True, inplace=True)\ndata2.rename(columns={'release_date':'date'}, inplace=True)\ndata2.drop(inplace=True, columns=['category', 'content', 'provider', 'url', 'article_id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data1, data2])\ndata.drop_duplicates(subset='title', keep='first', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tickerSymbol = \"MSFT\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmpData = {}\ntotal = data['date'].nunique()\nfor i in tqdm(data[data['ticker']==tickerSymbol]['date'].unique()):\n    tmpData[i] = data.loc[(data['ticker']==tickerSymbol) & (data['date'] == i)]['title'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"ONE_DAY = datetime.timedelta(days=1)\nHOLIDAYS_US = holidays.US()\ndef next_business_day(dateString):\n    datetimeObj = datetime.datetime.strptime(dateString, '%Y-%m-%d')\n    next_day = datetimeObj + ONE_DAY\n    while next_day.weekday() in holidays.WEEKEND or next_day in HOLIDAYS_US:\n        next_day += ONE_DAY\n    return next_day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def findPercentageBySentences(sentenceList):\n    posAvg, negAvg, neuAvg = 0, 0, 0\n    for sentence in sentenceList:\n        token = tokenizer.texts_to_sequences([sentence])\n        X = pad_sequences(token, maxlen=maxlen)\n        sentiment_dict = model.predict(X).tolist()[0]\n        negAvg += sentiment_dict[0]\n        neuAvg += sentiment_dict[1]\n        posAvg += sentiment_dict[2]\n    posAvg=(posAvg/len(sentenceList))*100\n    negAvg=(negAvg/len(sentenceList))*100\n    neuAvg=(neuAvg/len(sentenceList))*100\n    return {'numArticles': len(sentenceList), 'pos': posAvg, 'neg': negAvg, 'neu' : neuAvg}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dateSentimentGroups = {}\nfor i in tqdm(tmpData):\n    scores = findPercentageBySentences(tmpData[i])\n    dateSentimentGroups[i] = scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nticker = yf.Ticker(tickerSymbol)\nhist = ticker.history(period=\"max\")\nfor i in tqdm(dateSentimentGroups):\n  start = i\n  nextDay = next_business_day(start).strftime(\"%Y-%m-%d\")\n  try:\n    prevDay = hist.loc[start]\n    nextDay = hist.loc[nextDay]\n    percentageChange = ((nextDay['Close']-prevDay['Open'])/prevDay['Open'])*100\n    data.append([i, dateSentimentGroups[i]['numArticles'], dateSentimentGroups[i]['neu'], dateSentimentGroups[i]['pos'], dateSentimentGroups[i]['neg'], percentageChange])\n  except:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns =['date', 'numArticles', 'neutral', 'positive','negative','percentageChange'], data=data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model\n\n\nX = df[['numArticles','neutral','positive','negative']]\ny = df['percentageChange']\n\nregr = linear_model.LinearRegression()\nregr.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = regr.predict([[10, 8.078922e-04, 69.504652,30.494541]])\n\nprint(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df[['numArticles','neutral','positive','negative']]\ny = df['percentageChange']\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n  \npoly = PolynomialFeatures(degree = 3)\nX_poly = poly.fit_transform(X)\n  \npoly.fit(X_poly, y)\nlin2 = LinearRegression()\nlin2.fit(X_poly, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin2.predict(poly.fit_transform([[1,9.756931e-08,11.408260,88.591737]]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\n\nX = df[['numArticles','neutral','positive','negative']]\n # here we have 2 variables for multiple regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example.Alternatively, you may add additional variables within the brackets\nY = df['percentageChange']\n \n# with sklearn\nregr = linear_model.LinearRegression()\nregr.fit(X, Y)\n\nprint('Intercept: \\n', regr.intercept_)\nprint('Coefficients: \\n', regr.coef_)\n\n# prediction with sklearn\n\nprint ('Predicted Percent Change: \\n', regr.predict([[10, 8.078922e-04, 69.504652,30.494541]]))\n\n# with statsmodels\nX = sm.add_constant(X) # adding a constant\n \nmodel = sm.OLS(Y, X).fit()\npredictions = model.predict(X) \n \nprint_model = model.summary()\nprint(print_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}