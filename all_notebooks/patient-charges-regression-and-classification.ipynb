{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Medical Cost Personal Datasets.\n## Objectives.\n1. Preprocess and clean the data.\n2. Perform Statistical Analysis of the data.\n3. Perform Linear Regression to predict charges.\n4. Perform Logistic Analysis to predict if a person is a smoker or not.\n5. Perform SVM and predict if a person is a smoker or not.\n6. Perform Boosting algorithms and predict if a person is a smoker or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hide Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import pandas.\nimport pandas as pd\n# Import numpy.\nimport numpy as np\n# Import matplotlib.\nimport matplotlib.pyplot as plt\n# Import Seaborn.\nimport seaborn as sns\n\n# Read and display the data.\ndata = pd.read_csv('/kaggle/input/insurance/insurance.csv')\n\n# Check for null values.\nfor i in data.columns:\n    print('Null Values in {i} :'.format(i = i) , data[i].isnull().sum())\n\n    \n# # Change smoking to a categorical value.\n# data['smoker'] = data['smoker'].astype('category')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical Analysis."},{"metadata":{},"cell_type":"markdown","source":"## 1. Distribution of Charges."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.distplot(data['charges'])\nplt.title('Distribution of Charges.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Distribution of BMI."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.distplot(data['bmi'] , color='r')\nplt.title('Distribution of BMI.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. No of smokers as per age category."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a categorical variable age_category based on different age ranges.\ndef age_category(age):\n    if age <= 18:\n        return 'Teenager(<19)'\n    elif (age>18) & (age<=24):\n        return 'Youth(<25)'\n    elif (age>24) & (age<60):\n        return 'Adult(<60)'\n    else:\n        return 'Senior Citizen(>60)'\ndata['age_category'] = data['age'].apply(age_category)    \n\n# Converto age category to categorical variable.\ndata['age_category'] = data['age_category'].astype('category')\n\n# Plot the data.\nplt.figure(figsize=(12,12))\nsns.countplot(y = 'smoker' ,data = data , hue = 'age_category' , palette = 'muted' , order=['no', 'yes'])\nplt.title('Number of smokers as per Age Category.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Percentage of people as per bmi categorisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a categorical variable age_category based on different bmi ranges.\ndef bmi_category(bmi):\n    if (bmi>=18.5) & (bmi<=24.9):\n        return 'Normal'\n    elif (bmi>=25) & (bmi<=29.9):\n        return 'Overweight'\n    elif (bmi>=30) & (bmi<=34.9):\n        return 'Class I Obestity'\n    else:\n        return 'Class II Obestity'\ndata['bmi_category'] = data['bmi'].apply(bmi_category)    \n\n# Convert bmi category to categorical variable.\ndata['bmi_category'] = data['bmi_category'].astype('category')\n\n# Plot a pie chart.\nfont = {'family' : 'monospace',\n        'weight' : 'bold',\n        'size'   : 22}\nplt.rc('font', **font)\nlabel = data.bmi_category.value_counts().sort_values().index.values\nlst = data.bmi_category.value_counts().sort_values().values\nplt.figure(figsize=(12,12))\nplt.pie(x = lst , labels=label , explode = [0.1 , 0.1 , 0.1 , 0.1],  autopct='%.1f%%' , colors=['red' , 'green' , 'blue' , 'yellow'])\nplt.title(' Percentage of people by BMI Categorisation.' , fontsize = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Relationship between age and charges."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.set_style('whitegrid')\nsns.lineplot(x = 'age' , y = 'charges' , hue = 'sex' , data = data )\nplt.title('Age vs Charges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Relationship between smoking and bmi of a person."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.boxplot(x = 'smoker' , y = 'bmi' , hue = 'sex' , data = data , palette='pastel')\nplt.title('BMI vs Smoking')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Relationship between smokers and charges paid."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = 'smoker' , y = 'charges' , data = data , kind = 'boxen' , palette = 'deep' , height = 12 )\nplt.title('Correlation between smoking and medical charges.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can conclude that in general smoker pay higher medical charges."},{"metadata":{},"cell_type":"markdown","source":"## 8. Relation ship between charges and bmi."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.violinplot(y = 'charges' , x = 'children' , data = data , palette='pastel' )\nplt.title('BMI vs Smoking')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Relationship between age and charges."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.lmplot(x = 'age' , y = 'charges' , fit_reg = True, data=data , hue='bmi_category' , height=12)\nplt.title('Age vs Charges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Relationship between bmi and charges."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.lmplot(x = 'bmi' , y = 'charges' , fit_reg = True, data=data , hue='age_category' , height=12)\nplt.title('BMI vs Charges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.heatmap(data.corr() , annot=True)\nplt.title('Correlation Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression on data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import libraries.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\n\n# Code the categorical values.\nfrom sklearn.preprocessing import LabelEncoder\n\nx = data[['age', 'sex', 'bmi', 'children', 'smoker', 'region']]\n\n\n\n# Get the categorical features and convert them to a list\ncategorical_features = x.dtypes==object\ncategorical_cols = list(x.columns[categorical_features])\n\n# Label it using label encoder.\nle = LabelEncoder()\nx[categorical_cols] = x[categorical_cols].apply(lambda x:le.fit_transform(x))\n\n\ncategorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode it using one hot encoder.\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(categorical_features=categorical_features , sparse=False)\nx_ohe = ohe.fit_transform(x)\nx_ohe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply linear regression\ny = data['charges']\nr2_scr = []\nlr = LinearRegression()\n\n# Apply kfold to train the data.\nkf = KFold(n_splits = 10 , random_state=42)\nfor tr, ts in kf.split(x_ohe):\n    x_tr = x.loc[tr]\n    y_tr = y.loc[tr]\n    x_ts = x.loc[ts]\n    y_ts = y.loc[ts]\n    lr.fit(x_tr , y_tr)\n    y_pred = lr.predict(x_ts)\n    r2_scr.append(r2_score(y_ts , y_pred))\nnp.mean(r2_scr)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply Polynomial Regression on data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply polynomial regression.\nfrom sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree=2)\nx_poly = pf.fit_transform(x)\npoly_model = LinearRegression()\nr2_scr_pf = []\n\n# Apply kfold to train the data.\nkf = KFold(n_splits = 10 , random_state=42)\nfor tr, ts in kf.split(x_poly):\n    x_tr_pf = x_poly[tr]\n    y_tr_pf = y.loc[tr]\n    x_ts_pf = x_poly[ts]\n    y_ts_pf = y.loc[ts]\n    poly_model.fit(x_tr_pf , y_tr_pf)\n    y_pred_poly = poly_model.predict(x_ts_pf)\n    r2_scr_pf.append(r2_score(y_ts_pf, y_pred_poly))\nnp.mean(r2_scr_pf)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Random Forest to determine feature importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will scale the data using standar scaler and find the feature importances.\ndummy_data = pd.get_dummies(data[['age', 'sex', 'bmi', 'children', 'smoker', 'region']] ,\n                            columns=['sex' , 'smoker' , 'region' ])\nscaler = StandardScaler()    \nscaler.fit(dummy_data)\nscaled_data = scaler.transform(dummy_data)\nscaled_df = pd.DataFrame(scaled_data , columns= dummy_data.columns)\n\n# Import the libraries.\nfrom sklearn.ensemble import RandomForestRegressor\n\n# We use Random Forest Regressor to find the top feature. \nrf = RandomForestRegressor( n_estimators=100 , oob_score=True , random_state=42)\nrf.fit(scaled_df, y )\nimps = rf.feature_importances_[rf.feature_importances_.argsort()[::-1]]\nheadings = dummy_data.columns[rf.feature_importances_.argsort()[::-1]]\nfeature_imp = pd.DataFrame(imps.reshape(1,11) , columns=headings.values)\nprint(feature_imp)\n\n# Apply regression on top five values and see the increase in score.\nx_5 = scaled_df[['smoker_no', 'smoker_yes', 'bmi', 'age' , 'children']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Linear Regression.\nr2_scr5 = []\nlr_5 = LinearRegression()\nfor tr,ts in kf.split(x_5):\n    x_tr5 = x_5.loc[tr]\n    y_tr5 = y.loc[tr]\n    x_ts5 = x_5.loc[ts]\n    y_ts5 = y.loc[ts]\n    lr_5.fit(x_tr5 , y_tr5)\n    y_5 = lr_5.predict(x_ts5)\n    r2_scr5.append(r2_score(y_ts5 , y_5))\nnp.mean(r2_scr5)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying polynomial regression.\nr2_pf5 = []\npf_5 = PolynomialFeatures(degree=3)\nx_pf5 = pf_5.fit_transform(x_5)\np5 = LinearRegression()\nfor tr,ts in kf.split(x_pf5):\n    x_tr5 = x_pf5[tr]\n    y_tr5 = y.loc[tr]\n    x_ts5 = x_pf5[ts]\n    y_ts5 = y.loc[ts]\n    p5.fit(x_tr5 , y_tr5)\n    y_5 = p5.predict(x_ts5)\n    r2_pf5.append(r2_score(y_ts5 , y_5))\nnp.mean(r2_pf5)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.boxplot(y = data['charges'])\nplt.title('Plot of Charges representing the outliers')\n\n# Find the outliers using IQR.\nq1 = data['charges'].quantile(0.25)\nq3 = data['charges'].quantile(0.75)\niqr = q3 - q1\n\n# Create a new df removing the outliers.\ndata_iqr = data[['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges']][(data.charges>(q1-1.5*iqr))&(data.charges<(q3+1.5*iqr))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the categorical values.\ndata_iqr = pd.get_dummies(data_iqr )\nx_iqr = data_iqr[['smoker_no', 'smoker_yes', 'bmi', 'age' , 'children']].reset_index()\ny_iqr = data_iqr['charges'].reset_index()\n\n# Apply linear regression and observe change in accuracy.\n\nr2_iqr = []\nlr_iqr = LinearRegression()\nfor tr,ts in kf.split(x_iqr):\n    x_tr_iqr = x_iqr.loc[tr]\n    y_tr_iqr = y_iqr.loc[tr]\n    x_ts_iqr = x_iqr.loc[ts]\n    y_ts_iqr = y_iqr.loc[ts]\n    lr_iqr.fit(x_tr_iqr , y_tr_iqr)\n    y_pred_iqr = lr_iqr.predict(x_ts_iqr)\n    r2_iqr.append(r2_score(y_ts_iqr , y_pred_iqr))\nnp.mean(r2_iqr)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply polynomial regression and observe the change in accuracy.\n\nr2_pf_iqr = []\npf_iqr = PolynomialFeatures(degree=2)\nx_ipf = pf_iqr.fit_transform(x_iqr)\np_pf = LinearRegression()\nfor tr,ts in kf.split(x_iqr):\n    x_tr_ipf = x_ipf[tr]\n    y_tr_ipf = y_iqr.loc[tr]\n    x_ts_ipf = x_ipf[ts]\n    y_ts_ipf = y_iqr.loc[ts]\n    p_pf.fit(x_tr_ipf , y_tr_ipf)\n    y_ipf = p_pf.predict(x_ts_ipf)\n    r2_pf_iqr.append(r2_score(y_ts_ipf , y_ipf))\nnp.mean(r2_pf_iqr)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Polynomial Regression using top-five features gave the best result."},{"metadata":{},"cell_type":"markdown","source":"# Classifcation Model to decide if a person is a smoker."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First encode the data.\ndata_encoded = data[['age', 'sex' , 'bmi', 'children', 'smoker', 'charges']]\n\n# Encoding via labels and defining target variable.\ndata_encoded[pd.get_dummies(data['region'] , prefix = 'region_').columns] = pd.get_dummies(data['region'] , prefix = 'region_')\ndata_encoded['sex'] = data_encoded['sex'].apply(lambda x: 1 if x == 'male' else 0)\ndata_encoded['smoker'] = data_encoded['smoker'].apply(lambda x: 1 if x == 'yes' else 0)\n\n# Define target variable.\nX = data_encoded[['age', 'sex', 'bmi', 'children', 'charges',\n       'region__northeast', 'region__northwest', 'region__southeast',\n       'region__southwest']]\nY = data_encoded['smoker']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the library.\nfrom sklearn.linear_model import LogisticRegression\n\n# Import the scores.\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\n\n# Define variables to store accuracy and precision.\nacc = []\nprecision = []\n\n# Define the kfold and the model.\nkf = KFold(n_splits=10 , shuffle=True, random_state=42)\nmodel = LogisticRegression()\n\n# Find the mean scores.\nfor tr,ts in kf.split(x):\n    X_tr = X.loc[tr]\n    Y_tr = Y.loc[tr]\n    X_ts = X.loc[ts]\n    Y_ts = Y.loc[ts]\n    model.fit(X_tr , Y_tr)\n    y_pred = model.predict(X_ts)\n    acc.append(accuracy_score(Y_ts , y_pred))\n    precision.append(precision_score(Y_ts , y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The accuracy score is ' + str(np.mean(acc)))\nprint('The precision score is ' + str(np.mean(precision)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the train and test.\nfrom sklearn.model_selection import train_test_split\ntr_df_X, test_df_X , tr_df_Y , test_df_Y = train_test_split(X,Y,test_size = 0.33 , random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hypertune the parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the library\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Hypertune the parameters.\nn = [10,20,30,50,100,150,200,500,1000,1500,2000,2500,3000,5000]\nn_e = 0\ncounter = 0\nfor i in n:\n    gr = GradientBoostingClassifier(n_estimators=i , random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' N-Estmators ' , i)\n    print(' Accuracy ' , accuracy_score(test_df_Y , y_pred))\n    print(' Precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        n_e = i\n        temp_acc = accuracy_score(test_df_Y , y_pred)\n        temp_pre = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if  ((temp_pre)<precision_score(test_df_Y , y_pred)):\n            n_e = i\n            temp_acc = accuracy_score(test_df_Y , y_pred)\n            temp_pre = precision_score(test_df_Y , y_pred)\n    \nprint(' The value n_estimators is ' , n_e , ' with Accuracy ', temp_acc , ' with Precision ' , temp_pre)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = [50,75,80,90,100,150,200,250,500]\nmS = 0\ncounter = 0\nfor i in n:\n    gr = GradientBoostingClassifier(n_estimators=n_e , min_samples_split=i, random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' min-samples-split ' , i)\n    print(' Accuracy Score ' , accuracy_score(test_df_Y , y_pred))\n    print(' Precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        mS = i\n        temp_acc = accuracy_score(test_df_Y , y_pred)\n        temp_pre = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if ((temp_pre)<precision_score(test_df_Y , y_pred)):\n            mS = i\n            temp_acc = accuracy_score(test_df_Y , y_pred)\n            temp_pre = precision_score(test_df_Y , y_pred)\n    \nprint(' min-samples-split is ' , mS , ' with Accuracy ', temp_acc , ' with Precision ' , temp_pre)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = [10,20,25,30,35,38,32,40,50,75,80,90,100,150,200,250,500]\nmL = 0\ncounter = 0\nfor i in n:\n    gr = GradientBoostingClassifier(n_estimators=n_e , min_samples_split=mS, min_samples_leaf=i, random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' min_samples_leaf ' , i)\n    print(' Accuracy ' , accuracy_score(test_df_Y , y_pred))\n    print(' Prescision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        mL = i\n        temp_acc = accuracy_score(test_df_Y , y_pred)\n        temp_pre = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if  ((temp_pre)<precision_score(test_df_Y , y_pred)):\n            mL = i\n            temp_acc = accuracy_score(test_df_Y , y_pred)\n            temp_pre = precision_score(test_df_Y , y_pred)\n    \nprint(' min_samples_leaf is ' , mL , ' with R2 ', temp_acc , ' with Prescision ' , temp_pre)      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = [10,20,25,30,35,38,32,40,50,75,80,90,100,150,200,250,500,1000,2000]\nmD = 0\ncounter = 0\nfor i in n:\n    gr = GradientBoostingClassifier(n_estimators=n_e , min_samples_split=mS, min_samples_leaf=mL,max_depth=i,  random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' max_depth ' , i)\n    print(' Accuracy ' , accuracy_score(test_df_Y , y_pred))\n    print(' Precision ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        mD = i\n        temp_acc = accuracy_score(test_df_Y , y_pred)\n        temp_pre = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if  ((temp_pre)<precision_score(test_df_Y , y_pred)):\n            mD = i\n            temp_acc = accuracy_score(test_df_Y , y_pred)\n            temp_pre = precision_score(test_df_Y , y_pred)\n    \nprint(' max_depth is ' , mD , ' with Accuracy ', temp_acc , ' with Precision ' , temp_pre)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = [2,3,4,5,6,8,9,10,20,25,30,35,38,32,40,50,75,80,90,100,150,200,250,400]\nmxL = 0\ncounter = 0\nfor i in n:\n    gr = GradientBoostingClassifier(n_estimators=n_e , min_samples_split=mS, min_samples_leaf=mL,max_depth=i,max_leaf_nodes=i, random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' max_leaf_nodes ' , i)\n    print(' Accuracy Score ' , accuracy_score(test_df_Y , y_pred))\n    print(' Precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        mxL = i\n        temp_acc = accuracy_score(test_df_Y , y_pred)\n        temp_pre = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if ((temp_pre)<precision_score(test_df_Y , y_pred)):\n            mxL = i\n            temp_acc = accuracy_score(test_df_Y , y_pred)\n            temp_pre = precision_score(test_df_Y , y_pred)\n    \nprint(' max_leaf_nodes is ' , mD , ' with Accuracy ', temp_acc , ' with Precision ' , temp_pre)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = [0.1 * x for x in range(1,21)]\nlR = 0\ncounter = 0\nfor i in n:\n    gr = GradientBoostingClassifier(n_estimators=n_e , min_samples_split=mS, min_samples_leaf=mL,max_depth=mD,max_leaf_nodes=mxL, learning_rate=i, random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' learning_rate ' , i)\n    print(' accuracy Score ' , accuracy_score(test_df_Y , y_pred))\n    print(' precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        lR = i\n        temp_accuracy = accuracy_score(test_df_Y , y_pred)\n        temp_precision = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if ((temp_precision)<precision_score(test_df_Y , y_pred)):\n            lR = i\n            temp_accuracy = accuracy_score(test_df_Y , y_pred)\n            temp_precision = precision_score(test_df_Y , y_pred)\n    \nprint(' learning_rates is ' , lR , ' with accuracy ', temp_accuracy , ' with precision ' , temp_precision)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = [2,3,4,5,6,7,8,9]\nmF = 0\ncounter = 0\nfor i in n:\n    gr = GradientBoostingClassifier(n_estimators=n_e , min_samples_split=mS, min_samples_leaf=mL,max_depth=mD,max_leaf_nodes=mxL, learning_rate=lR,max_features=i, random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' max_-features ' , i)\n    print(' accuracy Score ' , accuracy_score(test_df_Y , y_pred))\n    print(' precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        mF = i\n        temp_accuracy = accuracy_score(test_df_Y , y_pred)\n        temp_precision = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if ((temp_precision)<precision_score(test_df_Y , y_pred)):\n            mF = i\n            temp_accuracy = accuracy_score(test_df_Y , y_pred)\n            temp_precision = precision_score(test_df_Y , y_pred)\n    \nprint(' max-features ' , mF , ' with accuracy ', temp_accuracy , ' with precision ' , temp_precision)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adaptive Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the library\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Hypertune the parameters.\nn = [10,20,30,50,100,150,200,500,1000]\nn_e = 0\ncounter = 0\nfor i in n:\n    gr = AdaBoostClassifier(n_estimators=i , random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' N-Estmators ' , i)\n    print(' Accuracy Score ' , accuracy_score(test_df_Y , y_pred))\n    print(' Precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        n_e = i\n        temp_acc = accuracy_score(test_df_Y , y_pred)\n        temp_pre = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if  ((temp_pre)<precision_score(test_df_Y , y_pred)):\n            n_e = i\n            temp_acc = accuracy_score(test_df_Y , y_pred)\n            temp_pre = precision_score(test_df_Y , y_pred)\n    \nprint(' The value n_estimators is ' , n_e , ' with Accuracy ', temp_acc , ' with Precision ' , temp_pre)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = [0.1 * x for x in range(1,51)]\nlR = 0\ncounter = 0\nfor i in n:\n    gr = AdaBoostClassifier(n_estimators=n_e , learning_rate=i , random_state=42)\n    gr.fit(tr_df_X , tr_df_Y)\n    y_pred = gr.predict(test_df_X)\n    print(' learning-rate ' , i)\n    print(' accuracy Score ' , accuracy_score(test_df_Y , y_pred))\n    print(' precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        lR = i\n        temp_accuracy = accuracy_score(test_df_Y , y_pred)\n        temp_precision = precision_score(test_df_Y , y_pred)\n        counter += 1\n    else:\n        if ((temp_precision)<precision_score(test_df_Y , y_pred)):\n            lR = i\n            temp_accuracy = accuracy_score(test_df_Y , y_pred)\n            temp_precision = precision_score(test_df_Y , y_pred)\n    \nprint(' learning-rate ' , lR , ' with accuracy ', temp_accuracy , ' with precision ' , temp_precision) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\n# Hypertune parameters.\nn = [2,8,10,12,15,20,25,28,30,35,50,55,60,70,75,80,100,150,125,175,200,250,300]\ncounter = 0\nc = 0\nfor i in n:\n    lSVR = LinearSVC(C=i )\n    lSVR.fit(tr_df_X, tr_df_Y)\n    y_pred = lSVR.predict(test_df_X)\n    print(' C ' , i)\n    print(' precision Score ' , accuracy_score(test_df_Y , y_pred))\n    print(' Precision Score ' , precision_score(test_df_Y , y_pred))\n    if counter == 0:\n        counter+=1\n        c=i\n        temp_acc = accuracy_score(test_df_Y , y_pred)\n        temp_pre = precision_score(test_df_Y , y_pred)\n    else:\n        if  ((temp_pre)<precision_score(test_df_Y , y_pred)):\n            c = i\n            temp_acc = accuracy_score(test_df_Y , y_pred)\n            temp_pre = precision_score(test_df_Y , y_pred)   \n \n    \nprint(' Penalty(C) ' , c , ' with accuracy ', temp_acc , ' with Precision ' , temp_pre)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM has performed the best in identifying smokers."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}