{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\n\nhn = pd.read_csv(\"/kaggle/input/hacker-news-posts/HN_posts_year_to_Sep_26_2016.csv\")\ntitles = hn['title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"[Pp]ython\"\npython_counts = titles.str.contains(pattern).sum()\nprint(python_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we learned is to use re.I — the ignorecase flag — to make our pattern case insensitive:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r'python'\npython_counts = titles.str.contains(pattern,flags = re.I).sum()\nprint(python_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r'[Ss][Qq][Ll]'\nsql_counts = titles.str.contains(pattern).sum()\nprint(sql_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r'[Ss][Qq][Ll]'\nsql_counts = titles.str.contains(pattern,flags = re.I).sum()\nprint(sql_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Using the Series.str.extract() method.\n- https://s3.amazonaws.com/dq-content/369/single_capture_group.svg"},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"(SQL)\"\nsql_capitalizations = titles.str.extract(pattern, flags=re.I)\nprint(sql_capitalizations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- using the Series.value_counts() method to create a frequency table "},{"metadata":{"trusted":true},"cell_type":"code","source":"sql_capitalizations_freq = sql_capitalizations[0].value_counts()\nprint(sql_capitalizations_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- extend this analysis by looking at titles that have letters immediately before the \"SQL,\" "},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"(\\w+SQL)\"\nsql_flavors = titles.str.extract(pattern,flags= re.I)\nsql_flavors_freq = sql_flavors[0].value_counts()\nprint(sql_flavors_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hn_sql = hn[hn['title'].str.contains(r\"\\w+SQL\", flags=re.I)].copy() \nhn_sql","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hn_sql[\"flavor\"] = hn_sql[\"title\"].str.extract(r\"(\\w+SQL)\", re.I)\nhn_sql[\"flavor\"] = hn_sql[\"flavor\"].str.lower()\nsql_pivot = hn_sql.pivot_table(index=\"flavor\",values='num_comments', aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sql_pivot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- capture groups to extract the version of Python https://s3.amazonaws.com/dq-content/369/python_versions.svg\n[\\d\\.]+ - one or more digit or character "},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"[Pp]ython ([\\d\\.]+)\"\n\npy_versions = titles.str.extract(pattern)\npy_versions_freq = dict(py_versions[0].value_counts())\npy_versions_freq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Next up: counting the mentions of the C language.\nhttps://s3.amazonaws.com/dq-content/369/c_regex_1.svg\n- \\b a word boundry ..[Cc]- the character Cc \\b a word boundry \n- re-useing the previous first_10_matches() function "},{"metadata":{"trusted":true},"cell_type":"code","source":"def first_10_matches(pattern):\n    \"\"\"\n    Return the first 10 story titles that match\n    the provided regular expression\n    \"\"\"\n    all_matches = titles[titles.str.contains(pattern)]\n    first_10 = all_matches.head(10)\n    return first_10\n\nfirst_10_matches(r\"\\b[Cc]\\b\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- use a negative set to prevent matches for the + character and the ."},{"metadata":{"trusted":true},"cell_type":"code","source":"def first_10_matches(pattern):\n    all_matches = titles[titles.str.contains(pattern)]\n    first_10 = all_matches.head(10)\n    return first_10\n    \n\npattern = r\"\\b[Cc]\\b [^ . +]\"\nfirst_10_matches(pattern)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Instead we'll need a new tool: lookarounds.( to avoid  irrelevant result line series C)\n- https://s3.amazonaws.com/dq-content/369/lookarounds.svg - four types of lookarounds\n- 1. tips\n* Inside the parentheses, the first character of a lookaround is always ?.\n* If the lookaround is a lookbehind, the next character will be <, which you can think of as an arrow head pointing behind the match.\n* The next character indicates whether the lookaround is positive (=) or negative (!)."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cases = ['Red_Green_Blue',\n              'Yellow_Green_Red',\n              'Red_Green_Red',\n              'Yellow_Green_Blue',\n              'Green']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_test_cases(pattern):\n    for tc in test_cases:\n        result = re.search(pattern, tc)\n        print(result or \"NO MATCH\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_test_cases(r\"Green(?=_Blue)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_test_cases(r\"Green(?!_Red)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- positive lookbehind"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_test_cases(r\"(?<=Red_)Green\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- negative lookbehind"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_test_cases(r\"(?<!Yellow_)Green\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_test_cases(r\"Green(?=.{5})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- now use lookarounds to exclude the matches we don't want"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_10_matches(r\"\\b[Cc]\\b[^.+]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"(?<!Series\\s)\\b[Cc]\\b(?![\\+\\.])\"\nc_mentions = titles.str.contains(pattern).sum()\nc_mentions ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- using backreferences for double letters HelloGoodbye\n- https://s3.amazonaws.com/dq-content/369/backreference_syntax_1.svg\n- Within a regular expression, we can use a backslash followed by that integer to refer to the group:\\2\\1"},{"metadata":{},"cell_type":"markdown","source":"- HelloGoodbyeGoodbyeHello - (\\w)\\1\n- https://s3.amazonaws.com/dq-content/369/backreference_syntax_2.svg"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cases = [\n              \"I'm going to read a book.\",\n              \"Green is my favorite color.\",\n              \"My name is Aaron.\",\n              \"No doubles here.\",\n              \"I have a pet eel.\"\n             ]\nfor tc in test_cases:\n    print(re.search(r\"(\\w)\\1\", tc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cases = pd.Series(test_cases)\ntest_cases\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_cases.str.contains(r\"(\\w)\\1\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"\\b(\\w+)\\s\\1\\b\"\n\nrepeated_words = titles[titles.str.contains(pattern)]\nrepeated_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- str.replace() method is used to replace simple substrings\n- We can achieve the same with regular expressions using the re.sub() function"},{"metadata":{"trusted":true},"cell_type":"code","source":"re.sub(pattern, repl, string, flags=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- repl parameter is the text that you would like to substitute for the match"},{"metadata":{"trusted":true},"cell_type":"code","source":"string = \"aBcDEfGHIj\"\n\nprint(re.sub(r\"[A-Z]\", \"-\", string))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- while working with pandas we use, Series.str.replace(pat, repl, flags=0)"},{"metadata":{},"cell_type":"markdown","source":"- there were multiple different capitalizations for SQL in our dataset. Lets make this uniform, Series.str.replace() method and a regular expression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sql_variations = pd.Series([\"SQL\", \"Sql\", \"sql\"])\n\nsql_uniform = sql_variations.str.replace(r\"sql\", \"SQL\", flags=re.I)\nprint(sql_uniform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Question here we are provided with email_variations,lets use the techique to make this uniofrm "},{"metadata":{"trusted":true},"cell_type":"code","source":"email_variations = pd.Series(['email', 'Email', 'e Mail',\n                        'e mail', 'E-mail', 'e-mail',\n                        'eMail', 'E-Mail', 'EMAIL'])\npattern = r\"e[\\-\\s]?mail\"\nemail_uniform = email_variations.str.replace(pattern, \"email\", flags=re.I)\nemail_uniform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles_clean = titles.str.replace(pattern, \"email\", flags=re.I)\ntitles_clean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we'll extract components of URLs from our dataset,task we will be performing first is extracting the different components of the URLs in order to analyze them. On this screen, we'll start by extracting just the domains.https://s3.amazonaws.com/dq-content/369/url_examples_1_updated.svg\n* following technique:\n\n- Using a series of characters that will match the protocol.\n- Inside a capture group, using a set that will match the character classes used in the domain.\n- Because all of the URLs either end with the domain, or continue with page path which starts with / (a character not found in any domains), we don't need to cater for this part of the URL in our regular expression."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_urls = pd.Series([\n 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n 'http://www.interactivedynamicvideo.com/',\n 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',\n 'HTTPS://github.com/keppel/pinn',\n 'Http://phys.org/news/2015-09-scale-solar-youve.html',\n 'https://iot.seeed.cc',\n 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',\n 'http://beta.crowdfireapp.com/?beta=agnipath',\n 'https://www.valid.ly?param',\n 'http://css-cursor.techstream.org'\n])\npattern = r\"https?://([\\w\\-\\.]+)\"\ntest_urls_clean = test_urls.str.extract(pattern,flags=re.I)\ntest_urls_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"domains = hn[\"url\"].str.extract(pattern,flags= re.I)\ntop_domains = domains[0].value_counts().head(5)\ntop_domains","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)Having extracted just the domains from the URLs, on this final screen we'll extract each of the three component parts of the URLs:\n\n* Protocol\n* Domain\n* Page path\n\nhttps://s3.amazonaws.com/dq-content/369/url_examples_2_updated.svg"},{"metadata":{"trusted":true},"cell_type":"code","source":"created_at = hn['created_at'].head()\nprint(created_at)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We'll use capture groups to extract these dates and times into two columns:\n- https://s3.amazonaws.com/dq-content/369/multiple_capture_groups.svg\n- (.+)\\s(.+) "},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"(.+)\\s(.+)\"\ndates_times = created_at.str.extract(pattern)\nprint(dates_times)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Question write a regular expression that will extract the URL components into individual columns of a dataframe\n- Write a regular expression that extracts URL components using three capture groups:\n* The first capture group should include the protocol text, up to but not including ://.\n* The second group should contain the domain, from after :// up to but not including /.\n* The third group should contain the page path, from after / to the end of the string."},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"(https?)://([\\w\\.\\-]+)/?(.*)\"\n\ntest_url_parts = test_urls.str.extract(pattern, flags=re.I)\ntest_url_parts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"url_parts = hn['url'].str.extract(pattern, flags=re.I)\nurl_parts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- final task will be to name unnamed columns using named capture groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"#e.g\ncreated_at = hn['created_at'].head()\n\npattern = r\"(.+) (.+)\"\ndates_times = created_at.str.extract(pattern)\nprint(dates_times)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- https://s3.amazonaws.com/dq-content/369/named_capture_groups.svg - In order to name a capture group we use the syntax ?P, where name is the name of our capture group"},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"(?P<date>.+) (?P<time>.+)\"\ndates_times = created_at.str.extract(pattern)\nprint(dates_times)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Question adding names to our capture group from the previous screen to create a dataframe with named columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = r\"(?P<protocol>https?)://(?P<domain>[\\w\\.\\-]+)/?(?P<path>.*)\"\nurl_parts = hn['url'].str.extract(pattern,flags=re.I)\nurl_parts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}