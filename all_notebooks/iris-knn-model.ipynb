{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # creating graphs for visualization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-06T17:06:23.406758Z","iopub.execute_input":"2021-07-06T17:06:23.407113Z","iopub.status.idle":"2021-07-06T17:06:23.412737Z","shell.execute_reply.started":"2021-07-06T17:06:23.407083Z","shell.execute_reply":"2021-07-06T17:06:23.410933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part One - Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### Importing Dataset using Pandas ","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T16:53:31.665009Z","iopub.execute_input":"2021-07-06T16:53:31.665576Z","iopub.status.idle":"2021-07-06T16:53:31.722729Z","shell.execute_reply.started":"2021-07-06T16:53:31.66553Z","shell.execute_reply":"2021-07-06T16:53:31.721715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating arrays to store features and dependent variable","metadata":{}},{"cell_type":"code","source":"X = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","metadata":{"execution":{"iopub.status.busy":"2021-07-06T16:58:45.927307Z","iopub.execute_input":"2021-07-06T16:58:45.927683Z","iopub.status.idle":"2021-07-06T16:58:45.933777Z","shell.execute_reply.started":"2021-07-06T16:58:45.92765Z","shell.execute_reply":"2021-07-06T16:58:45.932679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting Dataset into training set and test set using Scikit-Learn\nNote: the test size parameter simply determines the size of the dataset dedicated to the X_test and y_test variables. The random_state parameter is optional and ensures that the splits that I generate are reproducible for you.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\nX_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T16:58:48.375066Z","iopub.execute_input":"2021-07-06T16:58:48.375408Z","iopub.status.idle":"2021-07-06T16:58:49.592658Z","shell.execute_reply.started":"2021-07-06T16:58:48.375377Z","shell.execute_reply":"2021-07-06T16:58:49.591473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T16:59:11.206178Z","iopub.execute_input":"2021-07-06T16:59:11.206771Z","iopub.status.idle":"2021-07-06T16:59:11.218905Z","shell.execute_reply.started":"2021-07-06T16:59:11.206734Z","shell.execute_reply":"2021-07-06T16:59:11.217659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T16:59:15.576859Z","iopub.execute_input":"2021-07-06T16:59:15.577264Z","iopub.status.idle":"2021-07-06T16:59:15.582848Z","shell.execute_reply.started":"2021-07-06T16:59:15.577226Z","shell.execute_reply":"2021-07-06T16:59:15.581528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Scaling our model with the Scikit Learn StandardScaler class\nSince we will be building a K-Nearest Neighbors model, we have to do feature scaling. Also, you will notice we used the fit_transform method on the X_train variable but only the transform method on the X_test variable. This is done to prevent overfitting and information leakage.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:00:11.177669Z","iopub.execute_input":"2021-07-06T17:00:11.178092Z","iopub.status.idle":"2021-07-06T17:00:11.186542Z","shell.execute_reply.started":"2021-07-06T17:00:11.178047Z","shell.execute_reply":"2021-07-06T17:00:11.184789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 2 - Creating Our KNN Model","metadata":{}},{"cell_type":"markdown","source":"#### Determine best number of neighbors for our KNN model to achieve highest accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nk_range = list(range(1, 26))\nscores = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k, p = 2, metric=\"minkowski\")\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores.append(accuracy_score(y_test, y_pred))\n    \nplt.plot(k_range, scores)\nplt.xlabel(\"Value of k for KNN\")\nplt.ylabel(\"Accuracy Score\")\nplt.title(\"Accuracy Score for Values of K for KNN Classifier\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:01:25.23711Z","iopub.execute_input":"2021-07-06T17:01:25.237482Z","iopub.status.idle":"2021-07-06T17:01:25.505293Z","shell.execute_reply.started":"2021-07-06T17:01:25.237452Z","shell.execute_reply":"2021-07-06T17:01:25.504314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Implementing a value to create our model\nAs we can see from this graph, assigning n_neighbors to any value between 5 and 16 shows the highest accuracy. I will be using 10 neighbors as shown below:","metadata":{}},{"cell_type":"code","source":"classifier = KNeighborsClassifier(n_neighbors = 10, metric = \"minkowski\", p = 2)\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:02:53.577231Z","iopub.execute_input":"2021-07-06T17:02:53.577591Z","iopub.status.idle":"2021-07-06T17:02:53.586675Z","shell.execute_reply.started":"2021-07-06T17:02:53.577563Z","shell.execute_reply":"2021-07-06T17:02:53.585895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 3 - Testing the Accuracy of our model","metadata":{}},{"cell_type":"markdown","source":"#### Predicting certain output values based on given features","metadata":{}},{"cell_type":"code","source":"print(classifier.predict(sc.transform([[5.7,4.4,1.5,0.4]])))  # Expecting output: 'Iris-setosa'","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:04:25.127293Z","iopub.execute_input":"2021-07-06T17:04:25.127745Z","iopub.status.idle":"2021-07-06T17:04:25.139013Z","shell.execute_reply.started":"2021-07-06T17:04:25.127711Z","shell.execute_reply":"2021-07-06T17:04:25.137655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classifier.predict(sc.transform([[6.4,3.2,4.5,1.5]])))  # Expecting output: 'Iris-versicolor'","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:04:26.781694Z","iopub.execute_input":"2021-07-06T17:04:26.782135Z","iopub.status.idle":"2021-07-06T17:04:26.793035Z","shell.execute_reply.started":"2021-07-06T17:04:26.782101Z","shell.execute_reply":"2021-07-06T17:04:26.791491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classifier.predict(sc.transform([[6.7,3.1,5.6,2.4]])))   # Expecting output: 'Iris-virginica'","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:04:28.352395Z","iopub.execute_input":"2021-07-06T17:04:28.352907Z","iopub.status.idle":"2021-07-06T17:04:28.361769Z","shell.execute_reply.started":"2021-07-06T17:04:28.35286Z","shell.execute_reply":"2021-07-06T17:04:28.360577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reshaping y_pred and y_test to compare the dependent variables and its accuracy","metadata":{}},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:05:29.226769Z","iopub.execute_input":"2021-07-06T17:05:29.227299Z","iopub.status.idle":"2021-07-06T17:05:29.239189Z","shell.execute_reply.started":"2021-07-06T17:05:29.227241Z","shell.execute_reply":"2021-07-06T17:05:29.237869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating the Confusion Matrix and obtaining final accuracy score of our model ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred) * 100","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:05:51.364034Z","iopub.execute_input":"2021-07-06T17:05:51.364388Z","iopub.status.idle":"2021-07-06T17:05:51.37707Z","shell.execute_reply.started":"2021-07-06T17:05:51.364352Z","shell.execute_reply":"2021-07-06T17:05:51.375645Z"},"trusted":true},"execution_count":null,"outputs":[]}]}