{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom keras.preprocessing.text import Tokenizer\nfrom gensim.models.fasttext import FastText\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom string import punctuation\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize\nfrom nltk import WordPunctTokenizer\n\n#import wikipedia\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nen_stop = set(nltk.corpus.stopwords.words('english'))\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/flipkart-products/flipkart_com-ecommerce_sample.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['product_name', 'product_category_tree', 'description']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = WordNetLemmatizer()\n\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        # Lemmatization\n        tokens = document.split()\n        tokens = [stemmer.lemmatize(word) for word in tokens]\n        tokens = [word for word in tokens if word not in en_stop]\n        tokens = [word for word in tokens if len(word) > 3]\n\n        preprocessed_text = ' '.join(tokens)\n        \n        return preprocessed_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['new'] = df1['product_name'] + ' ' + df1['product_category_tree'] + ' ' + df1['description']\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_corpus = [preprocess_text(sentence) for sentence in df1['new'] if str(sentence).strip() !='']\nfinal_corpus[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_punctuation_tokenizer = nltk.WordPunctTokenizer()\nword_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\nword_tokenized_corpus[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 60\nwindow_size = 4\nmin_word = 5\ndown_sampling = 1e-2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nft_model = FastText(word_tokenized_corpus,\n                      size=embedding_size,\n                      window=window_size,\n                      min_count=min_word,\n                      sample=down_sampling,\n                      sg=1,\n                      iter=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from gensim.test.utils import get_tmpfile\nfname = get_tmpfile(\"../output/fasttext.model\")\nft_model.save(fname)\nft_model = FastText.load(fname)'''\n#ft_model.save_model('../input/ft_model.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = \"eternal\"\nword in ft_model.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = \"crazy\"\nword in ft_model.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_model.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_model['pet']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ft_model.most_similar(\"clothes\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ft_model.most_similar(\"kitchen\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ft_model.most_similar(\"mobile\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ft_model.most_similar(\"kids\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ft_model.most_similar(\"footwear\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent = [final_corpus[0]]\nfor word in sent:\n    print(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = 'alisha solid woman cycling short clothing woman clothing lingerie sleep swimwear short alisha short alisha solid woman cycling short feature alisha solid woman cycling short cotton lycra navy navy specification alisha solid woman cycling short short detail number content sale package pack fabric cotton lycra type cycling short general detail pattern solid ideal woman fabric care gentle machine wash lukewarm water bleach additional detail style code altht_3p_21 short'\nwords = [word for word in [doc]]\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # **Calculate the mean vector**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mean_vector(model, words):\n    # remove out-of-vocabulary words\n    wrds = [word for word in [words] if word in model.wv.vocab]\n    print(wrds)\n    if len(words) >= 1:\n        return np.mean(model[words], axis=0)\n    else:\n        return []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Generate embeddings for the entire corpus**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict = {} \n  \ndict[1]='anjani'\ndict[2]='vikas'\ndict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''for doc in final_corpus:\n    for word in doc.split():\n        if word in ft_model.wv.vocab:\n            print(word+'')'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ConvertVectorSetToVecAverageBased(vectorSet, ignore = []):\n    if len(ignore) == 0: \n        return np.mean(vectorSet, axis = 0)\n    else: \n        return np.dot(np.transpose(vectorSet),ignore)/sum(ignore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorSet = []\nfor doc in final_corpus:\n    wrds = [word for word in doc.split() if word in ft_model.wv.vocab]\n    for aWord in wrds:\n        try:\n            wordVector=ft_model[aWord]\n            vectorSet.append(wordVector)\n        except:\n            pass\n    ConvertVectorSetToVecAverageBased(vectorSet)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nvec = {}\nfor doc in final_corpus:\n    i = i+1\n    #print(doc)\n    #vec[i] = get_mean_vector(ft_model, doc)\n    wrds = [word for word in doc.split() if word in ft_model.wv.vocab]\n    #wrds = [word for word in doc]\n    #print(wrds)\n    #print(wrds)\n    if len(words) >= 1:\n        #vec.append(np.mean(ft_model[wrds], axis=0))\n        vec[i] = np.mean(ft_model[np.array(wrds)])\n        #print(i)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for doc in final_corpus:\n    print(doc + '*****')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}