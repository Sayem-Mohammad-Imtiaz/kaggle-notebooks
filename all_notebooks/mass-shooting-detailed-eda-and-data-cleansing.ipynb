{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{},"source":"This notebook aims at:\n1) Providing an in-depth analysis of the given dataset.\n\n2) Preparing the data to make it suitable for a Machine learning model.\n\n3) Giving you all what you need for a head start."},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ncolor = sns.color_palette()\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"26068a8881da8f0490b17914d439ddf4c5859cc0","_cell_guid":"a073ac41-b55d-48a7-9985-093ed6bade04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"So there are two versions of this dataset available to us. The dataset has been updated recently. As of now, I will be focusing on the updated version and will try to get into as much detail as possible with version 2. Let's start by looking at the dataset"},{"cell_type":"code","source":"data = pd.read_csv('../input/Mass Shootings Dataset Ver 2.csv', encoding=\"ISO-8859-1\")\nprint(data.shape)\ndata.head()","metadata":{"_uuid":"4f9e59990d79692ed4bb7f0accd3d395aa775042","_cell_guid":"d60d64f1-4f1f-473d-83b1-628b377411fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"a3d7c420d1d13229c58543a05f52037c7b5c1eb6","_cell_guid":"cc7a3564-1903-49cf-9592-a799d4e0a595"},"source":"So we have date column, categorical and numerical columns along with some text columns like Title and Summary. I can see a lot of NaN too in the longitude and latitude column. Don't worry, we will take care of them as we go. We will start analyzing the dataset column by column. Let's drop the S# column first and convert the date column also."},{"cell_type":"code","source":"# Drop the S# column\ndata.drop(['S#'], axis=1, inplace=True)\n\n# Convert the date column as appropriate datetime format\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Split the date column into year, month and day\ndata['Year'] = data.Date.dt.year\ndata['Month'] = data.Date.dt.month\ndata['Day'] = data.Date.dt.day\ndata['Weekday'] = data.Date.dt.dayofweek","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take an overview now\ndata.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the number of NaN values before moving on to each column \ndata.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"Okay, so we have a lot of missing values in three columns namely, location, longitude and latitude. There is only one missing value in the summary column and I don't think it's that important."},{"cell_type":"code","source":"# Let's start with the year first. \nyear_counts = data.Year.value_counts()\n\nplt.figure(figsize=(15,10))\nsns.barplot(year_counts.index, year_counts.values)\nplt.title('Number of mass shooting per year')\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(rotation=90)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"Overall the number of mass shooting has been increasing every year. The rate is almost exponential if we look at the data from 2010-2017. Is there some sort of pattern here? I suspect so. Presedential elections held in every four years. As the elections come nearby, number of mass shooting increases. Look at 2015-16(highest number of mass shooting), then look at 2012, 2007-08, 2003-04..."},{"cell_type":"code","source":"# What about the months? Which months count for maximum incidents?\nmonth_counts = data.Month.value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(month_counts.index, month_counts.values)\nmonths = ('Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')\nplt.title('Number of monthly mass shooting')\nplt.xlabel('Month', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(range(12), months)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"So, mass shooting happens most in Feb and then decreases at a good rate. Dec and Jan have almost the same number of mass shooting cases."},{"cell_type":"code","source":"# What about the days of a month? \nday_counts = data.Day.value_counts()\n\nplt.figure(figsize=(15,10))\nsns.barplot(day_counts.index, day_counts.values)\nplt.title('Number of mass shooting')\nplt.xlabel('Day of the month', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(rotation=90)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What about the day of the week?\nweekday_counts = data.Weekday.value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(weekday_counts.index, weekday_counts.values)\ndays = ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')\nplt.title('Number of mass shooting')\nplt.xlabel('Weekday', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(range(7), days)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"So, Thursday contributes to the maximum number of mass shootings followed by Sunday and Friday.\nEnough with the date, let's check the number of fatalities, injured and total victims "},{"cell_type":"code","source":"# Fatalities check\nprint(\"Maximum number of fatalities in a mass shooting : \", np.max(data['Fatalities']))\nprint(\"Minimum number of fatalities in a mass shooting : \", np.min(data['Fatalities']))\nprint(\"Average number of fatalities in any mass shooting : \", int(np.mean(data['Fatalities'])))\n\nfat_count = data.Fatalities\nplt.figure(figsize=(10,5))\nplt.scatter(range(len(fat_count)), np.sort(fat_count.values), alpha=0.7)\nplt.title(\"Fatalities count in mass shooting\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Moving on to the injured column\nprint(\"Maximum number of injured in a mass shooting : \", np.max(data['Injured']))\nprint(\"Minimum number of injured in a mass shooting : \", np.min(data['Injured']))\nprint(\"Average number of injured in any mass shooting : \", int(np.mean(data['Injured'])))\n\ninj_count = data['Injured']\nplt.figure(figsize=(10,5))\nplt.scatter(range(len(inj_count)), np.sort(inj_count.values), alpha=0.7)\nplt.title(\"Injured count in mass shooting\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"So, in case of **fatalities** we have at least two outliers and in case of **injuries**, we have at most one outlier."},{"cell_type":"code","source":"# Let's check the total number of victims in any case\nprint(\"Maximum number of victims in a mass shooting : \", np.max(data['Total victims']))\nprint(\"Minimum number of victims in a mass shooting : \", np.min(data['Total victims']))\nprint(\"Average number of victims in any mass shooting : \", int(np.mean(data['Total victims'])))\n\nvictim_count = data['Total victims']\nplt.figure(figsize=(10,5))\nplt.scatter(range(len(victim_count)), np.sort(victim_count.values), alpha=0.7)\nplt.title(\"Victim count in mass shooting\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mental health issues\nmental_health_count = data['Mental Health Issues'].value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(mental_health_count.index, mental_health_count.values)\nplt.title('Mental health')\nplt.xlabel('Issue', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(range(len(mental_health_count.index)), mental_health_count.index)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"A lot of people involved in shooting were suffereing from mental health issues. Just make sure you combine **Unknown** and **unknown** before passing the data to a machine learning model. "},{"cell_type":"code","source":"data['Mental Health Issues'] = data['Mental Health Issues'].apply(lambda x: 'Unknown' if x=='unknown' else x)\n\nmental_health_count = data['Mental Health Issues'].value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(mental_health_count.index, mental_health_count.values)\nplt.title('Mental health')\nplt.xlabel('Issue', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(range(len(mental_health_count.index)), mental_health_count.index)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"Let's move to the **Race** column now. This column may provide some deep insights about why mass shooting happened in the first place (e.g. hatred, etc.). Let's jump in."},{"cell_type":"code","source":"race_count = data['Race'].value_counts()\n\nplt.figure(figsize=(15,10))\nsns.barplot(race_count.values, race_count.index, orient='h')\nplt.xlabel('Count', fontsize=14)\nplt.ylabel('Race', fontsize=14)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"So maximum number of people involved in a mass shooting happens to be one of the following: White/European/Black/African American. There are people from other races too but that number is pretty low. \n\nThere is an important thing to notice here. The categories as per the records are messed up. For example, we have **White American or European American** as well as **White American or European American/Some other Race**. This can be misleading for a machine learning model in some case. Though there might be some other good way to deal with this kind of info but here is the what I suggest:\n\n1) Combine **white** and **White** as **White**\n\n2) Combine **black** and **Black** as **Black**\n\n3) Combine **White American or European American/Some other Race** and **White American or European American**\n\n4) Combine **Black American or African American/Unknown** and **Black American or African American**\n\n5) Combine **Asian American/Some other race** and **Asian American**\n\n6) Combine **Two or more races** and **Other** with **Unknown**"},{"cell_type":"code","source":"data['Race'] = data['Race'].apply(lambda x : 'White' if x=='white' else x)\ndata['Race'] = data['Race'].apply(lambda x : 'Black' if x=='black' else x)\n\ndata['Race'] = data['Race'].apply(lambda x : 'White American or European American' \n                                  if x=='White American or European American/Some other Race' else x)\n\ndata['Race'] = data['Race'].apply(lambda x : 'Black American or African American' \n                                  if x=='Black American or African American/Unknown' else x)\n\ndata['Race'] = data['Race'].apply(lambda x : 'Asian American' if x=='Asian American/Some other race' else x)\ndata['Race'] = data['Race'].apply(lambda x : 'Unknown' if x=='Two or more races' or x =='unknown' else x)\n\ndata['Race'] = data['Race'].apply(lambda x : 'Other' if x=='Some other race' else x)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the final results \nrace_count = data['Race'].value_counts()\n\nplt.figure(figsize=(15,10))\nsns.barplot(race_count.values, race_count.index, orient='h')\nplt.xlabel('Count', fontsize=14)\nplt.ylabel('Race', fontsize=14)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"Perfect!!"},{"cell_type":"code","source":"# Let's move to the Gender column now\ngender = data['Gender'].value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(gender.index, gender.values)\nplt.title('Gender involved in mass shooting')\nplt.xlabel('Gender', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(range(len(gender.index)), gender.index)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"Nothing odd that **Male** would have been involved in the mass shooting most. But here again, the data needs cleansing. So, let's do that first."},{"cell_type":"code","source":"data['Gender'] = data['Gender'].apply(lambda x: 'Male' if x=='M' else x)\ndata['Gender'] = data['Gender'].apply(lambda x: 'Unknown' if x=='M/F' or x=='Male/Female' else x)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check it once more now\ngender = data['Gender'].value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(gender.index, gender.values)\nplt.title('Gender involved in mass shooting')\nplt.xlabel('Gender', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(range(len(gender.index)), gender.index)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"Now, let's explore one more interesting and important column. The **Location** where mass shooting happened."},{"cell_type":"code","source":"data.Location.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can split the location column into two different columns: City and State. Let's do that first\ndata['City'] = data['Location'].str.rpartition(',')[0]\ndata['State'] = data['Location'].str.rpartition(',')[2]","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets' look at the values of the state column\ndata.State.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Either convert all States to their abbreviations or do the reverse. Because we have only few entries as abbreviations\n# we will do the reverse.\n\"\"\"\n1) Replace CA with California\n2) Replace NV with Nevada\n3) Replace LA with Louisiana\n4) Replace PA with Pennsylvania\n5) Replace WA with Washington D.C.\n\"\"\"\ndata['State'] = data.State.str.strip()\ndata['State'] = data.State.str.replace('CA', 'California')\ndata['State'] = data.State.str.replace('NV', 'Nevada')\ndata['State'] = data.State.str.replace('LA', 'Louisiana')\ndata['State'] = data.State.str.replace('PA', 'Pennsylvania')\ndata['State'] = data.State.str.replace('WA', 'Washington D.C.')","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check again\ndata.State.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"So, maximum shooting happens in California followed by Florida and Texas. There are 45 missing values in this column, remember? So, let's go ahead and replace them with some value say, **Unknown**"},{"cell_type":"code","source":"data.Location.fillna('Unknown', inplace=True)\ndata.City.fillna('Unknown', inplace=True)\ndata.State.fillna('Unknown', inplace=True)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's move to the summary column and check the wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(background_color='black',\n                        stopwords=stopwords,\n                        max_words=100,\n                        max_font_size=30, \n                        random_state=42).generate(str(data['Summary']))\n\nplt.figure(figsize=(20,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"The **Title** column contain a lot of info. for example, Las Vegas Strip Mass shooting, describes the place and the event along with its type that happened. We already have the location info from the location column. Let's modify the content of this column to something short and informative."},{"cell_type":"markdown","metadata":{},"source":"That's it for now as I need to do something and that's too urgent. Some of the ideas that  you can use to clean this data more and build some pretty good models are:\n\n1) You can extract valuable information like the cause of shooting from **Title** and **Summary** column. Better to combine the info and extract uncommon words.\n\n2) I haven't visited the longitude and latitude columns yet but you can use this info if you are building a model to predict the number of victims in a shooting.\n\n3) Are there sentiments in the **Summary** column?\n\n4) Can you build a model that can be used to predict the motive behind a shooting incident?\n\nI have tried to dig into the dataset as much as possible. If you have any suggestion, don't forget to mention it in the comments section and if you find this notebook useful, please upvote."},{"cell_type":"code","source":"","metadata":{"collapsed":true},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}