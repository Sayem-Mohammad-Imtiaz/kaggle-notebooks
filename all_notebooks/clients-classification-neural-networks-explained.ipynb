{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Clustering analysis Google Analytics, Full explained with GridSearch, Neural Networks, RandomForest and Logistic Regression models."},{"metadata":{},"cell_type":"markdown","source":"Pleas, any question, doubt or suggestion I will be happy to answer you."},{"metadata":{},"cell_type":"markdown","source":"# ***Objetive:***\n\nWe work as data scientists for a retail company that, due to the change in customer consumption habits, is widely promoting the online sales service. The company wants to run a machine learning model to rank customers based on the likelihood of generating revenue when shopping on the web.\n\nThe goal is to perform a series of specific actions for customers who are most likely to make purchases on the web."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import necesary libraries \nimport pandas as pd # Basic tool for transform and clean Data. \npd.options.display.max_columns = 100 # This option lets us see on the screen 100 columns of informatio\n                                    # that is really usefull for a better visualitation of our data\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.metrics as metrics\nimport warnings\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler\nimport tensorflow as tf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = pd.read_csv('../input/online-shoppers-intention/online_shoppers_intention.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(u'- El número de filas en el dataset es: {}'.format(XY.shape[0]))\nprint(u'- El número de columnas en el dataset es: {}'.format(XY.shape[1]))\nprint(u'- Los nombres de las variables son: {}'.format(list(XY.columns)))\nXY[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY.describe(include = 'bool')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we coud see that we need a binary cluster Ml model to indetify what kind of user will buy in the online stores. So, we will start  \n# <center> **Preprocessing Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We just have few NAN in our dataset so we'll just delete them.  \nXY.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = XY.dropna()\nXY.isnull().sum()\n#Now we dont have nan.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <center> Our next important step is analyze column by column, looking for stranger values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for describe every column:\n\ndef describe_column(df, col):\n    print(f'Columna: {col}  -  Tipo de datos: {df[col].dtype}')\n    print(f'Número de valores nulos: {df[col].isnull().sum()}  -  Número de valores distintos: {df[col].nunique()}')\n    print('Valores más frecuentes:')\n    for i, v in df[col].value_counts().iloc[:20].items() :\n        print(i, '\\t', v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Administrative')\n#All looks good.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Administrative_Duration')\n# Since this columns tell us the time spent on certain type of page, a -1 value\n# its a strange value, then we can infer that '-1' represent nan values also.  \n# we will take a look.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"menos_1 = XY['Administrative_Duration'] == -1\nmenos_uno = XY[menos_1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"menos_uno.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All this '-1' looks like wrong values so we'll drop them. We can do it without problems, because there are just few values and looks like there are conected with other -1 values in the nexts columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"XY = XY.drop(XY[XY['Administrative_Duration']==-1].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now our columns looks better. \ndescribe_column(XY, 'Administrative_Duration')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Informational') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Informational_Duration') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'ProductRelated') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'ProductRelated_Duration') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'BounceRates') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'ExitRates') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'PageValues') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'SpecialDay') #All perfect. Next one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Month') #We have two months left. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'OperatingSystems') #Ok","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Browser') #Ok","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Region') #Ok","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'TrafficType') #Ok","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'VisitorType') #This 'other' values looks suspicius. But ill keep\n#them for the moment. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Weekend')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_column(XY, 'Revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For graphs our variables, the first step is transform the categorical values to numeric."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the booleans object to numeric for our future ML or DL model and graphs\nXY['Weekend'] = XY['Weekend'].map({False:0, True:1})\nXY['Revenue'] = XY['Revenue'].map({False:0, True:1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For take a desicion about wich method we'll use, first we need to know how many variables have 'Month' and 'VisitorType' colums."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 categories (Months)\nXY.Month.unique() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just 3 categories\nXY.VisitorType.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since, we just have 13 categories, use the dummies functions its a good options for our dataframe. In case of have so much more categories values probably we should choose another option.\nMainly because the Dummies function adds X amount of columns to our dataframe based on how many categories our base columns have."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_D = pd.get_dummies(XY,columns=['Month','VisitorType'],drop_first=True)\nX_D[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_numeric = XY.drop(columns=['Month', 'VisitorType', 'Revenue'])\n# We create this DF witouth categories columns for plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <center> We finish, now we'll separete our target Y from the features X "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_D.drop('Revenue', axis=1)\ny = X_D['Revenue']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <center> GRAPHS"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=X)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representación de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the variables are concentrated on the lower part of the graph, with the expeption of 'ProductRelated_Duration'. That is a really good reason for standarize the Data before to train our ML or DL model.\n\n**its Important** also, that big outlier in 'ProductRelated_Duration', but taking in consideretion that this variable is the time spent inside of the specefic shoping page probably is not a wrong value. "},{"metadata":{},"cell_type":"markdown","source":"<center> <h1> Outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect the outliers on ProductRelated_Duration, which, according to our graphs,\n# was the only  column with possible outliers\n\nq = X[\"ProductRelated_Duration\"].quantile(0.99)\n\n# and then filter with:\n\ntime_spent=X[X[\"ProductRelated_Duration\"] > q]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_spent_in_seconds = time_spent['ProductRelated_Duration'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_spent_in_hours = time_spent_in_seconds/3600 # This columns its in seconds and here we transform it\n# in hours for better analysis.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(time_spent_in_hours, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we coud see that the mean of spent time in the shop page for the 0.01 higher quantile of 'ProductRelated_Duration' columns is 3.59 hours. That looks really normal. \n\n¿Who hasnt spent 3 hours choosing the best product?\n\n**The otliers will remain in our dataframe**"},{"metadata":{},"cell_type":"markdown","source":"<center><h1> Graphs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions that we'll use. Its not really necessary at this point that you understand every line of\n# code.\n\ndef relaciones_vs_target(X, Y, return_type='axes'):\n    '''\n    Función que representa gráficos de dispersión de las variables\n    en X en función a la variable Y\n    '''\n    fig_tot = (len(X_numeric.columns))\n    fig_por_fila = 4.\n    tamanio_fig = 4.\n    num_filas = int( np.ceil(fig_tot/fig_por_fila) )    \n    plt.figure( figsize=( fig_por_fila*tamanio_fig+5, num_filas*tamanio_fig+5 ) )\n    c = 0 \n    for i, col in enumerate(X.columns):\n        plt.subplot(num_filas, fig_por_fila, i+1)\n        sns.scatterplot(x=X_numeric[col], y=Y)\n        plt.title( '%s vs %s' % (col, 'target') )\n        plt.ylabel('Target')\n        plt.xlabel(col)\n    plt.show()\n\ndef represento_doble_hist(x_1, x_0, n_bins=11, title='', label_1='Clase 1', \n                          label_0='Clase 0', density=0):\n    '''\n    Función que recibe dos distribuciones de probabilidad y las representa\n    en el mismo gráfico\n    '''\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='red')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='green')\n    plt.title(title)\n    plt.legend(loc='best') \n\ndef hist_pos_neg_feat(x, y, density=0, nbins=11, targets=(0,1)):\n    '''\n    Representa las variables en x divididas en dos distribuciones\n    según su valor de y sea 1 o 0\n    '''\n    fig_tot = len(x.columns)\n    fig_tot_fila = 3.; fig_tamanio = 5.\n    num_filas = int( np.ceil(fig_tot/fig_tot_fila) )\n    plt.figure( figsize=( fig_tot_fila*fig_tamanio+2, num_filas*fig_tamanio+2 ) )\n    target_neg, target_pos = targets\n    for i, feat in enumerate(x.columns):\n        plt.subplot(num_filas, fig_tot_fila, i+1);\n        plt.title('%s' % feat)\n        idx_pos = y == target_pos\n        idx_neg= y == target_neg\n        represento_doble_hist(x[feat][idx_pos].values, x[feat][idx_neg].values, nbins, \n                   density = density, title=('%s' % feat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_pos_neg_feat(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to these graphs, we can see that customers who complete online purchases are evenly distributed within our data. Only with a visual analysis can not obtain new value data to contribute to the Marketing department."},{"metadata":{"trusted":true},"cell_type":"code","source":"matriz_correlaciones = X_numeric.corr(method='pearson')\nn_ticks = len(X_numeric.columns)\nplt.figure( figsize=(9, 9) )\nplt.xticks(range(n_ticks), X_numeric.columns, rotation='vertical')\nplt.yticks(range(n_ticks), X_numeric.columns)\nplt.colorbar(plt.imshow(matriz_correlaciones, interpolation='nearest', \n                            vmin=-1., vmax=1., \n                            cmap=plt.get_cmap('Blues')))\n_ = plt.title('Matriz de correlaciones de Pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation with Revenue\ndata_corr = XY.corr()['Revenue'] \nsns.barplot(data_corr[0:-1].index,data_corr[0:-1].values).set_title('Correlation with the Revenue')\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Resultantly, Page Values has the highest correlation( around 0.5) with Revenue compare to all other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see the Ratio of Revenue in each types\n\nplt.style.use('fivethirtyeight')\nfig,ax = plt.subplots(nrows = 2, ncols = 4,figsize = (17,10))\nfig.tight_layout(pad = 3)\n\n\nadm_rev = XY[['Administrative','Revenue']]\nrev_p1 = pd.DataFrame(XY.groupby('Revenue')['Administrative'].sum()).T\nrev_p1.plot.bar(stacked=True,ax=ax[0,0])\nax[0,0].set_xticklabels(['Administrative'], rotation=360)\nplt.legend(loc='best')\n\n\ninfo_rev = XY[['Informational','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['Informational'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,1],color = ['black','white'])\nax[0,1].set_xticklabels(['Informational'], rotation=360)\nplt.legend(loc='best')\n\ninfo_rev = XY[['ProductRelated','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['ProductRelated'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,2],color = ['purple','red'])\nax[0,2].set_xticklabels(['ProductRelated'], rotation=360)\nplt.legend(loc='best')\n\n\n\ninfo_rev = XY[['OperatingSystems','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['OperatingSystems'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,3],color = ['green','yellow'])\nax[0,3].set_xticklabels(['OperatingSystems'], rotation=360)\n\n\ninfo_rev = XY[['Browser','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['Browser'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,0],color = ['navy','orange'])\nax[1,0].set_xticklabels(['Browser'], rotation=360)\n\n\ninfo_rev = XY[['Region','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['Region'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,1],color = ['black','red'])\nax[1,1].set_xticklabels(['Region'], rotation=360)\n\n\ninfo_rev = XY[['TrafficType','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['TrafficType'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,2],color = ['blue','green'])\nax[1,2].set_xticklabels(['TrafficType'], rotation=360)\n\nfig.delaxes(ax[1,3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns :'Administrative', 'Informational' and 'ProductRelated' there are the most related with the revenue results. We have to keept that in mind. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#All perfect. Continue. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><h1> Standardization of data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_escalar = StandardScaler()\nX_estandarizado = obj_escalar.fit_transform(X_D)\n#Al estandarizar los datos mi regresion logistica me entraga un ROC de 1.","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"X_estandarizado","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, we divide our df in train and test for start applyng models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><H3> Now, We'll train 3 differents ML model looking for the best one "},{"metadata":{},"cell_type":"markdown","source":"<center><H2> Logistic Regresion"},{"metadata":{},"cell_type":"markdown","source":"GridSearchCV its a extremly useful code for ours ML model. Its lets us find the best combination of parameters. We shoud use it always"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo1 = LogisticRegression()\nparametros = {\"C\": [0.001, 0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06], \n              \"class_weight\":['balanced', None]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo_gs = GridSearchCV(modelo1, param_grid=parametros,\n                         cv = 5, scoring='roc_auc')\nmodelo_gs.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(modelo_gs.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs.best_score_,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.88 ROC AUC its a really nice result. But we must continue looking for other models. \nImportant that in the code cell above, 'best_params_ give us the result of the best parameters for use finally in our model for this Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_search = pd.DataFrame.from_dict(modelo_gs.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('C')\nplt.ylabel('roc_auc')\n_ = plt.plot( df_search['param_C'], df_search['mean_test_score'], '.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analizando el modelo con el mejor alpha\nEn este paso nos quedamos con los mejores parámetros obtenidos en el paso anterior:"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_log =  LogisticRegression(C=modelo_gs.best_params_['C'],\n                              class_weight=modelo_gs.best_params_['class_weight'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we train our model with the best parameters that GridSearch give us."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_log.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, is when the Test Set comes into play. When you want to validate an already chosen and optimized model.\nWith that model optimized, I predict tests to see how it behaves on data that you haven't seen before"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_prob = reg_log.predict_proba(X_test)\ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This figure represents in green the probabilities assigned by the model to data that are 0s (the closer the green distribution is to 0, the better) and in red the probabilities assigned to the data that are 1s (the closer the red distribution is to 1 best).\n\nEsta figura representa en verde las probabilidades que asigna el modelo a los datos que son 0s (cuanto más cerca de 0 la distribución verde mejor) y en rojo las probabilidades asignadas a los datos que son 1s (cuanto más cerca esté de 1 la distribución roja mejor)."},{"metadata":{"trusted":true},"cell_type":"code","source":"represento_doble_hist(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to choose the umbral for separate our results which are continuous values into a binary option of Yes or Not (1 or 0) about the revenue probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"umbral = 0.6\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(u\"Matriz de confusión\\n\", metrics.confusion_matrix(Y_test, y_umbralizadas))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_umbralizadas),2)))  \nprint(\"Sensitividad\\t{}\".format(round(metrics.recall_score(Y_test, y_umbralizadas),2)))\nprint(u\"Precisión\\t{}\".format(round(metrics.precision_score(Y_test, y_umbralizadas),2)))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"umbral = 0.7\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(u\"Matriz de confusión\\n\", metrics.confusion_matrix(Y_test, y_umbralizadas))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_umbralizadas),2)))  \nprint(\"Sensitividad\\t{}\".format(round(metrics.recall_score(Y_test, y_umbralizadas),2)))\nprint(u\"Precisión\\t{}\".format(round(metrics.precision_score(Y_test, y_umbralizadas),2)))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We finally keep in 0.7 the threshold because it give us best performance. "},{"metadata":{},"cell_type":"markdown","source":"<center><h1> Neural Network Clasification"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo = MLPClassifier()\nparametros = {'solver': ['lbfgs'], \n              'max_iter': [300,500, 800, 1000], # Iteraciones máximas en cada red\n              'alpha': 10.0 ** -np.arange(0.5, 2), # Parámetro de regularización L2 para evitar sobreajuste\n              'hidden_layer_sizes':np.arange(10, 35), # Número de neuronas en cada capa\n              'random_state':[0]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo_gs2 = GridSearchCV(modelo, param_grid=parametros, cv = 3, \n                         scoring='roc_auc', n_jobs=-1, verbose=10)\nmodelo_gs2.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In this case for GridSearch the Max Iter was 500, and we obtein the result that this param was the\n#Best, so now ill adjust GridSearch for testing with Higher Max Iter and looks if our model improve.\nprint(modelo_gs2.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs2.best_score_,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How we coud see, the ROC AUC improve using the max Iter again that i gave. First ill check with the\n# test DF for look if we also get the same improve or in the worst case we get and overfit. \nprint(modelo_gs2.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs2.best_score_,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best parameters"},{"metadata":{},"cell_type":"markdown","source":"As in the other models, we obtain the best parameters found and fit a model with those parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mejor_modelo = MLPClassifier(**modelo_gs2.best_params_, verbose=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mejor_modelo.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_prob = mejor_modelo.predict_proba(X_test) \ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"represento_doble_hist(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"umbral = 0.5\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(u\"Matriz de confusión\\n\", metrics.confusion_matrix(Y_test, y_umbralizadas))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_umbralizadas),2)))  \nprint(\"Sensitividad\\t{}\".format(round(metrics.recall_score(Y_test, y_umbralizadas),2)))\nprint(u\"Precisión\\t{}\".format(round(metrics.precision_score(Y_test, y_umbralizadas),2)))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modifing the Maxiter from 500 to 1000, improve the model but only on the train df. it keept\n# 0.88 accurary. Probably we coud tried with a higher number of itermax.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><h1> RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfclf = RandomForestClassifier(n_estimators = 30,max_depth = 10,random_state = 101)\n\n\nrfclf.fit(X_train,Y_train)\npred = rfclf.predict(X_test)\nprint(classification_report(Y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets Optimize the Random Forest Classifier using GridSearch\nparam_grid = {\n    'n_estimators' : [80,100],\n    'max_depth' : [10,15],\n    'min_samples_leaf' : [2,3],\n    'min_samples_split': [2,4]\n}\n\ngridsearch = GridSearchCV(estimator=rfclf,param_grid=param_grid,verbose = 1)\ngridsearch.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfclf = RandomForestClassifier(n_estimators = 100,max_depth = 10,min_samples_leaf = 3, min_samples_split = 2,random_state = 101)\nrfclf.fit(X_train,Y_train)\npred = rfclf.predict(X_test)\nprint(classification_report(Y_test,pred))\n\n# 0 is False, 1 is True, the precision of detecting True has increased\nfrom sklearn.metrics import accuracy_score\nrfacc = accuracy_score(Y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we have developed the 3 ML models to predict whether a visitor to the e-commerce page will make a purchase or not. But as far as we have seen so far we have the big problem that these results are not easily transformed into value for the marketing department or someone else within the company.\n\nThis is due to the \"Black Box\" characteristic of the vast majority of the models used, which translates into not being able to extract important insights from the application of Machine Learning models.\n\nIt is because of the above that the \"Permutation Importance\" technique will finally be used to determine which of all our features are the ones that provide the closest approximation to determining whether the purchase will be made or not.\n\n-----------\n\nFinalmente hemos desarrollado los 3 modelos de ML para predecir si un visitante a la pagina de comercio electrónico realizara una compra o no. Pero hasta donde hemos visto hasta ahora nos surge el gran problema de que estos resultados no son facilmente transformables en valor para el departamento de marketing o algun otro dentro de la compañia. \n\nEsto es debido a la caracteristica de \"Caja Negra\" de la gran mayoria de los modelos utilizados, lo que se traduce en no poder extraer importantes insigths desde la aplicacion de modelos de Machine Learning.\n\nEs en razon a lo anterior que se utilziara finalmente la tecnica de \"Permutation Importance\" para determinar cual de todas nuestras features son las que entregan mayor aproximacion a determinar si se realizará la compra o no. "},{"metadata":{},"cell_type":"markdown","source":"<h1><center> \"Permutation Importance\""},{"metadata":{},"cell_type":"markdown","source":"### Looking for the explanation of ours models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfclf, random_state=1).fit(X_test, Y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><h1> Shap method"},{"metadata":{},"cell_type":"markdown","source":"We see the same results again regarding which features are more important\n____\nVolvemos a ver los mismo resultados respecto a que featurees son mas importantes"},{"metadata":{},"cell_type":"markdown","source":"From this application of 'PermutationImportance' to two of our models 'PageValues' appears to us as the main feature of importance when defining who will or will not make the purchase. Faced with this, our next step, in order to generate insights and transform our analysis into money for the company, is to thoroughly analyze that column and how it behaves.\nThis could only be done by means of very detailed and advanced statistical graphics, which is not objective in this work, but it is undoubtedly what would make the difference between a successful case or just one more Machine Learning model.\n___\nDesde esta aplicacion de 'PermutationImportance' a dos de nuestros modelos 'PageValues' nos aparece como la principal feature de importancia al momento de definir quien realizará o no la compra. Frente a esto nuestro siguiente paso, de vista a generar insights y tranformar nuestro analisis en dinero para la empresa es analisar a fondo aquella columna y como se comporta. \nEsto solo se podria realizar mediantes graficas estadisticas muy detalladas y avanzadas, lo cual no es objetivo en el presente trabajo, pero sin duda es lo que marcaría la diferencia entre un caso de exito o solo un modelo más de Machine Learning."},{"metadata":{},"cell_type":"markdown","source":"<center><h1> Manual Insigths\n   "},{"metadata":{"trusted":true},"cell_type":"code","source":"compradores_si = XY['Revenue'] == 1\ncompradores = XY[compradores_si]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compradores_no = XY['Revenue'] == 0\nno_compradores = XY[compradores_no]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,k in zip(compradores.mean(),no_compradores.mean()):\n    print (i,k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(compradores.mean(), ), print(no_compradores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><h1> Conclusion"},{"metadata":{},"cell_type":"markdown","source":"From my analysis, customers who are interested on buying the products :\nTend to stay longer on the website especially when they are on the website which is productrelated. Less likely to bounce or exit\n\nAll of the aforementioned would not charge an important value for the company if this model is not put into production in the cloud to analyze the data of potential buyers in real time and in this way direct marketing strategies to all those people who they are more likely to make a purchase.\n___\nFinalmente, como todo lo obtenido mediante el presente analisis, se puede corroborar las caracteristicas principales de los compradores quienes en promedio gastan mas tiempo visitando las paginas web, como tambien el numero de veces que las visitan, al mismo tiempo que los promedios de BounceRate y ExitRates son menores para estos grupos.\n\nTodo lo anteriomente expuesto no cobraría un valor importante para la compañia si es que este modelo no se pone en produccion en la nube para analizar en tiempo real los datos de los posibles compradores y de esta forma direccionar las estrategias de marketing a todos aquellas personas que se encuentran mas propensas a realizar una compra. "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_plot= XY.drop(columns= ['OperatingSystems', 'Browser', 'Region', 'TrafficType'])\nfinal_plot.groupby('Revenue').mean().plot(kind='bar', figsize=(15,7))\nplt.title('Gasto medio por producto en cada clúster')\nplt.xlabel(u'Número de clúster')\n_ = plt.ylabel('Valor medio de gasto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}