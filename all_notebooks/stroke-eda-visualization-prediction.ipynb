{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I do:\n   - Visualize the effect of variables on the stroke\n   - Building the models to predict a stroke disease given the predictors\n\nThe main problem of this dataset is that it's highly imbalanced in target class (stroke). But the methods like SMOTE and adjucting the decision threshold can help us deal with this problem."},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import the fundamental modules."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set()\n%matplotlib inline\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv').drop(['id'], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Explore Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new columns for visualization\ndata['Stroke?'] = data['stroke']==1\ndata['Hypertension?'] = data['hypertension']==1\ndata['Heart Disease'] = data['heart_disease']==1\n\n# Declare size of figures\nmy_size = {'width':800, 'height':500}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1) Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, x='age',\n                   nbins=20, \n                   title='Age distribution', \n                   color_discrete_sequence=px.colors.qualitative.Antique,\n                   marginal='box', \n                   color='Stroke?',\n                   **my_size,)\n\nfig.update_layout(bargap=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clearly see that most of the people that have a stroke are elderly."},{"metadata":{},"cell_type":"markdown","source":"## 1.2) Disease record : Hypertension, heart disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.pivot_table(\n            data,\n            values = ['stroke'],\n            index = ['hypertension'],\n            columns = ['heart_disease'],\n            aggfunc = {'stroke':['count','mean']}\n        )\n\ntemp.columns = temp.columns.set_levels(['No', 'Yes'], level=2)\ntemp.index = pd.Index(['No','Yes'], name='Hypertension')\n\ntemp.style.set_properties(**{'background-color': 'khaki','border-color': 'white'},subset=[('stroke','mean','No'),('stroke','mean','Yes')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.imshow(\n    temp.loc[:,('stroke','mean')],\n    labels = dict(color='Stroke'),\n    title = 'Stroke probabilities',\n    color_continuous_scale = px.colors.sequential.Redor,\n    **my_size\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that people that have ever had both heart disease and hypertension are most likely to have a stroke. On the other hand, people that never have those diseases tend to not having a stroke too. "},{"metadata":{},"cell_type":"markdown","source":"## 1.3) Personal information"},{"metadata":{},"cell_type":"markdown","source":"In this section, we'll look into the effect of married status, working type, and residence type on a stroke."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_quick_report(feature):\n    temp = pd.pivot_table(\n                    data,\n                    values = 'stroke',\n                    index = feature,\n                    aggfunc = ['sum','count','mean']\n                )\n    temp.columns = pd.MultiIndex.from_arrays([['Stroke','Stroke','Stroke'],['sum','count','mean']])\n    \n    return temp\n\n    \ntemp_married = get_quick_report('ever_married')\ntemp_work = get_quick_report('work_type')\ntemp_residence = get_quick_report('Residence_type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_to_many(index):\n    out = []\n    for i in index.values:\n        out.append((index.name, i))\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.concat([temp_married, temp_work, temp_residence], axis=0)\n\narr = one_to_many(temp_married.index) + one_to_many(temp_work.index) + one_to_many(temp_residence.index)\n\ntemp.index = pd.MultiIndex.from_tuples(arr)\ntemp.style.background_gradient(sns.light_palette('darkorange',as_cmap=True), subset=[('Stroke','mean')])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4) Health information"},{"metadata":{},"cell_type":"markdown","source":"In this section, we'll look into the effect of smoke level, BMI, ,gender, and Glucose level on a stroke."},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    shared_yaxes =True,\n    rows=1, cols=2,\n    horizontal_spacing = 0.02,\n    subplot_titles = (\"Average Glucose level\", \"Body mass index\")\n)\n\nfor i in [0,1]:\n    if i == 0:\n        name = 'No'\n        color = 'rgb(217,175,107)'\n        group = 'g_No'\n    else:\n        name = 'Yes'\n        color = 'rgb(204,80,62)'\n        group = 'g_Yes'\n        \n    fig.add_trace(\n        go.Histogram(\n            x = data[data['stroke']==i]['avg_glucose_level'],\n            nbinsx  = 50,\n            legendgroup = group,\n            name = name,\n            marker = dict(color=color),\n            showlegend = False\n        ),\n        row=1, col=1,\n    )\n    \n    fig.add_trace(\n        go.Histogram(\n            x = data[data['stroke']==i]['bmi'],\n            nbinsx  = 50,\n            legendgroup = group,\n            name = name,\n            marker = dict(color=color)\n        ),\n        row=1, col=2\n    )\n\nfig.update_layout(barmode='overlay', bargap=0)\nfig.update_xaxes(row=1, col=1, title_text='Glucose level')\nfig.update_xaxes(row=1, col=2, title_text='BMI')\nfig.update_yaxes(row=1, col=1, title_text='count')\nfig.update_layout(legend_title_text='Stroke')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't see any clear relation between Glucose level, BMI to stroke. It seems like people can have a stroke at every level of Glucose and BMI."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = data.groupby(by='smoking_status')['Stroke?'].agg('mean')*100\n\nfig = make_subplots(\n    subplot_titles = [\"Smoke and stroke\"],\n    specs=[[{\"secondary_y\": True}]]\n)\n\nfor i in [0,1]:\n    \n    if i == 0:\n        name = 'No'\n        color = 'rgb(217,175,107)'\n    else:\n        name = 'Yes'\n        color = 'rgb(204,80,62)'\n        \n    \n    fig.add_trace(\n        go.Histogram(x=data[data['stroke']==i]['smoking_status'], \n                     name=name, \n                     marker = dict(color=color)),\n        secondary_y=False,\n    )\n\nfig.add_trace(\n    go.Scatter(x=temp.index, \n               y=temp.values, \n               name=\"Average\", \n               mode='markers', \n               marker=dict(size=20, color='royalblue')),\n    secondary_y=True,\n)\n\nfig.update_layout(legend_title_text='Stroke', **my_size)\nfig.update_yaxes(title_text='count', secondary_y=False)\nfig.update_yaxes(title_text='% stroke', secondary_y=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that people that are ever smoke(both formerly and presently) have a relatively high chance to have a stroke."},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(\n    data[data['gender']!='Other'],  # Because it has only 1 observation\n    x = 'gender',\n    color = 'Stroke?',\n    barmode = 'group',\n    color_discrete_sequence = px.colors.qualitative.Antique,\n    title = 'Gender and stroke',\n    **my_size\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Preprocess the data"},{"metadata":{},"cell_type":"markdown","source":"## - Missing values, Standardize, Encoding <br>\nLet's import the dataset again."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv').drop(['id'], axis=1)\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=data, x='stroke')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the dataset is very imbalanced. I'll do my best to deal with it later. <br>\nBut first let's see how many missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There're only missing values in bmi column. I'll fill the mean to them. <br>\nNext step, I'll do preprocessing the data and fitting it to models. Let's import the relevant classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost\n\n# Evaluation\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll rearrange the columns so that we can easily track the index of columns for ColumnTransformer in next step."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['stroke'],axis=1)\nY = data['stroke']\n\nX_category = X.select_dtypes(include='object')\nX_numeric = X.select_dtypes(exclude='object')\n\nX = pd.concat([X_category, X_numeric], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll use sklearn.pipeline.Pipeline to sequentially transform the numerical columns by imputting followed by scaling. Then, pass this pipeline along with OneHotEncoder to ColumnsTransformer to do the Preprocessing stuff. \n\nOf course, we have to split the data into train set and test set. Then we fit the ColumnsTransformer to the train set and transform it to both of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the preprocessing pipeline\nimp_std = Pipeline(\n    steps=[\n        ('impute', SimpleImputer(strategy='median')),\n        ('scale', StandardScaler()),\n    ]\n)\n\nct = ColumnTransformer(\n    remainder='passthrough',\n    transformers = [\n        (\"Encoding\",OneHotEncoder(),[0,1,2,3,4]),\n        (\"Scaler\", imp_std,[5,6,7,8,9])\n    ]\n)\n\n\n# Split the data\nX_train_idle, X_test_idle, y_train, y_test = train_test_split(X, Y, \n                                                              test_size=0.2, \n                                                              stratify=Y)\n\n# Fit our transformers to train set\nct.fit(X_train_idle)\n\n# Transform both train and test set\nX_train = ct.transform(X_train_idle)\nX_test = ct.transform(X_test_idle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the highly imbalance of this dataset, at my first run, the models perform very well in predicting major class (0: not having stroke) but very poorly for minor class. So, I'll try applying SMOTE to oversample the dataset in hope that the models can learn more efficiently."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nX_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Building models"},{"metadata":{},"cell_type":"markdown","source":"Building models with their default parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = dict()\nmodels['Dicision Tree'] = DecisionTreeClassifier(class_weight={0:1,1:2})\nmodels['Random Forest'] = RandomForestClassifier(class_weight={0:1,1:2})\nmodels['Logreg'] = LogisticRegression()\nmodels['GradientBoost'] = GradientBoostingClassifier()\nmodels['AdaBoost'] = AdaBoostClassifier()\nmodels['XGBoost'] = xgboost.XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the models to the resampled train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    models[model].fit(X_train_resampled, y_train_resampled)\n    print(model + ' : fit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See the performance on train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train set prediction\")\nfor x in models:\n        \n    print('------------------------'+x+'------------------------')\n    model = models[x]\n    y_train_pred = model.predict(X_train_resampled)\n    arg_train = {'y_true':y_train_resampled, 'y_pred':y_train_pred}\n    print(confusion_matrix(**arg_train))\n    print(classification_report(**arg_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance on train set is (too) good. That's because we use SMOTE. It makes model learn very well because of having a perfect balance dataset. <br>\nNext, see the performance in test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test set prediction\")\nfor x in models:\n        \n    print('------------------------'+x+'------------------------')\n    model = models[x]\n    y_test_pred = model.predict(X_test)\n    arg_test = {'y_true':y_test, 'y_pred':y_test_pred}\n    print(confusion_matrix(**arg_test))\n    print(classification_report(**arg_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metric I give more interest is **\"Recall\"** rather than accuracy because I don't want the situation like the following: <br>\n    - \"A person is very likely to have a stroke but the model tells he/she doesn't\"\n\nWhich is a very bad situation. The model will tell us like that when it has low recall (high False Negative rate). <br>\nThe True Negative situation (model tells that this a person will have a stroke but he/she actually doesn't) is not that bad compared to the first one. In the second case, a person will have to take a good care of his health."},{"metadata":{},"cell_type":"markdown","source":"Inspecting from models' classification report, I would say that Logistic regression model has done the best job here. <br>\n**Note:** Furthermore, We can try **tuning models' hyperparameters** to get the better result or **adjusting the probablity threshold** to improve their performance. (*I'll do that in the next update*)"},{"metadata":{},"cell_type":"markdown","source":"Lastly, let's see the roc curve to compare the performance of different models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\nfig, ax = plt.subplots()\nfig.set_size_inches(13,6)\n\nfor m in models:\n    y_pred = models[m].predict_proba(X_test)\n    fpr, tpr, _ = roc_curve(y_test, y_pred[:,1].ravel())\n    plt.plot(fpr,tpr, label=m)\nplt.xlabel('False-Positive rate')\nplt.ylabel('True-Positive rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('roc_auc_score')\nfor i in models:\n    model = models[i]\n    print(i + ' : ',roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]).round(4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}