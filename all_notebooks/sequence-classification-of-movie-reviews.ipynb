{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Sequence Classification of Movie Reviews\n\nSequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the task is to predict a category for the sequence.\n\nProblem with sequences is that\n* They can vary in length\n* Can be comprised of very large vocabulary of input symbols\n* May require model to learn long-term contexts or dependencies between input symbols\n\nIMBD movie reviews dataset has review and it's corresponding sentiment. The dataset has 50,000 such observations. We will use this datasets to build the following models and compare which works best for sentiment classification\n\n1. Model 1 : Baseline Model\n2. Model 2 : LSTM with 100 units\n3. Model 3 : LSTM with Dropout\n4. Model 4 : LSTM with CNNs\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Loading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from the dataset that sentiment label is in textual form. This needs to be converted to numeric. We will use `LabelEncoder` from `sklearn.preprocessing` to encode `positive` and `negative` sentiment to `1` and `0`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['sentiment'] = le.fit_transform(df.sentiment)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Defining hyperparameters for Text Processing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOP_WORDS      = 5000      # to keep only 5000 top used words\nMAX_REVIEW_LEN = 500       # caps the sequence length to this number (Keras requires all sequences to be of same length)\nOOV_TOKEN      = '<OOV>'   # any out of vocabulary word (not part of top words) is replaced with this text\nTRUNC_TYPE     = 'post'\nPADDING_TYPE   = 'post'\nTEST_SIZE      = 0.5\nEMBEDDING_LEN  = 32 \nEPOCHS         = 10\nBATCH_SIZE     = 64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenize\nWe will use `Tokenizer` from Keras API. The `Tokenizer` vectorizes the text corpus by converting text to integers (each integer being the index of a token in a dictionary).\nThe dictionary containing word integer mapping can be accessed from `tokenizer.word_index`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = TOP_WORDS, oov_token = OOV_TOKEN)\ntokenizer.fit_on_texts(df.review.to_numpy())\nword_index = tokenizer.word_index\nword_index_inv = dict([(v,k) for (k,v) in word_index.items()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can compare the tokenized and orignal sentences using the reversed `word_index`. In the sample below, the first 50 words of first review was tokenized and decoded to get the output strings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sentence(text):\n    return ' '.join([word_index_inv.get(i, '?') for i in text])\n\nsample_seq = [' '.join(df.review[0].split(' ')[:50])]\ntokenized_sample = tokenizer.texts_to_sequences(sample_seq)\nprint (sample_seq[0])\nprint ('------------------')\nprint (tokenized_sample[0])\nprint ('------------------')\nprint (decode_sentence(tokenized_sample[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**\n* All words are converted to lowercase\n* Words like *brutality* and *unflinching* do not feature in tokenized sentence and are replaced with our defined Out of Vocabulary token `<OOV>`\n* punctuations are removed - this could be a problem while extracting meanings of sentences. May not be an issue for sentiment classification\n\n### Train Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = df.review.to_numpy()\nlabels  = df.sentiment.to_numpy()\n\ntrain_count      = int(len(reviews) * (1 - TEST_SIZE))\ntraining_reviews = reviews[:train_count]\ntesting_reviews  = reviews[train_count:]\ny_train          = labels[:train_count]\ny_test           = labels[train_count:]\n\nprint ('Training Count :', len(training_reviews))\nprint ('Testing Count :', len(testing_reviews))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting reviews to sequences by fitting tokenizer to these reviews. The resulting sequences will be padded i.e. If the review size is less than `MAX_REVIEW_LEN` parameter, the resulting sequences will get populated with `0` after the sentence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sequences = tokenizer.texts_to_sequences(training_reviews)\nX_train            = pad_sequences(training_sequences, maxlen = MAX_REVIEW_LEN, padding = PADDING_TYPE, truncating = TRUNC_TYPE)\n\ntesting_sequences  = tokenizer.texts_to_sequences(testing_reviews)\nX_test             = pad_sequences(testing_sequences,  maxlen = MAX_REVIEW_LEN, padding = PADDING_TYPE, truncating = TRUNC_TYPE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Model\nWe first define the embedding layer which represents each words with 32 length vectors. Next, we define LSTM layer with 100 memory units. Lastly, we use a Dense layer with `sigmoid` activation to classify the sentiment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalAveragePooling1D\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential() \nmodel.add(Embedding(TOP_WORDS, EMBEDDING_LEN, input_length=MAX_REVIEW_LEN))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(100, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid')) \nmodel.compile(loss = 'binary_crossentropy', optimizer = Adam(0.0005), metrics = ['accuracy']) \nmodel.summary()\n%time history = model.fit(X_train, y_train, validation_data =(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test, batch_size = 128, verbose = 0)\nprint (f'Accuracy : {round(results[1]*100, 2)} %')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTMs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential() \nmodel.add(Embedding(TOP_WORDS, EMBEDDING_LEN, input_length=MAX_REVIEW_LEN))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation = 'sigmoid')) \nmodel.compile(loss = 'binary_crossentropy', optimizer = Adam(0.005), metrics = ['accuracy']) \nmodel.summary()\n%time history = model.fit(X_train, y_train, validation_data =(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test, batch_size = 128, verbose = 0)\nprint (f'Accuracy : {round(results[1]*100, 2)} %')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## LSTM with Dropout\nLSTMs are prone to overfitting, for which we can use Dropout. Dropout can be added \n1. To the embedding input layer\n2. Between embedding and LSTM layer\n3. Between LSTM and Dense layer\n4. To the input and recurrent connections of the memory units with the LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalAveragePooling1D, Dropout\n\nmodel = Sequential() \nmodel.add(Embedding(TOP_WORDS, EMBEDDING_LEN, input_length=MAX_REVIEW_LEN))\nmodel.add(Dropout(0.2)) \nmodel.add(LSTM(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = 'sigmoid')) \nmodel.compile(loss = 'binary_crossentropy', optimizer = Adam(0.005), metrics = ['accuracy']) \nmodel.summary()\n%time history = model.fit(X_train, y_train, validation_data =(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test, batch_size = 128, verbose = 0)\nprint (f'Accuracy : {round(results[1]*100, 2)} %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy becomes better with dropout than without for LSTM model","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## LSTM with CNN\nConvolutional neural networks excel at learning the spatial structure in input data. The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D\n\nmodel = Sequential() \nmodel.add(Embedding(TOP_WORDS, EMBEDDING_LEN, input_length = MAX_REVIEW_LEN))\nmodel.add(Conv1D(32, (3), activation = 'relu')) \nmodel.add(MaxPooling1D(2)) \nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = Adam(0.005), metrics = ['accuracy']) \nmodel.summary()\n%time history = model.fit(X_train, y_train, validation_data =(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test, batch_size = 128, verbose = 0)\nprint (f'Accuracy : {round(results[1]*100, 2)} %')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Next Steps\n\n1. Trying out different LSTM layers\n2. Experiment with different number of `TOP_WORDS`, `MAX_REVIEW_LEN` for input_data\n3. Try `pre` paddings and truncations while creating sequences\n4. Eperimenting with stacked mulitple convolutional and LSTM layers ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}