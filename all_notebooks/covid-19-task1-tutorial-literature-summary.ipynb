{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Task1 Let's make Study Summary Tutorial\n\n1st public version: 16/06/2020\n\nThis notebook is for Task 1 of COVID-19 Challenge.  \nIn this task, we are supposed to make literature summary.  \nI use the techniques of treating json file and web scraping. \n\nThe overview of my process in this notebook is like below:\n1. Get articles related to certain topic by filtering title (In this notebook, I chose 'school closure')\n2. Get general information from metadata.csv\n3. Get more specific information by using web scraping\n4. Fill out some other topics by rule-based algorithm\n\nEach step contains incompleteness, however, this approach works to some extent, I guess.\n\nHere is table of contents:\n- [Data Check and Treating Json Example](#Data-Check-and-Treating-Json-Example)\n- [Data Visualization](# Data-Visualization)\n- [Task 1 Challenge](#Task-1-Challenge)\n    -  [General Information](#General-Information)\n    -  [Web Scraping](#Web-Scraping)\n    -  [Study Type, Factors](#Study-Type,-Factors)\n    -  [Influential](#Influential)\n    -  [Excerpt](#Excerpt)\n- [Acknowledgement](#Acknowledgement)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import time\nimport re\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests    # For web scraping\nfrom bs4 import BeautifulSoup\n\n%matplotlib inline\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_TARGET_DIR = \"/kaggle/input/CORD-19-research-challenge/Kaggle/target_tables/2_relevant_factors/\"\n\nfor file in os.listdir(INPUT_TARGET_DIR):\n    print(file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Check and Treating Json Example","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at target csv example.\ntarget_df = pd.read_csv(INPUT_TARGET_DIR + \"Effectiveness of school distancing.csv\", index_col=0)\ntarget_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"INPUT_JSON_DIR =  \"/kaggle/input/CORD-19-research-challenge/document_parses/pdf_json/\"\njson_file = open(INPUT_JSON_DIR + \"566b5c62fc77292ebe09295d59e7fbf6fc914260.json\", \"r\")\njson_data = json.load(json_file)\njson_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the result above, JSON file is comprised of some dictionaries.  \nYou can find json format in json_schema.txt.  \nIf you want to know the format in the code, you can run the cell below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !cat \"/kaggle/input/CORD-19-research-challenge/json_schema.txt\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can access each object by key like below.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"json_data[\"metadata\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"json_data[\"abstract\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"json_data[\"body_text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"json_data[\"bib_entries\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_df = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\nmetadata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Info of metadata\nmetadata_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"17.url has more non-null objects than 15.pdf_json_files and 16.pmc_json_files.  \nSo in order to collect data, it is better to use scraping techniques than just searching pdf file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In these columns, countplots of source_x and license are seemed to be visualized.  \nOther columns have too many non-unique values to use countplot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source Count Plot\nplt.figure(figsize=(8, 6))\nsns.countplot(metadata_df[\"source_x\"])\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we can easily find that most of sources are combination of some basic sources.  \nWe should decompose these combined sources to plot count more clearly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can get decomposed sources by using split method.\ntemp_df = metadata_df[\"source_x\"].str.split(\";\", expand=True)\ntemp_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncount_dict = {}\n\nfor row in range(len(temp_df)):\n    for col in range(len(temp_df.columns)):\n        key = temp_df.iloc[row, col]\n        if key != None:\n            key = key.lstrip()\n        count_dict.setdefault(key, 0)\n        count_dict[key] += 1\n\ndel count_dict[None]    # We delete key:None \ncount_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_dict_sorted = dict(sorted(count_dict.items(), key=lambda x:x[1], reverse=True))\n\nplt.bar(count_dict_sorted.keys(), count_dict_sorted.values())\nplt.xticks(rotation=90)\nplt.ylabel(\"Count\")\nplt.xlabel(\"Source\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# License Count Plot\nsns.countplot(metadata_df[\"license\"])\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_df[\"year\"] = metadata_df[\"publish_time\"].str[:4].astype(\"float\")\nmetadata_df[\"year\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = metadata_df.groupby(\"year\")[\"cord_uid\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=temp_series.index, y=temp_series.values)\nplt.xticks(rotation=90)\nplt.ylabel(\"Count\")\nplt.title(\"Total Record Count Transition\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can easily find that record hits increase rapidly in 2020.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Task 1 Challenge\nFrom now on, let's move on to task challenge itself!\n\n## School Distancing Literature Summary\nFirst, I extract the articles related to school distancing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We change title to lowercase.\nmetadata_df[\"title\"] = metadata_df[\"title\"].str.lower()\nmetadata_df[\"title\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we extract the records that contains 'school' and 'distancing' keywords in paper title. \n# Machine Learning, especially, Topic Finding from text dataset can be useful in this task.\n# However, let's make it as simple as possible for my first step! \n# Actually, this rule base algorithm can be strong enough to extract the articles related to 'school distancing' from all datasets (about length 140k).\n\nschool_row = []\n\nfor row in range(len(metadata_df)):\n    try:\n        if (\"school\" in metadata_df.loc[row, \"title\"]) & (\"clos\" in metadata_df.loc[row, \"title\"]):\n            school_row.append(row)\n    except: \n        continue\n\nprint(\"We hit {} records when searching 'school' and 'closure'\".format(len(school_row)))\nprint(\"This is {:.2f} % of this dataset\".format(len(school_row) / len(metadata_df) * 100, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the metadata in school_rows\nmetadata_df.loc[school_row, :].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can find that articles regarding school closure can be extracted from its title.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check what we have to fill out for submission\ntarget_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### General Information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission DataaFrame, first we make its format and fill out the content later\nsummary_df = pd.DataFrame(columns = target_df.columns)\n\n# General Info\nsummary_df[\"Date\"] = metadata_df.loc[school_row, \"publish_time\"]\nsummary_df[\"Study\"] = metadata_df.loc[school_row, \"title\"]\nsummary_df[\"Study Link\"] = metadata_df.loc[school_row, \"url\"]\nsummary_df[\"Journal\"] = metadata_df.loc[school_row, \"journal\"]\nsummary_df[\"pdf_json_file\"] = metadata_df.loc[school_row, \"pdf_json_files\"]\nsummary_df[\"pmc_json_file\"] = metadata_df.loc[school_row, \"pmc_json_files\"]\nsummary_df[\"abstract\"] = metadata_df.loc[school_row, \"abstract\"]\nsummary_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to fill out Study Type, Factors, Influential Excerpt, Measure of Evidence and Added on columns.  \nBut for added_on column (i.e. date when the article is added on this Kaggle dataset), I couldn't find where I can get source.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list(summary_df[\"Study Link\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concated_url = 'https://doi.org/10.1186/s12879-017-2934-3; https://www.ncbi.nlm.nih.gov/pubmed/29321005/'\n\nurl_list = concated_url.split('; ')\nfor url in url_list:\n    if 'ncbi' in url:\n        print(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have to deal with errors \n# concated_url = np.nan\n\n# url_list = concated_url.split('; ')\n# for url in url_list:\n#     if 'ncbi' in url:\n#         print(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract only NCBI data because I couldn't check other sites' web scraping policies\nncbi_url = []\n\nfor concated_url in list(summary_df[\"Study Link\"]):\n    isncbi = 0\n    try:\n        url_list = concated_url.split('; ')\n        for url in url_list:\n            if 'ncbi' in url:\n                ncbi_url.append(url)\n                isncbi = 1\n            \n        if isncbi == 0:   \n            ncbi_url.append(np.nan)\n    except AttributeError:\n        ncbi_url.append(np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncbi_url","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this number does match to the number of records\nlen(ncbi_url)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Web Scraping\n\n#### Web Scraping Tutorial\nIn the following cells, the tutorial of web scraping is executed.  \n\n**Please Remind the Followings**\n - In some website, web scraping is prohibited, so we have to take the websites' policies. \n - In this tutorial, I access NCBI website by using scraping.\n   The website declares about web scraping in the policies and disclaimers tab. (https://www.ncbi.nlm.nih.gov/home/about/policies/)\n - It says too many web scraping queries is not allowed, but in this tutorial, we only access the website for a few times. So it seems OK.\n   \n> Guidelines for Scripting Calls to NCBI Servers\nDo not overload NCBI's systems. Users intending to send numerous queries and/or retrieve large numbers of records should comply with the following:\n> - Run retrieval scripts on weekends or between 9 pm and 5 am Eastern Time weekdays for any series of more than 100 requests.  \n> - Send E-utilities requests to https://eutils.ncbi.nlm.nih.gov, not the standard NCBI Web address.  \n> - Make no more than 3 requests every 1 second.  \n> - Use the URL parameter email, and tool for distributed software, so that we can track your project and contact you if there is a problem. For more information, please see the Usage Guidelines and Requirements section in the Entrez Programming Utilities Help Manual.  \n> - NCBI's Disclaimer and Copyright notice must be evident to users of your service. NLM does not claim the copyright on the abstracts in PubMed; however, journal publishers or authors may. NLM provides no legal advice concerning distribution of copyrighted materials, consult your legal counsel.   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"url = list(summary_df[\"Study Link\"])[0] \nr = requests.get(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(url)    # The first article URL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# print(r.headers)      # Header Information\nprint(r.content)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This result is difficult to read, so we use BeautifulSoup Library ","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"soup = BeautifulSoup(r.content, \"html.parser\")\nprint(soup.prettify())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result seems more beautiful with BeautifulSoup","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# You can access each object like below using BeautifulSoup instance\nprint(soup.title)\nprint(soup.title.name)\nprint(soup.title.string)\n\n# You can access 'a' tag  in html like below\nprint(soup.a)    # It just returns the first link.\nprint(soup.find_all('a'))    # Returns all the links","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Study Type, Factors","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"soup.get_text()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'd like to get some important keyword from this whole text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nprint(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color=\"white\", stopwords=ENGLISH_STOP_WORDS)\nwordcloud.generate(soup.body.get_text().replace('\\n','').replace('\\t',''))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.title(\"Show Description WordCloud in the first article\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We extracted the article related to 'school closure' (i.e. the article whose title includes 'school' and 'closur').  \nThus, the thema of this article is 'school closure', and we can see this phrase in the center.  \nOf course, the factor can be concluded as school closure.   \nBut in order to differentiate other articles in this dataframe, we try to extract the 2nd or 3rd keywords.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcount_dict = wordcloud.process_text(soup.body.get_text().replace('\\n','').replace('\\t',''))\n# wordcount_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For tutorial, I picked up ten keywords.\nwordcount_dict_sorted = dict(sorted(wordcount_dict.items(), key=lambda x:x[1], reverse=True))\nresult = {k:wordcount_dict_sorted [k] for k in list(wordcount_dict_sorted )[:10]}\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if \"school\" in result.keys():\n    del result[\"school\"]\nif \"closure\" in result.keys():\n    del result[\"closure\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For submission, I picked up five keywords.\nresult = {k:result[k] for k in list(result)[:5]}\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.loc[1459, \"Factors\"] = ', '.join(list(result.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.loc[1459, \"abstract\"].lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Study Type\n# I defined that study type could be classified into three types. (Modelling, Review or Summary, Others)\n# I created some basic rule regarding classification of these three types.\n# First, if 'modeling' in abstract,we think this article is about modeling.\n# Second, if 'review' or 'summary' in abstract, we think this article is about review or summary.\n# Third, we dive into the article body and search 'model' or 'review, summary' keywords.\n# If we can't find any words, we define it as Others category.\n\nstudy_type = 'Others'\n\nif 'model' in summary_df.loc[1459, \"abstract\"].lower():\n    print(\"This article abstract contains 'model' keywords\")\n    study_type = 'Modeling'\nelif 'review' in summary_df.loc[1459, \"abstract\"].lower() or 'summary' in summary_df.loc[1459, \"abstract\"].lower():\n    print(\"This article abstract contains 'review' or 'summary' keywords\")\n    study_type = 'Review'\n\nelif 'model' in wordcount_dict:\n    print(\"This article body contains 'model' keywords\")\n    study_type = 'Modeling'\n\nelif 'review' in wordcount_dict:\n    print(\"This article body contains 'review' or 'summary' keywords\")\n    study_type = 'Review'\n\n    \nsummary_df.loc[1459, \"Study Type\"] = study_type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Influential\t\n\nWhen you access the first article webpage of NCBI with your browser, you can find a block like below:\n> This article has been cited by other articles in PMC.\n\nExample in the first article case:\n\n>![](http://)<div class=\"fm-panel\"><div>This article has been <a href=\"/pmc/articles/PMC4021091/citedby/\">cited by</a> other articles in PMC.</div></div></div></div>\n\nActually, this is a link to the page which shows other articles that cited this original article.  \nWe can use this information to decide whether the article is influential or not.  \nSpecifically, the number of articles that cited the document can be one effective signal for estimating the study's importance.   \nSo from mow on, I'd like to get the number of times cited of this first document.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cited_url = 'https://www.ncbi.nlm.nih.gov' + soup.find(id=\"pmclinksbox\").find(\"a\").get(\"href\")\ncited_url","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we access this link.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r_cited = requests.get(cited_url)\nsoup_cited = BeautifulSoup(r_cited.content, \"html.parser\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(soup_cited.prettify())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can use .find method to get the target sentence.\ntext = soup_cited.find(\"h2\").get_text()\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We extract the number of cited using regex\nregex = re.compile('\\d+')\n\nnumber_cited = regex.findall(text)\nnumber_cited = int(number_cited[0])\nprint(number_cited)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If this number is larger than five, I regard this article as influential","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"is_influential = 'N'\nif number_cited >= 5:\n    is_influential = 'Y'\n\nsummary_df.loc[1459, \"Influential\"] = is_influential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Excerpt\nThis excerpt is the last two lines of Abstract.  \nThis rule is because conclusions are often written in the latter half of the abstract.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"abstract_text = summary_df.loc[1459, \"abstract\"] \nabstract_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'. '.join(abstract_text.split('. ')[-2:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a good excerpt of this article.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.loc[1459, \"Excerpt\"] = '. '.join(abstract_text.split('. ')[-2:])\nsummary_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorry, I couldn't come up with any idea to fill out Measure of Evidence and Added on column.  \nThen I do this process so far to all records.  \nAnd I only tried this method to pmc articles.  \nSorry for incomplete result. (I don't have time to examine how I should get important features from other websites.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"record_count = 0\n\nfor target_row in summary_df.index[:5]:\n    print(target_row)\n    print(\"Start Web Scraping\")\n    article_url = ncbi_url[record_count]\n    r = requests.get(article_url)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n    \n    # Factors\n    # We extracted the article related to 'school closure' (i.e. the article whose title includes 'school' and 'closur'). \n    # Thus, the thema of this article is 'school closure', and we can see this phrase in the center.  \n    # Of course, the factor can be concluded as school closure.   \n    # But in order to differentiate other articles in this dataframe, we try to extract the 2nd or 3rd keywords.\n    \n    wordcloud = WordCloud(background_color=\"white\", stopwords=ENGLISH_STOP_WORDS)\n    wordcount_dict = wordcloud.process_text(soup.body.get_text().replace('\\n','').replace('\\t',''))\n    \n    wordcount_dict_sorted = dict(sorted(wordcount_dict.items(), key=lambda x:x[1], reverse=True))\n    result = {k:wordcount_dict_sorted [k] for k in list(wordcount_dict_sorted )[:10]}\n\n    if \"school\" in result.keys():\n        del result[\"school\"]\n    if \"closure\" in result.keys():\n        del result[\"closure\"]\n    \n    result = {k:result[k] for k in list(result)[:5]}\n    summary_df.loc[target_row, \"Factors\"] = ', '.join(list(result.keys()))\n    \n    \n    # Study Type\n    # I defined that study type could be classified into three types. (Modelling, Review or Summary, Others)\n    # I created some basic rule regarding classification of these three types.\n    # First, if 'modeling' in abstract,we think this article is about modeling.\n    # Second, if 'review' or 'summary' in abstract, we think this article is about review or summary.\n    # Third, we dive into the article body and search 'model' or 'review, summary' keywords.\n    # If we can't find any words, we define it as Others category.\n\n    study_type = 'Others'\n\n    if 'model' in wordcount_dict:\n        print(\"This article body contains 'model' keywords\")\n        study_type = 'Modeling'\n\n    elif 'review' in wordcount_dict:\n        print(\"This article body contains 'review' or 'summary' keywords\")\n        study_type = 'Review'\n    \n    try:\n        if 'model' in summary_df.loc[target_row, \"abstract\"].lower():\n            print(\"This article abstract contains 'model' keywords\")\n            study_type = 'Modeling'\n        elif 'review' in summary_df.loc[target_row, \"abstract\"].lower() or 'summary' in summary_df.loc[target_row, \"abstract\"].lower():\n            print(\"This article abstract contains 'review' or 'summary' keywords\")\n            study_type = 'Review'\n    except AttributeError:\n        pass\n\n\n\n    summary_df.loc[target_row, \"Study Type\"] = study_type\n    \n    # Influential\n    # Actually, this is a link to the page which shows other articles that cited this original article.  \n    # We can use this information to decide whether the article is influential or not.  \n    # Specifically, the number of articles that cited the document can be one effective signal for estimating the study's importance.   \n    # So from mow on, I'd like to get the number of times cited of this first document.\n    \n    try:\n        cited_url = 'https://www.ncbi.nlm.nih.gov' + soup.find(id=\"pmclinksbox\").find(\"a\").get(\"href\")\n        r_cited = requests.get(cited_url)\n        soup_cited = BeautifulSoup(r_cited.content, \"html.parser\")\n\n        text = soup_cited.find(\"h2\").get_text()\n        regex = re.compile('\\d+')\n\n        number_cited = regex.findall(text)\n        number_cited = int(number_cited[0])\n        print(\"number_cited: \" + str(number_cited))\n        \n    except:\n        number_cited = 0\n        pass\n    \n    is_influential = 'N'\n    if number_cited >= 5:\n        is_influential = 'Y'\n\n    summary_df.loc[target_row, \"Influential\"] = is_influential\n    \n    # Excerpt\n    # This excerpt is the last two lines of Abstract.  \n    # This rule is because conclusions are often written in the latter half of the abstract.  \n    \n    try:\n        abstract_text = summary_df.loc[target_row, \"abstract\"] \n        summary_df.loc[target_row, \"Excerpt\"] = '. '.join(abstract_text.split('. ')[-2:])\n        \n    except AttributeError:\n        pass\n    \n    # For not accessing website so many times in a short time.\n    time.sleep(60)\n    \n    record_count += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete unnecessary cols\nsummary_df = summary_df.drop(columns=[\"pdf_json_file\", \"pmc_json_file\", \"abstract\"])\nsummary_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.to_csv(\"Effectiveness of school distancing.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgement\n\nI think this notebook is incomplete one, but I think this works for your tutorial of this task 1.  \nI hope this tutorial is somewhat useful for beginners who have difficulty getting started.  \nHowever, I need more time to complete this task...\n\nIf you like it, I appreciate all of your comments and upvotes, thank you for reading my notebook!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}