{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport spacy\n!python -m spacy download en_core_web_lg\nnlp = spacy.load('en_core_web_lg')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#help(pd.read_csv)\nraw = pd.read_csv(\"/kaggle/input/hasoc-2019/english_dataset.tsv\", sep='\\t')\nraw.head()\ndata = raw[{\"text\", \"task_1\", \"task_2\", \"task_3\"}]\ndata.head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy info\n!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\ntr = SentenceTransformer('paraphrase-distilroberta-base-v1')\ntr.encode(\"hello there\").shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the strings to word vecs\n\n# old def (avg of word embeddings)\ndef to_wordvec(string):\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(string).vector])\n        return vectors\n\n# new sentence embeddings\ndef to_wordvec(string):\n    vectors = tr.encode(string)\n    return [vectors]\n\n#data[\"content\"]=data[\"content\"].apply(to_wordvec)\nvecsSeries=tr.encode(data.text.values)\ndata.to_csv(\"text_hate_vecs.csv\");\ndata.head()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the dataframe to numpy X and y\n#data[\"content\"][1]\n#data[\"content\"].shape\n#data[\"content\"].to_numpy().shape\ntype(vecsSeries)\n\nX = vecsSeries\n#X = np.squeeze(np.stack(data.content.values))\ny = data.task_1.values\nX.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split training an test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.1, random_state=1)\n\ntype(X_train[0])\nX_train.shape\n#X_train\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Create the model\nmodel = LogisticRegression(random_state=1, max_iter=1000)\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Uncomment and run to see model accuracy\nprint(f'Model test accuracy: {model.score(X_test, y_test)*100:.3f}%')\n\ndef decode_class(Y):\n    return Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(decode_class(model.predict(to_wordvec(\"This is the end\"))))\nprint(decode_class(model.predict(to_wordvec(\"I love Donald Trump\"))))\nprint(decode_class(model.predict(to_wordvec(\"The Playstations are blocking the Suez Canal\"))))\nprint(decode_class(model.predict(to_wordvec(\"I am scared of this\"))))\nprint(decode_class(model.predict(to_wordvec(\"Yo listen up, here's a story\"))))\nprint(decode_class(model.predict(to_wordvec(\"I cannot believe this\"))))\nprint(decode_class(model.predict(to_wordvec(\"DJ Khaled: suffering from success\"))))\nprint(decode_class(model.predict(to_wordvec(\"Cut my life into pieces, this is my last resort\"))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#kfold\nprint(\"Doing k-fold...\")\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n# prepare the cross-validation procedure\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\n# create model\nmodel = LogisticRegression()\n# evaluate model\nscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encode y as one hot for NN\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\n\ndef encode_class(Y):\n    encoded_Y = encoder.transform(Y)\n    # convert integers to dummy variables (i.e. one hot encoded)\n    dummy_y = np_utils.to_categorical(encoded_Y)\n    return dummy_y\n\ndef decode_class(Y):\n    Y_argmax = np.argmax(Y)\n    return encoder.inverse_transform([Y_argmax])\n\ny_train_enc = encode_class(y_train)\ny_test_enc = encode_class(y_test)\n\ndecode_class(y_train_enc[0])\nX_test[0].shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets now try with a NN\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    layers.Dense(1024, activation='relu', input_shape=[768]),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(2, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nhistory = model.fit(\n    X_train, y_train_enc,\n    validation_data=(X_test, y_test_enc),\n    batch_size=256,\n    epochs=100,\n    callbacks=[early_stopping],\n    verbose=1,\n)\n\n\n# Show the learning curves\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nmodel.evaluate(X_test, y_test_enc, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for layer in model.layers:\n#    print(layer.input_shape)\n# new sentence embeddings\ndef to_wordvec(string):\n    vectors = tr.encode(string)\n    return vectors.reshape(1, 768)\n\n\nprint(decode_class(model.predict(to_wordvec(\"This is the end\"))))\nprint(decode_class(model.predict(to_wordvec(\"I love Donald Trump\"))))\nprint(decode_class(model.predict(to_wordvec(\"The Playstations are blocking the Suez Canal\"))))\nprint(decode_class(model.predict(to_wordvec(\"I am scared of this\"))))\nprint(decode_class(model.predict(to_wordvec(\"Yo listen up, here's a story\"))))\nprint(decode_class(model.predict(to_wordvec(\"I cannot believe this\"))))\nprint(decode_class(model.predict(to_wordvec(\"DJ Khaled: suffering from success\"))))\nprint(decode_class(model.predict(to_wordvec(\"Cut my life into pieces, this is my last resort\"))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(decode_class(model.predict(to_wordvec(\"Hate is a solution\"))))\n\nprint(decode_class(model.predict(to_wordvec(\"You should go fuck yourself\"))))\n\nprint(decode_class(model.predict(to_wordvec(\"Trump\"))))\n\nprint(decode_class(model.predict(to_wordvec(\"Timmy Trumpet\"))))\n\nprint(decode_class(model.predict(to_wordvec(\"Timmy Trumpet should join them in hell, such a traitor\"))))\n\nprint(decode_class(model.predict(to_wordvec(\"Donald Trump should join them in hell, such a traitor\"))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let us see how the model guessed wrong\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\nfor i in range(200,300):\n    X = data.text[i]\n    y = data.task_1[i]\n    ans = decode_class(model.predict(to_wordvec(X)))[0]\n    \n    if(y != \"HOF\"):\n        continue\n    \n    if(y == ans):\n        pass\n        #continue\n    \n    print(\"Sentence is: \"+bcolors.HEADER+X+bcolors.ENDC)\n    print(\"Model guess: \"+bcolors.OKBLUE+ans+bcolors.ENDC)\n    print(\"Answer is: \"+bcolors.OKBLUE+y)\n    print(bcolors.ENDC+\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}