{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data():\n    #get train data\n    train_data_path ='../input/house-prices-dataset/train.csv'\n    train = pd.read_csv(train_data_path)\n    \n    #get test data\n    test_data_path ='../input/house-prices-dataset/test.csv'\n    test = pd.read_csv(test_data_path)\n    \n    return train , test\n\ndef get_combined_data():\n  #reading train data\n  train , test = get_data()\n\n  target = train.SalePrice\n  train.drop(['SalePrice'],axis = 1 , inplace = True)\n\n  combined = train.append(test)\n  combined.reset_index(inplace=True)\n  combined.drop(['index', 'Id'], inplace=True, axis=1)\n  return combined, target\n\n#Load train and test data into pandas DataFrames\ntrain_data, test_data = get_data()\n\n#Combine train and test data to process them together\ncombined, target = get_combined_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cols_with_no_nans(df,col_type):\n    '''\n    Arguments :\n    df : The dataframe to process\n    col_type : \n          num : to only get numerical columns with no nans\n          no_num : to only get nun-numerical columns with no nans\n          all : to get any columns with no nans    \n    '''\n    if (col_type == 'num'):\n        predictors = df.select_dtypes(exclude=['object'])\n    elif (col_type == 'no_num'):\n        predictors = df.select_dtypes(include=['object'])\n    elif (col_type == 'all'):\n        predictors = df\n    else :\n        print('Error : choose a type (num, no_num, all)')\n        return 0\n    cols_with_no_nans = []\n    for col in predictors.columns:\n        if not df[col].isnull().any():\n            cols_with_no_nans.append(col)\n    return cols_with_no_nans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = get_cols_with_no_nans(combined , 'num')\ncat_cols = get_cols_with_no_nans(combined , 'no_num')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Number of numerical columns with no nan values :',len(num_cols))\nprint ('Number of nun-numerical columns with no nan values :',len(cat_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = combined[num_cols + cat_cols]\ncombined.hist(figsize = (12,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data[num_cols + cat_cols]\ntrain_data['Target'] = target\n\nC_mat = train_data.corr()\nfig = plt.figure(figsize = (15,15))\n\nsb.heatmap(C_mat, vmax = .8, square = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def oneHotEncode(df,colNames):\n    for col in colNames:\n        if( df[col].dtype == np.dtype('object')):\n            dummies = pd.get_dummies(df[col],prefix=col)\n            df = pd.concat([df,dummies],axis=1)\n\n            #drop the encoded column\n            df.drop([col],axis = 1 , inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There were {} columns before encoding categorical features'.format(combined.shape[1]))\ncombined = oneHotEncode(combined, cat_cols)\nprint('There are {} columns after encoding categorical features'.format(combined.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_combined():\n    global combined\n    train = combined[:1460]\n    test = combined[1460:]\n\n    return train , test ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = split_combined()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second : Make the Deep Neural Network\n\n    Define a sequential model\n    Add some dense layers\n    Use 'relu' as the activation function in the hidden layers\n    Use a 'normal' initializer as the kernal_intializer\n\n        Initializers define the way to set the initial random weights of Keras layers.\n\n    We will use mean_absolute_error as a loss function\n    Define the output layer with only one node\n    Use 'linear 'as the activation function for the output layer\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_model = Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"input layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"output layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"compile the network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_model.fit(train, target, epochs=30, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load wights file of the best model :\nwights_file = './Weights-073--21732.46875.hdf5' # choose the best checkpoint \nNN_model.load_weights(wights_file) # load it\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_submission(prediction, sub_name):\n  my_submission = pd.DataFrame({'Id':pd.read_csv('test.csv').Id,'SalePrice':prediction})\n  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n  print('A submission file has been made')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = NN_model.predict(test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}