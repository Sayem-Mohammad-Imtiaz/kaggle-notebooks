{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin/breast cancer.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.countplot(df['diagnosis'], palette='husl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## clean and prepare the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('id',axis=1,inplace=True)\ndf.drop('Unnamed: 32',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#def diagnosis_value(diagnosis):\n    if diagnosis == 'M':\n        return 1\n    else:\n        return 0\n\n#df['diagnosis'] = df['diagnosis'].apply(diagnosis_value)"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"radius_mean , perimeter _mean, area_mean have a high correlation with malignant tumor"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['diagnosis'], color='g')\nplt.title('Plot_Diagnosis (M=1 , B=0)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate a scatter plot matrix with the \"mean\" columns\ncols = ['diagnosis',\n        'radius_mean', \n        'texture_mean', \n        'perimeter_mean', \n        'area_mean', \n        'smoothness_mean', \n        'compactness_mean', \n        'concavity_mean',\n        'concave points_mean', \n        'symmetry_mean', \n        'fractal_dimension_mean']\n\nsns.pairplot(data=df[cols], hue='diagnosis', palette='rocket')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"almost perfectly linear patterns between the radius, perimeter and area attributes are hinting at the presence of multicollinearity between these variables. (they are highly linearly related)\nAnother set of variables that possibly imply multicollinearity are the concavity, concave_points and compactness."},{"metadata":{},"cell_type":"raw","source":"Multicollinearity is a problem as it undermines the significance of independent varibales and we fix it \nby removing the highly correlated predictors from the model\nUse Partial Least Squares Regression (PLS) or Principal Components Analysis, regression methods that cut the number \nof predictors to a smaller set of uncorrelated components."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate and visualize the correlation matrix\ncorr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"we can verify the presence of multicollinearity between some of the variables. \nFor instance, the radius_mean column has a correlation of 1 and 0.99 with perimeter_mean and area_mean columns, respectively.\nThis is because the three columns essentially contain the same information, which is the physical size of the observation\n(the cell). \nTherefore we should only pick ONE of the three columns when we go into further analysis."},{"metadata":{},"cell_type":"raw","source":"Another place where multicollienartiy is apparent is between the \"mean\" columns and the \"worst\" column.\nFor instance, the radius_mean column has a correlation of 0.97 with the radius_worst column."},{"metadata":{},"cell_type":"markdown","source":"also there is multicollinearity between the attributes compactness, concavity, and concave points. So we can choose just ONE\nout of these, I am going for Compactness."},{"metadata":{"trusted":true},"cell_type":"code","source":"# first, drop all \"worst\" columns\ncols = ['radius_worst', \n        'texture_worst', \n        'perimeter_worst', \n        'area_worst', \n        'smoothness_worst', \n        'compactness_worst', \n        'concavity_worst',\n        'concave points_worst', \n        'symmetry_worst', \n        'fractal_dimension_worst']\ndf = df.drop(cols, axis=1)\n\n# then, drop all columns related to the \"perimeter\" and \"area\" attributes\ncols = ['perimeter_mean',\n        'perimeter_se', \n        'area_mean', \n        'area_se']\ndf = df.drop(cols, axis=1)\n\n# lastly, drop all columns related to the \"concavity\" and \"concave points\" attributes\ncols = ['concavity_mean',\n        'concavity_se', \n        'concave points_mean', \n        'concave points_se']\ndf = df.drop(cols, axis=1)\n\n# verify remaining columns\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw the heatmap again, with the new correlation matrix\ncorr = df.corr().round(2)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(['diagnosis'],axis=1)\ny = df['diagnosis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test,y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\n\nX_train=ss.fit_transform(X_train)\nX_test=ss.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models and finding out the Best one"},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\n\nmodel1=lr.fit(X_train,y_train)\nprediction1=model1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm=confusion_matrix(y_test,prediction1)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm,annot=True)\nplt.savefig('h.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP=cm[0][0]\nTN=cm[1][1]\nFN=cm[1][0]\nFP=cm[0][1]\nprint('Testing Accuracy:',(TP+TN)/(TP+TN+FN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,prediction1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtc=DecisionTreeClassifier()\nmodel2=dtc.fit(X_train,y_train)\nprediction2=model2.predict(X_test)\ncm2= confusion_matrix(y_test,prediction2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cm2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,prediction2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc=RandomForestClassifier()\nmodel3 = rfc.fit(X_train, y_train)\nprediction3 = model3.predict(X_test)\nconfusion_matrix(y_test, prediction3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, prediction3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, prediction3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, prediction1))\n\nprint(classification_report(y_test, prediction2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K Nearest Neighbor (K NN)\n#### Support Vector Machine\n#### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[]\n\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model\n\nresults =[]\nnames=[]\nfor name , model in models:\n    kfold=KFold(n_splits=10, random_state=40)\n    cv_results= cross_val_score(model, X_train, y_train, cv=kfold,scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    \n    msg= '%s:, %f, (%f)' % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions on test datasets\n\nSVM = SVC()\nSVM.fit(X_train, y_train)\npredictions= SVM.predict(X_test)\nprint(accuracy_score(y_test, predictions))\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We are getting the best accuracy with SVM which is 96.4%  , the model is predicting with 96% accuracy on our test data\n"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"TP :112 cases are  correctly identified \nTN :53 are correctly rejected\nFN : 3 are incorrectly rejected and \nFP : 3 are incorrectly identified"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}