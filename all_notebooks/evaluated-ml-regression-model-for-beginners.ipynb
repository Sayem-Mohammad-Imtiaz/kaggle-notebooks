{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Evaluated Machine Learning Regression Model for Beginners"},{"metadata":{},"cell_type":"markdown","source":"### Importing the basic Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Importing as train and test"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/bigmart-sales-data/Train.csv\")\ntest = pd.read_csv(\"/kaggle/input/bigmart-sales-data/Test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To do examine the Data, we need to concatanate**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train,test],sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() #There are missing values and we need to fill or drop them ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Item Outlet Sales will be our label for regression algorithm*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data[\"Item_Outlet_Sales\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's check categorical and numerical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\ncategorical = train.select_dtypes(include = [np.object])\nprint(categorical.shape)\nnumerical = train.select_dtypes(include = [np.float64,np.int64,np.int32])\nprint(numerical.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test\ncategorical = test.select_dtypes(include = [np.object])\nprint(categorical.shape)\nnumerical = test.select_dtypes(include = [np.float64,np.int64,np.int32])\nprint(numerical.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **DATA CLEANING**\n* *Missing Values*\n* *Outlier Detection*\n* *Feature Scaling - Standardization, - Normalization*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- In the \"Item_Weight\" label exist 1463 missing value\n- In the \"Outlet_Size\" label exist 2410 missing value\n- Let's fix them"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum() #Similar missing values exit in test data too.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Item_Weight\"] = train[\"Item_Weight\"].fillna(train[\"Item_Weight\"].mean())\ntest[\"Item_Weight\"] = test[\"Item_Weight\"].fillna(test[\"Item_Weight\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Item_Weight\"].isna().sum() , test[\"Item_Weight\"].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fill missing values in the \"Item_Weight\" column by using \"median\""},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see,\n- Item_Weight column is float (numerical) format that's why we could use median method.\n- Outlet_Size column is object form. So we need to use different method to fill them"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Outlet_Size.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.Outlet_Size.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For categorical nan values we can use \"mode\" method it means filling with most common value"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Outlet_Size\"] = train[\"Outlet_Size\"].fillna(train[\"Outlet_Size\"].mode()[0])\ntest[\"Outlet_Size\"] = train[\"Outlet_Size\"].fillna(test[\"Outlet_Size\"].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Missing value quantity of train data:\",train[\"Outlet_Size\"].isna().sum())\nprint(\"Missing value quantity of test data:\",test[\"Outlet_Size\"].isna().sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*We fixed the whole nan values*"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA) "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Item_Identifier\"].value_counts() #We can not detect any irregularity for that column\n#try the others and checking irregularities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Item_Fat_Content\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are Low Fat, low fat also, LF lets merge them\n- There are reg also Regular too, we need to merge them too.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Item_Fat_Content'].replace(['low fat','LF','reg'],['Low Fat','Low Fat','Regular'],inplace = True) \n#By doing this we merged them as one feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Item_Fat_Content\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Item_Fat_Content'].replace(['low fat','LF','reg'],['Low Fat','Low Fat','Regular'],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Outlet_Identifier\"].value_counts() #There is nothing anormal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Item_Type\"].value_counts() #Nothing anormal too","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Outlet_Size\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Outlet_Location_Type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Outlet_Type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now, I wanna add a column which shows us how many years passed the item reported on the system \n- (Note: The time that Ä± wrote this notebook is 2021)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Duration\"] = train[\"Outlet_Establishment_Year\"].apply(lambda i:2021 - i)\ntest[\"Duration\"] = test[\"Outlet_Establishment_Year\"].apply(lambda i:2021 - i)\ntrain[\"Duration\"]= train[\"Duration\"].astype(\"str\")\ntest[\"Duration\"]= test[\"Duration\"].astype(\"str\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizations to see quantities of the values - Unvariate Data Analysis\n- We just do for the object columns to see how many of them exist"},{"metadata":{},"cell_type":"markdown","source":"But firstly how many object format column worth to visualize\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for i in train.columns:\n#    if train.columns[i].astype == 'O':\n#        print(\"For {} column unique value amount is : {}\".format(train.columns[i],train.columns[i].unique()))\n#    else:\n#        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"For Item_Identifier:\",train[\"Item_Identifier\"].unique())\nprint(\"For Item_Fat_Content:\",train[\"Item_Fat_Content\"].unique())\nprint(\"For Item_Type:\",train[\"Item_Type\"].unique())\nprint(\"For Outlet_Identifier:\",train[\"Outlet_Identifier\"].unique())\nprint(\"For Outlet_Size :\",train[\"Outlet_Size\"].unique())\nprint(\"For Outlet_Location_Type:\",train[\"Outlet_Location_Type\"].unique())\nprint(\"For Outlet_Type:\",train[\"Outlet_Type\"].unique())\nprint(\"For Duration:\",train[\"Duration\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**what makes sense to visualize which columns have few variables**\n- Item_Fat_Content\n- Outlet_Identifier\n- Outlet_Size\n- Outlet_Location_Type\n- Outlet_Type\n- Duration"},{"metadata":{},"cell_type":"markdown","source":"- It means that except Item_Identifer and Item_Type columns, visualization process might help to analyze the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = train[[\"Item_Fat_Content\",\"Outlet_Identifier\",\"Outlet_Size\",\"Outlet_Location_Type\",\"Outlet_Type\",\"Item_Type\",\"Duration\"]]\nfor i in cols:\n    plt.figure(figsize=(22,10))\n    ax = sns.countplot(cols[i],palette = \"CMRmap\")\n    print(cols[i].value_counts())\n    ax.set(ylabel = \"COUNT\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### INFERENCES From the Unvariate Visualizations\n- Low Fat people recorded as a costumer more than the Regular ones, \n- The medium size Outlets are more than the others\n- Tier 3 Outlets has the majority in the cities\n- As a outlet type Type 1 Supermarkets widely positioned than the others \n- The best stocked item types are Fruits, Vegetables and Snacks\n- And lastly, most of the outlets have established and stil working for 35 years"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Outlier Detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[detect_outliers(train,[\"Item_Weight\",\"Item_Visibility\",\"Item_MRP\",\"Outlet_Establishment_Year\",\n                                    \"Item_Outlet_Sales\"])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[detect_outliers(test,[\"Item_Weight\",\"Item_Visibility\",\"Item_MRP\",\"Outlet_Establishment_Year\"])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We checked the outlier value but as you see there is nothing\n- Reminder : We just detect outliers in datas which types are int,float or any numerical type"},{"metadata":{},"cell_type":"markdown","source":"### Visualizations to see sales amount relations with variables- Bivariate Data Analysis\n- We just do for the object columns to see how many of them affect the sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = train[[\"Item_Fat_Content\",\"Outlet_Identifier\",\"Outlet_Size\",\"Outlet_Location_Type\",\"Outlet_Type\",\"Item_Type\",\"Duration\"]]\nfor i in cols:\n    plt.figure(figsize=(22,7))\n    ax = sns.barplot(cols[i],data[\"Item_Outlet_Sales\"],palette = \"CMRmap\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### INFERENCES From the Bivariate Visualizations\n- Low Fat people have reported more than the Regular one but as we see when we investigate the sales, regular ones bought items more than low fat ones , \n- The medium size Outlets are more than the others but about sales, In the High Outlets sales numbers are better\n- Tier 3 Outlets has the majority in the cities, but best sales numbers recorded in Tier 2 Outlets \n- As a outlet type Type 1 Supermarkets widely positioned than the others but about sales Tier 3 Supermarkets have the best result\n- The most stocked item types are Fruits, Vegetables and Snacks but best seller is Starchy Foods\n- And lastly, most of the outlets have established and stil working for 35 years also they are the best about sales"},{"metadata":{},"cell_type":"markdown","source":"### Visualizations to see sales amount relations with multivariables- Multivariate Data Analysis\n- We just do for the object columns (multivariate relationship) to see how many of them affect the sales"},{"metadata":{},"cell_type":"markdown","source":"#### Firstly, investigating the correlation might help us"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation\nplt.figure(figsize = (20,20))\nsns.heatmap(train.corr(), annot=True, fmt = \".3f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Item_MRP and Item_Weight is highly correlated one of them need to drop\n- Item_MRP and Sales is so highly correlated\n- Item_Weight and Sales correlated too "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nsns.barplot('Item_Type','Item_Outlet_Sales',hue='Item_Fat_Content',data=train,palette='RdYlGn')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.barplot('Outlet_Location_Type','Item_Outlet_Sales',hue='Outlet_Type',data=train,palette='magma')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\n- Label Encoding\n- Dropping useless columns\n- Splitting label and train,test\n- Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"1) ***Label Encoding***\n- In Machine Learning classifiers have to be numerical format that's why we do label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncolslabeled = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type','Item_Type']\nfor i in colslabeled:\n    train[i] = le.fit_transform(train[i])\n    \nfor i in colslabeled:\n    test[i] = le.fit_transform(test[i])\n    \nxc = train[colslabeled]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for  i in xc:\n    print(\"For {} column, Number of unique values :{}\".format(xc[i].name,xc[i].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) *Dropping Columns*\n- \"Item_Identifier\",\n- \"Outlet_Identifier\",\n- \"Outlet_Establishment_Year\" is not useful for our model that's why we need to drop them"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop([\"Item_Identifier\",\"Outlet_Identifier\",\"Outlet_Establishment_Year\"],axis=1, inplace =True)\ntest.drop([\"Item_Identifier\",\"Outlet_Identifier\",\"Outlet_Establishment_Year\"],axis=1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) *Splitting Train Test Split and Label*\n- We'll use our test set as a validation set, so firstly;\n- We need to seperate our label ,\"Item_Outlet_Sales\", \n- Then we need to do train-test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train[\"Item_Outlet_Sales\"]\nX=train.drop([\"Item_Outlet_Sales\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4) *Feature Scaling*"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features =[]\nfor i in X.columns:\n    features.append([i])\n#The columns have added to features list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train-Test Split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# sc = StandardScaler()\n# X_train = sc.fit_transform(X_train)\n# X_test = sc.fit_transform(X_test)\n# #After the process, our data turns into numpy array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Regression Models\n- Linear Regression\n- Random Forest Regressor"},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nLr = LinearRegression(normalize=True)\nLr.fit(X_train,y_train)\ny_pred = Lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*- Evaluation Metrics*\n- R2\n+ Adjusted R2\n+ Accuracy Score\n+ Mean Absolute Error\n+ Mean Squared Error"},{"metadata":{"trusted":true},"cell_type":"code","source":"#R2 Score\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_test,y_pred)\nprint(\"r2 score is :\",R2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adjusted R2 Score\ndef adj_r2 (X,y,model):\n    r_squared = model.score(X,y)\n    return(1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1))\n\n#Checking Adjusted R2 score of the train and test datas \nprint(\"Adj. R2 of the train set\",adj_r2(X_train,y_train,Lr))\nprint(\"Adj. R2 of the test set\",adj_r2(X_test,y_test,Lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy Score\nprint(\"Score of the train set\",Lr.score(X_train,y_train))\nprint(\"Score of the test set\",Lr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mean Abs. Error and Mean Squared Error\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nMAE = mean_absolute_error(y_test,y_pred)\nMSE = mean_squared_error(y_test,y_pred)\nprint(\"Mean Absolute Error :\",MAE)\nprint(\"Mean Squared Error :\",MSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Our model is not accurate enough so let's try regularization technics*\n- Lasso (L1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"move = np.arange(0.01,0.99,0.05)\nmove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso (L1) Regularization \nfrom sklearn.linear_model import Lasso\nfor i in move:\n    lasso_model = Lasso(alpha =i)\n    lasso_model.fit(X_train,y_train)\n    print(\"Train, For alpha = {}, model score is {} \".format(i,lasso_model.score(X_train,y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nRf = RandomForestRegressor(n_estimators = 300,\n                           criterion = \"mse\", \n                           max_depth =4, \n                           n_jobs = -1,\n                           random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rf.fit(X_train,y_train)\ny_predrf = Rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*- Evaluation Metrics*\n- R2\n+ Adjusted R2\n+ Accuracy Score\n+ Mean Absolute Error\n+ Mean Squared Error"},{"metadata":{"trusted":true},"cell_type":"code","source":"#R2 Score\nfrom sklearn.metrics import r2_score\nR2rf = r2_score(y_test,y_predrf)\nprint(\"r2 score is :\",R2rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adjusted R2 Score\ndef adj_r2 (X,y,model):\n    r_squared = model.score(X,y)\n    return(1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1))\n\n#Checking Adjusted R2 score of the train and test datas \nprint(\"Adj. R2 of the train set\",adj_r2(X_train,y_train,Rf))\nprint(\"Adj. R2 of the test set\",adj_r2(X_test,y_test,Rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mean Abs. Error and Mean Squared Error\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nMAErf = mean_absolute_error(y_test,y_predrf)\nMSErf = mean_squared_error(y_test,y_predrf)\nprint(\"Mean Absolute Error :\",MAErf)\nprint(\"Mean Squared Error :\",MSErf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compare the Results "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear Regression r2 score is :\",R2)\nprint(\"Random Forest r2 score is :\",R2rf)\nprint(\"-----------------------------------\")\nprint(\"Linear Reg. Adj. R2 of the train set\",adj_r2(X_train,y_train,Lr))\nprint(\"Random Forest Adj. R2 of the train set\",adj_r2(X_train,y_train,Rf))\nprint(\"Linear Reg.Adj. R2 of the test set\",adj_r2(X_test,y_test,Lr))\nprint(\"Random Forest Adj. R2 of the test set\",adj_r2(X_test,y_test,Rf))\nprint(\"-----------------------------------\")\nprint(\"Linear Regression Mean Absolute Error :\",MAE)\nprint(\"Linear Regression Mean Squared Error :\",MSE)\nprint(\"Random Forest Mean Absolute Error :\",MAErf)\nprint(\"Random Forest Mean Squared Error :\",MSErf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclsion\n- **Clearly Random Forest more accurate than the Linear Regression for this problem** \n- Thanks"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}