{"cells":[{"metadata":{},"cell_type":"markdown","source":"## FB ADS: text mining and exploratory data analysis\n\nIn this simple notebook, I will go though the \"Political Advertisements from Facebook\" dataset and will try to get some relevant insights out of it.\n\nDisclaimer: \n- In this notebook I make use of a python package for text analytics I'm working on called [texthero](https://github.com/jbesomi/texthero/). Texthero is still in alpha version and it's a work-in-progress.\n- Work-in-progress; I will keep working on it in the next few days.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n!pip install texthero -q\nimport texthero as hero","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We start by loading and displaying the dataframe. As there are more than 3GB of data, this might take a while.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ads_df = pd.read_csv(\"/kaggle/input/political-advertisements-from-facebook/fbpac-ads-en-US.csv\")\nads_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are left with 162'324 advertisements.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ads_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For faster computation, we sub-sample 10k rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLE_SIZE = 10000\nads_df_ = ads_df.sample(SAMPLE_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the different columns:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ads_df_.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads_df_.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we are interested in **columns with text data**. Let's find them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_text_columns(df):\n    text_columns = []\n    for col in df.select_dtypes('object'):\n        if (df[col].str.split().str.len() > 5).any():\n            text_columns.append(df[col].name)\n    return text_columns\n\nget_text_columns(ads_df_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As supposed, among others we found column `message` and `title`. We could have found the same results by ourself, just by looking at the table description, doing programmatically is more fun.\n\nWe notice also the `entities` columns, this are entites from the text that have been extracted with a spefcific software. We might want to use our own methods for extract entities and compare it with their version.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### What are the most common words in Ads title?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TOP_WORDS = 10\n\nhero.top_words(ads_df_.title)[:TOP_WORDS]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"hmm, let's try again with lowercased text:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hero.top_words(ads_df_.title.str.lower())[:TOP_WORDS]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, we can spot some political words such as 'committe', 'international' and 'action'. Just by looking at the top words, we can have a feeling of the dataset.\n\nBut, there are many stopwords such as 'for', 'of', 'the' that does not help much. Let's get rid of them and try again.\n\nFor that, we make use of the powerful and handy pandas `pipe` function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(\n    ads_df_.title.str.lower()\n    .dropna()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### What are the most common words in the 'message'?\n\nLet's repeat the same action again, this time on the message column. We will skip the temporary steps and look just at the final result without _stopwords_.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(\n    ads_df_.message.str.lower()\n    .dropna()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hm, that's look strange. `top_words` returns to us all the words present in every columns of the Pandas series. If, for instance, the series is composed of a single row with content `hello world!`, `top_words` first split into words, i.e `hello` and `word` and not `word!`, and then it count.\n\nAs the `message` columns contains html tags, the top_words refer to the words inside a tag (i.e `<p>` becomes `p`). We need therefore to get rid of html tags.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html_tags(s: pd.Series) -> pd.Series():\n    \"\"\"Remove all html entities from a pandas series\"\"\"\n    \n    # TODO. Consider this more sophisticated solution: ('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    \n    return s.str.replace('<.*?>', '')\n\ns = pd.Series(\"<p>Hello world!</p>\")\nremove_html_tags(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads_df_['message'] = remove_html_tags(ads_df_['message'])\n\n(\n    ads_df_.message.str.lower()\n    .dropna()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we go. It's indeed interesting to notice how the top words for the title and the message are different. The top words of the message are `us`, `help`, `people`, `need` that sounds like words used in call-for-action sentence: \"**We need to get your vote today!**\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Show me some pics!\n\nYou are right; this notebook is boring. Let's add some images and colors. Let's start with old-style wordcloud:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hero.wordcloud(ads_df_.title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hero.wordcloud(ads_df_.message)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Who are the principal advertiser?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ads_df['advertiser'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A short google search reveals us that Beto O'Rourke is an American politicians (what a surprise!): \"Robert Francis \"Beto\" O'Rourke is a Decocratic American politician who represented Texas's 16th congressional district in the United States House of Representatives from 2013 to 2019\".","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Beto_O%27Rourke%2C_Official_portrait%2C_113th_Congress.jpg/440px-Beto_O%27Rourke%2C_Official_portrait%2C_113th_Congress.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"... and ACLU is the \"[American Civil Liberties Union](https://www.aclu.org/)\".\n\nAmong other, there is also the actual President of U.S.A J. Donald Trump with 1443 advertisements.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Show me some interesting data. What does the Donald Trump's ads says?\n\nAll right.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trump_df = ads_df[ads_df['advertiser'] == 'Donald J. Trump'].copy()\ntrump_df.title.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the Trump's advertisements just contains `Donald J. Trump`. hmm, what about the content?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trump_df['message'] = remove_html_tags(trump_df['message'])\n\n(\n    trump_df.message.str.lower()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about \"let's make America great again?\"\n\nRather than looking at single terms, let's look at the top n-grams:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trump_df['noun_chunks'] = hero.nlp.noun_chunks(trump_df.message)\ntrump_df['noun_chunks'].head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(hero.nlp.noun_chunks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `noun_chunks` functions return extra information regarding each noun_chunk found in the sentence, including the part-of-speech tagging and the start and end index of the noun chunk in the sentence.\n\nHere, we are simply interested in getting all noun_chunks and find the most relevant ones:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trump_df['noun_chunks'].apply(lambda row: [r[0] for r in row]).explode().value_counts()[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Continuation coming soon.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}