{"cells":[{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"import tensorflow as tf\n!pip install tensorflow_datasets\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds, info = tfds.load('iris', split='train', with_info=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total classes %d, %s' % (info.features['label'].num_classes, info.features['label'].names))\narr = []\nfor i in tfds.as_numpy(ds):\n    arr.append(i['features'])\nprint('Total samples: %d' % len(arr))\n\ncount_by_class = {0:0, 1:0, 2:0}\nlabels = []\nfor i in tfds.as_numpy(ds):\n    count_by_class[i['label']] += 1\n    labels.append(i['label'])\n\nfor cl, count in count_by_class.items():\n    print(\"Class #%d, samples: %d\" % (cl, count))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(arr)\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor i in range(0, 4):   \n    plt.subplot(2, 2, i + 1)\n    plt.title(\"Feature #%d\" % i)\n    plt.hist(df[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array(labels)\nfeature_num = 4\ndef plot_by_features(first_feature, second_feature, subplot_num):\n    first_class = df[labels == 0]\n    second_class = df[labels == 1]\n    third_class = df[labels == 2]\n\n    plt.subplot(feature_num, feature_num, subplot_num)\n    plt.title('%d %d ' % (first_feature, second_feature))\n    plt.plot(first_class[first_feature], first_class[second_feature], 'go')\n    plt.plot(second_class[first_feature], second_class[second_feature], 'ro')\n    plt.plot(third_class[first_feature], third_class[second_feature], 'bo')\n\nplt.figure(figsize=(15, 15))\nnum = 0\nfor i in range(0, feature_num):\n    for j in range(0, feature_num):\n        num += 1\n        plot_by_features(i, j, num)\n\nplt.suptitle('2D plotting')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ndef draw_3d(first_feature, second_feature, third_feature, subplotnum):\n    first_class = df[labels == 0]\n    second_class = df[labels == 1]\n    third_class = df[labels == 2]\n\n    ax = fig.add_subplot(2, 1, subplotnum, projection='3d')\n    ax.scatter(first_class[first_feature], first_class[second_feature], first_class[third_feature], marker='o')\n    ax.scatter(second_class[first_feature], second_class[second_feature], second_class[third_feature], marker='^')\n    ax.scatter(third_class[first_feature], third_class[second_feature], third_class[third_feature], marker='v')\n\nfig = plt.figure(figsize=(10, 10))\ndraw_3d(0, 1, 2, 1)\ndraw_3d(0, 1, 3, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_class = 0\n\nselected = df[labels == selected_class]\nnotselected = df[labels != selected_class]\nplt.plot(selected.iloc[:, [0]], selected.iloc[:, [3]], 'ro')\nplt.plot(notselected.iloc[:, [0]], notselected.iloc[:, [3]], 'go')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separating first class from the rest"},{"metadata":{},"cell_type":"markdown","source":"Class #1 is very easy to separate from the rest. Code below trains a classifier to separate first class from the rest"},{"metadata":{},"cell_type":"markdown","source":"### Simple logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(a):\n    return 1. / (1. + np.exp(-1 * a))\n\ndef compute_class_probability(sample, w, b):\n    a = 0\n    for i in range(len(sample)):\n        a += sample[i] * w[i]\n    a += b\n    return sigmoid(a)\n\ndef update_weights(batch, labels, w, b, learning_rate):\n    \"\"\"Stochastic gradient descent\n    Based on https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n    \n    Returns: updated parameters `w` and `b`\n    \"\"\"\n    loss = 0\n    for (sample_num, (_, sample)) in enumerate(batch.iterrows()):\n        sample = sample.to_numpy()\n        y = labels[sample_num]\n\n        a = compute_class_probability(sample, w, b)\n\n        if y == 0:\n            loss -= np.log(1 - a)\n        else:\n            loss -= np.log(a)\n\n        for i in range(len(sample)):\n            w[i] -= learning_rate * (a - y) * sample[i]\n        b -= learning_rate * (a - y)\n\n    return ((w, b), loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_x2(w1, w2, b, x1):\n    return (-1 * w1 * x1 - b) / w2\n\ndef plot_line(w1, w2, b):\n    first = [0, 8]\n    second = [compute_x2(w1, w2, b, 0), compute_x2(w1, w2, b, 8)]\n    \n    plt.plot(first, second) \n\ndef train_and_plot(ds, labels, epochs=3, learning_rate=0.01):\n    w = [0, 0]\n    b = 0\n\n    for round_num in range(4):\n        for i in range(epochs):\n            (w, b), loss = update_weights(ds, labels, w, b, learning_rate)\n        print(\"loss %f\" % loss)\n\n        plt.subplot(2, 2, round_num + 1)\n        plt.title('%d epochs' % ((round_num + 1) * epochs))\n        selected = ds[labels == 1]\n        notselected = ds[labels != 1]\n        plt.plot(selected.iloc[:, [0]], selected.iloc[:, [1]], 'go')\n        plt.plot(notselected.iloc[:, [0]], notselected.iloc[:, [1]], 'ro')\n        plot_line(w[0], w[1], b)\n\n    return (w, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\ncolumns = [0, 1]\ntrain_and_plot(df.iloc[:, columns], (labels == 0).astype(int))\nplt.suptitle('Features %s' % columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\ncolumns = [0, 2]\ntrain_and_plot(df.iloc[:, [0, 2]], (labels == 0).astype(int))\nplt.suptitle('Features %s' % columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\ncolumns = [0, 3]\ntrain_and_plot(df.iloc[:, columns], (labels == 0).astype(int))\nplt.suptitle('Features %s' % columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separating 1 and the rest"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_evaluate(df, labels):\n    w = [0, 0, 0, 0]\n    b = 0\n    iterations = 10\n    for round_num in range(4):\n        for i in range(iterations):\n            (w, b), loss = update_weights(df, labels, w, b, learning_rate=0.01)\n        print(\"loss %f\" % loss)\n\n    correct = 0\n    for sample_num, (_, sample) in enumerate(df.iterrows()):\n        predicted = int(compute_class_probability(sample, w, b) > 0.5)\n        actual = labels[sample_num]\n        if actual == predicted:\n            correct += 1\n\n    print(\"Accuracy %f\" % (float(correct) / len(df) ))\n    print((w, b))\n\ntrain_and_evaluate(df, (labels == 0).astype(int))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separating 2 and 3 class"},{"metadata":{"trusted":true},"cell_type":"code","source":"second_and_third_df = df[labels != 0]\nsecond_and_third_labels = labels[labels != 0]\nsecond_and_third_labels = (second_and_third_labels == 1).astype(int)\n\ntrain_and_evaluate(second_and_third_df, second_and_third_labels)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2+"}},"nbformat":4,"nbformat_minor":4}