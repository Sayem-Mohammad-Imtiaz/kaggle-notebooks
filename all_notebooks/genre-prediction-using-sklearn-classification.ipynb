{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting song's genre using ML\n* Problem definition - check several classification models and try to achieve best accuracy in prediction genre of a song from data provided\n* Data - Data has been obtained from Kaggle: https://www.kaggle.com/mrmorj/dataset-of-songs-in-spotify\n* Evaluation - try several models and reach the best possible accuracy\n* Features - data consists of several features such as danceability, energy, key, loudness, mode in numerical format and other categorical features.\n* Modelling - we will try K-Nearest Neigbors, Random Forest and Logistic Regression model for evaluation\n* Experimentation - this section will involve fine tuning some of the hyperparameters to see if we can improve the accuracy"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Preparing the tools\nPandas,Matplotlib, and Numpy for data analysis and manipulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import EDA (exploratory data analysis) and plotting library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n#Import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n#Model evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score,f1_score\nfrom sklearn.metrics import plot_roc_curve","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/dataset-of-songs-in-spotify/genres_v2.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the number of rows and columns\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check and view data in transposed form to view all columns\ndata.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we are going to predict the genre , let's check the \"genre\" column for data types\ndata[\"genre\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's plot the above date from \"genre\" columns in bar plot for better vizualization\ndata[\"genre\"].value_counts().plot(kind=\"barh\",color=[\"lightblue\"],title=\"Genres\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check and see if we have missing data\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above, missing values are only in last 3 columns and this will not affect our analysis of predicting genre"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally we can check the data types in our data frame\ndata.dtypes\n#Based on below we will base the prediction analysis only on numerical data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation analysis using Seaborn heatmap for data analysis\ncorr_matrix=data.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nax=sns.heatmap(corr_matrix,\n              annot=True,\n              linewidths=0.5,\n              fmt=\".2f\",\n              cmap=\"YlGnBu\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into X and y\nnum_data=data.drop([\"title\",\"Unnamed: 0\",\"song_name\",\"analysis_url\",\"track_href\",\"uri\",\"id\",\"type\"],axis=1) #drop all non-numeric columns\nX=num_data.drop(\"genre\",axis=1)\ny=num_data[\"genre\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split into training and test sets\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will use 3 different models for this problem:\n    1. Logistic regression\n    2. K-Nearest\n    3. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For that we will create a fuction in order to evaluate and compare models easily\nmodels={\"LogReg\":LogisticRegression(),\n       \"KNN\":KNeighborsClassifier(),\n       \"Random Forest\":RandomForestClassifier()}\ndef fit_and_score (models,X_train,X_test,y_train,y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models\n    \"\"\"\n    np.random.seed(1)\n    model_scores={}\n    for name , model in models.items():\n        model.fit(X_train,y_train)\n        model_scores[name]=model.score(X_test,y_test)\n    return model_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores=fit_and_score(models=models,X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test)\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_compare=pd.DataFrame(model_scores,index=[\"Accuracy\"])\nmodel_compare.T.plot.barh(color=[\"lightblue\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now as we have initial Accuracy scoring of 3 models, let's look at following:\n* Hyperparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross validation\n* Precision, Recall, F1 score\n* Classification report"},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tuning\n1. HP tuning by hand\n2. HP tuning with RandomizedSearchCV"},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter tuning for KNN manually"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tuning for KNN model\ntrain_scores=[]\ntest_scores=[]\n#Create list of different n-neighbors\nneighbors = range(1,15)\n#Setup KNN instance\nknn=KNeighborsClassifier()\n#Loop through different neigbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    knn.fit(X_train,y_train) #Fit the model\n    train_scores.append(knn.score(X_train,y_train)) #Update the train score list\n    test_scores.append(knn.score(X_test,y_test)) #Update test scores list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(neighbors,train_scores,label=\"Train scores\")\nplt.plot(neighbors,test_scores,label=\"Test scores\")\nplt.xticks(np.arange(1,15,1))\nplt.yticks(np.arange(0.1,1,0.1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model scores\")\nplt.legend;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above chart shows that the best K-value giving higher prediction is *1* where we can see 36% accuracy level which is still low.\nLet's look for other models and see whether we can improve accuracy"},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter tuning with RandomizedSearchCV for LogReg and Random Forest models\nLet's tune LogReg and Random Forest models using RandomizedSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyperparameter grid for Logistic Regression\nlog_reg_grid={\"C\": np.logspace(-4,4,20),\n             \"solver\":[\"liblinear\"]}\n#Hyperparameter grid for Random Forest\nrf_grid={\"n_estimators\": np.arange(10,1000,50),\n         \"max_depth\":[None,3,5,10],\n         \"min_samples_split\":np.arange(2,20,2),\n         \"min_samples_leaf\":np.arange(1,20,2)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tune LogReg model\nnp.random.seed(1)\nrs_log_reg=RandomizedSearchCV(LogisticRegression(),\n                             param_distributions=log_reg_grid,\n                             cv=5,\n                             n_iter=20,\n                             verbose=True)\n#Fit random hyperparameter search model to LogReg\nrs_log_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the best parameters\nrs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_log_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above score is very very small increase from initial scoring, now let's try same with Random Forest and see how it can improve the scoring"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tune Random Forest model\nnp.random.seed(42)\nrs_rf=RandomizedSearchCV(RandomForestClassifier(),\n                        param_distributions=rf_grid,\n                        n_iter=20,\n                        verbose=True)\nrs_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_rf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluating tuned machine learning classifier , beyond accuracy\n\n* Comparison of real and predicted results\n* Classification report\n* Precision\n* Recall\n* F1 score\n\nSince Random Forest tuned model give the best results, going further we will evaluate only that model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make predictions with tuned model\ny_preds=rs_rf.predict(X_test)\npreds_df=pd.DataFrame(y_preds)\npreds_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check actual vs predictions\ncomparison=pd.DataFrame(data={\"actual\":y_preds,\"prediction\":y_test})\ncomparison.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate number of true and false predictions\ncomparison[\"result\"]=comparison[\"actual\"]==comparison[\"prediction\"]\ncomparison[\"result\"].value_counts().plot(kind=\"barh\",color=[\"lightblue\"],title=\"Comparison\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification report for Precision, F1 Score and Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report for precision, recall, f1score and accuracy\nprint(classification_report(y_test,y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance\nChecking with features contributed most to the outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat=RandomForestClassifier(n_estimators= 460,       #Using best params obtained earlier\n                            min_samples_split= 6,\n                            min_samples_leaf= 9,\n                            max_depth= None)\nfeat.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Match coefficients of features to columns\nfeature_dict=dict(zip(num_data.columns,list(feat.feature_importances_)))\n#Vizualize the feature importance\nfeature_data=pd.DataFrame(feature_dict,index=[0])\nfeature_data.T.plot.barh(title=\"Feature importance\",legend=False,color=\"lightblue\",grid=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}