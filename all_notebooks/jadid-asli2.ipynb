{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install requests\n!pip install tabulate\n!pip install \"colorama>=0.3.8\"\n!pip install future","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nh2o.init()\nh2o.demo(\"glm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scikit-learn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pyspark import SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import DataFrameReader\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import RFormula\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.feature import RFormula\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pylab import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##logFile = \"E:\\\\anacanda/Lib/site-packages/pyspark/bin/README.md\"  # Should be some file on your system\nspark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n\n\n##Set data storage path. This is where data is sotred on the blob attached to the cluster.\ndataDir = '../output/kaggle/working/'; # The last backslash is needed;\ndataDir2 = '../output/kaggle/working2/'; # The last backslash is needed;\n\nsqlContext = SQLContext(spark)\n## READ IN TRIP DATA FRAME FROM CSV\n\ntrip_fare_join='../input/newyork-taxi-demand/yellow_tripdata_2016-01.csv'\ntrip_fare = spark.read.csv(path=trip_fare_join, header=True, inferSchema=True)\n\ntrip_fare.printSchema()\n\ndataframe1 = trip_fare.withColumnRenamed('  pickup_longitude', 'pickup_longitude')\ndataframe2 = dataframe1.withColumnRenamed('   pickup_latitude', 'pickup_latitude')\ndataframe3 = dataframe2.withColumnRenamed(' dropoff_longitude', 'dropoff_longitude')\ndataframe4 = dataframe3.withColumnRenamed('  dropoff_latitude', 'dropoff_latitude')\n\n\n\n## READ IN FARE DATA FRAME FROM CSV\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataDir3 ='../output/kaggle/working3/'; # The last backslash is needed;\n\ndataframe4path = dataDir3 + \"dataframe4\";\n\ndataframe4.write.mode(\"overwrite\").parquet(dataframe4path)\n\ndataframe4 = pd.read_parquet(dataframe4path)   # Dataframe as pd. convert Spark's PD to Pandas's PD\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data Shape',dataframe4.shape)\ndataframe4.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove coordinate outliers\ndataframe4 = dataframe4[dataframe4['pickup_longitude'] <= -73.75]\ndataframe4 = dataframe4[dataframe4['pickup_longitude'] >= -74.03]\ndataframe4 = dataframe4[dataframe4['pickup_latitude'] <= 40.85]\ndataframe4 = dataframe4[dataframe4['pickup_latitude'] >= 40.63]\ndataframe4 = dataframe4[dataframe4['dropoff_longitude'] <= -73.75]\ndataframe4 = dataframe4[dataframe4['dropoff_longitude'] >= -74.03]\ndataframe4 = dataframe4[dataframe4['dropoff_latitude'] <= 40.85]\ndataframe4 = dataframe4[dataframe4['dropoff_latitude'] >= 40.63]\n\ndataframe4.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values in dataframe4 data\ndataframe4.isnull().sum().sort_values(ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#highest fare is $250\n#highest tip is $125\n\ndataframe4 = dataframe4.drop(dataframe4[(dataframe4['fare_amount']==0)].index, axis=0)\ndataframe4 = dataframe4.drop(dataframe4[(dataframe4['fare_amount']>250)].index, axis=0)\ndataframe4 = dataframe4.drop(dataframe4[(dataframe4['fare_amount']<0)].index, axis=0)\n\ndataframe4 = dataframe4.drop(dataframe4[(dataframe4['tip_amount']>125)].index, axis=0)\ndataframe4 = dataframe4.drop(dataframe4[(dataframe4['tip_amount']<0)].index, axis=0)\n\n\n\n# import seaborn as sns\n\n# x=dataframe4['fare_amount']\n# y=dataframe4['tip_amount']\n\n\n# sns.scatterplot(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4['fare_amount'].sort_values(ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4['tip_amount'].sort_values(ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4.trip_distance[(dataframe4.trip_distance==0)].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4=dataframe4.drop(dataframe4[(dataframe4['trip_distance']==0)].index,axis=0)\ndataframe4=dataframe4.drop(dataframe4[(dataframe4['trip_distance']>21.000000)].index,axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4.pickup_latitude != dataframe4.dropoff_latitude) &\n              (dataframe4.pickup_longitude != dataframe4.dropoff_longitude) &\n              (dataframe4.trip_distance == 0)].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataframe4 = dataframe4.drop(dataframe4[(dataframe4.pickup_latitude!=dataframe4.dropoff_latitude)&\n#                                         (dataframe4.pickup_longitude!=dataframe4.dropoff_longitude)&\n#                                         (dataframe4.trip_distance == 0)].index, axis = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['trip_distance']==0)&(dataframe4['fare_amount']==0)].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['trip_distance']==0)&((dataframe4['fare_amount']==0)|(dataframe4['fare_amount']!=0))].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['trip_distance']==10)].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['trip_distance']==21.000000)].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nx=dataframe4['trip_distance']\ny=dataframe4['fare_amount']\n\n\nsns.scatterplot(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['trip_distance']==1)&((dataframe4['fare_amount']>100)|(dataframe4['fare_amount']>100))].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nx=dataframe4['trip_distance']\ny=dataframe4['passenger_count']\n\n\nsns.scatterplot(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['trip_distance']==1)&((dataframe4['passenger_count']>3)|(dataframe4['passenger_count']>6))].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nplt.hist(dataframe4['passenger_count'],bins=15)\nplt.xlabel('No of Passanger')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4=dataframe4.drop(dataframe4[(dataframe4['passenger_count']==0)].index,axis=0)\ndataframe4=dataframe4.drop(dataframe4[(dataframe4['passenger_count']>6)].index,axis=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['passenger_count']==0)].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['passenger_count']==2)].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['passenger_count']==3)].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['passenger_count']==4)].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['passenger_count']==5)].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4[(dataframe4['passenger_count']==6)].count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ncorrMat = dataframe4[::].corr(); \nax = plt.subplots(figsize=(13, 12))\nax = sns.heatmap(corrMat,vmin=-1, vmax=1, annot=True, square = True,linewidths=2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['trip_distance'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['payment_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['passenger_count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['improvement_surcharge'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['extra'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['mta_tax'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['VendorID'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4=dataframe4.drop(dataframe4[(dataframe4['total_amount']>255)].index,axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['store_and_fwd_flag'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = dataframe4.drop(columns=['tpep_dropoff_datetime'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_pickupdatetime_info(dataset):\n    #Convert to datetime format\n    dataset['tpep_pickup_datetime'] = pd.to_datetime(dataset['tpep_pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S\")\n    \n    dataset['pickup_hour'] = dataset.tpep_pickup_datetime.dt.hour\n    dataset['pickup_day'] = dataset.tpep_pickup_datetime.dt.day\n    dataset['pickup_month'] = dataset.tpep_pickup_datetime.dt.month\n    dataset['pickup_weekday'] = dataset.tpep_pickup_datetime.dt.weekday\n    dataset['pickup_year'] = dataset.tpep_pickup_datetime.dt.year\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4 = add_pickupdatetime_info(dataframe4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nplt.hist(dataframe4['pickup_hour'],bins=50)\nplt.xlabel('Hour')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4_copy_sklearn = dataframe4.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe4_copy_sklearn.drop(columns=['tpep_pickup_datetime'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataframe4_copy_sklearn['fare_amount']\n\nx = dataframe4_copy_sklearn.drop(columns=['fare_amount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pipeline = Pipeline([('rob_scale', RobustScaler())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_scaled = data_pipeline.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_scaled = pd.DataFrame(x_scaled,columns=x.columns,index=x.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nsns.distplot(a=dataframe4_copy_sklearn.total_amount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nsns.distplot(a=dataframe4_copy_sklearn.fare_amount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as pyplot\n\n\n# Mean distribution\nmu = x_scaled['total_amount'].mean()\n\n# Std distribution\nsigma = x_scaled['total_amount'].std()\nnum_bins = 100\n\n# Histogram \nfig = plt.figure(figsize=(8.5, 5))\nn, bins, patches = plt.hist(x_scaled['total_amount'], num_bins,\n                           edgecolor = 'black', lw = 1, alpha = .40)\n# Normal Distribution\n# y = mlab.normpdf(bins, mu, sigma)\n# plt.plot(bins, y, 'r--', linewidth=2)\npyplot.plot(bins, norm.pdf(bins, mu, sigma))\nplt.xlabel('total_amount')\nplt.ylabel('Probability density')\n\n# Adding a title\nplt.title(r'$\\mathrm{Trip\\ duration\\ skewed \\ to \\ the \\ right:}\\ \\mu=%.3f,\\ \\sigma=%.3f$'%(mu,sigma))\nplt.grid(True)\n#fig.tight_layout()\nplt.show()\n\n# Statistical summary\nx_scaled.describe()[['total_amount']].transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n# dataframe4_copy_sklearn.to_parquet(dataframe4_copy_sklearn,path=dataDir4, engine='pyarrow', compression='none', index=None, partition_cols=None)\n\ndef write_parquet_file():\n    df = dataframe4_copy_sklearn\n    df.to_parquet('/kaggle/working/x.parquet')\n    \nwrite_parquet_file()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataDir4 ='/kaggle/working/x.parquet';\n\nx = spark.read.parquet(dataDir4)   # Dataframe as pd. convert Spark's PD to Pandas's PD\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.createOrReplaceTempView(\"fareamount1\")\n\n## USING SQL: MERGE TRIP AND FARE DATA-SETS TO CREATE A JOINED DATA-FRAME\n## ELIMINATE SOME COLUMNS, AND FILTER ROWS WTIH VALUES OF SOME COLUMNS\nsqlStatement = \"\"\"SELECT\n  fa.total_amount, fa.tolls_amount,fa.dropoff_latitude,\n  fa.fare_amount,fa.pickup_latitude,fa.dropoff_longitude,\n  fa.tip_amount,fa.pickup_hour,fa.pickup_year,fa.pickup_month,fa.pickup_day,fa.pickup_weekday,fa.pickup_longitude\n  FROM fareamount1 fa \n  WHERE fa.tip_amount >= 0 AND fa.tip_amount <= 125 \n  AND fa.fare_amount >= 1 AND fa.fare_amount <= 250\n  AND fa.tip_amount < fa.fare_amount\"\"\"\n  \nfare_amountDF = spark.sql(sqlStatement)\n\n# REGISTER JOINED TRIP-FARE DF IN SQL-CONTEXT\nfare_amountDF.createOrReplaceTempView(\"fareamount2\")\n\n## SHOW WHICH TABLES ARE REGISTERED IN SQL-CONTEXT\nspark.sql(\"show tables\").show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SAMPLE 10% OF DATA, SPLIT INTO TRAIINING AND VALIDATION AND SAVE IN BLOB\ntrip_fare_featSampled = fare_amountDF.sample(False, 0.1, seed=1234)\ntrainfilename = dataDir + \"TrainData\";\ntrip_fare_featSampled.repartition(10).write.mode(\"overwrite\").parquet(trainfilename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## READ IN DATA FRAME FROM CSV\ntaxi_df = spark.read.parquet(trainfilename)\n## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\ntaxi_df_cleaned = taxi_df.drop('RatecodeID')\\\n    .filter(\"fare_amount >= 1 AND fare_amount < 100\" )\n\n## PERSIST AND MATERIALIZE DF IN MEMORY\ntaxi_df_cleaned.persist()\n\n## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\ntaxi_df_cleaned.createOrReplaceTempView(\"taxi_df\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"taxi_df_cleaned.printSchema()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.sql(\"show tables\").show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%sql -q -o sqlResultsPD\n#SELECT fare_amount, passenger_count, tip_amount FROM taxi_train WHERE passenger_count > 0 AND passenger_count < 7 AND fare_amount > 0 AND fare_amount < 100 AND tip_amount > 0 AND tip_amount < 15\n\nsqlResultsPDtest = \"\"\"SELECT fare_amount,total_amount,tolls_amount,pickup_hour,\ntip_amount FROM taxi_df\nWHERE fare_amount > 0 AND fare_amount < 100 AND tip_amount > 0 AND tip_amount < 15\"\"\"\nsqlResultsPD = spark.sql(sqlResultsPDtest)\n\nsqlResultsPD.createOrReplaceTempView(\"trip_test_final\")\n\nspark.sql(\"show tables\").show()\n\ntrip_fare_fit_final = sqlResultsPD\n\ntrainfilename2 = dataDir2 + \"TrainDatafinal\";\n\ntrip_fare_fit_final.write.mode(\"overwrite\").parquet(trainfilename2)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trip_fare_fit_final_df = pd.read_parquet(trainfilename2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%local\n%matplotlib inline\n## %%local creates a pandas data-frame on the head node memory, from spark data-frame,\n##which can then be used for plotting. Here, sampling data is a good idea, depending on the memory of the head node\n\n# TIP BY PAYMENT TYPE AND PASSENGER COUNT\nax1 = trip_fare_fit_final_df[['tip_amount']].plot(kind='hist', bins=25, facecolor='lightblue')\nax1.set_title('Tip amount distribution')\nax1.set_xlabel('Tip Amount ($)'); ax1.set_ylabel('Counts');\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n\n# # TIP BY PASSENGER COUNT\n# ax2 = trip_fare_fit_final_df.boxplot(column=['tip_amount'], by=['pickup_hour'])\n# ax2.set_title('Tip amount by pickup_hour')\n# ax2.set_xlabel('pickup_hour'); ax2.set_ylabel('Tip Amount ($)');\n# plt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n\n# TIP AMOUNT BY FARE AMOUNT, POINTS ARE SCALED BY PASSENGER COUNT\nax = trip_fare_fit_final_df.plot(kind='scatter', x= 'fare_amount', y = 'tip_amount', c='blue', alpha = 0.10, s=2.5*(trip_fare_fit_final_df.total_amount))\nax.set_title('Tip amount by Fare amount')\nax.set_xlabel('Fare Amount ($)'); ax.set_ylabel('Tip Amount ($)');\nplt.axis([-2, 80, -2, 20])\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\nsqlStatement = \"\"\"SELECT\n  total_amount,tolls_amount,dropoff_latitude,\n  fare_amount,pickup_latitude,dropoff_longitude,\n  tip_amount,pickup_hour,pickup_year,pickup_month,pickup_day,pickup_weekday,pickup_longitude,\n  CASE\n    WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN 'Night'\n    WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN 'AMRush' \n    WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN 'Afternoon'\n    WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN 'PMRush'\n    END as TrafficTimeBins\n    FROM taxi_df\"\"\"\n\ntaxi_df = spark.sql(sqlStatement)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"taxi_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\n\n# DEFINE THE TRANSFORMATIONS THAT NEEDS TO BE APPLIED TO SOME OF THE FEATURES\nsI4 = StringIndexer(inputCol=\"TrafficTimeBins\", outputCol=\"TrafficTimeBinsIndex\");\n\n# APPLY TRANSFORMATIONS\nencodedFinal = Pipeline(stages=[sI4]).fit(taxi_df).transform(taxi_df);\n\nencodedFinal.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingFraction = 0.75; testingFraction = (1-trainingFraction);\nseed = 1234;\n\n# SPLIT SAMPLED DATA-FRAME INTO TRAIN/TEST, WITH A RANDOM COLUMN ADDED FOR DOING CV (SHOWN LATER)\ntrainData,testData = encodedFinal.randomSplit([trainingFraction, testingFraction], seed=seed);\n\n\n\n\n# CACHE DATA FRAMES IN MEMORY\ntrainData.persist(); \ntestData.persist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import RFormula\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"fare_amount ~ pickup_day + pickup_hour + pickup_weekday + pickup_month + TrafficTimeBinsIndex + total_amount + tolls_amount + pickup_longitude + pickup_latitude + dropoff_longitude + dropoff_latitude + tip_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE ELASTIC NET REGRESSOR\neNet = LinearRegression(featuresCol=\"indexedFeatures\", maxIter=50,regParam=0.01, elasticNetParam=0.7)\n\n## Fit model, with formula and other transformations\nmodel = Pipeline(stages=[regFormula, featureIndexer, eNet]).fit(trainData)\n\n#print(\"Coefficients: \" + str(model.coefficients))\n#print(\"Intercept: \" + str(model.intercept))\n# model_fit = Pipeline.fit(trainData)\n# fitandlabel = model_fit.select(\"label\",\"model_fit\").rdd\n# testMetrics = RegressionMetrics(fitandlabel)\n# print(\"R-sqr = %s\" % testMetrics.r2)\n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = model.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import RFormula\nfrom sklearn.metrics import roc_curve,auc\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nmodelDir = '../output/kaggle/working3/'; # The last backslash is needed;\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"fare_amount ~ pickup_day + pickup_hour + pickup_weekday + pickup_month + TrafficTimeBinsIndex + total_amount + tolls_amount + pickup_longitude + pickup_latitude + dropoff_longitude + dropoff_latitude + tip_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\n#featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE RANDOM FOREST ESTIMATOR\nrandForest = RandomForestRegressor(featuresCol = 'features', labelCol = 'label', numTrees=10,\n                                   featureSubsetStrategy=\"auto\",impurity='variance', maxDepth=4, maxBins=100)\n\n## Fit model, with formula and other transformations\nmodel = Pipeline(stages=[regFormula,randForest]).fit(trainData)\n\n## SAVE MODEL\ndatestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"RandomForestRegressionModel_\" + datestamp;\nrandForestDirfilename = modelDir + fileName;\nmodel.save(randForestDirfilename)\n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = model.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%sql -q -o predictionsPD\n\ndataDir3 = '../output/kaggle/working3/';\n\nquery = \"SELECT * from tmp_results\"\n\nquery_df = spark.sql(query)\n\n# REGISTER JOINED TRIP-FARE DF IN SQL-CONTEXT\nquery_df.createOrReplaceTempView(\"tmp_results2\")\n\nquery_dfwr = query_df\n\ntrainfilename3 = dataDir3 + \"TrainDatafinal\";\n\nquery_dfwr.write.mode(\"overwrite\").parquet(trainfilename3)\n\npredictionsPD = pd.read_parquet(trainfilename3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%local\n\n\nax = predictionsPD.plot(kind='scatter', figsize = (5,5), x='label', y='prediction', color='blue', alpha = 0.25, label='Actual vs. predicted');\nfit = np.polyfit(predictionsPD['label'], predictionsPD['prediction'], deg=1)\nax.set_title('Actual vs. Predicted Tip Amounts ($)')\nax.set_xlabel(\"Actual\"); ax.set_ylabel(\"Predicted\");\nax.plot(predictionsPD['label'], fit[0] * predictionsPD['label'] + fit[1], color='magenta')\nplt.axis([-1, 15, -1, 15])\nplt.show(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain = trainData\ndftrain.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save('../output/kaggle/working5/train1.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftest = testData\ndftest.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save('../output/kaggle/working5/test1.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = h2o.import_file('../output/kaggle/working5/train1.csv')\ntest = h2o.import_file('../output/kaggle/working5/test1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.columns\ny = \"fare_amount\"\nx.remove(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainDataH20 = trainData\n# x = trainDataH20.drop('fare_amount')\n# y = \"fare_amount\"\n# testData = testData\n\n\n\nnfolds = 5\n\nmy_gbm = H2OGradientBoostingEstimator(ntrees=10,\n                                      max_depth=3,\n                                      min_rows=2,\n                                      learn_rate=0.2,\n                                      nfolds=nfolds,\n                                      fold_assignment=\"Modulo\",\n                                      keep_cross_validation_predictions=True,\n                                      seed=1)\nmy_gbm.train(x=x, y=y, training_frame=train)\n\n\n# Train and cross-validate a RF\nmy_rf = H2ORandomForestEstimator(ntrees=40,\n                                 nfolds=nfolds,\n                                 fold_assignment=\"Modulo\",\n                                 keep_cross_validation_predictions=True,\n                                 seed=1)\nmy_rf.train(x=x, y=y, training_frame=train)\n\n\n# Train a stacked ensemble using the GBM and GLM above\nensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_binomial\",\n                                       base_models=[my_gbm, my_rf])\nensemble.train(x=x, y=y, training_frame=train)\n\n\n# Eval ensemble performance on the test data\nperf_stack_train = ensemble.model_performance(train)\nperf_stack_test = ensemble.model_performance(test)\n\n\n# Compare to base learner performance on the train set\nperf_gbm_train = my_gbm.model_performance(train)\nperf_rf_train = my_rf.model_performance(train)\n\n# Compare to base learner performance on the test set\nperf_gbm_test = my_gbm.model_performance(test)\nperf_rf_test = my_rf.model_performance(test)\n\n\npred = ensemble.predict(test)\n\n#///////////////////////////////////////////\n\n# 2. Generate a random grid of models and stack them together\n\n# Specify GBM hyperparameters for the grid\nhyper_params = {\"learn_rate\": [0.01, 0.03],\n                \"max_depth\": [4,5],\n                \"sample_rate\": [0.7,0.8],\n                \"col_sample_rate\": [0.2,0.3,0.7,0.8]}\nsearch_criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 2, \"seed\": 1}\n\n# Train the grid\ngrid = H2OGridSearch(model=H2OGradientBoostingEstimator(ntrees=3,\n                                                        seed=1,\n                                                        nfolds=nfolds,\n                                                        fold_assignment=\"Modulo\",\n                                                        keep_cross_validation_predictions=True),\n                     hyper_params=hyper_params,\n                     search_criteria=search_criteria,\n                     grid_id=\"gbm_grid_binomial\")\ngrid.train(x=x, y=y, training_frame=train)\n\n# Train a stacked ensemble using the GBM grid\nensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_gbm_grid_binomial\",\n                                       base_models=grid.model_ids)\nensemble.train(x=x, y=y, training_frame=train)\n\n\n# Eval ensemble performance on the train data\nperf_stack_grid_train = ensemble.model_performance(train)\n\n\n# Eval ensemble performance on the test data\nperf_stack_grid_test= ensemble.model_performance(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DEFINE RANDOM FOREST MODELS\nrandForest = RandomForestRegressor(featuresCol = 'features', labelCol = 'label',\n                                   featureSubsetStrategy=\"auto\",impurity='variance', maxBins=100)\n\n## DEFINE MODELING PIPELINE, INCLUDING FORMULA, FEATURE TRANSFORMATIONS, AND ESTIMATOR\npipeline = Pipeline(stages=[regFormula, randForest])\n\n## DEFINE PARAMETER GRID FOR RANDOM FOREST\nparamGrid = ParamGridBuilder() \\\n    .addGrid(randForest.numTrees, [10, 25, 50]) \\\n    .addGrid(randForest.maxDepth, [3, 5, 7]) \\\n    .build()\n\n## DEFINE CROSS VALIDATION\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(metricName=\"rmse\"),\n                          numFolds=3)\n\n## TRAIN MODEL USING CV\ncvModel = crossval.fit(trainData)\n\n## PREDICT AND EVALUATE TEST DATA SET\npredictions = cvModel.transform(testData)\nevaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\nr2 = evaluator.evaluate(predictions)\nprint(\"R-squared on test data = %g\" % r2)\n\n## SAVE THE BEST MODEL\ndatestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"CV_RandomForestRegressionModel_\" + datestamp;\nCVDirfilename = modelDir + fileName;\ncvModel.bestModel.save(CVDirfilename);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import org.apache.spark.ml.regression.{StackingClassifier, RandomForestRegressor, LinearRegression}\n\nnew StackingClassifier()\n        .setBaseLearners(Array(new DecisionTreeClassifier(), new RandomForestClassifier())) //Base learners used by the meta-estimator.\n        .setStacker(new DecisionTreeClassifier()) //Learner that will combine the predictions of base learners.\n        .setParallelism(4) //Number of base learners trained at the same time.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"savedModel = PipelineModel.load(randForestDirfilename)\n\npredictions = savedModel.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## READ IN DATA FRAME FROM CSV\ntaxi_valid_df = spark.read.csv(path=taxi_valid_file_loc, header=True, inferSchema=True)\ntaxi_valid_df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## READ IN DATA FRAME FROM CSV\ntaxi_valid_df = spark.read.csv(path=taxi_valid_file_loc, header=True, inferSchema=True)\n\n## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\ntaxi_df_valid_cleaned = taxi_valid_df.drop('medallion').drop('hack_license').drop('store_and_fwd_flag').drop('pickup_datetime')\\\n    .drop('dropoff_datetime').drop('pickup_longitude').drop('pickup_latitude').drop('dropoff_latitude')\\\n    .drop('dropoff_longitude').drop('tip_class').drop('total_amount').drop('tolls_amount').drop('mta_tax')\\\n    .drop('direct_distance').drop('surcharge')\\\n    .filter(\"passenger_count > 0 and passenger_count < 8 AND payment_type in ('CSH', 'CRD') \\\n    AND tip_amount >= 0 AND tip_amount < 30 AND fare_amount >= 1 AND fare_amount < 150 AND trip_distance > 0 \\\n    AND trip_distance < 100 AND trip_time_in_secs > 30 AND trip_time_in_secs < 7200\" )\n\n## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\ntaxi_df_valid_cleaned.createOrReplaceTempView(\"taxi_valid\")\n\n### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\nsqlStatement = \"\"\" SELECT *, CASE\n     WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN \"Night\" \n     WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN \"AMRush\" \n     WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN \"Afternoon\"\n     WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN \"PMRush\"\n    END as TrafficTimeBins\n    FROM taxi_valid\n\"\"\"\ntaxi_df_valid_with_newFeatures = spark.sql(sqlStatement)\n\n## APPLY THE SAME TRANSFORATION ON THIS DATA AS ORIGINAL TRAINING DATA\nencodedFinalValid = Pipeline(stages=[sI1, sI2, sI3, sI4]).fit(taxi_df_train_with_newFeatures).transform(taxi_df_valid_with_newFeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LOAD SAVED MODEL, SCORE VALIDATION DATA, AND EVALUATE\nsavedModel = PipelineModel.load(CVDirfilename)\npredictions = savedModel.transform(encodedFinalValid)\nr2 = evaluator.evaluate(predictions)\nprint(\"R-squared on validation data = %g\" % r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"Predictions_CV_\" + datestamp;\npredictionfile = dataDir + fileName;\npredictions.select(\"label\",\"prediction\").write.mode(\"overwrite\").csv(predictionfile)\nspark.stop()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}