{"cells":[{"metadata":{},"cell_type":"markdown","source":"---\n---\n# 1) General Model Investigation\n\nPurpose:\n* Explore models with stratified, 10-fold cross validation using pycaret\n* Select 3 best models for further optimization"},{"metadata":{},"cell_type":"markdown","source":"---\n# 2) Installs & Imports\nThe pycaret module is not native and must be fully installed."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycaret","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom pycaret.classification import *\nimport matplotlib.pyplot as plt  \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# 3) Load & Format Data\nPycaret performs stratified k-fold cross validation naturally, so there is no need to split into training and validation groups. Pixel values are normalized before model creation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the training data\ntrain = pd.read_csv('../input/overheadmnist/version2/train.csv')\ntrain.dropna(axis = 0, inplace = True)\ntrain.iloc[:, 1:] /= 255.\n\n# Check for missing values\nprint(train.head().iloc[:, :5])\nprint(f'\\nThere are {train.isna().sum().sum()} missing examples.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# 4) Model Creation\n* This process can take several minutes\n* Reduce folds to avoid notebook timeout"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_setup = setup(data = train, target = 'label', n_jobs = -1, \n                     session_id = 42, log_data = True, verbose = True, \n                     fold = 3, use_gpu = True, silent = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# 5) Compare Classification Models\n* This takes several hours for large data sets\n* ***GPU REQUIRED***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return parameters for top 3 models\nmodel_comp = compare_models(n_select = 3, verbose = True)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# 6) Results & Discussion\n* CatBoost classifier has very large training time\n* Gradient Boosting family has best performance\n* Generality is not lost by reducing number of cross-validation folds\n\nThe increased number of examples in Version 2 changes the performance from the initial evaluation in the first. Here we see SVM perform poorly, while the gradient boosting models excel. Available models and hyper-parameters used are displayed below."},{"metadata":{"trusted":true},"cell_type":"code","source":" # View parameters in the final models\nmodel_comp     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# 7) Conclusion\n> 1. CatBoost ---> .8285\n> 2. Light Gradient Boosting Machine ---> .7998\n> 3. Extreme Gradient Boosting ---> .7921\n\nDue to large training time, it may not be worth using CatBoost to get a 3% increase in accuracy. The most promising is xgboost, which performed just below lightgbm but in one-third the time."},{"metadata":{},"cell_type":"markdown","source":"## Next Steps\n* Optimize top three models\n* Test results of image formatting\n* Explore feature engineering\n---\n---"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}