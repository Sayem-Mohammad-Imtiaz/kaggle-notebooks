{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let's Predict Strokes\nThe goal of this workbook is to take my rando background of data analytics, data science, and other random techniques and try and come up with the most successful (or **a successful**) algorithm to predict strokes.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#Kaggle introductory material\n# ----------------------------------------------------------------------------------------------------------------------\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# ----------------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strokedata = pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The stroke dataset contains 5110 data points each with 12 attributes to describe them.\n## All Attributes\n* id - ID of patient/datapoint\n* gender - mostly boolean - one other\n* age\n* hypertension - boolean\n* heart_disease - boolean\n* ever_married - boolean\n* work_type\n* Residence_type - boolean\n* avg_glucose_level\n* bmi\n* smoking_status\n* stroke - boolean\n\nTo get to know the data, I'll be using the 'describe' method to get an overview of our quantitative attributes and then 'value_counts' to get an idea of our boolean data and qualititive attributes.\n\nThe below table lists the attributes pulled in with the describe method and I listed the attributes I specifically listed in the 'value_counts' method\n\n| All Attributes   | 'describe' data  |'value_counts' data|\n| -----------      | -----------      |-----------        |\n| id               | id               |gender             |\n| gender           | age              |hypertension       |\n| age              | hypertension     |heart_disease      |\n| hypertension     | heart_disease    |ever_married       |\n| heart_disease    | avg_glucose_level|work_type          |\n| ever_married     | bmi              |Residence_type     |\n| work_type        | stroke           |smoking_status     |\n| Residence_type   |                  |stroke             |\n| avg_glucose_level|                  |                   |\n| bmi              |                  |                   |\n| smoking_status   |                  |                   |\n| stroke           |                  |                   |\n### ","metadata":{}},{"cell_type":"code","source":"strokedata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In addition to describe and value counts data, we should probably check for any Na data.","metadata":{}},{"cell_type":"code","source":"display(strokedata.isna().sum())\ndisplay(201/5110)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And apparently, all our NaN data is in BMI.  3.9% of our data has an N/A BMI.  To preserve the accuracy of our model, I'm extremely hesitant to keep this data by using an average BMI.  Especially knowing that BMI could potentially be a large predictor.  I think we nix this data and don't look back.\n\n","metadata":{}},{"cell_type":"code","source":"strokedata = strokedata.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vcattributes = ['gender','hypertension','heart_disease','ever_married','work_type','Residence_type','smoking_status','stroke']\n# want to get rid of ID, age, avg_glucose_level, bmi\n\nfor vcattribute in vcattributes:\n    print(strokedata[[vcattribute]].value_counts(),'\\n','-------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we've gotten a good look at what the data is, what values it has, and what type each attribute is, lets look at the best way to predict our data.\nHow to decide what algorithm to use.... good question.  Somebody who has created predictive algorithms may have keen insight into this issue, but I have 0 to no experience in this.  In the abscence somebody with experience recommending a path, a quick google search reveals this paper from India written by Ritabrata Maiti (https://arxiv.org/ftp/arxiv/papers/1802/1802.07756.pdf).  On page four there is a diagram by SciKit-Learn which details a good starting place for beginners on which algorithms to choose.  For additional reading about different algorithms by a professional, I will likely go back and refer to the article by Mr. Maiti.\n![Machine Learning Map by scikit-learn](https://scikit-learn.org/stable/_static/ml_map.png)\n\n\n## To start, I'm going to focus on K-Nearest Neighbors and then SVC using the boolean and numeric data that we have.\nThe goal: To predict stroke based on whether the patient has hypertension, heart disease, being married and residence.\n*this goal largely conceived based on my skill level.  I'm hoping that minimal data preprocessing and using boolean and numeric only data will be easier to work with\n* id - ID of patient/datapoint\n* hypertension - boolean\n* heart_disease - boolean\n* ever_married - boolean\n* Residence_type - boolean\n* stroke - boolean\n\n","metadata":{}},{"cell_type":"markdown","source":"First step, convert ever_married to 1 and 0.  1 = yes, 0 = no\nSecond step, convert Residence_type to 1 and 0. 1 = urban, 0 = rural","metadata":{}},{"cell_type":"code","source":"strokedata['ever_married']= strokedata['ever_married'].map(dict(Yes=1, No=0))\nstrokedata[['ever_married']].value_counts() # double check make sure it worked.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strokedata['Residence_type']= strokedata['Residence_type'].map(dict(Urban=1, Rural=0))\nstrokedata[['Residence_type']].value_counts() # double check make sure it worked.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"booleandata = strokedata[['id','hypertension','heart_disease','ever_married','Residence_type','stroke']]\n\nboolandnumeric = strokedata[['age','hypertension','heart_disease','avg_glucose_level','ever_married','Residence_type','bmi']]\nstroke = strokedata['stroke']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boolandnumeric #missing gender, work_type, id, and smokeing status. argueably important but that involves learning more. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing pipeline includes StandardScaler, fit, and transform.\n*process adapted from a great article found here:* https://towardsdatascience.com/how-to-find-the-optimal-value-of-k-in-knn-35d936e554eb\n\n* StandardScaler\n    * Standardize features by removing the mean and scaling to unit variance. The standard score of a sample x is calculated as: \n    \n      z = (x - u) / s \n      \n      where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n* fit\n    * Compute the mean and std to be used for later scaling.\n* transform\n    * Perform standardization by centering and scaling\n","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(boolandnumeric).transform(boolandnumeric.astype(float))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, stroke, test_size=0.2, random_state=4)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n#Train Model and Predict\nbest = 0\nfor k in range(1,20):\n    k = k  \n    neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\n    Pred_y = neigh.predict(X_test)\n    if best < metrics.accuracy_score(y_test, Pred_y):\n        best = metrics.accuracy_score(y_test, Pred_y)\n        bestk = k\n    print(\"Accuracy of model at K= \",k,\" is\",metrics.accuracy_score(y_test, Pred_y))\n\nprint(\"Most accurate model at K= \",bestk,\" is\",best)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Based on the above code, our most accurate model is at k = 6 so we will use that going forward.","metadata":{}},{"cell_type":"code","source":"#original code was off by one.  The index is -1 then the i value.  the minimum error is at k=6\nerror_rate = []\nfor i in range(1,40):\n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit(X_train,y_train)\n pred_i = knn.predict(X_test)\n error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:\",min(error_rate),\"at K =\",error_rate.index(min(error_rate))+1) #see above comment. this +1 is to fix this issue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The error in the original code can be plainly seen below.  \nThe minimum can be found at index 5 but the i value above started at 1 (for the sake of the algorithm, k cannot = 0).\n\nWhere index = 0, k = 1, and the error rate is 0.07331975560081466\n\nWhere index = 1, k = 2, and the error rate is 0.04276985743380855,\n\nWhere index = 2, k = 3, and the error rate is 0.045824847250509164,\n\nWhere index = 3, k = 4, and the error rate is 0.04073319755600815,\n\nWhere index = 4, k = 5, and the error rate is 0.04175152749490835,\n\nWhere index = 5, k = 6, and the error rate is 0.03767820773930754 ------**This is the minimum value for the error at k = 6**\n\nWhere index = 6, k = 7, and the error rate is 0.04175152749490835","metadata":{}},{"cell_type":"code","source":"error_rate[0:7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 6\nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nPred_y = neigh.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So... turns out, yay we have a model that is largely accurate based on the parameters that were entered and the data that we have.  If I find another dataset with these parameters I'll test it out.  still an error rate of 3% but that isn't too shabby. However, lets try the SVC algorithm.  I also kind of want to see if I can figure out a system to \"weight\" how much each attribute effects the accuracy of the model.  Which is the strongest predictor as it were.\n\nAn individual comment list on this question: https://stackoverflow.com/questions/35815992/how-to-find-out-weights-of-attributes-in-k-nearest-neighbors-algorithm/35816344 indicated that the inverse of the distance of a given point could tell me how much the algorithm weights it for that point. \n\n# to be continued","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}