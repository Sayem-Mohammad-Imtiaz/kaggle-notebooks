{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import copy\ndf=pd.read_csv(\"/kaggle/input/electric-motor-temperature/pmsm_temperature_data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# derived features\ndf['i_s']=np.sqrt(df['i_d']**2+df['i_q']**2)\ndf['u_s']=np.sqrt(df['u_d']**2+df['u_q']**2)\ndf['s']=1.5*df['i_s']*df['u_s']\ntarget_features = ['pm', 'stator_tooth', 'stator_yoke', 'stator_winding']\ndel df['torque']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs=['ambient','coolant','u_d','u_q','motor_speed','i_d','i_q','i_s','u_s','s','profile_id']\ntarget=['pm','stator_yoke','stator_tooth','stator_winding','profile_id']\n#keep a load profile seperate from training\nval_ind=df['profile_id']==20\n#train index\ntr_ind=df['profile_id']!=20\n\n#seperate training data\ndf_tr=df[tr_ind]\n#seperate test data\ndf_val=df[val_ind].reset_index()\ndel df_val['index']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spans = [6360, 3360, 1320, 9480]  # these values correspond to cutoff-frequencies in terms of low pass filters, or half-life in terms of EWMAs, respectively\n#spans=[600,2200,4200,8800]\nspans=[500,2161,4000,8890]\n#spans=[1900]\nmax_span = max(spans)\nenriched_profiles = []\nfor p_id, p_df in df_tr.groupby(['profile_id']):\n    target_df = p_df.loc[:, target_features].reset_index(drop=True)\n    # pop out features we do not want to calculate the EWMA from\n    p_df = p_df.drop(target_features + ['profile_id'], axis=1).reset_index(drop=True)\n    \n    # prepad with first values repeated until max span in order to get unbiased EWMA during first observations\n    prepadding = pd.DataFrame(np.zeros((max_span, len(p_df.columns))),\n                              columns=p_df.columns)\n    temperature_cols = [c for c in ['ambient', 'coolant'] if c in df]\n    prepadding.loc[:, temperature_cols] = p_df.loc[0, temperature_cols].values\n\n    # prepad\n    prepadded_df = pd.concat([prepadding, p_df], axis=0, ignore_index=True)\n    ewma = pd.concat([prepadded_df.ewm(span=s).mean().rename(columns=lambda c: f'{c}_ewma_{s}') for s in spans], axis=1).astype(np.float32)\n    ewma = ewma.iloc[max_span:, :].reset_index(drop=True)  # remove prepadding\n    assert len(p_df) == len(ewma) == len(target_df), f'{len(p_df)}, {len(ewma)}, and {len(target_df)} do not match'\n    new_p_df = pd.concat([p_df, ewma, target_df], axis=1)\n    new_p_df['profile_id'] = p_id\n    enriched_profiles.append(new_p_df.dropna())\nenriched_df = pd.concat(enriched_profiles, axis=0, ignore_index=True)  \n\n# normalize\n#save p_id\np_ids = enriched_df['profile_id']\n#p_ids = enriched_df.pop('profile_id')\nscaler = StandardScaler()\nenriched_df = pd.DataFrame(scaler.fit_transform(enriched_df), columns=enriched_df.columns)\nenriched_df['profile_id']=p_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enriched_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(enriched_df['i_d'][:10000])\nplt.plot(enriched_df['i_d_ewma_9480'][:10000])\nplt.plot(enriched_df['i_d_ewma_1320'][:10000])\nplt.plot(enriched_df['i_d_ewma_6360'][:10000])\nplt.plot(enriched_df['i_d_ewma_3360'][:10000])\n#plt.plot(enriched_df['i_d_ewma_620'][:10000])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_rg=enriched_df[target_features]\nfor name in target_features:\n    del enriched_df[name]\nprint(target_rg.shape,enriched_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targ=df_val[target_features]\n#spans=[1900]\n#spans = [6360, 3360, 1320, 9480]  # these values correspond to cutoff-frequencies in terms of low pass filters, or half-life in terms of EWMAs, respectively\n#spans=[600,2200,4200,8800]\nspans=[500,2161,4000,8890]\nmax_span = max(spans)\nenriched_profiles = []\nfor p_id, p_df in df_val.groupby(['profile_id']):\n    target_df = p_df.loc[:, target_features].reset_index(drop=True)\n    # pop out features we do not want to calculate the EWMA from\n    p_df = p_df.drop(target_features + ['profile_id'], axis=1).reset_index(drop=True)\n    \n    # prepad with first values repeated until max span in order to get unbiased EWMA during first observations\n    prepadding = pd.DataFrame(np.zeros((max_span, len(p_df.columns))),\n                              columns=p_df.columns)\n    temperature_cols = [c for c in ['ambient', 'coolant'] if c in df]\n    prepadding.loc[:, temperature_cols] = p_df.loc[0, temperature_cols].values\n\n    # prepad\n    prepadded_df = pd.concat([prepadding, p_df], axis=0, ignore_index=True)\n    ewma = pd.concat([prepadded_df.ewm(span=s).mean().rename(columns=lambda c: f'{c}_ewma_{s}') for s in spans], axis=1).astype(np.float32)\n    ewma = ewma.iloc[max_span:, :].reset_index(drop=True)  # remove prepadding\n    assert len(p_df) == len(ewma) == len(target_df), f'{len(p_df)}, {len(ewma)}, and {len(target_df)} do not match'\n    new_p_df = pd.concat([p_df, ewma, target_df], axis=1)\n    new_p_df['profile_id'] = p_id\n    enriched_profiles.append(new_p_df.dropna())\nenriched_df_val = pd.concat(enriched_profiles, axis=0, ignore_index=True)  \n\n# normalize\np_ids = enriched_df_val.pop('profile_id')\nscaler = StandardScaler()\nenriched_df_val = pd.DataFrame(scaler.fit_transform(enriched_df_val), columns=enriched_df_val.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(enriched_df_val.shape)\nfor name in target_features:\n    del enriched_df_val[name]\nprint(targ.shape,enriched_df_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_val={}\nfor name in targ.columns:\n    if name!='profile_id':\n        a=min(targ[name])\n        b=max(targ[name])\n        scale_val[name+\"min\"]=a\n        scale_val[name+\"max\"]=b\n        targ[name]=(targ[name]-a)/(b-a)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ols = LinearRegression(fit_intercept=False)\nols.fit(enriched_df,target_rg)\nols.score(enriched_df,target_rg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recon_predicted(y):\n    k=0\n    out_ls=['pm','stator_yoke','stator_tooth','stator_winding']\n    range_ls=[100,20,100,20,110,20,125,20]\n    for i,name in enumerate(out_ls):\n        a,b=min(df[name]),max(df[name])\n        mat_ls=[[a,1],[b,1]]\n        A=np.array(mat_ls)\n        mx,mn=range_ls[k],range_ls[k+1]\n        B=np.array([mn,mx])\n        X=np.linalg.inv(A) @ B\n        #print(A,B,X)\n        std,mean=X[0],X[1]\n        k+=2\n        y[:,i]=y[:,i]*std+mean\n    return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict train set temp "},{"metadata":{"trusted":true},"cell_type":"code","source":"y=ols.predict(enriched_df)\ny=recon_predicted(y)\ndf_tr[target_features]=recon_predicted(np.array(df_tr[target_features]))\ndef cal_map(y,y_hat):\n    mse_arr=sum(abs(y-y_hat))/y.shape[0]\n    return mse_arr\navg_map=[]\nout_ls=['pm','stator_yoke','stator_tooth','stator_winding']\nfor i,name in enumerate(out_ls):\n    map1=cal_map(y[:,i],df_tr[name])\n    avg_map.append(map1)\nprint(avg_map,sum(avg_map)/4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict val set temp"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=ols.predict(enriched_df_val)\ny=recon_predicted(y)\ndf_val[target_features]=recon_predicted(np.array(df_val[target_features]))\ndef cal_map(y,y_hat):\n    mse_arr=sum(abs(y-y_hat))/y.shape[0]\n    return mse_arr\navg_map=[]\nfig, big_axes = plt.subplots( figsize=(25, 4) , nrows=1, ncols=4, sharey=False) \nout_ls=['pm','stator_yoke','stator_tooth','stator_winding']\nfor i,name in enumerate(out_ls):\n    map1=cal_map(y[:,i],df_val[name])\n    avg_map.append(map1)\n    title=name\n    #ax = fig.add_subplot(1,4,i+1)\n    #plt.title(title)\n    \n    #plt.plot(y[:,i],label='Predicted by Linear regressor')\n    #plt.plot(df_val[name],label='Actual')\n    big_axes[i].set_title(title)\n    big_axes[i].plot(y[:,i],label='Linear regressor')\n    big_axes[i].plot(df_val[name],label='Actual')\n    k = int(len(df_val)> 4*3600) + 1\n    ticks_loc=df_val[name].index.values[::k*3600]\n    labels=[(y-ticks_loc[0])/2/3600 for y in ticks_loc]\n    big_axes[i].set_xticklabels(labels)\n    big_axes[i].set_xlabel(\"Time in hrs\")\n    big_axes[i].set_ylabel(\"Temperature\")\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"print(avg_map,sum(avg_map)/4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_val={}\ndef prepad_zero(window,out):\n    #target\n    df_y=enriched_df[out]\n    #input\n    df_x_=enriched_df.drop(target_features,axis=1).reset_index(drop=True)\n    \n    df_x=pd.DataFrame()\n    print(df_x_.shape)\n    #print(df_x_.head())\n    for p_id, p_df in df_x_.groupby(['profile_id']):\n        #remove profile_id column\n        #p_df = p_df.drop(['profile_id'], axis=1).reset_index(drop=True)\n        p_df=p_df.reset_index()\n        p_df=p_df.drop(['index'],axis=1)\n        prepadding = pd.DataFrame(np.zeros((window-1, len(p_df.columns))),\n                                  columns=p_df.columns)\n        #prepad with actual starting value of temperature and keep profile id same\n        prepadding['ambient']=(p_df['ambient'][0])\n        prepadding['coolant']=(p_df['coolant'][0])\n        prepadding['profile_id']=p_id\n        p_df=pd.concat([prepadding,p_df],ignore_index=True)\n        \n        # add to empty dataframe\n        df_x=pd.concat([df_x,p_df],ignore_index=True)\n    print(\"x\",df_x.shape,\"y\",df_y.shape)    \n    # scale target temperature to the (0,1) range\n    scale_val={}\n    for name in out:\n        if name!='profile_id':\n            a=min(df_y[name])\n            b=max(df_y[name])\n            scale_val[name+\"min\"]=a\n            scale_val[name+\"max\"]=b\n            df_y[name]=(df_y[name]-a)/(b-a)\n    #print(df_x_train.shape,df_y.shape)\n    \n    return df_x,df_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seq_idwise(df_x_train,df_y,window,out_length):\n    #make an aray of input varibale as a sequence\n    profile=list(set(df_x_train['profile_id'].values))\n    x=[]\n    y=[]\n    window=window\n    for i,_id in enumerate(profile):\n        p_df=df_x_train[df_x_train['profile_id']==_id]\n        y_df=df_y[df_y['profile_id']==_id]\n        \n        # assert if these are not equal\n        assert p_df.shape[0]-(window-1)==y_df.shape[0]\n    \n        #remove profile id column\n        y_df =y_df.drop(['profile_id'], axis=1).reset_index(drop=True)\n        p_df =p_df.drop(['profile_id'], axis=1).reset_index(drop=True)\n\n\n        # convert all input data into a sequence of shape (p_df.shape[0],8,10)\n        seq=[]\n        # -7 is done becuase we need to loop over original profile_id readings\n        for j in range(p_df.shape[0]-(window-1)):\n            seq.append(np.array(p_df[j:window+j]).reshape(window,30))\n\n        x.append(np.array(seq).reshape(p_df.shape[0]-(window-1),window,30))        \n\n        # add all data with the shape (y_df.shape[0],out_length)\n        y.append(np.array(y_df).reshape(y_df.shape[0],out_length))\n        \n        # assert if shape is not same\n        assert y[i].shape[0]==x[i].shape[0], '{},{},{}'.format(i,y[i].shape[0],x[i].shape[0])\n        \n        #print(len(seq),y_df.shape)\n    del df_x_train\n    del df_y\n    x_arr=np.concatenate(x)\n    y_arr=np.concatenate(y)\n    return x_arr,y_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n#from keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nfrom tqdm import tqdm_notebook as tqdm\nimport pickle\nimport gc\nfrom keras.callbacks import EarlyStopping\n\nimport keras.backend as K\nimport keras.layers\nfrom keras import optimizers\nfrom keras.engine.topology import Layer\nfrom keras.layers import Activation, Lambda\nfrom keras.layers import Conv1D, SpatialDropout1D\nfrom keras.layers import Convolution1D, Dense\nfrom keras.models import Input, Model\nfrom typing import List, Tuple\n\nimport keras.optimizers as opts\nimport keras.regularizers as regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_common_layers(z,dropout_rate):\n    activation='relu'\n    dropout_layer = keras.layers.AlphaDropout if activation == 'selu' else keras.layers.SpatialDropout1D\n    batchnorm=True\n    if batchnorm:\n        z = keras.layers.BatchNormalization()(z)\n    z = keras.layers.Activation(activation)(z)\n    z = dropout_layer(dropout_rate)(z)\n    return z\ndef seperate_model(name,arch='res',kernel_size= 6,dilation_start_rate= 1,lr_rate=1.4e-4,n_layers=4,t_steps=32,nb_filters=121,\n              reg_rate=1e-8,dropout_rate=0.29,batch_size=128,out_length=1):\n\n    regs = {'kernel_regularizer': regularizers.l2(reg_rate),\n            'bias_regularizer': regularizers.l2(reg_rate),\n            'activity_regularizer': regularizers.l2(reg_rate)}\n    x = keras.layers.Input(shape=(t_steps,50),batch_size=batch_size,name=\"Input_Measurements\")\n    y = x\n    for i in range(n_layers):\n        dilation_rate = dilation_start_rate * (2 ** i)\n        if i % 2 == 0 and arch == 'res':  # every two layers\n            shortcut = y\n\n        y = keras.layers.Conv1D(nb_filters, kernel_size, padding='causal',\n                          dilation_rate=dilation_rate,\n                          activation=None,\n                           **regs)(y)\n        y = add_common_layers(y,dropout_rate)\n\n        if i % 2 == 1 and arch == 'res':  # every two layers (anti-cyclic)\n            shortcut = keras.layers.Conv1D(nb_filters, kernel_size=1,\n                                     padding='causal',\n                                     dilation_rate=dilation_rate,\n                                     activation=None,\n                                      **regs)(shortcut)\n            y = keras.layers.add([shortcut, y])\n\n    y = keras.layers.GlobalMaxPooling1D()(y)\n    y = keras.layers.Dense(out_length)(y)\n    opt=opts.Adam(lr=lr_rate)\n    tcn = Model(inputs=x, outputs=y)\n    tcn.compile(loss='mse', optimizer=opt,metrics=[keras.metrics.RootMeanSquaredError()])\n    tcn.summary()\n    return tcn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stator_target=['stator_yoke','stator_tooth','stator_winding','profile_id']\nrotor_target=['pm','profile_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#names={'rotor':1, 'stator':3}\n#names={'stator':3}\nnames={'rotor':1}\nrotor_conf={'kernel_size': 2,'dilation_start_rate' :1,\"lr_rate\": 1e-4,\"n_layers\":2,\"t_steps\":33,\"nb_filters\":126,\n              \"reg_rate\":1e-9,\"dropout_rate\":0.35,\"batch_size\":128,\"out_length\":1}\nstator_conf={'kernel_size': 6,'dilation_start_rate' :1,\"lr_rate\": 1.4e-4,\"n_layers\":4,\"t_steps\":32,\"nb_filters\":121,\n              \"reg_rate\":1e-8,\"dropout_rate\":0.29,\"batch_size\":128,\"out_length\":3}\nmodels_tcn={}\nbatch_size=128\nfor name in names:\n    if name=='rotor':\n        conf=rotor_conf\n        out=rotor_target\n    else:\n        conf=stator_conf\n        out=stator_target\n    mod=seperate_model(name,arch='res',**conf)\n    df_x_tr,df_y_tr=prepad_zero(conf['t_steps'],out)\n    del enriched_df\n    x_tr,y_tr=seq_idwise(df_x_tr,df_y_tr,conf['t_steps'],names[name])\n    print(x_tr.shape,y_tr.shape)\n    history=mod.fit(x_tr[:953984],y_tr[:953984],batch_size=128,epochs=10,steps_per_epoch=953984/batch_size,verbose=1,shuffle='batch')\n    plt.plot(history.history['loss'][1:],label=name)\n    plt.legend()\n    models_tcn[name]=mod\n    mod.save('./{}.h5'.format(name))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recon_rotor(y):\n    k=0\n    out_ls=['pm']\n    range_ls=[100,20]\n    for i,name in enumerate(out_ls):\n        a,b=min(df[name]),max(df[name])\n        mat_ls=[[a,1],[b,1]]\n        A=np.array(mat_ls)\n        mx,mn=range_ls[k],range_ls[k+1]\n        B=np.array([mn,mx])\n        X=np.linalg.inv(A) @ B\n        #print(A,B,X)\n        std,mean=X[0],X[1]\n        k+=2\n        y=y*(b-a)+a\n        y=y*std+mean\n    return y\ndef recon_stator(y):\n    k=0\n    out_ls=['stator_yoke','stator_tooth','stator_winding']\n    range_ls=[100,20,110,20,125,20]\n    for i,name in enumerate(out_ls):\n        a,b=min(df[name]),max(df[name])\n        mat_ls=[[a,1],[b,1]]\n        A=np.array(mat_ls)\n        mx,mn=range_ls[k],range_ls[k+1]\n        B=np.array([mn,mx])\n        X=np.linalg.inv(A) @ B\n        #print(A,B,X)\n        std,mean=X[0],X[1]\n        k+=2\n        y[:,i]=y[:,i]*(b-a)+a\n        y[:,i]=y[:,i]*std+mean\n    return y\ndef cal_map(y,y_hat):\n    map_arr=sum(abs(y-y_hat))/y.shape[0]\n    return map_arr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_batches(df_x,df_y,batch_size,window):\n    samples_per_epoch = df_y.shape[0]\n    number_of_batches = samples_per_epoch/batch_size\n    counter=0\n    p_count=0\n    profile=list(set(df_x['profile_id'].values))\n    #df_x=df_x.drop(['profile_id'], axis=1).reset_index(drop=True)\n    #df_y=df_y.drop(['profile_id'], axis=1).reset_index(drop=True)\n    k=0\n    \n    while(True):\n        #form batches groupby on profile id as every profile is prepaded with the zeros \n        X_list=[]\n        \n        pd_x=df_x[df_x['profile_id']==profile[k]]\n        pd_y=df_y[df_y['profile_id']==profile[k]]\n        pd_x=pd_x.drop(['profile_id'], axis=1).reset_index(drop=True)\n        pd_y=pd_y.drop(['profile_id'], axis=1).reset_index(drop=True)\n        assert pd_y.shape[0]==pd_x.shape[0]-window+1\n        #add extra window length\n        pd_x_=pd_x[batch_size*p_count:batch_size*(p_count+1)+window-1]\n        #assert pd_y.shape[0]==pd_x_.shape[0]-window+1\n        if pd_x_.shape[0]==batch_size+window-1:\n            for j in range(batch_size):\n                X_list.append(np.array(pd_x_[j:j+window]).astype(\"float32\"))\n        #convert the list to array\n            \"\"\"try:\n            X_batch=np.stack(X_list)\n            #.reshape(128,window,50)\n            except:\n            X_batch=np.stack(X_list)#.reshape(-1,window,50)\n            k+=1\n            start_flag=True\"\"\"\n            \n            X_batch=np.stack(X_list)\n            y_batch = np.array(pd_y[batch_size*p_count:batch_size*(p_count+1)]).astype('float32')\n            p_count+=1\n            assert X_batch.shape[0]==y_batch.shape[0]\n            yeild_flag=True\n        else:\n            k+=1\n            p_count=0\n            yeild_flag=False\n        \n        #print(\"profile:\",k,\" main counter:\",counter,\" p_counter \",p_count,\" \",X_batch.shape,y_batch.shape)\n        if yeild_flag==True:\n            yield X_batch,y_batch\n        #increase counter with every batch  \n        counter += 1\n        #restart counter to yeild data in the next epoch as well\n        #print(counter)\n\n        if counter >= number_of_batches-1:\n            counter = 0\n            k=0\n            p_count=0\n            #print('New epoch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names={'rotor':1, 'stator':3}\nrotor_conf={'kernel_size': 2,'dilation_start_rate' :1,\"lr_rate\": 1e-4,\"n_layers\":2,\"t_steps\":33,\"nb_filters\":126,\n              \"reg_rate\":1e-8,\"dropout_rate\":0.35,\"batch_size\":128,\"out_length\":names['rotor']}\nstator_conf={'kernel_size': 6,'dilation_start_rate' :1,\"lr_rate\": 1.4e-4,\"n_layers\":4,\"t_steps\":32,\"nb_filters\":121,\n              \"reg_rate\":1e-9,\"dropout_rate\":0.29,\"batch_size\":128,\"out_length\":names['stator']}\nmodels_tcn={}\nbatch_size=128\nnames=['rotor']\nfor name in names:\n    if name=='rotor':\n        conf=rotor_conf\n        out=rotor_target\n    else:\n        conf=stator_conf\n        out=stator_target\n    mod=seperate_model(name,arch='res',**conf)\n    df_x_tr,df_y_tr=prepad_zero(conf['t_steps'],out)\n    \n    gen=generate_batches(df_x_tr,df_y_tr,batch_size,conf['t_steps'])\n    history=mod.fit_generator(generator=gen,\n                steps_per_epoch=df_y_tr.shape[0]/batch_size,\n                epochs=10,\n                verbose=1,\n                shuffle=False)\n    plt.plot(history.history['loss'][0:],label=name)\n    plt.legend()\n    models_tcn[name]=mod\n    mod.save('./{}.h5'.format(name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names={'rotor':1, 'stator':3}\nrotor_conf={'kernel_size': 2,'dilation_start_rate' :1,\"lr_rate\": 1e-4,\"n_layers\":2,\"t_steps\":33,\"nb_filters\":126,\n              \"reg_rate\":1e-8,\"dropout_rate\":0.35,\"batch_size\":128,\"out_length\":names['rotor']}\nstator_conf={'kernel_size': 6,'dilation_start_rate' :1,\"lr_rate\": 1.4e-4,\"n_layers\":4,\"t_steps\":32,\"nb_filters\":121,\n              \"reg_rate\":1e-9,\"dropout_rate\":0.29,\"batch_size\":128,\"out_length\":names['stator']}\nmodels_tcn={}\nbatch_size=128\nnames=['stator']\nfor name in names:\n    if name=='rotor':\n        conf=rotor_conf\n        out=rotor_target\n    else:\n        conf=stator_conf\n        out=stator_target\n    mod=seperate_model(name,arch='res',**conf)\n    df_x_tr,df_y_tr=prepad_zero(conf['t_steps'],out)\n    \n    gen=generate_batches(df_x_tr,df_y_tr,batch_size,conf['t_steps'])\n    history=mod.fit_generator(generator=gen,\n                steps_per_epoch=df_y_tr.shape[0]/batch_size,\n                epochs=10,\n                verbose=1,\n                shuffle=False)\n    plt.plot(history.history['loss'][0:],label=name)\n    plt.legend()\n    models_tcn[name]=mod\n    mod.save('./{}.h5'.format(name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train set rotor"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_list=[]\nwindow=33\ntr_targ=enriched_df[target_features]\nfor name in tr_targ.columns:\n    a=min(tr_targ[name])\n    b=max(tr_targ[name])\n    tr_targ[name]=(tr_targ[name]-a)/(b-a)\nfor name in target_features:\n    del enriched_df[name]\ndel enriched_df['profile_id']\nprint(tr_targ.shape,enriched_df.shape)\nfor j in range(100000):\n    X_list.append(np.array(enriched_df[j:j+window]).astype(\"float32\").reshape(window,50))\n\nX_batch=np.asarray(X_list)#.reshape(128,8,10)\ny_batch = np.array(tr_targ[window-1:100000*(1)+window-1]).astype('float32')\nassert X_batch.shape[0]==y_batch.shape[0], \"length of input and target not match. X_batch {} y_batch {}\".format(X_batch.shape[0],y_batch.shape[0])\nassert X_batch.shape[1]==window,\" seq not made correctly\"\ny_pred=models_tcn['rotor'].predict(X_batch)\ny_batch=recon_rotor(y_batch[:,0])\ny_pred=recon_rotor(y_pred)\navg_map_pm=cal_map(y_pred.squeeze(),y_batch)\nplt.plot(y_pred,label='Predicted by TCN')\nplt.plot(y_batch,label='Actual')\nplt.legend()\nplt.ylabel('Temperature(C)')\nplt.xlabel(\"Time (sec)\")\nplt.title(\"pm\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train rotor error \",avg_map_pm,\"K\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## test set rotor"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_list=[]\nwindow=33\n\nfor j in range(40000):\n    X_list.append(np.array(enriched_df_val[j:j+window]).astype(\"float32\").reshape(window,50))\n\nX_batch=np.asarray(X_list)#.reshape(128,8,10)\ny_batch = np.array(targ[window-1:40000*(1)+window-1]).astype('float32')\nassert X_batch.shape[0]==y_batch.shape[0], \"length of input and target not match. X_batch {} y_batch {}\".format(X_batch.shape[0],y_batch.shape[0])\nassert X_batch.shape[1]==window,\" seq not made correctly\"\ny_pred=models_tcn['rotor'].predict(X_batch)\ny_batch=recon_rotor(y_batch[:,0])\ny_pred=recon_rotor(y_pred)\navg_map_pm=cal_map(y_pred.squeeze(),y_batch)\nplt.plot(y_pred,label='Predicted by TCN')\nplt.plot(y_batch,label='Actual')\nplt.legend()\nplt.ylabel('Temperature(C)')\nplt.xlabel(\"Time (sec)\")\nplt.title(\"pm\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_map_pm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train set stator"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_list=[]\nwindow=32\ntrgt=['stator_yoke','stator_tooth','stator_winding']\ntr_targ=enriched_df[target_features]\nfor name in tr_targ.columns:\n    a=min(tr_targ[name])\n    b=max(tr_targ[name])\n    tr_targ[name]=(tr_targ[name]-a)/(b-a)\n\nfor name in target_features:\n    del enriched_df[name]\ndel enriched_df['profile_id']\nprint(tr_targ.shape,enriched_df.shape)\nfor j in range(100000):\n    X_list.append(np.array(enriched_df[j:j+window]).astype(\"float32\").reshape(window,50))\n\nX_batch=np.asarray(X_list)#.reshape(128,8,10)\n\ny_batch = np.array(tr_targ[window-1:100000*(1)+window-1]).astype('float32')\n\nassert X_batch.shape[0]==y_batch.shape[0], \"length of input and target not match. X_batch {} y_batch {}\".format(X_batch.shape[0],y_batch.shape[0])\nassert X_batch.shape[1]==window,\" seq not made correctly\"\ny_pred=models_tcn['stator'].predict(X_batch)\ny_batch=recon_stator(y_batch[:,1:])\ny_pred=recon_stator(y_pred)\navg_map=cal_map(y_batch,y_pred)\nfor i,name in enumerate(trgt):\n    plt.plot(y_pred[:,i],label='Predicted by TCN')\n    plt.plot(y_batch[:,i],label='Actual')\n    plt.title(name)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train error \",avg_map,\" K\", \"Average stator error \",sum(avg_map)/3,\"K\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## test set "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_list=[]\nwindow=32\ntrgt=['stator_yoke','stator_tooth','stator_winding']\n\"\"\"df_x=df_x_tr[df_x_tr['profile_id']==65]\ndf_y=df_y_tr[df_y_tr['profile_id']==65]\ndf_x=df_x.drop('profile_id',axis=1).reset_index(drop=True)\ndf_y=df_y.drop('profile_id',axis=1).reset_index(drop=True)\n\"\"\"\nfor j in range(40000):\n    X_list.append(np.array(enriched_df_val[j:j+window]).astype(\"float32\").reshape(window,50))\n\nX_batch=np.asarray(X_list)#.reshape(128,8,10)\n#y_batch = np.array(df_y[window-1:30000*(1)+window-1]).astype('float32')\ny_batch = np.array(targ[window-1:40000*(1)+window-1]).astype('float32')\nassert X_batch.shape[0]==y_batch.shape[0], \"length of input and target not match. X_batch {} y_batch {}\".format(X_batch.shape[0],y_batch.shape[0])\nassert X_batch.shape[1]==window,\" seq not made correctly\"\ny_pred=models_tcn['stator'].predict(X_batch)\ny_batch=recon_stator(y_batch[:,1:])\ny_pred=recon_stator(y_pred)\navg_map=cal_map(y_batch,y_pred)\nfor i,name in enumerate(trgt):\n    plt.plot(y_pred[:,i],label='Predicted by TCN')\n    plt.plot(y_batch[:,i],label='Actual')\n    plt.title(name)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(avg_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_list=[]\nwindow=32\ntrgt=['stator_yoke','stator_tooth','stator_winding']\n\nfor j in range(40000):\n    X_list.append(np.array(enriched_df_val[j:j+window]).astype(\"float32\").reshape(window,50))\n\nX_batch=np.asarray(X_list)#.reshape(128,8,10)\ny_batch = np.array(targ[window-1:40000*(1)+window-1]).astype('float32')\nassert X_batch.shape[0]==y_batch.shape[0], \"length of input and target not match. X_batch {} y_batch {}\".format(X_batch.shape[0],y_batch.shape[0])\nassert X_batch.shape[1]==window,\" seq not made correctly\"\ny_pred=models_tcn['stator'].predict(X_batch)\n\nfor i,name in enumerate(trgt):\n    plt.plot(y_pred[:,i],label='Predicted by TCN')\n    plt.plot(y_batch[:,i],label='Actual')\n    plt.title(name)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}