{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThe main goal of this notebook is to run a NLP - Sentiment Analysis. The idea is to compare two models capable of differentiating good versus bad opinions on a specific topic. In this case, positive / negative movie reviews.<br>\nThe first one is a “Naïve Bayes Classifier”, which uses Bayes’ Theorem and conditional probability to classify reviews. The second is a classic “Logistic Regression”, based on the logit function. For further details check the APPENDIX and SKLearn documentation.<br>\nIn addition, this notebook compares two ways of vectorizing sentences. One is based only on words frequency, building a \"bag of words\". The other, “Term Frequency – Inverse Document Frequency”, incorporates the idea that too frequent terms (words) that are present on both classifications have “low differentiating power”, therefore receiving less weight. On the other hand, there are terms that appear more frequently on only one of the classifications, thus having a “high differentiating power” and receiving higher weight.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries and Dataset\n## Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# classic libraries\nimport pandas as pd\nimport numpy as np\n\n# Charts\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\nimport seaborn as sns\n\n# cleaning text \nimport nltk\nfrom string import punctuation\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom nltk import tokenize\n\n# vectorizer for model\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n# performance\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset\nThe dataset contains 50k movie reviews labeled as positive (50%) or negative (50%).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews=pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\nprint(reviews.shape)\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# % of positive and negative reviews\nrev_sent=reviews.groupby(['sentiment']).sentiment.count().to_frame('Count').reset_index()\nplt.pie(rev_sent['Count'],labels=rev_sent['sentiment'],autopct='%1.1f%%',colors=('#e64040','#40a1e6'))\nplt.title('Reviews %')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a column with binary classification positive (1) and negative (0)\nreviews['sentiment_bin']=reviews['sentiment'].replace(['positive','negative'],[1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning\nI'm making only a few changes. <br>\nFirst part:<br>\n(i) all words lowercase <br>\n(ii) removing \"stopwords\" (ie. pronouns and prepositions and others) <br>\n(iii) removing punctutation<br>\n\nSecond part:<br>\n(iv) Stemmer - The idea is to reduce the word inflection to its root or origin. For instance, reviewers => review<br>\n\nCheck the sentence comparison<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning text - 1\n\n# stopwords\nirrelevant_stuff=nltk.corpus.stopwords.words(\"english\")\n# stopwords + punctuation\npunct=['br','/><','/>','10','15','20','30','80','.<']\nfor p in punctuation:\n    punct.append(p)\nirrelevant_stuff=irrelevant_stuff+punct\n\n# function to split a sentence into a list of words\nsplit_token=tokenize.WordPunctTokenizer()\n\n# changes are: lower case / remove punctuation and stopwords\nclean_text=[]\nfor opinion in reviews['review']:\n    clean_list_words=[]\n    list_words=split_token.tokenize(opinion)\n    for word in list_words:\n        if word.lower() not in irrelevant_stuff:\n            clean_list_words.append(word.lower())\n    clean_text.append(' '.join(clean_list_words))\nreviews['clean_text_1']=clean_text\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning text - 2 \n\n# Stemming - reducing word inflections to the root or origin\nstemmer=SnowballStemmer(\"english\")\n\n# Stemming words\nclean_text=[]\nfor opinion in reviews['clean_text_1']:\n    clean_list_words=[]\n    list_words=split_token.tokenize(opinion)\n    for word in list_words:\n        clean_list_words.append(stemmer.stem(word))\n    clean_text.append(' '.join(clean_list_words))\nreviews['clean_text_2']=clean_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing sentences\nprint(' '.join(reviews['review'][0].split()[0:10]))\nprint(' '.join(reviews['clean_text_1'][0].split()[0:10]))\nprint(' '.join(reviews['clean_text_2'][0].split()[0:10]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the data (split train test/ vectorizer)\nBefore running any actual model, we need first to split the data between training and testing sample. This way it is possible to develop a model and later validate it. SKLearn provides an easy to use function “train_test_split”.\nAfter splitting the dataset, it is time to vectorize the sentences. This step is necessary because we cannot directly process “sentences/words”, but we can access the frequency of “words” in “sentences” for each classification (positive or negative review). The \"bag of words\" transforms the dataset into a matrix where each row represents a sentence and each column a word. The number in each row x column represents the frequency of a specific “word” in a specific “sentence” (there is an example below).\nAs for \"Term Frequency - Inverse Document Frequency\" it inversely weights the words based on their frequency (details are provided in the APPENDIX).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data for training and testing\ntrain,test,class_train,class_test =train_test_split(reviews['clean_text_2'],reviews['sentiment_bin'],random_state=100,train_size=0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming reviews into vectors, results in a sparse matrix with words as columns and \"sentence ID\" in rows\n# Bag of words\nvectorize=CountVectorizer()\nbag_of_words_train=vectorize.fit_transform(train)\nbag_of_words_test=vectorize.transform(test)\n\n# Term Frequency * Inverse Document Frequency model (TDIDF)\ntfidf=TfidfVectorizer()\ntf_train=tfidf.fit_transform(train)\ntf_test=tfidf.transform(test)\n\n\n# Just to have a rough idea what the vectorizer is doing\nsparse_matrix=pd.DataFrame.sparse.from_spmatrix(bag_of_words_train,columns=vectorize.get_feature_names())\nsparse_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\n## Naive Bayes Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_1_1 Multinominal Naive Bayes Classifier\nNBC=MultinomialNB()\nmodel_NBC=NBC.fit(bag_of_words_train,class_train)\nNBC_predict=model_NBC.predict(bag_of_words_test)\nacc_NBC=accuracy_score(class_test,NBC_predict)\nprint(f'The accuary for NBC was {acc_NBC*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(class_test,NBC_predict)\nprint(cm)\nprint(f'There are {cm[0,0]} true negatives,{cm[1,0]} false negatives, {cm[1,1]} true positives and {cm[0,1]} false positives,')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_1_2 Multinominal Naive Bayes Classifier + tfidf\nmodel_tf=NBC.fit(tf_train,class_train)\ntf_predict=model_tf.predict(tf_test)\nacc_NBC_tf=accuracy_score(class_test,tf_predict)\nprint(f'The accuary for NBC with TFIDF was {acc_NBC_tf*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(class_test,tf_predict)\nprint(cm)\nprint(f'There are {cm[0,0]} true negatives,{cm[1,0]} false negatives, {cm[1,1]} true positives and {cm[0,1]} false positives,')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_2_1 Logistic Regression\nreg_log=LogisticRegression(solver=\"lbfgs\")\nmodel=reg_log.fit(bag_of_words_train, class_train)\npredict_lr=model.predict(bag_of_words_test)\nacc_lr=reg_log.score(bag_of_words_test,class_test)\nprint(f'The accuary for Log Reg was {acc_lr*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(class_test,predict_lr)\nprint(cm)\nprint(f'There are {cm[0,0]} true negatives,{cm[1,0]} false negatives, {cm[1,1]} true positives and {cm[0,1]} false positives,')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_2_2 Logistic Regression + tfidf\nreg_log=LogisticRegression(solver=\"lbfgs\")\nmodel=reg_log.fit(tf_train, class_train)\npredict_lr_tf=model.predict(tf_test)\nacc_lr_tf=reg_log.score(tf_test,class_test)\nprint(f'The accuary for Log Reg with TFIDF was {acc_lr_tf*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(class_test,predict_lr_tf)\nprint(cm)\nprint(f'There are {cm[0,0]} true negatives,{cm[1,0]} false negatives, {cm[1,1]} true positives and {cm[0,1]} false positives,')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparison Chart\nTwo intereting points to be noted:<br>\n(i) TF IDF performed slightly better in NBC (almost the same) but significantly better in LR<br>\n(ii) In overall, Logistic Regression performed better than NBC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I got his from matplotlib documentation\nlabels =['NBC','LR']\n\nvec=[100*acc_NBC,100*acc_lr]\ntf=[100*acc_NBC_tf,100*acc_lr_tf]\n\n# label locations\nx = np.arange(len(labels)) \n# bar width\nwidth = 0.35  \n\nfig, axs = plt.subplots()\naxs.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n\nm1 =axs.bar(x - width/2, vec, width, label='Vec')\nm2 = axs.bar(x + width/2, tf, width, label='TF IDF')\n\naxs.set_ylabel('Accuracy (%)')\naxs.set_ylim([40, 100])\naxs.set_title('Accuracy by Model and Vector Type')\naxs.set_xticks(x)\naxs.set_xticklabels(labels) \naxs.legend(loc=8)\n\ndef autolabel(ms):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for m in ms:\n        height = m.get_height()\n        axs.annotate(f'{height:.3f}%',\n                    xy=(m.get_x() + m.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\nautolabel(m1)\nautolabel(m2)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Charts\n## Word Clouds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive reviews\npos_rev=reviews.query(\"sentiment=='positive'\")\npos_all_words=\" \".join([text for text in pos_rev['clean_text_2']])\n    \npos_rev_word_cloud=WordCloud(width=800, height=500,\n                         max_font_size=110,\n                         collocations=False).generate(pos_all_words)\n\n# negative reviews\nneg_rev=reviews.query(\"sentiment=='negative'\")\nneg_all_words=\" \".join([text for text in neg_rev['clean_text_2']])   \nneg_rev_word_cloud=WordCloud(width=800, height=500,\n                         max_font_size=110,\n                         collocations=False).generate(neg_all_words)\n\n# Plotting\nfig, axs=plt.subplots(1,2,figsize=(20,7))\n\n# Pos\naxs[0].imshow(pos_rev_word_cloud,interpolation='bilinear')\naxs[0].set_title('Positive Reviews',size=15)\n# Neg \naxs[1].imshow(neg_rev_word_cloud,interpolation='bilinear')\naxs[1].set_title('Negative Reviews',size=15)\n\nfig.suptitle('Wordcloud by Sentiment',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Frequency","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizer and n most frequent words\nsplit_token=tokenize.WhitespaceTokenizer()\nn=10\n\n# Positive review\npos_token=split_token.tokenize(pos_all_words)\npos_freq=nltk.FreqDist(pos_token)\npos_df_freq=pd.DataFrame({\"Words\":list(pos_freq.keys()),\"Frequency\":list(pos_freq.values())})\npos_df_freq=pos_df_freq.nlargest(n,'Frequency')\n\n# Negative review\nneg_token=split_token.tokenize(neg_all_words)\nneg_freq=nltk.FreqDist(neg_token)\nneg_df_freq=pd.DataFrame({\"Words\":list(neg_freq.keys()),\"Frequency\":list(neg_freq.values())})\nneg_df_freq=neg_df_freq.nlargest(n,'Frequency')\n\n# Plotting charts\nfig, axs=plt.subplots(1,2,figsize=(10,5))\n\n# pos\naxs[0].bar(pos_df_freq['Words'],pos_df_freq['Frequency'])\naxs[0].set_xticklabels(pos_df_freq['Words'], rotation=45)\naxs[0].set_title('Positive Reviews',size=10)\n\n# neg\naxs[1].bar(neg_df_freq['Words'],neg_df_freq['Frequency'])\naxs[1].set_xticklabels(neg_df_freq['Words'], rotation=45)\naxs[1].set_title('Negative Reviews',size=10)\n\n\nfig.suptitle('Word Frequency by Sentiment',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# APPENDIX","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes Classifier\nps is positive sentiment<br>\nns is negative sentiment<br>\nreviews is the complete sentence<br>\nwords are a part o review<br>\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/nlp-equations/NBC.JPG')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression\np is the probability of the positive review and 1 - p, the negative <br>\nβ are the coefficient for each X (words/features)<br>\ne is the error term<br>\n\np/(1-p) is also known as odds\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/nlp-equations/Logistic_Regression.JPG')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Term Frequency * Inverse Document Frequency\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/nlp-equations/TFIDF.JPG')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}