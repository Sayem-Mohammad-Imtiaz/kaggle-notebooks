{"cells":[{"metadata":{},"cell_type":"markdown","source":"An intelligence quotient (IQ) is a total score derived from a set of standardized tests or subtests designed to assess human intelligence. The abbreviation \"IQ\" was coined by the psychologist William Stern for the German term Intelligenzquotient, his term for a scoring method for intelligence tests at University of Breslau he advocated in a 1912 book.\n\nHistorically, IQ was a score obtained by dividing a person's mental age score, obtained by administering an intelligence test, by the person's chronological age, both expressed in terms of years and months. The resulting fraction (quotient) is multiplied by 100 to obtain the IQ score. For modern IQ tests, the median raw score of the norming sample is defined as IQ 100 and scores each standard deviation (SD) up or down are defined as 15 IQ points greater or less. By this definition, approximately two-thirds of the population scores are between IQ 85 and IQ 115. About 2.5 percent of the population scores above 130, and 2.5 percent below 70. https://en.wikipedia.org/wiki/Intelligence_quotient","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#codes from Rodrigo Lima  @rodrigolima82\nfrom IPython.display import Image\nImage(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRmr9DLU2itRpL5VaXZFXiBSVgQDuXFDRqHDsyePcJNIiBea8jR&usqp=CAU',width=400,height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"iq-test.net","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\nfrom plotly.offline import iplot\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/alphaversion-fullscale-iq-test-responses/data.csv', encoding='ISO-8859-2')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes from Will Koehrsen https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_missing= missing_values_table(df)\ndf_missing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the number of columns of each data type. int64 and float64 are numeric variables (which can be either discrete or continuous). object columns contain strings and are categorical features. .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of each type of column\ndf.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now look at the number of unique entries in each of the object (categorical) columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique classes in each object column\ndf.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the categorical variables have a relatively large number of unique entries. We will need to find a way to deal with these categorical variables!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). \nLabel encoding: assign each unique category in a categorical variable with an integer. No new columns are created.\nOne-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n\nThe only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions (while still trying to preserve information).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Label Encoding and One-Hot Encoding\n\nLet's implement the policy described above: for any categorical variable (dtype == object) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding.\n\nFor label encoding, we use the Scikit-Learn LabelEncoder and for one-hot encoding, the pandas get_dummies(df) function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in df:\n    if df[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(df[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(df[col])\n            # Transform both training and testing data\n            df[col] = le.transform(df[col])\n            #app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\ndf = pd.get_dummies(df)\n#app_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', df.shape)\n#print('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ext_data = df[['VQ1s', 'testelapse', 'introelapse', 'endelapse', 'MQ6e']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['testelapse']).copy()\n\n# Add in the age of the client in years\nplot_data['introelapse'] = df['introelapse']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'VQ1s', \n                    vars = [x for x in list(plot_data.columns) if x != 'VQ1s'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('VQ1s & introelapse Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes from Parul Pandey  https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n#msno.bar(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#msno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#msno.heatmap(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#msno.dendrogram(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#df_1 = df.copy()\n#df_1['VQ2a'].mean() #pandas skips the missing values and calculates mean of the remaining values.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basic Imputation Techniques\nImputating with a constant value\nImputation using the statistics (mean, median or most frequent) of each column in which the missing values are located\nFor this we shall use the The SimpleImputer class from sklearn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing with a constant\n\nfrom sklearn.impute import SimpleImputer\ndf_constant = df.copy()\n#setting strategy to 'constant' \nmean_imputer = SimpleImputer(strategy='constant') # imputing using constant value\ndf_constant.iloc[:,:] = mean_imputer.fit_transform(df_constant)\ndf_constant.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\ndf_most_frequent = df.copy()\n#setting strategy to 'mean' to impute by the mean\nmean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \ndf_most_frequent.iloc[:,:] = mean_imputer.fit_transform(df_most_frequent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_most_frequent.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#K-Nearest Neighbor Imputation\n\nThe KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach.Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_knn = df.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer\ndf_knn = df.copy(deep=True)\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ndf_knn['RQ6a'] = knn_imputer.fit_transform(df_knn[['RQ6a']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_knn['RQ6a'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Multivariate feature imputation - Multivariate imputation by chained equations (MICE)\n\nA strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. It performns multiple regressions over random sample ofthe data, then takes the average ofthe multiple regression values and uses that value to impute the missing value. In sklearn, it is implemented as follows:","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\ndf_mice = df.copy(deep=True)\n\nmice_imputer = IterativeImputer()\ndf_mice['RQ6a'] = mice_imputer.fit_transform(df_mice[['RQ6a']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mice['RQ6a'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#IQ tests have wielded a great deal of power on society over the last 120 years. \n\n#In the 1900s, eugenicists used the test to judge people for sterilization.\n\n#More recently, IQ has helped inmates avoid corporal punishment and kids get the right education.\n\n#Scientists still debate the merit of IQ, however.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#codes from Rodrigo Lima  @rodrigolima82\nfrom IPython.display import Image\nImage(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSC45MgUFkcXqMBKdsOL0oSYBbbBHWfRub5K1R54SXHeOSumQ2O&usqp=CAU',width=400,height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"free-iqtest.net","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Kaggle Notebook Runner: Marília Prata  @mpwolke","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}