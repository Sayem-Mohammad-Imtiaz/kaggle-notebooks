{"cells":[{"metadata":{"_uuid":"360f2f9aa59753c6cd8e8cd95502b6890b2c0e48"},"cell_type":"markdown","source":"**WE WILL LEARN **\n\n1. Cleaning data\n2. bag of word\n3. text classification\n4. Principal Component Analysis\n5. Model Selection\n6. Grid search with knn\n7. Grid Search logistic regression\n8. Recomendation Systems"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#if we don't use encoding=\"latin1\", program will give an error \ndata=pd.read_csv(\"../input/twitter-gender/gender-classifier.csv\",encoding=\"latin1\")\n\n#we will prepare our data\n#with pd.concat, we can combine our datas which can be series or dataframe \ndata=pd.concat([data.gender,data.description],axis=1)\n\n#we will clean Nan values from our data with dropna \ndata.dropna(axis=0,inplace=True)\n\n#we will change gender type male=0,female=1\ndata[\"gender\"]=[1 if i==\"female\" else 0 for i in data[\"gender\"]]\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84a764820efdbff94d78e5f412b132a135e18e58"},"cell_type":"code","source":"#data cleaning\n#regular expression\nimport re\n\n#first we will clean one data from our data to make an example to clean data\n\nexample_description=data.description[4] #4th row data\n\n#with re.sub , find character :),%,/,# (except from a to z and from A to Z in alphabet) and change with space\nclean=re.sub(\"[^a-zA-Z]\",\" \",example_description)\nclean=clean.lower() # our LETTER words will be changed by lower()\n\n#now we have just english lower words\nclean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"868619cfb479d156f84cf2b755f565b4f0c2c0c9"},"cell_type":"code","source":"#stopwords (irrelevent words)\nimport nltk # natural language tool kit library\nnltk.download(\"stopwords\") # for this download we need internet. if we dont have an internet connection \n# we can have an error\n\nfrom nltk.corpus import stopwords\n\n#clean=clean.split()# with split every word will be value and stored in a list, we can use word_tokenize too\nclean=nltk.word_tokenize(clean) #with tokenize we can split like this words shouldn't to should and n't\n\nclean=[word for word in clean if not word in set(stopwords.word(\"english\"))]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a82af2a25bff9f69c98125cbe4199686d2ae7914"},"cell_type":"code","source":"import nltk as nlp\ndescription_list=[]\n\nfor description in data.description:\n    description=re.sub(\"[^a-zA-Z]\",\" \",description)\n    description=description.lower() #change from capital to low words\n    #description=[word for word in description if not word in set(stopwords.words(\"english\"))]\n    description=nltk.word_tokenize(description)\n    lemma=nlp.WordNetLemmatizer()\n    description=[ lemma.lemmatize(word) for word in description]\n    description=\" \".join(description)\n    description_list.append(description)\ndescription_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef30b80dd19c6a16ddc6865cfb265fb985538d83"},"cell_type":"markdown","source":"# Bag of words"},{"metadata":{"trusted":true,"_uuid":"0093667731d6f7af9026175542d3e84e006e1bfb"},"cell_type":"code","source":"#to create bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nmax_features=5000\ncount_vectorizer=CountVectorizer(max_features=max_features,stop_words=\"english\")\n\nsparce_matrix=count_vectorizer.fit_transform(description_list).toarray()\nprint(\"the most using  {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f85ad6dc4d842d94deb92b7155507ec11631dd50"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx=sparce_matrix\ny=data[\"gender\"] # male or female\n\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=21)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6126a5e6fa3d254662aab4a838a594e2153568ba"},"cell_type":"code","source":"#Naive bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(xtrain,ytrain)\n\n#prediction\ny_prediction=nb.predict(xtest).reshape(-1,1)\n\nprint(\"accuracy : {}\".format(nb.score(y_prediction,ytest)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09c73f4e472ad4483c69b480cc13c58e3076c9c6"},"cell_type":"markdown","source":"# Principal Component Analysis (PCA)\nWhere can we use PCA\n1. Feature extraction\n2. Feature dimension reduction\n3. Stock market prediction\n4. Gene data analaysis\n"},{"metadata":{"trusted":true,"_uuid":"800995221e76550746fe82a547076825097f3b12"},"cell_type":"code","source":"#we will create our data from sklearn library\nfrom sklearn.datasets import load_iris\n\niris=load_iris()\ndata=iris.data\nfeature_names=iris.feature_names\ny=iris.target\n\ndf=pd.DataFrame(data,columns=feature_names)\ndf[\"class\"]=y\nx=data\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b6f9bcd8a71b94a4c304d95b1d3e2404d4d480f"},"cell_type":"code","source":"#PCA\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=2,whiten=True) #whitten = normalize, we will have 2-size\n\npca.fit(x)\nx_pca=pca.transform(x) # it will trasnform dimension from to 2\n\nprint(\"variance ratio: {}\".format(pca.explained_variance_ratio_))\n\n#we changed our data's dimensiton but we still have the same features( we will see the varience=0.9776..)\nprint(\"sum: {}\".format(sum(pca.explained_variance_ratio_)))\n\n#PCA visualization\ndf[\"p1\"]=x_pca[:,0] #principal component\ndf[\"p2\"]=x_pca[:,1] # second component\n\ncolor=[\"red\",\"green\",\"blue\"]\nfor each in range(3):\n    plt.scatter(df.p1[df[\"class\"]==each],df.p2[df[\"class\"]==each],color=color[each],label=iris.target_names[each])\n\nplt.legend()\nplt.xlabel(\"p1\")\nplt.ylabel(\"p2\")\nplt.show()\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f0b439d7f4edc6b76e58c92e2a6923b6262556f"},"cell_type":"markdown","source":"# Model Selection\nK-Fold cross validation"},{"metadata":{"trusted":true,"_uuid":"d9f4b394cbb9117a78bfebdd3a83a4415286e760"},"cell_type":"code","source":"# we will use iris data set\niris=load_iris()\nx=iris.data\ny=iris.target\n\n#we will make normalization\nx=(x-np.min(x))/(np.max(x)-np.min(x))\n\n#we will make train and test split\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3)\n\n#we will use knn algoritm\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3)\n\n#we will use cross validation\nfrom sklearn.model_selection import cross_val_score\naccuracies=cross_val_score(estimator=knn,X=xtrain,y=ytrain,cv=10)\n\nprint(\"average accuracy: {}\".format(np.mean(accuracies)))\nprint(\"Standart deviation (std): {}\".format(np.std(accuracies)))\n\n\nknn.fit(xtrain,ytrain)\n\nprint(\"test acuuracy: {}\".format(knn.score(xtest,ytest)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"771f588bad8a61fb892cc9b6c8d8a1acaa8f1bf7"},"cell_type":"markdown","source":"# Grid search with knn"},{"metadata":{"trusted":true,"_uuid":"e13e70b32d142ff42a750c3127ac5f5c0699dd1d"},"cell_type":"code","source":"# we will find best k number\n#grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\n\ngrid={\"n_neighbors\":np.arange(1,50)}\nknn =KNeighborsClassifier()\nknn_cros_validation=GridSearchCV(knn,grid,cv=10)# grid search cross validation\nknn_cros_validation.fit(x,y)\n\nprint(\"tuned hyperparameter K: {}\".format(knn_cros_validation.best_params_))\nprint(\"Best Score: {}\".format(knn_cros_validation.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f106655ad973961147c1be8f2ccfc3a89b29bfab"},"cell_type":"markdown","source":"# Grid search  cross validation with logistic regression"},{"metadata":{"trusted":true,"_uuid":"86c16f27f933ad8b3752b71b351f4fe0b69522e4"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\niris=load_iris()\nx=iris.data\ny=iris.target\n\nx=x[:100,:]\ny=y[:100]\n\n#we will make normalization\nx=(x-np.min(x))/(np.max(x)-np.min(x))\n\n#we will make train and test split\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25)\n\n\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}\n\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(xtrain,ytrain)\n\nprint(\"best parameters(hyperparameters: )\",logreg_cv.best_params_)\nprint(\"accuracy: {}\".format(logreg_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4971d6799af30d45ba154ac67bd655042899cc1a"},"cell_type":"code","source":"logreg2=LogisticRegression(C=1,penalty=\"l2\")\nlogreg2.fit(xtrain,ytrain)\nprint(\"score\",logreg2.score(xtest,ytest))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e49e1d5d2c70e388f3cd009c8e04af73cbcde0e"},"cell_type":"markdown","source":"# Recommendation System"},{"metadata":{"trusted":true,"_uuid":"184c06ba89adafa258caf4bb46532e2f7514e6f2"},"cell_type":"code","source":"movie=pd.read_csv(\"../input/movielens-20m-dataset/movie.csv\")\nmovie.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d66b62fbc078a5f04ec6ec25349de3ae42f9f5b2"},"cell_type":"code","source":"movie=movie.loc[:,[\"movieId\",\"title\"]]\nmovie.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"537687195e5c12efaa089f96013f198e849c034d"},"cell_type":"code","source":"rating=pd.read_csv(\"../input/movielens-20m-dataset/rating.csv\")\nrating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa4526dadae26d1ed82e2905616eacf1cbc1219"},"cell_type":"code","source":"rating=rating.loc[:,[\"userId\",\"movieId\",\"rating\"]]\nrating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7485b55847a0a1e283b504780cef835f0a443dc"},"cell_type":"code","source":"#we can compile (merge) movie and rating data\ndata=pd.merge(movie,rating)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae9c1dcf62a1cf7b764a17f05071dfe73f9a17cf"},"cell_type":"code","source":"#to make rows user and columns movies, we need to use pivot table\n#we will chose first 1000000 data\ndata=data.loc[:1000000,:]\npivot_table=data.pivot_table(index=[\"userId\"],columns=[\"title\"],values=\"rating\")\npivot_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12065cd90d182263d9cfc1f11a1eb64e9d6a1d10"},"cell_type":"code","source":"movie_watched=pivot_table[\"Babe (1995)\"]\nsimilarity_with_other_movies=pivot_table.corrwith(movie_watched) # correlation\nsimilarity_with_other_movies=similarity_with_other_movies.sort_values(ascending=False)\nsimilarity_with_other_movies.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd79d3a824f7a75bb669989c52f6a1b67a440650"},"cell_type":"markdown","source":"Thank you for looking my kernel and thank you in advance for your comment and votes\n\nThanks to DATAI Team"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}