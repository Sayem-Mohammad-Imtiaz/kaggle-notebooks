{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Semantic Segmentation Using TensorFlow Keras\n\n### Semantic segmentation can be defined as the process of pixel-level image classification into two or more Object classes. It differs from image classification entirely, as the latter performs image-level classification. For instance, consider an image that consists mainly of a zebra, surrounded by grass fields, a tree and a flying bird. Image classification tells us that the image belongs to the ‘zebra’ class. It can not tell where the zebra is or what its size or pose is. But, semantic segmentation of that image may tell that there is a zebra, grass field, a bird and a tree in the given image (classifies parts of an image into separate classes). And it tells us which pixels in the image belong to which class!\n","metadata":{}},{"cell_type":"markdown","source":"# Import necessary frameworks, libraries and modules","metadata":{}},{"cell_type":"code","source":"# for path related functionalities\nimport os\n# for array operations\nimport numpy as np\n# tensorflow framework\nimport tensorflow as tf\n# keras API for deep learning\nfrom tensorflow import keras\n# for image visulaizations\nimport matplotlib.pyplot as plt\n# for legends and other supporting functionalities\nimport matplotlib as mpl\n# for viewing iteration status\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:46:54.84164Z","iopub.execute_input":"2021-07-02T09:46:54.842044Z","iopub.status.idle":"2021-07-02T09:47:00.9732Z","shell.execute_reply.started":"2021-07-02T09:46:54.841942Z","shell.execute_reply":"2021-07-02T09:47:00.972143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download and prepare data","metadata":{}},{"cell_type":"markdown","source":"### This dataset has 1000 images of people (one person per image). There are 1000 masks corresponding to those original images. Label images have 59 segmented classes corresponding to classes such as hair, bag, shirt, shoes, skin, sunglasses and cap.","metadata":{}},{"cell_type":"code","source":"# a list to collect paths of 1000 images\nimage_path = []\nfor root, dirs, files in os.walk('../input/people-clothing-segmentation/png_images/IMAGES'):\n    # iterate over 1000 images\n    for file in files:\n        # create path\n        path = os.path.join(root,file)\n        # add path to list\n        image_path.append(path)\nlen(image_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:00.974759Z","iopub.execute_input":"2021-07-02T09:47:00.975197Z","iopub.status.idle":"2021-07-02T09:47:01.093748Z","shell.execute_reply.started":"2021-07-02T09:47:00.975153Z","shell.execute_reply":"2021-07-02T09:47:01.092302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a list to collect paths of 1000 masks\nmask_path = []\nfor root, dirs, files in os.walk('../input/people-clothing-segmentation/png_masks/MASKS'):\n    #iterate over 1000 masks\n    for file in files:\n        # obtain the path\n        path = os.path.join(root,file)\n        # add path to the list\n        mask_path.append(path)\nlen(mask_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:01.096381Z","iopub.execute_input":"2021-07-02T09:47:01.096809Z","iopub.status.idle":"2021-07-02T09:47:01.217552Z","shell.execute_reply.started":"2021-07-02T09:47:01.096769Z","shell.execute_reply":"2021-07-02T09:47:01.216532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(image_path[:5])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:01.22114Z","iopub.execute_input":"2021-07-02T09:47:01.221435Z","iopub.status.idle":"2021-07-02T09:47:01.231907Z","shell.execute_reply.started":"2021-07-02T09:47:01.221406Z","shell.execute_reply":"2021-07-02T09:47:01.230548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mask_path[:5])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:01.23395Z","iopub.execute_input":"2021-07-02T09:47:01.234518Z","iopub.status.idle":"2021-07-02T09:47:01.240789Z","shell.execute_reply.started":"2021-07-02T09:47:01.234474Z","shell.execute_reply":"2021-07-02T09:47:01.239521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Images are unsorted in the dataset by default. We must sort them by their file names to obtain the right image-mask pairs.","metadata":{}},{"cell_type":"code","source":"image_path.sort()\nmask_path.sort()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:01.242787Z","iopub.execute_input":"2021-07-02T09:47:01.243427Z","iopub.status.idle":"2021-07-02T09:47:01.2516Z","shell.execute_reply.started":"2021-07-02T09:47:01.243384Z","shell.execute_reply":"2021-07-02T09:47:01.25021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read and decode the images and masks; store them in separate lists","metadata":{}},{"cell_type":"code","source":"# create a list to store images\nimages = []\n# iterate over 1000 image paths\nfor path in tqdm(image_path):\n    # read file\n    file = tf.io.read_file(path)\n    # decode png file into a tensor\n    image = tf.image.decode_png(file, channels=3, dtype=tf.uint8)\n    # append to the list\n    images.append(image)\n\n# create a list to store masks\nmasks = []\n# iterate over 1000 mask paths\nfor path in tqdm(mask_path):\n    # read the file\n    file = tf.io.read_file(path)\n    # decode png file into a tensor\n    mask = tf.image.decode_png(file, channels=1, dtype=tf.uint8)\n    # append mask to the list\n    masks.append(mask)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:01.253784Z","iopub.execute_input":"2021-07-02T09:47:01.254533Z","iopub.status.idle":"2021-07-02T09:47:35.242829Z","shell.execute_reply.started":"2021-07-02T09:47:01.254487Z","shell.execute_reply":"2021-07-02T09:47:35.241582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How many images and masks are there?","metadata":{}},{"cell_type":"code","source":"len(images), len(masks)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:35.24766Z","iopub.execute_input":"2021-07-02T09:47:35.24803Z","iopub.status.idle":"2021-07-02T09:47:35.257861Z","shell.execute_reply.started":"2021-07-02T09:47:35.248002Z","shell.execute_reply":"2021-07-02T09:47:35.25608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample few images and visualize","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nfor i in range(1,4):\n    plt.subplot(1,3,i)\n    img = images[i]\n    plt.imshow(img)\n    plt.colorbar()\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:35.261472Z","iopub.execute_input":"2021-07-02T09:47:35.262023Z","iopub.status.idle":"2021-07-02T09:47:36.012537Z","shell.execute_reply.started":"2021-07-02T09:47:35.26192Z","shell.execute_reply":"2021-07-02T09:47:36.011422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample corresponding masks and visualize","metadata":{}},{"cell_type":"code","source":"# Define a normalizer that can be applied while visualizing masks to have a consistency\n# min class value is 0\n# max class value is 58\nNORM = mpl.colors.Normalize(vmin=0, vmax=58)\n\n# plot masks\nplt.figure(figsize=(16,5))\nfor i in range(1,4):\n    plt.subplot(1,3,i)\n    img = masks[i]\n    plt.imshow(img, cmap='jet', norm=NORM)\n    plt.colorbar()\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:36.014138Z","iopub.execute_input":"2021-07-02T09:47:36.014553Z","iopub.status.idle":"2021-07-02T09:47:36.781831Z","shell.execute_reply.started":"2021-07-02T09:47:36.01451Z","shell.execute_reply":"2021-07-02T09:47:36.780504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Downstack with a Pre-trained CNN","metadata":{}},{"cell_type":"markdown","source":"### We have to build a computer vision model that can convert an input image into a segmented image (also called masked image or label image). Building a model from scratch and training it is not a good idea as we have very limited data for training (1000 images are insufficient for 59 unbalanced classes). So we prefer a pre-trained model through transfer learning.\n\n### By understanding how semantic segmentation works, we can easily come up with an idea of how to choose our pre-trained model. One of the popular architectural approaches is FCNN (Fully Convolutional Neural Networks). In contrast to CNNs in image classification, where the decision head is made up of dense layers, an FCNN is made up of layers related to convolutional operations only. Because the final output is an image of a shape identical to the input image. \n\n### An FCNN contains two parts: an encoder and a decoder. An encoder is a downstack of convolutional neural layers that extract features from the input image. A decoder is an upstack of transpose convolutional neural layers that builds the segmented image from the extracted features. The sizes of feature maps go down while downsampling (e.g. 128, 64, 32, 16, 8, 4 – in order), and they go up while upsampling (e.g. 4, 8, 16, 32, 64, 128 – in order).\n\n### Among FCNNs, U-Net is one of the successful architectures acclaimed for its performance in Medical Image Segmentation. It encourages skip connections between a few specific-sized layers of downstack and upstack. Skip-connections yield better performance because of the truth that upstack struggles to build finer details of the image on its own during upsampling. Skip-connections bye-pass a large stack of layers to feed finer details from a downstack layer to its corresponding upstack layer.","metadata":{}},{"cell_type":"markdown","source":"### Find below a Typical U-Net Architecture with Skip-Connections (Source: https://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a/u-net-architecture.png)","metadata":{}},{"cell_type":"markdown","source":"![A typical U-Net Architecture with Skip-Connections (Source: https://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a/u-net-architecture.png)](https://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a/u-net-architecture.png)","metadata":{}},{"cell_type":"markdown","source":"### Here, we wish to use the functional approach of U-Net architecture, but we will have our own architecture suitable to our task. The downstack can be a pre-trained CNN, trained for image classification (e.g. MobileNetV2, ResNet, NASNet, Inception, DenseNet, or EfficientNet). It can effectively extract the features. But, we have to build our upstack to match our classes (here, 59), build skip-connections, and train it with our data. \n\n### We prefer a pre-trained DenseNet121 to be the downstack that can be obtained through transfer learning and build the upstack with pix2pix, a publicly available generative upstack template (it saves our time and code).","metadata":{}},{"cell_type":"code","source":"# Use pre-trained DenseNet121 without head\nbase = keras.applications.DenseNet121(input_shape=[128,128,3], \n                                      include_top=False, \n                                      weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:36.783678Z","iopub.execute_input":"2021-07-02T09:47:36.784157Z","iopub.status.idle":"2021-07-02T09:47:41.205011Z","shell.execute_reply.started":"2021-07-02T09:47:36.784115Z","shell.execute_reply":"2021-07-02T09:47:41.20382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How many layers does this model have?","metadata":{}},{"cell_type":"code","source":"len(base.layers)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:41.206883Z","iopub.execute_input":"2021-07-02T09:47:41.207396Z","iopub.status.idle":"2021-07-02T09:47:41.217486Z","shell.execute_reply.started":"2021-07-02T09:47:41.207361Z","shell.execute_reply":"2021-07-02T09:47:41.21633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The DenseNet121 model has 427 layers. We need to identify suitable layers whose output will be used for skip connections. Plot the entire model, along with the feature shapes.","metadata":{}},{"cell_type":"code","source":"keras.utils.plot_model(base, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:41.219436Z","iopub.execute_input":"2021-07-02T09:47:41.22036Z","iopub.status.idle":"2021-07-02T09:47:44.490724Z","shell.execute_reply.started":"2021-07-02T09:47:41.220179Z","shell.execute_reply":"2021-07-02T09:47:44.489491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From the above plot, we select the final ReLU activation layer for each feature map size, i.e. 4, 8, 16, 32, and 64, required for skip-connections. Write down the names of the selected ReLU layers in a list.","metadata":{}},{"cell_type":"code","source":"skip_names = ['conv1/relu', # size 64*64\n             'pool2_relu',  # size 32*32\n             'pool3_relu',  # size 16*16\n             'pool4_relu',  # size 8*8\n             'relu'        # size 4*4\n             ]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:44.492691Z","iopub.execute_input":"2021-07-02T09:47:44.493445Z","iopub.status.idle":"2021-07-02T09:47:44.498822Z","shell.execute_reply.started":"2021-07-02T09:47:44.4934Z","shell.execute_reply":"2021-07-02T09:47:44.497901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obtain the outputs of these layers.","metadata":{}},{"cell_type":"code","source":"skip_outputs = [base.get_layer(name).output for name in skip_names]\nfor i in range(len(skip_outputs)):\n    print(skip_outputs[i])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:44.501161Z","iopub.execute_input":"2021-07-02T09:47:44.502133Z","iopub.status.idle":"2021-07-02T09:47:44.523904Z","shell.execute_reply.started":"2021-07-02T09:47:44.502094Z","shell.execute_reply":"2021-07-02T09:47:44.522055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the downstack with the above layers. We use the pre-trained model as such, without any fine-tuning.","metadata":{}},{"cell_type":"code","source":"downstack = keras.Model(inputs=base.input,\n                       outputs=skip_outputs)\n# freeze the downstack layers\ndownstack.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:44.526269Z","iopub.execute_input":"2021-07-02T09:47:44.527169Z","iopub.status.idle":"2021-07-02T09:47:44.61087Z","shell.execute_reply.started":"2021-07-02T09:47:44.527119Z","shell.execute_reply":"2021-07-02T09:47:44.609492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Upstack","metadata":{}},{"cell_type":"markdown","source":"### Build the upstack using an upsampling template. Let's use pix2pix template available open-source in `tensorflow_examples` repository","metadata":{}},{"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/examples.git","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:47:44.612742Z","iopub.execute_input":"2021-07-02T09:47:44.613453Z","iopub.status.idle":"2021-07-02T09:48:01.119277Z","shell.execute_reply.started":"2021-07-02T09:47:44.613409Z","shell.execute_reply":"2021-07-02T09:48:01.117753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_examples.models.pix2pix import pix2pix","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:01.123553Z","iopub.execute_input":"2021-07-02T09:48:01.12386Z","iopub.status.idle":"2021-07-02T09:48:01.13997Z","shell.execute_reply.started":"2021-07-02T09:48:01.123827Z","shell.execute_reply":"2021-07-02T09:48:01.138809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the upstack","metadata":{}},{"cell_type":"code","source":"# Four upstack layers for upsampling sizes \n# 4->8, 8->16, 16->32, 32->64 \nupstack = [pix2pix.upsample(512,3),\n          pix2pix.upsample(256,3),\n          pix2pix.upsample(128,3),\n          pix2pix.upsample(64,3)]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:01.141811Z","iopub.execute_input":"2021-07-02T09:48:01.142367Z","iopub.status.idle":"2021-07-02T09:48:01.191801Z","shell.execute_reply.started":"2021-07-02T09:48:01.142336Z","shell.execute_reply":"2021-07-02T09:48:01.19079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"upstack[0].layers","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:01.193262Z","iopub.execute_input":"2021-07-02T09:48:01.193657Z","iopub.status.idle":"2021-07-02T09:48:01.199713Z","shell.execute_reply.started":"2021-07-02T09:48:01.193627Z","shell.execute_reply":"2021-07-02T09:48:01.198546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build U-Net model with skip-connections","metadata":{}},{"cell_type":"markdown","source":"### Build a U-Net model by merging downstack and upstack with skip-connections.","metadata":{}},{"cell_type":"code","source":"# define the input layer\ninputs = keras.layers.Input(shape=[128,128,3])\n\n# downsample \ndown = downstack(inputs)\nout = down[-1]\n\n# prepare skip-connections\nskips = reversed(down[:-1])\n# choose the last layer at first 4 --> 8\n\n# upsample with skip-connections\nfor up, skip in zip(upstack,skips):\n    out = up(out)\n    out = keras.layers.Concatenate()([out,skip])\n    \n# define the final transpose conv layer\n# image 128 by 128 with 59 classes\nout = keras.layers.Conv2DTranspose(59, 3,\n                                  strides=2,\n                                  padding='same',\n                                  )(out)\n# complete unet model\nunet = keras.Model(inputs=inputs, outputs=out)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:01.201503Z","iopub.execute_input":"2021-07-02T09:48:01.202305Z","iopub.status.idle":"2021-07-02T09:48:02.903183Z","shell.execute_reply.started":"2021-07-02T09:48:01.202245Z","shell.execute_reply":"2021-07-02T09:48:02.902147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's plot our model","metadata":{}},{"cell_type":"code","source":"keras.utils.plot_model(unet, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:02.904928Z","iopub.execute_input":"2021-07-02T09:48:02.905441Z","iopub.status.idle":"2021-07-02T09:48:03.178353Z","shell.execute_reply.started":"2021-07-02T09:48:02.905373Z","shell.execute_reply":"2021-07-02T09:48:03.177008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"images[0].shape, masks[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:03.18433Z","iopub.execute_input":"2021-07-02T09:48:03.184753Z","iopub.status.idle":"2021-07-02T09:48:03.19357Z","shell.execute_reply.started":"2021-07-02T09:48:03.184721Z","shell.execute_reply":"2021-07-02T09:48:03.191979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Resize data as the model expects. ","metadata":{}},{"cell_type":"code","source":"def resize_image(image):\n    # scale the image\n    image = tf.cast(image, tf.float32)\n    image = image/255.0\n    # resize image\n    image = tf.image.resize(image, (128,128))\n    return image\n\ndef resize_mask(mask):\n    # resize the mask\n    mask = tf.image.resize(mask, (128,128))\n    mask = tf.cast(mask, tf.uint8)\n    return mask    ","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:03.196866Z","iopub.execute_input":"2021-07-02T09:48:03.197549Z","iopub.status.idle":"2021-07-02T09:48:03.206747Z","shell.execute_reply.started":"2021-07-02T09:48:03.197499Z","shell.execute_reply":"2021-07-02T09:48:03.205163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = [resize_image(i) for i in images]\ny = [resize_mask(m) for m in masks]\nlen(X), len(y)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:03.209027Z","iopub.execute_input":"2021-07-02T09:48:03.210085Z","iopub.status.idle":"2021-07-02T09:48:05.675896Z","shell.execute_reply.started":"2021-07-02T09:48:03.210038Z","shell.execute_reply":"2021-07-02T09:48:05.674832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images[0].dtype, masks[0].dtype, X[0].dtype, y[0].dtype","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:05.680957Z","iopub.execute_input":"2021-07-02T09:48:05.683683Z","iopub.status.idle":"2021-07-02T09:48:05.696057Z","shell.execute_reply.started":"2021-07-02T09:48:05.683634Z","shell.execute_reply":"2021-07-02T09:48:05.694804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize a resized image and a resized mask","metadata":{}},{"cell_type":"code","source":"# plot an image\nplt.imshow(X[0])\nplt.colorbar()\nplt.show()\n\n#plot a mask\nplt.imshow(y[0], cmap='jet')\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:05.701694Z","iopub.execute_input":"2021-07-02T09:48:05.704759Z","iopub.status.idle":"2021-07-02T09:48:06.518354Z","shell.execute_reply.started":"2021-07-02T09:48:05.704712Z","shell.execute_reply":"2021-07-02T09:48:06.517322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data for training and validation","metadata":{}},{"cell_type":"markdown","source":"### We split data into training and validation set in 80/20 ratio. We develop TensorFlow Dataset objects for train and validation sets to ease further processing and data handling.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split data into 80/20 ratio\ntrain_X, val_X,train_y, val_y = train_test_split(X, y, test_size=0.2, \n                                                      random_state=0\n                                                     )\n# develop tf Dataset objects\ntrain_X = tf.data.Dataset.from_tensor_slices(train_X)\nval_X = tf.data.Dataset.from_tensor_slices(val_X)\n\ntrain_y = tf.data.Dataset.from_tensor_slices(train_y)\nval_y = tf.data.Dataset.from_tensor_slices(val_y)\n\n# verify the shapes and data types\ntrain_X.element_spec, train_y.element_spec, val_X.element_spec, val_y.element_spec","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:06.519869Z","iopub.execute_input":"2021-07-02T09:48:06.520511Z","iopub.status.idle":"2021-07-02T09:48:07.500898Z","shell.execute_reply.started":"2021-07-02T09:48:06.52047Z","shell.execute_reply":"2021-07-02T09:48:07.499655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation ","metadata":{}},{"cell_type":"markdown","source":"### We have less data (just 800 examples in training set) that is not enough for deep learning. Hence we should increase the amount of training data by performing data augmentations. Define Functions for data augmentation.","metadata":{}},{"cell_type":"code","source":"def brightness(img, mask):\n    # adjust brightness of image\n    # don't alter in mask\n    img = tf.image.adjust_brightness(img, 0.1)\n    return img, mask\n\ndef gamma(img, mask):\n    # adjust gamma of image\n    # don't alter in mask\n    img = tf.image.adjust_gamma(img, 0.1)\n    return img, mask\n\ndef hue(img, mask):\n    # adjust hue of image\n    # don't alter in mask\n    img = tf.image.adjust_hue(img, -0.1)\n    return img, mask\n\ndef crop(img, mask):\n    # crop both image and mask identically\n    img = tf.image.central_crop(img, 0.7)\n    # resize after cropping\n    img = tf.image.resize(img, (128,128))\n    mask = tf.image.central_crop(mask, 0.7)\n    # resize afer cropping\n    mask = tf.image.resize(mask, (128,128))\n    # cast to integers as they are class numbers\n    mask = tf.cast(mask, tf.uint8)\n    return img, mask\n\ndef flip_hori(img, mask):\n    # flip both image and mask identically\n    img = tf.image.flip_left_right(img)\n    mask = tf.image.flip_left_right(mask)\n    return img, mask\n\ndef flip_vert(img, mask):\n    # flip both image and mask identically\n    img = tf.image.flip_up_down(img)\n    mask = tf.image.flip_up_down(mask)\n    return img, mask\n\ndef rotate(img, mask):\n    # rotate both image and mask identically\n    img = tf.image.rot90(img)\n    mask = tf.image.rot90(mask)\n    return img, mask","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:07.50479Z","iopub.execute_input":"2021-07-02T09:48:07.505116Z","iopub.status.idle":"2021-07-02T09:48:07.519404Z","shell.execute_reply.started":"2021-07-02T09:48:07.505085Z","shell.execute_reply":"2021-07-02T09:48:07.518134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Perform data augmentation with original training set and concatenate with enlarged training set. Do not perform data augmentation with validation set. With 7 augmentation functions and 800 input examples, we can get 7*800 = 5600 new examples. Including original examples, we get 5600+800 = 6400 examples for training. That sounds good!","metadata":{}},{"cell_type":"code","source":"# zip images and masks\ntrain = tf.data.Dataset.zip((train_X, train_y))\nval = tf.data.Dataset.zip((val_X, val_y))\n\n# perform augmentation on train data only\n\na = train.map(brightness)\nb = train.map(gamma)\nc = train.map(hue)\nd = train.map(crop)\ne = train.map(flip_hori)\nf = train.map(flip_vert)\ng = train.map(rotate)\n\n# concatenate every new augmented sets\ntrain = train.concatenate(a)\ntrain = train.concatenate(b)\ntrain = train.concatenate(c)\ntrain = train.concatenate(d)\ntrain = train.concatenate(e)\ntrain = train.concatenate(f)\ntrain = train.concatenate(g)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:07.522364Z","iopub.execute_input":"2021-07-02T09:48:07.522947Z","iopub.status.idle":"2021-07-02T09:48:08.029107Z","shell.execute_reply.started":"2021-07-02T09:48:07.522847Z","shell.execute_reply":"2021-07-02T09:48:08.027925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Batch datasets as our SGD based optimizer would expect, and prefetch to have memory efficient training","metadata":{}},{"cell_type":"code","source":"BATCH = 64\nAT = tf.data.AUTOTUNE\nBUFFER = 1000\n\nSTEPS_PER_EPOCH = 800//BATCH\nVALIDATION_STEPS = 200//BATCH","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:08.030695Z","iopub.execute_input":"2021-07-02T09:48:08.031298Z","iopub.status.idle":"2021-07-02T09:48:08.038268Z","shell.execute_reply.started":"2021-07-02T09:48:08.031254Z","shell.execute_reply":"2021-07-02T09:48:08.036881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.cache().shuffle(BUFFER).batch(BATCH).repeat()\ntrain = train.prefetch(buffer_size=AT)\nval = val.batch(BATCH)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:08.040266Z","iopub.execute_input":"2021-07-02T09:48:08.040732Z","iopub.status.idle":"2021-07-02T09:48:08.060497Z","shell.execute_reply.started":"2021-07-02T09:48:08.04069Z","shell.execute_reply":"2021-07-02T09:48:08.059226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check for Model and Data compatibility","metadata":{}},{"cell_type":"markdown","source":"### Can our data fit into our model? Is the shape, batch size and everything okay? Check compatibility by inferencing with the untrained model.","metadata":{}},{"cell_type":"code","source":"# infer on train dataset\nexample = next(iter(train))\npreds = unet(example[0])\n# visualize an image\nplt.imshow(example[0][60])\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:08.06206Z","iopub.execute_input":"2021-07-02T09:48:08.0627Z","iopub.status.idle":"2021-07-02T09:48:14.4285Z","shell.execute_reply.started":"2021-07-02T09:48:08.062668Z","shell.execute_reply":"2021-07-02T09:48:14.427329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the predicted mask\npred_mask = tf.argmax(preds, axis=-1)\npred_mask = tf.expand_dims(pred_mask, -1)\nplt.imshow(pred_mask[0], cmap='jet', norm=NORM)\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:14.430425Z","iopub.execute_input":"2021-07-02T09:48:14.430904Z","iopub.status.idle":"2021-07-02T09:48:14.718697Z","shell.execute_reply.started":"2021-07-02T09:48:14.430858Z","shell.execute_reply":"2021-07-02T09:48:14.717352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compile the model with the RMSprop optimizer, a low learning rate, the accuracy metric, and the sparse categorical cross entropy loss function","metadata":{}},{"cell_type":"code","source":"def Compile_Model():\n    unet.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            optimizer=keras.optimizers.RMSprop(lr=0.001),\n            metrics=['accuracy']) \nCompile_Model()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:14.72051Z","iopub.execute_input":"2021-07-02T09:48:14.721035Z","iopub.status.idle":"2021-07-02T09:48:14.752801Z","shell.execute_reply.started":"2021-07-02T09:48:14.720924Z","shell.execute_reply":"2021-07-02T09:48:14.751712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"### Trainable part of our model is very small with fewer parameters. Hence we can train the model for more epochs quickly. However, version 8 of this notebook suggested that the model hardly learnt anything after 50 epochs (which was trained for 200 epochs). So we try train the model for 50 epochs by freezing the pre-trained model and unfreezing afterwards.","metadata":{}},{"cell_type":"code","source":"hist_1 = unet.fit(train,\n               validation_data=val,\n               steps_per_epoch=STEPS_PER_EPOCH,\n               validation_steps=VALIDATION_STEPS,\n               epochs=50)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:48:14.754376Z","iopub.execute_input":"2021-07-02T09:48:14.754794Z","iopub.status.idle":"2021-07-02T09:50:22.489779Z","shell.execute_reply.started":"2021-07-02T09:48:14.754751Z","shell.execute_reply":"2021-07-02T09:50:22.487495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make some prediction and visualize them to evaluate the model qualitatively.","metadata":{}},{"cell_type":"code","source":"# select a validation data batch\nimg, mask = next(iter(val))\n# make prediction\npred = unet.predict(img)\nplt.figure(figsize=(20,28))\n\nk = 0\nfor i in pred:\n    # plot the predicted mask\n    plt.subplot(4,3,1+k*3)\n    i = tf.argmax(i, axis=-1)\n    plt.imshow(i,cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Prediction')\n    \n    # plot the groundtruth mask\n    plt.subplot(4,3,2+k*3)\n    plt.imshow(mask[k], cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Ground Truth')\n    \n    # plot the actual image\n    plt.subplot(4,3,3+k*3)\n    plt.imshow(img[k])\n    plt.axis('off')\n    plt.title('Actual Image')\n    k += 1\n    if k == 4: break\nplt.suptitle('Predition After 50 Epochs (No Fine-tuning)', color='red', size=20)  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:03:40.30178Z","iopub.execute_input":"2021-07-02T10:03:40.302273Z","iopub.status.idle":"2021-07-02T10:03:42.586266Z","shell.execute_reply.started":"2021-07-02T10:03:40.302218Z","shell.execute_reply":"2021-07-02T10:03:42.585032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-tune the model after 50 epochs. Unfreeze the downstack, compile the model once again. Re-train it for 100 more epochs.","metadata":{}},{"cell_type":"code","source":"downstack.trainable = True\n# compile again\nCompile_Model()\n# train from epoch 51 to 150\nhist_2 = unet.fit(train,\n               validation_data=val,\n               steps_per_epoch=STEPS_PER_EPOCH,\n               validation_steps=VALIDATION_STEPS,\n               epochs=150, initial_epoch = 50\n                 )","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:50:22.4924Z","iopub.execute_input":"2021-07-02T09:50:22.493332Z","iopub.status.idle":"2021-07-02T09:56:42.014292Z","shell.execute_reply.started":"2021-07-02T09:50:22.493237Z","shell.execute_reply":"2021-07-02T09:56:42.01316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"### The model is trained. Make prediction and visualize the output. Compare the results with groundtruth masks","metadata":{}},{"cell_type":"code","source":"# select a validation data batch\nimg, mask = next(iter(val))\n# make prediction\npred = unet.predict(img)\nplt.figure(figsize=(20,30))\n\nk = 0\nfor i in pred:\n    # plot the predicted mask\n    plt.subplot(4,3,1+k*3)\n    i = tf.argmax(i, axis=-1)\n    plt.imshow(i,cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Prediction')\n    \n    # plot the groundtruth mask\n    plt.subplot(4,3,2+k*3)\n    plt.imshow(mask[k], cmap='jet', norm=NORM)\n    plt.axis('off')\n    plt.title('Ground Truth')\n    \n    # plot the actual image\n    plt.subplot(4,3,3+k*3)\n    plt.imshow(img[k])\n    plt.axis('off')\n    plt.title('Actual Image')\n    k += 1\n    if k == 4: break\nplt.suptitle('Predition After 150 Epochs (By Fine-tuning from 51th Epoch)', color='red', size=20)  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T09:58:37.755671Z","iopub.execute_input":"2021-07-02T09:58:37.756094Z","iopub.status.idle":"2021-07-02T09:58:39.638892Z","shell.execute_reply.started":"2021-07-02T09:58:37.756058Z","shell.execute_reply":"2021-07-02T09:58:39.637586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-tuning has improved the model performance significantly!","metadata":{}},{"cell_type":"markdown","source":"# Performance Curves","metadata":{}},{"cell_type":"markdown","source":"### Plot the performance curves to understand how the model learnt on the data ","metadata":{}},{"cell_type":"code","source":"history_1 = hist_1.history\nacc=history_1['accuracy']\nval_acc = history_1['val_accuracy']\n\nhistory_2 = hist_2.history\nacc.extend(history_2['accuracy'])\nval_acc.extend(history_2['val_accuracy'])\n\nplt.plot(acc[:150], '-', label='Training')\nplt.plot(val_acc[:150], '--', label='Validation')\nplt.plot([50,50],[0.7,1.0], '--g', label='Fine-Tuning')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.ylim([0.7,1.0])\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:48.840287Z","iopub.execute_input":"2021-07-02T10:27:48.84065Z","iopub.status.idle":"2021-07-02T10:27:49.063413Z","shell.execute_reply.started":"2021-07-02T10:27:48.84062Z","shell.execute_reply":"2021-07-02T10:27:49.061946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model performance can be improved by tuning hyper-parameters and increasing the training data with few more data augmentation functions.","metadata":{}},{"cell_type":"markdown","source":"### Thank you for your time!","metadata":{}}]}