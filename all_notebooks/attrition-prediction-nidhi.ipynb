{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n# np.random.seed(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, classification_report, confusion_matrix, roc_auc_score, plot_roc_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/employee-attrition/HR-Employee-Attrition.csv\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > **EDA**","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From above information, we can get to know that there are no null values in any of the columns.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Age'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Attrition'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Attrition'] = np.where(df['Attrition']=='Yes', 1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Attrition'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can see that data is imbalanced since for 'Yes' its only 237 and '1233' for NO. We will check on this later.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['BusinessTravel'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7,7))\nax = sns.countplot(x='Attrition', hue='BusinessTravel', data=df)\nplt.title('Distribution of Truck Configurations')\nplt.xlabel('Attrition')\nplt.ylabel('BusinessTravel')\n\nfor p in ax.patches:\n        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ppl who travel rarely are less likely to attrition","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['DailyRate'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\n\nax = sns.barplot(x='JobRole', y='DailyRate', hue='Attrition', data=df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We see that healthcare representatives earn the most and managers are more likely to be attrited ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Department'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='Department', y='DistanceFromHome', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Sales department members travel the most","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=\"Education\", y='JobRole', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Employees as Research Directors have the max number of degress to their name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['DistanceFromHome'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.barplot(x='DistanceFromHome', y='Attrition', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Education'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['EducationField'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"EducationField\", hue='Attrition', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The rate of attrition is highest for employees under LifeSciences and least for HR employees","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['EmployeeCount'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df.drop(columns=['EmployeeCount'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['Gender'] = np.where(df['Gender']=='Male', 0, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['HourlyRate'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MaritalStatus'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MonthlyIncome'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='StockOptionLevel', y='MonthlyIncome', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top Stock option is NOT so widely provided for employees with high salary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['JobInvolvement'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax = sns.countplot(hue='Attrition', x='TotalWorkingYears', data=df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Employees with less experience have higher chance of attrition","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['JobRole'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='YearsSinceLastPromotion', hue='Attrition', data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['JobSatisfaction'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Over18'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df.drop(columns = ['Over18'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['OverTime'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['OverTime'] = np.where(df['OverTime']=='Yes', 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['OverTime'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StandardHours'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.drop(columns=['StandardHours'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['EmployeeNumber'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.drop(columns=['EmployeeNumber'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['Attrition']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df.drop(columns=['Attrition', 'EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_numeric_cols = x.select_dtypes(include=np.number)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_numeric_cols.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns.difference(df_numeric_cols.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_numeric_cols.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif['Features'] = df_numeric_cols.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif['VIF'] = [variance_inflation_factor(df_numeric_cols.values, i) for i in range(len(df_numeric_cols.columns))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will try to remove the features with high variance after running the model with all the features first\n# Then we can compare the performance after removing each feature with high variance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding categorical data","metadata":{}},{"cell_type":"code","source":"x['Gender'] = np.where(x['Gender'] == 'Male', 0, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat_cols = x.select_dtypes(include='object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat_cols.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_cat_cols.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_cat_encoded = pd.get_dummies(df_cat_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat_encoded.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_final = pd.concat([df_numeric_cols, df_cat_encoded], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_final.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_x = scaler.fit_transform(x_final)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_x_df = pd.DataFrame(data=scaled_x, columns=x_final.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_x_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX, testX, trainY, testY = train_test_split(scaled_x_df,y,test_size=0.3, random_state=220, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainY.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"def model_fit(model, testX, testY, trainX, trainY, cv=True, cv_folds=10):\n    model.fit(trainX, trainY)\n    predictY = model.predict(testX)\n    if cv:\n        cv_score = cross_val_score(model, trainX, trainY, cv=cv_folds, scoring='f1_macro')\n    \n    print(\"CV report: Mean %.3g| Std %.3g| Min %.3g | Max %.3g\"%((np.mean(cv_score), \n                                                                  np.std(cv_score), \n                                                                  np.min(cv_score), \n                                                                  np.max(cv_score))))\n    print(\"Classification Report:\\n\",classification_report(testY, predictY))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(testY, predictY))\n    return predictY\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\npredictY = model_fit(lr, testX, testY, trainX, trainY)\n# plot_roc_curve(lr, trainX, trainY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting a recall of 1, need to check","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = roc_auc_score(testY, predictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, predictY)\nplt.plot(fpr, tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"def knn_model(trainX, trainY, testX, testY, k_value=5, weights='distance', metric='manhattan'):\n#     scaling = StandardScaler()\n#     train_scaled = scaling.fit_transform(trainX)\n#     test_scaled =  scaling.transform(testX)\n    knn = KNeighborsClassifier(n_neighbors=k_value, weights=weights, metric=metric, n_jobs=-1)\n    knn.fit(trainX, trainY)\n    trainpredictY = knn.predict(trainX)\n    testpredictY = knn.predict(testX)\n    return knn, trainpredictY, testpredictY\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k_values in range(3,11):\n    print(\"For k value of {}\".format(k_values))\n    knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, k_values)\n    print(\"Classification report for train\\n\", classification_report(trainY, trainpredictY))\n    print(\"\\n\\n\")\n    print(\"Classification report for test\\n\", classification_report(testY, testpredictY))\n    print(\"\\n\\n\")\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can select the k value of 5 since it provides the highest accuracy of 86","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for weights in ['uniform', 'distance']:\n    print(\"For weights as {}\".format(weights))\n    knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, weights=weights)\n    print(\"Classification Report for train\\n\", classification_report(trainY, trainpredictY))\n    print(\"\\n\\n\")\n    print(\"Classification Report for test\\n\", classification_report(testY, testpredictY))\n    print(\"\\n\\n\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Both uniform and distance weights provide the same accuracy, we will go with distance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for metrics in [\"manhattan\", \"chebyshev\", \"minkowski\", \"euclidean\"]:\n    print(\"For metric {}\\n\".format(metrics))\n    knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, metric=metrics)\n    print(\"Classification Report for train\\n\", classification_report(trainY, trainpredictY))\n    print(\"\\n\\n\")\n    print(\"Classification Report for test\\n\", classification_report(testY, testpredictY))\n    print(\"\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Manhattan is the metric to be chosen, from the accuracy score obtained.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, k_value=7, weights='distance', metric='manhattan')\nprint(\"Classification Report for trained data\\n\", classification_report(trainY, trainpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(trainY, trainpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(trainY, trainpredictY))\ntestY_re = np.array(testY).reshape(-1,1)\n# testpredictY_re = np.array(testpredictY).reshape(-1,1)\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)\n# fpr_re = fpr.reshape(-1,1)\n# tpr_re = tpr.reshape(-1,1)\n# plot_roc_curve(knn, fpr_re, tpr_re)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to remove few features that are highly correlated since we are getting a hight recall of 1.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes\n","metadata":{}},{"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(trainX, trainY)\ntrainpredictY = nb.predict(trainX)\ntestpredictY = nb.predict(testX)\nprint(\"Confusion Matrix for train data \\n\", confusion_matrix(trainY, trainpredictY))\nprint(\"Classification Report for train data\\n\", classification_report(trainY, trainpredictY))\nprint(\"Confusion Matrix for test data \\n\", confusion_matrix(testY, testpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\ntrainY = np.array(trainY).reshape(-1,1)\ntrainpredictY = np.array(trainpredictY).reshape(-1,1)\ntestY = np.array(testY).reshape(-1,1)\ntestpredictY = np.array(testpredictY).reshape(-1,1)\n\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Logistic Regression with higher degrees","metadata":{}},{"cell_type":"code","source":"def poly(trainX, trainY, degrees):\n    poly = PolynomialFeatures(degree=degrees)\n    train_poly = poly.fit_transform(trainX)\n    test_poly = poly.fit_transform(testX)\n    return train_poly, test_poly","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for degrees in [1,2,3]:\n    print(\"For degree {}\".format(degrees))\n    train_poly, test_poly = poly(trainX, trainY, degrees)\n    lr = LogisticRegression()\n    model_fit(lr, test_poly, testY, train_poly, trainY)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No Degrees are required, since degree 1 gave best results in terms of recall, precision, accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NO change in accuracy observed with increase in polynomial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Smote with Logisitc regression, KNN, Gausian NB","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=5)\nprint(trainX.shape)\nprint(trainY.shape)\ntrainX_oversampled, trainY_oversampled = sm.fit_resample(trainX, trainY)\n\n\nprint(\"The training data is changed to \\n\")\n\nprint((pd.DataFrame(data=trainY, columns=['Attrition'])).value_counts())\nprint((pd.DataFrame(data=trainY_oversampled, columns=['Attrition'])).value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\npredictY = model_fit(lr, testX, testY, trainX_oversampled, trainY_oversampled)\nauc = roc_auc_score(testY, predictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, predictY)\nplt.plot(fpr, tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The output from the Smote looks more realistic since 863 entries for Attrition1 and 0 are taken to train the Regression Model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn, trainpredictY, testpredictY = knn_model(trainX_oversampled, trainY_oversampled, testX, testY, k_value=7, weights='distance', metric='manhattan')\nprint(\"Classification Report for trained data\\n\", classification_report(trainY_oversampled, trainpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(trainY_oversampled, trainpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(testY, testpredictY))\ntestY_re = np.array(testY).reshape(-1,1)\n# testpredictY_re = np.array(testpredictY).reshape(-1,1)\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above analysis, the recall, precision, accuracy and roc_auc_score is better for Logistic regression with smote applied.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(trainX_oversampled, trainY_oversampled)\ntrainpredictY = nb.predict(trainX_oversampled)\ntestpredictY = nb.predict(testX)\nprint(\"Confusion Matrix for test data \\n\", confusion_matrix(testY, testpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\n# trainY = np.array(trainY).reshape(-1,1)\n# trainpredictY = np.array(trainpredictY).reshape(-1,1)\n# testY = np.array(testY).reshape(-1,1)\n# testpredictY = np.array(testpredictY).reshape(-1,1)\n\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Hence Logisitc regression with oversampling(smote) technique for the given imbalanced data provides the best result in terms of accuracy, precision, recall, roc_auc_curve","metadata":{}},{"cell_type":"markdown","source":"> Note: Tried removing features with high correlation, but did not see improvement in model performnace, hence the features with high variance are also used to train the model.","metadata":{}}]}