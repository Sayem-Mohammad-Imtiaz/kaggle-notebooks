{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data mining Term Project\n## Hengchao Wang\n\n### Reference. \nBoardGameGeek Reviews Baseline Model\nhttps://www.kaggle.com/ellpeeaxe/boardgamegeek-reviews-baseline-model\n\nWord2vec In Supervised NLP Tasks. Shortcut\nhttps://www.kaggle.com/vladislavkisin/word2vec-in-supervised-nlp-tasks-shortcut/comments\n\nCuz the scale of the dataset is super big. Cannot use one hot expression to exprese words and sentences. It will cause the curse of dimensionality. Which means the matrix is big and sparse to be compute. So I decide to use Word2Vec word embedding model to reduce dimension of matrix. I have two references. The link is shown above. \n\nThe based task of this question is a regression problem. The imput data is 300-dimensional word vector, output is the prediction of rate for each review."},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport re,string,unicodedata\nimport seaborn as sns\nimport gensim\nimport sklearn\n\nfrom pandas import Series\nfrom wordcloud import WordCloud,STOPWORDS\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.corpus import stopwords\nfrom gensim.models import word2vec, Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nimport joblib\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn import ensemble","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get data from csv"},{"metadata":{"trusted":false},"cell_type":"code","source":"# get review and rating columns\nreview_path = 'bgg-13m-reviews.csv'\n\ndata = pd.read_csv(review_path, usecols=[2,3])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# remove null comment\ndef remove_nan(data):\n    data['comment']=data['comment'].fillna('null')\n    data = data[~data['comment'].isin(['null'])]\n    data = data.reset_index(drop=True)\n    return data\ndata = remove_nan(data)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is data describtion. The number of review is 2.637756e+06**"},{"metadata":{"trusted":false},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing\n\nFor data preprocessing I using **tokenizer()** from **NLTK** library to tokenize the words. Load stopword from **NLTK** and load html strips from **beautifulsoup4** library. Use regular expression to remove them and some special characters."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Tokenization of text\ntokenizer=ToktokTokenizer()\n#Setting English stopwords\nstopword_list=nltk.corpus.stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Removing the html strips\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    return text\n#Apply function on review column\ndata['comment']=data['comment'].apply(remove_between_square_brackets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Define function for removing special characters\ndef remove_special_characters(text, remove_digits=True):\n    pattern=r'[^a-zA-z0-9\\s]'\n    text=re.sub(pattern,'',text)\n    return text\n#Apply function on review column\ndata['comment']=data['comment'].apply(remove_special_characters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Stemming the text\ndef simple_stemmer(text):\n    ps=nltk.porter.PorterStemmer()\n    text= ' '.join([ps.stem(word) for word in text.split()])\n    return text\n#Apply function on review column\ndata['comment']=data['comment'].apply(simple_stemmer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#set stopwords to english\nstop=set(stopwords.words('english'))\nprint(stop)\n\n#removing the stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n#Apply function on review column\ndata['comment']=data['comment'].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After we remove the stopword we need to remove the empty review again because come short review after remove stopword will change into empty.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = remove_nan(data)\ndata.to_csv('data_after_remove_st.csv', header=False, index=False, encoding = 'utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"columns = ['rate', 'comment']\ndata = pd.read_csv('data_after_remove_st.csv',names = columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data['comment'] = data.comment.str.lower()\ndata['document_sentences'] = data.comment.str.split('.') \n# data['tokenized_sentences'] = data['document_sentences']\ndata['tokenized_sentences'] = list(map(lambda sentences:list(map(nltk.word_tokenize, sentences)),data.document_sentences))  \ndata['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)), data.tokenized_sentences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Challenge 1\n**Here is a hint: Because the String[] cannot save as csv. The tokenized_sentences after save into csv will change the format into String and cannot load again.** This is one of a challenge I met. At the first few round of training Word2Vec model. The final accuracy is super low. I check the word expression of each word. The output from Word2Vec is less than 0.0001. That means that these word almost doesn't appear in the dataset. That doesn't make sence. So I check the model. The model.wv.vocab.keys() is small too and the vocabelory are latters, not words. So it must be the split problem or the format problem. So I check the type of each variable. The type of \"tokenized_sentences\" is changes. After google the issue. I found the point is you cannot save string[] in csv.\n\nI wrote the wrong code as a comment in next 2 cells."},{"metadata":{"trusted":false},"cell_type":"code","source":"# data.to_csv(\"data_after_pre.csv\",sep=',',index=False, encoding = 'utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# data = pd.read_csv('data_after_pre.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next cell will not be run when I train the Word2Vec. I train the Word2Vec model by using the whole Dataset.\nThe next cell will be run when I train the regression model. Cuz the computation I have only can use 50k reviews to train the regression model. So I use 10k and 50k reviews and compare them."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Take the top 10k after random ordering\ndata = data.reindex(np.random.permutation(data.index))[:100000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# split the data into training data and test data.\ntrain, test, y_train, y_test = train_test_split(data, data['rate'], test_size=.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"type(train.tokenized_sentences[993001])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Collecting a vocabulary\nvoc = []\nfor sentence in train.tokenized_sentences:\n    voc.extend(sentence)\n#     print(sentence)\n\nprint(\"Number of sentences: {}.\".format(len(voc)))\nprint(\"Number of rows: {}.\".format(len(train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"voc[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word2Vec model train save and load\n\nThe number of feature in my Word2Vec model is 300. The matrix using one-hot expression is about 150k * 2.6M. Curse of dimensionality is gone."},{"metadata":{"trusted":false},"cell_type":"code","source":"# word2vector\nnum_features = 300    \nmin_word_count = 3     # Frequency < 3 will not be count in.\nnum_workers = 16       \ncontext = 8           \ndownsampling = 1e-3   \n\n# Initialize and train the model\nW2Vmodel = Word2Vec(sentences=voc, sg=1, hs=0, workers=num_workers, size=num_features, min_count=min_word_count, window=context,\n                    sample=downsampling, negative=5, iter=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_voc = set(W2Vmodel.wv.vocab.keys()) \nprint(len(model_voc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# model save\nW2Vmodel.save(\"Word2Vec2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# model load\nW2Vmodel = Word2Vec.load('Word2Vec2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Challenge 2\n\nTrain the model sentence by sentence is more accurate than the whole review. Cuz the length of the sentence are similar so that the feature of each input is similar. So I did not remove '.' when I remove noise character. That's come from comparison."},{"metadata":{"trusted":false},"cell_type":"code","source":"def sentence_vectors(model, sentence):\n    #Collecting all words in the text\n#     print(sentence)\n    sent_vector = np.zeros(model.vector_size, dtype=\"float32\")\n    if sentence == [[]] or sentence == []  :\n        return sent_vector\n    words=np.concatenate(sentence)\n#     words = sentence\n    #Collecting words that are known to the model\n    model_voc = set(model.wv.vocab.keys()) \n#     print(len(model_voc))\n\n    # Use a counter variable for number of words in a text\n    nwords = 0\n    # Sum up all words vectors that are know to the model\n    for word in words:\n        if word in model_voc: \n            sent_vector += model[word]\n            nwords += 1.\n\n    # Now get the average\n    if nwords > 0:\n        sent_vector /= nwords\n    return sent_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['sentence_vectors'] = list(map(lambda sen_group:\n                                      sentence_vectors(W2Vmodel, sen_group),\n                                      train.tokenized_sentences))\ntest['sentence_vectors'] = list(map(lambda sen_group:\n                                    sentence_vectors(W2Vmodel, sen_group), \n                                    test.tokenized_sentences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def vectors_to_feats(df, ndim):\n    index=[]\n    for i in range(ndim):\n        df[f'w2v_{i}'] = df['sentence_vectors'].apply(lambda x: x[i])\n        index.append(f'w2v_{i}')\n    return df[index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = vectors_to_feats(train, 300)\nX_test = vectors_to_feats(test, 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.concat([X_train, y_train], axis=1)\ntest = pd.concat([X_test, y_test], axis=1)\ntrain.to_csv('train_w2v_100k.csv')\ntest.to_csv('test_w2v_100k.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('train_w2v_1000k.csv').drop(columns = 'Unnamed: 0')\ntest = pd.read_csv('test_w2v_1000k.csv').drop(columns = 'Unnamed: 0')\nX_train = train.drop(columns = 'rate')\nX_test = test.drop(columns = 'rate')\ny_train = train.rate\ny_test = test.rate","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implement different regression model\nI implement 4 regression model and compare them with Root Mean Square Error (RMSE) and Mean absolute error(MAE).\n\n**RMSE:** Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. It can tells you how concentrated the data is around the line of best fit. \n\n**MAE:**  Mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. It is thus an arithmetic average of the absolute errors |ei|=|yi-xi|, where yi is the prediction and xi the true value. "},{"metadata":{},"cell_type":"markdown","source":"### Linear regression model\nLinear regression is a basic and commonly used type of predictive analysis. Parameter calculation of linear equation using least squares method.\n\n[Linear regression introduction](https://machinelearningmastery.com/linear-regression-for-machine-learning/)"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_lr = LinearRegression()\nmodel_lr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr_y_predict=model_lr.predict(X_test)\ny_test = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test,lr_y_predict))\n\n# (MAE)\nmae = mean_absolute_error(y_test, lr_y_predict)\n\nprint('linear_regression_rmse = ', rmse)\nprint('linear_regression_mae = ', mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"joblib.dump(model_lr, 'save/model_lr.pkl')\n\n# model_lr = joblib.load('save/model_lr_1000k.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVR model\nSupport vector regression(SVR) is an application of support vector machine(SVM) to regression problem.\n\nRegression is like looking for the internal relationship of a bunch of data. Regardless of whether the pile of data consists of several categories, a formula is obtained to fit these data. When a new coordinate value is given, a new value can be obtained. So for SVR, it is to find a face or a function, and you can fit all the data (that is, all data points, regardless of the type, the closest distance from the data point to the face or function)\n\n[SVR introduction introduction](https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2)"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_svm = SVR()\nmodel_svm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svm_y_predict=model_svm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test,svm_y_predict))\n\n# (MAE)\nmae = mean_absolute_error(y_test, svm_y_predict)\n\nprint('svm_rmse = ', rmse)\nprint('svm_mae = ', mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"joblib.dump(model_lr, 'save/model_svm.pkl')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Ridge model\nIn the Bayesian viewpoint, we formulate linear regression using probability distributions rather than point estimates. The response, y, is not estimated as a single value, but is assumed to be drawn from a probability distribution.\n\nThe output, y is generated from a normal (Gaussian) Distribution characterized by a mean and variance. The mean for linear regression is the transpose of the weight matrix multiplied by the predictor matrix. The variance is the square of the standard deviation σ (multiplied by the Identity matrix because this is a multi-dimensional formulation of the model).\n\n[Bayesian Ridge introduction](https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7)"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_bayes_ridge = BayesianRidge()\nmodel_bayes_ridge.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bayes_y_predict = model_bayes_ridge.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test,bayes_y_predict))\n\n# (MAE)\nmae = mean_absolute_error(y_test, bayes_y_predict)\n\nprint('BayesianRidge_rmse = ', rmse)\nprint('BayesianRidge_mae = ', mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"joblib.dump(model_bayes_ridge, 'save/model_bayes.pkl')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regression model\n\nRandom forest is a bagging technique and not a boosting technique. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees.\n\nThe throught of Random Forest Regression is using the Boosting and ensemble in decision tree. In the lecture mentioned.\n\n[Random Forest Regression introduction](https://towardsdatascience.com/random-forest-and-its-implementation-71824ced454f)"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_random_forest_regressor = ensemble.RandomForestRegressor(n_estimators=20)\nmodel_random_forest_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"random_forest_y_predict = model_random_forest_regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# (RMSE)\nrmse = np.sqrt(mean_squared_error(y_test,random_forest_y_predict))\n\n# (MAE)\nmae = mean_absolute_error(y_test, random_forest_y_predict)\n\nprint('BayesianRidge_rmse = ', rmse)\nprint('BayesianRidge_mae = ', mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"joblib.dump(model_random_forest_regressor, 'save/model_random_forest.pkl')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict function for one review with four model"},{"metadata":{"trusted":false},"cell_type":"code","source":"def predict(text):\n    model_lr = joblib.load('save/model_lr.pkl')\n    model_svm = joblib.load('save/model_svm.pkl')\n    model_random_forest_regressor = joblib.load('save/model_random_forest.pkl')\n    model_bayes_ridge = joblib.load('save/model_bayes.pkl')\n    data = {'comment': Series(text)}\n    data = pd.DataFrame(data)\n    print(data)\n    data['comment'] = data['comment'].apply(remove_between_square_brackets)\n    data['comment'] = data['comment'].apply(remove_special_characters)\n    data['comment'] = data['comment'].apply(simple_stemmer)\n    data['comment'] = data['comment'].apply(remove_stopwords)\n\n    data['comment'] = data.comment.str.lower()\n    data['document_sentences'] = data.comment.str.split('.')\n    data['tokenized_sentences'] = data['document_sentences']\n    data['tokenized_sentences'] = list(\n        map(lambda sentences: list(map(nltk.word_tokenize, sentences)), data.document_sentences))\n    data['tokenized_sentences'] = list(\n        map(lambda sentences: list(filter(lambda lst: lst, sentences)), data.tokenized_sentences))\n    print(data)\n    # sentence = data['tokenized_sentences'][0]\n    W2Vmodel = Word2Vec.load(\"Word2Vec2\")\n\n    data['sentence_vectors'] = list(map(lambda sen_group:\n                                        sentence_vectors(W2Vmodel, sen_group),\n                                        data.tokenized_sentences))\n    text = vectors_to_feats(data, 300)\n    print(text)\n    lr_y_predict = model_lr.predict(text)\n    svm_y_predict = model_svm.predict(text)\n    bayes_y_predict = model_bayes_ridge.predict(text)\n    random_forest_y_predict = model_random_forest_regressor.predict(text)\n\n    return lr_y_predict, svm_y_predict, random_forest_y_predict, bayes_y_predict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(predict([\"This is a great game.  I've even got a number of non game players enjoying it.  Fast to learn and always changing.\",\n        \"This is a great game.  I've even got a number of non game players enjoying it.  Fast to learn and always changing.\"]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}