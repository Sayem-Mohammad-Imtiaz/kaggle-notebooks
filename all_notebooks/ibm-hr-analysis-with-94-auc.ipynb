{"cells":[{"metadata":{"_cell_guid":"0a96149b-8d98-4262-b3c4-bbc9d3413382","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"62d310cc891ec09c95ad9bb2ac53194bb3edd98e"},"cell_type":"markdown","source":"**Introduction: The objective of the current code is to predict whether an employee is going to resign.**"},{"metadata":{"_cell_guid":"97602284-3d8c-4b06-8a58-95ca288856e7","_uuid":"509958860d7d704b8a6336c90f6d581d4cdffb02"},"cell_type":"markdown","source":"Importing required libraries"},{"metadata":{"_cell_guid":"e7650c06-16d7-4311-b8cc-ab7bd623090b","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"8f0780cca91a478aed63e785f1e6bf4edfc5ed3c","collapsed":true,"trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as sm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88235bf5c071deb86484fc119e4dc69ea967c516"},"cell_type":"markdown","source":"Define a function for visualization of ROC curves of further analyses"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"36ef9afc5d56ac499750ba2e0876c0e5082b5102"},"cell_type":"code","source":"def ROC_GEN(Title, Labels, Output): \n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    fpr, tpr, _ = roc_curve(Labels, Output)   \n    roc_auc = auc(fpr, tpr)    \n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(Title)\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return;","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8eb8f178-550e-438e-80d8-8100e2d76d79","_uuid":"e4d16bb907bf16a4f9dc283bede5f91b70bd7cc4"},"cell_type":"markdown","source":"Reading data from a flat-file, Obtaining dimention of it"},{"metadata":{"_cell_guid":"e7712c8b-3889-4a58-975f-9998f5467512","_uuid":"4a0b625b4d5f718943512cda89499f7cbc75c8dd","collapsed":true,"trusted":false},"cell_type":"code","source":"data = pd.read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\nrows = data.shape[0]\ncolumns = data.shape[1]\nprint(\"The dataset contains {0} rows and {1} columns\".format(rows, columns))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"64499eee-7c01-4881-88ec-d28e047e0553","_uuid":"886cb8dcafd7473353ed44310441186099bc9cac"},"cell_type":"markdown","source":"Let's take a look at the header and the ype of data columns."},{"metadata":{"_cell_guid":"323e45eb-a00f-4fb4-b260-715b7d2e77fb","_uuid":"84a0c6d8dd973ab15807770ade7f0bf5f609ca17","collapsed":true,"trusted":false},"cell_type":"code","source":"data.head(1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"358f172f-4225-4114-b0b5-6d2bd24edaac","_uuid":"853078f45d2b6c35e16d586c2d49532ff960f955"},"cell_type":"markdown","source":"Therefore, this data is composed of both numerical and string fields. Now, let's perform some statistical processes on the data. By mean operation, the range of each numerical field is discovered.  "},{"metadata":{"_cell_guid":"1ba507dd-1087-4b3a-aabb-4796fd9d7d9d","_uuid":"c2d8e52add3434357d0d430c9e8211265c1d7876","collapsed":true,"trusted":false},"cell_type":"code","source":"data.mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c46065fd-3d47-4af8-993a-bc0b4dc8c202","_uuid":"2338ef090012e47c7b9d4f728488bead452ab057"},"cell_type":"markdown","source":"Now, we compare some of the attributes (e.g., \"Age\", \"Education\" and \"JobLevel\" with each other."},{"metadata":{"_cell_guid":"fa637934-cb0e-4156-8a3f-0eda9162e5a8","_uuid":"391f6355df49d65c6d6538ee63511ab73d2cb181","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.pairplot(data[[\"Age\", \"Education\", \"JobLevel\"]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe997b9e-6b31-4556-809d-8a15f610f934","_uuid":"c974f27b95106888b5e7949d5beae9bdb259e4ae"},"cell_type":"markdown","source":"Some general information can be extracted from the above comparison:\n\n1. Education and job level of older employees is higer than the younger ones.\n2, There is not any considerable relation between education and job level.\n\nIn the next step, using Kmeans clustering, we cluster the data and show them.\n\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"96c6762dde1ca4baff9b52a1da7689fd7bd7cb42"},"cell_type":"code","source":"kmeans_model = KMeans(n_clusters=5, random_state=1)\ngood_columns = data._get_numeric_data().dropna(axis=1)\nkmeans_model.fit(good_columns)\nlabels = kmeans_model.labels_\npca_2 = PCA(2)\nplot_columns = pca_2.fit_transform(good_columns)\nplt.scatter(x=plot_columns[:,0], y=plot_columns[:,1],c=labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47089ab7-74bb-42e3-be90-158962069767","_uuid":"00f10be78dbdcbf13829e8d148b5cadf2a3a0dcf"},"cell_type":"markdown","source":"Then, we make a copy of data to make some changes on the copy so that the original data doesn't affect.Since we want to use output labels of \"Attrition\" in our numerical process. we have to use the equal numbers. For example \"1\" and \"0\" for \"Yes\" and\"No\" respectively. We divide the data to two sets of test and train for further processes."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6acd67200b22c303841fe2bd954b8526273925f3"},"cell_type":"code","source":"data_copy = data.copy()\ndata_copy[\"Attrition\"] = data_copy[\"Attrition\"].replace([\"Yes\",\"No\"],[1,0]);\ntrain = data_copy.sample(frac=0.5, random_state=1)\ntest = data_copy.loc[~data_copy.index.isin(train.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24072effcce896be21f39d53313510e80188afb1"},"cell_type":"markdown","source":"Let's take a deeper look at the data. Thus, we look at 20 records of data."},{"metadata":{"trusted":false,"_uuid":"db33801a2f4b2edb4d8264f8f7b45d01f446589f"},"cell_type":"code","source":"data.head(20)","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"69e73e331aad5c00b4c4e448b60d7a9d22654b0b"},"cell_type":"markdown","source":"It can be seen, Some attributes like \"EmployeeCount\" and \"StandardHours\" don't vary for various records. Thus, we can ignore them for the later processing. On the other hand, since text fields can not be used in numerical processing, we have to ignore them as well. The rest of the attributes are selected for the processing."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"dee98c4febd96a65522e72967e07339185d10d1a"},"cell_type":"code","source":"Effective_Columns = [\"Age\", \"DailyRate\", \"DistanceFromHome\", \"Education\", \"MonthlyIncome\",\"MonthlyRate\" ,\"NumCompaniesWorked\",\n\"PercentSalaryHike\",\"PerformanceRating\",\"RelationshipSatisfaction\",\n\"StockOptionLevel\",\"TotalWorkingYears\",\"TrainingTimesLastYear\",\"WorkLifeBalance\",\"YearsAtCompany\",\n\"YearsInCurrentRole\",\"YearsSinceLastPromotion\",\"YearsWithCurrManager\",\n\"EmployeeNumber\",\"EnvironmentSatisfaction\",\"HourlyRate\",\"JobInvolvement\",\"JobLevel\",\"JobSatisfaction\"]","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"c3e2d583ed7127e1a7c88acdf9bf2b82db2d701e"},"cell_type":"markdown","source":"Now, we build and train a Random forest for our classification problem."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4cea7b6ca4f1f154ddc9449ea5dd29242d7f16ad"},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=3)\nrf.fit(train[Effective_Columns], train[\"Attrition\"])\npredictions = rf.predict(test[Effective_Columns])","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"5ad132ecce92c24e2773f98e93a3b9b5917f6caf"},"cell_type":"markdown","source":"For the visualization of results, we want to draw ROC using our previous-defined function. But, we have to do some pre-processing for type casting."},{"metadata":{"trusted":false,"_uuid":"e6c39694fa7a8a6f07aa157e9a84c14cd1837ed3"},"cell_type":"code","source":"len = predictions.shape[0];\ntest_label = [0 for x in range(len)] \ntest_attr = test[\"Attrition\"];\n\nfor i in range(len):\n    test_label[i] = test_attr[test_attr.index[i]];\n\nROC_GEN('RF1-50%', test_label, predictions)","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"003932cb366bed7adb88866870d3f5402d480438"},"cell_type":"markdown","source":"We got AUC (Area Under Curve) of 70 percent. Let's change the structure of RF to see what happens..."},{"metadata":{"trusted":false,"_uuid":"1d7360821dcf8ab78134c7980339cfd4a5358f40"},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=10)\nrf.fit(train[Effective_Columns], train[\"Attrition\"])\npredictions = rf.predict(test[Effective_Columns])\n\nROC_GEN('RF2-50%', test_label, predictions)","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"a779627b82def54a971dfafa22f0c6b9e5d4bf81"},"cell_type":"markdown","source":"No improvement! we should improve it more. Let's try SVM (Support Vector Machine) by two kernels of \"linear\" and \"RBF\". First we need to perform some preprocessing for type-casting and preparing train and test sets."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b31ea9a4b31656a90d846a78736c4b3c8c07f5a5"},"cell_type":"code","source":"data = data_copy[Effective_Columns];\nout = data_copy[\"Attrition\"];\n\nlen = out.shape[0];\ndout = [0 for x in range(len)] \nfor i in range(len):\n    dout[i] = out[out.index[i]];\n\n\ndin = [[0 for x in range(data.shape[1])] for y in range(len)] \nfor i in range(len):\n    for j in range(data.shape[1]-9,data.shape[1]):#data.shape[1]):       \n        din[i][j] = data[Effective_Columns[j]][i];\n\nX_train, X_test, y_train, y_test = train_test_split(din, dout, test_size=.5,\n                                                    random_state=0)","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"f34fa353cdcc775091ab1c6faf2943d73d97bae8"},"cell_type":"markdown","source":"First, we try a linear SVM."},{"metadata":{"trusted":false,"_uuid":"2800c5a4dbcef8c44293132feb725be9d8df0ddd"},"cell_type":"code","source":"random_state = np.random.RandomState(0)\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('LinearSVM-50%', y_test, y_score)","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"230f835b3e67d87c40bd1486053160a589a37b16"},"cell_type":"markdown","source":"Therefore, results is similar ro RF. Let's change the kernel function to \"RBF\"."},{"metadata":{"trusted":false,"_uuid":"0ee5f9e4ba608827a2166cb3a0ca8b43ab70a1cc"},"cell_type":"code","source":"classifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('RBFSVM-50%', y_test, y_score)","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"59466c166611bcc775ed81ce7f6fe88b020d5fc0"},"cell_type":"markdown","source":"The performance fell down. In the next step we increase the percentage of the training data (from 50% to 80%) and repeat RF and SVM experiments for them."},{"metadata":{"trusted":false,"_uuid":"b15b1e51f95740e0cf21734711af41a0f0004563"},"cell_type":"code","source":"train = data_copy.sample(frac=0.8, random_state=1)\ntest = data_copy.loc[~data_copy.index.isin(train.index)]\n\nrf = RandomForestRegressor(n_estimators=100, min_samples_leaf=3)\nrf.fit(train[Effective_Columns], train[\"Attrition\"])\npredictions = rf.predict(test[Effective_Columns])\n\nlen = predictions.shape[0];\ntest_label = [0 for x in range(len)] \ntest_attr = test[\"Attrition\"];\n\nfor i in range(len):\n    test_label[i] = test_attr[test_attr.index[i]];\n\nROC_GEN('RF-80%', test_label, predictions)\n\n\nX_train, X_test, y_train, y_test = train_test_split(din, dout, test_size=.2,\n                                                    random_state=0)\n######################################## SVM LINEAR \nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('LinearSVM-80%', y_test, y_score)\n\n\n######################################## SVM RBF \nclassifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('RBFSVM-80%', y_test, y_score)","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"f68fcc008c53f2037834835ac2cd45bfd0539f7b"},"cell_type":"markdown","source":"For RF and linear SVM, the performance has been increased by 5-6 %. But it is still poor. Let's extend the feature (attribute) vector. We undertake it with numerizing of the text-based attributes and adding them to the previous feature vector."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"358df5b8a85abb52a4e8606336f9c3ad5c84353b"},"cell_type":"code","source":"Text_Baesd_columns = [\"BusinessTravel\",\"Department\", \"EducationField\",\"OverTime\",\"Gender\",\"JobRole\",\"MaritalStatus\"];\n#data[Text_baesd_columns]\ndata_copy[Text_Baesd_columns]\n\ndata_corrected = data_copy.copy()\n\n\ndata_corrected[\"BusinessTravel\"] = data_copy[\"BusinessTravel\"].replace([\"Travel_Rarely\",\"Travel_Frequently\",\"Non-Travel\"],[0,1,2]);\n\ndata_corrected[\"MaritalStatus\"] = data_copy[\"MaritalStatus\"].replace([\"Single\",\"Married\",\"Divorced\"],[0,1,2]);\ndata_corrected[\"Gender\"] = data_copy[\"Gender\"].replace([\"Female\",\"Male\"],[0,1]);\n\ndata_corrected[\"Department\"] = data_copy[\"Department\"].replace([\"Sales\",\"Research & Development\",\"Human Resources\"],[0,1,2]);\n\ndata_corrected[\"EducationField\"] = data_copy[\"EducationField\"].replace([\"Life Sciences\",\"Other\",\"Medical\",\"Marketing\",\"Technical Degree\",\"Human Resources\"],[0,1,2,3,4,5]);\n\ndata_corrected[\"OverTime\"] = data_copy[\"OverTime\"].replace([\"Yes\",\"No\"],[0,1]);\n\ndata_corrected[\"JobRole\"] = data_copy[\"JobRole\"].replace([\"Sales Executive\",\"Sales Representative\",\"Research Scientist\",\"Laboratory Technician\",\"Manufacturing Director\",\"Healthcare Representative\",\"Manager\",\"Research Director\",\"Human Resources\"],[0,1,2,3,4,5,6,7,8]);\n\nCorrected_columns = Effective_Columns + Text_Baesd_columns ","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"8bb5581fb5ceedb5dd3499e6cb1ee04767786de2"},"cell_type":"markdown","source":"Now with the new data, we retrain and rebuild our system and test it again."},{"metadata":{"trusted":false,"_uuid":"33bee5bcbe8ee4b043f695f582bcdcd2cd1834f6"},"cell_type":"code","source":"######################################## Classification with new columns\ntrain = data_corrected.sample(frac=0.5, random_state=1)\ntest = data_corrected.loc[~data_corrected.index.isin(train.index)]\nrf = RandomForestRegressor(n_estimators=100, min_samples_leaf=3)\nrf.fit(train[Corrected_columns], train[\"Attrition\"])\npredictions = rf.predict(test[Corrected_columns])\n\nlen = predictions.shape[0];\ntest_label = [0 for x in range(len)] \ntest_attr = test[\"Attrition\"];\n\nfor i in range(len):\n    test_label[i] = test_attr[test_attr.index[i]];\n\nROC_GEN('Corrected-Data-RF1-50%', test_label, predictions)\n\n######################################## RANDOM FOREST2\nrf = RandomForestRegressor(n_estimators=100, min_samples_leaf=10)\nrf.fit(train[Corrected_columns], train[\"Attrition\"])\npredictions = rf.predict(test[Corrected_columns])\n\nROC_GEN('Corrected-Data-RF2-50%', test_label, predictions)\n\n#---------------SVM PREPARE \ndata = data_copy[Corrected_columns];\nout = data_copy[\"Attrition\"];\nlen = out.shape[0];\ndout = [0 for x in range(len)] \n#test_attr = test[\"Attrition\"];\n\nfor i in range(len):\n    dout[i] = out[out.index[i]];\n\n\ndin = [[0 for x in range(data.shape[1])] for y in range(len)] \n\ndata_input = data_corrected[Corrected_columns]\n\n\nfor i in range(len):\n    for j in range(data.shape[1]-9,data.shape[1]):  #data_input.shape[1]   \n        din[i][j] = data_input[Corrected_columns[j]][i];\n    \n    \n    \nX_train, X_test, y_train, y_test = train_test_split(din, dout, test_size=.5,\n                                                    random_state=0)\n\n######################################## SVM LINEAR \nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('Corrected-Data-LinearSVM-50%', y_test, y_score)\n\n\n######################################## SVM RBF \nclassifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('Corrected-Data-RBFSVM-80%', y_test, y_score)\n\n#§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§ Train 80%\n#§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§ Train 80%\n\ntrain = data_corrected.sample(frac=0.8, random_state=1)\ntest = data_corrected.loc[~data_corrected.index.isin(train.index)]\n\n######################################## RANDOM FOREST1\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=100, min_samples_leaf=3)\nrf.fit(train[Corrected_columns], train[\"Attrition\"])\npredictions = rf.predict(test[Corrected_columns])\n\nlen = predictions.shape[0];\ntest_label = [0 for x in range(len)] \ntest_attr = test[\"Attrition\"];\n\nfor i in range(len):\n    test_label[i] = test_attr[test_attr.index[i]];\n\nROC_GEN('Corrected-Data-RF1-80%', test_label, predictions)\n\n\nX_train, X_test, y_train, y_test = train_test_split(din, dout, test_size=.2,\n                                                    random_state=0)\n######################################## SVM LINEAR \nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('Corrected-Data-LinearSVM-80%', y_test, y_score)\n\n\n######################################## SVM RBF \nclassifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('Corrected-Data-RBFSVM-80%', y_test, y_score)","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"0a46a81676c57b25b4bd6a7212a2675892a23ab0"},"cell_type":"markdown","source":"The best result so far is 78% \"corrected-Data-RF1-50%\". In the sence of data labels (i.e., output) the data is not balanced. Thus , we should balance it."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"be49b311223e895b0873fb1d731139f44d9f47cc"},"cell_type":"code","source":"kind = ['svm'];\nsm = [SMOTE(kind=k) for k in kind]\nX_resampled = []\ny_resampled = []\n#X_res_vis = []\nfor method in sm:\n    X_res, y_res = method.fit_sample(din, dout)","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"4fd6bc3f147c509c6c9614533e86bebc21407da7"},"cell_type":"markdown","source":"Again, we perform our analysis with the balanced data."},{"metadata":{"trusted":false,"_uuid":"656de7ba607202ed36da4b89fd96fbf384aee07c"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=.2,\n                                                    random_state=0)\n######################################## SVM LINEAR \nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('Balanced-Data-LinearSVM-80%', y_test, y_score)\n\n\n######################################## SVM RBF \nclassifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\nROC_GEN('Balanced-Data-RBFSVM-80%', y_test, y_score)\n\nfor method in sm:\n    X_res, y_res = method.fit_sample(data_corrected[Corrected_columns], data_corrected[\"Attrition\"])\n\n\nrf = RandomForestRegressor(n_estimators=100, min_samples_leaf=3)\nrf.fit(X_train, y_train)\npredictions = rf.predict(X_test)\n\nROC_GEN('Balanced-Data-RF1-80%', y_test, predictions)","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"223664aeb9e4f45fe23466caa41bfd3a64a51aeb"},"cell_type":"markdown","source":"Congradulation!!!!! We got AUC 0f 94% using the balanced data and random forest\n\nI will be very glad to leran more from your nice comments\n\nThanks for your attention!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}