{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Naver Conversation of the Day","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![Conversation of the Day](https://raw.githubusercontent.com/rareloto/workspace/master/naverdictionary-conversationoftheday-scraper/naver-conversationoftheday-20200815-cropped.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hi! Welcome to my notebook.\n\nI want to create a compilation of **Korean - English sentence pairs** from **Naver Dictionary's Conversation of the Day**.\n\nAt first I thought, I could type them out one by one, day by day to reinforce my Korean language learning in addition to creating a dataset for future projects (*like maybe a bilingual AI chatbot*).\n\nBut then, I remembered something called **'web scraping'**. Simply put, it is a technique of automatically extracting (*or scraping*) specific data you want from websites and saving them to a file or a database.\n\nIn this notebook, I'm going to take my first dip in web scraping in order to compile months, years of 'Conversation of the Day' from [Naver Dictionary](https://learn.dict.naver.com/conversation#/korean-en).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What I'm Expecting to Get From Scraping","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nconvos_samp = pd.read_csv('../input/naver-dictionary-conversation-of-the-day/conversations.csv').fillna('')\nconvos_samp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"convo_titles_samp = pd.read_csv('../input/naver-dictionary-conversation-of-the-day/conversation_titles.csv').fillna('')\nconvo_titles_samp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lots and lots of Korean - English conversation parallel text pairs","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's get started!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import urllib.request\nimport re\n\n# Let's try one webpage for now\nurl = 'https://learn.dict.naver.com/conversation#/korean-en'\n\npage = urllib.request.urlopen(url)\npage = str(page.read().decode())\n\n# problem: not retrieving complete page html like Firefox html download does\n#          BeautifulSoup doesn't seem to retrieve complete html either\n# possible solution: should i try using headless firefox?\n# solved! - In-kernel Web scraping via a headless Firefox browser with Selenium at the bottom page","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find patterns in the html code around the data we want to extract**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![View source code](https://raw.githubusercontent.com/rareloto/workspace/master/naverdictionary-conversationoftheday-scraper/photos/view-page-source-find-patterns-koreng.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Extract data from html:** \n\n*Korean - English sentence pairs, grammar, related words, date*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# regex to find conversation date\ndate = re.findall(r'var regionDate = \"([0-9]+)\"', page)\n# regex to find conversation title\nconvo_title = re.findall(r'id=\"ogTitle\" content=\"(.+)\">', page)\n# regex to extract sentence pairs\neng_sents = re.findall(r'<div class=\"txt_trans ng-binding\" ng-show=\"transDisplay\" ng-bind=\"item.trsl_sentence\">(.+)<.div>', page)\n# how to strip html from text - jxb-bind-compiled-html binding ?\nkor_sents = re.findall(r'<span class=\"u_word_dic\" data-hook=\"tip\" data-type=\"arken\" data-lang=\"ko\">(.+)</span>.</span></span>', page)\n\n# extracting other data (e.g. conversation title, grammar, grammar description, related words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's see if we're able to extract the data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kor_sents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_sents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sentence pairs were not extracted because the page source we retrieved is incomplete.\n\nMaybe I'll try a headless Firefox browser here to download the complete webpage from the browser.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Web Scraping via headless Firefox with Selenium","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Installing Firefox browser","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking kernel OS info\n!cat /etc/os-release","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! Just like what I'm using right now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downloading Firefox for Linux\n!wget 'https://download-installer.cdn.mozilla.net/pub/firefox/releases/79.0/linux-x86_64/en-US/firefox-79.0.tar.bz2'\n\n# Extracting Firefox binary\n!tar -xjf 'firefox-79.0.tar.bz2'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking working directory\n!ls /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding read/write/execute capabilities to 'firefox' directory\n!chmod -R 777 '../working/firefox'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing Firefox dependencies\n!apt-get install -y libgtk-3-0 libdbus-glib-1-2 xvfb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Installing GeckoDriver ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing Python module for automatic handling of GeckoDriver download and installation\n!pip install webdriverdownloader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing GeckoDriver\nfrom webdriverdownloader import GeckoDriverDownloader\n\ngdd = GeckoDriverDownloader()\ngdd.download_and_install('v0.23.0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing Selenium\n!pip install selenium","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Python modules to use\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Image\nimport time\n\nfrom selenium import webdriver as selenium_webdriver\nfrom selenium.webdriver.firefox.options import Options as selenium_options\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities as selenium_DesiredCapabilities\n\nfrom selenium.webdriver.common.by  import By as selenium_By\nfrom selenium.webdriver.support.ui import Select as selenium_Select\nfrom selenium.webdriver.support.ui import WebDriverWait as selenium_WebDriverWait\nfrom selenium.webdriver.support    import expected_conditions as selenium_ec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up a virtual screen for Firefox\n!export DISPLAY=:99","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Firing up a headless browser session with a screen size of 1920x1080\nbrowser_options = selenium_options()\nbrowser_options.add_argument(\"--headless\")\nbrowser_options.add_argument(\"--window-size=1920,1080\")\n\ncapabilities_argument = selenium_DesiredCapabilities().FIREFOX\ncapabilities_argument[\"marionette\"] = True\n\nbrowser = selenium_webdriver.Firefox(\n    options=browser_options,\n    firefox_binary=\"../working/firefox/firefox\",\n    capabilities=capabilities_argument\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Navigating to website\nbrowser.get(\"https://learn.dict.naver.com/conversation#/korean-en\")\nprint(browser.current_url)\n\n# Giving the page up to 10 seconds to load\nwait = selenium_WebDriverWait(browser, 10)\nwait.until(selenium_ec.visibility_of_element_located((selenium_By.XPATH, '//div[@class=\"reading_lst_wrap\"]')))\n\n# Taking a screenshot of the webpage\nbrowser.save_screenshot(\"screenshot.png\")\nImage(\"screenshot.png\", width=800, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome! Looks like we're on the same page.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Viewing page source**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Waiting for another 10 seconds to make sure the page is complete\ntime.sleep(10)\n\n# Retrieving page source\npage = browser.page_source\npage[0:1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A bit messy.. Let's use regular expressions to find what we need.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# regex to extract conversation date\ndate = re.findall(r'var regionDate = \"([0-9]+)\"', page)\n# regex to extract conversation title in korean\nkor_title = re.findall(r'id=\"ogTitle\" content=\"오늘의 회화 - (.+)\">', page)\n# regex to extract conversation title in english\neng_title = re.findall(r'<span class=\"txt_trans ng-binding\" ng-bind=\"title_translation\">(.+)</span>', page)\n# regex to extract sentence pairs\neng_sents = re.findall(r'<div.+item.trsl_sentence\">(.+)</div>', page)\nkor_sents = re.findall(r'<span class=\"u_word_dic\" data-hook=\"tip\" data-type=\"arken\" data-lang=\"ko\">(.+)</span></span>', page)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Did we get it?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kor_title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_sents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kor_sents[0:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stripping HTML tags from text\ndef strip_tags(sent):\n    sent = re.sub(r'<.+?>', '', sent)\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kor_sents = list(map(strip_tags, kor_sents))\nkor_sents = kor_sents[0:len(eng_sents)]\nkor_sents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sweet!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Extracting more variables**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting grammar of the day\ngrammar = re.findall(r'<span jxb-bind-compiled-html.+item[.]entry_name.+\"ng-scope\">(.+)</span></span>\\s+</div>', page)\ngrammar = list(map(strip_tags, grammar))\ngrammar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting grammar description\ngrammar_desc = re.findall(r'<span class=\"txt_trans ng-binding\" ng-bind=\"item.mean\">(.+)</span>\\s+</div>', page)\ngrammar_desc = list(map(strip_tags, grammar_desc))\ngrammar_desc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting grammar of the day sentence examples\ngrammar_sents_eng = re.findall(r'<span class=\"txt_trans ng-binding\" ng-bind-html=\"desc[.]trans.+toHtml\">(.+)</span>', page)\ngrammar_sents_eng","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grammar_sents_kor = re.findall(r'<span class=\"txt_origin ng-isolate-scope\" jxb-bind-compiled-html=\"toAutolinkText\\(desc[.]origin\\)\"><span class=\"ng-scope\"><span class=\"u_word_dic\" data-hook=\"tip\" data-type=\"arken\" data-lang=\"ko\">(.+)</span></span>', page)\ngrammar_sents_kor = list(map(strip_tags, grammar_sents_kor))\ngrammar_sents_kor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Organizing variables for DataFrame creation**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's create two DataFrames:\n\n**convo_titles**\n(with columns: date, kor_title, eng_title, grammar, grammar_desc, grammar_sents)\n\n**convos**\n(with columns: date, conversation_id, kor_sent, eng_sent, qna_id)\n\nEach convo_title has more or less 4 - 8 Korean-English sentence pairs found in convos_table.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Creating convo_titles DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convo_titles dataframe columns\ntitle_cols = {\n    'date': date,\n    'kor_title': kor_title,\n    'eng_title': eng_title,\n    'grammar': grammar,\n    'grammar_desc': grammar_desc\n}\n\n# Creating convo_titles DataFrame\nconvo_titles = pd.DataFrame(title_cols)\nconvo_titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding new columns: grammar sentence examples    \nfor i in range(len(grammar_sents_eng)):\n    col = f'grammar_kor_sent_{i+1}'\n    convo_titles[col] = grammar_sents_kor[i]\n    col = f'grammar_eng_sent_{i+1}'\n    convo_titles[col] = grammar_sents_eng[i]\n    \nconvo_titles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating convos DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convos dataframe columns\nconvos_cols = {\n    'date': [date for date in date for _ in range(len(eng_sents))],\n    'conversation_id': [id+1 for id, _ in enumerate(eng_sents)],\n    'kor_sent': kor_sents,\n    'eng_sent': eng_sents,\n    'qna_id': ''  # from sender or receiver, message or feedback\n}\n\n# Creating convos DataFrame\nconvos = pd.DataFrame(convos_cols)\nconvos","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's do these for years' worth of conversations.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Scraping years' worth of data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's now try to scrape all the Naver Conversation of the Day data from 2017 up to today.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating 2 empty DataFrames to hold conversations and conversation titles\ntitle_cols = [\n    'date',  # 'Conversation of the Day' date\n    'kor_title',  # 'Conversation of the Day' title in Korean\n    'eng_title',  # english translation of the title\n    'grammar',  # grammar of the day\n    'grammar_desc'  # grammar description\n]\nconvo_titles = pd.DataFrame(columns = title_cols)\n\nconvos_cols = [\n    'date',  # 'Conversation of the Day' date\n    'conversation_id',  # ordered numbering to indicate conversation flow\n    'kor_sent',  # korean sentence\n    'eng_sent',  # english translation\n    'qna_id'  # from sender or receiver, message or feedback\n]\nconvos = pd.DataFrame(columns = convos_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to strip html tags from text\ndef strip_tags(sent):\n    sent = re.sub(r'<.+?>', '', sent)\n    return sent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting conversations from December 4, 2017 to August 19, 2019","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstart_time = time.time()\n\nstart_date = '12/04/2017'\nend_date = '8/19/2020'\n\nfor d in pd.date_range(start=start_date, end=end_date):\n    \n    # Skip date if Sunday (Weekly Review Quiz)\n    if d.day_name() == 'Sunday':\n        continue\n    \n    date = d.strftime('%Y%m%d')\n    \n    # Navigating to website\n    url = f\"https://learn.dict.naver.com/conversation#/korean-en/{date}\"\n    browser.get(url)\n    # print(browser.current_url)\n    \n    # Giving the page up to 10 seconds to load\n    wait = selenium_WebDriverWait(browser, 10)\n    wait.until(selenium_ec.visibility_of_element_located((selenium_By.XPATH, '//div[@class=\"reading_lst_wrap\"]')))\n    \n    # Waiting for another 10 seconds before retrieving page source\n    time.sleep(10)\n    \n    # Retrieving page source\n    page = browser.page_source\n    \n    # Extracting data from page\n    # regex to extract conversation title in korean\n    kor_title = re.findall(r'id=\"ogTitle\" content=\"오늘의 회화 - (.+)\">', page)\n    # regex to extract conversation title in english\n    eng_title = re.findall(r'<span class=\"txt_trans ng-binding\" ng-bind=\"title_translation\">(.+)</span>', page)\n    # regex to extract sentence pairs\n    eng_sents = re.findall(r'<div.+item.trsl_sentence\">(.+)</div>', page)\n    kor_sents = re.findall(r'<span class=\"u_word_dic\" data-hook=\"tip\" data-type=\"arken\" data-lang=\"ko\">(.+)</span></span>', page)\n    \n    # Stripping HTML tags from kor_sents\n    kor_sents = list(map(strip_tags, kor_sents))\n    kor_sents = kor_sents[0:len(eng_sents)]\n    \n    # Extracting grammar of the day\n    grammar = re.findall(r'<span jxb-bind-compiled-html.+item[.]entry_name.+\"ng-scope\">(.+)</span></span>\\s+</div>', page)\n    grammar = list(map(strip_tags, grammar))\n    \n    # Extracting grammar description\n    grammar_desc = re.findall(r'<span class=\"txt_trans ng-binding\" ng-bind=\"item.mean\">(.+)</span>\\s+</div>', page)\n    grammar_desc = list(map(strip_tags, grammar_desc))\n    \n    # Extracting grammar of the day sentence examples\n    grammar_sents_eng = re.findall(r'<span class=\"txt_trans ng-binding\" ng-bind-html=\"desc[.]trans.+toHtml\">(.+)</span>', page)\n    grammar_sents_kor = re.findall(r'<span class=\"txt_origin ng-isolate-scope\" jxb-bind-compiled-html=\"toAutolinkText\\(desc[.]origin\\)\"><span class=\"ng-scope\"><span class=\"u_word_dic\" data-hook=\"tip\" data-type=\"arken\" data-lang=\"ko\">(.+)</span></span>', page)\n    grammar_sents_kor = list(map(strip_tags, grammar_sents_kor))\n    \n    # Creating new DataFrame to append to convo_titles\n    title_data = {\n        'date': date,\n        'kor_title': kor_title,\n        'eng_title': eng_title,\n        'grammar': ['. '.join(grammar)],\n        'grammar_desc': ['. '.join(grammar_desc) if len(grammar_desc) > 0 else '']\n    }\n    title = pd.DataFrame(title_data)\n    \n#     # Additional columns of title DataFrame\n#     for i in range(len(grammar_sents_eng)):\n#         col = f'grammar_kor_sent_{i+1}'\n#         title[col] = grammar_sents_kor[i]\n#         col = f'grammar_eng_sent_{i+1}'\n#         title[col] = grammar_sents_eng[i]\n    \n    # Creating new DataFrame to append to convos\n    convo_data = {\n        'date': [date for date in [date] for _ in range(len(eng_sents))],\n        'conversation_id': [id+1 for id, _ in enumerate(eng_sents)],\n        'kor_sent': kor_sents,\n        'eng_sent': eng_sents,\n        'qna_id': ''\n    }\n    convo = pd.DataFrame(convo_data)\n    \n    # Appending extracted data to convo_titles and convos DataFrames\n    convo_titles = convo_titles.append(title, ignore_index = True)\n    convos = convos.append(convo, ignore_index = True)\n    \n# Printing shapes\nprint('convos shape:', convos.shape)\nprint('convo_titles shape:', convo_titles.shape)\nprint('Time taken to extract data:', '{:.2f}'.format((time.time() - start_time) / 60))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the content of our DataFrames.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"convos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"convo_titles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good~","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Save to file: conversations.csv, conversation_titles.csv**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exporting to CSV files\nconvos.to_csv('conversations.csv', index = False)\nconvo_titles.to_csv('conversation_titles.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting unwanted files in working directory\n!rm -rf firefox\n!rm firefox-79.0.tar.bz2\n!rm geckodriver.log\n!ls ../working","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What's next?**\n* Automation and scripting: Extracting more data; On-demand data extraction via script\n* Dataset creation: Making the dataset user-friendly\n* Applications: I could make tons of flashcards, for now","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# References\n* [K-MOOC: Python Web Scraping](http://blog.naver.com/PostView.nhn?blogId=powhy123&logNo=221193422772&categoryNo=19&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView)\n* [Kaggle web scraping via headless Firefox+selenium](https://www.kaggle.com/dierickx3/kaggle-web-scraping-via-headless-firefox-selenium)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}