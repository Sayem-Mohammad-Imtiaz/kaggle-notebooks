{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Build a model to predict the value of the question in the TV game show  “Jeopardy!”.**\n"},{"metadata":{},"cell_type":"markdown","source":"Data description \n* > 'category' : the question category, e.g. \"HISTORY\" \n* > ‘value' : $ value of the question as string, e.g. \"$200\" (Note - \n* \"None\" for Final Jeopardy! and Tiebreaker questions) \n* > 'question' : text of question (Note: This sometimes contains  \n* hyperlinks and other things messy text such as when there's a  \n* picture or video question) \n* > 'answer' : text of answer \n* > round' : one of \"Jeopardy!\",\"Double Jeopardy!\",\"Final Jeopardy!\"  or \"Tiebreaker\" (Note: Tiebreaker questions do happen but  \n* they're very rare (like once every 20 years)) \n* > 'show_number' : string of show number, e.g '4680' \n* > 'air_date' : the show air date in format YYYY-MM-DD \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing necessary libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\n\nfrom stop_words import get_stop_words\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading Csv**"},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df = pd.read_csv(file_path)\nprint(f\"Shape of jeo_df is :- {jeo_df.shape}\")\njeo_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Name of the columns\nprint(f\"Column names:- {jeo_df.columns}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df.isna().count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming the columns names since it contains spaces for few col names\njeo_df.columns = [\"Show_Number\", \"Air_Date\", \"Round\", \"Category\", \"Value\", \"Question\", \"Answer\"] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n# Function to draw bar plot\ndef get_catogorical_features_plot(feat_name, plot=True):\n    if plot:\n        jeo_df.groupby(feat_name).size().plot(kind = 'bar')\n        rows = jeo_df.shape[0]\n        res = jeo_df[feat_name].value_counts()/rows \n        print(res)\n        print()\n    else:\n        res = jeo_df[feat_name].value_counts().shape[0]\n        print(res)\n        print()\n    \n  \nget_catogorical_features_plot(\"Round\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When Round==\"Final Jeopardy! or Round==\"Tiebreaker\" replace that with None\n# and removing from the csv since there were very few number of rows containing this data 0.016% and  0.000014%\njeo_df['Value'] = np.where((jeo_df['Round'] == \"Final Jeopardy!\")\n                           | (jeo_df['Round'] == \"Tiebreaker\"), \n                           \"None\",      \n                           jeo_df['Value'])      \njeo_df = jeo_df[jeo_df['Value'] != \"None\"]\nprint(f\"Shape of jeo_df is :- {jeo_df.shape}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total unique show number\nget_catogorical_features_plot(\"Show_Number\", plot=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Unique Category\nget_catogorical_features_plot(\"Category\", plot=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Unique value\nget_catogorical_features_plot(\"Value\", plot=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if any column contains null value or not\njeo_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df['Answer'].value_counts().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fill the answer with maximun number of repeating answer. Since only 2 of them is missing\njeo_df['Answer'] = jeo_df['Answer'].fillna(jeo_df['Answer'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df[\"Question\"].to_list()[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df[\"Answer\"].to_list()[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As seen from above question answer, we can conclude that Answer to each question is a Noun"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where((jeo_df['Round'] == \"Jeopardy!\")\n                           & (jeo_df['Show_Number'] == 4680))[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modifying Value column as numeric and reducing the number of classes to predict\njeo_df['Modified_Value'] = jeo_df['Value'].apply(\n    lambda value: int(value.replace(',', '').replace('$', '').replace(\" \", \"\"))\n)\ndef binning(value):\n    if value < 1000:\n        return np.round(value, -2)\n    elif value < 10000:\n        return np.round(value, -3)\n    else:\n        return np.round(value, -4)\n\njeo_df['Modified_Bins'] = jeo_df['Modified_Value'].apply(binning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df['Modified_Bins'].value_counts().to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping 20000 bins since it has total count less than 3**"},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df = jeo_df[jeo_df['Modified_Bins'] != 20000] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = jeo_df[\"Modified_Bins\"] \njeo_df.drop(['Modified_Value', \"Value\", \"Modified_Bins\"], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stop_words = list(get_stop_words('en'))         \n# nltk_words = list(stopwords.words('english'))\n# stop_words.extend(nltk_words)\n# stop_words = set(stop_words)\n# len(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort dataframe based on time pandas python: https://stackoverflow.com/a/49702492/4084039\njeo_df[\"Date\"] = pd.to_datetime(jeo_df['Air_Date'])\njeo_df.drop('Air_Date', axis=1, inplace=True)\njeo_df.sort_values(by=['Date'], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Text Preprocessing****"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_question = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(jeo_df['Question'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https://gist.github.com/sebleier/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_question.append(sent.lower().strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jeo_df['Preprocessed_Question'] = preprocessed_question    #create new column having name  with preprocessed data\njeo_df.drop(['Question', 'Date'], axis=1, inplace=True) #delete the column\njeo_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering\n\ndef count(line):\n    num_text=[]\n    for words in line:\n        splitted = words.split()\n        length = len(splitted)\n        num_text.append(length)\n    return num_text  \n\njeo_df['Count_Question'] = count(jeo_df['Preprocessed_Question'])    #create new column having name count_Question with preprocessed data\njeo_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Model Training starts"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_test_split\nfrom sklearn.model_selection import train_test_split\nproject_data_train, project_data_test, project_data_y_train, project_data_y_test = train_test_split(jeo_df, target, test_size=0.33, stratify=target)\nproject_data_train, project_data_cv, project_data_y_train, project_data_y_cv = train_test_split(project_data_train, project_data_y_train, test_size=0.33, stratify=project_data_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Question\n#https://stackoverflow.com/questions/48090658/sklearn-how-to-incorporate-missing-data-when-one-hot-encoding\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer3 = CountVectorizer(lowercase=False, binary=True, max_features=2000)\nvectorizer3.fit(project_data_train['Preprocessed_Question'].values)\n#print(vectorizer3.get_feature_names())\n\nfeat_1_train = vectorizer3.transform(project_data_train['Preprocessed_Question'].values)\nfeat_1_cv = vectorizer3.transform(project_data_cv['Preprocessed_Question'].values)\nfeat_1_test = vectorizer3.transform(project_data_test['Preprocessed_Question'].values)\n\nprint(\"After vectorizations\")\nprint(feat_1_train.shape, project_data_y_train.shape)\nprint(feat_1_cv.shape, project_data_y_cv.shape)\nprint(feat_1_test.shape, project_data_y_test.shape)\nprint(\"=\"*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Round\n#https://stackoverflow.com/questions/48090658/sklearn-how-to-incorporate-missing-data-when-one-hot-encoding\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer3 = CountVectorizer(lowercase=False, binary=True, max_features=2000)\nvectorizer3.fit(project_data_train[\"Round\"].values)\n#print(vectorizer3.get_feature_names())\n\nfeat_2_train = vectorizer3.transform(project_data_train['Round'].values)\nfeat_2_cv = vectorizer3.transform(project_data_cv['Round'].values)\nfeat_2_test = vectorizer3.transform(project_data_test['Round'].values)\n\nprint(\"After vectorizations\")\nprint(feat_2_train.shape, project_data_y_train.shape)\nprint(feat_2_cv.shape, project_data_y_cv.shape)\nprint(feat_2_test.shape, project_data_y_test.shape)\nprint(\"=\"*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Category\n#https://stackoverflow.com/questions/48090658/sklearn-how-to-incorporate-missing-data-when-one-hot-encoding\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer3 = CountVectorizer(lowercase=False, binary=True, max_features=2000)\nvectorizer3.fit(project_data_train[\"Category\"].values)\n#print(vectorizer3.get_feature_names())\n\nfeat_3_train = vectorizer3.transform(project_data_train['Category'].values)\nfeat_3_cv = vectorizer3.transform(project_data_cv['Category'].values)\nfeat_3_test = vectorizer3.transform(project_data_test['Category'].values)\n\nprint(\"After vectorizations\")\nprint(feat_3_train.shape, project_data_y_train.shape)\nprint(feat_3_cv.shape, project_data_y_cv.shape)\nprint(feat_3_test.shape, project_data_y_test.shape)\nprint(\"=\"*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Answer\n#https://stackoverflow.com/questions/48090658/sklearn-how-to-incorporate-missing-data-when-one-hot-encoding\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer3 = CountVectorizer(lowercase=False, binary=True, max_features=2000)\nvectorizer3.fit(project_data_train[\"Answer\"].values)\n#print(vectorizer3.get_feature_names())\n\nfeat_4_train = vectorizer3.transform(project_data_train['Answer'].values)\nfeat_4_cv = vectorizer3.transform(project_data_cv['Answer'].values)\nfeat_4_test = vectorizer3.transform(project_data_test['Answer'].values)\n\nprint(\"After vectorizations\")\nprint(feat_4_train.shape, project_data_y_train.shape)\nprint(feat_4_cv.shape, project_data_y_cv.shape)\nprint(feat_4_test.shape, project_data_y_test.shape)\nprint(\"=\"*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check this one: https://www.youtube.com/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# quantity_standardized = standardScalar.fit(project_data['quantity'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nnum_title_scalar = StandardScaler()\nnum_title_scalar.fit(project_data_train['Count_Question'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n#print(f\"Mean : {num_title_scalar.mean_[0]}, Standard deviation : {np.sqrt(num_title_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nfeat_5_train = num_title_scalar.transform(project_data_train['Count_Question'].values.reshape(-1, 1))\nfeat_5_cv = num_title_scalar.transform(project_data_cv['Count_Question'].values.reshape(-1, 1))\nfeat_5_test = num_title_scalar.transform(project_data_test['Count_Question'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check this one: https://www.youtube.com/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# quantity_standardized = standardScalar.fit(project_data['quantity'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nnum_title_scalar = StandardScaler()\nnum_title_scalar.fit(project_data_train['Show_Number'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n#print(f\"Mean : {num_title_scalar.mean_[0]}, Standard deviation : {np.sqrt(num_title_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nfeat_6_train = num_title_scalar.transform(project_data_train['Show_Number'].values.reshape(-1, 1))\nfeat_6_cv = num_title_scalar.transform(project_data_cv['Show_Number'].values.reshape(-1, 1))\nfeat_6_test = num_title_scalar.transform(project_data_test['Show_Number'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\nfrom scipy.sparse import hstack\n\n# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\nlr_train_1=hstack((feat_1_train, feat_2_train, feat_3_train, feat_4_train, feat_5_train, feat_6_train)).tocsr()\nlr_cv_1=hstack((feat_1_cv, feat_2_cv, feat_3_cv, feat_4_cv, feat_5_cv, feat_6_cv)).tocsr()\nlr_test_1=hstack((feat_1_test, feat_2_test, feat_3_test, feat_4_test, feat_5_test, feat_6_test)).tocsr()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Different models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random as r\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-3, 3)]\ncv_log_error_array=[]\nfor i in alpha:\n    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced')\n    logisticR.fit(lr_train_1,project_data_y_train)\n    sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n    sig_clf.fit(lr_train_1, project_data_y_train)\n    predict_y = sig_clf.predict_proba(lr_cv_1)\n    cv_log_error_array.append(log_loss(project_data_y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\n    print(f\"Done alpha== {i}\")\n    \nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nbest_alpha = np.argmin(cv_log_error_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nlogisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],class_weight='balanced')\nlogisticR.fit(lr_train_1,project_data_y_train)\nsig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\nsig_clf.fit(lr_train_1, project_data_y_train)\npred_y=sig_clf.predict(lr_test_1)\n\npredict_y = sig_clf.predict_proba(lr_train_1)\nprint ('log loss for train data',log_loss(project_data_y_train, predict_y, labels=logisticR.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(lr_cv_1)\nprint ('log loss for cv data',log_loss(project_data_y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(lr_test_1)\nprint ('log loss for test data',log_loss(project_data_y_test, predict_y, labels=logisticR.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha=[5,10,50]\ncv_log_error_array=[]\nfor i in alpha:\n    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)\n    x_cfl.fit(lr_train_1,project_data_y_train)\n    sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n    sig_clf.fit(lr_train_1, project_data_y_train)\n    predict_y = sig_clf.predict_proba(lr_cv_1)\n    cv_log_error_array.append(log_loss(project_data_y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\n    print(f\"Done alpha== {i}\")\n    \nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nbest_alpha = np.argmin(cv_log_error_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nx_cfl=XGBClassifier(n_estimators=i,nthread=-1)\nx_cfl.fit(lr_train_1,project_data_y_train)\nsig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\nsig_clf.fit(lr_train_1, project_data_y_train)\npred_y=sig_clf.predict(lr_test_1)\n\npredict_y = sig_clf.predict_proba(lr_train_1)\nprint ('log loss for train data',log_loss(project_data_y_train, predict_y, labels=logisticR.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(lr_cv_1)\nprint ('log loss for cv data',log_loss(project_data_y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(lr_test_1)\nprint ('log loss for test data',log_loss(project_data_y_test, predict_y, labels=logisticR.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n1. We can also use different vectorizer other than count vectorizer. Ex:- (Tfidf, w2v etc)\n2. Training time of w2v will be much greater than count and tfidf vectorizer.\n3. We can also try RandomForect, knn, NeuralNetwork.\n4. Loss is logloss for multiclass classification.\n5. We can also tweak number of alpha's, to check robustness of above model.\n6. Divided train, cv and test and then vectorize it to avoid data leakage(Increase model robustness)"},{"metadata":{},"cell_type":"markdown","source":"**If you like this work then give a upvote**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}