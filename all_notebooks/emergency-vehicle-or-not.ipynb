{"cells":[{"metadata":{},"cell_type":"markdown","source":"## PROBLEM STATEMENT\n\n**Emergency vs Non-Emergency Vehicle Classification**\n\nFatalities due to traffic delays of emergency vehicles such as ambulance & fire brigade is a huge problem. In daily life, we often see that emergency vehicles face difficulty in passing through traffic. So differentiating a vehicle into an emergency and non emergency category can be an important component in traffic monitoring as well as self drive car systems as reaching on time to their destination is critical for these services.\n\n\n## Data Description\n- '0' class refers to non-emergency vehicles\n- '1' class refers to emergency vehicles .Eg Police car,ambulance ,etc\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import style\nimport seaborn as sns\nimport os\n\nimport sklearn\nimport os \nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n  \nimport os,cv2\nfrom IPython.display import Image\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D , BatchNormalization\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.initializers import glorot_normal,glorot_uniform, he_normal, he_uniform\nfrom keras.optimizers import Adamax,Adam, Adadelta, Adagrad, RMSprop, Nadam\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.regularizers import l2,l1\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications import NASNetLarge\nfrom keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = pd.read_csv(\"../input/jantahackcomputervision/test_vc2kHdQ.csv\")\nsubmission = pd.read_csv(\"../input/jantahackcomputervision/ss.csv\")\ntrain_labels = pd.read_csv(\"../input/jantahackcomputervision/train_SOaYf6m/train.csv\")\n\n\ntrain_labels[\"emergency_or_not\"] = train_labels[\"emergency_or_not\"].astype(str)\nsubmission[\"emergency_or_not\"] = submission[\"emergency_or_not\"].astype(str)\n\nrandom = [ '0' if i%4==0 else '1' for i in range(706)]\nsubmission[\"emergency_or_not\"]  = random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at class distribution\ntrain_labels[\"emergency_or_not\"].value_counts().plot(kind=\"bar\");\n\nprint(train_labels[\"emergency_or_not\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Both the classes i.e 0 and 1 are in comparable amount i.e. it is a balanced dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Visualizing some images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\ndef view_imgs(df,rows,cols):\n    IMAGE_DIR =\"../input/jantahackcomputervision/train_SOaYf6m/images/\"\n    axes=[]\n    fig=plt.figure(figsize=(20,12))    \n    for i in range(rows*cols):\n        idx = np.random.randint(len(df), size=1)[0]\n        image_name , label = df.loc[idx,\"image_names\"],df.loc[idx,\"emergency_or_not\"]\n        image = Image.open(IMAGE_DIR+image_name)\n        label = \"emergency\" if label=='1' else \"non-emergency\"\n        axes.append( fig.add_subplot(rows, cols, i+1))\n        subplot_title=(\"Category :\"+str(label))\n        axes[-1].set_title(subplot_title)  \n        plt.imshow(image)\n    fig.tight_layout()  \n    plt.show()\n        \n        \nview_imgs(train_labels,2,4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Approach for solving the problem\n- Because of very little training data i.e 1646 in total it's very difficult to train a model with high performance and accuracy Hence we used transfer learning.\n- **Transfer Learning** (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n\n\n\n<img src=\"https://miro.medium.com/proxy/1*1CxVzTNILTHgDs5yJO4W9A.png\" width=\"600\" height=\"400\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MODEL 1 : VGG16\n- **Using Vgg16 pre-trained network and then fine tuning the top layers of pre-trained network**\n- Below image shows the architecture of VGG16\n\n<img src=\"https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg\">\n\n\n- Pretrained network means we will use the weights which are provided by-default by imagenet .Since it was trained on Imagenet which has lots of classes using default weights will speed up the training process.Because the earlier layers of any CNN model recognizes basic features like edges,curves,etc then further layers combines them to make object like shape such as box,rectangle,circle,etc . Only the deeper layers are more specific to what we are building.\n\n- Since Deeper layers are specific to our problem we will fine-tune them that means we will freeze the earlier layers and only train the last Conv layer of vgg and the FC layer with very small weight updates.\n\nFor Fine Tuning I used the below keras blog as reference it explains fine tuning in depth:<br>\nhttps://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading Images \n\n- We use ImageDataGenerator class to load images and apply augmentations.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffling the dataframe\ntrain_labels = train_labels.sample(frac=1).reset_index()\n\ndatagen = ImageDataGenerator(rescale=1./255)\nbatch_size=64\nimg_dir = \"../input/jantahackcomputervision/train_SOaYf6m/images\"\n\ntrain_generator=datagen.flow_from_dataframe(\n    dataframe=train_labels[:1318],\n    directory=img_dir,\n    x_col='image_names',\n    class_mode=None,\n    batch_size=batch_size,\n    target_size=(224,224),\n    shuffle=False\n)\n\n# class_mode is none since we only need the bottle_neck features \n\nvalidation_generator = datagen.flow_from_dataframe(\n    dataframe=train_labels[1318:],\n    directory=img_dir,x_col='image_names',\n    class_mode=None,\n    batch_size=32,\n    target_size=(224,224),\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bottleneck Predictions","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"vgg16 = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\n\nfor layers in vgg16.layers:\n    layers.trainable = False\n    \n\n# Looking at the model architecture\nvgg16.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"bottleneck_features_train = vgg16.predict(train_generator,verbose=1)\nbottleneck_features_validation = vgg16.predict(validation_generator,verbose=1)\nprint(bottleneck_features_train.shape,bottleneck_features_validation.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Fully Connected layer i.e Top Classifier Architecture and training it for stable weights \n- The reason for not directly fine tuning is that if we try to fine tune the Top Classifier with randomly initialized weights our pretrained weights will get destroyed and it will lead to the problem of Exploding Gradients. Hence it's better to first train the top-classifier for stable weights then fine tune with small learning rate and SGD or RMSProp so that the magnitude of updates remain small and our peviously learned weights do not get wrecked.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = bottleneck_features_train.copy()\ntrain_class = train_labels.loc[:1317,\"emergency_or_not\"]\n\nvalidation_data = bottleneck_features_validation.copy()\nvalidation_class = train_labels.loc[1318:,\"emergency_or_not\"]\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=train_data.shape[1:]))\nmodel.add(Dense(2048,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using checkpoints to save the best model.\n\nfrom keras.callbacks import ModelCheckpoint\nfilename = \"./bottle_neck_best_wts.hdf5\"\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\",save_weights_only=True)\n\nmodel.fit(\n    train_data, \n    train_class,\n    epochs=2,\n    batch_size=32,\n    validation_data=(validation_data, validation_class),\n    callbacks=[checks]\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fine Tuning Last conv block of VGG with stable weights of TopClassifiers\n- Final Model = VGG(fine tune) + TopClassifier(stable wts)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# After instantiating the VGG base and loading its weights, we add our previously trained fully-connected classifier on top of it\n\nvgg = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\nfor layer in vgg.layers[:-4]:\n    layer.trainable = False\n\nfinal_model = Sequential()\nfinal_model.add(vgg)\n\n# Trained Classifier\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=final_model.output_shape[1:]))\ntop_model.add(Dense(2048,activation='relu'))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(1,activation=\"sigmoid\"))\ntop_model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note that it is necessary to start with a fully-trained classifier, including the top classifier in order to do fine-tuning\n\ntop_model_weights_path = \"../input/cv-av-models/bottle_neck_best_wts.hdf5\"\ntop_model.load_weights(top_model_weights_path)\n\n# add the model on top of the convolutional base\nfinal_model.add(top_model)\nfinal_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compiling the model using SGD with momentum and a very slow learning rate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.compile(loss='binary_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now for final training we used data augmentation techniques\n\n**Data Augmentation** :- In normal terms we take a single image and apply some transformations/edit them to create new images this will help us to create more data and to significantly increase the diversity of data available for training models, without actually collecting new data.\n\n### **Augmentations Used**\n- Horizontal Flipping\n- Zooming\n- Shear","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Image Data Generator with augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare data augmentation configuration\n\ndef return_data_generators(img_width,img_height,train_labels,batch_size=16,class_mode=\"binary\"):\n    \"\"\"\n    img_width,img_height : - image dimension to be resized to this sizes during data loading\n    \n    returns ImageDatagenerators for training and testing purpose\n    \n    \"\"\"\n    \n    \n    train_labels = train_labels.sample(frac=1).reset_index()\n\n    train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True)\n\n    test_datagen = ImageDataGenerator(rescale=1./255)\n\n\n    img_dir = \"../input/jantahackcomputervision/train_SOaYf6m/images\"\n\n\n    train_generator = train_datagen.flow_from_dataframe(dataframe=train_labels[:1318],directory=img_dir,x_col='image_names',\n                                                y_col='emergency_or_not',class_mode=class_mode,batch_size=batch_size,\n                                                target_size=(img_width,img_height),shuffle=True)\n\n    validation_generator = test_datagen.flow_from_dataframe(dataframe=train_labels[1318:],directory=img_dir,x_col='image_names',\n                                                    y_col='emergency_or_not',class_mode=class_mode,batch_size=batch_size,\n                                                    target_size=(img_width,img_height),shuffle=True)\n    \n    test_generator = test_datagen.flow_from_dataframe(dataframe=test_labels,directory=img_dir,x_col='image_names',\n                                                class_mode=None,batch_size=batch_size,\n                                                target_size=(img_width,img_height),shuffle=False)\n    \n    return train_generator,validation_generator,test_generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"batch_size = 32\nepochs = 30\n\ntrain_generator,validation_generator,test_generator = return_data_generators(224,224,train_labels,batch_size)\n\n# Checkpoints\nfilename = \"./fine_tuned.hdf5\"\n\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\")\n\n# fine-tune the model\nhistory_finetune = final_model.fit_generator(train_generator,epochs=epochs,\n                              validation_data=validation_generator,\n                             callbacks=[checks],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TRY THIS FUNCTION ONCE\n\ndef plot_loss_acc(history,epochs,filename):\n    fig = plt.figure(figsize=(14,9))\n\n    ax1 = plt.subplot2grid((2,1), (0,0))\n    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n    \n    epoch = list(range(1,epochs+1,1))\n    losses = history.history[\"loss\"]\n    val_losses = history.history[\"val_loss\"]\n\n    accuracies = history.history[\"accuracy\"]\n    val_accs = history.history[\"val_accuracy\"]\n\n    \n    ax1.plot(epoch, accuracies, label=\"acc\")\n    ax1.plot(epoch, val_accs, label=\"val_accuracy\")\n    ax1.legend(loc=2)\n    ax1.set_ylabel(\"Accuracy\")\n    \n    ax2.plot(epoch,losses, label=\"loss\")\n    ax2.plot(epoch,val_losses, label=\"val_loss\")\n    ax2.legend(loc=2)\n    ax2.set_xlabel(\"No of epochs\")\n    ax2.set_ylabel(\"Loss\")\n\n    \n    \n    plt.savefig(\"./\"+filename+\".png\", dpi=300, bbox_inches='tight')\n    plt.show()\n\n    \nplot_loss_acc(history_finetune,30,\"keras_finetune_vgg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## INFERENCE FROM GRAPH\n- The oscillations are due to Stochastic Gradient Descent as the weights are updated after each example\n- The model starts overfitting after apprxoimately 15 epochs.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## We tried L2 rgularization\n- As there is a large gap between validation and train accuracy in attempt to reduce overfitting to  the model for better accuracy and predictions on unseen data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MODEL 2 : VGG16 with Dropout and Regularization","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# VGG16 with l2 regularization\n\n# Instantiating the VGG base and loading its weights \n\nvgg = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\nfor layer in vgg.layers:\n    layer.trainable = False\n\n    \nmodel_l2 = Sequential()\nmodel_l2.add(vgg)\n\n## Trained Classifier\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=model_l2.output_shape[1:]))\ntop_model.add(Dense(2048,activation='relu',kernel_initializer=glorot_uniform(), kernel_regularizer=l2()))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(2,activation=\"softmax\",kernel_initializer=glorot_uniform(), kernel_regularizer=l2()))\n\n# top_model_weights_path = \"../input/cv-av-models/bottle_neck_best_wts.hdf5\"\n# top_model.load_weights(top_model_weights_path)\n\n# add the model on top of the convolutional base\nmodel_l2.add(top_model)\nmodel_l2.summary()\n\nmodel_l2.compile(loss='categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])\n\n# Checkpoints\nfilename = \"./fine_tuned_l2_augnmentation.hdf5\"\n\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\")\n\ntrain_generator,validation_generator,test_generator = return_data_generators(224,224,train_labels,32,\"categorical\")\n\n# fine-tune the model\nhistory_l2 = model_l2.fit_generator(train_generator,epochs=30,validation_data=validation_generator,verbose=1,callbacks=[checks])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss_acc(history_l2,30,\"vgg_l2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Trying an advanced model i.e NASnet Large\n\n- \" The key contribution of this work is the design of a new search space (which we call the “NASNet search space”) which enables transferability. In our experiments, we search for the best convolutional layer (or “cell”) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a “NASNet architecture”. \n\n* The NASNet architechure introduces the design of a new search space which the researchers called the **NASNet search space** which enables transferability. \n\n* We picked this model because it searches it's own architecture based on problem and data. Our data set is small and also the problem is difficult in itself, we have to classify objects of same type i.e both are vehicles only either emergency or non-emergency so we tried experimenting with this model which resulted in good predictions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MODEL 3 : NASNet Large without Dropout and regularization\n## MODEL 4 : NASNet Large with Dropout and regularization\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nasnet = NASNetLarge(weights = \"imagenet\", input_shape = (331, 331, 3), include_top = False)\nfor layer in nasnet.layers:\n    layer.trainable = False\n\nnasnet_model = Sequential()\n \n\n## Adding a covolutional base with l2 regularzer and FC layer on top of it\nnasnet_model.add(nasnet)\n\nnasnet_model.add(Conv2D(1024, (3, 3), activation = \"relu\"))\nnasnet_model.add(BatchNormalization())\nnasnet_model.add(MaxPooling2D(2, 2))\n \nnasnet_model.add(Flatten())\n  \nnasnet_model.add(Dense(2048, activation = \"relu\",kernel_regularizer=l2()))\nnasnet_model.add(BatchNormalization())\nnasnet_model.add(Dropout(0.5))\n \nnasnet_model.add(Dense(256, activation = \"relu\",kernel_regularizer=l2()))\nnasnet_model.add(BatchNormalization()) \nnasnet_model.add(Dropout(0.5))\n \nnasnet_model.add(Dense(1, activation = \"sigmoid\"))\n \nnasnet_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Compiling and training the model\n\n# Checkpoints\nfilename = \"./nasnet_l2.hdf5\"\n\ntrain_generator,validation_generator,test_generator = return_data_generators(331,331,train_labels,32)\n\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\")\n\nepochs = 20\nnasnet_model.compile(loss = 'binary_crossentropy',\n              optimizer = \"adam\",\n              metrics = ['accuracy']\n              )\n\nhistory_nasnet = nasnet_model.fit_generator(train_generator,epochs=epochs,\n                              validation_data=validation_generator,\n                             callbacks=[checks],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss_acc(history_nasnet,20,\"nasnet_l2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## MODEL 5 :\n- A normal CNN architecture made by us which was trained for stratified 5 fold cross validation\n- Then we took the mode of this 5 predictions .\n- It was just a test and this model gave the accuracy of 0.86 on the public leaderboard","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation for stratified split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = train_labels.copy()\ntest_dataset = test_labels.copy()\n\ntrain_image_name = list(train_dataset[\"image_names\"])\ntest_image_name = list(test_dataset[\"image_names\"])\n\nprint(len(train_image_name),len(test_image_name))\n\ntrain_images = list()\ntest_images = list()\ntrain_image_res = list()\n\ndirname = \"../input/jantahackcomputervision/train_SOaYf6m/images\"\n\nfor i,filename in enumerate(train_image_name):\n    img = cv2.imread(os.path.join(dirname,filename))\n    y_val = int(train_dataset[train_dataset.image_names == filename][\"emergency_or_not\"])\n    train_image_res.append(y_val)\n    train_images.append(img)\n    #print(i+1)\n\n\nfor i,filename in enumerate(test_image_name):\n    img = cv2.imread(os.path.join(dirname,filename))\n    test_images.append(img)\n    #print(i+1)\n    \n\ntrain_images = np.array(train_images)\ntest_images = np.array(test_images)\ntrain_image_res = np.array(train_image_res)\n    \nprint(train_images.shape,test_images.shape,train_image_res.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating fold_model\n\nfold_model = Sequential()\nfold_model.add(Conv2D(32,(3,3),input_shape=(224, 224,3)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Conv2D(32,(3,3)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Conv2D(128,(3,3)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Conv2D(512,(5,5)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\nfold_model.add(Dense(512))\nfold_model.add(Activation('relu'))\nfold_model.add(Dropout(0.5))\nfold_model.add(Dense(2))\nfold_model.add(Activation('softmax'))\n\nfold_model.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n\nfilename = \"./kfold.hdf5\"\ncheckpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stopping_monitor = EarlyStopping(monitor='val_accuracy',patience=7)\ncallbacks_list = [early_stopping_monitor, checkpoint]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Training and predictions\nK = 5\n\nskf = StratifiedKFold(n_splits=K, shuffle=True, random_state = 7)\nnew_submission = pd.DataFrame()\n\nX = train_images\nY = train_image_res\nX_test = test_images\n\nj = 1\n\ny_valid = None \nX_valid = None\n\nfor (train_data_image, test_data_image) in skf.split(X, Y):\n    \n    y_train, y_valid = Y[train_data_image], Y[test_data_image]\n    X_train, X_valid = X[train_data_image], X[test_data_image]\n    \n    y_train = to_categorical(y_train, num_classes=2)\n    y_valid = to_categorical(y_valid, num_classes=2)\n    \n    fold_model_history = fold_model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n                        epochs=25,callbacks=[callbacks_list],verbose=1)\n    \n    new_submission[\"predict\" + str(j)] = np.argmax(fold_model.predict(X_test,verbose=1), axis = 1)\n    \n    j = j + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission[\"image_names\"] = test_image_name\nsubmission[\"emergency_or_not\"] = new_submission.mode(axis=1)\nsubmission.to_csv(\"./sub.csv\",index = False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}