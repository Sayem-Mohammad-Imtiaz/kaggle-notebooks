{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"version":"3.6.3","file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python"}},"cells":[{"source":"# Breast Cancer Data - Statistical Analysis ","metadata":{"_uuid":"39f1319907ff2c654fd625b1ac0b519d34f9ec6a","_cell_guid":"383905e0-cef6-4a46-b6b0-6040eb153fdf"},"cell_type":"markdown"},{"source":"-- Rohith R Pai","metadata":{"_uuid":"fc59350d47a3126789e52e5e48cc8050b4d97f3c","_cell_guid":"a1d7e210-9e11-4a41-8a63-3fe8be13226d"},"cell_type":"markdown"},{"source":"# 1. Introduction ","metadata":{"_uuid":"a1fab6bafe838aa6b242070db7ed3428e7f06fed","_cell_guid":"50ae2360-0d4f-4533-ae59-43e78c62db72"},"cell_type":"markdown"},{"source":"The objective of this notebook is to analyse the breastcancer data supplied by University of Wisconsin, Madison and find a suitable Machine learning algorithm to predict the diagnosis based on the pre recorded data set. All the atributes of this data-set is computed from digitizing the image of fine needle aspirate(FNA) of a breast mass and was donated to University of Wisconsin in the year 1995.\n    \n    Some of the important atributes in this data set are as follows:\n    1) ID number\n    2) Diagnosis (M = malignant, B = benign) 3-32)\n    3) radius (mean of distances from center to points on the perimeter) \n    4) texture (standard deviation of gray-scale values) \n    5) perimeter \n    6) area \n    7) smoothness (local variation in radius lengths) \n    8) compactness (perimeter^2 / area - 1.0) \n    9) concavity (severity of concave portions of the contour) \n    10) concave points (number of concave portions of the contour) \n    11) symmetry \n    12) fractal dimension (\"coastline approximation\" - 1)\n    \n    ","metadata":{"_uuid":"c79dea8bc9acb2e17dc0afc271f22794bc97b045","_cell_guid":"0f45207b-2a16-4e81-88c0-0544a057f268"},"cell_type":"raw"},{"source":"![](https://www.cancer.org/cancer/breast-cancer/about/what-is-breast-cancer/_jcr_content/par/image.img.gif/1499806644266.gif)","metadata":{"_uuid":"cc8e778b25ba0d6192359206a57110698af516d4","_cell_guid":"e41a2015-ee20-4938-baf5-18286c124e9d"},"cell_type":"markdown"},{"source":"**Breast Cancer: **\n    \nThe American Cancer Society defines Breast Cancer as the condition whtn the cells in the brast grows out of control.These cells usually form a tumor that can often be seen on an x-ray or felt as a lump. The tumor is malignant (cancer) if the cells can grow into (invade) surrounding tissues or spread (metastasize) to distant areas of the body. Breast cancer occurs almost entirely in women, but men can get breast cancer, too.\n\n**Breast Cancer Signs and Symptoms:**\n    \nThe most common symptom of breast cancer is a new lump or mass. A painless, hard mass that has irregular edges is more likely to be cancer, but breast cancers can be tender, soft, or rounded. They can even be painful. For this reason, it is important to have any new breast mass or lump or breast change checked by a health care provider experienced in diagnosing breast diseases.\n\nThe above mentioned data set has a number of diagnoisis of lumps and masses that were found in the patients. Based on the diagnosis the tumor or lump is either classified as malignant (denoted by letter 'M') or benign (denoted by letter 'B').\n    \n    ","metadata":{"_uuid":"e6eb2aefdb9af95a4be056b751f3a684dc37ae54","_cell_guid":"cf8b315b-1a7b-496f-a210-80dc2d7b5b18"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"# importing the modules for analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","metadata":{"_uuid":"4a70fd41c103135f6ef13c3909579a639b743fda","collapsed":true,"_cell_guid":"5b895ed4-36bb-4af3-9d0b-80de0606be82"},"cell_type":"code"},{"source":"The data set is imported using the pyton padas module.","metadata":{"_uuid":"460881c862bf40756313b1cb94aac052a26b9148","_cell_guid":"74fccf1f-d869-4ef3-8f19-3ab6fa0871ce"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"df = pd.read_csv('../input/data.csv')","metadata":{"_uuid":"dda1956c10102684ea90b043ad1af69efb06df36","collapsed":true,"_cell_guid":"e8d2f1c5-65a2-4988-b73c-7e93c82357f3"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"df.info()","metadata":{"_uuid":"ca2b0e3e0b2f7725254aaff0c103abb0f25ec5c1","_cell_guid":"a9f9d001-8db2-40bf-a571-8c5d70e52eed"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"df.head()","metadata":{"_uuid":"5cc5c2435ad34380699d4b33847575db194aa282","_cell_guid":"169b82d0-931f-40d5-95e6-8722568ef0d4"},"cell_type":"code"},{"source":"The data is a 2-d matrix of [569 X 32] data. Now we need to trim the data of all the unwanted columns before we proceed with the analysis \nIt can be observed from df.info(), that the column name \"Unnamed: 32\" has no values in it and, the \"id\" columns show no relavance to the analysis on hand. It's ideal to remove these columns before we start to visualize the data and decide upon the statistical classification algorithm.\n","metadata":{"_uuid":"1c907381a5ab6d4ea1f93714a3abd4e283c47b8b","_cell_guid":"405daa0e-4500-4be2-91cc-d65d84516687"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"cancer_df =df.drop(['id','Unnamed: 32'], axis = 1)","metadata":{"_uuid":"c1398c343f53be589cbf117b27f7dba27c2af639","collapsed":true,"_cell_guid":"4506bae3-6757-43a1-bd65-5625b230fac2"},"cell_type":"code"},{"source":"Before we start the process of visualization of the data, we need to convert all the diagnosis data into suitable numbers (0 & 1 in this case) to facilitate easy recognition of the data by the computer.\n\nIf the diagnosis is Malignant it is denoted by 1 and if it's benign it's denoted by 0.\n","metadata":{"_uuid":"55cb58eaf9ad4ec413187ae30c7a3e08958882c5","_cell_guid":"fa48d5c7-725f-4c89-8e6c-f8d296e3309d"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"cancer_df= pd.get_dummies(cancer_df,'diagnosis',drop_first=True) # dropping the column called diagnosis and having a columns of 0 and 1\ncancer_df.head() ","metadata":{"_uuid":"68f2456b7a022a143e947fba746f1b0d4ff3c72e","_cell_guid":"39cb3527-67b0-4456-a567-b7648b28b0b0"},"cell_type":"code"},{"source":"## 2. Data visualization ","metadata":{"_uuid":"46698c02957ed59d903ed8a8964e839d4b5c83a5","_cell_guid":"de4a682b-8a67-4cab-8227-029241fe2bab"},"cell_type":"markdown"},{"source":"A simple count plot would give a good indication of the split between the malignant and benign diagnosis based of the dataset. ","metadata":{"_uuid":"d8628905a221691cfcfd36333a2b3649dfb706d7","_cell_guid":"b6610c1f-98a5-4d00-8943-c9615f83c35b"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"sns.countplot(x='diagnosis',data = df,palette='BrBG')","metadata":{"_uuid":"f6c22ef1478fd2891e66ab211a97312a5be6ea0a","_cell_guid":"353c93b7-e7f4-44b6-a409-f2add8a6d587"},"cell_type":"code"},{"source":"It can be obsereved from the above plot that there were 350 cases in which the diagnosis of the lump/tumor was benign, and there were about 200 cases in which the prognosis was malignant","metadata":{"_uuid":"b27febd9d584ab005a8baabe4cd3700ed0c5450c","_cell_guid":"41458ffb-2c57-46e3-948e-7e16de22d6bf"},"cell_type":"markdown"},{"source":"Simple distribution plot of all the attributes of the data set","metadata":{"_uuid":"a9baf1cbaa93555cb692be33dbc35a373adde55e","_cell_guid":"f05bb68c-4f4e-4807-bc3b-371e20eb54ea"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"colors = np.array('b g r c m y k'.split()) #Different colors for plotting\n\nfig,axes = plt.subplots(nrows =15,ncols=2, sharey=True,figsize = (15,50))\nplt.tight_layout()\nrow = 0\niteration = 0\nfor j in range(0,len(cancer_df.columns[:-1])):\n    iteration+=1\n    if(j%2==0):\n        k = 0\n    else:\n        k = 1\n    sns.distplot(cancer_df[cancer_df.columns[j]],kde=False,hist_kws=dict(edgecolor=\"w\", linewidth=2),color = np.random.choice(colors) ,ax=axes[row][k])\n    if(iteration%2==0):\n        row+=1\n        plt.ylim(0,200)","metadata":{"_uuid":"c65cb2faa74c91c74fd8c430519bc7d7740c5b2f","_cell_guid":"3036401e-def7-4a59-b291-7ef48f42ed0c"},"cell_type":"code"},{"source":"It can be observed that all the attributes of this data set follows more or less a normalized distribution. Let's try out a few relationship plots between different attributes. The standard deviation of each of the attributes is as shown below.\n","metadata":{"_uuid":"1582b9f915914be361408dc49fe08b637cc5a7e9","collapsed":true,"_cell_guid":"41ebb576-48c8-4d80-a284-6ea4ef8c53df"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"cancer_df.std()","metadata":{"_uuid":"daad4c90824380a40b5f311fb21876fef7484c16","_cell_guid":"dd6a2d89-ad60-465f-be42-73c42b89999c"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"plt.figure(figsize =(20,6))\nsns.barplot(x='radius_mean',y='texture_mean',data =df, hue= 'diagnosis',palette='viridis')\nplt.xlabel('Mean Radius of the lump')\nplt.ylabel('Texture of the lump')","metadata":{"_uuid":"72411ad6ae8be47d32fd6205e572a210897b6d26","_cell_guid":"f0bacdb4-9efb-4f08-8bf8-af7866d4aeed"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"plt.figure(figsize =(20,6))\nsns.barplot(x='perimeter_worst',y='area_worst',data =df, hue= 'diagnosis')\n","metadata":{"_uuid":"c735946e0e6149750913af5472f96b9348d5c29a","_cell_guid":"6ed150d1-9d46-4a3e-a3e9-7c8f9534a043"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"plt.figure(figsize= (10,10), dpi=100)\nsns.heatmap(cancer_df.corr()) # plotting the correlation matrix of the dataset","metadata":{"_uuid":"954a0868df987fe4e7982e5ddb8eda2b569a0a23","_cell_guid":"f41284b6-cdfd-4111-a15d-9811b70cbe89"},"cell_type":"code"},{"source":"It can be observed from the two relationship plots that the lump/node attribute have a linear relationship with one another. As seen from the correlation matrix map and the barplot the mean radius of the lump varies linearly/proportionally with the mean texture. The same is is the case with almost all the parameters involved in the diagnosis","metadata":{"_uuid":"1f32a4805ecadb0199cac126d06b26c9f4be0239","_cell_guid":"ba9324b3-1148-4be9-80cf-ef030dd9a783"},"cell_type":"markdown"},{"source":"# 3. Prediction based on Logistical Regression Model","metadata":{"_uuid":"b8a118dbdf3459ed937aa826c2c1fb7298ec1a10","_cell_guid":"68d9af4b-dacc-4d59-aab8-63ef2fcb7336"},"cell_type":"markdown"},{"source":"Logistical regression method of classification is a quite famous algorithm that being used to solve a variety of problems involving \"Binary Classification\" of the data. \n\nSome of the example of the Logistical Regression problems are:\n1) Spam vs Ham email deduction.\n2) Loan Defualt chances (yes/no) based on customer transaction data.\n3) Disease diagnosis.\n\nThe convention for binary classification is to have twoclasses 0 and 1. We can't use linear regression on these binary groups as they won't lead a good fit. Instead we use a sigmoid function to classify the data. The sigmoid function takes in any values and gives an output between 0 & 1.\n\nIn the case of the breast cancer the diagnostics can either Malignant or benign. Therefore we can use Logistical regression model to train this data and predict the out come of a prognosis.\n\n\n","metadata":{"_uuid":"adda25d70ff73d829e8af2b9d16d8ad77d651b92","_cell_guid":"63da046e-7981-4613-b30e-1927dbfa4cba"},"cell_type":"markdown"},{"source":"![Confusion Matrix Representation](https://qph.ec.quoracdn.net/main-qimg-7c9b7670c90b286160a88cb599d1b733)","metadata":{"_uuid":"9c44fe29081dd697225e1c3d6a1d3b7f49d7407a","_cell_guid":"e26fecb9-d6ca-47e9-b02a-d72d48a02bec"},"cell_type":"markdown"},{"source":"The code for the Logistical regression algorithm is as follows:","metadata":{"_uuid":"4d165843cae8450abc870f9189ce752ab6a253b3","_cell_guid":"b49a3f6c-4303-44f0-bc59-ada1b45d93ab"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import train_test_split #Importing module\nX = cancer_df.drop('diagnosis_M',axis=1)\ny = cancer_df['diagnosis_M']","metadata":{"_uuid":"dedfcc22332778011c2a2b592b355ac713db7f5c","collapsed":true,"_cell_guid":"774e62f3-a06e-4c61-8dbb-4e595527afd2"},"cell_type":"code"},{"source":"Now we need to split the original data into test data and training data. For the purpose of this analysis we are considering 70% of the original data as the training data and remainig 30% of the data will be used at the end to check the effectiveness of the model","metadata":{"_uuid":"eedcfd9a00d76761e7f281137852bd4b7cebeacc","_cell_guid":"44f6270f-0a1f-476a-a688-81e544e3c6de"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3) # splitting the data for training and testing","metadata":{"_uuid":"356e584e179a647c93e6cf08baccaf8fc9d67df4","collapsed":true,"_cell_guid":"e8ebc814-c910-4010-b43d-72abe3241514"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.linear_model import LogisticRegression #Logistical Regression Module\nlm = LogisticRegression()\nlm.fit(X_train,y_train)\nprediction = lm.predict(X_test)","metadata":{"_uuid":"e3d486b6c17dd535dfefdd0cc7603a5ab2599b98","collapsed":true,"_cell_guid":"d6f5f671-cc01-4a88-91dd-cfac41053df2"},"cell_type":"code"},{"source":"Confusion matrix and Classification Report","metadata":{"_uuid":"676d899cabd23b12d1bf93b520f1b1334101ee52","_cell_guid":"0ee22b40-0309-4744-9673-508181241087"},"cell_type":"markdown"},{"source":"![Confusion Matrix Representation](http://dni-institute.in/blogs/wp-content/uploads/2015/02/ConfusionMatrix.png)","metadata":{"_uuid":"4da3f5460de824b165755823f4a405d8d41fd6a2","_cell_guid":"0a9cb0e7-3820-40e7-9542-09f9a794c197"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"from sklearn.metrics import classification_report,confusion_matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,prediction))\nCM = confusion_matrix(y_test,prediction)\naccuracy = (CM[0,0]+CM[1,1])/CM.sum()*100\nerror = (CM[0,1]+CM[1,0])/CM.sum()*100\nprint('Accuracy of the model: {0:.2f}%'.format(accuracy))\nprint('Error/ Misclassification rate: {0:.2f}%'.format(error))\n\nprint('\\n\\n')\nprint('Classification Report')\nprint(classification_report(y_test,prediction))","metadata":{"_uuid":"87ab1850880f632392cd48325d27e3965207722d","_cell_guid":"29669b73-2428-498d-9d32-f9729a667fad"},"cell_type":"code"},{"source":"From the confusion matrix the accuracy of the model was found to be 94.7% and the Error rate was around 5.2%. This indicates that linear regression model is a good predictor of the diagnosis when supplied with the various attributes of the lump.\n\nThe classification reports shows how precisely the model predicts the diagnosis. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The model has a precision of 94% for this particular data set.\n\nThe recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.","metadata":{"_uuid":"b443b414c025aea4658b57987a4397fe4387e7ef","_cell_guid":"49e4cd11-8494-4c51-8512-ab33b2779faf"},"cell_type":"markdown"},{"source":"# 4. Prediction based on K Nearest Neighbors Model (KNN Model)","metadata":{"_uuid":"7990f0f59dc9630e66deba1bc0c51c8f35dd4701","_cell_guid":"860bcdea-e64c-4f95-a8af-cb42ca4995c5"},"cell_type":"markdown"},{"source":"K Nearest Neighbors is a classification algorithm that operates on a very simple principle. It's a non-parametric method of classification. Majority voting among the data records in the neighbourhood is usually used to decide the classification of the dataset with or without consideration of distance-based weighting. However, to apply kNN we need to choose an appropriate value for \"k\", and the success of classification is very much dependent on this value.\n\nThe working principe of this algorithm is as follows:\n\nTraining Algorithm:\n1. Store all the Data (Preferably in the form of a Data Frame) \n\nPrediction Algorithm:\n1. Calculate the distance from x to all points in your data. ('x' here refers to the data you want to test with)\n2. Sort the points in your data by increasing distance from x.\n3. Predict the majority label of the “k” closest points.\n\nKNN algorithm is one of the simplest classification algorithm. Even with such simplicity, it can give highly competitive results. KNN algorithm can also be used for regression problems. The only difference from the discussed methodology will be using averages of nearest neighbors rather than voting from nearest neighbors. ","metadata":{"_uuid":"96f1ad3917471ea4532e1943566d9b3a906dcbfd","_cell_guid":"d7e27ea3-b495-4657-b256-89599a1c004a"},"cell_type":"markdown"},{"source":"![Confusion Matrix Representation](https://www.researchgate.net/profile/Victor_Sheng/publication/260612049/figure/fig2/AS:214207917236228@1428082555895/The-principle-diagram-of-the-kNN-classification-algorithm.png)","metadata":{"_uuid":"7430e605787a81ec83c5628dd217c11407da6a83","_cell_guid":"47b1b5ad-2389-4565-84a2-717afc24eb70"},"cell_type":"markdown"},{"source":"Scalarization of the data:\n\nThere are several reasons why scalarization is often used for solving multi-objective problems, and whether it is useful or not depends on your application and the structure of your multiobjective problem.\nSometimes in applications you can express the various objectives in a single unit (e.g. costs) by weighting them properly, and making them comparable in that way. Then the multiobjective problem can by scalarization be solved as  a single objective problem.\n\nIf scalarization is performed carefully, the solution of the scalarized problem will be a Pareto optimal point\n","metadata":{"_uuid":"0720d930e2f4bffa5fc2615bd76ceb88f5f3084a","_cell_guid":"9d28f6dc-67e7-4834-a96c-c161b2169514"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\nscalar.fit(cancer_df.drop('diagnosis_M',axis=1))\nscalar_features = scalar.transform(cancer_df.drop('diagnosis_M',axis=1))\n\n# Converting these features into a Data frame\ndf_feat = pd.DataFrame(scalar_features,columns=cancer_df.columns[:-1])","metadata":{"_uuid":"d35c386bdc0bf0027c0188c34c4e37d769a5eed9","collapsed":true,"_cell_guid":"4d176b4b-9218-460a-a523-5de35d82b6f3"},"cell_type":"code"},{"source":"Predicting using KNN model:","metadata":{"_uuid":"6d4960a701409a4012933c1307d918fd42d46712","_cell_guid":"b5cc406c-43b9-42e1-9fc8-680ba783b5bf"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"from sklearn.neighbors import KNeighborsClassifier # importing KNN module\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_test,y_test)# fitting the test data to the model\n\n# Predicting the outcome for the test data\nprediction = knn.predict(X_test)","metadata":{"_uuid":"ce7291176c2dd614eea3c37bf5a6f129198a1d55","collapsed":true,"_cell_guid":"a15d983c-c92d-4c08-8931-c18377e94825"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Confusion matrix and Classsification report for K = 1\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint('Confusion matrix and Classsification report for K = 1')\nprint('\\n')\n\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,prediction))\nCM = confusion_matrix(y_test,prediction)\naccuracy = (CM[0,0]+CM[1,1])/CM.sum()*100\nerror = (CM[0,1]+CM[1,0])/CM.sum()*100\nprint('Accuracy of the model: {0:.2f}%'.format(accuracy))\nprint('Error/ Misclassification rate: {0:.2f}%'.format(error))\n\nprint('\\n\\n')\nprint('Classification Report')\nprint(classification_report(y_test,prediction))","metadata":{"_uuid":"60bc9c65ebff8fc6d58bfd52b67db498f9366362","_cell_guid":"54c01d86-11bf-4f6a-9ffc-d1da7565c319"},"cell_type":"code"},{"source":"It can be observed that the KNN model predicts the diagnosis with an accuracy of 100%. K=1 is a good value for prediciting the diagnosis with high accuracy and precision for this data set. \n\nJust for the sake of comparison, we can find how the model behaves for the other K values. The model is iterated over a K value ranging between 1 & 40.\n","metadata":{"_uuid":"12c868223bc5a636d9439ff2340ba59eb1ff633e","collapsed":true,"_cell_guid":"aff49e31-30ad-4eaa-9c75-831243da80ad"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"error_rate = []\nfor k in range(1,41):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_test,y_test)# fitting the test data to the model\n    predi = knn.predict(X_test)\n    error_rate.append(np.mean(predi!=y_test))\n\n    ","metadata":{"_uuid":"e4498bc405116e16f18706c8e4be916d253c6555","collapsed":true,"_cell_guid":"e41ea11b-182b-488d-a8f7-395fd2fd1dc5"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"plt.figure(figsize = (10,10))\nplt.plot(range(1,41),error_rate,ls = '--',color = 'blue',marker = 'o',markerfacecolor = 'red')\nplt.xlabel(\"K- value\")\nplt.ylabel(\"Error Rate\")\nplt.xlim((0,40))\nplt.title(\"Error Rate vs K- value\")\n","metadata":{"_uuid":"a17fde3b278e78c1fb7125593f14bc0e42fa96f4","_cell_guid":"36172437-8187-4b55-8fa0-842086e19e0c"},"cell_type":"code"},{"source":"From the above plot it can be observed that the error is zero at K =1 but for all the other values of \"K\" the error rate keeps on increasing.","metadata":{"_uuid":"65e45a8d4b37f9b5725c74ca2eb35c31fbf74e7c","_cell_guid":"3b870e90-93ac-4cb3-ab42-7b5371e443db"},"cell_type":"markdown"},{"source":"**Cross validation of the KNN algorithm**\n","metadata":{"_uuid":"273a4e600296296ad442659fece6f01e318ff6ff","_cell_guid":"e3db956e-f7d4-493e-b77b-3e073730735f"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import cross_val_score\nCV_scores = cross_val_score(knn,X_test,y_test,cv =5)\nCV_scores","metadata":{"_uuid":"feac11c9fb8ac7979e2c0f872211b3c3ea0b5f12","_cell_guid":"f41e6e90-6f77-44fd-9eb3-cc1eab6dc87b"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"print(\"Accuracy: %0.2f (+/- %0.2f)\" % (CV_scores.mean(),CV_scores.std() * 2))\n","metadata":{"_uuid":"adfb3be538b7d201c4ec8eefe8f103369294bb59","_cell_guid":"46e9dbb4-2029-47a0-a356-ea401fefa4dd"},"cell_type":"code"},{"source":"# 5. Conclusion ","metadata":{"_uuid":"49eafeeafe2a52796e27acc40676c2e1325d0794","_cell_guid":"e818c93c-adaa-4225-a01f-5b950dbfc464"},"cell_type":"markdown"},{"source":"Analysing the results from the both the models it can be concluded that,  at k = 1, the KNN model is the best predictor of the two alogorithams for the given set of data. But this data set has only 17070 data points, and it would be interesting to observe the behaviour of the KNN model for considerably larger data set. ","metadata":{"_uuid":"e5c7ceace9300f6b397493d6494abbb916b30881","_cell_guid":"a2eb95bf-6695-4797-a9a7-a92cc008e660"},"cell_type":"markdown"}]}