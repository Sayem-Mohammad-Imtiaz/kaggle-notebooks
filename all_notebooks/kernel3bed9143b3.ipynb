{"cells":[{"metadata":{},"cell_type":"raw","source":"In this project our goal is to predict if a customer may churn using machine learning techniques to help the bank to take steps in incentivising those customers to stay. \n\nThis is a classification problem and we used binary classification algorithm as our target has 2 classes only, the customer may churn or stay."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.manifold import Isomap\nfrom sklearn.preprocessing import normalize, Normalizer # data normalizers\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path to save figures\nif not os.path.exists('./Figures'):\n    os.mkdir('./Figures')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the data using pandas routine\ndata = pd.read_csv('../input/Churn_Modelling.csv')\n\n#printing 5 first rows od the data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the table above; We can drop the first three columns as neither RowNUmber nor customerId or the name of the customer contribute to the fact that a customer can exit the bank or not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# general information about the data\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we realize that we have 3 categorical data. We can drop one of them(Surname) as it will not contribute to our target feature(Exited) during this work. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the irrelevant columns  as shown above\ndata = data.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot encoding our categorical attributes\n\"\"\"As Gender will gibve us two columns we can drop male column and renaim the Gender_Female to Gender \nsuch that 1 corresponds to the female customer while 0 corresponds to male customer\"\"\"\ncat_attr = ['Geography', 'Gender']\ndata = pd.get_dummies(data, columns = cat_attr, prefix = cat_attr)\n\ndata = data.drop([\"Gender_Male\"], axis = 1)\ndata.rename(columns={\"Gender_Female\": \"Gender(1:F,0:M)\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# five point descriptive statistics, and std\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above; We realize that there is no missing data from the count row."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for unique value in the data attributes\ndata.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets check the class distributions\nsns.countplot(\"Exited\",data=data)\nplt.title('Histogram')\nplt.savefig('./Figures/Class distribution')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets check the class distributions\nsns.countplot(\"Gender(1:F,0:M)\",data=data)\nplt.title('Histogram')\nplt.savefig('./Figures/Gender distribution')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Data are unmbalanced from the histogram. let us chetch the class percentage\"\"\"\nN,D = data.shape\nno_exited_pct = np.sum(data.iloc[:,-5] == 0)/N\nexited_pct = np.sum(data.iloc[:,-5] == 1)/N\n\nprint('No exited customer  Class precentage: {:2f}'.format(100*no_exited_pct))\nprint('Exited customer Class precentage: {:2f}'.format(100*exited_pct))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group the data by the target variable \ndata_grp= data.groupby('Exited')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seeing the count per Exited customers data\ndata_grp.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# general describtive per Exited customers data\ndata_grp.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# general data correlation\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# general data correlation heatmap\nsns.heatmap(data.corr(),cmap='YlGnBu')\nplt.title('Correlation')\nplt.savefig('./Figures/corr_matrix_plot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#arranging features with are highly correlated with the target (ascending order)\ncor = data.corr()\ncorr_t = (cor [\"Exited\"]).abs()\n\nprint(\"The features which are most correlated with Exited feature:\\n\", corr_t.sort_values(ascending=False)[1:14].index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting all features \n#sns.pairplot(data,hue='Exited',palette=\"dark\")\nsns.pairplot(data,palette=\"dark\")\nplt.title('Pair plot')\nplt.legend(['Not churn', 'churn'])\nplt.savefig('./Figures/All in one plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From the above output; It shows that :\n1.Most of churned customers are between 25 - 70 years.\n2.The tenure distribution tend to be the same for both categories (churned and not churned customers)\n3.For customers having < 100000 as they balance; their distribution shows that they have a lower rate of churn while for customers having > 100000 on their account have  a high rate of churning.\n4.Customers with 1 and 2 products has a low rate of churn while customers with 3 and 4 products have a high rate of churn.\n5.Customers from Spain are likely to churn more than customers from other regions.\n6.Female are most likely to churn than Male customers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#  churned customers class distributions on gender\nchurn     = data[data[\"Exited\"] == 1]\nnot_churn = data[data[\"Exited\"] == 0]\nsns.countplot(\"Gender(1:F,0:M)\",data=churn)\nplt.title('Histogram_churn_Customers')\nplt.savefig('./Figures/Gender_churn distribution')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  not churned customers class distributions on gender\nchurn     = data[data[\"Exited\"] == 1]\nnot_churn = data[data[\"Exited\"] == 0]\nsns.countplot(\"Gender(1:F,0:M)\",data=not_churn)\nplt.title('Histogram_churn_Customers')\nplt.savefig('./Figures/Gender_not_churn distribution')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dimentionality reduction\nThe data we have is highly nonlinear as shown by the scater plots from pair plot. We are considering the dimentionality reduction as a preprocessig and feature engenering step to better represent the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimentionality reduction\n\npca = PCA(2) # to get the independent representation\ntsne = TSNE(2) # state of the art / the data is nonlinear\niso = Isomap() # follwing the assumption that the data lives in nonlinear manifold.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the dimensionality reduction algorithm \n\npca_data = pca.fit_transform(normalize(data))\ntsne_data = tsne.fit_transform(normalize(data))\niso_data = iso.fit_transform(normalize(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ploting \nf, (ax1, ax2, ax3) = plt.subplots(3, 1,)\n\n\nax1.set_title('PCA')\nax1.scatter(pca_data[:,0],pca_data[:,1], c = data['Exited'])\nplt.title('PCA plot')\n\n\nax2.set_title('TSNE')\nax2.scatter(tsne_data[:,0],tsne_data[:,1], c = data['Exited'])\nplt.title('TSNE')\n\n\nax3.set_title('Isomap')\nax3.scatter(iso_data[:,0],iso_data[:,1], c = data['Exited'])\nplt.title('ISOMAP')\nplt.savefig('./Figures/PCA,TSNE,ISOMAP')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = data.drop([\"Exited\"],axis=1)\ny = data.Exited\ntrain_data,test_data, target_train, target_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data))\nprint(len(test_data))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation criteria\nAs the data is unbalance we are going to look at confusion matrix values like recal and f1 score which is the best way to judge the model from unbalanced data as we can not totally relay on the testing accuracy."},{"metadata":{},"cell_type":"markdown","source":"# 1.1- k-Nearest Neighbors (k-NN) Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnp.random.seed(6)\nX=normalize(train_data)\ny=target_train\n# search for an optimal value of K for KNN with cross validation\n# range of k we want to try\nk_range = range(2, 39)\n# empty list to store scores\nk_scores = []\n# 1. we will loop through reasonable values of k\nfor k in k_range:\n    # 2. run KNeighborsClassifier with k neighbours\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n    scores = cross_val_score(knn, X, y, cv = 10, scoring='accuracy')\n    #scores = cross_val_score(knn, inputs, label, cv = 10, scoring='accuracy')\n    # 4. append mean of scores for k neighbors to k_scores list\n    k_scores.append(scores.mean())\n\nprint(\"k different averages:\", k_scores)\nprint(\"\\nMax average, index of Max:\", max(k_scores),\"||\", k_scores.index(max(k_scores)))\npos = k_scores.index(max(k_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot how accuracy changes as we vary k\n# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-validated accuracy')\nplt.title('Cross-Validate Accuracy Vs K')\nplt.savefig('./Figures/Cross-Validate Accuracy Vs K')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = KNeighborsClassifier(3)\nprint(\"\\t %%% Now lets fit our model first and then we test with our test set %%%\")\nmodel = model.fit(X,y)\nprint(\"\\n Training score: \",model.score(X, y)) \npred = model.predict(normalize(test_data))\nscore = metrics.accuracy_score(pred, target_test)\nprint(\"\\nThe accuracy score that we get is: \",score)  \nprint(\"\\n Confusion Matrix: \", confusion_matrix(target_test, pred))\nprint(metrics.classification_report(target_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.2- Extra trees Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nETC = ExtraTreesClassifier(random_state=5)\nETC = ETC.fit(X, y)\nprint(\"\\n Training score: \",ETC.score(X, y)) #evaluating the training error\npred = ETC.predict(normalize(test_data))\nscore = metrics.accuracy_score(pred,target_test)\nprint(\"\\nThe accuracy score that we get is: \",score)\nprint(\"\\n Confusion Matrix: \", confusion_matrix(target_test, pred))\nprint(metrics.classification_report(target_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  1.3- Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train, label_input = X, y\ninput_test, label_test = normalize(test_data), target_test\n\nRF = RandomForestClassifier(max_depth=11, random_state=5)\nRF = RF.fit(input_train, label_input)\nprint(\"\\n Training score: \",RF.score(input_train, label_input)) #evaluating the training error\npred = RF.predict(input_test)\nscore = metrics.accuracy_score(pred,label_test)\nprint(\"\\nThe accuracy score that we get is: \",score)\nprint(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\nprint(metrics.classification_report(label_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting featrure importance\nRF.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.4- Dummy Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndc = DummyClassifier(strategy=\"uniform\",random_state=5)\ndc = dc.fit(input_train, label_input)\nprint(\"\\n Training score: \",dc.score(X, y)) #evaluating the training error\npred = dc.predict(input_test)\nscore = metrics.accuracy_score(pred,label_test)\nprint(\"\\nThe accuracy score that we get is: \",score)\nprint(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\nprint(metrics.classification_report(label_test, pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.5- Ada Boost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nABC = AdaBoostClassifier(random_state=5)\nABC = ABC.fit(input_train, label_input)\nprint(\"\\n Training score: \",ABC.score(input_train, label_input)) #evaluating the training error\npred = ABC.predict(input_test)\nscore = metrics.accuracy_score(pred,label_test)\nprint(\"\\nThe accuracy score that we get is: \",score)\nprint(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\nprint(metrics.classification_report(label_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.6- Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nGBC = GradientBoostingClassifier(random_state=5)\nGBC = GBC.fit(input_train, label_input)\nprint(\"\\n Training score: \",GBC.score(input_train, label_input)) #evaluating the training error\npred = GBC.predict(input_test)\nscore = metrics.accuracy_score(pred,label_test)\nprint(\"\\nThe accuracy score that we get is: \",score)\nprint(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\nprint(metrics.classification_report(label_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.7- Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndt = tree.DecisionTreeClassifier(random_state=5)\ndt = dt.fit(input_train, label_input)\nprint(\"\\n Training score: \",dt.score(input_train, label_input)) #evaluating the training error\npred = dt.predict(input_test)\nscore = metrics.accuracy_score(pred,label_test)\nprint(\"\\nThe accuracy score that we get is: \",score)\nprint(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\nprint(metrics.classification_report(label_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.8 - SGD Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsgdc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=25,random_state=5)\nsgdc = sgdc.fit(input_train, label_input)\nprint(\"\\n Training score: \",sgdc.score(input_train, label_input)) #evaluating the training error\npred = sgdc.predict(input_test)\nscore = metrics.accuracy_score(pred,label_test)\nprint(\"\\nThe accuracy score that we get is: \",score)\nprint(\"\\n Confusion Matrix: \", confusion_matrix(label_test, pred))\nprint(metrics.classification_report(label_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  # 1.9 -  Train a binary classifier to predict if the customer will churn or not\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Scaling data\nscaler = MinMaxScaler()\nscaler.fit(X)\ndfx = scaler.transform(X)\nscaler.fit(test_data)\ndfx_test = scaler.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neural network\n\nmodel0 = keras.Sequential([\n    keras.layers.Flatten(input_shape=dfx.shape[1:]),\n    keras.layers.Dense(128, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\n\nmodel0.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metrics=['accuracy']);\n\n\nmodel0.fit(dfx, y, epochs=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model0.evaluate(dfx_test, target_test)\n\nprint('Test accuracy:', test_acc)\n\n\ny_hat = model0.predict_classes(dfx_test)\n\nprint(metrics.classification_report(target_test, y_hat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Using Sampling methods to overcome the unbalance in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper ploting function \n\ndef plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# balancing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy = 'auto', kind = 'regular',random_state=5)\n\ndata1 = data.drop([\"Exited\"],axis=1)\ninputs,label = sm.fit_sample(data1, data['Exited'])\nprint(\"Original dataset: \",data['Exited'].value_counts())\n\n\ncompt = 0\nfor i in range(len(label)):\n    if label[i]==1:\n        compt += 1\nprint(\"\\nNumber of 1 in the new  dataset: \",compt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_2d_space(inputs, label, 'balanced data scatter plot')\nplt.savefig(\"Scatter plot balanced data\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xc = inputs\nyc = label\nXc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 - Modelling with balanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-----------------Gradient Boost Classifier--------------------')\nGBC = GradientBoostingClassifier(random_state=5)\nGBC = GBC.fit(Xc_train, yc_train)\npred = GBC.predict(Xc_test)\nscore = metrics.accuracy_score(pred,(yc_test))\nprint(\"Accuracy---\", accuracy_score(yc_test,pred))\nprint(metrics.classification_report(yc_test, pred))\n\n\nprint('-----------------Ada Boost Classifier--------------------')\nABC = AdaBoostClassifier(random_state=5)\nABC = ABC.fit(Xc_train, yc_train)\npred = ABC.predict(Xc_test)\nprint(\"Accuracy---\", accuracy_score(yc_test,pred))\nprint(classification_report(yc_test,pred))\n\n\nprint('-----------------RandomForestClassifier--------------------')\nmodel  = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0).fit(Xc_train, yc_train)\npred = model.predict(Xc_test)\nprint(\"Accuracy---\", accuracy_score(yc_test,pred))\nprint(classification_report(yc_test,pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Try the neural network also\n#Scaling data\nscaler = MinMaxScaler()\nscaler.fit(Xc_train)\ndfx = scaler.transform(Xc_train)\nscaler.fit(Xc_test)\ndfx_test = scaler.transform(Xc_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model0.fit(dfx, yc_train, epochs=100);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model0.evaluate(dfx_test, yc_test)\n\nprint('Test accuracy:', test_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = model0.predict_classes(dfx_test)\n\nprint(metrics.classification_report(yc_test, y_hat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"raw","source":"\nIn this project we build a model that predict how likely a customer is going to churn. During the data exploration analysis we realize that our data was not balanced and After trying different 9 algorithms with the orginal data, we decide to use sampling method to overcome the unbalanced problem. \n\nFrom the original data; the female customer are the most likely to churn, customer that are located in Germany are the most churned, and also customer having only one and two products are the most churned. By building classifiers on our original data; we realize that we had good training accuracy than testing ones but with bad f1 score. Combining the f1_score and accuracy(testing), top 3 algorithms are:\n    -Gradient Boost Classifier \n    -Ada Boost Classifier\n    -Random forest Classifier.\n    \n    \nWe trained a neural network to predict if the customer will churn or not and the f1_score was not good. We balanced the data and we realie that with balanced data; there was an improvement in accuracy and f1_score. \n\n\nWe ended up by concluding that Gradient Boost Classifier algorithm is our choice for this case study. Our choice is based 2 facts:\n    -From the original data;  Gradient Boost Classifier has a good perfomamnce on the data more than other 8 algorithms. \n    -The improvement from sampling method where f1_score improves from 91% to 92% for class 0(not churned customers) and \n    43% to  91% for class 1 (churned customer) which leads to the accuracy improvement from 84% to 91% .\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}