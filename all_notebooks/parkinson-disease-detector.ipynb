{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"park = pd.read_csv(\"../input/parkinson-disease-detection/Parkinsson disease.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"park[\"status\"].value_counts()\n# there are 147 datapoints where it shows they have disease and  48 datapoints where it shows they don't have disease, \n# which means the dataset is skewed.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                       CHALLENGES:\n                                                        \n* In the target column, there are 48 healthy people & 147 people with Parkinson's disease i.e; one of the 2 classes is under represented or skewed for which the accuaracy at model level can be misleading. So need to consider the accuracy at class level i.e; recall using confusion matrix\n* 'name' is object and also it doesn't contribute in model building so it has to be removed from dataset\n* Large set of attributes, so building and analysing pair plot is difficult\n* In 'status' attribute's pair plot, datapoints overlapped over majority region, so distinguishing between classes is difficult."},{"metadata":{"trusted":true},"cell_type":"code","source":"park[park.isnull().any(axis=1)]\n#no missing/null data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='status',data=park)\n#Shows the distribution of status column - univariate analysis of the target column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(park)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                OBSERVATIONS FROM PAIR PLOT:\n\n* In 'status' column's pair plot, datapoints (classes) overlapped over majority region, so distinguishing between classes is difficult.\n* We can see few of the columns are normally distributed like HNR column.\n* We can see few of the columns are positively correlated like the Jitter:DDP and MDVP:Shimmer columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"park = park.drop(\"name\",axis=1)\n#Dropped name column as it doesnot contribute to model building","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\npark.boxplot(['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)','HNR'],ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=[]\nfor i in park.columns:\n    for j in park[i]:\n        if (j<1 and j>0):\n            k.append(i)\n            break\n\nfig, ax = plt.subplots(figsize=(15,5))\npark.boxplot(k,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                OBSERVATIONS FROM BOXPLOTS:\n\n* From the box plots above, we can see that there are outliers or long tails or skewness in almost all the columns except MDVP:Fo(Hz), RPDE and DFA columns.\n* In the columns which have outliers, most of them are positively skewed except HNR which is negatively skewed.\n* In the column 'spread2', we can see the tails or outliers being present on both the sides."},{"metadata":{"trusted":true},"cell_type":"code","source":"park.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                            OBSERVATIONS FROM FIVE POINT SUMMARY:\n\n* A low standard deviation indicates that the data points tend to be close to the mean of the data set, while a high standard deviation indicates that the data points are spread out over a wider range of values.\n* So, from the abaove we can infer that, except MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz), rest o fthe columns have a spread closer to the mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = park.drop(\"status\",axis=1)\ny = park[\"status\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=9)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Created the model using “entropy” method of reducing the entropy and fitted it to training  data."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier(criterion=\"entropy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tested the model on test data and the accuracy achieved. Captured the predicted  values and did a crosstab. "},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_test)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_train,y_train)\n#accuracy of the model obtained for the train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(y_test,preds)\n#accuracy of the model obtained for the test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_test,preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Used regularization parameters of max_depth, min_sample_leaf to recreate the model and checked its impact on the model accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_reg = DecisionTreeClassifier(criterion=\"entropy\",max_depth=10,min_samples_leaf=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_reg = model_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_reg.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(y_test,preds_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                        OBSERVATIONS:\n\n* After regularizing, the model accuracy has decreased (for test data).\n* But the model without regularization was a overfit model as the train accuracy was 100% & there was a significant drop in test accuracy.\n* Whereas after regularization, we have managed to bring both the train and test accuracies at the same level which is not a overfit model anymore."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfcl = RandomForestClassifier(n_estimators=100,max_depth=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfcl.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_rfcl = rfcl.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfcl.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(y_test,preds_rfcl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z=0\nb=0\nfor i in np.arange(10,150):\n    rfcl = RandomForestClassifier(n_estimators = i, max_depth=15)\n    rfcl.fit(X_train, y_train)\n    preds_rfcl=rfcl.predict(X_test)\n    acc=accuracy_score(y_test,preds_rfcl)\n    if acc>z:\n        z=acc\n        b=i\nprint(\"For\",b,\"number of trees,accuracy is\",z)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                    OBSERVATIONS:\n\n* After using Random forest classifier, we can see a drastic increase in test accuracy score.\n* We have used a 'for' loop to determine the optimal number of trees that gives the best result and it is shown above."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}