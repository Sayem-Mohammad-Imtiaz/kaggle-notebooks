{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analytics Vidhya - Guided Hackathon 2\n\n# Problem Statement\n\nAs YouTube becomes one of the most popular video-sharing platforms, YouTuber is developed as a new type of career in recent decades. YouTubers earn money through advertising revenue from YouTube videos, sponsorships from companies, merchandise sales, and donations from their fans. In order to maintain a stable income, the popularity of videos become the top priority for YouTubers. Meanwhile, some of our friends are YouTubers or channel owners in other video-sharing platforms. This raises our interest in predicting the performance of the video. If creators can have a preliminary prediction and understanding on their videosâ€™ performance, they may adjust their video to gain the most attention from the public.\n\nYou have been provided details on videos along with some features as well. Can you accurately predict the number of likes for each video using the set of input variables?\n\n\n**Refer the dataset :-** https://www.kaggle.com/jainpooja/av-guided-hackathon"},{"metadata":{},"cell_type":"markdown","source":"**Load the libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-dark')\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression, ElasticNet\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.simplefilter('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load the dataset**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ss = pd.read_csv('../input/av-guided-hackathon/sample_submission_cxCGjdN.csv')\ntrain = pd.read_csv('../input/av-guided-hackathon/train.csv')\ntest = pd.read_csv('../input/av-guided-hackathon/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Identify target columns and feature variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_COL, TARGET_COL = 'video_id', 'likes'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'\\nTrain data contains {train.shape[0]} samples and {train.shape[1]} variables')\nprint(f'\\nTest data contains {test.shape[0]} samples and {test.shape[1]} variables')\n\nfeatures = [c for c in train.columns if c not in [ID_COL, TARGET_COL]]\nprint(f'\\nThe dataset contains total {len(features)} features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check the target distribution as it is a regression problem**"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = train[TARGET_COL].plot(kind = 'density', title = 'Likes Distribution', fontsize=14, figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we see the data is highly right skewed, we will apply log transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = pd.Series(np.log1p(train[TARGET_COL])).plot(kind = 'density', title = 'Log Likes Distribution', fontsize=14, figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let check the datatype of all the columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear from the above output that there are no null values"},{"metadata":{},"cell_type":"markdown","source":"**Unique values in each variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['views', 'dislikes', 'comment_count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Univariate Analysis**"},{"metadata":{},"cell_type":"markdown","source":"**Numeric Columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 1, figsize=(8, 9))\nfor i, c in enumerate(num_cols):\n  _ = train[[c]].boxplot(ax=axes[i], vert=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Log Transformation of Numerical Columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in num_cols + ['likes']:\n  train[c] = np.log1p(train[c]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(4, 1, figsize=(8, 9))\nfor i, c in enumerate(num_cols + ['likes']):\n  _ = train[[c]].boxplot(ax=axes[i], vert=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bivariate Analysis**"},{"metadata":{},"cell_type":"markdown","source":"**Correlation heatmaps**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 8))\n_ = sns.heatmap(train[num_cols + ['likes']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pair Plots**"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.pairplot(train[num_cols + ['likes']], height=5, aspect=24/16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical Columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['channel_title'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['category_id', 'country_code', 'channel_title']\nfig, axes = plt.subplots(1, 2, figsize=(24, 10))\n\nfor i, c in enumerate(['category_id', 'country_code']):\n    _ = train[c].value_counts()[::-1].plot(kind = 'pie', ax=axes[i], title=c, autopct='%.0f', fontsize=18)\n    _ = axes[i].set_ylabel('')\n    \n_ = plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7, 8.27)})\n\ntop_20_channels = train['channel_title'].value_counts()[:20].reset_index()\ntop_20_channels.columns = ['channel_title', 'num_videos']\n\n_ = sns.barplot(data = top_20_channels, y = 'channel_title', x = 'num_videos')\n_ = plt.title(\"Top 20 Channels with maximum number of videos\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bivariate Analysis**"},{"metadata":{},"cell_type":"markdown","source":"**Country wise number of videos for channels**"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_wise_channels = train.groupby(['country_code', 'channel_title']).size().reset_index()\ncountry_wise_channels.columns = ['country_code', 'channel_title', 'num_videos']\ncountry_wise_channels = country_wise_channels.sort_values(by = 'num_videos', ascending=False)\nfig, axes = plt.subplots(4, 1, figsize=(10, 20))\n\nfor i, c in enumerate(train['country_code'].unique()):\n  country = country_wise_channels[country_wise_channels['country_code'] == c][:10]\n  _ = sns.barplot(x = 'num_videos', y = 'channel_title', data = country, ax = axes[i])\n  _ = axes[i].set_title(f'Country Code {c}')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CatPlots**"},{"metadata":{},"cell_type":"markdown","source":"**Likes distribution per Category**"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.catplot(x=\"category_id\", y=\"likes\", data=train, height=5, aspect=24/8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Likes Distribution per country**"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.catplot(x=\"country_code\", y=\"likes\", data=train, height=5, aspect=24/8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Average likes per Country**"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = train.groupby('country_code')['likes'].mean().sort_values().plot(kind = 'barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like videos posted in England have an higher average number of likes compared to videos posted in India."},{"metadata":{},"cell_type":"markdown","source":"**DateTime Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['publish_date'] = pd.to_datetime(train['publish_date'], format='%Y-%m-%d')\ntest['publish_date'] = pd.to_datetime(test['publish_date'], format='%Y-%m-%d')\ntrain['publish_date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['publish_date'].min(), train['publish_date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['publish_date'].dt.year.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Number of Videos in data datewise**"},{"metadata":{"trusted":true},"cell_type":"code","source":"latest_data_train = train[train['publish_date'] > '2017-11']\nlatest_data_test = test[test['publish_date'] > '2017-11']\n_ = latest_data_train.sort_values(by = 'publish_date').groupby('publish_date').size().rename('train').plot(figsize=(18, 6), title = 'Number of Videos')\n_ = latest_data_test.sort_values(by = 'publish_date').groupby('publish_date').size().rename('test').plot(figsize=(18, 6), title = 'Number of Videos')\n_ = plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Average likes in data sorted by date**"},{"metadata":{"trusted":true},"cell_type":"code","source":"latest_data = train[train['publish_date'] > '2017-11']\n_ = latest_data.sort_values(by = 'publish_date').groupby('publish_date')['likes'].mean().plot(figsize=(18, 6), title=\"Mean Likes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Number of videos by country**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = latest_data.groupby(['publish_date', 'country_code']).size().reset_index()\n_ = tmp.pivot_table(index = 'publish_date', columns = 'country_code', values=0).plot(subplots=True, figsize=(20, 20),\n                                                                                           title='Number of Videos by country',\n                                                                                           sharex=False,\n                                                                                           fontsize=20)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Average number of likes by country order by date**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = latest_data.groupby(['publish_date', 'country_code'])['likes'].mean().reset_index()\n_ = tmp.pivot_table(index = 'publish_date', columns = 'country_code', values='likes').plot(subplots=True, figsize=(20,20),\n                                                                                           title='Average Number of Likes by country',\n                                                                                           sharex=False,\n                                                                                           fontsize=20)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analyze Textual Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cols = ['title', 'tags', 'description']\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nwc = WordCloud(stopwords = set(list(STOPWORDS) + ['|']), random_state = 42)\nfig, axes = plt.subplots(2, 2, figsize=(20, 12))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, c in enumerate(text_cols):\n  op = wc.generate(str(train[c]))\n  _ = axes[i].imshow(op)\n  _ = axes[i].set_title(c.upper(), fontsize=24)\n  _ = axes[i].axis('off')\n\n_ = fig.delaxes(axes[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Country wise highly liked Youtube videos top words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_countrywise(country_code = 'IN'):\n  country = train[train['country_code'] == country_code]\n  country = country[country['likes'] > 10]\n  fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n  axes = [ax for axes_row in axes for ax in axes_row]\n\n  for i, c in enumerate(text_cols):\n    op = wc.generate(str(country[c]))\n    _ = axes[i].imshow(op)\n    _ = axes[i].set_title(c.upper(), fontsize=24)\n    _ = axes[i].axis('off')\n\n  fig.delaxes(axes[3])\n  _ = plt.suptitle(f\"Country Code: '{country_code}'\", fontsize=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_countrywise(\"US\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_countrywise(\"GB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_countrywise(\"IN\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_countrywise(\"CA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Helper Function to Download Test Predictions as CSV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def download_preds(preds_test, file_name = 'hacklive_sub.csv'):\n\n  ## 1. Setting the target column with our obtained predictions\n  ss[TARGET_COL] = preds_test\n\n  ## 2. Saving our predictions to a csv file\n\n  ss.to_csv(file_name, index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv('../input/av-guided-hackathon/sample_submission_cxCGjdN.csv')\ntrain = pd.read_csv('../input/av-guided-hackathon/train.csv')\ntest = pd.read_csv('../input/av-guided-hackathon/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Segregate different types of columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['views', 'dislikes', 'comment_count']\ncat_cols = ['category_id', 'country_code']\ntext_cols = ['title', 'channel_title', 'tags', 'description']\ndate_cols = ['publish_date']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Concatenate train and test data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test], axis=0).reset_index(drop = True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One hot Encoding on Categorical columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns = cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna(-999)\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[num_cols + ['likes']] = df[num_cols + ['likes']].apply(lambda x: np.log1p(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['likes']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split data into train and test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_proc, test_proc = df[:train.shape[0]], df[train.shape[0]:].reset_index(drop = True)\nfeatures = [c for c in train_proc.columns if c not in [ID_COL, TARGET_COL]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split train data into train and validation sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"trn, val = train_test_split(train_proc, test_size=0.2, random_state = 420)\n\n###### Input to our model will be the features\nX_trn, X_val = trn[features], val[features]\n\n###### Output of our model will be the TARGET_COL\ny_trn, y_val = trn[TARGET_COL], val[TARGET_COL]\n\n##### Features for the test data that we will be predicting\nX_test = test_proc[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To check results on validation dataset after train the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_squared_log_error\n\ndef rmsle(y_true, y_pred):\n  return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\ndef av_metric(y_true, y_pred):\n  return 1000 * np.sqrt(mean_squared_error(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model on Numerical and Categorical columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in X_trn.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in text_cols + date_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LinearRegression()\n\n_ = clf.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = clf.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeRegressor(random_state=420)\n\n_ = clf.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = clf.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regr = RandomForestRegressor(max_depth=6, random_state=42)\n_ = regr.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = regr.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HyperParameter Tuning with Random Search CV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\n\nprint(random_grid)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestRegressor(n_jobs=-1)\nrf_random = RandomizedSearchCV(estimator = forest, param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring='neg_mean_squared_error')\n# Fit the random search model\nsearch = rf_random.fit(train_proc[cat_num_cols], train_proc[TARGET_COL])\nsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = {'n_estimators': 155,\n 'min_samples_split': 5,\n 'max_features': 'sqrt',\n 'max_depth': 23}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regr = RandomForestRegressor(**best_params)\n_ = regr.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = regr.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test = clf.predict(X_test[cat_num_cols])\n\npreds_test = np.expm1(preds_test)\n\ndownload_preds(preds_test, 'regr_num_cat.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Stratified - K_Fold Validation**"},{"metadata":{},"cell_type":"markdown","source":"**Helper Function to run Stratified K-Fold**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(np.arange(10), 5, labels = False, duplicates='drop')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\ndef run_clf_kfold(clf, train, test, features):\n\n  N_SPLITS = 5\n\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  target = train[TARGET_COL]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[TARGET_COL], 10, labels = False, duplicates='drop')\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ############# Get train, validation and test sets along with targets ################\n  \n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    ############# Scaling Data ################\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n\n\n    ############# Fitting and Predicting ################\n\n    _ = clf.fit(X_trn, y_trn)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = av_metric(y_val, preds_val)\n    print(f'\\nAV metric score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test / N_SPLITS\n\n\n  oofs_score = av_metric(target, oofs)\n  print(f'\\n\\nAV metric for oofs is {oofs_score}')\n\n  return oofs, preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-Fold on Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_params = best_params = {'n_estimators': 155,\n 'min_samples_split': 5,\n 'max_features': 'sqrt',\n 'max_depth': 23}\n\nclf = RandomForestRegressor(**rf_params)\n        \n\ndt_oofs, dt_preds = run_clf_kfold(clf, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Boosting Algorithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_gradient_boosting(clf, fit_params, train, test, features):\n  N_SPLITS = 5\n  oofs = np.zeros(len(train_proc))\n  preds = np.zeros((len(test_proc)))\n\n  target = train[TARGET_COL]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[TARGET_COL], 10, labels = False, duplicates='drop')\n\n  feature_importances = pd.DataFrame()\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n\n    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\n    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = av_metric(y_val, preds_val)\n    print(f'\\nAV metric score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test / N_SPLITS\n\n\n  oofs_score = av_metric(target, oofs)\n  print(f'\\n\\nAV metric for oofs is {oofs_score}')\n\n  feature_importances = feature_importances.reset_index(drop = True)\n  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\n  fi.plot(kind = 'barh', figsize=(12, 6))\n\n  return oofs, preds, fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"**Helper Function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def join_df(train, test):\n\n  df = pd.concat([train, test], axis=0).reset_index(drop = True)\n  features = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\n  df[num_cols + ['likes']] = df[num_cols + ['likes']].apply(lambda x: np.log1p(x))\n\n  return df, features\n\ndef split_df_and_get_features(df, train_nrows):\n\n  train, test = df[:train_nrows].reset_index(drop = True), df[train_nrows:].reset_index(drop = True)\n  features = [c for c in train.columns if c not in [ID_COL, TARGET_COL]]\n  \n  return train, test, features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df, features = join_df(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['category_id', 'country_code', 'channel_title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Label Encoding\n\ndf[cat_cols] = df[cat_cols].apply(lambda x: pd.factorize(x)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['publish_date'] = pd.to_datetime(df['publish_date'], format='%Y-%m-%d')\ndf['publish_date_days_since_start'] = (df['publish_date'] - df['publish_date'].min()).dt.days\n\ndf['publish_date_day_of_week'] = df['publish_date'].dt.dayofweek\ndf['publish_date_year'] = df['publish_date'].dt.year\ndf['publish_date_month'] = df['publish_date'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_num_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['channel_title_num_videos'] = df['channel_title'].map(df['channel_title'].value_counts())\ndf['publish_date_num_videos'] = df['publish_date'].map(df['publish_date'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_num_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['channel_in_n_countries'] = df.groupby('channel_title')['country_code'].transform('nunique')\ndf['channel_in_n_countries'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Grouping Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['channel_title_mean_views'] = df.groupby('channel_title')['views'].transform('mean')\ndf['channel_title_max_views'] = df.groupby('channel_title')['views'].transform('max')\ndf['channel_title_min_views'] = df.groupby('channel_title')['views'].transform('min')\n\ndf['channel_title_mean_comments'] = df.groupby('channel_title')['comment_count'].transform('mean')\ndf['channel_title_max_comments'] = df.groupby('channel_title')['comment_count'].transform('max')\ndf['channel_title_min_comments'] = df.groupby('channel_title')['comment_count'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_preds_t = np.expm1(cb_preds)\ndownload_preds(cb_preds_t, file_name = 'catboost_5_folds.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering for text data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title_len'] = df['title'].apply(lambda x: len(x))\ndf['description_len'] = df['description'].apply(lambda x: len(x))\ndf['tags_len'] = df['tags'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bag of Words Approach for Text Based Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n?CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TOP_N_WORDS = 50\n\nvec = CountVectorizer(max_features = TOP_N_WORDS)\ntxt_to_fts = vec.fit_transform(df['description']).toarray()\ntxt_to_fts.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = 'description'\ntxt_fts_names = [c + f'_word_{i}_count' for i in range(TOP_N_WORDS)]\ndf[txt_fts_names] = txt_to_fts\n\ntrain_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_num_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostRegressor(n_estimators = 4000,\n                       learning_rate = 0.06,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=4200,\n                       )\n\nfit_params = {'verbose': 300, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_preds_t = np.expm1(cb_preds)\ndownload_preds(cb_preds_t, file_name = 'catboost_text_cols_bow.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LGBMRegressor(n_estimators = 4000,\n                        learning_rate = 0.04,\n                        colsample_bytree = 0.65,\n                        metric = 'None',\n                        )\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n\nlgb_oofs, lgb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_preds_t = np.expm1(lgb_preds)\ndownload_preds(lgb_preds_t, file_name = 'lgb_text_cols_bow.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBRegressor(n_estimators = 3000,\n                    max_depth = 7,\n                    learning_rate = 0.05,\n                    colsample_bytree = 0.5,\n                    random_state=4200,\n                    )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\nxgb_oofs, xgb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_preds_t = np.expm1(xgb_preds)\n\ndownload_preds(xgb_preds_t, file_name = 'xgb_text_cols_bow.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ensembling**"},{"metadata":{"trusted":true},"cell_type":"code","source":" av_metric(np.log1p(train[TARGET_COL]), lgb_oofs * 0.7 + cb_oofs * 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new = train[[ID_COL, TARGET_COL]]\ntrain_new[TARGET_COL] = np.log1p(train_new[TARGET_COL])\n\ntest_new = test[[ID_COL]]\n\ntrain_new['lgb'] = lgb_oofs\ntest_new['lgb'] = lgb_preds\n\ntrain_new['cb'] = cb_oofs\ntest_new['cb'] = cb_preds\n\ntrain_new['xgb'] = xgb_oofs\ntest_new['xgb'] = xgb_preds\n\nfeatures = [c for c in train_new.columns if c not in [ID_COL, TARGET_COL]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LinearRegression()\n\nens_oofs, ens_preds = run_clf_kfold(clf, train_new, test_new, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ens_preds_t = np.expm1(ens_preds)\ndownload_preds(ens_preds_t, file_name = 'hacklive_ensemble_final.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('hacklive_ensemble_final.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}