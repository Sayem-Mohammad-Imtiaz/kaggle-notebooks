{"nbformat_minor":1,"cells":[{"source":"## Summary\nI really liked this [kernel](https://www.kaggle.com/the1owl/classify-me-again). I think it is very creative and funny to create a model to predict personality from a text and to apply it over kaggle comments. This kernel is a variation of the indicated one where I calculate the accuracy of that model and try other model alternatives. At the end, we apply our best model to know what are the Kaggle community most common personalities, but don't take it too seriously :-)","metadata":{"_uuid":"f349ecc9323d6ae852a9e4fd9ef13d516d7ff381","_cell_guid":"f52726af-c202-451e-bf81-d4db266dc601"},"cell_type":"markdown"},{"source":"%matplotlib inline\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom bs4 import BeautifulSoup\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\n\npy.init_notebook_mode(connected=True)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"4440861b925a1a1e60deca6614391070d55f01da","_cell_guid":"d1d634f3-8a15-49ca-b04c-3d2a78b9b59a"},"cell_type":"code","outputs":[]},{"source":"train = pd.read_csv('../input/mbti-type/mbti_1.csv')\nus = pd.read_csv('../input/meta-kaggle/Users.csv')\nps = pd.read_csv('../input/meta-kaggle/ForumMessages.csv')\nmbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition', \n        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling', \n        'J':'Judging', 'P': 'Perceiving'}","execution_count":null,"metadata":{"collapsed":true,"_uuid":"0090ee7b44d610b5b28d81c2c526543edf86af01","_cell_guid":"56b7cee7-e051-48f3-81a9-1cae8992c275"},"cell_type":"code","outputs":[]},{"source":"train.shape","execution_count":null,"metadata":{"collapsed":true,"_uuid":"91df29df60acc4c529ea35c9012a9f8484fffe92","_cell_guid":"1ac9542c-9ea1-45f7-95f0-ce97ccf8d8ec"},"cell_type":"code","outputs":[]},{"source":"We take a look to the classes. It looks like it is a very unbalanced dataset:","metadata":{"_uuid":"f0d2252b28928bedd661a5e5a0f513644b90380a","_cell_guid":"1d770298-0917-497d-91da-767d9a5f7431"},"cell_type":"markdown"},{"source":"cnt_srs = train['type'].value_counts()\n\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Types', fontsize=12)\nplt.show()","execution_count":null,"metadata":{"collapsed":true,"_uuid":"38496216d2bc92bd57160f96c971e8dcaa7ab288","_cell_guid":"6d58e5ef-3cc3-434a-b4f8-84e7eac3db1e"},"cell_type":"code","outputs":[]},{"source":"ps['Message'] = ps['Message'].fillna('')","execution_count":null,"metadata":{"collapsed":true,"_uuid":"f8ed2ac97aa28198781e0fbd71f631b87e4b9618","_cell_guid":"beaa9d80-02d6-44c3-89e7-495eabe24690"},"cell_type":"code","outputs":[]},{"source":"ps_join = ps.groupby('AuthorUserId')['Message'].agg(lambda col: ' '.join(col)).reset_index()","execution_count":null,"metadata":{"collapsed":true,"_uuid":"3bec49bca532eb74a978064493261b059c0ba82b","_cell_guid":"75256a31-292e-4d24-af58-b83711daa6f5"},"cell_type":"code","outputs":[]},{"source":"### ExtraTreesClassifier with SVD\nthis is the model used in the  [kernel](https://www.kaggle.com/the1owl/classify-me-again). We want to evaluate its performance.","metadata":{"_uuid":"d6f0c056d85cf6d1d391cbf971d83d35d8fa8683","_cell_guid":"2b05b1a6-13cf-4553-8e3d-476bfb387de3"},"cell_type":"markdown"},{"source":"etc = ExtraTreesClassifier(n_estimators = 20, max_depth=4, n_jobs = -1)\ntfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\ntsvd = TruncatedSVD(n_components=10)\nmodel = Pipeline([('tfidf1', tfidf), ('tsvd1', tsvd), ('etc', etc)])","execution_count":null,"metadata":{"collapsed":true,"_uuid":"0a4e1fea557cb26515b536768d4c195e732dbe13","_cell_guid":"c45de79a-4c69-48a4-9ca3-ce0957c72c7c"},"cell_type":"code","outputs":[]},{"source":"kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"da8ac5bbb30532e21dde6f8e427b68416fd6e0b4","_cell_guid":"261fc1d2-204c-466e-bf69-819a0ccd9b8a"},"cell_type":"code","outputs":[]},{"source":"np.random.seed(1)\n\nscoring = {'acc': 'accuracy',\n           'neg_log_loss': 'neg_log_loss',\n           'f1_micro': 'f1_micro'}\n\nresults = cross_validate(model, train['posts'], train['type'], cv=kfolds, \n                          scoring=scoring, n_jobs=-1)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"37fcf6b89d83901a8f1fb21753989196ffa49836","_cell_guid":"7407ae0b-343b-42a0-a563-67a28435e0e1"},"cell_type":"code","outputs":[]},{"source":"print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_acc']),\n                                                          np.std(results['test_acc'])))\n\nprint(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_f1_micro']),\n                                                          np.std(results['test_f1_micro'])))\n\nprint(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results['test_neg_log_loss']),\n                                                          np.std(-1*results['test_neg_log_loss'])))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"1900d1e53cb748996d1c750035fb2dc3c4fc0fb2","_cell_guid":"c3bfdf28-5513-48d5-9836-a42c876285e5"},"cell_type":"code","outputs":[]},{"source":"As the dataset is very unbalanced F1 score is a better metric than accuracy","metadata":{"_uuid":"a87ad0739590a10dd24fbe74279076fcff86309e","_cell_guid":"ccb017bb-f95e-4450-aa88-e0c387eb00ab"},"cell_type":"markdown"},{"source":"## Alternative models\nLet's try if we can find a more accuracy model, although we haven't got a lot of data","metadata":{"collapsed":true,"_uuid":"ec934b96fa4a2e98949250eadbeedaad0be2f774","_cell_guid":"2ab8aa27-46f0-4590-9297-e5defc509bbf"},"cell_type":"markdown"},{"source":"def cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    return text","execution_count":null,"metadata":{"collapsed":true,"_uuid":"e72cf6748481b22bf1804cda91ebd1e34e22be67","_cell_guid":"07980b64-22c1-4e4b-8c7c-6451d0492e35"},"cell_type":"code","outputs":[]},{"source":"train['clean_posts'] = train['posts'].apply(cleanText)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"48dd4ff1d2d5546e8cda19baf5c02155cdb5f332","_cell_guid":"8389d694-00a3-4e46-8d77-ef6a616988ca"},"cell_type":"code","outputs":[]},{"source":"### Naive Bayes","metadata":{"_uuid":"92b9be0f1c3edaff08ec80ce9ce60a1d3e71692f","_cell_guid":"e204249b-a4c0-4ce2-a195-28f5bb8b4731"},"cell_type":"markdown"},{"source":"np.random.seed(1)\n\ntfidf2 = CountVectorizer(ngram_range=(1, 1), \n                         stop_words='english',\n                         lowercase = True, \n                         max_features = 5000)\n\nmodel_nb = Pipeline([('tfidf1', tfidf2), ('nb', MultinomialNB())])\n\nresults_nb = cross_validate(model_nb, train['clean_posts'], train['type'], cv=kfolds, \n                          scoring=scoring, n_jobs=-1)\n","execution_count":null,"metadata":{"collapsed":true,"_uuid":"37b9cebd34dbcf517313fca395e3f8067c4a3732","_cell_guid":"c3d7af4d-37b9-4715-9403-9ab465dd8a79"},"cell_type":"code","outputs":[]},{"source":"print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_acc']),\n                                                          np.std(results_nb['test_acc'])))\n\nprint(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_f1_micro']),\n                                                          np.std(results_nb['test_f1_micro'])))\n\nprint(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_nb['test_neg_log_loss']),\n                                                          np.std(-1*results_nb['test_neg_log_loss'])))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"15254d05c861149f3332e27dd6f144a5126f0496","_cell_guid":"01878ba2-0403-4473-ba21-cb528e2c4030"},"cell_type":"code","outputs":[]},{"source":"### Logistic Regression","metadata":{"_uuid":"057e6abfea2ac10f2883c80a59e2ba8e1211c9bb","_cell_guid":"f32ed58c-9484-47f3-bc21-65971542c4e1"},"cell_type":"markdown"},{"source":"np.random.seed(1)\n\ntfidf2 = CountVectorizer(ngram_range=(1, 1), stop_words='english',\n                                                 lowercase = True, max_features = 5000)\n\nmodel_lr = Pipeline([('tfidf1', tfidf2), ('lr', LogisticRegression(class_weight=\"balanced\", C=0.005))])\n\nresults_lr = cross_validate(model_lr, train['clean_posts'], train['type'], cv=kfolds, \n                          scoring=scoring, n_jobs=-1)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"0bb63efb6763d61b3cc7d3107b8ee88f0c2cd0ce","_cell_guid":"89638742-3b92-4246-b02f-f1daa3ea537b"},"cell_type":"code","outputs":[]},{"source":"print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_acc']),\n                                                          np.std(results_lr['test_acc'])))\n\nprint(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_f1_micro']),\n                                                          np.std(results_lr['test_f1_micro'])))\n\nprint(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_lr['test_neg_log_loss']),\n                                                          np.std(-1*results_lr['test_neg_log_loss'])))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"b14744889e1d4d14594f27c57e3a4c33d47b445d","_cell_guid":"12cc90b3-5132-4e30-9fd0-498252525237"},"cell_type":"code","outputs":[]},{"source":"Is this model overtitting? could we have a better model with more data? Let's see the learning curve:","metadata":{"collapsed":true,"_uuid":"92ea4a3bc6b3d40576a63111e76b19e19082f959","_cell_guid":"e54e11e9-b7bf-4c2b-9488-f196b89d71ed"},"cell_type":"markdown"},{"source":"train_sizes, train_scores, test_scores = \\\n    learning_curve(model_lr, train['clean_posts'], train['type'], cv=kfolds, n_jobs=-1, \n                   scoring=\"f1_micro\", train_sizes=np.linspace(.1, 1.0, 10), random_state=1)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"29b7985e4ded72f0a88df4f783d995c5a4b4d4ed","_cell_guid":"37ba98a5-7653-4014-8798-94a22a1cf3cd"},"cell_type":"code","outputs":[]},{"source":"def plot_learning_curve(X, y, train_sizes, train_scores, test_scores, title='', ylim=None, figsize=(14,8)):\n\n    plt.figure(figsize=figsize)\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"metadata":{"collapsed":true,"_uuid":"d475b6a10c2b6adafc56bceeddd125ff22fc89af","_cell_guid":"30f1955b-db37-44c8-b98d-f5d4aeca36b3"},"cell_type":"code","outputs":[]},{"source":"import matplotlib.pyplot as plt\n\nplot_learning_curve(train['posts'], train['type'], train_sizes, \n                    train_scores, test_scores, ylim=(0.1, 1.01), figsize=(14,6))\nplt.show()","execution_count":null,"metadata":{"collapsed":true,"_uuid":"2171c7e33f3dbb68fa80afd6376c606e78c8ded8","_cell_guid":"6c0769c1-1292-432e-8fd2-386afb28e353"},"cell_type":"code","outputs":[]},{"source":"It looks like that with more data the model gets better and that it is not overfitting. ","metadata":{"_uuid":"713f37af2cbab4b4d48822f1fb25c938ea4d4d85","_cell_guid":"8f650510-c0d8-45c1-9add-72f562b5ba74"},"cell_type":"markdown"},{"source":"## Kaggle users personality\nLet's apply our last model to whole kaggle users comments. Let's see what is the most common kaggle user personalities","metadata":{"_uuid":"6f28e921316e6b43bb437f8f666e14cff3f44ac4","_cell_guid":"d3da668d-f47f-4119-9cc9-7c2d2569c7b5"},"cell_type":"markdown"},{"source":"ps_join['clean_comments'] = ps_join['Message'].apply(cleanText)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"969492e6e25f62e0c56b4d32faa8b78e889ade00","_cell_guid":"30c525f0-d92f-4789-8a3d-c502379c3e80"},"cell_type":"code","outputs":[]},{"source":"model_lr.fit(train['clean_posts'], train['type'])\npred_all = model_lr.predict(ps_join['clean_comments'])","execution_count":null,"metadata":{"collapsed":true,"_uuid":"4f8580366f7367f92b23a774ada6ab796c61a169","_cell_guid":"54bd1804-1d39-4917-81db-eebdbee4c198"},"cell_type":"code","outputs":[]},{"source":"cnt_all = np.unique(pred_all, return_counts=True)\n\npred_df = pd.DataFrame({'personality': cnt_all[0], 'count': cnt_all[1]},\n                      columns=['personality', 'count'], index=None)\n\npred_df.sort_values('count', ascending=False, inplace=True)\n\nplt.figure(figsize=(12,6))\nsns.barplot(pred_df['personality'], pred_df['count'], alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Personality', fontsize=12)\nplt.show()","execution_count":null,"metadata":{"collapsed":true,"_uuid":"b6bc9d7007dc45196b418227755b6abe092418ac","_cell_guid":"d3c3cc67-f9a1-401c-b1ce-01109c78ce6a"},"cell_type":"code","outputs":[]},{"source":"pred_df['percent'] = pred_df['count']/pred_df['count'].sum()","execution_count":null,"metadata":{"collapsed":true,"_uuid":"429daf8d9d16ac54971cdcff1d2bfed1cd6f488d","_cell_guid":"50de1627-13bd-48b4-8e4c-8e0e4edf7f4c"},"cell_type":"code","outputs":[]},{"source":"pred_df['description'] = pred_df['personality'].apply(lambda x: ' '.join([mbti[l] for l in list(x)]))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"3343c05412f2c53e73aca7e1aeb3e834f282d2d4","_cell_guid":"955de0ce-5ae5-4398-bb5c-43e8ee507b96"},"cell_type":"code","outputs":[]},{"source":"pred_df","execution_count":null,"metadata":{"collapsed":true,"_uuid":"1f7297123451d95c0ff43ba75100bc8da2d2c4e7","_cell_guid":"b987ed57-b032-4df5-a08e-b8524fa5c6cb"},"cell_type":"code","outputs":[]},{"source":"labels = pred_df['description']\nsizes = pred_df['percent']*100\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Kaggle Personality Distribution'\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"_uuid":"06f6ffb586a079e0c98584bca53a0c53266f6383","_cell_guid":"827373d4-d0d7-46cd-b8bd-07ee45f59afe","collapsed":true},"cell_type":"code","outputs":[]},{"source":"## Conclusions\nApparently the most common Kaggle users personality is ESFP (Extroversion Sensing Feeling Perceiving), but we are getting this conclusion based on users comments: it is reasonable to think that users who participate more writting comments are more extrovert. Our sample data don't came from all Kaggle user population, it comes from Kaggle users who write comments so, our conclusion can't be applied to all Kaggle users, only to those who write comments. Farthermore, more accuracy models could be obtained with more data.\n\n","metadata":{"_uuid":"744341e313d4b5f5e4b9f7a194523028079f113b","_cell_guid":"9acc2f66-3e27-4c7d-9838-3832dafba930"},"cell_type":"markdown"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3","name":"python"}},"nbformat":4}