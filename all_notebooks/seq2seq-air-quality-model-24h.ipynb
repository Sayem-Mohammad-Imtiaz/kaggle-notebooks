{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras import optimizers\nimport tensorflow as tf\n\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\nseed = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploración de datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install xlrd==1.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/air-quality-time-series-data-uci/AirQualityUCI.xlsx')\ndf = df.rename(columns={'Date': 'date', \n                        'Time': 'time',\n                        'CO(GT)': 'co_gt',\n                        'NMHC(GT)': 'nmhc_gt',\n                        'C6H6(GT)': 'c6h6_gt',\n                        'NOx(GT)': 'nox_gt',\n                        'NO2(GT)': 'no2_gt',\n                        'PT08.S1(CO)': 'co_pt',\n                        'PT08.S2(NMHC)': 'nmhc_pt',\n                        'PT08.S3(NOx)': 'nox_pt',\n                        'PT08.S4(NO2)': 'no2_pt',\n                        'PT08.S5(O3)': 'o3_pt',\n                        'T': 't',\n                        'RH': 'rh',\n                        'AH': 'ah',\n                       })\ndf['full_timestamp'] = (df['date'].astype(str) + ' ' + df['time'].astype(str)).map(pd.Timestamp) \ndf = df.sort_values('full_timestamp', ascending=True).reset_index(drop=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una de las primeras dudas que se pueden plantear es si, efectivamente, hay una medición por hora y si ninguna de esas mediciones tiene valores nulos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('¿La diferencia de tiempo entre dos mediciones es siempre de una hora?:', \n      (df.full_timestamp.diff().iloc[1:] == pd.Timedelta(hours=1)).all())\nprint('¿Hay valores nulos?', pd.isna(df).any().any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que no hay valores nan. Sin embargo, en la descripción del dataset aparece indicado -200 como valor por defecto para los valores faltantes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fijamos como índice la fecha de cada muestra.\ndf = df.set_index('full_timestamp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vamos a ver la proporción de mediciones faltantes de cada variable\ndisplay((df.drop(columns=['date', 'time']) == -200).mean())\n# y la proporción de horas en las que falta al menos una medición\nprint('Proporción de horas en las que falta al menos una medición:',\n      (df.drop(columns=['date', 'time']) == -200).any(axis=1).mean())\nprint('Proporción de horas en las que faltan todas las mediciones:',\n      (df.drop(columns=['date', 'time']) == -200).all(axis=1).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Veamos ahora la proporción de horas en las que falta alguna medición distinta de nmhc_gt, \n# ya que es la que menos aparece:\nprint('Proporción de horas en las que falta al menos una medición (no nmhc_gt):',\n      (df.drop(columns=['date', 'time', 'nmhc_gt']) == -200).any(axis=1).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Para facilitar el filtrado de los valores faltantes vamos a sustituirlos por nan.\ndf = df.replace(to_replace=-200, value=np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a ver si existe algún patrón claro en los valores que faltan para ver si es posible rellenarlos de alguna forma."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in df.columns.drop(['date', 'time']):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    df[df.notna()][col].plot(linewidth=1, label=col)\n    plt.scatter(df[df.isna()].index, np.ones(df[df.isna()].shape[0])*(-200), c='r', s=0.5, label='NaN')\n    plt.title('Evolución temporal de ({}) y valores NaN'.format(col))\n    plt.ylabel(col)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parece que la densidad de valores NaN es muy alta en todas las variables (no se aprecia ninguna discontinuidad en la línea roja). Veamos si se aprecia algo por horas y por meses."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in df.columns.drop(['date', 'time']):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    plt.plot(range(24), \n             [df[df[col].isna() & df.index.map(lambda x: x.hour == h)].shape[0] for h in range(24)])\n    plt.title('Valores NaN por hora del día ({})'.format(col))\n    plt.xticks(ticks=range(24))\n    plt.ylabel(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in df.columns.drop(['date', 'time']):\n    fig, ax = plt.subplots(figsize=(10, 10))\n    plt.plot(range(12), \n             [df[df[col].isna() & df.index.map(lambda x: x.month == m)].shape[0] for m in range(12)])\n    plt.title('Valores NaN por mes ({})'.format(col))\n    plt.xticks(ticks=range(12))\n    plt.ylabel(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parece haber algún patrón similar en varias variables, pero no parece ser algo claro que nos pueda ayudar a rellenar los datos. La pregunta que habría que hacerse ahora es cómo de grandes son los lapsos temporales en los que no hay ningún valor, para ver si tiene sentido utilizar los valores previos de una variable para rellenar los posteriores.\n\nVamos a calcular las longitudes de las secuencias de NaN que se encuentran seguidas en los datos."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in df.columns.drop(['date', 'time']):\n    s = df.isna().cumsum().diff()[col] \n    mask = s.ne(s.shift())\n\n    ids = s[mask].to_numpy()\n    counts = s.groupby(mask.cumsum()).cumcount().add(1).groupby(mask.cumsum()).max().to_numpy()\n\n    ids[ids == 'nan'] = np.nan\n    ser_out = pd.Series(counts, index=ids, name='counts')\n    ser_out = ser_out[ser_out.index==1]\n    \n    fig, ax = plt.subplots()\n    plt.hist(ser_out)\n    plt.title('Histograma de valores NaN seguidos ({})'.format(col))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dado que hay muchos casos en los que la secuencia de NaNs es demasiado larga como para asignar a un tiempo t el primer valor anterior a ese tiempo t en el que se tenía la medición, por lo que vamos a probar a rellenar valores de ciertas variables en base a otras que compartan una correlación fuerte.\n\nVamos a ver la tabla de correlaciones:"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = df.drop(columns=['date', 'time']).corr()\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parece que va a ser posible hacer la sustitución por este método. Para evitar meter una cantidad excesivo de ruido a los datos, vamos a eliminar las variables t, rh y ah, ya que guardan bastante menos correlación con las demás. \n\nPara cada variable, vamos a quedarnos en cada caso con las otras dos variables con una correlación más fuerte, para usarlas para rellenar datos."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = df.drop(columns=['t', 'ah', 'rh'])\ncorrelations = df.corr()\ncorrelations_map = {col: correlations[col].sort_values()[1:].index.tolist() for col in correlations.columns}\ncorrelations_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La forma de rellenar datos va a ser:\n- Para cada medición en un momento t de una variable v que sea NaN tomaremos, de entre los datos anteriores temporalmente a t, las dos variables disponibles (en ese momento t) con más correlación, v1 y v2, con v.\n- Tomaremos las tres mediciones más cercanas a los valores v1 y v2.\n- El valor asignado a la variable v será la media de esas tres mediciones."},{"metadata":{"trusted":true},"cell_type":"code","source":"def manhattan_distance(p1, p2, q1, q2):\n    \"\"\"\n    Distancia manhattan entre dos puntos con dos dimensiones cada uno.\n    \"\"\"\n    return np.abs(p1-q1) + np.abs(p2-q2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_to_fill va a ser el dataframe que vayamos rellenando, para no utilizar como inputs \n# del algoritmo de generación valores creados.\ndf_to_fill = df.copy()\n# Creamos una copia normalizada para obtener una medida de la distancia normalizada.\ndf_normalized = df.drop(columns=['time', 'date']).copy()\nmax_norm, min_norm = df_normalized.max(), df_normalized.min()\ndf_normalized = (df_normalized - min_norm) / (max_norm - min_norm)\n\nfor index, row in df_normalized.iterrows():\n    for col in df_normalized.columns:\n        if pd.isna(row[col]):\n            filling_vars = [v for v in correlations_map[col] if pd.notna(row[v])][:2]\n            if len(filling_vars) == 2:\n                p = row[filling_vars[0]], row[filling_vars[1]]\n                aux_df = df_normalized.loc[:index][:-1]\n                aux_df = aux_df.loc[pd.notna(aux_df[col])][filling_vars + [col]]\n                aux_df['distance'] = aux_df.apply(lambda row: manhattan_distance(row[filling_vars[0]], \n                                                                                 row[filling_vars[1]], \n                                                                                 p[0], p[1]), \n                                                  axis=1)\n                new_value = aux_df.nsmallest(3, 'distance')[col].mean()\n                df_to_fill.at[index, col] = new_value * (max_norm[col] - min_norm[col]) + min_norm[col]\n            else:\n                # En caso de no existir ningún valor a partir del cual rellenar, tomaremos el inmediato más cercano.\n                df_to_fill.at[index, col] = df_to_fill.loc[:index].iloc[-2][col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df_to_fill.to_csv('df_to_fill.csv', index=False)\n# df_to_fill = pd.read_csv('df_to_fill.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelado"},{"metadata":{},"cell_type":"markdown","source":"Nuestro objetivo va a ser predecir las variables de ground truth (no2_gt, nox_gt, nmhc_gt, co_gt, c6h6_gt) de las próximas 24 horas en base al histórico previo (vamos a limitarnos a las 48 horas anteriores)."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Longitud de la serie previa a la predicción y de la serie posterior.\nprevious_series_length = 48\nposterior_series_length = 24\n\n# Hacemos un split temporal dejando el 60% para train, un 20% para validación y otro 20% para test.\nnum_samples = df_to_fill.shape[0]\ntrain_df = df_to_fill.iloc[:int(num_samples*0.6)]\ntrain_df['hour'] = train_df.index.map(lambda x: x.hour)\ntrain_df = train_df.reset_index(drop=True)\nval_df = df_to_fill.iloc[int(num_samples*0.6):int(num_samples*0.8)]\nval_df['hour'] = val_df.index.map(lambda x: x.hour)\nval_df = val_df.reset_index(drop=True)\ntest_df = df_to_fill.iloc[int(num_samples*0.8):]\ntest_df['hour'] = test_df.index.map(lambda x: x.hour)\ntest_df = test_df.reset_index(drop=True)\n\n# Definimos las columnas de variables predictoras y de targets\nfeature_columns = ['hour', 'co_pt', 'nmhc_pt', 'nox_pt', 'no2_pt', 'o3_pt']\ntarget_columns = ['no2_gt', 'nox_gt', 'nmhc_gt', 'co_gt', 'c6h6_gt']\n\nX_train = train_df[feature_columns + target_columns].values\ny_train = train_df[target_columns].values\nX_val = val_df[feature_columns + target_columns].values\ny_val = val_df[target_columns].values\nX_test = test_df[feature_columns + target_columns].values\ny_test = test_df[target_columns].values\n\n# Escalamos los datos y guardamos el normalizador para usarlo en las futuras predicciones.\ndata_scaler = StandardScaler()\ntarget_scaler = StandardScaler()\n\nX_train = data_scaler.fit_transform(X_train)\nX_val = data_scaler.transform(X_val)\nX_test = data_scaler.transform(X_test)\n\ny_train = target_scaler.fit_transform(y_train)\ny_val = target_scaler.transform(y_val)\ny_test = target_scaler.transform(y_test)\n\nwith open('data_scaler.pkl', 'wb') as f:\n    pickle.dump(data_scaler, f)\nwith open('target_scaler.pkl', 'wb') as f:\n    pickle.dump(target_scaler, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_timeseries(arr, length=48):\n    \"\"\"\n    Devuelve series temporales de un array ordenado temporalmente.\n    \"\"\"\n    shape = arr.shape\n    ts_arr = np.zeros((shape[0]-length+1, length, shape[1]))\n    for n in range(shape[0]-length+1):\n        ts_arr[n] = arr[n:length+n]\n    return ts_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = build_timeseries(X_train[:-posterior_series_length], length=previous_series_length)\nX_val = build_timeseries(X_val[:-posterior_series_length], length=previous_series_length)\nX_test = build_timeseries(X_test[:-posterior_series_length], length=previous_series_length)\n\ny_train = build_timeseries(y_train[previous_series_length:], length=posterior_series_length)\ny_val = build_timeseries(y_val[previous_series_length:], length=posterior_series_length)\ny_test = build_timeseries(y_test[previous_series_length:], length=posterior_series_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, y_train = shuffle(X_train, y_train, random_state=seed)\nX_val, y_val = shuffle(X_val, y_val, random_state=seed)\nX_test, y_test = shuffle(X_test, y_test, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para esta tarea vamos a construir un modelo Seq2Seq basado en redes neuronales de tipo LSTM, especializadas en procesar datos temporales. Esta arquitectura nos va a permitir hacer predicciones de un paso temporal basándonos en las de los pasos previos que ya hemos ido prediciendo."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def train_seq2seq_hps(latent_dim, optimizer, dropout, recurrent_dropout, lr, epochs=1000, batch_size=64):\n    tf.random.set_seed(seed)\n    \n    encoder_inputs = Input(shape=(None, 11))\n    encoder = LSTM(latent_dim, dropout=dropout, recurrent_dropout=recurrent_dropout, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n    encoder_states = [state_h, state_c]\n\n    decoder_inputs = Input(shape=(None, 5))\n    decoder_lstm = LSTM(latent_dim, dropout=dropout, recurrent_dropout=recurrent_dropout, \n                        return_sequences=True, return_state=True)\n    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                         initial_state=encoder_states)\n    decoder_dense = Dense(5)\n    decoder_outputs = decoder_dense(decoder_outputs)\n\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n    early_stopping = EarlyStopping(patience=5, min_delta=0.001, restore_best_weights=True)\n    opt = optimizer(lr=lr)\n    model.compile(optimizer=opt, loss='mse')\n    model.fit([X_train, y_train[:,:-1]], y_train[:,1:],\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=([X_val, y_val[:,:-1]], y_val[:,1:]),\n              callbacks=[early_stopping])\n\n    encoder_model = Model(encoder_inputs, encoder_states)\n\n    decoder_state_input_h = Input(shape=(latent_dim,))\n    decoder_state_input_c = Input(shape=(latent_dim,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    decoder_outputs, state_h, state_c = decoder_lstm(\n        decoder_inputs, initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs] + decoder_states)\n    \n    encoder_model.save('encoder_{}_{}_{}_{}_{}.h5'.format(latent_dim, \n                                                          opt.__class__.__name__,\n                                                          int(dropout*10),\n                                                          int(recurrent_dropout*10),\n                                                          int(lr*100)))\n    decoder_model.save('decoder_{}_{}_{}_{}_{}.h5'.format(latent_dim, \n                                                          opt.__class__.__name__,\n                                                          int(dropout*10),\n                                                          int(recurrent_dropout*10),\n                                                          int(lr*100)))\n    \n    best_epoch = np.argmin(model.history.history['val_loss'])\n    dic = {metric: model.history.history[metric][best_epoch] for metric in ['loss', 'val_loss']}\n    dic['latent_dim'] = latent_dim\n    dic['optimizer'] = opt.__class__.__name__\n    dic['dropout'] = dropout\n    dic['recurrent_dropout'] = recurrent_dropout\n    dic['lr'] = lr\n    \n    return dic\n\n\nhps_list = [{'latent_dim': latent_dim, \n             'optimizer': optimizer, 'dropout': dropout, 'recurrent_dropout': recurrent_dropout, 'lr': lr} \n            for latent_dim in [10, 50, 100]\n            for optimizer in [optimizers.Adam, optimizers.RMSprop, optimizers.Adadelta]\n            for dropout in [0.0, 0.3]\n            for recurrent_dropout in [0.0, 0.3]\n            for lr in [0.01, 0.1]]\n\nhps_list = shuffle(hps_list)\nresults = []\nfor hps in hps_list:\n    print(hps)\n    results.append(train_seq2seq_hps(**hps))\n    df_results = pd.DataFrame(results)\n    df_results.to_csv('results.csv', index=False)\n    \ndef decode_sequence(inputs, encoder_model, decoder_model):\n    \"\"\"\n    Recibe un modelo encoder y uno decoder y genera toda la serie temporal a partir de los pasos previos.\n    \"\"\"\n    states_value = encoder_model.predict(inputs)\n    target_seq = inputs[:,-1:,-5:]\n    decoded_sentence = np.zeros((1,posterior_series_length,5))\n    \n    for i in range(posterior_series_length):\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n        decoded_sentence[0,i] = output_tokens[0,0] \n        target_seq = output_tokens\n        states_value = [h, c]\n\n    return decoded_sentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Cargamos el dataframe de resultados.\ndf_results = pd.read_csv('results.csv')\n\ndf_results['val_loss'] = df_results.apply(lambda row: row['val_loss'][row['best_epoch']], axis=1)\ndf_results['loss'] = df_results.apply(lambda row: row['loss'][row['best_epoch']], axis=1)\n\ndf_results = df_results.sort_values('val_loss', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Cargamos el decoder y el encoder del mejor modelo:\nbest_comb = df_results.iloc[0]\nencoder_model = load_model('encoder_{}_{}_{}_{}_{}.h5'.format(best_comb.latent_dim,\n                                                             best_comb.optimizer,\n                                                             int(best_comb.dropout*10),\n                                                             int(best_comb.recurrent_dropout*10),\n                                                             int(best_comb.lr*100)))\ndecoder_model = load_model('decoder_{}_{}_{}_{}_{}.h5'.format(best_comb.latent_dim,\n                                                             best_comb.optimizer,\n                                                             int(best_comb.dropout*10),\n                                                             int(best_comb.recurrent_dropout*10),\n                                                             int(best_comb.lr*100)))\nencoder_model.save('best_model_encoder.h5')\ndecoder_model.save('best_model_decoder.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a visualizar algunos casos concretos de test para terminar. Lo ideal sería hacerlo en un tramo temporal en el que los datos estén completos. Sin embargo, no es posible porque la variable nmhc_gt no existe en todo el conjunto de test.  Así que vamo a hacer las predicciones directamente en el dataset con los datos rellenados. \n\nOtra posibilidad hubiera sido eliminarla por completo del dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=5, ncols=5, figsize=(25,25))\nfor n in range(5):\n    previous_ts = data_scaler.inverse_transform(X_test[n])[:,6:]\n    future_ts = target_scaler.inverse_transform(y_test[n])\n    total_ts = np.concatenate([previous_ts, future_ts])\n    pred = target_scaler.inverse_transform(decode_sequence(X_test[n:n+1], encoder_model, decoder_model))[0]\n    for i in range(5):\n        ax[n, i].plot(range(72), total_ts[:,i], label=['Ground Truth'])\n        ax[n, i].vlines(48, min(total_ts[:,i].min(), pred[:,i].min()), max(total_ts[:,i].max(), pred[:,i].max()))\n        ax[n, i].plot(range(48, 72), pred[:,i], color='r', label=['Predicted'])\n        ax[n, i].set_title(target_columns[i])\n        ax[n, i].set_ylabel(target_columns[i])\n        ax[n, i].set_xlabel('timestep')\n        ax[n, i].set_xticks(range(0, 72, 12))\n        \nhandles, labels = ax[0,0].get_legend_handles_labels()\nfig.legend(handles, list(map(lambda x: x[2:-2], labels)), loc='upper center', prop={'size': 20})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusión"},{"metadata":{},"cell_type":"markdown","source":"El resultado parece ser bastante bueno a simple vista, además de coherente, pues todas las variables evolucionan de forma similar. Faltaría hacer el cálculo del error por cada timestep, que probablemente vaya incrementándose ligeramente a lo largo de la serie predicha.\n\nDebido a falta de tiempo, no se ha podido realizar una búsqueda de hiperparámetros más exhaustiva, lo que hubiera dado unos resultados más precisos.\n\nTambién quedaría pendiente comparar el rendimiento del seq2seq con otros modelos, así como probar otros métodos de relleno de valores NaN."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Vamos a generar un df de prueba para lanzar el script.\ndf = pd.read_excel('AirQualityUCI.xlsx')\ntest_df = df.fillna(0)[:48]\ntest_df.to_csv('test_df.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}