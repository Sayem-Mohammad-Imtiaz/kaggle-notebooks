{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Text Modeling techniques:\nIn this notebook, I am going to use spacy, nltk and other libraries to explore different basic NLP modeling techniques. We will be training a bunch of small text classification methods. This is a good resource notebook for those who are starting with NLP and want to explore the techniques and simple methodologies."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport nltk\nimport re\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"text_data = pd.read_csv('/kaggle/input/dataisbeautiful/r_dataisbeautiful_posts.csv')\nprint(\"the data shape is:\",text_data.shape)\ntext_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(text_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic information:\nThe data shape is 1,93,091,i.e. 193k rows are there. This is moderately big dataset.<br/>\nSo the reddit data has the following columns:<br/>\n(1) **id**: this represents a unique id for each post.<br/>\n(2) **title**: each reddit post contains a title, this is the title text.<br/>\n(3) **score**: each reddit post can be upvoted or downvoted. And thereby receives a score. This is that score.<br/>\n(4) **author**: this is basically user name.<br/>\n(5) **author_flair_text**: Need to know exactly what does it represent. We will inspect the data first and check in other notebooks too.<br/>\n(6) **removed_by**: this is removed by what option. i.e. if the post is eventually removed, who removed it. This is a very interesting source of data.<br/>\n(7) **created_utc**: when was this post created in utc timing but it is in [unix epoch](https://www.utctime.net/) format. We need to transform it into normal date time to work on it. <br/>\n(8) **full_link**: what is the full_link for the reddit post. This url will contain reddit's domain, subreddit and other informations. Need to parse the link to create subreddit and other different informations.<br/>\n(9)**num_comments**: this is total number of comments which were seen in the post.<br/>\n(10) **over_18**: this is basically the NSFW tag in reddit. Denotes whether the post contains something adult or not.<br/>"},{"metadata":{},"cell_type":"markdown","source":"## First action: Build a NSFW classifier\nIn this section we are going to create different features and going to build a NSFW classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"re.sub('\\[oc\\]','','[oc]granny')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = text_data.drop(['id','author','author_flair_text',\n                       'created_utc','awarders'],axis = 1)\ntext['over_18'] = text['over_18']*1\ndef replacer(x):\n    return re.sub('\\[OC\\]','',x)\ndef replacer_fulllink(x):\n    path_reduced = re.sub('https://www.reddit.com/r/','',x)\n    path_reduced_list = path_reduced.split(\"/\")\n    return path_reduced_list[0]\ntext['title'] = text['title'].apply(lambda x: replacer(str(x)))\ntext['removed_by'] = text['removed_by'].fillna(\"\")\ntext['subreddit'] = text['full_link'].apply(lambda x: replacer_fulllink(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text['subreddit'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text['over_18'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom scipy.stats import norm\nsns.distplot(text['score'].tolist(),fit = norm, kde = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text['log_score'] = text['score'].apply(lambda x: np.log(x+1)/np.log(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(text['log_score'].tolist(),fit = norm, kde = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"value less than 3 is\",text[text['log_score']<=3].shape)\nprint(\"value more than 3 is\",text[text['log_score']>3].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is extremely imbalanced. So we will have to keep that in mind."},{"metadata":{"trusted":true},"cell_type":"code","source":"text['removed_by'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for elem in ['moderator','deleted','automod_filtered',\n             'reddit','author']:\n    text['removed_by_'+elem] = text['removed_by'].apply(lambda x: (x==elem)*1.0)\ntext['Not_removed'] = text['removed_by'].apply(lambda x: (x=='')*1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = text.drop(['subreddit','full_link','removed_by'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bag of word creation:\nFor NSFW posts, let's create a bag of words from the nsfw posts which are never present in non-nsfw posts. Then we will create features out of this."},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_cleaning(text):\n    forbidden_words = set(stopwords.words('english'))\n    if text:\n        text = ' '.join(text.split('.'))\n        text = re.sub('\\/',' ',text)\n        text = re.sub(r'\\\\',' ',text)\n        text = re.sub(r'((http)\\S+)','',text)\n        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', text.strip().lower())).strip()\n        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n        text = [word for word in text.split() if word not in forbidden_words]\n        return text\n    return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.sub(r'\\\\',' ','aof\\god')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text['title'] = text['title'].apply(lambda x: ' '.join(text_cleaning(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nsfw_text = ''\nsfw_text = ''\nfor elem in text['title'][text['over_18']==1].tolist():\n    nsfw_text = nsfw_text+elem\nfor elem in text['title'][text['over_18']==0].tolist():\n    sfw_text = sfw_text+elem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_top_words(text,words = 10):\n    allWords = nltk.tokenize.word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')\n    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in allWords if w not in stopwords)    \n    mostCommontuples= allWordExceptStopDist.most_common(words)\n    mostCommon = [tupl[0] for tupl in mostCommontuples]\n    return mostCommon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_200_nsfw_words = return_top_words(nsfw_text,400)\ntop_200_sfw_words = return_top_words(sfw_text,400)\ntop_nsfw_exclusive = list(set(top_200_nsfw_words).difference(set(top_200_sfw_words)))\ntop_sfw_exclusive = list(set(top_200_sfw_words).difference(set(top_200_nsfw_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_vocab = top_nsfw_exclusive + top_sfw_exclusive\nfor word in total_vocab:\n    text['Is_'+word+'_in_title'] = text['title'].apply(lambda x: (word in x)*1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = text.drop('title',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import classification_report\nY = text['over_18']\nX = text.drop('over_18',axis = 1)\nX_train,X_val,Y_train,Y_val = tts(X,Y,test_size = 0.2,stratify = Y,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.fillna(0)\nX_val = X_val.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFC\nforest = RFC(n_estimators = 128,max_depth = 22,class_weight = {0:1,1:192},\n            n_jobs = -1,random_state = 42)\nforest.fit(X_train,Y_train)\npred_train = forest.predict(X_train)\npred_val = forest.predict(X_val)\nprint(classification_report(Y_train,pred_train))\nprint(classification_report(Y_val,pred_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So clearly, around 33% F1-score can be reached with a random forest for nsfw detection. We could use a LSTM model for this too, but as clearly the problem is not semantical so LSTM is not a suitable thing here according to what I think. We will now move on to the next problem."},{"metadata":{},"cell_type":"markdown","source":"## Score prediction from this dataset\nAs we already saw, the score is a very highly left skewed number; clearly a poisson variable with inflation around 0. We will try to predict the scores, or atleast 0 and non-zero from title, award number and other variables in the dataset."},{"metadata":{},"cell_type":"markdown","source":"First things first, let's drop the useless columns. But wait, actually none of the features are not that insignificant. An author's name can have some implicit effect on scores. Also, link length, number of comments, total_awards_recieved, awarder list, whether it was removed or not, whether its NSFW or not every things will matter.<br/>\nSo we are dropping none other than id."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data.drop('id',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"need to change the created_utc to see if there is any effect of time in score. This is unix epoch time, so will try something to decode it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nts = int(\"1284101485\")\n\n# if you encounter a \"year is out of range\" error the timestamp\n# may be in milliseconds, try `ts /= 1000` in that case\nprint(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_unix_change(x):\n    x = int(x)\n    return datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d  %H:%M:%S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['time'] = text_data['created_utc'].apply(lambda x: time_unix_change(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data.time.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data.drop(['author','full_link','created_utc'],axis = 1)\ntext_data['removed_by'] = text_data['removed_by'].fillna('')\ntext['over_18'] = text['over_18']*1\ndef replacer(x):\n    return re.sub('\\[OC\\]','',x)\ntext_data['title'] = text_data['title'].apply(lambda x: replacer(str(x)))\ntext_data['num_title'] = text_data['title'].apply(lambda x: len(x))\ntext_data['over_18'] = text_data['over_18']*1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['hr'] = text_data['time'].apply(lambda x: x[12:14])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['author_flair_text'] = text_data['author_flair_text'].fillna('')\nprint(text_data['author_flair_text'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flair_cleaner(x):\n    x =  re.sub('\\[OC\\]','',x)\n    x = re.sub('OC','',x)\n    x = re.sub('|','',x)\n    x = re.sub('oc','',x)\n    x = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', x.strip().lower())).strip()\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['author_flair_text'] = text_data['author_flair_text'].apply(lambda x: flair_cleaner(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data.drop(['awarders','time'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data[~text_data['score'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['removed_by'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['Not_removed'] = text_data['removed_by'].apply(lambda x: (x == '')*1.0)\nfor elem in ['automod_filtered', 'moderator', 'reddit', 'deleted', 'author']:\n    text_data['removed_by_'+elem] = text_data['removed_by'].apply(lambda x: (x == elem)*1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data.drop('removed_by',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['author_flair_text'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"words like researcher, statistics, practitioner, nasa, institute, economics, prof gives out a sense of trust because the sub reddit is data related. So let's create a feature for that."},{"metadata":{"trusted":true},"cell_type":"code","source":"def expertise(x):\n    count = 0\n    for el in ['researcher','statistics','practitioner','nasa','economics','prof']:\n        if el in x: count = count + 1\n    return count\ntext_data['author_flair_text'] = text_data['author_flair_text'].apply(lambda x: expertise(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data.drop('title',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time is a cyclic feature, so let's transform it."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['hour_sin'] = text_data['hr'].apply(lambda x : np.sin(2 * np.pi * float(x)/23.0))\ntext_data['hour_cos'] = text_data['hr'].apply(lambda x : np.cos(2 * np.pi * float(x)/23.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data.rename(columns = {'author_flair_text':'expertise_count_author'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data = text_data.drop('hr',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(text_data['score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data[text_data['score']<100].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like that 1 is a very dominant score; as that is the first score which gets assigned to most cases; and therefore we will first train a model for 1 vs not 1. "},{"metadata":{"trusted":true},"cell_type":"code","source":"text_data['score_class'] = text_data['score'].apply(lambda x: (x == 1)*1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = text_data['score'].tolist()\ntext_data = text_data.drop('score',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = text_data['score_class']\nX = text_data.drop('score_class',axis = 1)\nX_train,X_val,Y_train,Y_val = tts(X,Y,test_size = 0.2,stratify = Y,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.fillna(0)\nX_val = X_val.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RFC(n_estimators = 128,max_depth = 22,class_weight = {0:1.33,1:1},\n            n_jobs = -1,random_state = 42)\nforest.fit(X_train,Y_train)\npred_train = forest.predict(X_train)\npred_val = forest.predict(X_val)\nprint(classification_report(Y_train,pred_train))\nprint(classification_report(Y_val,pred_val))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}