{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mushroom Classification"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I will make use of the Mushroom Classification dataset to try to predict if a Mushroom is poisonous or not by looking at the given features. I will successivelly try differerent feature elimination techniques to see how this can affect training times and overall model accuracy."},{"metadata":{},"cell_type":"markdown","source":"Reducing the number of features in a dataset, can lead to:\n- Accuracy improvements\n- Overfitting risk reduction\n- Speed up in training\n- Improved Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv')\npd.options.display.max_columns = None\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = df.isnull().sum() * 100 / len(df)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"ticks\")\nf = sns.countplot(x=\"class\", data=df, palette=\"bwr\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now divide our dataset in features (X) and labels (Y). We then transoform all our dataset categorical features in numeric features using One Hot Encoding. <br> <br>\nA new column gets created for all the diffent cases in a categorical feature. <br> <br>\nFor example, the **bruises** feature contains two categorical cases **f** and **t**. This categorical feature will be splitted in two numeric features, one having 1s in all the rows for which the mushroom which had bruises f (**bruises_f**) and the second one having 1s for all the  rows for which the mushroom had bruises t (**bruises_t**). <br> <br>\nIn the case of our labels (Y), we instead encode them. In our example, we have two different possible outcomes (edible or not). Therefore, we set the color of the first outcome equal to 0 and the second possible outcome equal to 1. And all the informations for this class will be contained in a single array (column). <br> <br>\n\nI decided not to adopt this same approch for the features (X), because some Machine Learning classifier might think that higher numbers are more important than lower ones and therefore would not give the same importance to the all the different categories in the feature (this doesn't instead happen when we encode the labels)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['class'], axis = 1)\nY = df['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X, prefix_sep='_')\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = LabelEncoder().fit_transform(Y)\n#np.set_printoptions(threshold=np.inf)\nY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\nX2 = StandardScaler().fit_transform(X)\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X2, Y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.process_time()\ntrainedmodel = LogisticRegression().fit(X_Train,Y_Train)\nprint(time.process_time() - start)\npredictions =trainedmodel.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.process_time()\ntrainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\nprint(time.process_time() - start)\npredictionsvm = trainedsvm.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.process_time()\ntrainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\nprint(time.process_time() - start)\npredictionstree = trainedtree.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionstree))\nprint(classification_report(Y_Test,predictionstree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\ndata = export_graphviz(trainedtree,out_file=None,feature_names= X.columns,\n                       class_names=['edible', 'poisonous'],  \n                       filled=True, rounded=True,  \n                       max_depth=2,\n                       special_characters=True)\ngraph = graphviz.Source(data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"There are many different methods which can be applied for Feature Selection. Some of the most important ones are: <br>\n1. Filter Method = filtering our dataset and taking only a subset of it containg all the relevant features (eg. correlation matrix using Pearson Correlation)\n2. Wrapper Method = follows the same objective of the FIlter Method but uses a Machine Learning model as it's evaluation criteria (eg. Forward/Backward/Bidirectional/Recursive Feature Elimination). We feed some features to our Machine Learning model, evaluate their performance and then decide if add or remove feature to increase accuracy. As a result, this mothod can be more accurate than filtering, is more computationally expensive.\n3. Embedded Method = like the FIlter Method also the Embedded Method makes use of a Machine Learning model. The difference between the two different methods is that the Embedded Method examines the different training iterations of our ML model and then ranks the importance of each feature based on how much each of the features contributed to the ML model training (eg. LASSO Regularization)."},{"metadata":{},"cell_type":"markdown","source":"#### Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importaqnce of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it's predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model (our confuse it to make a wrong decision!)."},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(20, 22), dpi=80, facecolor='w', edgecolor='k')\n\nfeat_importances = pd.Series(trainedforest.feature_importances_, index= X.columns)\nfeat_importances.nlargest(19).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Reduced = X[['odor_n','odor_f', 'gill-size_n','gill-size_b']]\nX_Reduced = StandardScaler().fit_transform(X_Reduced)\nX_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(X_Reduced, Y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train2,Y_Train2)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_Test2)\nprint(confusion_matrix(Y_Test2,predictionforest))\nprint(classification_report(Y_Test2,predictionforest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recursive Feature Elimination"},{"metadata":{},"cell_type":"markdown","source":"Recursive Feature Elimination (RFE) takes as input the instance of a Machine Learning model and the final desired number of features to use. It then recursively reduces the number of features to use by ranking them using the Machine Learning model accuracy as metrics. Creating a for loop in which the number of input features is our variable, it could then be possible to found out the optimal number of features our model needs by keeping track of the accuracy registred in each loop iteration. Using RFE support_ method, we can then find out the names of the features which have been evaluated as most important (rfe.support_ return a boolean list in which TRUE represent that a feature is considered as important and FALSE represent that a feature is not considered important).  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nmodel = RandomForestClassifier(n_estimators=700)\nrfe = RFE(model, 4)\nstart = time.process_time()\nRFE_X_Train = rfe.fit_transform(X_Train,Y_Train)\nRFE_X_Test = rfe.transform(X_Test)\nrfe = rfe.fit(RFE_X_Train,Y_Train)\nprint(time.process_time() - start)\nprint(\"Overall Accuracy using RFE: \", rfe.score(RFE_X_Test,Y_Test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=700)\nrfe = RFE(model, 4)\nRFE_X_Train = rfe.fit_transform(X_Train,Y_Train)\nmodel.fit(RFE_X_Train,Y_Train) \nprint(\"Number of Features: \", rfe.n_features_)\nprint(\"Selected Features: \")\ncolcheck = pd.Series(rfe.support_,index = list(X.columns))\ncolcheck[colcheck == True].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SelectFromModel: Meta-transformer for selecting features based on importance weights."},{"metadata":{},"cell_type":"markdown","source":"SelectFromModel is another Scikit-learn method which can be used for Feature Selection. This method can be used with all the different types of Scikit-learn models (after fitting) which have a coef_ or feature_importances_ attribute. Compared to RFE, SelectFromModel is a less robust solution. In fact, SelectFromModel just removes less important features based on a calculated threshold (no optimization iteration process involved). <br>\n\nIn order to test SelectFromModel efficacy, I decided to use an ExtraTreesClassifier in this example. ExtraTreesClassifier (Extremely Randomized Trees) is tree based ensamble classifier which can yield less variance compared to Random Forest methods (reducing therefore the risk of overfitting). The main difference between Random Forest and Extremely Randomized Trees is that in Extremely Randomized Trees nodes are sampled without replacement."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel = ExtraTreesClassifier()\nstart = time.process_time()\nmodel = model.fit(X_Train,Y_Train)\nmodel = SelectFromModel(model, prefit=True)\nprint(time.process_time() - start)\nSelected_X = model.transform(X_Train)\nSelected_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(Selected_X, Y_Train)\nprint(time.process_time() - start)\nSelected_X_Test = model.transform(X_Test)\npredictionforest = trainedforest.predict(Selected_X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\nimportances = trainedforest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in trainedforest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(Selected_X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(Selected_X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(Selected_X.shape[1]), indices)\nplt.xlim([-1, Selected_X.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correlation Matrix Analysis"},{"metadata":{},"cell_type":"markdown","source":"Using Seaborn, we can now plot the Pearson correlation heatmap of our dataset. Inspecting this plot, we can then be able to see the correlation of our independent variables (X) with our label (Y). Finally, we can then select just the features which are most correlated with Y and train/test an SVM model to test the results of this approach."},{"metadata":{},"cell_type":"markdown","source":"Using Pearson correlation our returned coefficient values will vary between -1 and 1:\n- If the correlation between two features is 0 this means that changing any of these two features will not affect the other.\n- If the correlation between two features is greater than 0 this means that increating the values in one feature will make increase also the values in the other feature (the closer the correlation coefficient is to 1 and the stronger is going to be this bond between the two different features). \n- If the correlation between two features is less than 0 this means that increating the values in one feature will make decrease the values in the other feature (the closer the correlation coefficient is to -1 and the stronger is going to be this relationship between the two different features). "},{"metadata":{},"cell_type":"markdown","source":" Another possible aspect to control in this analysis would be to check if the selected variables are highly correlated each other. If they are, we would then need to keep just one of the correlated ones and drop the others."},{"metadata":{"trusted":true},"cell_type":"code","source":"Numeric_df = pd.DataFrame(X)\nNumeric_df['Y'] = Y\nNumeric_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n\ncorr= Numeric_df.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)\n\n# Selecting only correlated features\ncorr_y = abs(corr[\"Y\"])\nhighest_corr = corr_y[corr_y >0.5]\nhighest_corr.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Reduced2 = X[['bruises_f' , 'bruises_t' , 'gill-color_b' , 'gill-size_b' , 'gill-size_n' , 'ring-type_p' , 'stalk-surface-below-ring_k' , 'stalk-surface-above-ring_k' , \n                'odor_f', 'odor_n']]\nX_Reduced2 = StandardScaler().fit_transform(X_Reduced2)\nX_Train3, X_Test3, Y_Train3, Y_Test3 = train_test_split(X_Reduced2, Y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.process_time()\ntrainedsvm = svm.LinearSVC().fit(X_Train3, Y_Train3)\nprint(time.process_time() - start)\npredictionsvm = trainedsvm.predict(X_Test3)\nprint(confusion_matrix(Y_Test3,predictionsvm))\nprint(classification_report(Y_Test3,predictionsvm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Univariate Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Univariate Feature Selection is a statistical method used to select the features which have the strongest relationship with our corrispondent labels. Using the **SelectKBest** method we can decide which metrics to use to evaluate our features and the number of K best features we want to keep. Different types of scoring functions are available depending on our needs: \n- Classification: chi2, f_classif, mutual_info_classif\n- Regression: f_regression, mutual_info_regression\n\nIn this example, we will be using chi2. [Chi-squared (Chi2)](https://en.wikipedia.org/wiki/Chi-squared_test) can take as input just non-negative values, therefore, first of all we scale our input data in a range between 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_scaler = preprocessing.MinMaxScaler()\nScaled_X = min_max_scaler.fit_transform(X2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX_new = SelectKBest(chi2, k=2).fit_transform(Scaled_X, Y)\nX_Train3, X_Test3, Y_Train3, Y_Test3 = train_test_split(X_new, Y, test_size = 0.30, random_state = 101)\nstart = time.process_time()\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train3,Y_Train3)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_Test3)\nprint(confusion_matrix(Y_Test3,predictionforest))\nprint(classification_report(Y_Test3,predictionforest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lasso Regression"},{"metadata":{},"cell_type":"markdown","source":"When applying regularization to a Machine Learning model, we add a penalty to the model parameters so that to avoid that our model tries to resemble too closely our input data. In this way, we can make our model less complex and we can avoid overfitting (making learn to our model not just the key data characheteristics but also it's intrinsic noise). <br>\n\nOne of the possible Regularization Methods is Lasso (L1) Regrssion. When using Lasso Regression, the coefficients of the inputs features gets shrinken if they are not positively contributing towards our Machine Learning model training. In this way, some of the features might get automatically discarded assigning them coefficients equal to zero.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\nregr = LassoCV(cv=5, random_state=101)\nregr.fit(X_Train,Y_Train)\nprint(\"LassoCV Best Alpha Scored: \", regr.alpha_)\nprint(\"LassoCV Model Accuracy: \", regr.score(X_Test, Y_Test))\nmodel_coef = pd.Series(regr.coef_, index = list(X.columns[:-1]))\nprint(\"Variables Eliminated: \", str(sum(model_coef == 0)))\nprint(\"Variables Kept: \", str(sum(model_coef != 0))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n\ntop_coef = model_coef.sort_values()\ntop_coef[top_coef != 0].plot(kind = \"barh\")\nplt.title(\"Most Important Features Identified using Lasso (!0)\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}