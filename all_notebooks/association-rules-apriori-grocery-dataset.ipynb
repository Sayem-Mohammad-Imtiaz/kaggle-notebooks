{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Step 1: Importing Required Libraries.."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Dataset Loading & Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/groceries/groceries.csv\", sep=\";\", header= None)\n\n# when we set header=None it will consider as csv file which has no header.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for above output:\n\n* Each row is one transction.\n\n* Products in each row is nothing but items purchased by buyer/customer."},{"metadata":{},"cell_type":"markdown","source":" ### Finding out all unique items available at grocery."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_items_list = []\n\n# for each index it will iter row by row\nfor index, row in dataset.iterrows():  \n    \n    # splitting items with , and creating a new list for row & it will going add it agian \n    # ...item_series list for each iteration..so item_series will be list of lists..\n    items_series = list(row.str.split(','))\n    \n    \n    # agian reading each list elements from item_Series which is big list as mentioned above code\n    for each_row_list in items_series:\n        \n        # iterating each item from each_row_lists\n        for item in each_row_list:\n            \n            # for first iteration..unique_items_list is empty so first item directly append to it.\n            #...from next onwards..it will start to check condition 'not in'\n            #....& if item not found in unique_items_list list then it will append to it.\n            #......finally we will get one unique item list..\n            if item not in unique_items_list:\n                unique_items_list.append(item)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Now, Generating empty Dataframe with unique_items_list elements as column names."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_apriori = pd.DataFrame(columns=unique_items_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_apriori","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset1 =df_apriori.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Sorting items from main dataset agian & assigning in respective column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## If for the item names obesrved w.r.t. each list will be assigned as number 1 & those items are not in \n##...row number iterating will be assigned with nuber 0.\n\nfor index, row in dataset.iterrows():\n    items = str(row[0]).split(',')\n    one_hot_encoding = np.zeros(len(unique_items_list),dtype=int)\n    for item_name in items:\n        for i,column in enumerate(dataset1.columns):\n            if item_name == column:\n                one_hot_encoding[i] = 1\n    dataset1.at[index] = one_hot_encoding\n\n# Transction encoder is fastest method to do all this.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of the dataset1\n\ndataset1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sinced efault datatype saved as 'object'. Converting in 'integer' datatype\n\ndataset1 = dataset1.astype('uint8')\ndataset1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Checking for Purchased and not purchased item qty. details to get insights"},{"metadata":{"trusted":true},"cell_type":"code","source":"zero =[]\none = []\nfor i in df_apriori.columns:\n    zero.append(list(dataset1[i].value_counts())[0])\n    one.append(list(dataset1[i].value_counts())[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = pd.DataFrame([zero,one], columns=df_apriori.copy().columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing row names...\n\ncount_df.index = ['Not_Purchased', 'Purchased']\ncount_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grocery shop contains total 169 numbers of items."},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHECKING WHICH PRODUCTE\n\nprint('maximum purchased item:',count_df.idxmax(axis = 1)[1],':',count_df.loc['Purchased'].max())\nprint('minimum purchased item:',count_df.idxmax(axis = 1)[0],':',count_df.loc['Not_Purchased'].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simplest way to sort elements..\n\nsorted_df = pd.DataFrame(count_df.sort_values(by=['Purchased'],axis=1,ascending=False).transpose())\nsorted_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding Purchased% table into the dataset1.\n\nsorted_df['Purchased%']= sorted_df.Purchased/sum(sorted_df.Purchased)\nsorted_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding out avergae of the total purchased% so that we get idea about min_support value setting.\n\nnp.mean(sorted_df['Purchased%'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting sorted top purchased products..\n\nfig = plt.subplots(figsize=(20,10))\npurchased = sorted_df.head(50).xs('Purchased' ,axis = 1)\npurchased.plot(kind='bar',fontsize=16)\nplt.title('Purchased top Count',fontsize=30)\nplt.xlabel('Products', fontsize=20)\nplt.ylabel('total qty. purchased', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### People purchased more is daily need items & transction for all them is above 1000 nos."},{"metadata":{},"cell_type":"markdown","source":"'whole milk', 'other vegetables', 'rolls/buns', 'soda', 'yogurt','bottled water', 'root vegetables', 'tropical fruit'"},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Apriori Rule\n***\n\nref. used: https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Measure 1: Support \n***"},{"metadata":{},"cell_type":"markdown","source":" #### Concept:"},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/ShrikantUppin/Association_Rules/blob/main/measure1_formula.png?raw=true)\n\n<img src=\"https://github.com/ShrikantUppin/Association_Rules/blob/main/measure1.png?raw=true\" width=\"300\" height=\"300\">\n"},{"metadata":{},"cell_type":"markdown","source":" This says how popular an itemset is, as measured by the proportion of transactions in which an itemset appears. In Table 1 below, the support of {apple} is 4 out of 8, or 50%. Itemsets can also contain multiple items. For instance, the support of {apple, beer, rice} is 2 out of 8, or 25%.\n"},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":" ### Finding out support for each possible products or diff. product sets present in transction dataframe(dataset1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.frequent_patterns import apriori, association_rules\n\nfreq_items = apriori(dataset1, min_support=0.02, use_colnames=True, max_len=5)\n\n# min_support value can be choose by the user/business need\n# max_len is item combinations..here i have taken as 5. total items in combination formed should not be more than 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_items.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p algin='justify'> This says how popular an itemset is, as measured by the proportion of transactions in which an itemset appears.</p> \n\nFor example:\n\nIn Table sorted_df, the support of {whole milk} is 2513 out of total 9835 row tranctions. i.e. 25.55%. \n\n\nItemsets can also contain multiple items. For instance, the support of {bottled water, soda} is 285 out of 9835, or 2.89%"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking first 10 rows\n\nfreq_items.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking last 10 rows \n\nfreq_items.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you discover that sales of items beyond a certain proportion tend to have a significant impact on your profits, you might consider using that proportion as your support threshold. You may then identify itemsets with support values above this threshold as significant itemsets.\n\n***\n***"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Measure 2: Confidence\n***\n***"},{"metadata":{},"cell_type":"markdown","source":" #### Concept:"},{"metadata":{},"cell_type":"markdown","source":" This says how likely item Y is purchased when item X is purchased, expressed as {X -> Y}. This is measured by the proportion of transactions with item X, in which item Y also appears. In Table 1, the confidence of {apple -> beer} is 3 out of 4, or 75%.\n\n\n\n<img src=\"https://github.com/ShrikantUppin/Association_Rules/blob/main/measure2.png?raw=true\" >\n\n***\n* Drawbacks of Confidence measure:\n***\n\n* it might misrepresent the importance of an association. \n\n* This is because it only accounts for how popular apples are, but not beers. If beers are also very popular in general, there will be a higher chance that a transaction containing apples will also contain beers, thus inflating the confidence measure. \n\n\nNote: To account for the base popularity of both constituent items, we use a third measure called lift."},{"metadata":{},"cell_type":"markdown","source":" ### Building Association rules using confidence metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for this we need support value dataframe..that is fre_items from measure1.\n\nconfidence_association = association_rules(freq_items, metric='confidence', min_threshold=0.2)\n\n# min_threshold is nothing but setting min % crieteria. In this case i have choosen 20% \n#...confidence should be minimum 20%.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking combination in first 10 rows from dataset\n\nconfidence_association.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"0.028978*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking combination in last 10 rows from dataset\n\nconfidence_association.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Understanding terminologies:\n ***\n \n [Basic Terminology](https://michael.hahsler.net/research/recommender/associationrules.html#:~:text=Leverage%20measures%20the%20difference%20of,expected%20from%20the%20independent%20sells)"},{"metadata":{},"cell_type":"markdown","source":" #### 1 . Antecedent and Consequent\n \nThe IF component of an association rule is known as the antecedent. The THEN component is known as the consequent. The antecedent and the consequent are disjoint; they have no items in common.\n\n\n #### 2. antecedent support\n \n It is antecedent support with all transction numbers.\n \n \n #### 3. consequent support\n\n It is consequent  support with all transction numbers.\n \n \n #### 4. Support:\n \n Here support is considered for antecedent+consequent combination.\n \n \n #### 5. confidence\n \n Confidence is related to 'consequent item' or 'consequent item combination' w.r.t. antecedent item  or item set.\n \n \n #### 6. lift\n \nLift measures how many times more often X and Y occur together than expected if they where statistically independent. Lift is not down-ward closed and does not suffer from the rare item problem.\n \n In short firm possibilities of buying consequent whenever Antecedent item is purchaed by customer\n \n \n #### 7. Leverage\n \n Leverage measures the difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically dependent. The rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells.\n \n leverage also can suffer from the rare item problem.\n \n leverage(X -> Y) = P(X and Y) - (P(X)P(Y))\n \n \n #### 8. conviction\n \n conviction(X -> Y) = P(X)P(not Y)/P(X and not Y)=(1-sup(Y))/(1-conf(X -> Y))\n\nConviction compares the probability that X appears without Y if they were dependent with the actual frequency of the appearance of X without Y. In that respect it is similar to lift (see section about lift on this page), however, it contrast to lift it is a directed measure. Furthermore, conviction is monotone in confidence and lift.\n\n\n#### 9. Coverage\n\ncoverage(X) = P(X) = sup(X)\n\nA simple measure of how often a item set appears in the data set."},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Measure 3: Lift \n***"},{"metadata":{},"cell_type":"markdown","source":" #### Concept:\n    \nThis says how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is. In Table 1, the lift of {apple -> beer} is 1,which implies no association between items. A lift value greater than 1 means that item Y is likely to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is bought.\n\n\n<img src=\"https://github.com/ShrikantUppin/Association_Rules/blob/main/measure3.png?raw=true\" >"},{"metadata":{},"cell_type":"markdown","source":" ### Building Association rules using confidence metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"lift_association = association_rules(freq_items, metric=\"lift\", min_threshold=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lift_association.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lift_association.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lift_association.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Eliminating redudancy sets...\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"# As per above output observation, it is clear that when same items repeated..\n#...(for ex: in first row: A-->B, and in next row B-->A) gives same leverage & lift but confidence is different.\n#...this is known as redudency when same item set shuffled as ancedents & consequent.\n#.... so to eliminates in easist way..will sort n the basis of leverage & confidence.\n\nredundancy = lift_association.sort_values(by=['leverage','confidence'],axis=0, ascending=False).reset_index()\nredundancy = redundancy.drop(['index'], axis=1)\nredundancy.shape\nredundancy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"redundancy.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now check output of above cells, when leverage and lift are same for consequent rows..then compare with the value of confidence\n#...if confidence of middle cell found less than two side cells..drop it.\n# dropping odd index rows..since it contains less confidence\n# ultimately this will help us to elminate repeated combination..which has low lift & confidence..\n\nunique_rules = redundancy.iloc[::2]\nunique_rules.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_rules.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Summary:\n \n \n * freq_items = apriori(dataset1, min_support=0.02, use_colnames=True, max_len=5)\n \n \n\n * confidence_association = association_rules(freq_items, metric='confidence', min_threshold=0.2)\n \n \n * lift_association = association_rules(freq_items, metric=\"lift\", min_threshold=1)\n "},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Generated Rules analysis/Processing"},{"metadata":{},"cell_type":"markdown","source":"We have obtained unique_rules with metric='lift'. Now, this unique_rules dataframe will be used for analysis..just filtering as per threshold value set/required & obtaining diff. pairs of item sets.\n\n\nNote: lift is set to 1. in previous code. Since if lift is equal to or greater than 1..that means chances to pick consequents items by customer is more..!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_20 = unique_rules[unique_rules['lift']>1.5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Top 20 combinations w.r.t. Lift more than 1.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_20_sort = top_20.sort_values(by='lift', ascending=False)\ntop_20_sort.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_20_sort = top_20_sort.drop(['index'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_20_sort.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Unique item names from to 20 lift combinations.."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = top_20_sort[['antecedents','consequents']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_list = []\nfor i in x.antecedents.to_list():\n    for j in list(set(i)):\n        item_list .append(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in x.consequents.to_list():\n    for q in list(set(p)):\n        item_list.append(q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique(list1): \n    # insert the list to the set \n    list_set = set(list1) \n    # convert the set to the list \n    unique_list = (list(list_set))\n    top_items =[]\n    for m in unique_list:\n        top_items.append(m)\n    print(top_items)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique(item_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above are the top 20 products items & the shuffled combination gives top lift result."},{"metadata":{},"cell_type":"markdown","source":"***\n***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}