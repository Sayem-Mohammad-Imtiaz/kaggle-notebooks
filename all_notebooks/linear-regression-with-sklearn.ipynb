{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Linear Regression Analysis with sklearn\n\nIn this notebook, we will apply Linear Regression one-by-one for following:\n1. Simple Linear Regression\n2. Multivariate Linear Regression\n3. Feature Selection","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd  # another way to import multiple libraries in single line\nimport matplotlib.pyplot as plt, seaborn as sns\nsns.set()\n\nfrom sklearn.linear_model import LinearRegression  # importing requred modules from sklearn","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:12.901054Z","iopub.execute_input":"2021-07-01T17:24:12.901684Z","iopub.status.idle":"2021-07-01T17:24:12.908075Z","shell.execute_reply.started":"2021-07-01T17:24:12.901625Z","shell.execute_reply":"2021-07-01T17:24:12.906604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Simple Linear Regression","metadata":{}},{"cell_type":"markdown","source":"### 1.1. Loading Data","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv(\"../input/real-estate-price-size/real_estate_price_size.csv\")\ndata1.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:12.997629Z","iopub.execute_input":"2021-07-01T17:24:12.997956Z","iopub.status.idle":"2021-07-01T17:24:13.03955Z","shell.execute_reply.started":"2021-07-01T17:24:12.997927Z","shell.execute_reply":"2021-07-01T17:24:13.038301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Assigning Dependent and Independent Variables","metadata":{}},{"cell_type":"code","source":"x1 = data1[\"size\"]  # independent variable\ny1 = data1[\"price\"]  # dependent variable","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.041468Z","iopub.execute_input":"2021-07-01T17:24:13.041789Z","iopub.status.idle":"2021-07-01T17:24:13.045691Z","shell.execute_reply.started":"2021-07-01T17:24:13.041756Z","shell.execute_reply":"2021-07-01T17:24:13.04495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3. Reshaping x to make regression on it possible","metadata":{}},{"cell_type":"code","source":"print(x1.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.046791Z","iopub.execute_input":"2021-07-01T17:24:13.04703Z","iopub.status.idle":"2021-07-01T17:24:13.062261Z","shell.execute_reply.started":"2021-07-01T17:24:13.047007Z","shell.execute_reply":"2021-07-01T17:24:13.061408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"shape = (100,) shows that x is a vector, we need a matrix to apply regression on it. We will use reshape() for this.","metadata":{}},{"cell_type":"code","source":"x1_matrix = x1.values.reshape(x1.values.size, 1)  # x.values.size = 100 as it tell total array size\nx1_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.063371Z","iopub.execute_input":"2021-07-01T17:24:13.063681Z","iopub.status.idle":"2021-07-01T17:24:13.079582Z","shell.execute_reply.started":"2021-07-01T17:24:13.06365Z","shell.execute_reply":"2021-07-01T17:24:13.078355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4. Visualization:\nThis step is not a part of regression application but is very useful to assess whether linear regression can be applied on the data or not.","metadata":{}},{"cell_type":"code","source":"plt.scatter(x1_matrix, y1)\nplt.xlabel(\"Size\", fontsize = 20)\nplt.ylabel(\"Price\", fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.092151Z","iopub.execute_input":"2021-07-01T17:24:13.092628Z","iopub.status.idle":"2021-07-01T17:24:13.332501Z","shell.execute_reply.started":"2021-07-01T17:24:13.092587Z","shell.execute_reply":"2021-07-01T17:24:13.330935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.5. Applying Regression","metadata":{}},{"cell_type":"code","source":"reg1 = LinearRegression()  # making an instance of class LinearRegression\nreg1.fit(x1_matrix, y1)  # applying regression on given data","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.334262Z","iopub.execute_input":"2021-07-01T17:24:13.334553Z","iopub.status.idle":"2021-07-01T17:24:13.360412Z","shell.execute_reply.started":"2021-07-01T17:24:13.334524Z","shell.execute_reply":"2021-07-01T17:24:13.358785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**R-Squared $(R^2)$:**","metadata":{}},{"cell_type":"code","source":"reg1.score(x1_matrix, y1)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.361884Z","iopub.execute_input":"2021-07-01T17:24:13.362201Z","iopub.status.idle":"2021-07-01T17:24:13.374156Z","shell.execute_reply.started":"2021-07-01T17:24:13.362171Z","shell.execute_reply":"2021-07-01T17:24:13.372844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Coefficient:**","metadata":{}},{"cell_type":"code","source":"reg1.coef_","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.376217Z","iopub.execute_input":"2021-07-01T17:24:13.376676Z","iopub.status.idle":"2021-07-01T17:24:13.387293Z","shell.execute_reply.started":"2021-07-01T17:24:13.37663Z","shell.execute_reply":"2021-07-01T17:24:13.386189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Intercept:**","metadata":{}},{"cell_type":"code","source":"reg1.intercept_","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.391159Z","iopub.execute_input":"2021-07-01T17:24:13.391549Z","iopub.status.idle":"2021-07-01T17:24:13.401865Z","shell.execute_reply.started":"2021-07-01T17:24:13.39152Z","shell.execute_reply":"2021-07-01T17:24:13.400529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.6. Predicting for New Houses","metadata":{}},{"cell_type":"code","source":"reg1.predict([[750], [500]])  # takes a DataFrame or array as argument and returns dependent variable","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.403736Z","iopub.execute_input":"2021-07-01T17:24:13.40411Z","iopub.status.idle":"2021-07-01T17:24:13.421165Z","shell.execute_reply.started":"2021-07-01T17:24:13.404073Z","shell.execute_reply":"2021-07-01T17:24:13.419908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Multivariate Linear Regression","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Loading Data","metadata":{}},{"cell_type":"code","source":"data2 = pd.read_csv(\"../input/real-estate-price-size-year/real_estate_price_size_year.csv\")\ndata2.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.422666Z","iopub.execute_input":"2021-07-01T17:24:13.423116Z","iopub.status.idle":"2021-07-01T17:24:13.448741Z","shell.execute_reply.started":"2021-07-01T17:24:13.423079Z","shell.execute_reply":"2021-07-01T17:24:13.447516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Assigning Dependent and Independent Variables","metadata":{}},{"cell_type":"code","source":"x2 = data2[[\"size\", \"year\"]]  # features, another word for regressors and independent varaibles\ny2 = data2[\"price\"]  # target, another word for dependent variable","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.450174Z","iopub.execute_input":"2021-07-01T17:24:13.450568Z","iopub.status.idle":"2021-07-01T17:24:13.459642Z","shell.execute_reply.started":"2021-07-01T17:24:13.450529Z","shell.execute_reply":"2021-07-01T17:24:13.458364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x2.shape  # there is no need to reshape in multivariate linear regression, as it already is a matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.460936Z","iopub.execute_input":"2021-07-01T17:24:13.461198Z","iopub.status.idle":"2021-07-01T17:24:13.471804Z","shell.execute_reply.started":"2021-07-01T17:24:13.461172Z","shell.execute_reply":"2021-07-01T17:24:13.47091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3. Visualization","metadata":{}},{"cell_type":"code","source":"plt.scatter(x2[\"size\"], y2)\nplt.xlabel(\"Size\", fontsize = 20)\nplt.ylabel(\"Price\", fontsize = 20)\nplt.title(\"Price - Size\")\nplt.show()\nplt.scatter(x2[\"year\"], y2)\nplt.xlabel(\"Year\", fontsize = 20)\nplt.ylabel(\"Price\", fontsize = 20)\nplt.title(\"Price - Year\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.472882Z","iopub.execute_input":"2021-07-01T17:24:13.473154Z","iopub.status.idle":"2021-07-01T17:24:13.875851Z","shell.execute_reply.started":"2021-07-01T17:24:13.473129Z","shell.execute_reply":"2021-07-01T17:24:13.873145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4. Applying Regression","metadata":{}},{"cell_type":"code","source":"reg2 = LinearRegression()\nreg2.fit(x2, y2)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.877205Z","iopub.execute_input":"2021-07-01T17:24:13.877479Z","iopub.status.idle":"2021-07-01T17:24:13.888123Z","shell.execute_reply.started":"2021-07-01T17:24:13.877438Z","shell.execute_reply":"2021-07-01T17:24:13.887284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**R-Squared $(R^2)$:**","metadata":{}},{"cell_type":"code","source":"r2 = reg2.score(x2, y2)\nr2","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.889525Z","iopub.execute_input":"2021-07-01T17:24:13.889854Z","iopub.status.idle":"2021-07-01T17:24:13.900281Z","shell.execute_reply.started":"2021-07-01T17:24:13.889826Z","shell.execute_reply":"2021-07-01T17:24:13.899499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adjusted R-Squared $R^2$:**\n\nThere is no set method in sklearn to find adjusted value of R-Squared like in statsmodel. But what we can do is manually find out its values by using this formula:\n\nadj_r2 = 1-(1-r2)*(n-1)/(n-p-1), where\n- r2 - R-Squared\n- n - x2.shape[0] - number of observations\n- p - x2.shape[1] - number of features","metadata":{}},{"cell_type":"code","source":"n = x2.shape[0]\np = x2.shape[1]\nprint(n, p)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.901769Z","iopub.execute_input":"2021-07-01T17:24:13.902147Z","iopub.status.idle":"2021-07-01T17:24:13.908794Z","shell.execute_reply.started":"2021-07-01T17:24:13.902116Z","shell.execute_reply":"2021-07-01T17:24:13.907995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\nadj_r2","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.912224Z","iopub.execute_input":"2021-07-01T17:24:13.912554Z","iopub.status.idle":"2021-07-01T17:24:13.921372Z","shell.execute_reply.started":"2021-07-01T17:24:13.912527Z","shell.execute_reply":"2021-07-01T17:24:13.920609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Coefficient:**","metadata":{}},{"cell_type":"code","source":"reg2.coef_","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.923189Z","iopub.execute_input":"2021-07-01T17:24:13.923497Z","iopub.status.idle":"2021-07-01T17:24:13.935541Z","shell.execute_reply.started":"2021-07-01T17:24:13.923469Z","shell.execute_reply":"2021-07-01T17:24:13.934491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Intercept:**","metadata":{}},{"cell_type":"code","source":"reg2.intercept_","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.937019Z","iopub.execute_input":"2021-07-01T17:24:13.937385Z","iopub.status.idle":"2021-07-01T17:24:13.947204Z","shell.execute_reply.started":"2021-07-01T17:24:13.937345Z","shell.execute_reply":"2021-07-01T17:24:13.946091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5. Predicting Targets","metadata":{}},{"cell_type":"code","source":"reg2.predict([[750, 2009], [640, 2015]])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.948693Z","iopub.execute_input":"2021-07-01T17:24:13.949071Z","iopub.status.idle":"2021-07-01T17:24:13.962714Z","shell.execute_reply.started":"2021-07-01T17:24:13.949031Z","shell.execute_reply":"2021-07-01T17:24:13.96162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Feature Selection\nFeature selection is a very important part of multivariate linear regressions, so basically, of all linear regressions. When we have many features, we through this decide which feature actually hold importanct in variability of our target as there can be some features which are playing no role in variability of dependent variable.\n\nFor feature selection, we can use following techniques.\n1. Feature Selection using F Regression\n2. Feature Selection and Standardization","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Feature Selection using F Regression\nF Regression creates simple linear regression for each feature and independent variable. (i.e. n number of features => n regressions)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import f_regression  # importing module for f regression","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:13.964491Z","iopub.execute_input":"2021-07-01T17:24:13.965029Z","iopub.status.idle":"2021-07-01T17:24:14.057294Z","shell.execute_reply.started":"2021-07-01T17:24:13.964985Z","shell.execute_reply":"2021-07-01T17:24:14.056483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will be taking same example used in Multivariate Regression further for this\nf_regression(x2, y2) # this will return an array for each feature containg two values","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.058516Z","iopub.execute_input":"2021-07-01T17:24:14.058999Z","iopub.status.idle":"2021-07-01T17:24:14.070619Z","shell.execute_reply.started":"2021-07-01T17:24:14.058967Z","shell.execute_reply":"2021-07-01T17:24:14.069803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First value of the array is f-statistic of each regression and second value is the p-value.","metadata":{}},{"cell_type":"code","source":"feat_1, feat_2 = f_regression(x2, y2)  # saving each array in separate variable\nf_statistics = f_regression(x2, y2)[0]  # saving an array of f-statistics in an array\np_values = f_regression(x2, y2)[1]  # saving an array of p-values in an array","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.072091Z","iopub.execute_input":"2021-07-01T17:24:14.072651Z","iopub.status.idle":"2021-07-01T17:24:14.083996Z","shell.execute_reply.started":"2021-07-01T17:24:14.072619Z","shell.execute_reply":"2021-07-01T17:24:14.082767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_values","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.085483Z","iopub.execute_input":"2021-07-01T17:24:14.086049Z","iopub.status.idle":"2021-07-01T17:24:14.094933Z","shell.execute_reply.started":"2021-07-01T17:24:14.086014Z","shell.execute_reply":"2021-07-01T17:24:14.093871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"P-Values are most usefull here, as they tell us that how much important the feature is for the model.","metadata":{}},{"cell_type":"markdown","source":"**Creating a Summary**\n\nUnlike statsmodel, sklearn does not have a method to summarize whole regression model for us, but we can buit out own summary.","metadata":{}},{"cell_type":"code","source":"summary2 = pd.DataFrame([\"Size\", \"Year\"], columns = [\"features\"])  # making a dataframe with all features listed in it\nsummary2[\"coeff\"] = reg2.coef_  # adding a column for coefficients and placing their value for each feature\nsummary2[\"p-values\"] = p_values.round(3)  # adding p-values for all features\nsummary2  # printing summary","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.098967Z","iopub.execute_input":"2021-07-01T17:24:14.099977Z","iopub.status.idle":"2021-07-01T17:24:14.113427Z","shell.execute_reply.started":"2021-07-01T17:24:14.099934Z","shell.execute_reply":"2021-07-01T17:24:14.112721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, looking at this summary and remembering the fact that feaures with p-value greater than 0.05 are insignificant for the model, we should be disregarding the 'year' feature. But, we will be keeping it. Why? Our next technique for feature selection will give the answer.","metadata":{}},{"cell_type":"markdown","source":"### 3.2. Feature Selection and Standardization\nStandardization means feature scaling here. This technique may serve other purposes too, alongwith feature selection.\n\nWe have different ranges of values for differnt features. We, through feature scaling, standardize them. So different features and their weight in the model can be compared.\n\nFor standardizing, we substract mean value of the feature from each feature value and then divide it with the std of that feature's value. In sklearn, we have a method for that.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler  # importing module for feature scaling","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.115128Z","iopub.execute_input":"2021-07-01T17:24:14.115811Z","iopub.status.idle":"2021-07-01T17:24:14.126637Z","shell.execute_reply.started":"2021-07-01T17:24:14.115767Z","shell.execute_reply":"2021-07-01T17:24:14.125607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()  # instantiating StandardScaler class\nscaler.fit(x2)  # fitting standardization on feature data","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.127855Z","iopub.execute_input":"2021-07-01T17:24:14.12814Z","iopub.status.idle":"2021-07-01T17:24:14.144318Z","shell.execute_reply.started":"2021-07-01T17:24:14.12811Z","shell.execute_reply":"2021-07-01T17:24:14.143381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x2_scaled = scaler.transform(x2)  # transforming feature data into standardized feature data\nx2_scaled  # will print standardized features","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.145733Z","iopub.execute_input":"2021-07-01T17:24:14.146016Z","iopub.status.idle":"2021-07-01T17:24:14.166766Z","shell.execute_reply.started":"2021-07-01T17:24:14.14599Z","shell.execute_reply":"2021-07-01T17:24:14.165438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Applying Regression on Standardized Data**\n\nSame as we did earlier.","metadata":{}},{"cell_type":"code","source":"reg3 = LinearRegression()\nreg3.fit(x2_scaled, y2)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.168162Z","iopub.execute_input":"2021-07-01T17:24:14.16856Z","iopub.status.idle":"2021-07-01T17:24:14.175761Z","shell.execute_reply.started":"2021-07-01T17:24:14.168519Z","shell.execute_reply":"2021-07-01T17:24:14.175052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Making Regression Summary**","metadata":{}},{"cell_type":"code","source":"summary3 = pd.DataFrame([\"Bias\", \"Size\", \"Year\"], columns = [\"Features\"])  # bias is the term used for intercept\nsummary3[\"coeff\"] = [reg3.intercept_, reg3.coef_[0], reg3.coef_[1]]\nsummary3","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.176702Z","iopub.execute_input":"2021-07-01T17:24:14.177059Z","iopub.status.idle":"2021-07-01T17:24:14.194165Z","shell.execute_reply.started":"2021-07-01T17:24:14.177032Z","shell.execute_reply":"2021-07-01T17:24:14.193211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we can see actual weight of each feature and compare it. Previously, with unstandardized feature we could see the independent weights of the features but we could not compared them as each of the featured ranged different.\n\nAlso, this also answers why we did not remove 'year' as feature despite of having p-values significantly larger. The reason is both features' weight, it is clear that size has almost 6 times more weight than the year making its impact smaller. Actually, when we apply regression through sklearn the weight of feature with higher p-values is reduced to compliment to that.","metadata":{}},{"cell_type":"markdown","source":"**Predicting Targets with Standardized Features**","metadata":{}},{"cell_type":"code","source":"new_houses3 = pd.DataFrame([[643, 2015], [600, 2008], [800, 2021], [300, 2004], [600, 2018]], columns = [\"Size\", \"Year\"])\nreg3.predict(new_houses3).round(2)  # givings values way higher than values in the sample, weird","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.195716Z","iopub.execute_input":"2021-07-01T17:24:14.196286Z","iopub.status.idle":"2021-07-01T17:24:14.212615Z","shell.execute_reply.started":"2021-07-01T17:24:14.196244Z","shell.execute_reply":"2021-07-01T17:24:14.211323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reason behind these weird predictions is that we have standardized features for training our model and now for this test data, we are trying to use unstandardized values of independent variables.","metadata":{}},{"cell_type":"code","source":"standardized_new_houses3 = scaler.transform(new_houses3)  # transforming new data into standardized form","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.214438Z","iopub.execute_input":"2021-07-01T17:24:14.214882Z","iopub.status.idle":"2021-07-01T17:24:14.221747Z","shell.execute_reply.started":"2021-07-01T17:24:14.214839Z","shell.execute_reply":"2021-07-01T17:24:14.220973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_houses3[\"predictions\"] = reg3.predict(standardized_new_houses3).round(0)  # predicting values and saving them in df","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.223288Z","iopub.execute_input":"2021-07-01T17:24:14.223687Z","iopub.status.idle":"2021-07-01T17:24:14.236263Z","shell.execute_reply.started":"2021-07-01T17:24:14.223656Z","shell.execute_reply":"2021-07-01T17:24:14.235283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_houses3","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.237786Z","iopub.execute_input":"2021-07-01T17:24:14.238049Z","iopub.status.idle":"2021-07-01T17:24:14.252223Z","shell.execute_reply.started":"2021-07-01T17:24:14.238023Z","shell.execute_reply":"2021-07-01T17:24:14.251432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What if we had removed year as it was not as significant as size?**","metadata":{}},{"cell_type":"code","source":"reg_simple = LinearRegression()\nx_simple_matrix = x2_scaled[:, 0].reshape(-1, 1) # only transforming first column into standardized form\nreg_simple.fit(x_simple_matrix, y2)\nreg_simple.predict(standardized_new_houses3[:, 0].reshape(-1, 1)).round()  # doing same transformation for new data","metadata":{"execution":{"iopub.status.busy":"2021-07-01T17:24:14.253436Z","iopub.execute_input":"2021-07-01T17:24:14.253709Z","iopub.status.idle":"2021-07-01T17:24:14.261316Z","shell.execute_reply.started":"2021-07-01T17:24:14.253682Z","shell.execute_reply":"2021-07-01T17:24:14.260269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These results are different but not very significantly differents as we got with both features. So, our choice of not excluding 'year' from the model was compensated by regression model via assignment of lesser weightage to the less significant feature.  ","metadata":{}}]}