{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:03.64119Z","iopub.execute_input":"2021-06-16T06:01:03.6415Z","iopub.status.idle":"2021-06-16T06:01:03.647349Z","shell.execute_reply.started":"2021-06-16T06:01:03.641473Z","shell.execute_reply":"2021-06-16T06:01:03.646276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('../input/spookyauthor/train.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:03.648794Z","iopub.execute_input":"2021-06-16T06:01:03.649041Z","iopub.status.idle":"2021-06-16T06:01:03.731955Z","shell.execute_reply.started":"2021-06-16T06:01:03.649017Z","shell.execute_reply":"2021-06-16T06:01:03.730561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#finding the shape of the data (number of rows and columns)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:03.733922Z","iopub.execute_input":"2021-06-16T06:01:03.734189Z","iopub.status.idle":"2021-06-16T06:01:03.741094Z","shell.execute_reply.started":"2021-06-16T06:01:03.734162Z","shell.execute_reply":"2021-06-16T06:01:03.740303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#number of unique authors\ndata['author'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:03.743019Z","iopub.execute_input":"2021-06-16T06:01:03.743287Z","iopub.status.idle":"2021-06-16T06:01:03.754082Z","shell.execute_reply.started":"2021-06-16T06:01:03.743259Z","shell.execute_reply":"2021-06-16T06:01:03.753141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing the class proportions\nsns.countplot(x=data['author'])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:03.755296Z","iopub.execute_input":"2021-06-16T06:01:03.755567Z","iopub.status.idle":"2021-06-16T06:01:03.904891Z","shell.execute_reply.started":"2021-06-16T06:01:03.755542Z","shell.execute_reply":"2021-06-16T06:01:03.903391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"# function to remove punctuations\nimport string\ndef remove_punct(text):\n    translator=str.maketrans('','',string.punctuation)\n    return text.translate(translator)\ndata['text']=data['text'].apply(remove_punct)\ndata['text'].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:03.905941Z","iopub.execute_input":"2021-06-16T06:01:03.906221Z","iopub.status.idle":"2021-06-16T06:01:04.022515Z","shell.execute_reply.started":"2021-06-16T06:01:03.906194Z","shell.execute_reply":"2021-06-16T06:01:04.02146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing stopwords\nsw=stopwords.words('english')\nnp.array(sw)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:04.023722Z","iopub.execute_input":"2021-06-16T06:01:04.024253Z","iopub.status.idle":"2021-06-16T06:01:04.032208Z","shell.execute_reply.started":"2021-06-16T06:01:04.024211Z","shell.execute_reply":"2021-06-16T06:01:04.031247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of unique stopwords:',len(sw))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:04.033629Z","iopub.execute_input":"2021-06-16T06:01:04.033953Z","iopub.status.idle":"2021-06-16T06:01:04.044443Z","shell.execute_reply.started":"2021-06-16T06:01:04.033924Z","shell.execute_reply":"2021-06-16T06:01:04.043241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to remove stopwords\ndef stopwords(text):\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)\ndata['text']=data['text'].apply(stopwords)\ndata['text'].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:04.04609Z","iopub.execute_input":"2021-06-16T06:01:04.046372Z","iopub.status.idle":"2021-06-16T06:01:04.809163Z","shell.execute_reply.started":"2021-06-16T06:01:04.046345Z","shell.execute_reply":"2021-06-16T06:01:04.80815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# most frequent occuring texts with the help of countvectorizer\ncount_vector=CountVectorizer()\ncount_vector.fit(data['text'])\n# collect the vocabulary items used in the vectorizer\ndictionary = count_vector.vocabulary_.items()  ","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:04.811664Z","iopub.execute_input":"2021-06-16T06:01:04.812062Z","iopub.status.idle":"2021-06-16T06:01:05.141174Z","shell.execute_reply.started":"2021-06-16T06:01:04.812021Z","shell.execute_reply":"2021-06-16T06:01:05.140299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# storing count and vocab in a dataframe\n# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\nvocab_bef_stem = pd.Series(count, index=vocab)\n# sort the dataframe\nvocab_bef_stem = vocab_bef_stem.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:05.144841Z","iopub.execute_input":"2021-06-16T06:01:05.145276Z","iopub.status.idle":"2021-06-16T06:01:05.180045Z","shell.execute_reply.started":"2021-06-16T06:01:05.145235Z","shell.execute_reply":"2021-06-16T06:01:05.179105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_vacab = vocab_bef_stem.head(10)\ntop_vacab.plot(kind = 'barh', figsize=(12,5), xlim= (25230, 25260))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:05.181432Z","iopub.execute_input":"2021-06-16T06:01:05.181687Z","iopub.status.idle":"2021-06-16T06:01:05.338905Z","shell.execute_reply.started":"2021-06-16T06:01:05.181663Z","shell.execute_reply":"2021-06-16T06:01:05.338065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an object of stemming function\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) ","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:05.339944Z","iopub.execute_input":"2021-06-16T06:01:05.340215Z","iopub.status.idle":"2021-06-16T06:01:05.344355Z","shell.execute_reply.started":"2021-06-16T06:01:05.340188Z","shell.execute_reply":"2021-06-16T06:01:05.343661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['text'].apply(stemming)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:05.34521Z","iopub.execute_input":"2021-06-16T06:01:05.345465Z","iopub.status.idle":"2021-06-16T06:01:08.379528Z","shell.execute_reply.started":"2021-06-16T06:01:05.34544Z","shell.execute_reply":"2021-06-16T06:01:08.378611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the object of tfid vectorizer\ntfid_vectorizer = TfidfVectorizer(\"english\")\n# fit the vectorizer using the text data\ntfid_vectorizer.fit(data['text'])\n# collect the vocabulary items used in the vectorizer\ndictionary = tfid_vectorizer.vocabulary_.items()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:08.380703Z","iopub.execute_input":"2021-06-16T06:01:08.380943Z","iopub.status.idle":"2021-06-16T06:01:08.670596Z","shell.execute_reply.started":"2021-06-16T06:01:08.38092Z","shell.execute_reply":"2021-06-16T06:01:08.669488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\nvocab_after_stem = pd.Series(count, index=vocab)\n# sort the dataframe\nvocab_after_stem = vocab_after_stem.sort_values(ascending=False)\n# plot of the top vocab\ntop_vacab = vocab_after_stem.head(10)\ntop_vacab.plot(kind = 'barh', figsize=(5,10), xlim= (15120, 15145))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:08.671848Z","iopub.execute_input":"2021-06-16T06:01:08.672134Z","iopub.status.idle":"2021-06-16T06:01:08.839213Z","shell.execute_reply.started":"2021-06-16T06:01:08.672107Z","shell.execute_reply":"2021-06-16T06:01:08.838207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the text lenght of each author\ndef length(text):\n    return len(text)\ndata['length']=data['text'].apply(length)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:08.840456Z","iopub.execute_input":"2021-06-16T06:01:08.840732Z","iopub.status.idle":"2021-06-16T06:01:08.861354Z","shell.execute_reply.started":"2021-06-16T06:01:08.840704Z","shell.execute_reply":"2021-06-16T06:01:08.860474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  visualizing the length of texts of authors\nEAP=data[data['author']=='EAP']\nHPL=data[data['author']=='HPL']\nMWS=data[data['author']=='MWS']\n\nmatplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\nbins = 500\nplt.hist(EAP['length'], alpha = 0.6, bins=bins, label='EAP')\nplt.hist(HPL['length'], alpha = 0.8, bins=bins, label='HPL')\nplt.hist(MWS['length'], alpha = 0.4, bins=bins, label='MWS')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,300)\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:08.862426Z","iopub.execute_input":"2021-06-16T06:01:08.86271Z","iopub.status.idle":"2021-06-16T06:01:11.315226Z","shell.execute_reply.started":"2021-06-16T06:01:08.86268Z","shell.execute_reply":"2021-06-16T06:01:11.314005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the tfid representation matrix of the text data\ntfid_matrix = tfid_vectorizer.transform(data['text'])\n# collect the tfid matrix in numpy array\narray = tfid_matrix.todense()\n# store the tf-idf array into pandas dataframe\ndf = pd.DataFrame(array)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:11.316767Z","iopub.execute_input":"2021-06-16T06:01:11.317163Z","iopub.status.idle":"2021-06-16T06:01:12.671139Z","shell.execute_reply.started":"2021-06-16T06:01:11.31712Z","shell.execute_reply":"2021-06-16T06:01:12.670183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"df['output'] = data['author']\ndf['id'] = data['id']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:12.674261Z","iopub.execute_input":"2021-06-16T06:01:12.674556Z","iopub.status.idle":"2021-06-16T06:01:12.711753Z","shell.execute_reply.started":"2021-06-16T06:01:12.674526Z","shell.execute_reply":"2021-06-16T06:01:12.710742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = df.columns.tolist()\noutput = 'output'\n# removing the output and the id from features\nfeatures.remove(output)\nfeatures.remove('id')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:12.712998Z","iopub.execute_input":"2021-06-16T06:01:12.713353Z","iopub.status.idle":"2021-06-16T06:01:12.720596Z","shell.execute_reply.started":"2021-06-16T06:01:12.713313Z","shell.execute_reply":"2021-06-16T06:01:12.719453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:12.722568Z","iopub.execute_input":"2021-06-16T06:01:12.723065Z","iopub.status.idle":"2021-06-16T06:01:12.731121Z","shell.execute_reply.started":"2021-06-16T06:01:12.723023Z","shell.execute_reply":"2021-06-16T06:01:12.729936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha_list1 = np.linspace(0.006, 0.1, 20)\nalpha_list1 = np.around(alpha_list1, decimals=4)\n\n# parameter grid\nparameter_grid = [{\"alpha\":alpha_list1}]\n# classifier object\nclassifier1 = MultinomialNB()\n# gridsearch object using 4 fold cross validation and neg_log_loss as scoring paramter\ngridsearch1 = GridSearchCV(classifier1,parameter_grid, scoring = 'neg_log_loss', cv = 4)\n# fit the gridsearch\ngridsearch1.fit(df[features], df[output])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:01:12.732235Z","iopub.execute_input":"2021-06-16T06:01:12.732546Z","iopub.status.idle":"2021-06-16T06:04:38.438684Z","shell.execute_reply.started":"2021-06-16T06:01:12.732518Z","shell.execute_reply":"2021-06-16T06:04:38.437569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best score: \",gridsearch1.best_score_) ","metadata":{"execution":{"iopub.status.busy":"2021-06-16T06:04:38.443747Z","iopub.execute_input":"2021-06-16T06:04:38.446588Z","iopub.status.idle":"2021-06-16T06:04:38.457959Z","shell.execute_reply.started":"2021-06-16T06:04:38.4465Z","shell.execute_reply":"2021-06-16T06:04:38.456778Z"},"trusted":true},"execution_count":null,"outputs":[]}]}