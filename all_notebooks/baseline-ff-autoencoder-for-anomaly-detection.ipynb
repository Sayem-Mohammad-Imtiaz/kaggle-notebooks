{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Anomaly detection in technical systems: anomaly in the water circulation loop"},{"metadata":{},"cell_type":"markdown","source":"## Testbed description"},{"metadata":{},"cell_type":"markdown","source":"The testbed of the industrial Internet of things is intended for:\n\n- Demonstration of the possibilities and benefits associated with the implementation of industrial Internet of things technologies;\n- Approbation, verification and validation of new technologies related to the industrial Internet of Things in laboratory conditions in order to determine a set of scientific theories, algorithms and practical tools for the application of these technologies in urgent industrial problems;\n- Conducting educational and research work on the subject of the industrial Internet of things.\n\nThe simple and intuitive user interface of the program allows you to operate the systems in accordance with the instructions presented in the user manual separately.\n\nThe testbed is a closed-loop water pump, a power supply system, control systems, data collection and monitoring system, built using industrial Internet of things technologies.\nThe stand consists of the following systems:\n\n1. Water circulation system.\n2. Control system of the water circulation system (hereinafter referred to as the Control System).\n3. System for monitoring the state of the water circulation system (hereinafter - Monitoring System).\n4. TSN technology demonstration system.\n5. System for storing, processing and visualizing data.\n\nThe water circulation system is designed to simulate a water supply system in a laboratory environment and circulates water through water pipes using a water pump.\nThe water circulation system simulates the following faults:\n\n- Introduction of imbalance on the connecting shaft (misalignment) of the motor and the water pump;\n- Changing the flow rate of the valve at the pump inlet (REDUCTION OF FLOW AREA);\n- Changing the flow rate of the valve at the pump outlet (REDUCTION OF FLOW AREA).\n\nThe system consists of the following components:\n- Water pump\n- Electric motor\n- Inverter\n- Electrovalve (1)\n- Electrovalve (2)\n- Mechanical lever for misalignment\n- Vibration sensors\n- Water tank with pipes\n- Pressure sensor\n- Flow meter\n- Thermocouple"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename=\"../input/skoltech-anomaly-benchmark-skab-teaser/look.png\", width=1000, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Front panel and composition of the water circulation, control and monitoring systems: 1,2 - solenoid valve (amount - 1); 3 - a tank with water (1); 4 - a water pump (1); 5 - emergency stop button (1); 6 - electric motor (1); 7 - inverter (1); 8 - compactRIO (1); 9 - a mechanical lever for shaft misalignment (1). Not shown parts - vibration sensor (2); pressure meter (1); flow meter (1); thermocouple (2)."},{"metadata":{},"cell_type":"markdown","source":"## Working with the data"},{"metadata":{},"cell_type":"markdown","source":"The dataset contains 4 anomalies (incidents):\n\n- MISALIGNMENT OF THE PUMP AND ENGINE SHAFT (abruptly)  \nAbrupt appearance of a defect: 18:39:22  \nAbrupt defect shutdown: 18:42:32\n\n- MISALIGNMENT OF PUMP AND MOTOR SHAFT (slow)  \nSlow appearance of the defect: 18:44:36-18:45:49  \nAbrupt defect shutdown: 18:46:51\n\n- REDUCTION OF FLOW AREA SECTION-1 (top)  \nSlow appearance of the defect: 19:06:57-19:07:37  \nSlow defect shutdown: 19:10:45-19:11:31\n\n- REDUCTION OF FLOW AREA SECTION-2 (bottom)  \nSlow appearance of the defect: 19:14:40-19:16:24  \nSlow defect shutdown: 19:19:15-19:21:16"},{"metadata":{},"cell_type":"markdown","source":"## Problem statement"},{"metadata":{},"cell_type":"markdown","source":"- **DS problem in terms of business:** We need to detect anomalies as soon as they appear.\n\nMetric:\nAverage Detection Delay (ADD)\n\n$\\text{ADD} = \\frac{1}{|Y|}\\sum_{y \\in Y} ( \\tau_y - \\theta_y )$,\n\nwhere $|Y|$ - total amount of changepoints,  \n$\\tau_y$ - moment of detection,  \n$\\theta_y$ - moment of changepoint (anomaly appearing).\n\n- **DS problem in terms of math:** We need to propose (construct) the model, that most accurately describes the normal operation of the testbed.\n\nMetric:\nMean Absolute Error (MAE)\n\n$\\text{MAE} = \\frac{1}{N} \\sum^{N}_{i=1}|x_i - \\hat{x}_i|$,\n\nwhere $N$ - Total amount of data instances,  \n$x_i$ - true value at a time moment $i$,  \n$\\hat{x}_i$ - predicted value at a time moment $i$."},{"metadata":{},"cell_type":"markdown","source":"### Libraries importing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the data\nraw_data = pd.read_csv('../input/skoltech-anomaly-benchmark-skab-teaser/SkAB teaser.csv', \n                   sep=';', \n                   index_col='datetime', \n                   parse_dates=True).drop('index',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing the first 10 rows of the table with the data\nraw_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's pivot the table, because we need each signal to be in the separate column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pivoting the table\nraw_data = raw_data.pivot_table(values='value', index=raw_data.index, columns='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing first 5 rows of the table\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting\nraw_data.plot(figsize=(12,6), marker='o', markersize=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's cut off the beginning interval of the data to exclude the transition period (warming up)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cutting off\nraw_data = raw_data['2019-07-08 17:52:29':]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting\nraw_data.plot(figsize=(12,6), marker='o', markersize=3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving processed data\n# raw_data.to_csv('raw_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'The shape of the table (dataframe): {raw_data.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting separate pictures for signals\nfor name in raw_data.columns:\n    raw_data[name].plot(figsize=(12,3), marker='o', markersize=2)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.title(f'Signal: {name}')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Additional data processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# todo\ndef preprocessing(raw_data):\n    data = raw_data.copy()\n    \n    # your code\n    \n    return data\n\ndata = preprocessing(raw_data=raw_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into training, validation and test sets"},{"metadata":{},"cell_type":"markdown","source":"- training sample — sample for model's parameters optimization.\n- validation sample — sample for selecting the best model from the set of models built on the training sample.\n- test sample — sample for assesing the quality of the problem solution.\n\nDescriptions and options for the definitions of training, validation and test samples are presented in the [article](https://medium.com/@tekaround/train-validation-test-set-in-machine-learning-how-to-understand-6cdd98d4a764).\n\n[Article](https://hunch.net/?p=22) about overfitting in ML."},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# Showing the training, validation and test sets\ndata.plot(figsize=(12,6))\nplt.axvspan(data.index[0], \n            '2019-07-08 18:25', \n            color='green', \n            alpha=0.1, \n            label='Training set')\nplt.axvspan('2019-07-08 18:25', \n            '2019-07-08 18:35', \n            color='yellow', \n            alpha=0.1, \n            label='Validation set')\nplt.axvspan('2019-07-08 18:35', \n            data.index[-1], \n            color='red', \n            alpha=0.1, \n            label='Test set')\nplt.legend(bbox_to_anchor =(0.8, -0.2), ncol = 3)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Training, validation and test sets');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data scaling (normalizing)"},{"metadata":{},"cell_type":"markdown","source":"Most of all ML algorithms need data to be scaled before fitting."},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# Scaler initialization\nStSc = StandardScaler()\n# Fitting Scaler on the training and validation sets\nStSc.fit(data[:'2019-07-08 18:25'])\n\n# Applying scaler\n# training set\ntrain_sc = StSc.transform(data[:'2019-07-08 18:25'])\n# validation set\nval_sc = StSc.transform(data['2019-07-08 18:25':'2019-07-08 18:35'])\n# all data\ndata_sc = StSc.transform(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting the model"},{"metadata":{},"cell_type":"markdown","source":"- Link to the Keras with TensorFlow backend course: https://youtu.be/qFJeN9V1ZsI"},{"metadata":{},"cell_type":"markdown","source":"### Libraries importing"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Dropout\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom itertools import product","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function for results reproducibility"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def Random(seed_value):\n    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n    import os\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n\n    # 2. Set `python` built-in pseudo-random generator at a fixed value\n    import random\n    random.seed(seed_value)\n\n    # 3. Set `numpy` pseudo-random generator at a fixed value\n    import numpy as np\n    np.random.seed(seed_value)\n\n    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n    import tensorflow as tf\n    tf.random.set_seed(seed_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Autoencoder description"},{"metadata":{},"cell_type":"markdown","source":"![ae](https://miro.medium.com/max/700/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)"},{"metadata":{},"cell_type":"markdown","source":"## Useful links\n\n- Autoencoder Explained:\nhttps://www.youtube.com/watch?v=H1AllrJ-_30\n\n- Outlier Detection with Autoencoder Ensembles:\nhttps://saketsathe.net/downloads/autoencode.pdf\n\n- About batch normalization layer:\nhttps://arxiv.org/pdf/1502.03167v2.pdf"},{"metadata":{"jupyter":{"outputs_hidden":true},"scrolled":true,"trusted":true},"cell_type":"code","source":"# Function for specific architecture fitting\ndef arch(param, data):\n    \"\"\"Specific architecture fitting\n\n    Parameters\n    ----------\n    param : list\n    \n    data : np.array\n    \"\"\"\n    Random(0)\n    input_dots = Input((8,))\n\n    x = Dense(param[0])(input_dots)\n    x = BatchNormalization()(x)\n    x = Activation('elu')(x)\n\n    x = Dense(param[1])(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    bottleneck = Dense(param[2], activation='linear')(x)\n\n    x = Dense(param[1])(bottleneck)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Dense(param[0])(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    out = Dense(8, activation='linear')(x)\n\n    model = Model(input_dots, out)\n    model.compile(optimizer=Adam(param[3]), loss='mae', metrics=[\"mse\"])\n    \n    model.fit(data, data,\n                validation_split=0.2,\n                epochs=10,\n                batch_size=param[4],\n                verbose=0,\n                shuffle=True,\n               )\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's fit the random model (architecture)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = arch(param=(6, 5, 4, 0.0001, 30), data=train_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(val_sc.shape[1]):\n    plt.figure(figsize=(12,3))\n    plt.plot(StSc.inverse_transform(val_sc)[:, i])\n    plt.plot(StSc.inverse_transform(model.predict(val_sc))[:, i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Absolute Error (MAE)\n\n$\\text{MAE} = \\frac{1}{N} \\sum^{N}_{i=1}|x_i - \\hat{x}_i|$,\n\nwhere $N$ - Total amount of data instances,  \n$x_i$ - true value at a time moment $i$,  \n$\\hat{x}_i$ - predicted value at a time moment $i$."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(val_sc, model.predict(val_sc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameters to optimize"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting the parameners' grid for checking\nn1=[6, 5]\nn2=[4, 3]\nn3=[2, 1]\nlr=[0.05, 0.01]\nbatch_size=[32, 64]\n\nparameters = product(n1, n2, n3, lr, batch_size)\nparameters_list = list(parameters)\nprint(f'Total number of parameter combinations: {len(parameters_list)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Table with the parameters' grid\npd.DataFrame(parameters_list, columns=['neurons 1st layer',\n                                      'neurons 2nd layer',\n                                      'neurons 3rd layer',\n                                      'learning rate',\n                                      'batch size']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results of the model selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# Greedy brute force\nerrors = []\nfor params in tqdm(parameters_list):\n    \n    model = arch(params, train_sc)\n    train_pred = model.predict(train_sc, batch_size=params[4])\n    val_pred = model.predict(val_sc, batch_size=params[4])\n    \n    train_error = mean_absolute_error(train_sc, train_pred)\n    val_error = mean_absolute_error(val_sc, val_pred)\n    \n    errors.append(list(params)+[train_error, val_error])\n\n# Sort the parameters by the error value\ndf_errors = pd.DataFrame(errors,\n                         columns=['neurons 1st layer', \n                                  'neurons 2nd layer', \n                                  'neurons 3rd layer', \n                                  'learning rate', \n                                  'batch size', \n                                  'mae train', \n                                  'mae val'])\ndf_errors.sort_values('mae val').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the best model"},{"metadata":{"jupyter":{"outputs_hidden":true},"scrolled":true,"trusted":true},"cell_type":"code","source":"best_params = parameters_list[df_errors.sort_values('mae val').index[0]]\n\nmodel = arch(best_params, train_sc)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(val_sc.shape[1]):\n    plt.figure(figsize=(12,3))\n    plt.plot(StSc.inverse_transform(val_sc)[:, i])\n    plt.plot(StSc.inverse_transform(model.predict(val_sc))[:, i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Health indicator"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"test_residuals = data_sc - model.predict(data_sc)\n\npd.DataFrame(test_residuals, columns=data.columns, index = data.index).plot(figsize=(12,6))\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.title('Residuals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"train_residuals = train_sc - model.predict(train_sc)\nval_residuals = val_sc - model.predict(val_sc)\n\nUCL = pd.DataFrame(val_residuals).abs().sum(axis=1).quantile(0.99)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# Health indicator\npd.DataFrame(test_residuals, index=data.index).abs().sum(axis=1).plot(marker='o', \n                                                                      markersize=2, \n                                                                      alpha=0.2, \n                                                                      figsize=(12,6), \n                                                                      label='Health indicator')\n# Health indicator with the median filter\npd.DataFrame(test_residuals, index=data.index).abs().sum(axis=1).rolling(3).median().plot(marker='o', \n                                                                                          markersize=2, \n                                                                                          alpha=0.7, \n                                                                                          figsize=(12,6),\n                                                                                          label='Smoothed Health indicator')\n\nplt.axvspan(data.index[0], \n            '2019-07-08 18:25', \n            color='green', \n            alpha=0.1, \n            label='Training sample')\nplt.axvspan('2019-07-08 18:25', \n            '2019-07-08 18:35', \n            color='yellow', \n            alpha=0.1, \n            label='Validation sample')\nplt.axvspan('2019-07-08 18:35', \n            data.index[-1], \n            color='red', \n            alpha=0.1, \n            label='Test sample')\n\nplt.axhline(UCL, color='r', label='Upper control limit')\nplt.ylim([0, 4*UCL])\nplt.xlabel('Time')\nplt.ylabel('Health indicator value')\nplt.legend(bbox_to_anchor =(0.8, -0.2), ncol = 3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax.axvspan(\n    data.index[data.index=='2019-07-08 18:39:22'][0],\n    data.index[data.index=='2019-07-08 18:42:32'][0],\n    alpha=0.2, \n    color='red')\nax.axvspan(\n    data.index[data.index=='2019-07-08 18:44:36'][0],\n    data.index[data.index=='2019-07-08 18:46:51'][0],\n    alpha=0.2, \n    color='red')\nax.axvspan(\n    data.index[data.index=='2019-07-08 19:06:57'][0],\n    data.index[data.index=='2019-07-08 19:11:31'][0],\n    alpha=0.2, \n    color='red')\nax.axvspan(\n    data.index[data.index=='2019-07-08 19:14:40'][0],\n    data.index[data.index=='2019-07-08 19:21:16'][0],\n    alpha=0.2, \n    color='red', label='Anomalies (incidents)')\nax.plot(data.index, pd.DataFrame(test_residuals).abs().sum(axis=1), \n        marker='o', markersize=2, alpha=0.2, label='Health indicator')\nax.plot(data.index, pd.DataFrame(test_residuals).abs().sum(axis=1).rolling(3).median(), \n        marker='o', markersize=2, alpha=0.7, label='Smoothed Health indicator')\n\nax.axhline(UCL, color='r', label='Upper control limit')\nax.set_ylim([0, 4*UCL])\nax.set_xlabel('Time')\nax.set_ylabel('Health indicator value')\nplt.legend(bbox_to_anchor =(0.8, -0.1), ncol = 3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can implement and calculate Average Detection Delay (ADD)\n\n$\\text{ADD} = \\frac{1}{|Y|}\\sum_{y \\in Y} ( \\tau_y - \\theta_y )$,\n\nwhere $|Y|$ - total amount of changepoints,  \n$\\tau_y$ - moment of detection,  \n$\\theta_y$ - moment of changepoint (anomaly appearing)."},{"metadata":{},"cell_type":"markdown","source":"### Feature importance calculation"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def feature_importance(residuals, analysis_type=\"collective\", date_from=None, date_till=None, weigh=True):\n    \"\"\"Feature importance calculation\n\n    Parameters\n    ----------\n    residuals : pandas.DataFrame()\n\n    analysis_type : str, \"single\"/\"collective\", \"single\" by default\n\n    date_from : str in format 'yyyy-mm-dd HH:MM:SS', None by default\n\n    date_till : str in format 'yyyy-mm-dd HH:MM:SS', None by default\n\n    weigh : boolean, True by default\n        If analysis_type == \"collective\".\n\n    Returns\n    -------\n    data : pandas.DataFrame().\n    \"\"\"\n    if date_from is None:\n        start = 0\n    if date_till is None:\n        end = -1\n    data = residuals[date_from:date_till].abs().copy()\n\n    if (analysis_type == \"collective\") & (weigh == False):\n        data = data.div(data.sum(axis=1), axis=0) * 100\n        return pd.DataFrame(data.mean(), columns=['Feature importance, %']).T\n    elif (analysis_type == \"collective\") & (weigh == True):\n        data = data.mean().div(data.mean().sum(), axis=0) * 100\n        return pd.DataFrame(data, columns=['Feature importance, %']).T\n    elif analysis_type == \"single\":\n        return data.div(data.sum(axis=1), axis=0) * 100","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"for dates in [['2019-07-08 18:39:22','2019-07-08 18:42:32'],\n              ['2019-07-08 18:44:36','2019-07-08 18:46:51'],\n              ['2019-07-08 19:06:57','2019-07-08 19:11:31'],\n              ['2019-07-08 19:14:40','2019-07-08 19:21:16']]:\n    print(f'Incident since {dates[0]} till {dates[1]}')\n    display(feature_importance(pd.DataFrame(test_residuals, index=data.index, columns=data.columns), date_from=dates[0], date_till=dates[1]))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you for your attention!"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}