{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Brief\n\n#### An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\n#### The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\n#### Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. \n\n#### There are a lot of leads generated in the initial stage but only a few of them come out as paying customers at the last stage. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n\n#### X Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = pd.read_csv('/kaggle/input/lead-scoring-x-online-education/Leads X Education.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target variable\nleads['Converted'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for the number of null values in each column \n\nleads.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for the percentage of null values in each column \n\nround((leads.isnull().sum(axis = 0)/ len(leads.index))*100 , 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we can see some of the columns have substantial number of null or missing values. If we drop all these columns we will lose a lot of information so instead of dropping them, for some of the feature variables we will create a new value as 'Unknown' i.e. not specified to replace null or missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imputing missing values and Dropping columns where imputation is not possible"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the columns 'Asymmetrique Activity Index' and 'Asymmetrique Profile Index' as there is score column for both\n\nleads = leads.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Asymmetrique Activity Score column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Asymmetrique Activity Score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['Asymmetrique Activity Score'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Asymmetrique Activity Score'] = leads['Asymmetrique Activity Score'].fillna('Unknown')\nprint(leads['Asymmetrique Activity Score'].value_counts())\nprint('\\n')\nprint('Number of null values = ',sum(leads['Asymmetrique Activity Score'].isnull()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Asymmetrique Profile Score column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Asymmetrique Profile Score'] = leads['Asymmetrique Profile Score'].fillna('Unknown')\nleads['Asymmetrique Profile Score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Lead Quality column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Lead Quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['Lead Quality'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Lead Quality'].fillna(\"Unknown\", inplace = True)\nleads['Lead Quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Tags column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tags column\nleads['Tags'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['Tags'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Tags'] = leads['Tags'].fillna('Unknown')\nleads['Tags'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['Tags'].isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Country column "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Country column \nsum(leads['Country']=='India')/len(leads.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since maximum number of values in the country columns have \"India\" we are going to create 2 values for the country columns one being 'India' and the other being 'Foreign Country'"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Country'] = leads['Country'].apply(lambda x: 'India' if x=='India' else 'Foreign Country')\nleads['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Total visits column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total visits column\nleads['TotalVisits'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['TotalVisits'].median() #Since the above column has lot of outliers we will impute with the median value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['TotalVisits'].replace(np.NaN, leads['TotalVisits'].median(), inplace =True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Page views per visit column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Page Views Per Visit column null values are similarly imputed using the median values\n\nleads['Page Views Per Visit'].replace(np.NaN, leads['Page Views Per Visit'].median(), inplace =True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Last Activity column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Last Activity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['Last Activity'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Last Activity'].fillna(\"Unknown\", inplace = True)\nleads['Last Activity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Specialization column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Specialization'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['Specialization'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Specialization'].replace('Select', 'Unknown', inplace =True)\nleads['Specialization'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Specialization'].fillna(\"Unknown\", inplace = True)\nleads['Specialization'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### How did you hear about X Education"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['How did you hear about X Education'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will drop this columns as most of the values in this column is 'Select' which does not add any information to our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = leads.drop('How did you hear about X Education', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### What is your current occupation column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['What is your current occupation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['What is your current occupation'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['What is your current occupation'].fillna(\"Unknown\", inplace = True)\nleads['What is your current occupation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### What matters most to you in choosing a course column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['What matters most to you in choosing a course'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['What matters most to you in choosing a course'].isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will drop this column as most of the values in this column belong to one category and others are null"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = leads.drop('What matters most to you in choosing a course', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Lead Profile column"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Lead Profile'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['Lead Profile'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Lead Profile'].replace('Select', 'Unknown', inplace =True)\nleads['Lead Profile'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['Lead Profile'].fillna(\"Unknown\", inplace = True)\nleads['Lead Profile'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### City column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# City column\nleads['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(leads['City'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['City'].fillna(\"Unknown\", inplace = True) # Replacing null values with 'NotSpecified' \nleads['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads['City'].replace('Select', 'Unknown', inplace =True)\nleads['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# re-checking for the percentage of null values in each column \n\nround((leads.isnull().sum(axis = 0)/ len(leads.index))*100 , 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will remove the rows with missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing all the rows with null values\n\nleads = leads.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking again for missing values in the dataframe \n\nround((leads.isnull().sum(axis = 0)/ len(leads.index))*100 , 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in leads.columns:\n    print(col, ':', leads[col].nunique())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prospect ID and Lead Number are the same thing so having both the columsn is redundant so we will drop the Prospect ID column\n\nleads = leads.drop('Prospect ID',axis=1)\n\n# Also a lot of the columns have just one unique value so they are of no use as they do not provide any information so dropping them as well\nleads = leads.drop(['Magazine','Receive More Updates About Our Courses',\n                    'Update me on Supply Chain Content','Get updates on DM Content',\n                    'I agree to pay the amount through cheque'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(leads.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mapping 'Yes' and 'No' to '1' and '0'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mapping(x):\n    return x.map({'Yes':1, 'No':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = ['Search',\n            'Do Not Email',\n            'Do Not Call',\n            'Newspaper Article',\n            'X Education Forums',\n            'Newspaper',\n            'Digital Advertisement',\n            'Through Recommendations',\n            'A free copy of Mastering The Interview']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads[col_list] = leads[col_list].apply(mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating dummy variables for categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating dummy variables for some of the other categorical columns \nleads = pd.get_dummies(leads, columns=['Lead Origin', 'Lead Source', 'Country', 'Last Notable Activity'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummmy variables for the rest of the columns and dropping the level called 'Unknown'\n\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Asymmetrique Activity Score'], prefix='Asymmetrique Activity Score')\nfinal_dummy = dummy.drop(['Asymmetrique Activity Score_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Asymmetrique Profile Score'], prefix='Asymmetrique Profile Score')\nfinal_dummy = dummy.drop(['Asymmetrique Profile Score_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'Last Activity'\ndummy = pd.get_dummies(leads['Last Activity'], prefix='Last Activity')\nfinal_dummy = dummy.drop(['Last Activity_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'What is your current occupation'\ndummy = pd.get_dummies(leads['What is your current occupation'], prefix='What is your current occupation')\nfinal_dummy = dummy.drop(['What is your current occupation_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'Lead Profile'\ndummy = pd.get_dummies(leads['Lead Profile'], prefix='Lead Profile')\nfinal_dummy = dummy.drop(['Lead Profile_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'Specialization'\ndummy = pd.get_dummies(leads['Specialization'], prefix='Specialization')\nfinal_dummy = dummy.drop(['Specialization_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['City'], prefix='City')\nfinal_dummy = dummy.drop(['City_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Lead Quality'], prefix='Lead Quality')\nfinal_dummy = dummy.drop(['Lead Quality_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Tags'], prefix='Tags')\nfinal_dummy = dummy.drop(['Tags_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping the columns for which we have created dummy variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads = leads.drop(['Lead Quality','Asymmetrique Profile Score','Asymmetrique Activity Score','Last Activity', \n                    'What is your current occupation', 'Lead Profile','Specialization','City','Tags'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for outliers in the continuous variables\n\nnumerical = leads[['TotalVisits','Total Time Spent on Website', 'Page Views Per Visit']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(numerical['TotalVisits'])\n\nplt.subplot(2,2,2)\nsns.boxplot(numerical['Total Time Spent on Website'])\n\nplt.subplot(2,2,3)\nsns.boxplot(numerical['Page Views Per Visit'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing outliers using the IQR\n\nQ1 = leads['TotalVisits'].quantile(0.25)\nQ3 = leads['TotalVisits'].quantile(0.75)\nIQR = Q3 - Q1\nleads = leads.loc[(leads['TotalVisits'] >= Q1 - 1.5*IQR) & (leads['TotalVisits'] <= Q3 + 1.5*IQR)]\n\nQ1 = leads['Page Views Per Visit'].quantile(0.25)\nQ3 = leads['Page Views Per Visit'].quantile(0.75)\nIQR = Q3 - Q1\nleads=leads.loc[(leads['Page Views Per Visit'] >= Q1 - 1.5*IQR) & (leads['Page Views Per Visit'] <= Q3 + 1.5*IQR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(leads['TotalVisits'])\n\nplt.subplot(2,2,2)\nsns.boxplot(leads['Total Time Spent on Website'])\n\nplt.subplot(2,2,3)\nsns.boxplot(leads['Page Views Per Visit'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have removed most of the outliers and so we can proceed with model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the head of the dataframe again\nleads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at the info of the dataframe again\nleads.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data into Training and Test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = leads.drop(['Lead Number', 'Converted'], axis = 1)\ny = leads['Converted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round((y.sum()/len(y))*100,2) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see we have 38% conversion rate"},{"metadata":{},"cell_type":"markdown","source":"# Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression model\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family=sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selestion using RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20) # running RFE with 20 variables\nrfe = rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_] # rfe.support_ = false ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Assessing the model with StatsModels"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating a dataframe with the actual churn flag and the predicted probabilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model has about 92% accuracy"},{"metadata":{},"cell_type":"markdown","source":"### Checking VIFs"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Our variables do not have high VIF which is good as it indicates we do not have multicolinearity issues to deal with"},{"metadata":{},"cell_type":"markdown","source":"The variable 'Tags_Diploma holder (Not Eligible)' has high high P-value. So let's start by dropping that."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = col.drop('Tags_Diploma holder (Not Eligible)', 1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the overall accuracy hasn't dropped after dropping the 'Tags_Diploma holder (Not Eligible)' column "},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Checking VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable 'Tags_wrong number given' has very high P-value. So we will drop that"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = col.drop('Tags_wrong number given', 1)\n\n# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again the accuracy hasn't dropped after dropping the 'Tags_wrong number given' feature column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Checking VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable 'Tags_number not provided' has very high P-value. So we will drop that"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = col.drop('Tags_number not provided', 1)\n\n# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again the model accuracy hasn't decreased after removing the variable 'Tags_number not provided'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Checking VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. We can see that now most of our P-values for all our variables are equal to 'Zero' which indicates that these variables are statistically significant so we do not need to drop more feature variables\n#### 2. Also the accuracy of our model hasn't dropped even after removing so many of the feature columns at around 91.6%"},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix \nplt.figure(figsize = (20,10),dpi=200)  \nsns.heatmap(X_train[col].corr(),annot = True)\nplt.show()\n\nplt.savefig('corr.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics beyond simply accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic (RoC) curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, \n                                         drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Area under the ROC curve is 0.97"},{"metadata":{},"cell_type":"markdown","source":"# Finding Optimal Cutoff Point"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\n\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.vlines(x=0.34, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above curve, 0.34 seems to be the optimum point to take as the cutoff probability"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.34 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision and Recall"},{"metadata":{},"cell_type":"markdown","source":"Precision: \nTP / TP + FP"},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2[1,1]/(confusion2[0,1]+confusion2[1,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall: TP / TP + FN"},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2[1,1]/(confusion2[1,0]+confusion2[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(y_train_pred_final.Converted, y_train_pred_final.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train_pred_final.Converted, y_train_pred_final.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train_pred_final.Converted, y_train_pred_final.final_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[col]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding constant for statsmodel\nX_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making prediction on the test set\ny_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred = pd.DataFrame(y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting LeadID to index\ny_test_df['LeadID'] = y_test_df.index\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenating both the prediction and the orginal labels\ny_pred_final = pd.concat([y_test_df, y_pred],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Conversion_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearranging the columns\ny_pred_final = y_pred_final[['LeadID','Converted','Conversion_Prob']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['Predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.34 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\naccuracy_score=metrics.accuracy_score(y_pred_final.Converted, y_pred_final.Predicted)\naccuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_test_set = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.Predicted)\nprint(confusion_test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion_test_set[1,1] # true positive \nTN = confusion_test_set[0,0] # true negatives\nFP = confusion_test_set[0,1] # false positives\nFN = confusion_test_set[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sensitivity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Specificity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### False Postive Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting converion when customer does not have converted\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Positive Predicted Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Negative Predicted Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"#precision\nconfusion_test_set[1,1]/(confusion_test_set[0,1]+confusion_test_set[1,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"#recall\nconfusion_test_set[1,1]/(confusion_test_set[1,0]+confusion_test_set[1,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_pred_final.Converted, y_pred_final.Predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Precision recall curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_pred_final.Converted, y_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the ROC Curve for Test Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic (ROC) curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr,tpr, thresholds\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_pred_final.Converted, y_pred_final.Conversion_Prob, drop_intermediate = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_pred_final.Converted, y_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Area under the ROC curve is around 0.96 which means our model seems to be doing well on the test set as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['Lead Score'] = y_pred_final['Conversion_Prob']*100\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final = pd.merge(leads[['Lead Number']], y_pred_final,how='inner',left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()  # test dataset with all the Lead Score values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_df = y_train_pred_final[['Converted', 'Conversion_Prob', 'LeadID','Predicted']]\ny_train_pred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_df = pd.merge(leads[['Lead Number']], y_train_pred_df,how='inner',left_index=True, right_index=True)\ny_train_pred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_df['Lead Score'] = y_train_pred_df['Conversion_Prob']*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_df.head()     # train dataset with all the Lead Score values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final dataframe with all the Lead Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df_lead_score = pd.concat([y_train_pred_df,y_pred_final],axis=0)\nfinal_df_lead_score.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df_lead_score = final_df_lead_score.set_index('LeadID')\n\nfinal_df_lead_score = final_df_lead_score[['Lead Number','Converted','Conversion_Prob','Predicted','Lead Score']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final dataframe with the Lead Scores for all the LeadID"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df_lead_score.head()  # final dataframe with all the Lead Scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df_lead_score.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Determining Feature Importance of our final model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# coefficients of our final model \n\npd.options.display.float_format = '{:.2f}'.format\nnew_params = res.params[1:]\nnew_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\n\nfeature_importance = new_params\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nfeature_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sorting the feature variables based on their relative coefficient values\n\nsorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Top three variables in your model which contribute most towards the probability of a lead getting converted"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df = pd.DataFrame(feature_importance).reset_index().sort_values(by=0,ascending=False)\nfeature_importance_df = feature_importance_df.rename(columns={'index':'Variables', 0:'Relative coeffient value'})\nfeature_importance_df = feature_importance_df.reset_index(drop=True)\nfeature_importance_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The top 3 variables are:\n1. Tags_Lost to EINS\t\n2. Tags_Closed by Horizzon\n3. Lead Source_Welingak Website"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}