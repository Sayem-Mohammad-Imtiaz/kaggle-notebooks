{"cells":[{"metadata":{"_uuid":"c449922741df5f634270b8db5da0e6bc5d0e1c69"},"cell_type":"markdown","source":"# Predicting NYC apartment's value with open data\n\nAny good property broker in NYC will tell you, apartment valuations are based on a variety of things, most importantly:\n\n1. Recent sales in the building / neighbourhood\n2. Price per sq/ft for recent sales\n3. Renovation status\n4. Views, closeness to subway, # of bedrooms etc. \n\nUnfortunately this data does not come easily. It's available on Streeteasy and other [REBNY](https://www.rebny.com/) members but not downloadable for us data scentists! :)\n\nHere I present an alternative approach to pricing NYC apartments using only public data from [NYC Open Data](https://opendata.cityofnewyork.us/). \n\n**tl;dr** Boldly predict a NYC apartments market value using only public / open data. Check out the [Price Predictor](https://colab.research.google.com/github/somya/nyc_apt_price_predictor/blob/master/index.ipynb#scrollTo=qvHipZ9AG27O) \n"},{"metadata":{"trusted":true,"_uuid":"680de1ae42ca6fd4c479bd22c919696c398f56b5","_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom scipy.spatial import distance\n\nimport ipywidgets as widgets\nfrom ipywidgets import IntSlider\n\nfrom datascience import *\nfrom datetime import timedelta\nfrom datetime import date\nfrom datetime import datetime\nimport time\n\nfrom IPython.display import display\nfrom IPython.display import HTML\n\n# from __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n%matplotlib inline\nimport matplotlib.pyplot as plots\nplots.style.use('fivethirtyeight')\n\n\nimport locale\n%load_ext line_profiler\nimport os\nimport xlrd as xlrd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bec0f77aed06249f4a628e1ee544eed45e87d82c"},"cell_type":"markdown","source":"# Constants\n\nLet's define some constants to help make our code more readable"},{"metadata":{"trusted":true,"_uuid":"c394d92b9c216ed18444ce88e10eca041f164ec8","_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"raw_directory = \"../input/\"\n\n# Declare column names to allow for auto completion :)\n\nCOL_SALE_DATE = 'SALE DATE'\nCOL_PURCHASE_DATE = 'PURCHASE DATE'\nCOL_SOLD_DATE = 'SOLD DATE'\nCOL_PURCHASE_PRICE = 'PURCHASE PRICE'\nCOL_SOLD_PRICE = 'SOLD PRICE'\nCOL_FULL_ADDRESS = 'FULL ADDRESS'\nCOL_PRICE_CHANGE = 'PRICE CHANGE'\nCOL_PERIOD = 'PERIOD'\n\nCOL_SALE_YEAR = 'SALE_YEAR'\nCOL_SALE_MONTH = 'SALE_MONTH'\nCOL_SALE_PRICE = 'SALE PRICE'\n\n# Daily price change column name\nCOL_DAILY_PRICE_CHANGE = 'DAILY PRICE CHANGE'\n\nCOL_PURCHASE_DATE_SU = 'PURCHASE DATE SU'\nCOL_PURCHASE_PRICE_SU = 'PURCHASE PRICE SU'\nCOL_SOLD_DATE_SU = 'SOLD DATE SU'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa96c30168a7616c9f01dc8daa94178ff12c52b0"},"cell_type":"markdown","source":"# Functions\n\nAlso lets define some functions to assist in our analysis"},{"metadata":{"trusted":true,"_uuid":"866bb057713db5c83e238a3d941457ecd803e85f","_kg_hide-input":true},"cell_type":"code","source":"def standard_units(any_numbers):\n    \"Convert any array of numbers to standard units.\"\n    return (any_numbers - np.mean(any_numbers)) / np.std(any_numbers)\n\n\ndef correlation(t, x, y):\n    return np.mean(standard_units(t.column(x)) * standard_units(t.column(y)))\n\n\ndef slope(table, x, y):\n    r = correlation(table, x, y)\n    return r * np.std(table.column(y)) / np.std(table.column(x))\n\n\ndef intercept(table, x, y):\n    a = slope(table, x, y)\n    return np.mean(table.column(y)) - a * np.mean(table.column(x))\n\n\ndef fit(table, x, y):\n    a = slope(table, x, y)\n    b = intercept(table, x, y)\n    return a * table.column(x) + b\n\n\ndef residual(table, x, y):\n    return table.column(y) - fit(table, x, y)\n\n\ndef scatter_fit(table, x, y):\n    plots.scatter(table.column(x), table.column(y), s=20)\n    plots.plot(table.column(x), fit(table, x, y), lw=2, color='gold')\n    plots.xlabel(x)\n    plots.ylabel(y)\n\n\n# Helpers Functions\n\ndef print_stats(data):\n    '''Prints common stats for a data array'''\n\n    data_mean = np.mean(data)\n    data_std = np.std(data)\n    data_min = min(data)\n    data_max = max(data)\n\n    percent_5 = percentile(5, data)\n    percent_95 = percentile(95, data)\n    percent_1 = percentile(1, data)\n    percent_99 = percentile(99, data)\n\n    percent_25 = percentile(25, data)\n    percent_50 = percentile(50, data)\n    percent_75 = percentile(75, data)\n\n    print(\"Avg:\", data_mean, \"\\tStd:\", data_std, \"\\tMin:\", data_min, \"\\tMax:\", data_max)\n    print(\" 5%:\", percent_5, \"\\t95%:\", percent_95)\n    print(\" 1%:\", percent_1, \"\\t99%:\", percent_99)\n    print(\"25%:\", percent_25, \"\\t50%:\", percent_50, '\\t75%', percent_75)\n\n\ndef print_col_stats(table, col_name):\n    ''' Print the stats For column named'''\n\n    print(col_name, \"Stats\")\n    data = table.column(col_name)\n    print_stats(data)\n\n\ndef draw_hist(table: Table, col_name, offset_percent=0):\n    ''' Draw a histogram for table with an additional offset percent'''\n    data = table.column(col_name)\n    offset_start = percentile(offset_percent, data)\n    offset_end = percentile(100 - offset_percent, data)\n    table.hist(col_name, bins=np.arange(offset_start, offset_end, (offset_end - offset_start) / 20))\n\n\ndef col_stats(table, col_name):\n    ''' Prints state for a column in table'''\n    print_col_stats(table, col_name)\n    draw_hist(table, col_name)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b94172f89d2b8b5679098b3613c7d629eb55c824"},"cell_type":"markdown","source":"# Step 1: Import data\n\nThis kernel uses data available from NYC Open Data:\n\n1. [Annualized Sales Data](https://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page)\n2. [Rolling Sale data](https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page)"},{"metadata":{"trusted":true,"_uuid":"0d9dca3902bf68966a033dd671d152e9a56e8912","_kg_hide-output":true},"cell_type":"code","source":"# List data files and directories in current directory\nexcel_files = os.listdir(raw_directory)\n\n# Select only tje xls files\nexcel_files = [k for k in excel_files if '.xls' in k]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ef1fd75f6cea464e2ea5fd031520ece8bac5755"},"cell_type":"markdown","source":"Unfortunately not all the data files have the same format. Some have the header in row 4, others in row 5. We can check by making sure 'BOROUGH' is the first column in the imported dataset "},{"metadata":{"trusted":true,"_uuid":"04ddd40b912b656268e861b324b57d05a9a5afdd","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Create an data frame to store\nall_sales_data = pd.DataFrame()\n\n# Load individual excel files. \nfor excel_file in excel_files:\n    print(excel_file)\n    \n    # Read excel, Note the headers could in row 4 or row 5 (index=3 or 4). \n    yearly_sales_data = pd.read_excel(raw_directory+excel_file, header=3, encoding='sys.getfilesystemencoding()')\n   \n    # Check if the first column is \"BOROUGH\"\n    if not yearly_sales_data.columns[0].startswith('BOROUGH'):\n        # Otherwise the data starts from row 5.\n         yearly_sales_data = pd.read_excel(raw_directory+excel_file, header=4, encoding='sys.getfilesystemencoding()')\n    \n    yearly_sales_data.rename(columns=lambda x: x.strip(), inplace=True)\n    \n    all_sales_data = all_sales_data.append(yearly_sales_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6485167b3ed76f3a0e76e7f0787d263e8aa2fbd4"},"cell_type":"markdown","source":"Let's review  the data"},{"metadata":{"trusted":true,"_uuid":"d688c33b8d004ba529c216aedfc662150d2e71af"},"cell_type":"code","source":"all_sales_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8c56a92c65369ce220f80854ac63d184114d9f2"},"cell_type":"markdown","source":"## Duplicates"},{"metadata":{"trusted":true,"_uuid":"2d125933662f44a96a4679b03518e4752dd41359","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# Check for duplicate entries\nprint('Duplicate rows:', sum(all_sales_data.duplicated(all_sales_data.columns)))\n\n#Delete the duplicates and check that it worked\nall_sales_data = all_sales_data.drop_duplicates(all_sales_data.columns, keep='last')\nsum(all_sales_data.duplicated(all_sales_data.columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b12b79ca0c6eccbfac52a7100ae29c837e339af"},"cell_type":"markdown","source":"# Step 2. Clean up the data and arrange for analysis"},{"metadata":{"trusted":true,"_uuid":"4c908615ce408a98dff0d10c35e4cec07a65b8b2","_kg_hide-output":true},"cell_type":"code","source":"#SALE DATE is object but should be datetime\nall_sales_data[COL_SALE_DATE] = pd.to_datetime(all_sales_data[COL_SALE_DATE], errors='coerce')\nall_sales_data['APARTMENT NUMBER'] = all_sales_data['APARTMENT NUMBER'].astype(str)\n\n# remove additional whitespace in strings\nall_sales_data = all_sales_data.applymap(lambda x: x.strip() if type(x) is str else x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d3fab0cdcfa9bc79b7600dd8d11f1ecfbbaf41c"},"cell_type":"markdown","source":"Convert the data frame into a datacience Table for analysis"},{"metadata":{"trusted":true,"_uuid":"089abe90b95c7c9d41872533d7313c287d28c338"},"cell_type":"code","source":"all_sales_data = Table.from_df(all_sales_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"933b94df4a33868a9db5bd752721305731de013c"},"cell_type":"markdown","source":"Remove any sales less than $10,000 they are likely to be a property transfer rather than an actual sale that we're interested in."},{"metadata":{"trusted":true,"_uuid":"68596c706bc9fd9c0682e4849f59e1c370b8aa27"},"cell_type":"code","source":"all_sales_data = all_sales_data.where(COL_SALE_PRICE, are.above(10000))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d6fdaf272ea8183dc20ab824cfe5b94a81c3928"},"cell_type":"markdown","source":"Understand the data labels we might be interested in"},{"metadata":{"trusted":true,"_uuid":"f8ba9394e57c5fb4bca9cf5f9e74b36afbdccae4"},"cell_type":"code","source":"all_sales_data.labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08b39874e8e6cd43c7788ee12fa4fff82f021b1f"},"cell_type":"code","source":"# Remove columns we don't actually need. e.g. lot, block etc\n\nall_sales_data = all_sales_data.select(['SALE DATE', 'SALE PRICE', 'ADDRESS','APARTMENT NUMBER', 'YEAR BUILT', 'NEIGHBORHOOD', 'ZIP CODE', 'BUILDING CLASS AT TIME OF SALE', 'BUILDING CLASS CATEGORY'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d61bbdc873f6520452a3fec686a148c5aa2b64b5"},"cell_type":"markdown","source":"## Cleanup Addresses\n\nThe address data here messy, sometimes the address column contains apartment numbers, other times it's seperated into the Apt No column. **Solution:** Let's create a full address column that combines them into a single address\n"},{"metadata":{"trusted":true,"_uuid":"e9a56611702d5184dbeb9dbb97418bae45ddd2b3","_kg_hide-output":true},"cell_type":"code","source":"def combine_address(address, aptNo):\n    \"\"\"Combine the address and Apartment into a single result\"\"\"\n    temp = address.strip()\n    if len(aptNo.strip()) > 0:\n        temp = temp + ', ' + aptNo.strip()\n    return temp\n\nfull_address = all_sales_data.apply(combine_address, ['ADDRESS', 'APARTMENT NUMBER'])\n\n# Add a Full Address column\nall_sales_data =  all_sales_data.with_column(COL_FULL_ADDRESS, full_address)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9a2265a994dceaf63dbfc6ceec872111cffe91e"},"cell_type":"markdown","source":"## Building Codes\nLet's understand  Building Class Codes. What are the most common codes?"},{"metadata":{"trusted":true,"_uuid":"c94a3b08c27f0e05f1273cd8f1b7d7ab242a2b5e"},"cell_type":"code","source":"all_sales_data.group(['BUILDING CLASS AT TIME OF SALE', 'BUILDING CLASS CATEGORY']).sort('count', descending=True).show(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfd90cd1d39a818451f1a8ef3f9498a042986037"},"cell_type":"markdown","source":"Mostly condos and co-ops as expected\n\n**Question** what's R5 -  COMMERCIAL CONDOS? We'll be ignoring these for now for now, Let's pick out the condos for now. \nReference Data: [NYC Building Codes](https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html) \n\nLet's focus on condos for now"},{"metadata":{"trusted":true,"_uuid":"d6e88d277e36f05498b38aea764be42a54622d07","_kg_hide-input":true},"cell_type":"code","source":"condos = all_sales_data.where('BUILDING CLASS AT TIME OF SALE', are.contained_in(\"R1R2R3R4R6\"))\n\n# Spot Check condo data\ncondos.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4611ebc8e74b990f40562cc6eb7c45a436569bfe"},"cell_type":"markdown","source":"Now let's find condos with multiple sales, so we can start to build a picture of how prices have changed over time."},{"metadata":{"trusted":true,"_uuid":"d1daefce3505cffe7523b307c2629a6247f77c8a","_kg_hide-input":true},"cell_type":"code","source":"condos.group(COL_FULL_ADDRESS).sort(1, descending=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cef96388319a943d335e05d54e937290717571bd"},"cell_type":"markdown","source":"Hmm, apt numbers are missing for lot of the  sales. In order to focus on a typical NYC apartment let's ignore anything without an apt number. i.e. anything without a ',' (comma) in the Full Address"},{"metadata":{"trusted":true,"_uuid":"2dd5cf0dd96ec8f03a196a4c3bc4a7d8f2ce6bfd","_kg_hide-input":true},"cell_type":"code","source":"multi_sale_condos = condos.where(COL_FULL_ADDRESS, are.containing(',')).group(COL_FULL_ADDRESS).sort(1, descending=True).where('count', are.above(1))\nmulti_sale_condos","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cecd9a1199219243fe72bdb5ae9f8856a42e7127"},"cell_type":"markdown","source":"There are a lot less records with apt numbers, but still roughly 16K records, enough to proceed.\n\nLet's define a new table for condos with multiple sales. "},{"metadata":{"trusted":true,"_uuid":"eb2c5faceaa5607264e7e5908006b92b37c4d12f","_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"multi_sale_condos = multi_sale_condos.join(COL_FULL_ADDRESS, condos)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e90cdccea30f04bb82f8a22cb40ec7470953a071"},"cell_type":"markdown","source":"## Time period between sales"},{"metadata":{"trusted":true,"_uuid":"fab60ea20395e535eb9102f01a23b70fc996e3a8"},"cell_type":"code","source":"purchase_dates = multi_sale_condos.select(COL_FULL_ADDRESS, COL_SALE_DATE).group([0], min)\nsold_dates = multi_sale_condos.select(COL_FULL_ADDRESS, COL_SALE_DATE).group([0], max)\n\n# Note for the purposes of this analysis, we can ignore any additnal sales between min and max dates\n\n# Spot check data\npurchase_dates.show(5)\nsold_dates.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f6958fc23e392944c0f6260c3c3bb4be46ce89a","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# Update Labels\n\npurchase_dates = purchase_dates.relabel(1, COL_PURCHASE_DATE)\nsold_dates = sold_dates.relabel(1, COL_SOLD_DATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a7193a5b2de3a587860c75373d627ca9c2c5475"},"cell_type":"code","source":"# Join with Condos to get the sale price\npurchase_dates = purchase_dates.join(COL_FULL_ADDRESS, condos, COL_FULL_ADDRESS).where(COL_SALE_DATE, are.equal_to, COL_PURCHASE_DATE)\npurchase_dates = purchase_dates.select( COL_FULL_ADDRESS, COL_PURCHASE_DATE, COL_SALE_PRICE)\n\n\nsold_dates = sold_dates.join(COL_FULL_ADDRESS, condos, COL_FULL_ADDRESS).where(COL_SALE_DATE, are.equal_to, COL_SOLD_DATE)\nsold_dates = sold_dates.select( COL_FULL_ADDRESS, COL_SOLD_DATE, COL_SALE_PRICE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d2c7fecb2643fbe20f6e008b94760429a096cdf"},"cell_type":"code","source":"purchase_dates.show(5)\nsold_dates.show(5)\n\n# Hmm earlier we had 15871 now we have more! Could we have multiple sale records for the same date??","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97901ec1e8bd7be27616589d955fcab931679140"},"cell_type":"markdown","source":"Yep duplicate sales on the same day!"},{"metadata":{"trusted":true,"_uuid":"0957b4db4a1b662742fb274178cae60357aa1095","_kg_hide-input":true},"cell_type":"code","source":"purchase_dates.groups([COL_FULL_ADDRESS, COL_PURCHASE_DATE]).sort(2, descending=True).where(2, are.above(1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42c906488f0d0303cf3555514a45cbdbfdceaceb"},"cell_type":"markdown","source":"Spot check some duplicate addresses to understand what's going on"},{"metadata":{"trusted":true,"_uuid":"f59dd212f78cbad933400054a2e8da013d251ee8","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"condos.where(COL_FULL_ADDRESS, are.equal_to('2 EAST 55 STREET, 921'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab02f3d02117ffa9144fe10ac665d5293327b106"},"cell_type":"markdown","source":"There could be a number of things going on.  Instead of speculating, let's just take the min and max value for each date to keep moving along.\n"},{"metadata":{"trusted":true,"_uuid":"22f0e578667d8509489e9e23a3c9043bf09ee37e","_kg_hide-input":true},"cell_type":"code","source":"purchase_dates = purchase_dates.group([COL_FULL_ADDRESS, COL_PURCHASE_DATE], min)\nsold_dates = sold_dates.group([COL_FULL_ADDRESS, COL_SOLD_DATE], max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4c68efc1d0d65f079464deb603fb7cedfdd49a9","_kg_hide-input":true},"cell_type":"code","source":"# Relabel and join the first and last sale tables to create a new condo sales table\n\npurchase_dates = purchase_dates.relabel(2, COL_PURCHASE_PRICE)\nsold_dates = sold_dates.relabel(2, COL_SOLD_PRICE)\n\ncondo_sales = purchase_dates.join(COL_FULL_ADDRESS, sold_dates, COL_FULL_ADDRESS)\ncondo_sales","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2ecc3dce25c4c8e1d39a169c09e29b90f47079c"},"cell_type":"markdown","source":"## Price Change\nCalculate Price and date diffs"},{"metadata":{"trusted":true,"_uuid":"adf7bc26bbdd9d5890946d8e3176402a03298098","_kg_hide-input":true},"cell_type":"code","source":"price_diffs = condo_sales.column(COL_SOLD_PRICE) - condo_sales.column(COL_PURCHASE_PRICE)\ndate_diffs = condo_sales.column(COL_SOLD_DATE) - condo_sales.column(COL_PURCHASE_DATE)\n\ndate_diffs = [ d.days for d in date_diffs ]\n\ncondo_sales = condo_sales.with_column( COL_PRICE_CHANGE, price_diffs, COL_PERIOD, date_diffs)\ncondo_sales.set_format(COL_PRICE_CHANGE, NumberFormatter())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d792ea334ac3238a737811288af8be99e7defe4c"},"cell_type":"markdown","source":"Let's dive into price change data"},{"metadata":{"trusted":true,"_uuid":"b61ab42a60c903c99a748be13115c1a3f7f7a62f","_kg_hide-input":true},"cell_type":"code","source":"col_stats(condo_sales, COL_PRICE_CHANGE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6395ca2ddad336571095d4387345cfa281b3159e"},"cell_type":"markdown","source":"There are signifiant outliers here. The 99 percentile is 5MM but the max is 44 MM a significant outlier. The min value is -183MM, i.e. a 183 million dollor loss! \n\nLet's remove these outliers so they dont impact pricing analysis, but we definitely need to come back and look into what happened here. "},{"metadata":{"trusted":true,"_uuid":"e0bc9c8d99297173024d27cd304d324e5a90f55a"},"cell_type":"code","source":"# strip out the Price chnage outliers . \npercent_1 = percentile(1, price_diffs)\npercent_99 = percentile(99, price_diffs)\n\nlargest_losses = condo_sales.where(COL_PRICE_CHANGE, are.below_or_equal_to(percent_1))\nlargest_gains = condo_sales.where(COL_PRICE_CHANGE, are.above_or_equal_to(percent_99))\n\ncondo_sales = condo_sales.where(COL_PRICE_CHANGE, are.between(percent_1, percent_99))\ncol_stats(condo_sales, COL_PRICE_CHANGE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b18bb70affa9766d90ee6da9566082d9f2dd79a0"},"cell_type":"markdown","source":"The data is starting to look a lot more reasonable now. Although, there are still some sales that are on recorded twice for the same date!"},{"metadata":{"trusted":true,"_uuid":"31e70a0a3a23bca44efbf13622e1ee465aea0924"},"cell_type":"code","source":"condo_sales.where( COL_PURCHASE_DATE, are.equal_to, COL_SOLD_DATE )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b460c1e7920cb5ef17104572536ea788101029ae"},"cell_type":"code","source":"# Let's spot check these\n\nall_sales_data.where(COL_FULL_ADDRESS, are.equal_to('100 CENTRAL PARK SOUTH, 4B')).sort(0)\n\n# These look to be duplicate records, Let's ignore them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abb396a48f5b93f4bba1e51d4799fa793550b22e","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# Ignore multiple sales on same date\ncondo_sales = condo_sales.where( COL_PURCHASE_DATE, are.not_equal_to, COL_SOLD_DATE )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ecfc14a75a0dd4814502f5ee43af80362a5d876"},"cell_type":"markdown","source":"## Time between sales\n\nLooking into the time between a purchase and sale for the same apartment we find some unusal data. There are at times only 1 day between when a proprty was bought and sold. "},{"metadata":{"trusted":true,"_uuid":"cd9a22ebc7677392db176511d1b8493cfbd1e388"},"cell_type":"code","source":"condo_sales.sort(COL_PERIOD)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f82a1f1eeb69bb52d4e6e61f97952758fc3e4ec"},"cell_type":"markdown","source":"For most regular sale cycles we expect a gap of atleast 60-90 days. Let's ignore anything less that 3 months apart i.e. 90 days. \n"},{"metadata":{"trusted":true,"_uuid":"8661f13ef0fba31fc5116d00a9becd55b956c474"},"cell_type":"code","source":"condo_sales = condo_sales.where( COL_PERIOD, are.above(90) )\ncol_stats(condo_sales, COL_PERIOD)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b03128ebe8700a21f847d46e75923b472bbb744"},"cell_type":"markdown","source":"## Average Daily Price Change\n\nLet's calulate the average daily price change to spot any other odd data. "},{"metadata":{"trusted":true,"_uuid":"bc1878bd38e99d5048907fd52fb434fca68e7640"},"cell_type":"code","source":"daily_change = condo_sales.column(COL_PRICE_CHANGE) / condo_sales.column(COL_PERIOD) \n\ncondo_sales = condo_sales.with_column(COL_DAILY_PRICE_CHANGE , daily_change ).sort(COL_DAILY_PRICE_CHANGE, descending=True)\ncol_stats(condo_sales, COL_DAILY_PRICE_CHANGE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cccb288215f2e8175e0e6b8cf1bbd752fa46de23"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"462d450e5da300e5466019b99c7457cd89c839fc"},"cell_type":"markdown","source":"Again there are some significant outliers. These could be for a variety of reasons . Perhaps they underpriced for when purchased and then corrected when sold later. Again I'd be intresting to investigate further, but for the purposes of this analysis let's ignore the significant outliers. "},{"metadata":{"trusted":true,"_uuid":"a63a96b1dce004f884d378df9598950a28838199","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# strip out the Dailys Price change outliers. \nprice_change_diffs = condo_sales.column(COL_DAILY_PRICE_CHANGE)\n\npercent_1 = percentile(1, price_change_diffs)\npercent_99 = percentile(99, price_change_diffs)\n\ncondo_sales = condo_sales.where(COL_DAILY_PRICE_CHANGE, are.between(percent_1, percent_99))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"425baef526d277065b95572633b267cebb27e589"},"cell_type":"markdown","source":"# Step 3: NYC Sale data analysis"},{"metadata":{"_uuid":"5c632de5f53680e2afa6b488bd1bc6cf9c26ad85"},"cell_type":"markdown","source":"# Understanding the data\n\nLet's try to get an overview of the data by looking at the movement of the average sale price per year."},{"metadata":{"trusted":true,"_uuid":"bd89b67c695f58aff51eeb10ea909c1d83747b92","_kg_hide-input":true},"cell_type":"code","source":"years = [ d.year for d in condos.column(COL_SALE_DATE) ]\n\nmonths = [ d.month for d in condos.column(COL_SALE_DATE) ]\n\ncondos = condos.with_column('SALE_YEAR', years, 'SALE_MONTH', months)\n\ncondo_mean = condos.select(COL_SALE_YEAR, COL_SALE_PRICE).group(COL_SALE_YEAR, np.mean).sort(0)\ncondo_mean.plot(COL_SALE_YEAR)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf785763c20914f5284516aac9d735416b80dfaa"},"cell_type":"markdown","source":"## Sales by neighbourhood \n\nAlso helpful to usederstand how prices have changed in different neighborhoods. "},{"metadata":{"trusted":true,"_uuid":"57703a96fa26a11c96d01135548fe259051d6d7f","_kg_hide-input":true},"cell_type":"code","source":"neighborhoods = condos.group('NEIGHBORHOOD').sort(0).column(0)\n\ndef plot_neighborhood(neighborhood:str):\n    '''Plot the average sale for a specified neign'''\n    condos.where('NEIGHBORHOOD', are.equal_to(neighborhood)).select(COL_SALE_YEAR, COL_SALE_PRICE).group(0, np.mean).plot(0, label=neighborhood)\n    plots.title = neighborhood\n    print(neighborhood)\n    plots.plot(condo_mean.column(0), condo_mean.column(1), color='gold', label=neighborhood )\n    return\n\n# ignore = interact(plot_neighborhood, neighborhood=neighborhoods)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dc4f72aa704b124542dd4d7de81e19317028262"},"cell_type":"code","source":"plot_neighborhood('ALPHABET CITY')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"303c1ab538d8c9dede16deb6591744de5b374785"},"cell_type":"code","source":"plot_neighborhood('MIDTOWN EAST')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e22164936e25b23ad5d6c6bcc144ae305dfd372"},"cell_type":"markdown","source":"## Sampling\n\nLet's take a deeper dive at apartment sales in 2010 as a sample. **Note:** we could have selected any range. This is just a random selection to reduce the noise in the data\n\n"},{"metadata":{"trusted":true,"_uuid":"6c3e0aee399346fd2d732c4709763bbf75e8dedc","_kg_hide-input":true},"cell_type":"code","source":"sales_2010 = condo_sales.where(COL_PURCHASE_DATE, are.between( datetime(2010, 1, 1), datetime(2010, 12, 31)))\n\nTable().with_columns(\n    'PERIOD',  sales_2010.column(COL_PERIOD), \n    'PRICE CHANGE', sales_2010.column(COL_PRICE_CHANGE)\n).scatter(0, 1, fit_line=True)\n\nprint('Correlation betweeen Price Change and Time: ', correlation(sales_2010, COL_PERIOD, COL_PRICE_CHANGE))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e21d41c7b05f56630a2964466e1ed8336ef6a4"},"cell_type":"markdown","source":"That's a low correlation, I was expectating a closer relationship that is roghly keeping tracking with avg overall sale price we plotted earlier.\n\n\nOk, let's look at the correlation between the first and last sale price. "},{"metadata":{"trusted":true,"_uuid":"37af85fe92deeb46c4eaefef75c936a9ab0d9746","_kg_hide-input":true},"cell_type":"code","source":"Table().with_columns(\n    'PURCHASED PRICE',  sales_2010.column(COL_PURCHASE_PRICE), \n    'SOLD PRICE', sales_2010.column(COL_SOLD_PRICE)\n).scatter(0, fit_line=True)\n\nprint('Correlation betweeen Purchase Price and Sold Price: ', correlation(sales_2010, COL_PURCHASE_PRICE, COL_SOLD_PRICE))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd05c86cde2b7a45f549c92304f8c69a76d7d1b8"},"cell_type":"markdown","source":"Wow! that's a really high correlation. According to this we could predict the last sale price of a property, just based on it's first sale price. i.e. independent of the time between sales! \n\nSomehting doesn't seem right, we know market moves over time. Let's dig in a little deeper and plot the residuals."},{"metadata":{"trusted":true,"_uuid":"79c1ef44968bc38b3c1c4b5795a627f03ff23147","_kg_hide-input":true},"cell_type":"code","source":"a = slope(condo_sales, COL_PURCHASE_PRICE, COL_SOLD_PRICE)\nb = intercept(condo_sales, COL_PURCHASE_PRICE, COL_SOLD_PRICE)\n\nfirst_prices = condo_sales.column(COL_PURCHASE_PRICE)\n\npredicted = first_prices * a + b\n\nerrors = condo_sales.column(COL_SOLD_PRICE) - predicted\n\n\nTable().with_columns(\n    'PURCHASE PRICE',  condo_sales.column(COL_PURCHASE_PRICE), \n    'ERRORS', errors\n).scatter(0, fit_line=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f89211dee095211ce83a790cab463fcc3be7016f"},"cell_type":"markdown","source":"The residuals are not evenly spread out, also some large outliers are skewing the results so a liner regression model isn't the right approach here.\n\nLet's take a deeper look into a particular price bands to see what could be going on. "},{"metadata":{"trusted":true,"_uuid":"6b337c15843058f0efcac1de568f16eaf6589ec7"},"cell_type":"code","source":"price_band = condo_sales.where(COL_PURCHASE_PRICE, are.between(750000, 1000000))\n\nprice_band.scatter(COL_PURCHASE_PRICE, COL_SOLD_PRICE, fit_line=True)\nprint('Correlation betweeen Purchase Price and Sold Price for apts between 750K-1MM: ', correlation(price_band, COL_PURCHASE_PRICE, COL_SOLD_PRICE))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdff31fa8fde32306ee9a867752551dedea69cff"},"cell_type":"markdown","source":"Now the correlation is much lower, The data is more spread out. Also a average seems to skew a little higher do the outliers a visible in the chart.\n\n## Removing Outliers\n\nLet's look at the purchase price outliers"},{"metadata":{"trusted":true,"_uuid":"1bc7c3b2728dc9f47c9eaee4a7c0417ac30b9152","_kg_hide-input":true},"cell_type":"code","source":"col_stats(condo_sales, COL_PURCHASE_PRICE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c934f536141d4d8e2c2c86151687b6eb3387223"},"cell_type":"markdown","source":"Let's remove the ouliers so we can get a better picture of the data. "},{"metadata":{"trusted":true,"_uuid":"1cb07f27cd3e6348c5287b7f3e4a356ce9227912","_kg_hide-input":true},"cell_type":"code","source":"purchase_prices = condo_sales.column(COL_PURCHASE_PRICE)\n\npercent_1 = percentile(1, purchase_prices)\npercent_99 = percentile(99, purchase_prices)\n\ncondo_sales = condo_sales.where(COL_PURCHASE_PRICE, are.between(percent_1, percent_99))\ncol_stats(condo_sales, COL_PURCHASE_PRICE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93fdc262892e916fa09416a26f5b1b4e368cecda"},"cell_type":"markdown","source":"much better!"},{"metadata":{"_uuid":"5daf52059d40a1a5eb855a270f1382077b212996"},"cell_type":"markdown","source":"# Price change % \n\nNow let's calculate the price change as a % of the purchase price. \nLooking into how  the price change % data is distributed, again we need to filter out the outliers."},{"metadata":{"trusted":true,"_uuid":"c3e459d04f5c53deeafac627794effed88983a2b","_kg_hide-input":true},"cell_type":"code","source":"percents = sales_2010.column(COL_PRICE_CHANGE) / sales_2010.column(COL_PURCHASE_PRICE) * 100\n\nCOL_PRICE_PERCENT = 'PRICE CHANGE %'\n\nsales_2010 = sales_2010.with_column(COL_PRICE_PERCENT, percents)\ncol_stats(sales_2010, COL_PRICE_PERCENT)\n# draw_hist(sales_2010, COL_PRICE_PERCENT, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"750b1269e1ea4442377fd0457bc0a566678b5527","_kg_hide-input":true},"cell_type":"code","source":"# strip out the Price Percent change outliers. \n\nprice_changes = sales_2010.column(COL_PRICE_PERCENT)\n\npercent_2 = percentile(2, price_changes)\npercent_98 = percentile(98, price_changes)\n\nsales_2010 = sales_2010.where(COL_PRICE_PERCENT, are.between(percent_2, percent_98))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"057f0bc7a8bb39440cb2171c1e15aeb6c552ace8"},"cell_type":"markdown","source":"Let's look into the correlation betweeen the change in price and the time between sales"},{"metadata":{"trusted":true,"_uuid":"7d90b2c6fb8e084c0d41c6f7726dffea9946a8f5"},"cell_type":"code","source":"sales_2010.scatter(COL_PERIOD, COL_PRICE_PERCENT, fit_line=True)\nprint('Correlation Price Change, Time Period', correlation(sales_2010, COL_PERIOD, COL_PRICE_PERCENT))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65c898ba0e8d30bfd2ea2ea0b146813e13c74f70"},"cell_type":"markdown","source":"There look to be an upward trend here and price changes increase over time. However, the correlation isn't linear. Smoothing the data into monthly (30) intervals look like the following"},{"metadata":{"trusted":true,"_uuid":"af73631e604f050447fb0875cc41afdaf2f04abd","_kg_hide-input":true},"cell_type":"code","source":"\nperiods = sales_2010.column(COL_PERIOD)\n\nmin_period = min(periods)\nmax_period = max(periods)\n\n\nperiod_groups = []\nperiod_sales = []\n\nfor i in np.arange(min_period, max_period, 30 ):\n    period_groups.append(i)\n    period_sales.append(np.mean(sales_2010.where(COL_PERIOD, are.between(i, i+30)).column(COL_PRICE_PERCENT)))\n    \n\nTable().with_columns(\n    COL_PERIOD,  period_groups, \n    COL_PRICE_PERCENT, period_sales\n).scatter(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e8f5d85db39aa1b95c7480c304835b76dc778e9"},"cell_type":"markdown","source":"Defintitely shows a trend! \n\nLet's take a look at other years"},{"metadata":{"trusted":true,"_uuid":"3d865de7f39cf568ab8de98a6b3eab624e1cc237","_kg_hide-input":true},"cell_type":"code","source":"# Trim the outliers. \npercents = condo_sales.column(COL_PRICE_CHANGE) / condo_sales.column(COL_PURCHASE_PRICE) * 100\n\ncondo_sales = condo_sales.with_column(COL_PRICE_PERCENT, percents)\n\npercent_1 = percentile(1, percents)\npercent_99 = percentile(99, percents)\n\ncondo_sales = condo_sales.where(COL_PRICE_PERCENT, are.between(percent_1, percent_99))\n\ndef plot_price_change_year(year):\n    ''' Plot the price change % for a given year'''\n    valid_sales = condo_sales.where(COL_PURCHASE_DATE, are.between_or_equal_to( datetime(year, 1, 1), datetime(year+1, 1, 1)))\n\n    min_period = min(periods)\n    max_period = max(periods)\n\n\n    period_groups = []\n    period_sales = []\n\n    for i in np.arange(min_period, max_period, 30 ):\n        period_groups.append(i)\n        period_sales.append(np.mean(valid_sales.where(COL_PERIOD, are.between(i, i+30)).column(COL_PRICE_PERCENT)))\n\n    Table().with_columns(\n        COL_PERIOD,  period_groups, \n        COL_PRICE_PERCENT, period_sales\n    ).scatter(0)\n\n# Removed interact for published\n# _ = interact(plot_price_change_year, year=np.arange(2003,2019) )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"233f8419a618ccd60d1c9c4caebad32af281cfcf"},"cell_type":"code","source":"print('2007')\nplot_price_change_year(2007)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76d1d0ccf5ed520e69a3520134a69a3f83643b5e"},"cell_type":"code","source":"print('2013')\nplot_price_change_year(2012)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bb179991f82f063fbfa84cf11ced6bdece8c6e9"},"cell_type":"markdown","source":"We can now say there is a a correlation between the price change and time reflective of the overall movement in the market over time. \n\nLet's make some predictions. \n\n# Step 4: Predicting NYC property prices"},{"metadata":{"_uuid":"8373a0569ab6c71dab330ae6dd86c5f0f1e60907"},"cell_type":"markdown","source":"## Prediction model: nearest neighbor\n\nOur prediction model should use both the purchase price and date for prediction. Let's create it now.\n\nWe will first convert purchase date, price and sold dates into standard units so they can be used to compute distance. \n\nThen we will create the training and testing set."},{"metadata":{"trusted":true,"_uuid":"53a13ccfb305ed58cf40b5207c6dbb59caed82d6"},"cell_type":"code","source":"# Convert to standard units\npurchase_dates_timestamps = [ date.timestamp() for date in condo_sales.column(COL_PURCHASE_DATE)]\npurchase_dates_su = standard_units(purchase_dates_timestamps)\n\nsold_dates_timestamps = [ date.timestamp() for date in condo_sales.column(COL_SOLD_DATE)]\nsold_dates_su = standard_units(sold_dates_timestamps)\n\npurchase_price_su = standard_units(condo_sales.column(COL_PURCHASE_PRICE))\n\ncondo_sales_su = condo_sales.with_column(\n    COL_PURCHASE_DATE_SU, purchase_dates_su, \n    COL_PURCHASE_PRICE_SU, purchase_price_su, \n    COL_SOLD_DATE_SU, sold_dates_su,\n)\n\n# Create the training and testing sets.\ntraining_sales, test_sales = condo_sales_su.split(int(condo_sales.num_rows * 0.6))\ntraining_sales\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f0cb05e4e5e4cf4a9e53999d641b719ccd067eb","_kg_hide-input":true},"cell_type":"code","source":"# Columns used to the calculate the distance between two point i.e. 2 properties that were purchased and sold. \n# We picked the purchase date & price and the sold date converted to standard units as the important columns\n# to use for calculating the distance.\ndistance_columns = [COL_PURCHASE_DATE_SU, COL_PURCHASE_PRICE_SU, COL_SOLD_DATE_SU]\n\ndef all_distances(training, new_point):\n    \"\"\"Returns an array of distances\n    between each point in the training set\n    and the new point (which is a row of attributes)\"\"\"\n    attributes = training.select(distance_columns)\n    return distance.cdist( attributes.to_array().tolist(), [new_point]).flatten()\n\ndef table_with_distances(training, new_point):\n    \"\"\"Augments the training table \n    with a column of distances from new_point\"\"\"\n    return training.with_column('Distance', all_distances(training, new_point))\n\ndef closest(training, new_point, k):\n    \"\"\"Returns a table of the k rows of the augmented table\n    corresponding to the k smallest distances\"\"\"\n    with_dists = table_with_distances(training, new_point)\n    sorted_by_distance = with_dists.sort('Distance')\n    topk = sorted_by_distance.take(np.arange(k))\n    return topk\n\ndef estimate(training, purchase_point, k):\n    \"\"\"Estimate a price based on nearest neighbours\"\"\"\n    close_points = closest(training_sales, purchase_point, k)\n    avg_price_change = np.mean(close_points.column(COL_PRICE_PERCENT))\n    return avg_price_change\n\ndef evaluate_accuracy(training, test, k):\n    \"\"\"Evalute the accuracy of the model generating using training data on test data\"\"\"\n    # select the columns to compare\n    test_attributes = test.select(distance_columns)\n\n    # compute the predicted change for each test row\n    def price_testrow(row):\n        return estimate(training, row, k)\n\n    # Calculate the predicted price and error\n    c = test_attributes.apply(price_testrow)\n    \n    estimated = test[COL_PURCHASE_PRICE] * (100 + c )/100\n    error = (test[COL_SOLD_PRICE] - estimated  ) / test[COL_SOLD_PRICE] * 100\n    \n    return test.with_column(\"Estimated\", estimated, 'Error %', error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f04f26e48fc0cc47c8030c9f6c846f696a3922e"},"cell_type":"code","source":"estimates = evaluate_accuracy(training_sales, test_sales, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae33326c23fd81f530cdb6e0e292c2aef4b76fc1"},"cell_type":"markdown","source":"Let's look that how this algorithm performs"},{"metadata":{"trusted":true,"_uuid":"b7ba943c17c5fa52b3f72e1ea6f8fe682bd69e23"},"cell_type":"code","source":"estimates.scatter(COL_SOLD_PRICE, 'Error %')\n\ncol_stats(estimates, \"Error %\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1e4444a4e581b4bea03c9f1c8157b7d379e0b23"},"cell_type":"markdown","source":"**Not bad!** 90% of the time we're withing -37% to 28% of the sale price"},{"metadata":{"_uuid":"eae7b721906798b03ba6046caec371082a17415b"},"cell_type":"markdown","source":"# Price Predictor\n\nCheck out the interactive  [Price Predictor](https://colab.research.google.com/github/somya/nyc_apt_price_predictor/blob/master/index.ipynb#scrollTo=qvHipZ9AG27O) based on this approach. **Note** You will need to run the linked notebook using your google account."},{"metadata":{"_uuid":"fd04c2e399ee94367a99fd7900bccac76ebc18c4"},"cell_type":"markdown","source":"# Further Explorations\n\nIt would be interesting furhter explore this data by:\n\n1. Applying different weights to the distanace calculations. e.g. The purchase price is  more important than dates. \n2. Including physical location when calculating the distance between two properties. \n\nAny other suggestions or thoughts? Please let me know in the comments section below. \n\nThank you for reading, please upvote! :) "},{"metadata":{"trusted":true,"_uuid":"044e3a43ac71b1147942915a02792292c6d74ac3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}