{"cells":[{"metadata":{},"cell_type":"markdown","source":"**I will be using the Aotizhongxin dataset (the first dataset among the ones I have uploaded) in this kernel for analysis. I will print the outputs of only the first 5 entries for all arrays and lists else you guys have to scroll through a lot to reach the next cell and it also takes a lot of time to commit. If you want to print all the entries, you can fork this kernel and just run a loop until the length of the array or list and print all entries. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns;\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading the data into the dataframe\ndf = pd.read_csv('/kaggle/input/air-quality-of-cities-in-china/PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv')\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# viewing info about the columns\ndf.info();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viewing few rows from the top\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of rows and columns in the dataset\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#statistical information about columns\nprint(df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking how many null values are in each column\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping all the rows with NaN values\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have dropped all null values, there should be no rows with NaN values. Let's check using the below command and as we can see, there are 0 null values in each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining training and testing data\nx_train = df[:24865]\ny_train = x_train['PM2.5']\nx_test = df[24865:31898]\ny_test = x_test['PM2.5']\nprint(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many pollutants. Let's first try to predict PM2.5 concentration values. Let the years 2016 and 2017 be the testing set. As you can see below, these 2 years account for 20.06% of the data (test set)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[24865:31898].count() / df.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize training data\ntrain_norm = x_train['PM2.5'] \ntrain_norm_arr = np.asarray(train_norm)\ntrain_norm = np.reshape(train_norm_arr, (-1, 1))\nscaler = MinMaxScaler(feature_range=(0, 1))\ntrain_norm = scaler.fit_transform(train_norm)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even after normalization and scaing, null values are possible (many people disregard this). Let's check if any null values are present."},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor i in range(len(train_norm)):\n    if train_norm[i] == 0:\n        count = count +1\nprint('Number of null values in train_norm = ', count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It says 0 because after running the below cell, I ran the above cell. I had to this because I forgot to add a print statement. There were 291 null values and we had to get rid of this. Else, it's gonna be a problem while training."},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing null values \ntrain_norm = train_norm[train_norm!=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_norm = x_test['PM2.5']\ntest_norm_arr = np.asarray(test_norm)\ntest_norm = np.reshape(test_norm_arr, (-1, 1))\nscaler = MinMaxScaler(feature_range=(0, 1))\ntest_norm = scaler.fit_transform(test_norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's repeat the same procedure for test_norm."},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor i in range(len(test_norm)):\n    if test_norm[i] == 0:\n        count = count + 1 \nprint('Number of null values in test_norm = ', count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"There are 86 null values and we have to get rid of them as below."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_norm = test_norm[test_norm != 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this is a time seris data, we should be predicting the values after looking at a set of values rather than just a single value like we usually do. This takes into account the correlation between the data points and the timestamps. Because the neighbours should be considered for how the values change over time. Let's define a function to do this."},{"metadata":{},"cell_type":"markdown","source":"The below function called split_sequence splits the sequence into sets of n values. This n is given as n_steps (step_size). For example, if n=3, we split the sequence in groups of 3. We create 2 empty lists and append the split sequences."},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix > len(sequence)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X),array(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the number of features = 1 as we will be predicting a single value. Let's reshape the split sequences into the format of number of rows, number of columns. (shape[0], shape[1]). In the output, we can see that groups of 3 since n_steps = 3 have been obtained."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = 3\nX_split_train, y_split_train = split_sequence(train_norm, n_steps)\nn_features = 1\nX_split_train = X_split_train.reshape((X_split_train.shape[0], X_split_train.shape[1], n_features))\nfor i in range(1):\n    print(X_split_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see below that, we predict the value for the first 3 values, then consider that output as one of the 3 values in the next set.\nFor example, we preedict 0.1 first, then we take that 0.1 as input in the second set and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_split_test, y_split_test = split_sequence(test_norm, n_steps)\nfor i in range(5):\n    print(X_split_test[i], y_split_test[i])\nn_features = 1\nX_split_test = X_split_test.reshape((X_split_test.shape[0], X_split_test.shape[1], n_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define our neural network (LSTM: Long Short Term Memory). Let's add 50 nodes in our first layer with a ReLU (Rectified linear unit) activation. Their shape will be step size, number of features. Then we will add, a dense layer with one node for the output."},{"metadata":{},"cell_type":"markdown","source":"We can try out different optimizers to see which minimizes loss and maximizes accuracy. Stochastic gradient descent (SGD), Adam, AdaBoost, RMSProp are few of them. lr = learning rate, decay = by how much to decay the learning rate, momentum = how much should the gradient descent be accelerated to dampen oscillations, nesterov = whether to use nesterov momentum. Nesterov has stronger convergence for convex functions. And then we compile using MSE (mean squared loss) as our loss function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\n#sgd = optimizers.SGD(lr=0.001, decay=1e-5, momentum=1.0, nesterov=False)\nsgd = optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True) #good\n#keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9)\nkeras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\nmodel.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nhist = model.fit(X_split_train, y_split_train, validation_data=(X_split_test, y_split_test), epochs=10, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(hist.history.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make our predictions using model.predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = model.predict(X_split_test)\nfor i in range(5):\n    print(yhat[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse = mean_squared_error(y_split_test, yhat)\nprint('MSE: %.5f' % mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, I have plotted the actual true values (first plot) and preedicted values (second plot). One can visually see that the distribution is almost the same. This says that our predictions are very accurate."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_split_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, train_acc = model.evaluate(X_split_train, y_split_train, verbose=0)\n_, test_acc = model.evaluate(X_split_test, y_split_test, verbose=0)\nprint('Train: %.6f, Test: %.5f' % (train_acc, test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above, accuracy increase a lot in the last few epochs. Below, the loss gradually decrease. These are positive signs that our model is doing very good."},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Until now, we just ran our model for prediction of a single pollutant. We have 6 pollutants in our dataset and can make predictions for all of them. So, I have made a function which can be used to predict the other pollutants rather than having to write the code again and again. I have commented the function calls. You can fork this kernel to uncomment and predit the other pollutants (Coz it would take up a lot of space and time)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute(var):\n    train_norm = x_train[var] \n    train_norm_arr = np.asarray(train_norm)\n    train_norm = np.reshape(train_norm_arr, (-1, 1))\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    train_norm = scaler.fit_transform(train_norm)\n\n    test_norm = x_test[var]\n    test_norm_arr = np.asarray(test_norm)\n    test_norm = np.reshape(test_norm_arr, (-1, 1))\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    test_norm = scaler.fit_transform(test_norm)\n\n    X_split_train, y_split_train = split_sequence(train_norm, n_steps)\n    X_split_train = X_split_train.reshape((X_split_train.shape[0], X_split_train.shape[1], n_features))\n\n    X_split_test, y_split_test = split_sequence(test_norm, n_steps)\n    X_split_test = X_split_test.reshape((X_split_test.shape[0], X_split_test.shape[1], n_features))\n\n    hist = model.fit(X_split_train, y_split_train, validation_data=(X_split_test, y_split_test), epochs=10, verbose = 1)\n\n    yhat = model.predict(X_split_test)\n\n    mse = mean_squared_error(y_split_test, yhat)\n    print(mse)\n    \n    plt.plot(hist.history['accuracy'])\n    plt.plot(hist.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    plt.plot(hist.history['loss'])\n    plt.plot(hist.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \n# compute('PM2.5')\n# compute('PM10')\n# compute('SO2')\n# compute('NO2')\n# compute('CO')\n# compute('O3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, we will do a lot of visualizations to understand our data using various scatterplots, jointplots, pairplots, heatmap and correlation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#jointplot for PM2.5 concentration and PM10 concentration\nsns.jointplot(x=df['PM2.5'], y=df['PM10'], data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot gives us the idea that these two conentrations are positively correlated with very few outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding correlation\ncorrmat = df.corr()\nfig, ax = plt.subplots(figsize=(11,11))\n\n#Heatmap\nsns.heatmap(corrmat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To generate pairplots for all features.\ng = sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#density plots\ndf.plot(kind='density', subplots=True, layout=(4,4), sharex=False, figsize=(10,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plots\ndf.plot.scatter(x='PM2.5', y='PM10', c='DarkBlue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_split_test, yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot.scatter(x='PM10', y='SO2', c='DarkBlue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot.scatter(x='SO2', y='NO2', c='DarkBlue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot.scatter(x='NO2', y='CO', c='DarkBlue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot.scatter(x='CO', y='O3', c='DarkBlue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmap is a very useful visualization tool to know how much each feature is correlated. \nvmax = max value of the heatmap\nfmt = number of decimal places upto which the value is shown\nsquare = do you want the heatmap to be square shaped\nlinewidth = width of the lines in the heatmap\nannot = should the boxes be labelled with the value"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = df.corr()\nfig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Don't forget to upvote if you learnt and enjoyed this notebook as much as I did and share this kernel with your friends!!! Bye.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}