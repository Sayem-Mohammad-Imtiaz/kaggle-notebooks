{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing required libraries for visualization\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the data set\ndf = pd.read_csv('/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sneak Peek over the dataset \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing EDA"},{"metadata":{},"cell_type":"markdown","source":"Step 1: Handling missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset has 4238 number of rows and 16 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column education, currentSmoker, cigsPerDay, BPMeds, totChol, BMI, heartRate, glucose has missing values which has to be handled. Below output shows how much of the individual columns has missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handling the missing values by filling their respective mean / median / mode values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['education'].fillna(1,inplace=True)\ndf['cigsPerDay'].fillna(df['cigsPerDay'].median(),inplace=True)\ndf['BPMeds'].fillna(0,inplace=True)\ndf['totChol'].fillna(df['totChol'].mean(),inplace=True)\ndf['BMI'].fillna(df['BMI'].mean(),inplace=True)\ndf['heartRate'].fillna(df['heartRate'].mean(),inplace=True)\ndf['glucose'].fillna(df['glucose'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the missing values has been handled "},{"metadata":{},"cell_type":"markdown","source":"Step 2: Handling Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.boxplot(data=df,x='heartRate',whis=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the outliers\ndf[df['heartRate']>125]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([339,358,3142],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3: Handling Skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting categorical and numerical data\ndf_num = df[['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']]\ndf_cat = df[['male', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes','TenYearCHD']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew\nfor col in df_num:\n  try:\n    print(col,\"=\",skew(df_num[col]))\n    sns.distplot(df_num[col])\n    plt.show()\n  except:\n    pass\n  finally:\n    print(\"**********************************************\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As all the column has positive skewness, with least correlation with the Target Variable\nNone of the columns have negative values, hence handling skewness for all the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num['cigsPerDay'] = np.sqrt(df_num['cigsPerDay'])\ndf_num['totChol'] = np.sqrt(df_num['totChol'])\ndf_num['sysBP'] = np.log(df_num['sysBP'])\ndf_num['diaBP'] = np.sqrt(df_num['diaBP'])\ndf_num['BMI'] = np.sqrt(df_num['BMI'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column glucose has highly skewed data. As it has a higher correlation with diabetes, considering diabetes feature for model prediction and hence excluding glucose from dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num.drop('glucose',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concatenating both categorical and numerical dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = pd.concat([df_num,df_cat],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing scaling over the dataset using Min-Max Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset before performing scaling \ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfor col in df_new:\n  mm = MinMaxScaler()\n  df_new[col] = mm.fit_transform(df_new[[col]])\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling and Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing the dataset into train and test data\nfrom sklearn.model_selection import train_test_split\n\nx = df_new.drop('TenYearCHD',axis=1)\ny = df_new['TenYearCHD']\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing Logistic Regression on the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogr = LogisticRegression()\nlogr.fit(x_train,y_train)\n\ny_hat = logr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing few metrics to check model performace\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import(accuracy_score, recall_score, precision_score, f1_score)\ncm = confusion_matrix(y_test,y_hat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix \nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score: \",accuracy_score(y_test, y_hat))\nprint(\"Recall Score: \",recall_score(y_test, y_hat))\nprint(\"Precision Score: \",precision_score(y_test, y_hat))\nprint(\"F1 Score: \",f1_score(y_test, y_hat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting ROC-AUC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_test,y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, threshold = roc_curve(y_test,y_hat)\nplt.plot(fpr,tpr,'r-',label=\"Logistic Model\")\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ROC-AUC score for Logistic regression is pretty low "},{"metadata":{},"cell_type":"markdown","source":"Checking the performance using Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ndt.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing feature selection using ANNOVA Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import SelectKBest\nannova = SelectKBest(score_func=f_regression,k=10)\nannova.fit(x_train,y_train)\nx_train_annova = annova.transform(x_train)\nx_test_annova = annova.transform(x_test)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train_annova, y_train)\ny_hat_annova = lr.predict(x_test_annova)\nprint(\"Bias = \",lr.score(x_train_annova,y_train))\nprint(\"Variance = \",lr.score(x_test_annova,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test,y_hat_annova)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cm)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}