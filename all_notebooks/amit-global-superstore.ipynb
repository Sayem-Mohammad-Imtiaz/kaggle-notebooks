{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What all business questions shall we consider here?\n* Analysis of various business entities like :\n    * Customer Analysis\n    * Sales Analysis\n    * Order Analysis\n    * Product Analysis etc.\n* Analysis of the above entities across dimensions like : \n    * Time Hierarchy\n    * Geographical Hierarchy\n    * Product Hierarchy etc.\n* Analysis of KPIs (Key Performance Indicators) like :\n    * Sales\n    * Profits\n    * Customer Retention Rate\n    * On-Time Delivery\n    * Return Rate\n    * Inventory Turns\n    * Days in Inventory etc.\n    "},{"metadata":{},"cell_type":"markdown","source":"## Brainstorm on what metrics can be created to build an appealing storyline\n* Sales value($)\n* Sales Volume\n* Sales CAGR\n* Footfalls\n* Transactions\n* Profit Margin(%)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# read the dataset\ndf = pd.read_csv('../input/superstore-data/superstore_dataset2011-2015.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# without encoding error\ndf = pd.read_csv('../input/superstore-data/superstore_dataset2011-2015.csv',encoding = \"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fetch the customer level details :\n\ndf_customer = df[['Customer ID','Customer Name', 'Segment']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_customer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fetch details at Customer and Order Level : \n\ndf_customer_order = df[['Customer ID','Order ID','Order Date', 'Ship Date', 'Ship Mode',]].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_customer_order.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new feature describing the time taken between placing of an order and \n# shipment of the same\n\ndf_customer_order['Order_to_Ship_Days'] = (pd.to_datetime(df_customer_order['Ship Date']) \n                                           - pd.to_datetime(df_customer_order['Order Date'])).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_customer_order.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating average order-to-ship time(in days) at Customer level\n\ndf_customer_days = df_customer_order.groupby('Customer ID')['Order_to_Ship_Days'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving this information as a Dataframe\n\ndf_customer_days = df_customer_days.to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the column name correctly\n\ndf_customer_days.columns = ['Avg_Order_to_Ship_Days']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_customer_days.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grp_customer = df.groupby(['Customer ID'])\ndf1 = grp_customer['Order ID','Sales'].agg({'Order ID':np.size,'Sales':np.sum})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a function which will do all the aggregation of metrics at Customer level : \n\ndef agg_customer(x):\n    d = []\n    d.append(x['Order ID'].nunique())\n    d.append(x['Sales'].sum())\n    d.append(x['Shipping Cost'].sum())\n    d.append(pd.to_datetime(x['Order Date']).min())\n    d.append(pd.to_datetime(x['Order Date']).max())\n    d.append(x['City'].nunique())\n    return pd.Series(d, index=['#Purchases','Total_Sales','Total_Cost','First_Purchase_Date','Latest_Purchase_Date','Location_Count'])\n\ndf_agg = df.groupby('Customer ID').apply(agg_customer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the names of the new aggregated dataframe\n\ndf_agg.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new features on top of the aggregated dataframe already created above : \n\nfrom datetime import datetime\ndf_agg['Duration'] = (df_agg['Latest_Purchase_Date'] - df_agg['First_Purchase_Date']).dt.days\ndf_agg['Frequency'] = df_agg['Duration']/df_agg['#Purchases']\ndf_agg['Days_Since_Last_Purchase'] = df_agg['Latest_Purchase_Date'].apply(lambda x: datetime.strptime('2016-01-01', \"%Y-%m-%d\") - x).dt.days\ndf_agg['Sales_Contribution'] = (df_agg['Total_Sales']/df_agg['Total_Sales'].sum())\ndf_agg['Average_Basket_Value'] = df_agg['Total_Sales']/df_agg['#Purchases']\ndf_agg['CLTV'] = df_agg['Total_Sales'] - df_agg['Total_Cost']\ndf_agg.sort_values(by=\"Latest_Purchase_Date\",ascending = False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's do some product analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Product ID'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  We would like to understand which products are high priced ones, medium and low priced ones \n\ndf_prod = df.groupby(['Product ID'])['Quantity','Sales'].agg(np.sum)\ndf_prod['Average_Price_Point'] = df_prod['Sales']/df_prod['Quantity']\ndf_prod['Price_Point_Perc_Rank'] = df_prod['Average_Price_Point'].rank(pct=True)\ndf_prod['Ticket_Type'] = df_prod['Price_Point_Perc_Rank'].apply(lambda x: 'High' if x>0.7 else ('Medium' if (x>0.4 and x<=0.7) else 'Low'))\ndf_prod.sort_values(by='Price_Point_Perc_Rank',ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check count of rows by Ticket Type :\n\ndf_prod['Ticket_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we join the base table with the Product level table to bring in\n# the Ticket_Type column as a part of the base table\n\ndf_with_ticket = pd.merge(df,df_prod,left_on='Product ID',right_on=df_prod.index,how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_with_ticket.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we create a pivot table with Customer ID in rows\n# and Ticket type in columns\n# and count of rows as values\ndf_tickettype_pivot = pd.pivot_table(df_with_ticket[['Customer ID','Ticket_Type']],index=[\"Customer ID\"],\n               columns=[\"Ticket_Type\"],aggfunc=[np.size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fetch column names from level 2 of the Multi-Index\n\ndf_tickettype_pivot_columns = df_tickettype_pivot.columns.get_level_values(1).tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tickettype_pivot.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge customer aggregated data with Ticket type data \ndf_customer_tickettype = pd.merge(df_agg,df_tickettype_pivot,on = 'Customer ID',how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_agg.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_customer_tickettype.columns = df_agg.columns.tolist() + df_tickettype_pivot_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_customer_tickettype.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bringing in customer meta-data\ndf_total_customer = pd.merge(df_customer_tickettype,df_customer,on='Customer ID',how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total_customer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add average Order-to-Ship days to the dataset\n\ndf_tot_cust_order_final = pd.merge(df_total_customer,df_customer_days,on='Customer ID',how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tot_cust_order_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read returns data\ndf_returns = pd.read_csv('../input/product-returns/Returned orders.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_returns.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# connect with product table to categorize the returned products : \ndf_returns_tickettype = pd.merge(df_returns,df_prod,on='Product ID',how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_returns_tickettype.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cust_prd_tktype = pd.pivot_table(df_returns_tickettype[['Customer ID','Ticket_Type']],index=[\"Customer ID\"],\n               columns=[\"Ticket_Type\"],aggfunc=[np.size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cust_prd_tktype.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cust_prd_tktype.columns = ['High_Returns','Medium_Returns','Low_Returns']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cust_prd_tktype.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cust_prd_tktype.fillna(0,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cust_prd_tktype.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge with the final aggregated table created before to develop the final dataset to work on : \n\ndata_final = pd.merge(df_tot_cust_order_final,df_cust_prd_tktype,on='Customer ID',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final = data_final.fillna({'High':0,\n                                'Medium':0,\n                                'Low':0,\n                                'High_Returns':0,\n                                'Medium_Returns':0,\n                                'Low_Returns':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now, the modelling journey commences!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode the segment column\n\ndf_segment_ohe = pd.get_dummies(data_final['Segment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_segment_ohe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = pd.concat([data_final,df_segment_ohe],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns which are of text type or of no use\n\ndf_clean.drop(['Customer ID','Customer Name','First_Purchase_Date','Latest_Purchase_Date','Segment'\n               ,'Total_Sales','Total_Cost']\n                , axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's split the feature and response variables : \ny = df_clean['CLTV']\nX = df_clean[df_clean.columns.difference(['CLTV'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's split the data now\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's scale the data now\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train_std=sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = X.corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation of features')\n# Draw the heatmap using seaborn\n#sns.heatmap(house_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"PuBuGn\", linecolor='k', annot=True)\nsns.heatmap(X.corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"coolwarm\", linecolor='k', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wow! That was big! It is better to find the most correlated features\nmost_corr_features = corr.index[abs(corr[\"#Purchases\"])>0.6]\nplt.figure(figsize=(15,15))\nsns.heatmap(X[most_corr_features].corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score,mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(X_test_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(y_pred,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_pred,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a lasso regressor\nlasso = Lasso(alpha=0.2, normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X_train_std,y_train)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_\nprint(lasso_coef)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_coeffs = pd.DataFrame()\ndf_coeffs['feature_names'] = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_coeffs['values'] = lasso_coef","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_coeffs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression\nffs = f_regression(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nlreg = LinearRegression()\nrfe = RFE(lreg,5)\nrfe = rfe.fit(X_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe.ranking_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_coeffs['ranking'] = rfe.ranking_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_coeffs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_cols = df_coeffs[df_coeffs['ranking'] == 1]['feature_names']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = X[new_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's split the data now\nfrom sklearn.model_selection import train_test_split\nX_new_train,X_new_test,y_new_train,y_new_test=train_test_split(X_new,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's scale the data now\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_new_train_std=sc.fit_transform(X_new_train)\nX_new_test_std = sc.transform(X_new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_new_train_std,y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_new_pred = lr.predict(X_new_test_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(y_new_pred,y_new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_new_pred,y_new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(y_new_pred,y_new_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}