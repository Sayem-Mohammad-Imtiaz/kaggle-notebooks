{"cells":[{"metadata":{"_uuid":"3ae3ca361241dc976e36259107f9264616ddf25e"},"cell_type":"markdown","source":"![Ethereum Wallet loading screen](https://i.imgur.com/BDs8lGK.png)\n\n# ETHPrize dev interviews analysis\n\n## Context\n\nVarious community members and developers have been interviewing the best developers\nwe know currently working on Ethereum over the last few months.\nWe have interviewed nearly 100 developers and plan on doing many more.\nThe aim is to use all this data being gathered to identify the most important developer tools\nand infrastructure needs our open source community has.\nWe can then fund bounties for teams to actually go out and focus entirely on building the tools\nand infrastructure the network requires to scale and attract the best talent.\nWe already have a bounty out to do the analysis and have someone working on the problem,\nusing some awesome tools that are also open source\nand will hopefully contribute back to the data scientist community in general:\nhttps://github.com/status-im/ETHPrize/issues/14\nHowever, if you'd like to dig in for yourself and show us what you got,\nplease go ahead! As this is ongoing there will be many more interviews coming through.\n\n## Content\nNearly 100 interviews divided into 14 questions\n(1 question is quite general and contains several sub questions depending\non the domain specific knowledge of the person being interviewed).\nNot all interviews get to all questions based on time and connectivity constraints,\nbut most cover a fair number of them.\n\n## Acknowledgements\nWe wouldn't be here without the help of others. <-- That pretty much says it all.\n\nThis is a completely open source, community-led initiative\nApart from a few Status tokens, we're all doing this our of the love of our hearts ;)\n\n## Inspiration\nI would like to see interactive insights for each question. However, I am most interested in\n\n  1. What is causing the biggest frustrations?\n  2. What tools don't exist right now that can and need to be built?\n  3. Where are the best educational resources and how did these developers learn what they know?\n  4. What are the best tools (especially those not often mentioned, and will require some context to identify, people working on it)\n  5. What are the other great bounties we can create to help the whole community?\n\n# Challenges\n\nThis is a pure open-ended text dataset:\n  - no numerical data\n  - no time data\n  - no categorical data\n  - only conversations\n\nIt's also very niche, with lots of Ethereum, dev and crypto jargon, meaning our generic tool will probably struggle.\n\nOh well!\n\n# What you will find\n\nHow to use state of the art Natural Language Processing (Gensim and spaCy) to extract topics from the dataset.\nNote that we use a pretrained model based on general web pages. I'm not even sure crypto and Ethereum was a thing at the time.\nA dedicated Ethereum-focused model would probably have much better results."},{"metadata":{"_uuid":"861ca3a63ac3466cb39f2649fe5d422dc6d87754","toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ETHPrize-dev-interviews-analysis\" data-toc-modified-id=\"ETHPrize-dev-interviews-analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ETHPrize dev interviews analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Context\" data-toc-modified-id=\"Context-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Context</a></span></li><li><span><a href=\"#Content\" data-toc-modified-id=\"Content-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Content</a></span></li><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Acknowledgements</a></span></li><li><span><a href=\"#Inspiration\" data-toc-modified-id=\"Inspiration-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Inspiration</a></span></li></ul></li><li><span><a href=\"#Challenges\" data-toc-modified-id=\"Challenges-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenges</a></span></li><li><span><a href=\"#What-you-will-find\" data-toc-modified-id=\"What-you-will-find-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>What you will find</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-imports\" data-toc-modified-id=\"Basic-imports-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Basic imports</a></span></li><li><span><a href=\"#Loading-data\" data-toc-modified-id=\"Loading-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Loading data</a></span></li><li><span><a href=\"#Setting-up-for-EDA\" data-toc-modified-id=\"Setting-up-for-EDA-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Setting up for EDA</a></span></li><li><span><a href=\"#Q1---What-are-the-tools/libraries/frameworks-you-use?\" data-toc-modified-id=\"Q1---What-are-the-tools/libraries/frameworks-you-use?-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Q1 - What are the tools/libraries/frameworks you use?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-the-data\" data-toc-modified-id=\"Cleaning-the-data-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Cleaning the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-data\" data-toc-modified-id=\"Raw-data-4.4.1.1\"><span class=\"toc-item-num\">4.4.1.1&nbsp;&nbsp;</span>Raw data</a></span></li><li><span><a href=\"#Cleaning-needs\" data-toc-modified-id=\"Cleaning-needs-4.4.1.2\"><span class=\"toc-item-num\">4.4.1.2&nbsp;&nbsp;</span>Cleaning needs</a></span></li><li><span><a href=\"#Clean-text\" data-toc-modified-id=\"Clean-text-4.4.1.3\"><span class=\"toc-item-num\">4.4.1.3&nbsp;&nbsp;</span>Clean text</a></span></li><li><span><a href=\"#Simple-word-cloud\" data-toc-modified-id=\"Simple-word-cloud-4.4.1.4\"><span class=\"toc-item-num\">4.4.1.4&nbsp;&nbsp;</span>Simple word cloud</a></span></li></ul></li></ul></li><li><span><a href=\"#Q2---What-are-your-biggest-frustrations?\" data-toc-modified-id=\"Q2---What-are-your-biggest-frustrations?-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Q2 - What are your biggest frustrations?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-the-data\" data-toc-modified-id=\"Cleaning-the-data-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Cleaning the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-data\" data-toc-modified-id=\"Raw-data-4.5.1.1\"><span class=\"toc-item-num\">4.5.1.1&nbsp;&nbsp;</span>Raw data</a></span></li></ul></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Extracting-the-topics\" data-toc-modified-id=\"Extracting-the-topics-4.5.3\"><span class=\"toc-item-num\">4.5.3&nbsp;&nbsp;</span>Extracting the topics</a></span></li><li><span><a href=\"#Topic-visualization\" data-toc-modified-id=\"Topic-visualization-4.5.4\"><span class=\"toc-item-num\">4.5.4&nbsp;&nbsp;</span>Topic visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#As-text\" data-toc-modified-id=\"As-text-4.5.4.1\"><span class=\"toc-item-num\">4.5.4.1&nbsp;&nbsp;</span>As text</a></span></li><li><span><a href=\"#Interactive-visualization\" data-toc-modified-id=\"Interactive-visualization-4.5.4.2\"><span class=\"toc-item-num\">4.5.4.2&nbsp;&nbsp;</span>Interactive visualization</a></span></li></ul></li><li><span><a href=\"#Cleanup\" data-toc-modified-id=\"Cleanup-4.5.5\"><span class=\"toc-item-num\">4.5.5&nbsp;&nbsp;</span>Cleanup</a></span></li></ul></li></ul></li><li><span><a href=\"#Topic-modelling-at-scale\" data-toc-modified-id=\"Topic-modelling-at-scale-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Topic modelling at scale</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introducing-the-production-functions\" data-toc-modified-id=\"Introducing-the-production-functions-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Introducing the production functions</a></span></li><li><span><a href=\"#Sanity-checks-on-developers-frustrations\" data-toc-modified-id=\"Sanity-checks-on-developers-frustrations-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Sanity checks on developers frustrations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modelization\" data-toc-modified-id=\"Modelization-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Modelization</a></span></li><li><span><a href=\"#Visualization-(text-and-interactive)\" data-toc-modified-id=\"Visualization-(text-and-interactive)-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Visualization (text and interactive)</a></span></li></ul></li><li><span><a href=\"#Let's-try-it-on-tooling-as-well\" data-toc-modified-id=\"Let's-try-it-on-tooling-as-well-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Let's try it on tooling as well</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modelization\" data-toc-modified-id=\"Modelization-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Modelization</a></span></li><li><span><a href=\"#Visualization-(text-and-interactive)\" data-toc-modified-id=\"Visualization-(text-and-interactive)-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Visualization (text and interactive)</a></span></li></ul></li></ul></li><li><span><a href=\"#Analyzing-easier-non-niche-fields\" data-toc-modified-id=\"Analyzing-easier-non-niche-fields-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analyzing easier non-niche fields</a></span><ul class=\"toc-item\"><li><span><a href=\"#Who-are-you-and-what-are-you-working-on?\" data-toc-modified-id=\"Who-are-you-and-what-are-you-working-on?-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Who are you and what are you working on?</a></span></li><li><span><a href=\"#What-are-you-excited-about?\" data-toc-modified-id=\"What-are-you-excited-about?-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>What are you excited about?</a></span></li></ul></li></ul></div>"},{"metadata":{"_uuid":"fecde448bd90873483e084bc9bbfc8fdea43defb"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_uuid":"b8e59999814c7344de68b46f25ad05de0abead03"},"cell_type":"markdown","source":"## Basic imports"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:45.349461Z","start_time":"2018-06-05T15:50:45.21935Z"},"_uuid":"a6c3d789e958f5b81a5d351212a8503b30858ec6","trusted":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd","execution_count":1,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:45.471934Z","start_time":"2018-06-05T15:50:45.350845Z"},"_uuid":"999e74c181b79cda69e7940f4b06743a439fe89c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Visualization\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nimport pprint # pretty printing","execution_count":2,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:46.760113Z","start_time":"2018-06-05T15:50:45.473312Z"},"_uuid":"75042b91bf486f17ecbe602c5a346b517a02e543","trusted":false,"collapsed":true},"cell_type":"code","source":"# Natural language processing\n\nimport spacy\n\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nimport gensim.corpora as corpora\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.models import CoherenceModel\n\nimport pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()","execution_count":3,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:46.763673Z","start_time":"2018-06-05T15:50:46.761433Z"},"_uuid":"f1685b244f8bef0e79c8911341d5bdd659e599f9","trusted":false,"collapsed":true},"cell_type":"code","source":"import warnings # remove all the deprecation and future warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning) ","execution_count":4,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:52.778806Z","start_time":"2018-06-05T15:50:46.765088Z"},"_uuid":"f1708eea790466426789eb9ef674f36e046595f0","trusted":false,"collapsed":true},"cell_type":"code","source":"# Download the model with 'python -m spacy download en_core_web_lg --user'\n# Note: this is a 800MB model\n#       'en_core_web_sm', 29MB can be used as alternative\n#       see https://spacy.io/models/en#section-en_core_web_lg\nnlp = spacy.load('en_core_web_lg')","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"158812e23e4d52c5ec86a10e13bbc13ad5addd30"},"cell_type":"markdown","source":"## Loading data"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:52.788906Z","start_time":"2018-06-05T15:50:52.780064Z"},"_uuid":"d6857b4ad3e02617fb0a0636af03e85647570bd7","trusted":false,"collapsed":true},"cell_type":"code","source":"df = pd.read_csv('../input/ETHPrize Developer Interviews.csv')\n\n# Anonymize answers (interviewees are OK to have their names public)\n# df.drop(columns=['Name'], inplace = True)","execution_count":6,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:52.806135Z","start_time":"2018-06-05T15:50:52.790199Z"},"_uuid":"131299d0290714c51e8c395fc3c4da039786fdc2","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"# View a snippet of the data\ndf.head(3)","execution_count":7,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:52.837845Z","start_time":"2018-06-05T15:50:52.807684Z"},"_uuid":"8966fe6ece336dce5ba3013e9f4d8b9742a02934","trusted":false,"collapsed":true},"cell_type":"code","source":"# View the counts of answers and non-empty answers\ndf.describe()","execution_count":8,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:52.84297Z","start_time":"2018-06-05T15:50:52.838938Z"},"_uuid":"c9a233a76141998c8d6bb04aa31835474b8cb828","trusted":false,"collapsed":true},"cell_type":"code","source":"# What does a full answer look like:\ndf['What are the tools/libraries/frameworks you use?'][0]","execution_count":9,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-01T12:31:56.120813Z","start_time":"2018-06-01T12:31:56.119034Z"},"_uuid":"eb76570cf656a1277faaf6d3a9b5060c0fd2e22c"},"cell_type":"markdown","source":"## Setting up for EDA\n\nLet's start by giving each columns shortnames to ease working on them.\n\nWe will also replace the NaN by blank answers. NaN will trip spaCy otherwise."},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:52.849496Z","start_time":"2018-06-05T15:50:52.844124Z"},"_uuid":"2c3eba53159cfcdf88767968baaa350a76da5a3d","trusted":false,"collapsed":true},"cell_type":"code","source":"df.columns = [\n    'Name',              # Name\n    'tooling',           # What are the tools/libraries/frameworks you use?\n    'frustrations',      # What are your biggest frustrations?\n    'testing',           # How do you handle testing?\n    'smart_contract',    # How do you handle smart contract verif & security?\n    'bounties',          # Other bounties\n    'who_what',          # Who are you and what are you working on?\n    'domain_questions',  # Other domain specific questions?\n    'missing_tools',     # What tools don’t exist at the moment?\n    'easier_expected',   # Was anything easier than expected?\n    'excited_about',     # What are you most excited about in the short term?\n    'hardest_part',      # What was the hardest part to develop with Ethereum?\n    'best_resources',    # What are the best educational resources?\n    'questions_to_ask',  # Are there any other questions we should be asking?\n    'people_talk_to'     # Who do you think we should talk to?\n]\n\ndf.fillna('', inplace = True)","execution_count":10,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:52.865828Z","start_time":"2018-06-05T15:50:52.850588Z"},"_uuid":"6f96e872803947b9a9808677d6a5f10fc9247567","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head(3)","execution_count":11,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-04T08:26:52.65041Z","start_time":"2018-06-04T08:26:52.64855Z"},"_uuid":"1394f1e3bac4612f7f6fb0235425ab40750196fd"},"cell_type":"markdown","source":"## Q1 - What are the tools/libraries/frameworks you use?\n\nWe will use Named Entity Recognition (NER) techniques to extract the tools/libraries/framework names used by devs.\n\nNote that this is quite approximative as the pretrained model used, 'en_core_web_lg', is trained on general english (extracted from Common Crawl) not on devs or crypto focused publications. Still a non-negligible part of the web focus on dev tools/tutorials. See details: https://spacy.io/models/en#en_core_web_lg"},{"metadata":{"_uuid":"87e72a9ad0db49c3aaec27acbce42aed400c9cca"},"cell_type":"markdown","source":"### Cleaning the data\n\n#### Raw data\n\nFirst, let's check on the first 10 answers:\n\n- what does the raw answers look like?\n- can we run naive Named Entity Recognition on it?"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:53.179142Z","start_time":"2018-06-05T15:50:52.86718Z"},"_uuid":"1924aa7d63e1890837a781db2029c8c7345c44a9","trusted":false,"collapsed":true},"cell_type":"code","source":"for i in range(10):\n    print(f\"\"\"\n####################################\ndoc {i}, number of characters: {len(df['tooling'][i])}\n\"\"\")\n    if df['tooling'][i] != '':\n        doc = nlp(df['tooling'][i]) # nlp = spacy.load('en_core_web_lg')\n        print(doc)\ndel doc","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"c9b924d60c4bf296ee9a40332bbd578274c0577b"},"cell_type":"markdown","source":"Wow that is intimidating. We have completely empty answers and answers with 4430 characters. And that's just within the first 10 interviews.\n\nLet's run NER on doc 0, 5 and 8, as they have lots of contents and a mix of various potentially problematic\ncharacters like / - –– -- !! and !!!"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:53.377389Z","start_time":"2018-06-05T15:50:53.180338Z"},"_uuid":"1f203015b564a01ea17a053e3291b48f1ca16e59","trusted":false,"collapsed":true},"cell_type":"code","source":"for i in [0, 5, 8]:\n    print(f\"\"\"\n####################################\ndoc {i}, number of characters: {len(df['tooling'][i])}\n\ndocument | {\"Label\".rjust(20)} | {\"Entity type\".rjust(15)}\"\"\")\n    doc = nlp(df['tooling'][i])\n    for entity in doc.ents:\n        print(f'{i:>8} | {entity.text.rjust(20)} | {entity.label_.rjust(15)}')","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"2d7320fd3b7301c16c7da39d80e1666b66ba6167"},"cell_type":"markdown","source":"#### Cleaning needs\n\n- '/' should be replaced by ', ' to avoid Geth/Parity\n- Carriage returns '\\n' should be replaced by '. '\n- We should keep ORG, PERSON, LOC, PRODUCT\n\nNot catched:\n- doc0: EthereumJS, Unix, EthPM\n- doc5: Visual Studio Code, Solidity, Solium,\n        web3.js, testrpc, react, redux,\n        augur, ethereumjs-blockstream, keythereum\n- doc8: WASM, ethers.js, solidity\n\nConclusions:\n- Lots of tools that doesn't start with an uppercase are missed.\n- Tools that starts with uppercase\n  but begin the sentence are sometimes not recognized\n- The model is not good enough to extract dev tools.\n  (If only we could have a ReCaptcha\n  for Ethereum so that people tag dev tools =) )\n- One way to improve the model would be to:\n  - extract ethereum tagged repos from Github and add them to keywords\n  - idem from Ethereum-related wikis\n  - have proper casing for common stuff like solidity -> Solidity\n  - Some tagger like Prodi.gy https://prodi.gy/ or Aylien https://aylien.com/"},{"metadata":{"_uuid":"a047f18c2ebf8faf5bd8cc55503a5de9d9ee3c67"},"cell_type":"markdown","source":"#### Clean text\n\nLet's do some basic cleaning to see how it improve things still"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:53.38221Z","start_time":"2018-06-05T15:50:53.378667Z"},"_uuid":"636bf9f19a2605ec418b6b503e798f00680bea6f","trusted":false,"collapsed":true},"cell_type":"code","source":"def clean_punct(txt):\n    x = txt.replace(\"\\n\", \". \")\n    x = x.replace(\" -- \", \": \")\n    x = x.replace(\" - \", \": \")\n    return x\n\ndef reCase(txt):\n    ## recasing common words so that spaCy picks them up as entities\n    ## an ethereum specific NLP model shouldn't need that\n    ## Also this is inefficient as we could replace everything in one pass\n    ## but we'll worry about that for 10k+ interviews.\n    x = txt.replace(\"solidity\", \"Solidity\")\n    x = x.replace(\"truffle\", \"Truffle\")\n    x = x.replace(\" eth\", \" Eth\") # space to avoid recasing geth into gEth\n    x = x.replace(\" geth\", \" Geth\") # avoid together -> toGether ¯_(ツ)_/¯\n    x = x.replace(\"jQuery\", \"JQuery\")\n    x = x.replace(\" react\", \" React\")\n    x = x.replace(\" redux\", \" Redux\")\n    x = x.replace(\"testRPC\", \"TestRPC\")\n    x = x.replace(\"keythereum\", \"Keythereum\")\n    # ...\n    return x","execution_count":14,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:53.585751Z","start_time":"2018-06-05T15:50:53.383429Z"},"_uuid":"aedf514b6892cff7420be4720f8ad84327c5dcf7","trusted":false,"collapsed":true},"cell_type":"code","source":"for i in [0, 5, 8]:\n    print(f\"\"\"\n####################################\ndoc {i}, number of characters: {len(df['tooling'][i])}\n\ndocument | {\"Label\".rjust(20)} | {\"Entity type\".rjust(15)}\"\"\")\n    doc = nlp(reCase(clean_punct(df['tooling'][i])))\n    for entity in doc.ents:\n        print(f'{i:>8} | {entity.text.rjust(20)} | {entity.label_.rjust(15)}')\ndel doc","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"971e0ca45aeca60ce6a56aead802e581d5b4f6da"},"cell_type":"markdown","source":"Ugh, this will be better to analyze manually at the moment,\nautomatic analysis is too unreliable as\ntooling is too niche and the model does not have data on that.\n\nLet's do a word cloud for at the very least on PERSON, ORG and PRODUCT."},{"metadata":{"ExecuteTime":{"end_time":"2018-06-04T10:09:56.829063Z","start_time":"2018-06-04T10:09:56.827168Z"},"_uuid":"9d41dd58320b93c74ff71f410036435fbf879aa5"},"cell_type":"markdown","source":"#### Simple word cloud"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:53.589997Z","start_time":"2018-06-05T15:50:53.587011Z"},"_uuid":"ac052b83fcd81a26c98e6f508b83a94cf1e16d9a","trusted":false,"collapsed":true},"cell_type":"code","source":"# Remove noise \nignore_words = {\n    'Ethereum',\n    'UI',\n    'ETH', 'Eth',\n    'IDE',\n    'ABI'\n}\n\ndef tooling_extraction(txt):\n    if txt == '':\n        return ''\n    doc = nlp(reCase(clean_punct(txt)))\n    tools = []\n    for named_entity in doc.ents:\n        if named_entity.label_ in {'PERSON', 'ORG', 'PRODUCT'} and named_entity.text not in ignore_words:\n            txt = named_entity.text.replace(' ', '_')\n            tools.append(txt)\n    return ', '.join(tools)","execution_count":16,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.08478Z","start_time":"2018-06-05T15:50:53.591172Z"},"_uuid":"6f29f0d50555ba809b5fc0dfaa14d8e3cc102f16","trusted":false,"collapsed":true},"cell_type":"code","source":"df['tooling_extracted'] = df['tooling'].apply(tooling_extraction)","execution_count":17,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.090407Z","start_time":"2018-06-05T15:50:55.086103Z"},"_uuid":"93a1559440337dca5d4d27c56c34646602534f86","trusted":false,"collapsed":true},"cell_type":"code","source":"# Reminder - a lot is missed due to the very niche domain\n# while the NLP model was trained on general web publications.\ndf['tooling_extracted']","execution_count":18,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.330464Z","start_time":"2018-06-05T15:50:55.091498Z"},"_uuid":"00e00e83fec56580ddfe9e810e7b6d0e79879c32","trusted":false,"collapsed":true},"cell_type":"code","source":"# Concatenating all tools\ntools = ''\nfor idx, row in df['tooling_extracted'].iteritems():\n    tools += ', ' + row \n\n# Setting up the figure\n# Don't ask me what title and suptitle (super-title) are supposed to do ...\nplt.figure(figsize=(12,6))\nwordcloud = WordCloud(background_color='white', width=500, height=300,\n                      max_font_size=50, max_words=80).generate(tools)\nplt.imshow(wordcloud)\nplt.title(\"\"\"\nETHPrize devs - tools, library frameworks\n\n\"\"\", fontsize=20)\nplt.suptitle(\"\"\"This is a basic approach, manual processing is recommended.\nAt scale tagging 10~20% of the data manually\nbefore automated extraction on the rest would probably be much better\n\"\"\", fontsize=14)\n\nplt.axis(\"off\")\nplt.show()\n\n# Cleanup\ndel tools","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"6a31fd79356802bcb47c0f5b99f838e982b2bfd0"},"cell_type":"markdown","source":"Visualization can still be seriously improved, more cleaning can be done\nas we have \\_Parity/Parity, some person names (because products are recognized as person).\n\nI'm not even talking about the colors ¯\\\\_(ツ)_/¯.\nAn example of what can be done can be seen on [avsa twitter on Ethereum shared value survey](https://twitter.com/avsa/status/1003655572590342145):\n\n<div style=\"width: 400px;\">![Ethereum shared values](https://pbs.twimg.com/media/De2tXpUXcAA1s-J.jpg)</div>\n\nI'm open to jupyter-compatible wordcloud packages.\n\nIn any case I\nrecommend to process this field manually for now.\nIt is too niche for a general purpose NLP model."},{"metadata":{"_uuid":"1d67f7fa1f69dcd335c66600c0a8ae90c753b25f"},"cell_type":"markdown","source":"## Q2 - What are your biggest frustrations?\n\nThis field is probably much easier to process as frustrations are probably general English.\n\nWe will extract coherent \"topics\" from the answer using\na LDA model (Latent Dirichlet Allocation) which is a probabilistic method.\nAlternatively a NMF model (Non-Negative Matrix Factorization) can be used\nwhich is a linear algebraic method.\nOne might work better than the other but only doing both will tell. \n\n### Cleaning the data\n#### Raw data\n\nLike before, let's first start by checking the raw data.\n\n- what does the raw answers look like?\n- can we run naive topic modelling on it?"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.574989Z","start_time":"2018-06-05T15:50:55.331688Z"},"_uuid":"420c442e78f1b52a07143ea5c75960b7c386e248","trusted":false,"collapsed":true},"cell_type":"code","source":"for i in range(10):\n    print(f\"\"\"\n####################################\ndoc {i}, number of characters: {len(df['frustrations'][i])}\n\"\"\")\n    if df['frustrations'][i] != '':\n        doc = nlp(df['frustrations'][i]) # nlp = spacy.load('en_core_web_lg')\n        print(doc)\ndel doc","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"aad3b45ce4adfe5050affe3add15bd10de49487d"},"cell_type":"markdown","source":"Seems like we have proper English words here (gas costs, scalability, documentation, errors, ...)\nso we will build a simple topic model.\n\nLet's pick doc 0 and see if tokenization works fine, we will pick the last part as it has lots of special characters."},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.600437Z","start_time":"2018-06-05T15:50:55.576282Z"},"_uuid":"093f56418d6387f085513eb0e1d562873491c019","trusted":false,"collapsed":true},"cell_type":"code","source":"doc = nlp(df['frustrations'][0][-299:])\nfor token in doc:\n    print(token.text)\ndel doc","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"30dcbfee4e9c3f0af743517303cec824692fbc25"},"cell_type":"markdown","source":"Tokenization seems to be good (further checks shows that stuff like \"e.g.\" are properly recognized as well)\n\n### Preprocessing\n\nFor preprocessing we will need to:\n\n  - Remove the stopwords: \"a, to, or, ...\"\n  - domain specific stopwords: \"Ethereum\" at least\n  - question specific stopwords: \"frustrations\"\n  - detect and create bigrams and trigrams, i.e. words often associated together like smart_contract\n  - Lemmatize: i.e. specific, specificiy, specifically should count the same.\n  - keep only what carry topics/sense (nouns, adjectives, verbs, adverbs) and discard the fluff"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.603683Z","start_time":"2018-06-05T15:50:55.601639Z"},"_uuid":"93b725fa85e1c60753a2d7041b1afd9be68bc0f5","trusted":false,"collapsed":true},"cell_type":"code","source":"# Lets have a look at the included stopwords\n\nprint(nlp.Defaults.stop_words)","execution_count":22,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.608197Z","start_time":"2018-06-05T15:50:55.604715Z"},"_uuid":"31fc9d3eaf6d4a490299254e6855e121848e4ea4","trusted":false,"collapsed":true},"cell_type":"code","source":"def topic_modelling_preprocess(txt):    \n    # Remove stopwords\n    stop_words = nlp.Defaults.stop_words\n    stop_words.add('ethereum')\n    stop_words.add('frustrations')\n    \n    cleaned = [word for word in simple_preprocess(txt, deacc = True) if word not in stop_words]\n    if cleaned == []:\n        return []\n    \n    # Lemmatization - TODO improve\n    doc = nlp(\" \".join(cleaned))    \n    result = [token.lemma_ for token in doc if token.pos_ in {'NOUN', 'ADJ', 'VERB', 'ADV'}]\n    return result","execution_count":23,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:55.625254Z","start_time":"2018-06-05T15:50:55.609212Z"},"_uuid":"b46c24801d9344bcc762e5dba7600ddb83c4a52e","trusted":false,"collapsed":true},"cell_type":"code","source":"# Let's try it on doc 5 which is short (394 characters)\n\nprint('#### Before\\n')\nprint(df['frustrations'][5])\n\nprint('#### After\\n')\ntopic_modelling_preprocess(df['frustrations'][5])","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"a2c0b4369c90e2d006a6068071ddc2db7114350d"},"cell_type":"markdown","source":"Thoughts:\n\n  - We will miss the intensity in individual reply \"really bad\"\n  - The time information \"right now\"\n  - Still common themes and pain point will emerge after more than a hundred interviews\n  - In the future, a model can be trained on each frustration to characterize the intensity and objectivity\n    and the words associated to it."},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:50:56.836567Z","start_time":"2018-06-05T15:50:55.626363Z"},"_uuid":"0624e53634b616bdaec048d8f483bbc905fe6b5b","trusted":false,"collapsed":true},"cell_type":"code","source":"# Let's start with a quick and dirty model\n\nfrustrations = df['frustrations'].apply(topic_modelling_preprocess)\nfrustrations","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"fe4444a8931c8298df2fcd11048901da1280d3f9"},"cell_type":"markdown","source":"I don't know if we should be happy that lemmatization doesn't:\n  - Transform solidity into solid\n  - Transform customize into custom, monitoring into monitor, regularly into regular, ...\n \nNote that we don't have n-grams yet."},{"metadata":{"ExecuteTime":{"end_time":"2018-06-04T15:31:32.431428Z","start_time":"2018-06-04T15:31:32.429656Z"},"_uuid":"6369acaa420bc61fc5555d2c8c61ba47ece74479"},"cell_type":"markdown","source":"### Extracting the topics"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:00.034183Z","start_time":"2018-06-05T15:50:56.83779Z"},"_uuid":"c7f00d75452b4c767b7c35d19f7a1f0546afdfda","trusted":false,"collapsed":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(frustrations)\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in frustrations]\n\n# Modelization, nuber of topics need to be tuned\nlda_model = LdaModel(corpus=corpus,\n                     id2word=id2word,\n                     num_topics=10,    # <--- tune this\n                     random_state=1337,\n                     update_every=1,\n                     chunksize=100,\n                     passes=10,\n                     alpha='auto',\n                     per_word_topics=True)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"bb8acb4d241a4921bb2a69e1d412f27ed93f0a7f"},"cell_type":"markdown","source":"### Topic visualization\n\nLet's see the main keywords for each topics. Note that topics are created\nfrom probabilities of words being associated together.\nThey are unbiaised (no human preconception about an error topic, documentation topic, ...)"},{"metadata":{"_uuid":"39c73e69d813951e0a9259cfbe9b183faeffb063"},"cell_type":"markdown","source":"#### As text"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:00.044203Z","start_time":"2018-06-05T15:51:00.035556Z"},"_uuid":"4c24439ae9a3150fd638267b3d83a86e6c3dfa20","trusted":false,"collapsed":true},"cell_type":"code","source":"pprint.pprint(lda_model.print_topics())","execution_count":27,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-04T15:35:20.416661Z","start_time":"2018-06-04T15:35:20.414895Z"},"_uuid":"f58319e8e1626ec0dee2484155d978e170ff9542"},"cell_type":"markdown","source":"#### Interactive visualization\n\nNote: topic ID by pyLDAvis use a different index from the previous pretty-printed topics.\n\nAlso, the interactive visualization doesn't work on Github."},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:00.647366Z","start_time":"2018-06-05T15:51:00.045579Z"},"_uuid":"e608b5aef18648324492dcb1e034183d2c666743","trusted":false,"collapsed":true},"cell_type":"code","source":"vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"6e916a5031a6ba0fe50874493e15bcb76ee50a58"},"cell_type":"markdown","source":"With this preprocessing, 10 topics are a bit too much, as we have at least 2 topics overlapping\nover the PC1/PC2 axis (the 2 principal components, 2 axis that explains the most the variance in the datasets)\n\nOutcomes:\n  - Test, people, solidity, [smart?] contract, gas are recurring themes\n  - Hopefully using n-grams will draw a better picture.\n  \n### Cleanup"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:00.65188Z","start_time":"2018-06-05T15:51:00.649119Z"},"_uuid":"873279221612d6c218219ba9bd6e618f1b09e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"del id2word\ndel corpus\ndel frustrations\ndel vis","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"12b79ae63654ff4526fcb2473a55316309a4db78"},"cell_type":"markdown","source":"# Topic modelling at scale\n\nWe put everything with done manually in a function so we can do topic modelling in a one-liner for the answers of the other questions.\n\nThe model also measure its own performance via perplexity and coherence.\n\nNote: we do lemmatization after n_grams to as many combinations like \"non-technical\" have a canonical use.\n\n## Introducing the production functions"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:00.656999Z","start_time":"2018-06-05T15:51:00.653372Z"},"_uuid":"ae02498e373dfbac80da0c13255075522cd1829b","trusted":false,"collapsed":true},"cell_type":"code","source":"def gen_stop_words(extra_stop_words):\n    ## Expect a list of stop words in lowercase\n    result = nlp.Defaults.stop_words\n    for word in extra_stop_words:\n        result.add(word)\n    return result","execution_count":30,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:00.667705Z","start_time":"2018-06-05T15:51:00.658475Z"},"_uuid":"0cd5111311a993e0a5754f5941cc65ecbd4740db","trusted":false,"collapsed":true},"cell_type":"code","source":"def model_topics(txts, num_topics, stop_words, use_ngrams = False, n_grams_min_count = 5, n_grams_score_threshold = 1):\n    # Note: here we process the whole dataframe series\n    \n    # Transform the serie into a list of list of words,\n    # Remove stopwords at the same time\n    cleaned = []\n    for idx, txt in txts.iteritems():\n        # Remove stopwords\n        cleaned += [[word for word in simple_preprocess(txt, deacc = True) if word not in stop_words]]\n    \n    if use_ngrams:\n        # Build bigrams and trigrams\n        bigrams = Phraser(Phrases(cleaned, min_count=n_grams_min_count, threshold=n_grams_score_threshold))\n        trigrams = Phraser(Phrases(bigrams[cleaned], threshold=n_grams_score_threshold))\n        \n        # Now create the bag of words with the new trigrams\n        cleaned = [trigrams[bigrams[txt]] for txt in cleaned]\n    \n    # Lemmatization - TODO improve\n    lemmatized = []\n    for txt in cleaned:\n        if txt == []:\n            lemmatized += []\n        else:\n            doc = nlp(\" \".join(txt))    \n            lemmatized += [[token.lemma_ for token in doc if token.pos_ in {'NOUN', 'ADJ', 'VERB', 'ADV'}]]\n            \n    print(\"Snippet of keywords for topic modelling for the first 3 answers\")\n    print(lemmatized[0:3])\n        \n    # Create Dictionary\n    id2word = corpora.Dictionary(lemmatized)\n\n    # Term Document Frequency\n    corpus = [id2word.doc2bow(text) for text in lemmatized]\n    \n    # Modelling\n    lda_model = LdaModel(corpus=corpus,\n                         id2word=id2word,\n                         num_topics=num_topics,\n                         random_state=1337,\n                         update_every=1,\n                         chunksize=100,\n                         passes=10,\n                         alpha='auto',\n                         per_word_topics=True)\n    \n    ## Model performance\n    print(\"\\nModel performance\\n\")\n    \n    ## Perplexity\n    print(f\"\"\"Perplexity: {lda_model.log_perplexity(corpus)}. Lower is better.\n    See https://en.wikipedia.org/wiki/Perplexity.\n    The best number of topics minimize perplexity.\n    \"\"\")\n    \n    ## Coherence\n    coherence = CoherenceModel(\n        model=lda_model,\n        texts=lemmatized,\n        dictionary=id2word,\n        coherence='c_v'\n    )\n    ## Corpus coherence\n    print(f'Whole model coherence: {coherence.get_coherence()}.')\n    \n    ## By topic coherence\n    topic_coherences = coherence.get_coherence_per_topic()\n    print(f\"\"\"\nBy topic coherence. Higher is better.\n    Measure how \"well related\" are the top words within the same topic.\n    \"\"\")\n    \n    print(f'topic_id | {\"top 3 keywords\".rjust(45)} | topic coherence')\n    for topic_id in range(num_topics):\n        words_proba = lda_model.show_topic(topic_id, topn=3)\n        words = [words for words,proba in words_proba]\n        print(f'{topic_id:>8} | {str(words).rjust(45)} | {topic_coherences[topic_id]:>8.4f}')\n    \n    return lda_model, corpus, id2word","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"be07c2f0db3a6c8e3b1ae970681d95dacc36eaba"},"cell_type":"markdown","source":"## Sanity checks on developers frustrations"},{"metadata":{"_uuid":"54b8e82cb9fe3e67ca0a5af9c40099ff0bc47024"},"cell_type":"markdown","source":"### Modelization"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:05.554075Z","start_time":"2018-06-05T15:51:00.669052Z"},"_uuid":"b1502a28306e9128f6c9bca8d295c6095ba16246","trusted":false,"collapsed":true},"cell_type":"code","source":"stop_words = gen_stop_words(['ethereum', 'frustrations'])\n\nlda_model, corpus, id2word = model_topics(\n    df['frustrations'], 10, stop_words,\n    use_ngrams = True, n_grams_min_count = 1, n_grams_score_threshold = 1 # Need more data to have less permissive thresholds\n)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"09e200d3a10fc24fabf6d2b9df499f2f2ab715b9"},"cell_type":"markdown","source":"Wow this is much better with bigrams like gas_limit. My raw thoughts:\n\n```\ntopic_id |                                top 3 keywords | topic coherence\n       0 |                 ['contract', 'thing', 'hard'] |   0.4731 -> documentation on smart contract\n       1 |                 ['solidity', 'need', 'build'] |   0.4084 -> improve solidity build system\n       2 |                       ['test', 'run', 'need'] |   0.3365 -> we need more tests\n       3 |    ['gas_limit', 'state_channel', 'possible'] |   0.2845 -> unsure\n       4 |                   ['use', 'contract', 'tool'] |   0.2732 -> we need usage demos\n       5 |           ['block', 'address', 'transaction'] |   0.3087 -> ...\n       6 |                     ['user', 'need', 'block'] |   0.3906\n       7 |                 ['contract', 'need', 'build'] |   0.4742\n       8 |                    ['thing', 'error', 'need'] |   0.4019\n       9 |                ['chain', 'write', 'solidity'] |   0.6289\n```\n       \nMore info can be extracted with the interactive visualization"},{"metadata":{"_uuid":"9992c73488fc535689161eba4e9899b1b53bb99c"},"cell_type":"markdown","source":"### Visualization (text and interactive)\nNote that interactive visualization does not work on Github"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:06.137725Z","start_time":"2018-06-05T15:51:05.555788Z"},"_uuid":"43a4f3c373fa3080373daf0390509f9dfd6ffb7f","trusted":false,"collapsed":true},"cell_type":"code","source":"pprint.pprint(lda_model.print_topics())\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis # Interactive vizualization, doesn't work on Github","execution_count":33,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-04T16:07:42.902454Z","start_time":"2018-06-04T16:07:42.899632Z"},"_uuid":"39a7e03fb3b2e837b942e57525e155ffd2199cc9"},"cell_type":"markdown","source":"## Let's try it on tooling as well\n\nSince Named Entity Extraction didn't work that well"},{"metadata":{"_uuid":"5f1b75ddcaf100cffdb3686c2aca32f860eebf25"},"cell_type":"markdown","source":"### Modelization"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:09.477373Z","start_time":"2018-06-05T15:51:06.139138Z"},"_uuid":"db0a58e20f222a118b7efb21e8041ef13cbdef99","trusted":false,"collapsed":true},"cell_type":"code","source":"stop_words = gen_stop_words(['ethereum', 'tool', 'tooling', 'tools'])\n\nlda_model, corpus, id2word = model_topics(\n    df['tooling'], 7, stop_words,\n    use_ngrams = True, n_grams_min_count = 1, n_grams_score_threshold = 1\n    # Need more data to have less permissive thresholds\n)","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"d5fcc477b94ec1eb20716771252c266367c463bf"},"cell_type":"markdown","source":"### Visualization (text and interactive)\nNote that interactive visualization does not work on Github"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:09.48522Z","start_time":"2018-06-05T15:51:09.478919Z"},"_uuid":"be7c7f6ce2ceafce90e5e18c69bc5ba1e88e6287","trusted":false,"collapsed":true},"cell_type":"code","source":"pprint.pprint(lda_model.print_topics())","execution_count":35,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:09.93445Z","start_time":"2018-06-05T15:51:09.486442Z"},"_uuid":"50bf057eaeb2fe2314ef366a250a05d87332bc98","trusted":false,"collapsed":true},"cell_type":"code","source":"vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis # Interactive vizualization, doesn't work on Github","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"a5916ee29475b6db061d184198f778b70e70e54a"},"cell_type":"markdown","source":"Mmmh, that doesn't seem to work as well, many tools appears less frequently than generic stuff like \"**web\" or \"standard\"."},{"metadata":{"_uuid":"e48229abe95bfcfd2c74ffa255c59164becdca49"},"cell_type":"markdown","source":"# Analyzing easier non-niche fields\n\nThere are other fields related to tools and very specific Ethereum development jargon.\n\nWe first start with fields that can be processed with a \"normal English\"."},{"metadata":{"_uuid":"0b9db9d1247e5a16c0eec2d418686b04a5c8e96b","collapsed":true},"cell_type":"markdown","source":"## Who are you and what are you working on?"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:14.420164Z","start_time":"2018-06-05T15:51:09.935865Z"},"trusted":false,"_uuid":"8b7e1b67a312e699136d4b807aebebea4077b135","collapsed":true},"cell_type":"code","source":"stop_words = gen_stop_words(['ethereum'])\n\nlda_model, corpus, id2word = model_topics(\n    df['who_what'], 7, stop_words,\n    use_ngrams = True, n_grams_min_count = 1, n_grams_score_threshold = 1\n    # Need more data to have less permissive thresholds\n)","execution_count":37,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:15.015027Z","start_time":"2018-06-05T15:51:14.421478Z"},"trusted":false,"_uuid":"fd4d8b8e0fe2385e69440c56006b6260050feb07","collapsed":true},"cell_type":"code","source":"vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis # Interactive vizualization, doesn't work on Github","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"81a3173e75b3aa7523b39923c1a9988c6ccba2e8"},"cell_type":"markdown","source":"Not too sure what to make of this, apart from smart_contract being a recurrent theme.\n\nSome topics are related to:\n  - app\n  - client\n  - hiring /recruiting\n  - wasm\n  - tokens / auction\n  - bounties\n  \n  Ideally a future direction would be \"extractive text summarization\"\n  \n  Or simpler --> pick top sentences related to each topic.\n  \n  Some ideas here: https://github.com/mathsyouth/awesome-text-summarization"},{"metadata":{"_uuid":"ee89cdd6ebef190bb88e3b48abedec7e911b40c5"},"cell_type":"markdown","source":"## What are you excited about?"},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:16.3542Z","start_time":"2018-06-05T15:51:15.016364Z"},"trusted":false,"_uuid":"a50991cd4c9b7c5553384f5e66596d185803b05d","collapsed":true},"cell_type":"code","source":"stop_words = gen_stop_words(['ethereum', 'exciting', 'excited'])\n\nlda_model, corpus, id2word = model_topics(\n    df['excited_about'], 7, stop_words,\n    use_ngrams = True, n_grams_min_count = 1, n_grams_score_threshold = 1\n    # Need more data to have less permissive thresholds\n)","execution_count":39,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-06-05T15:51:16.790299Z","start_time":"2018-06-05T15:51:16.355886Z"},"trusted":false,"_uuid":"a5d8f362a96cae13f6ccaf0f7f8bacfbe43ebe8b","collapsed":true},"cell_type":"code","source":"vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis # Interactive vizualization, doesn't work on Github","execution_count":40,"outputs":[]}],"metadata":{"MarkdownCell":{"cm_config":{"lineWrapping":true}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}