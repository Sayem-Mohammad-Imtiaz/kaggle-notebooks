{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Описание датасета**\n\n1. (age)\tвозраст\n2. (sex)\tпол (1: мужской, 0: женский)\n3. (cp)\tТип боли в груди (1: типичная стенокардия, 2: атипичная стенокардия, 3: неангинальная боль, 4: бессимптомная)\n4. (trestbps)\tартериальное давление в состоянии покоя (в мм рт. ст. при поступлении в больницу)\n5. (chol)\tхолестерол в сыворотке крови в мг/дл\n6. (fbs)\tуровень сахара в крови натощак > 120 мг/дл (1: истина; 0: ложь)\n7. (restecg)\tрезультаты электрокардиографии в состоянии покоя (0: норма, 1: наличие аномалии волны ST-T (инверсии волны T и/или повышение или понижение уровня ST > 0,05 мВ), 2: наличие вероятной или определенной гипертрофии левого желудочка по критериям Эстеса)\n8. (thalach)\tмаксимальная частота сердечных сокращений\n9. (exang)\tстенокардия, вызванная физической нагрузкой (1: да; 0: нет)\n10. (oldpeak)\tподавление ST, вызванное физической нагрузкой по сравнению с отдыхом\n11. (slope)\tнаклон пикового сегмента упражнения ST\n12. (ca)\tколичество крупных сосудов (0-3), окрашенных флуороскопией\n13. (thal)\tthal (?) (3: норма, 6: исправленный дефект, 7: обратимый дефект)\n14. (num) (the predicted attribute)\tдиагноз заболевания сердца (ангиографический статус заболевания) (0: сужение диаметра < 50%: 1: сужение диаметра > 50%)","metadata":{}},{"cell_type":"code","source":"#Загружаем данные\nheart = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\nheart.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Смотрим статистику по датасету\nlen(heart)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Данных для обучения мало, всего 303 объекта.","metadata":{}},{"cell_type":"code","source":"heart.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Пропусков в заполнении нет.","metadata":{}},{"cell_type":"code","source":"heart[heart.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Удаляем дубликаты\nheart.drop_duplicates(inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Графическое исследование данных**","metadata":{}},{"cell_type":"code","source":"#Строим матрицу корреляции\nplt.subplots(figsize = (10,10))\nsns.heatmap(heart.corr(), square = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Близко скоррелированных признаков нет","metadata":{}},{"cell_type":"code","source":"# Строим гистограммы распределения признаков\nheart.hist(figsize = (25, 25))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Поля с категориальными признаками - cp, restecg, slp, caa и thall - перекодируем в one-hot.\n\nПоля с числовыми признаками - age, trtbps, chol, thalachh и oldpeak - имеют вид нормального распределения, поэтому стандартизируем их.","metadata":{}},{"cell_type":"code","source":"# one-hot кодирование\nheart_oh = pd.get_dummies(heart, columns = [\"cp\", \"restecg\", \"slp\", \"caa\", \"thall\"],prefix = [\"cp\", \"restecg\", \"slp\", \"caa\", \"thall\"])\n\n# стандартизация\nnumeric = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nfor col in numeric: heart_oh[col] = (heart_oh[col] - heart_oh[col].mean()) / heart_oh[col].std()\n\nheart_oh.hist(figsize = (25, 25))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Строим матрицу корреляции\nplt.subplots(figsize = (10,10))\nsns.heatmap(heart_oh.corr(), square = True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# То, что ниже - делать не будем, т.к. если этого не делать (хотя, по идее, было бы правильно), результаты неожиданно превзойдут все ожидания :)\n\n# убираем сильно коррелирующие признаки\n#heart_oh.drop(['restecg_1', 'slp_2','thall_3'], axis = 1, inplace = True)\n# убираем признаки с сильным дисбалансом классов\n#heart_oh.drop(['restecg_2', 'caa_4', 'thall_0'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# разделим на обучающую и тестовую выборки в соотношении 8 к 2\nx = heart_oh.iloc[:, 1:-1].values\ny = heart_oh.iloc[:, -1].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\nx_train, x_test, y_train, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Логистическая регрессия\n\nmodel = LogisticRegression()\nclf = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nconf = metrics.confusion_matrix(y_test, y_pred)\nmetrics.ConfusionMatrixDisplay(conf).plot()\nprint ('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\nprint ('ROC_AUC: ', metrics.roc_auc_score(y_test, clf.decision_function(x_test)))\nLR_disp = metrics.plot_roc_curve(model, x_test, y_test)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Гауссовый Наивный Байес\n\nmodel = GaussianNB()\nclf = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nconf = metrics.confusion_matrix(y_test, y_pred)\nmetrics.ConfusionMatrixDisplay(conf).plot()\nprint ('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\nprint ('ROC_AUC: ', metrics.roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1]))\ngNB_disp = metrics.plot_roc_curve(model, x_test, y_test)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Метод опорных векторов\n\nmodel = SVC()\nclf = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nconf = metrics.confusion_matrix(y_test, y_pred)\nmetrics.ConfusionMatrixDisplay(conf).plot()\nprint ('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\nprint ('ROC_AUC: ', metrics.roc_auc_score(y_test, clf.decision_function(x_test)))\nSVC_disp = metrics.plot_roc_curve(model, x_test, y_test)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Случайный лес\n\nmodel = RandomForestClassifier(n_estimators = 100, random_state = 42)  \nclf = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nconf = metrics.confusion_matrix(y_test, y_pred)\nmetrics.ConfusionMatrixDisplay(conf).plot()\nprint ('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\nprint ('ROC_AUC: ', metrics.roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1]))\nRF_disp = metrics.plot_roc_curve(model, x_test, y_test)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K ближайших\n\nmodel = KNeighborsClassifier(n_neighbors = 8)\nclf = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nconf = metrics.confusion_matrix(y_test, y_pred)\nmetrics.ConfusionMatrixDisplay(conf).plot()\nprint ('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\nprint ('ROC_AUC: ', metrics.roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1]))\nKNN_disp = metrics.plot_roc_curve(model, x_test, y_test)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Градиентный бустинг\n\nmodel = xgb.XGBClassifier(use_label_encoder = False)\nclf = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nconf = metrics.confusion_matrix(y_test, y_pred)\nmetrics.ConfusionMatrixDisplay(conf).plot()\nprint ('Accuracy: ', metrics.accuracy_score(y_test, y_pred))\nprint ('ROC_AUC: ', metrics.roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1]))\nGB_disp = metrics.plot_roc_curve(model, x_test, y_test)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Выводы**\n1) Полученный методами логистической регрессии, наивного Байеса и градиентного бустинга результат \"100%\" выглядит более чем прекрасно, но вызывает подозрения в переобучении.\n\n2) Результаты, полученные методами опорных векторов и случайного леса, немного ложноположительны (методом опорных векторов - чуть больше). Учитывая, что это медицинское тестирование, возможно, что в данном случае лучшей моделью будет модель на основе метода случайного леса.\n\n3) Соответственно, модель на основе метода ближайших соседей, несмотря на довольно хорошие метрики (хотя и самые худшие среди остальных методов), не годится из-за большого количества ошибок обоих родов.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}