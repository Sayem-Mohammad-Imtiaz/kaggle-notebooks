{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Importing the libraries to be used"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/creditcard-fraud-detection/creditcard.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" All the variables/ features  are standardised apart from amount and time , hence standardsing it"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import zscore\ndf1=df[['Time','Amount']].apply(zscore)\ndf.drop(['Time','Amount'],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()\ndf2=pd.concat((df,df1),axis=1)\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the no of rows and columns:\ndf2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the descriptive statistics\ndf2.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the target\ndf2['Class'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Hence this is an imbalanced dataset with majority of the class belonging to class 0. We will try to balance the data by oversampling using SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing Values\ndf2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence no missing values in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df2.drop('Class',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of the features and target\ncols= list(df3.columns)\nfor i in cols:\n    sns.boxplot(y=i,x=df2['Class'],data=df3)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the box plot ,looks like most of the features have huge amount of outliers ,hence we will first keep the ouliers and make the model"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Making the first model using stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df3\ny=df2['Class']\nx_const = sm.add_constant(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OVERSAMPLING USING SMOTE TO HANDEL IMBALANCED DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import imblearn\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=2)\nx, y = smote.fit_sample(x, y.ravel())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After oversampling the number of rows have increased from 284807 to 568630."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_const = sm.add_constant(x)\nx_const.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"model1=sm.Logit(y,x_const).fit()\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" The p-value of the model is <0.05 (considering alpha to be 0.05)  and hence the model is significant.\n The p-value of individual features shows that  the fetures V21 and V27 are insignificant as the p-values are greater than the alpha."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba=model1.predict(x_const)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pro(y_pred):\n    if y_pred <0.5:\n        y_pred=0\n    elif y_pred>0.5:\n        y_pred=1\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=y_pred_proba.apply(pro)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\nprint('accuracy_ score :',accuracy_score(y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('roc_auc_score:',roc_auc_score(y,y_pred_proba))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":" # removing the insignificant features and making a model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_const=x_const.drop(['V21','V27'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2= sm.Logit(y,x_const).fit()\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba = model2.predict(x_const)\ny_pred=y_pred_proba.apply(pro)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('accuracy_score:', accuracy_score(y_pred,y))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('roc_auc_score : ',roc_auc_score(y,y_pred_proba))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the p-value we  can find that  all the remaining variables are significant"},{"metadata":{},"cell_type":"markdown","source":"The accuracy score and roc-auc score shows that their is no significant increase in the performance after removing insignificant features as well"},{"metadata":{"trusted":false},"cell_type":"code","source":"MACHINE LEARNING MODEL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr= LogisticRegression()\nlr.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=lr.predict(x_train)\ny_pred_test=lr.predict(x_test)\ny_train_prob = lr.predict_proba(x_train)[:,1]\ny_test_proba = lr.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,confusion_matrix,classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_train_prob))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_test_proba))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy and the AUC score for both train and test is good showing no overfitting issues."},{"metadata":{},"cell_type":"markdown","source":"##### checking multicollinearity using vif\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=list(x.columns)\nvif= [variance_inflation_factor(x.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" V7 has the highest vif ,hence we can remove it and try modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=x.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=x1.drop('V7',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(x1,y,test_size=0.3,random_state=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr1=LogisticRegression()\nlr1.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=lr1.predict(X_train)\ny_proba_train = lr1.predict_proba(X_train)[:,1]\ny_pred_test =lr1.predict(X_test)\ny_proba_test = lr1.predict_proba(X_test)[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_proba_train))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_proba_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=list(x1.columns)\nvif= [variance_inflation_factor(x1.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per vif the V17 column exhibit the highest multicollinearity , hence we will drop that and have a check."},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=x1.drop('V17',axis=1)\nX_train,X_test,Y_train,Y_test = train_test_split(x1,y,test_size=0.3,random_state=True)\nlr1=LogisticRegression()\nlr1.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=lr1.predict(X_train)\ny_proba_train = lr1.predict_proba(X_train)[:,1]\ny_pred_test =lr1.predict(X_test)\ny_proba_test = lr1.predict_proba(X_test)[:,1]\n\nprint('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_proba_train))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_proba_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score has slightly increases while AUC score has slightly decreased after removal of V17."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=list(x1.columns)\nvif= [variance_inflation_factor(x1.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column V12 is exhibiting the highest VIF hence dropping it to check the accuracy of model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=x1.drop('V12',axis=1)\nX_train,X_test,Y_train,Y_test = train_test_split(x1,y,test_size=0.3,random_state=True)\nlr1=LogisticRegression()\nlr1.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=lr1.predict(X_train)\ny_proba_train = lr1.predict_proba(X_train)[:,1]\ny_pred_test =lr1.predict(X_test)\ny_proba_test = lr1.predict_proba(X_test)[:,1]\n\nprint('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_proba_train))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_proba_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy and the AUC score of the model is decreasing hence VIF is not helping in improving the performance of the model,so lets try Backward elimination to find significant features."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=list(x1.columns)\nvif= [variance_inflation_factor(x1.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still have features exhibiting multicollinearity as per VIF but since removal of feature is decreasing the scores hence we will try other methods of Feature selection."},{"metadata":{},"cell_type":"markdown","source":"#### Backward elimination to check significant features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(x.columns)\npmax = 0\nwhile (len(cols)>1):\n   \n    X_1 = x[cols]\n    X_1 = sm.add_constant(X_1)\n    model2 = sm.Logit(y,X_1).fit()\n    p = model2.pvalues     \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Two features were eliminated by Backward Elimination Technique. The features eliminated are 'V21' and 'V27'."},{"metadata":{"trusted":true},"cell_type":"code","source":"x2=x[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V22', 'V23', 'V24', 'V25', 'V26', 'V28', 'Time', 'Amount']]\nx2_const = sm.add_constant(x2)\nmodel3= sm.Logit(y,x2_const).fit()\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba = model2.predict(x2_const)\ny_pred=y_pred_proba.apply(pro)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('roc_auc_score : ',roc_auc_score(y,y_pred_proba))\nprint('accuracy_score:',accuracy_score(y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much improvement from the base model. The accuracy and AUC score are still the same."},{"metadata":{},"cell_type":"markdown","source":"Forward Selection Method for Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector \n\nlr = LogisticRegression()\n\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 0)\n\n\n# Build step forward feature selection\nsfs = SequentialFeatureSelector(lr,k_features = 30,forward=True,\n           floating=False, scoring='r2',\n           verbose=2,\n           cv=5)\n\nsfs = sfs.fit(X_train, y_train)\n\nsfs.k_feature_names_ ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to forward selection all the features are important.Hence the accuracy will remain same as the base moel which was built using all the features."},{"metadata":{},"cell_type":"markdown","source":"All the above methods gets almost similar accuracy and Roc-Auc score hence not much parameter tuning or feature elimination is required to improve the performance of the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ncreditcard = pd.read_csv(\"../input/creditcard-fraud-detection/creditcard.csv\")","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}