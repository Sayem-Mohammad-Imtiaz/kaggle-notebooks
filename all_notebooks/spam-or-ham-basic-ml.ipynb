{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing unnamed columns and renaming the columns v1 and v2"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1, inplace=True)\ndata.rename(columns={'v1':'Label', 'v2':'Message',}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Label', data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more 'ham' or 'not-spam' messages than 'spam' messages.\n\n"},{"metadata":{},"cell_type":"markdown","source":"Selecting the features and labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['Message']\ny = data['Label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into training and testing sets before extracting features from the test data and building machine learning models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting features from the text data using tfidfvectorizer from sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vect = TfidfVectorizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The vectorizer is fit on the training data. Later the training and testing data is transformed using the vectorizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = tfidf_vect.fit_transform(X_train)\nX_test = tfidf_vect.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also see the features/tokens identified by the vectorizer by accessing the get_feature_names method."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tfidf_vect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see most of them are numbers, abbreviations(short cuts). Also notice that we are not restricting the stopwords here as they might play an important role in classifying a message to be 'spam' or 'ham'.  \n\nWe will also see the wether removing stopwords might help with the classification task in the later step."},{"metadata":{},"cell_type":"markdown","source":"Let us import different models and evaluation metrics. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n\nlr = LogisticRegression()\nnb = MultinomialNB()\nknc = KNeighborsClassifier()\nsvc = SVC(gamma = 'auto')\ndtc = DecisionTreeClassifier()\nrfc = RandomForestClassifier(n_estimators=100)\ngbc = GradientBoostingClassifier()\nabc = AdaBoostClassifier()\n\n\n\nmodels = {'Logistic Regression':lr, 'Naive Bayes classifier':nb, 'k-nearest neighbors':knc, \n          'Support Vector Machine':svc, 'Decision Tree Classifier':dtc, \n          'Random Forest Classifier':rfc, 'Gradient Boosting Classifier':gbc, 'AdaBoost Classifier':abc}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Writing a function to fit the model on training data and make predictions on test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model):\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    test_accuracy = accuracy_score(y_test, y_pred)\n    conf_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['ham', 'spam'], index=['ham','spam'])\n    \n    return test_accuracy, conf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracies = []\nconfusion_matrices = []\nfor name, model in models.items():\n    test_acc, conf_matrix = eval_model(model) \n    test_accuracies.append(test_acc)\n    confusion_matrices.append(conf_matrix)\n    print(f'{name} ---> Test accuracy - {test_acc*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(test_accuracies, index=list(models.keys()), columns=['test_acc'])\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.barplot(x ='test_acc', y=results.index, data=results)\nplt.xlim(0.85, 1.0)\nplt.title('Performance comparision')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like SVM and K-nearest neighbors are not a good choice for this task. The performance of all the other models is almost similar in this case."},{"metadata":{},"cell_type":"markdown","source":"Now let us include the effect of stopwords by passing the 'english' keyword to the stopwords argument to the tfidf vectorizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n\ntfidf_vect = TfidfVectorizer(stop_words='english')\n\nX_train = tfidf_vect.fit_transform(X_train)\nX_test = tfidf_vect.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice a decrease in number of features of the vectorizer after removing the stopwords from 7206 to 6946.\n\n\nLets initialize the same models again to fit them on the features without stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nnb = MultinomialNB()\nknc = KNeighborsClassifier()\nsvc = SVC(gamma = 'auto')\ndtc = DecisionTreeClassifier()\nrfc = RandomForestClassifier(n_estimators=100)\ngbc = GradientBoostingClassifier()\nabc = AdaBoostClassifier()\n\n\n\nmodels = {'Logistic Regression':lr, 'Naive Bayes classifier':nb, 'k-nearest neighbors':knc, \n          'Support Vector Machine':svc, 'Decision Tree Classifier':dtc, \n          'Random Forest Classifier':rfc, 'Gradient Boosting Classifier':gbc, 'AdaBoost Classifier':abc}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracies_no_stopwords = []\nconfusion_matrices_no_stopwords = []\nfor name, model in models.items():\n    test_acc, conf_matrix = eval_model(model) \n    test_accuracies_no_stopwords.append(test_acc)\n    confusion_matrices_no_stopwords.append(conf_matrix)\n    print(f'{name} ---> Test accuracy - {test_acc*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results['test_acc_without_stopwords'] = pd.Series(test_accuracies_no_stopwords, index=list(models.keys()))\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the results there is not much of an improvment after removing stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrices(models, confusion_matrices):\n    fig, axs = plt.subplots(2,4, figsize=(10,5)) \n\n    m = 0\n    for i, ax_r in enumerate(axs):\n        for j, ax in enumerate(ax_r):\n            sns.heatmap(confusion_matrices[m], annot=True, cbar=False, cmap='Blues', fmt='g', ax = ax)\n            ax.set_xlabel('Predicted label')\n            ax.set_ylabel('True label')\n            ax.set_title(f'{list(models.keys())[m]}', fontsize=12, fontweight='bold')\n            m += 1\n\n    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.tight_layout()\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrices(models, confusion_matrices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally let us build a pipeline to fit and predict on raw text data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('tfidf_vect', TfidfVectorizer()),\n    ('classifier', RandomForestClassifier(n_estimators=100))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data oncemore, to make sure the 'eval_model' function uses the right data for fitting and evaluating the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use the same function to fit and predict with the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc, conf_matrix = eval_model(pipeline) \n\nprint('Test accuracy - ',test_acc)\nprint('Confusion matrix - \\n', conf_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen from the confusion matrix, 39 'spam' messages are being predicted as 'ham' messages"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification Report \\n', classification_report(y_test, pipeline.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another good thing about pipeline is, we can directly make predictions on new raw text messages.\n\nLets create some text messages and predict if they are 'spam' or 'ham'."},{"metadata":{"trusted":true},"cell_type":"code","source":"messages = ['Thank you for subscribing! You will be notified when you win your 1 Million Dollar prize money! Please call our customer service representative on 0800012345 for further details ',\n          'Hi, hope you are doing well. Please call me as soon as possible!']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.predict(messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}