{"cells":[{"metadata":{},"cell_type":"markdown","source":"![title](https://i.imgur.com/NsCStAG.png)\n<p style=\"text-align:center\">\n    Este é um notebook de apresentação da resolução do business case de Data Science fornecido pela Loft.<br>\n    <span style=\"font-size:10px;\">Desenvolvido por Pedro Almeida.</span>\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import cluster\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explorando os dados"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/base-imoveis-123i/base_123i.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# A visual check of each column's value\ndef check_df_columns(df):\n    for col in df.columns:\n\n        print(f\" ----- {col} ----- \")\n        print(100 * (df[col].value_counts() / df.shape[0]))\n        \ncheck_df_columns(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print how many nulls on each column\ndef print_null_cols(df):\n    for col in df.columns:\n        nulls_value = df[col].isna().sum()\n        percentage = 100*(nulls_value / df.shape[0])\n        message = \"Column {} has {} nulls / {}% \".format(col, nulls_value, percentage)\n        print(message)\n        \nprint_null_cols(df)\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to clean data\ndef clean_data(df):\n    \n    # remove weird data with values equal to -1\n    df = df[df.maximum_estimate != -1]\n    df = df[df.minimum_estimate != -1]\n    df = df[df.point_estimate != -1]\n    df = df[df.garages != -1]\n    df = df[df.rooms != -1]\n    df = df[df.useful_area != -1]\n    \n    # remove cities with low data\n    city_dominance = 100 * (df['city'].value_counts() / df.shape[0])\n    for a, b in zip(city_dominance.index, city_dominance):\n        print(a)\n        print(b)\n        if b < 1:\n            df = df[df.city != a]\n            \n    # lower case state\n    df['state'] = [x.lower() for x in df['state']]\n    \n    return df\n\ndf = clean_data(df)\n\ndf = df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(100 * (df['city'].value_counts() / df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Google Maps API para aprimorar o dataset\n\nÉ possível utilizar o Google Maps API para recuperar o Bairro de cada imóvel utilizando as informações de **Latitude** e **Longitude** fornecidas no dataset. O código abaixo realiza esta extração e insere uma nova coluna no dataframe com o bairros.\n\n**Obs:** O código abaixo não foi rodado devido a necessidade de utilização da API do Google. Para recuperar os bairros da base de dados seria necessário gastar algumas centenas de reais pelo uso da API e por isso fica aqui somente como sugestão de que é possível ser feito."},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"\nneighborhood_list = []\nfor latitude, longitude in tqdm_notebook(zip(df['latitude'], df['longitude']), total=len(df)):\n\n    #ex: https://maps.googleapis.com/maps/api/geocode/json?key=APIKEY&latlng=-23.56417040,-46.65790930\n    \n    try:\n        response = requests.get('https://maps.googleapis.com/maps/api/geocode/json?key=APIKEY&latlng={0},{1}'.format(latitude, longitude))\n        resp_json_payload = response.json()        \n        #type_component = resp_json_payload['results'][0]['address_components'][2]['types']\n        type_component = resp_json_payload['results'][0]['address_components']\n        \n        neighborhood = None\n        for i in type_component:            \n            list_component_type = i['types']            \n            if 'sublocality' in list_component_type:            \n                neighborhood = i['long_name']\n                break\n    except:\n        neighborhood = None\n        \n    neighborhood_list.append(neighborhood)\n\ndf['neighborhood'] = neighborhood_list\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encontrando bairros através de Clustering\n\nConsiderando que obter as informações sobre o bairro dos imóveis usando uma fonte externa (como o Google Maps API) em alguns casos não seja possível ou desejável, uma outra forma de contornar este problema usando os dados fornecidos pela base é através de *Clustering*!\n\nPodemos clusterizar as informações de Latitude e Longitude para estimar as sublocalidades. Na função abaixo utilizamos o algoritmo de **Kmeans** para prever os bairros dos imóveis nos dados."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create cluster using Kmeans\ndef find_cluster(df):\n\n    location = df.copy()\n\n    columns_to_keep = ['latitude', 'longitude']\n\n    for col in location:\n        if col not in columns_to_keep:\n            location = location.drop(col, 1)\n    \n    location['lng_parsed'] = pd.to_numeric(location['longitude'], errors='coerce')\n    location['lat_parsed'] = pd.to_numeric(location['latitude'], errors='coerce')\n    \n    location = location.drop('longitude', 1)\n    location = location.drop('latitude', 1)\n\n    # Remove Outliers do dataframe\n    location = location[((location.lat_parsed - location.lat_parsed.mean()) / location.lat_parsed.std()).abs() < 3]\n    location = location[((location.lng_parsed - location.lng_parsed.mean()) / location.lng_parsed.std()).abs() < 3]\n    location = location[np.abs(location.lat_parsed-location.lat_parsed.mean()) <= (3*location.lat_parsed.std())]\n    location = location[np.abs(location.lng_parsed-location.lng_parsed.mean()) <= (3*location.lng_parsed.std())]\n\n    for i in range(0, len(df)):\n        if i not in location.index:\n            df = df[df.index != i]\n\n    df = df.reset_index(drop=True)\n\n    x1 = location.lng_parsed\n    x2 = location.lat_parsed\n    \n    # Plot charts and execute Kmeans clustering\n    plt.figure(num=None, figsize=(8, 6), facecolor='w', edgecolor='k')\n    plt.title('Latitude x Longitude')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.scatter(x1, x2)\n    plt.show()\n\n    \n    plt.figure(num=None, figsize=(8, 6), facecolor='w', edgecolor='k')\n    plt.scatter(location.lng_parsed, location.lat_parsed)\n    plt.title('Clustering neighborhoods')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n\n    location_full = location\n\n    location_np = np.array(location)\n\n    # Execute Kmeans clustering\n    # São Paulo has approximately 100 neighborhoods\n    k = 100 # Define the value of k\n    kmeans = cluster.KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(location_np)\n\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    for i in range(k):\n        # select only data observations with cluster label == i\n        ds = location_np[np.where(labels==i)]\n        # plot the data observations\n        plt.plot(ds[:,0],ds[:,1],'o', markersize=6)\n        # plot the centroids\n        lines = plt.plot(centroids[i,0],centroids[i,1],'kx')\n        # make the centroid x's bigger\n        plt.setp(lines,ms=8.0)\n        plt.setp(lines,mew=3.0)\n    plt.show()\n\n    list_areas_kmeans = []\n    for f, b in zip(labels, location_full.index):\n        list_areas_kmeans.append(f)\n\n    return list_areas_kmeans, df, location","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"list_areas_kmeans, df, location = find_cluster(df)\n\ndf['area_kmeans'] = list_areas_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Informações estretégicas\n\nPodemos utilizar as informações da base para fazer análises sobre coisas interessantes em relação a lógica de négocio do setor imobiliário. Responder algumas perguntas e chegar em algumas conclusões importantes com o uso de dados.\n\n## Variação de preços\n\nAbaixo podemos ver um gráfico estilo Boxplot que mostra a variação de preço dos imóveis (point_estimate) de acordo com as áreas encontradas pela clusterização. "},{"metadata":{"trusted":false},"cell_type":"code","source":"order_by_median = df.groupby(by=[\"area_kmeans\"])[\"point_estimate\"].median().sort_values(ascending=True).index\n\nimport matplotlib.ticker as ticker\n\nsns.set(rc={'figure.figsize':(17,5)})\nsns.set(palette=\"pastel\")\nsns.boxplot(x=\"area_kmeans\", y=\"point_estimate\", color=\"orange\", data=df, showfliers=False)\nsns.despine(offset=10, trim=True)\nax = plt.gca()\nax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\nax.xaxis.set_major_locator(ticker.MultipleLocator(base=5))\nplt.show()\n\nsns.set(rc={'figure.figsize':(17,5)})\nsns.set(font_scale=0.78)\nsns.boxplot(x=\"area_kmeans\", y=\"point_estimate\", palette=\"rainbow\", data=df, showfliers=False, order=order_by_median)\nsns.despine(offset=10, trim=True)\n#ax = plt.gca()\n#ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\n#ax.xaxis.set_major_locator(ticker.MultipleLocator(base=5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os clusters abaixo são as áreas que possuem a maior média de preço dos imóveis:"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(order_by_median[-5:].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saber quais são as áreas de uma cidade que possuem as maiores ou menores variabilidades nos preços é uma informação bastante importante para empresas que atuam no setor imobiliário. Estratégias de marketing, por exemplo, podem fazer uso dessa informação para elaborar ações em regiões específicas da cidade. Equipes responsáveis pela aquisição de novos imóveis também podem utilizar esta informação para identificar as diferentes realidades das diversas regiões da cidade."},{"metadata":{},"cell_type":"markdown","source":"## Heatmap\n\nPodemos utilizar os dados de Latitude e Longitude, junto com as áreas encontradas pelo clustering, para criar um **mapa de calor** onde pode ser visualizado todos os imóveis da base de dados. Conseguimos identificar quais regiões os imóveis da base estão localizados e também quais os clusters que possuem a maior média (marcador vermelho) e menor média (marcador azul) de preço dos imóveis."},{"metadata":{"trusted":false},"cell_type":"code","source":"latitude_list = []\nlongitude_list = []\nlabel_area_kmeans = []\nfor i in order_by_median[-5:]:\n    expensive_areas = df.loc[(df['area_kmeans'] == i)]\n    latitude_list.append(expensive_areas['latitude'][0:1])\n    longitude_list.append(expensive_areas['longitude'][0:1])\n    label_area_kmeans.append(i)\n    \nlatitude_list_less = []\nlongitude_list_less = []\nlabel_area_kmeans_less = []\nfor i in order_by_median[0:5]:\n    less_expensive_areas = df.loc[(df['area_kmeans'] == i)]\n    latitude_list_less.append(less_expensive_areas['latitude'][0:1])\n    longitude_list_less.append(less_expensive_areas['longitude'][0:1])\n    label_area_kmeans_less.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import folium\nfrom folium import plugins\n\nheatmap = df.copy()\nheatmap['count'] = 1\nbase_heatmap = folium.Map(location=['-23.5713874', '-46.6522521'], zoom_start=12)\n\nfor a, b, c in zip(latitude_list, longitude_list, label_area_kmeans):    \n    folium.Marker((a, b), popup=c, icon=folium.Icon(color='red')).add_to(base_heatmap)\n    \nfor a, b, c in zip(latitude_list_less, longitude_list_less, label_area_kmeans_less):    \n    folium.Marker((a, b), popup=c, icon=folium.Icon(color='darkblue')).add_to(base_heatmap)\n\nplugins.HeatMap(data=heatmap[['latitude', 'longitude', 'count']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist(), radius=8, max_zoom=4).add_to(base_heatmap)\n\nbase_heatmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A visualização dessas informações pode ser bastante importante para uma empresa que deseja, por exemplo, expandir para outras cidades onde não se possui conhecimento tácito sobre o local. Saber previamente qual a variação de preço dos imóveis das subregiões da cidade pode ser uma informação crucial para o sucesso do negócio."},{"metadata":{},"cell_type":"markdown","source":"# Modelo\n\nAbaixo é explorado uma possível solução para a precificação de imóveis utilizando machine learning. Foram rodadas versões diferentes do modelo como forma de mostrar alternativas para melhorar os resultados encontrados."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove unuseful columns\ndef drop_columns(df, list_columns):\n    df = df.drop(list_columns, axis=1)    \n    \n    return df\n\nlist_columns = ['address', 'tower_name', 'latitude', 'longitude', 'city', 'state']\ndf = drop_columns(df, list_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert some columns to dummies\ndef one_hot_encoder(df):\n    one_hot = pd.get_dummies(df['building_type'])\n    df = df.drop('building_type',axis = 1)\n    df = df.join(one_hot)\n    \n    return df\n\ndf = one_hot_encoder(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split data/target\ndef get_data_target(df, target):\n    y = df[target]    \n    X = df.drop(target,axis = 1)\n    \n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to train the model\ndef train(model):\n\n    model = model\n\n    model.fit(X_train, y_train,\n            eval_set = [(X_train, y_train), (X_test, y_test)],\n            eval_metric = 'rmse',\n            early_stopping_rounds = 5,\n            verbose=True)\n\n    best_iteration = model.get_booster().best_ntree_limit\n    preds = model.predict(X_test, ntree_limit=best_iteration)\n    \n    return preds, model, best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to evaluate some metrics\ndef evaluate_metrics(preds, model, best_iteration):\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    r2 = r2_score(y_test, preds, multioutput='variance_weighted')\n    \n    evals_result = model.evals_result()  \n    plt.rcParams[\"figure.figsize\"] = (8, 6)\n    xgb.plot_importance(model, max_num_features=None)\n    plt.show()\n\n    range_evals = np.arange(0, len(evals_result['validation_0']['rmse']))\n\n    val_0 = evals_result['validation_0']['rmse']\n    val_1 = evals_result['validation_1']['rmse']\n    plt.figure(num=None, figsize=(8, 6), facecolor='w', edgecolor='k')\n    plt.plot(range_evals, val_1, range_evals, val_0)\n    plt.ylabel('Validation error')\n    plt.xlabel('Iteration')\n    plt.show()\n    \n    print(\"Best iteration: %f\" % (int(best_iteration)))\n    print(\"RMSE: %f\" % (rmse))\n    print(\"R2: %f\" % (r2))\n    print('Observed value single sample: {}'.format(y_test[0:1].tolist()[0]))\n    print('Predicted value single sample: {}'.format(int(model.predict(X_test[0:1], ntree_limit=best_iteration)[0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treinamento e previsões\n\nAbaixo rodamos diversas versões do modelo e analisamos os resultados.\n\n<b>Primeiro modelo:</b>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# First model\ntarget = 'point_estimate'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analisando os resultados, fica claro que esta versão do modelo acima sofre de **Data Leakage**, pois as variáveis *minimum_estimate* e *maximum_estimate* são altamente correlacionadas com o target utilizado -> *point_estimate*. Isto acontece quando os dados que estamos usando para treinar o algoritmo contém informações do que nós estamos tentando prever. No caso, é possível descobrir o nosso target (point_estimate) simplesmente manipulando as outras variáveis do modelo, sem nem mesmo precisar utilizar Machine Learning.\n\nPor mais que estejamos utilizando *early stopping* para evitar overfitting no modelo, são as proprias features que estão causando este problema. Neste caso, é importante ficar atento, pois, por mais que este modelo esteja aparentemente gerando excelentes previsões com os dados de teste, o modelo não é generalizavel para dados novos.\n\nLogo abaixo rodamos uma outra versão do modelo, agora retirando as features *minimum_estimate* e *maximum_estimate*.\n\n<b>Segundo modelo:</b>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Second model\nlist_columns = ['minimum_estimate', 'maximum_estimate']\ndf = drop_columns(df, list_columns)\n\ntarget = 'point_estimate'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neste modelo anteior, como já era esperado, ocorreu um aumento do erro na métrica RMSE e uma diminuição do R2. Contudo, nos livramos das variáveis que estavam causando Data Leakage e temos um modelo mais confiável e generalizável.\n\nPodemos tentar melhorar mais ainda o modelo utilizando algumas técnicas para aprimoramento das *features*. A função **feature_engineering** gera mais seis novas *features* no dataset para tentar ajudar o modelo a ter uma melhor performance.\n\nCalculamos a média e o desvio padrão dos valores de quartos, garagens e área útil para cada cluster.\n\n<b>Terceiro modelo:</b>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to create new features\ndef feature_engineering(df):\n\n    mean_rooms_list = []\n    mean_garages_list = []\n    mean_useful_area_list = []    \n    std_rooms_list = []\n    std_garages_list = []\n    std_useful_area_list = []\n    \n    for i in tqdm_notebook(df.index):\n\n        rooms_df = df.loc[(df['rooms']) & ((df['area_kmeans'] ==  df['area_kmeans'][i]))]\n        mean_rooms = rooms_df['rooms'].mean()\n        mean_rooms_list.append(float(mean_rooms))\n        std_rooms = rooms_df['rooms'].std()\n        std_rooms_list.append(float(std_rooms))\n\n        garages_df = df.loc[(df['garages']) & ((df['area_kmeans'] ==  df['area_kmeans'][i]))]\n        mean_garages = garages_df['garages'].mean()\n        mean_garages_list.append(float(mean_garages))\n        std_garages = garages_df['garages'].std()\n        std_garages_list.append(float(std_garages))\n        \n        useful_area_df = df.loc[(df['useful_area']) & ((df['area_kmeans'] ==  df['area_kmeans'][i]))]\n        mean_useful_area = useful_area_df['useful_area'].mean()\n        mean_useful_area_list.append(float(mean_useful_area))\n        std_useful_area = useful_area_df['useful_area'].std()\n        std_useful_area_list.append(float(std_useful_area))\n\n    df['mean_rooms'] = mean_rooms_list\n    df['mean_garages'] = mean_garages_list\n    df['mean_useful_area'] = mean_useful_area_list\n    df['std_rooms'] = std_rooms_list\n    df['std_garages'] = std_garages_list\n    df['std_useful_area'] = std_useful_area_list    \n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = feature_engineering(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Third model\ntarget = 'point_estimate'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que o processo de *feature engineering*, ou seja, gerar novas *features* derivadas das *features* já existentes, fornece um bom resultado. O modelo melhorou e essas novas *features* estão com uma \"importância\" alta."},{"metadata":{},"cell_type":"markdown","source":"## Testando um dataset mais profundo\n\nCertamente um dos caminhos para melhorar o modelo de *valuation* de imóveis seria melhorar a base de dados. A base fornecida possui um número limitado de *features* em relação ao que é possível encontrar dentro da lógica do negócio.\n\n**Por exemplo:** Seria importante incluir mais características em relação aos imóveis, como quantidade de banheiros, suítes, elevadores, piscina, o bairro do imóvel, o valor do condomínio, se o imóvel é novo, se o apartamento está sendo vendido mobiliado, etc.\n\nQuanto mais características em relação aos imóveis, melhor será a performance do algoritmo, e várias características podem ser facilmente obtidas atráves de webscraping ou em parcerias com outras empresas que possuem uma base de dados de imóveis.\n\nAlguns meses atrás eu fiz o upload no Kaggle de um dataset com 13 mil imóveis localizados na cidade de São Paulo. Esses imóveis foram obtidos através do webscraping de diversos sites de imóveis do Brasil.\n\nApesar deste dataset ser menor em relação ao dataset fornecido para o case, ele possui mais *features*.\n\nLogo abaixo eu rodei o mesmo modelo utilizado acima, mas agora utilizando os dados desta nova base de dados, que possui mais features em relação a base fornecida para o case."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/sao-paulo-real-estate-sale-rent-april-2019/sao-paulo-properties-april-2019.csv')\ndf = df[df['Negotiation Type'] == 'sale']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Quarto modelo:</b>"},{"metadata":{"trusted":false},"cell_type":"code","source":"list_columns = ['Property Type', 'Latitude', 'Longitude', 'Negotiation Type']\ndf = drop_columns(df, list_columns)\n\n#df['District'] = df['District'].astype('category')\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['District'] = labelencoder.fit_transform(df['District'])\n\ntarget = 'Price'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A métrica **RMSE**, que avalia a perfomance do modelo, foi muito melhor com esta nova base de dados. Isto mostra que obter mais *features* em relação aos imóveis melhora o modelo de *valuation*."},{"metadata":{},"cell_type":"markdown","source":"## Uma previsão real\n\nSão inúmeras as possibilidades de aplicação deste tipo de modelo para o setor imobiliário. Conseguir saber com fundamento em dados o valor de um imóvel de acordo com suas características é de fato algo muito importante.\n\nAbaixo eu aplico o último modelo treinado em um imóvel da própria Loft localizado no bairro Jardim Paulista, utilizando as características apresentadas no site. Conseguimos assim obter uma previsão de preço, que representa um valor teórico dos imóveis com as mesmas características na região."},{"metadata":{},"cell_type":"markdown","source":"![title](https://i.imgur.com/K7Tow8m.png)"},{"metadata":{"trusted":false},"cell_type":"code","source":"sample = pd.DataFrame({'Condo': 1300, 'Size': 100, 'Rooms': 2, 'Toilets': 2, 'Suites': 1, 'Parking': 1, 'Elevator': 1, 'Furnished': 0, 'Swimming Pool': 0, 'New': 0, 'District': 40}, index=[0])\n\nprint(model.predict(sample, ntree_limit=best_iteration))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este tipo de análise pode ser útil tanto para a elaboração do preço final de venda do imóvel, quanto para a aquisição de imóveis. O output de resultados como este, em conjunto com o modelo de negócio, pode acelerar ainda mais a compra instantânea de imóveis."},{"metadata":{},"cell_type":"markdown","source":"\n# Conclusões e sugestões\n\nEste presente estudo procurou explorar a base de dados fornecida pela Loft e extrair informações importantes com implicações para o seu negócio.\n\nMesmo não possuindo informações sobre os bairros onde os imóveis da base estavam localizados, uma informação bastante importante no mercado imobiliário, foi possível utilizar ténicas de clusterização para obter uma aproximação dessa informação. Esses clusters se mostraram bastante importantes como *feature* (característica) para o modelo desenvolvido.\n\nDescobrimos que mesmo a base possuindo preços máximos e mínimos estimados para os imóveis, estas informações não são úteis para o desenvolvimento de um modelo de machine learning. Porque a utilização dessas informações leva ao problema de Data Leakage, onde um modelo aparenta bons resultados, mas não é generalizável para dados novos.\n\nVimos que realizar processos de *feature engineering*, onde novas *features* são criadas derivadas das *features* já existentes, é bastante relevante para melhorar as previsões do modelo.\n\nPor fim, apresentei uma sugestão de outra estrutura de base de dados do setor imobiliário, desenvolvida por mim alguns meses atrás, que possui informações mais detalhadas sobre imóveis na região de São Paulo. Esta nova base se apresentou mais eficaz e mostrou melhores resultados quando aplicada ao modelo de *valuation* de imóveis."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}