{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Preparing Dataset for Classification**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import minmax_scale  \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom matplotlib.lines import Line2D\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\nprint(os.listdir(\"../input/dataset-wine\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions\n\n# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n    \n# Showing Incorrect Classification\ndef plot_comp_test_data(X_test, y_test, y_head, title):\n    plt.figure(figsize=(12,6))\n    plt.scatter(X_test[:,0], X_test[:,6], c=[colors[i] for i in y_head], marker=\"*\", s=60)\n    n = np.size(y_head)\n    for i in range(0, n):\n        if y_head[i] != y_test[i]:\n            plt.scatter(X_test[i,0], X_test[i,6], c=[colors[y_test[i]]], marker=\"X\", s=120, alpha=.4)\n    legend_elements = [Line2D([0], [0], marker='*', color='w', label='Prediction', markerfacecolor='k', markersize=14),\n                       Line2D([0], [0], marker='X', color='w', label='Correct Class', markerfacecolor='k', markersize=12)]\n    plt.legend(handles=legend_elements)\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Defining dataset and browsing content\ndata = pd.read_csv('../input/dataset-wine/wine.csv')\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show first 10 sample\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data : X(feature) and y(target) \nX = np.array(data.drop(['Wine'],1))\ny = np.array(data['Wine'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization Data\nfig = plt.figure(1, figsize=(12, 6))\nax = Axes3D(fig)\ncolors = {1:\"r\", 2:\"g\", 3:\"b\"}\nax.scatter(X[:,0], X[:,6], c=[colors[i] for i in y])\nax.set_xlabel(\"Alcohol\")\nax.set_ylabel(\"Flavanoids\")\nplt.title(\"Values of Wine Data\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling each feature to a 0 to 1\nX = minmax_scale(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Train and Test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nprint(\"X_train\", X_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_test\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **k-Nearest Neighbors (k-NN) Classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create k-NN classification model with GridSearchCV\n# n_neighbors: Number of neighbors to use\n# weights: weights is used for distance function (uniform: All points in each neighborhood are weighted equally ||\n#                                                 distance: Weight points by the inverse of their distance.)\n# p: Power parameter for calculate distance. (1: Manhattan Distance. 2: Eucledian Distance. 3 and upper: Minkowski Distance.)\nknn_grid = {\"n_neighbors\":np.arange(1,15), \"weights\":[\"uniform\", \"distance\"], \"p\":[1, 2, 3] }\nknn = GridSearchCV(KNeighborsClassifier(), knn_grid, cv=10, iid=False)\nknn.fit(X_train, y_train)\n\nprint(\"k-NN Tuned Hyperparameters\", knn.best_params_)\nprint(\"k-NN Tuned Best Score:\", round(knn.best_score_,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use best classification model\nbest_clf_knn = knn.best_estimator_\nbest_clf_knn.fit(X_train, y_train)\nknn_y_head = best_clf_knn.predict(X_test)\nprint(\"k-Nearest Neighbors (k-NN) Classification Accuracy: {}%\" .format(round(best_clf_knn.score(X_test, y_test)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, knn_y_head,title=\"k-Nearest Neighbors Classification\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Confusion Matrix\nplot_cm(y_test, knn_y_head, title=\"k-NN Confusion Matrix\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ** Decision Tree (DT) Classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create DT classification model with GridSearchCV\n# criterion: The function to measure the quality of a split. (Gini: Gini impurity. || Entropy: Information gain)\n# min_samples_split: The minimum number of samples required to split an internal node.\n# min_samples_leaf: The minimum number of samples required to be at a leaf node.\ndtree_grid = {\"criterion\":[\"gini\", \"entropy\"], \"min_samples_split\":[3,4,5,6,7,8,9,10], \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9]}\ndtree = GridSearchCV(DecisionTreeClassifier(random_state=42), dtree_grid, cv=10, iid=False)\ndtree.fit(X_train, y_train)\n\nprint(\"DT Tuned Hyperparameters\", dtree.best_params_)\nprint(\"DT Tuned Best Score:\", round(dtree.best_score_,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use best classification model\nbest_clf_dt = dtree.best_estimator_\nbest_clf_dt.fit(X_train, y_train)\ndtree_y_head = best_clf_dt.predict(X_test)\nprint(\"Decision Tree (DT) Classification Accuracy: {}%\" .format(round(best_clf_dt.score(X_test, y_test)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, dtree_y_head, title=\"Decision Tree Classification\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Confusion Matrix \nplot_cm(y_test, dtree_y_head, title=\"DT Confusion Matrix\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ** Random Forest (RF) Classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create RF classification model with GridSearchCV\n# n_estimators: The number of trees in the forest.\n# criterion, min_samples_split and min_samples_leaf are same to decision tree parameters.\nrf_grid = {\"n_estimators\": [50, 100, 150], \"criterion\": [\"gini\", \"entropy\"],\n           \"min_samples_split\":[3,4,5,6,7,8,9,10], \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9]}\nrforest = GridSearchCV(RandomForestClassifier(random_state=42), rf_grid, cv=10, iid=False)\nrforest.fit(X_train, y_train)\n\nprint(\"RF Tuned Hyperparameters\", rforest.best_params_)\nprint(\"RF Tuned Best Score:\", round(rforest.best_score_,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf_rforest = rforest.best_estimator_\nbest_clf_rforest.fit(X_train, y_train)\nrforest_y_head = best_clf_rforest.predict(X_test)\nprint(\"Random Forest (RF) Classification Accuracy: {}%\" .format(round(best_clf_rforest.score(X_test, y_test)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, rforest_y_head, title=\"Random Forest Classification\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Confusion Matrix\nplot_cm(y_test, rforest_y_head, title=\"RF Confusion Matrix\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Support Vector Machine (SVM) Classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create SVM classification model with GridSearchCV\n# C: Penalty parameter C of the error term.\n# kernel: Specifies the kernel type to be used in the algorithm.\n# gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\nsvm_grid = {\"C\":[0.001, 0.01, 0.1, 1, 10, 100], \"kernel\":[\"rbf\", \"poly\", \"linear\"],\n        \"gamma\":[\"auto\", \"scale\"]}\n\nsvm = GridSearchCV(SVC(decision_function_shape='ovo'), svm_grid, cv=10, iid=False)\nsvm.fit(X_train, y_train)\n\nprint(\"SVM Tuned Hyperparameters\", svm.best_params_)\nprint(\"SVM Tuned Best Score:\", round(svm.best_score_,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use best classification model\nbest_clf_svm = svm.best_estimator_\nbest_clf_svm.fit(X_train, y_train)\nsvm_y_head = best_clf_svm.predict(X_test)\nprint(\"Support Vector Machine (SVM) Classification Accuracy: {}%\" .format(round(best_clf_svm.score(X_test, y_test)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, svm_y_head, title=\"Support Vector Machine Classification\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Confusion Matrix \nplot_cm(y_test, svm_y_head, title=\"SVM Confusion Matrix\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **Naive Bayes (NB) Classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Naive Bayes model GaussianNB classifier\n# priors: Prior probabilities of the classes.\nnb = GaussianNB(priors=None)\nnb.fit(X_train, y_train)\n\nprint(\"Gaussian Naive Bayes (NB) Classification Accuracy: {}%\" .format(round(nb.score(X_test, y_test)*100, 2)))\nnb_y_head = nb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Incorrect Classification\nplot_comp_test_data(X_test, y_test, nb_y_head, title=\"Naive Bayes Classification\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Confusion Matrix\nplot_cm(y_test, nb_y_head,title=\"NB Confusion Matrix\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}