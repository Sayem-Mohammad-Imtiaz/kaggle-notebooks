{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Exploracion inicial "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/yelow-taxi-2019-01/yellow_tripdata_2019-01.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"son 7,6 millones de registros en el periodo de enero del 2019"},{"metadata":{},"cell_type":"markdown","source":"# Tipo de datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2019-01-14 17:24:55\ndata['tpep_pickup_datetime'] = pd.to_datetime(data['tpep_pickup_datetime'] ,format='%Y-%m-%d %H:%M:%S')\ndata['tpep_dropoff_datetime'] = pd.to_datetime(data['tpep_dropoff_datetime'] ,format='%Y-%m-%d %H:%M:%S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.sort_values(by=['tpep_pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. EDA\n1. Analisis de zonas \n2. Viajes por distrito"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shapefile\nfrom shapely.geometry import Polygon\nfrom descartes.patch import PolygonPatch\nimport matplotlib as mpl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# utilitly functions\ndef get_lat_lon(sf):\n    content = []\n    for sr in sf.shapeRecords():\n        shape = sr.shape\n        rec = sr.record\n        loc_id = rec[shp_dic['LocationID']]\n        \n        x = (shape.bbox[0]+shape.bbox[2])/2\n        y = (shape.bbox[1]+shape.bbox[3])/2\n        \n        content.append((loc_id, x, y))\n    return pd.DataFrame(content, columns=[\"LocationID\", \"longitude\", \"latitude\"])\n\ndef get_boundaries(sf):\n    lat, lon = [], []\n    for shape in list(sf.iterShapes()):\n        lat.extend([shape.bbox[0], shape.bbox[2]])\n        lon.extend([shape.bbox[1], shape.bbox[3]])\n\n    margin = 0.01 # buffer to add to the range\n    lat_min = min(lat) - margin\n    lat_max = max(lat) + margin\n    lon_min = min(lon) - margin\n    lon_max = max(lon) + margin\n\n    return lat_min, lat_max, lon_min, lon_max\n\ndef draw_region_map(ax, sf, heat={}):\n    continent = [235/256, 151/256, 78/256]\n    ocean = (89/256, 171/256, 227/256)    \n    \n    reg_list={'Staten Island':1, 'Queens':2, 'Bronx':3, 'Manhattan':4, 'EWR':5, 'Brooklyn':6}\n    reg_x = {'Staten Island':[], 'Queens':[], 'Bronx':[], 'Manhattan':[], 'EWR':[], 'Brooklyn':[]}\n    reg_y = {'Staten Island':[], 'Queens':[], 'Bronx':[], 'Manhattan':[], 'EWR':[], 'Brooklyn':[]}\n    \n    # colorbar\n    if len(heat) != 0:\n        norm = mpl.colors.Normalize(vmin=math.sqrt(min(heat.values())), vmax=math.sqrt(max(heat.values()))) #norm = mpl.colors.LogNorm(vmin=1,vmax=max(heat))\n        cm=plt.get_cmap('Reds')\n        #sm = plt.cm.ScalarMappable(cmap=cm, norm=norm)\n        #sm.set_array([])\n        #plt.colorbar(sm, ticks=np.linspace(min(heat.values()),max(heat.values()),8), \\\n        #             boundaries=np.arange(min(heat.values())-10,max(heat.values())+10,.1))\n    \n    ax.set_facecolor(ocean)\n    for sr in sf.shapeRecords():\n        shape = sr.shape\n        rec = sr.record\n        reg_name = rec[shp_dic['borough']]\n        \n        if len(heat) == 0:\n            norm = mpl.colors.Normalize(vmin=1,vmax=6) #norm = mpl.colors.LogNorm(vmin=1,vmax=max(heat))\n            cm=plt.get_cmap('Pastel1')\n            R,G,B,A = cm(norm(reg_list[reg_name]))\n            col = [R,G,B]\n        else:\n            R,G,B,A = cm(norm(math.sqrt(heat[reg_name])))\n            col = [R,G,B]\n            \n        # check number of parts (could use MultiPolygon class of shapely?)\n        nparts = len(shape.parts) # total parts\n        if nparts == 1:\n            polygon = Polygon(shape.points)\n            patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n            ax.add_patch(patch)\n        else: # loop over parts of each shape, plot separately\n            for ip in range(nparts): # loop over parts, plot separately\n                i0 = shape.parts[ip]\n                if ip < nparts-1:\n                    i1 = shape.parts[ip+1]-1\n                else:\n                    i1 = len(shape.points)\n\n                polygon = Polygon(shape.points[i0:i1+1])\n                patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n                ax.add_patch(patch)\n                \n        reg_x[reg_name].append((shape.bbox[0]+shape.bbox[2])/2)\n        reg_y[reg_name].append((shape.bbox[1]+shape.bbox[3])/2)\n        \n    for k in reg_list:\n        if len(heat)==0:\n            plt.text(np.mean(reg_x[k]), np.mean(reg_y[k]), k, horizontalalignment='center', verticalalignment='center',\n                        bbox=dict(facecolor='black', alpha=0.5), color=\"white\", fontsize=12)     \n        else:\n            plt.text(np.mean(reg_x[k]), np.mean(reg_y[k]), \"{}\\n({}K)\".format(k, heat[k]/1000), horizontalalignment='center', \n                     verticalalignment='center',bbox=dict(facecolor='black', alpha=0.5), color=\"white\", fontsize=12)       \n\n    # display\n    limits = get_boundaries(sf)\n    plt.xlim(limits[0], limits[1])\n    plt.ylim(limits[2], limits[3])\n\ndef draw_zone_map(ax, sf, heat={}, text=[], arrows=[]):\n    continent = [235/256, 151/256, 78/256]\n    ocean = (89/256, 171/256, 227/256)\n    theta = np.linspace(0, 2*np.pi, len(text)+1).tolist()\n    ax.set_facecolor(ocean)\n    \n    # colorbar\n    if len(heat) != 0:\n        norm = mpl.colors.Normalize(vmin=min(heat.values()),vmax=max(heat.values())) #norm = mpl.colors.LogNorm(vmin=1,vmax=max(heat))\n        cm=plt.get_cmap('Reds')\n        sm = plt.cm.ScalarMappable(cmap=cm, norm=norm)\n        sm.set_array([])\n        plt.colorbar(sm, ticks=np.linspace(min(heat.values()),max(heat.values()),8),\n                     boundaries=np.arange(min(heat.values())-10,max(heat.values())+10,.1))\n    \n    for sr in sf.shapeRecords():\n        shape = sr.shape\n        rec = sr.record\n        loc_id = rec[shp_dic['LocationID']]\n        zone = rec[shp_dic['zone']]\n        \n        if len(heat) == 0:\n            col = continent\n        else:\n            if loc_id not in heat:\n                R,G,B,A = cm(norm(0))\n            else:\n                R,G,B,A = cm(norm(heat[loc_id]))\n            col = [R,G,B]\n\n        # check number of parts (could use MultiPolygon class of shapely?)\n        nparts = len(shape.parts) # total parts\n        if nparts == 1:\n            polygon = Polygon(shape.points)\n            patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n            ax.add_patch(patch)\n        else: # loop over parts of each shape, plot separately\n            for ip in range(nparts): # loop over parts, plot separately\n                i0 = shape.parts[ip]\n                if ip < nparts-1:\n                    i1 = shape.parts[ip+1]-1\n                else:\n                    i1 = len(shape.points)\n\n                polygon = Polygon(shape.points[i0:i1+1])\n                patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n                ax.add_patch(patch)\n        \n        x = (shape.bbox[0]+shape.bbox[2])/2\n        y = (shape.bbox[1]+shape.bbox[3])/2\n        if (len(text) == 0 and rec[shp_dic['Shape_Area']] > 0.0001):\n            plt.text(x, y, str(loc_id), horizontalalignment='center', verticalalignment='center')            \n        elif len(text) != 0 and loc_id in text:\n            #plt.text(x+0.01, y-0.01, str(loc_id), fontsize=12, color=\"white\", bbox=dict(facecolor='black', alpha=0.5))\n            eta_x = 0.05*np.cos(theta[text.index(loc_id)])\n            eta_y = 0.05*np.sin(theta[text.index(loc_id)])\n            ax.annotate(\"[{}] {}\".format(loc_id, zone), xy=(x, y), xytext=(x+eta_x, y+eta_y),\n                        bbox=dict(facecolor='black', alpha=0.5), color=\"white\", fontsize=12,\n                        arrowprops=dict(facecolor='black', width=3, shrink=0.05))\n    if len(arrows)!=0:\n        for arr in arrows:\n            ax.annotate('', xy = arr['dest'], xytext = arr['src'], size = arr['cnt'],\n                    arrowprops=dict(arrowstyle=\"fancy\", fc=\"0.6\", ec=\"none\"))\n    \n    # display\n    limits = get_boundaries(sf)\n    plt.xlim(limits[0], limits[1])\n    plt.ylim(limits[2], limits[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sf = shapefile.Reader('../input/taxi-zone/taxi_zones/taxi_zones.shp')\nfields_name = [field[0] for field in sf.fields[1:]]\nshp_dic = dict(zip(fields_name, list(range(len(fields_name)))))\nattributes = sf.records()\nshp_attr = [dict(zip(fields_name, attr)) for attr in attributes]\n\ndf_loc = pd.DataFrame(shp_attr).join(get_lat_lon(sf).set_index(\"LocationID\"), on=\"LocationID\")\ndf_loc.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,8))\nax = plt.subplot(1, 2, 1)\nax.set_title(\"Boroughs in NYC\")\ndraw_region_map(ax, sf)\nax = plt.subplot(1, 2, 2)\nax.set_title(\"Zones in NYC\")\ndraw_zone_map(ax, sf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zone_dict = dict(zip(df_loc.LocationID, df_loc.zone))\ndistrict_dict = dict(zip(df_loc.LocationID, df_loc.borough))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zone_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['pickup_zona'] = data.PULocationID.map(zone_dict)\ndata['pickup_district'] = data.PULocationID.map(district_dict)\ndata['dropoff_zona'] = data.DOLocationID.map(zone_dict)\ndata['dropoff_district'] = data.DOLocationID.map(district_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_histogram(title: str = '<No title specified>', \n                   field_names=None, xlabel_title='<Not specified>', ylabel_title='<Not specified>', \n                   xticks_label_rotation: int = None,\n                   sort_values: bool = None, bins=10, width: int = 20.0, height: int = 5.0):\n    plt.rcParams['figure.figsize'] = [width, height]\n    if xticks_label_rotation:\n        plt.xticks(rotation=xticks_label_rotation)\n    plt.suptitle(title, fontsize=\"xx-large\", fontstyle=\"normal\")\n    if sort_values:\n        ax = data[field_names].sort_values(ascending=sort_values).hist(bins=bins)\n    else:\n        ax = data[field_names].hist(bins=bins)\n    ax.set_xlabel(xlabel_title)\n    ax.set_ylabel(ylabel_title)\n\ndef plot_value_counts(title: str = '<No title specified>', \n                      field_names=None, xlabel_title='<Not specified>', ylabel_title='<Not specified>',\n                      kind='barh', value_filter_func=None,\n                      width: int = 20.0, height: int = 5.0):\n    plt.rcParams['figure.figsize'] = [width, height]\n    value_counts_dropoff = data[field_names].value_counts()\n    value_counts_dropoff = value_counts_dropoff.sort_values(ascending=True)\n    if value_filter_func: \n        cutoff_filter = value_filter_func(value_counts_dropoff)\n        ax = value_counts_dropoff[cutoff_filter].plot(kind=kind, title=title)\n    else:\n        ax = value_counts_dropoff.plot(kind=kind, title=title)\n    \n    ax.set_xlabel(xlabel_title)\n    ax.set_ylabel(ylabel_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(\"Distrito mas popular de salida\", 'pickup_district', \n               xlabel_title='NYC Distritos',ylabel_title='Viajes realizados',)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(\"Distrito mas popular de llegada\", 'dropoff_district',\n                ylabel_title='Viajes realizados', xlabel_title='NYC Distrito',)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(\"Zona mas popular de salida\", \n                  xlabel_title='Viajes realizados', ylabel_title='NYC Vecindarios',\n                  field_names=['PULocationID','pickup_zona', 'pickup_district'], \n                  value_filter_func=lambda x: x > 10_000,\n                  width=15.0, height=20.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_counts(\"Zona mas popular de llegada\", \n                  xlabel_title='Viajes realizados', ylabel_title='NYC Distrito',\n                  field_names=['dropoff_zona', 'dropoff_district'], \n                  value_filter_func=lambda x: x > 10_000,\n                  width=15.0, height=15.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Cleaning\n1. Viajes fuera de fecha\n2. Numero de pasajeros\n3. Distancia de los viajes\n3. Duracion de los viajes \n4. Velocidad de los viajes"},{"metadata":{},"cell_type":"markdown","source":"# Viajes fuera de fecha"},{"metadata":{},"cell_type":"markdown","source":"Los viajes de salida deben estar comprendidos en el periodo de enero del 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"tpep_pickup_datetime\"].max())\nprint(data[\"tpep_pickup_datetime\"].min())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"menor al periodo enero 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.tpep_pickup_datetime < '20190101']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_clean = data[data.tpep_pickup_datetime >= '20190101']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"mayor al periodo enero 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean[data_clean.tpep_pickup_datetime >= '20190201']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean = data_clean[data_clean.tpep_pickup_datetime < '20190201']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_clean[\"tpep_pickup_datetime\"].max())\nprint(data_clean[\"tpep_pickup_datetime\"].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"habian 441 datos por debajo del periodo enero 2019 y 96 datos por arriba del periodo"},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot(columna,label,data):\n    fig = plt.figure(figsize = (10,6))\n    ax = sns.boxplot(columna, data = data, orient = \"v\")\n\n    plt.tick_params(labelsize = 20)\n    plt.ylabel(label, fontsize = 20)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Numero de pasajeros\nLa cantidad máxima de pasajeros permitidos en un taxi amarillo por ley es de cuatro (4) en un taxi de cuatro (4) pasajeros o de cinco (5) pasajeros en un taxi de cinco (5) pasajeros, excepto que se debe aceptar un pasajero adicional si el pasajero es menor de siete (7) años y está sostenido en el regazo de un pasajero adulto sentado en la parte trasera \".\n\nLos viajes con 0 pasajeros son considerados por omision de dato y no deberia eliminarse. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean.passenger_count.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('passenger_count','passenger_count',data_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean = data_clean[data_clean.passenger_count<=7]\nboxplot('passenger_count','passenger_count',data_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distancia de los viajes"},{"metadata":{},"cell_type":"markdown","source":"La distancia mas larga que se puede recorrer entre los distritos mas alejados es de 40 millas, lo que indica que no se debe sobrepasar dichos limites.\nLa distancia de los viajes deben ser mayor a cero. Sino seria un error."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean.trip_distance.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('trip_distance','Trip Distance(Millas)',data_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean = data_clean[(data_clean.trip_distance>0.0)&(data_clean.trip_distance<40.0)]\n\nboxplot('trip_distance','Trip Distance(Millas)',data_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Duracion y velocidad de los viajes\nAl menos un viaje debe durar 30 segundos para ser considerado sino seria un error. Tambien los viajes permitidos no deben sobresar las 12 horas segun reglamento. \nSe crean dos columnas : \n1. Trip_duration el cual es la duracion del viaje en minutos\n2. speed es la velocidad promedio del viaje expresado en millas por hora"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean[\"trip_duration\"] = (data_clean[\"tpep_dropoff_datetime\"] - data_clean[\"tpep_pickup_datetime\"])/ np.timedelta64(1, 'm')\ndata_clean[\"speed\"] = (data_clean[\"trip_distance\"]/data_clean[\"trip_duration\"])*60  #speed  millas/hr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('trip_duration','trip_duration(minutos)',data_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean = data_clean[(data_clean.trip_duration > 0.5 )&(data_clean.trip_duration < 12*60)]\nboxplot('trip_duration','trip_duration(minutos)',data_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La velocidad maxima permitida es variable , pero debe ser fisicamente posible en los limites de 50 millas por hora para una ciudad transitada"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('speed','speed(millas por hora)',data_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quantile_speed = data_clean.speed.quantile(np.round(np.arange(0.00, 1.01, 0.01), 2))\nprint(quantile_speed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Existe  0.01% de viajes que tienen velocidades irrazonables, por lo tanto son errores de medicion"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean = data_clean[(data_clean.speed > 0 )&(data_clean.speed < 40)]\nboxplot('speed','speed(millas por hora)',data_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Costo por viaje"},{"metadata":{},"cell_type":"markdown","source":"¿Cómo se calculan las tarifas estándar?\n\nCargo inicial de $ 2.50.\n\nMás 50 centavos por 1/5 de milla cuando viaja a más de 12 mph o por 60 segundos en tráfico lento o cuando el vehículo está parado.\n\nMás un recargo de 50 centavos por noche de 8 p.m. a 6 a.m.\n\nMás un recargo de $ 1.00 por hora punta de 4:00 p.m. a 8:00 p.m. los días de semana, excepto los días festivos.\n\nMás un recargo por congestión del estado de Nueva York de $ 2.50 \n\nMás propinas y peajes.\n\nNo hay cargo por pasajeros adicionales, equipaje o maletas, ni pago con tarjeta de crédito.\n\nEl mensaje de tarifa en pantalla debe leer: \"Tarifa n. ° 01 - Tarifa estándar de la ciudad\".\n\nAsegúrese de llevar siempre su recibo.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('fare_amount','costo viaje',data_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quantile_costo = data_clean.fare_amount.quantile(np.round(np.arange(0.00, 1.01, 0.01), 2))\nprint(quantile_costo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removemos los viajes que tienen costo menor a 2.5 dolares por ser el cargo inicial y los pagos maximos estan por los 52 dolares."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean = data_clean[(data_clean.fare_amount >= 2.5)&(data_clean.fare_amount < 55)]\nboxplot('fare_amount','costo viaje',data_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Agregacion de datos\n1. Resampling por hora"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['tpep_pickup_datetime',\n            'PULocationID',\n\n          ]\ndata = data_clean[columns]\n#data = data.set_index(\"tpep_pickup_datetime\")\n#data_resample =data.resample('1H').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data =data.groupby('PULocationID').resample('D', on='tpep_pickup_datetime').count()\ndata = data.drop(\"tpep_pickup_datetime\", axis=1)\ndata = data.unstack('PULocationID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nheat_map = sns.heatmap(data, yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seleccionando la zona"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns2 = ['tpep_pickup_datetime',\n            'PULocationID',\n            'pickup_district'\n          ]\n\ndata_manhattan = data_clean[columns2]\ndata_manhattan = data_manhattan[data_manhattan.pickup_district=='Manhattan']\ndata_manhattan = data_manhattan.drop('pickup_district', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_manhattan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\nResampling por dia"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_manhattanxdia = data_manhattan.groupby('PULocationID').resample('D', on='tpep_pickup_datetime').count()\ndata_manhattanxdia = data_manhattanxdia.drop(\"tpep_pickup_datetime\", axis=1)\ndata_manhattanxdia = data_manhattanxdia.unstack('PULocationID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_manhattanxdia","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nsns.heatmap(data_manhattanxdia, yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nzonas_test= [237,236,161,162] # zonas con mas movimiento\nfor i in zonas_test:\n    plt.plot(data_manhattanxdia['PULocationID'][i], label=i)\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=len(zonas_test), mode=\"expand\", borderaxespad=0. , prop={\"size\":20})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La actividad de taxis disminuye en los fines de semana.\nEnero tuvo 4 fines de semana:\nSe observan 4 valles que corresponden a los fines de semanas."},{"metadata":{},"cell_type":"markdown","source":"# resampling por  hora"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_manhattanxhora =data_manhattan.groupby('PULocationID').resample('H', on='tpep_pickup_datetime').count()\ndata_manhattanxhora = data_manhattanxhora.drop(\"tpep_pickup_datetime\", axis=1)\ndata_manhattanxhora = data_manhattanxhora.unstack('PULocationID')\ndata_manhattanxhora.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nzonas_test= [237,236,161,162]\nfor i in zonas_test:\n    plt.plot(data_manhattanxhora['PULocationID'][i], label=i)\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=len(zonas_test), mode=\"expand\", borderaxespad=0. , prop={\"size\":20})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El mes de enero tiene 31 dias\nSe pueden ver 31 picos que corresponden a las horas activas (horas laborales) y los valles corresponden a las horas inactivas (horas de descanso)"},{"metadata":{},"cell_type":"markdown","source":"# Resampling cada 10 minutos"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_manhattanx10min =data_manhattan.groupby('PULocationID').resample('10Min', on='tpep_pickup_datetime').count()\ndata_manhattanx10min = data_manhattanx10min.drop(\"tpep_pickup_datetime\", axis=1)\ndata_manhattanx10min = data_manhattanx10min.unstack('PULocationID')\ndata_manhattanx10min.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(data_manhattanx10min.isna().sum()['PULocationID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_manhattanx10min.columns[data_manhattanx10min.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_manhattanx10min= data_manhattanx10min.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nzonas_test= [237,236,161,162]\nfor i in zonas_test:\n    plt.plot(data_manhattanx10min['PULocationID'][i], label=i)\nplt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left', ncol=len(zonas_test), mode=\"expand\", borderaxespad=0. , prop={\"size\":20})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelado"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.preprocessing import RobustScaler\nseries = data_manhattanxhora['PULocationID'][237].to_numpy()\nseries = series.reshape((len(series), 1))\ntransformer = RobustScaler()\ntransformer = transformer.fit(series)\nseries =transformer.transform(series)"},{"metadata":{"trusted":true},"cell_type":"code","source":"series = data_manhattanx10min['PULocationID'][237].to_numpy()\nplt.figure(figsize=(30,15))\nplt.plot(series, label = \"serie\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_time = 3500\ntime = np.arange(len(series), dtype=\"float32\")\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_test = time[split_time:]\nx_test = series[split_time:]\n\nplt.figure(figsize=(30,15))\nplt.plot(time_train,x_train, label = \"train\")\nplt.plot(time_test,x_test, label = \"test\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    #series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[-1]))\n    return ds.batch(batch_size).prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"window_size = 5\nbatch_size = 2\nshuffle_buffer_size = 1000\nwindow=windowed_dataset(series, window_size, batch_size, shuffle_buffer_size)\nprint(window)\nfor x, y in window:\n  print(\"x \" ,x.numpy(),\" y \", y.numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelo 1 : Redes Densas"},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 7\nbatch_size = 64\ntf.keras.backend.clear_session()\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, input_shape=[window_size])\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=20)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = []\nresults = []\nfor time in range(len(time_test)):\n  #print(series[time:time + window_size][np.newaxis] , \" --> \",model.predict(series[time:time + window_size][np.newaxis]))\n    forecast.append(model.predict(series[split_time+time-window_size:split_time+time][np.newaxis]))\n\n#forecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.plot(results, label =\"pronostico\")\nplt.plot(x_test, label = \"serie\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelo 2 : Redes Recurrentes GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 7\nbatch_size = 64\ntf.keras.backend.clear_session()\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]),\n    tf.keras.layers.GRU(64),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x:x*100)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=30)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = []\nresults = []\nfor time in range(len(time_test)):\n  #print(series[time:time + window_size][np.newaxis] , \" --> \",model.predict(series[time:time + window_size][np.newaxis]))\n    forecast.append(model.predict(series[split_time+time-window_size:split_time+time][np.newaxis]))\n\n#forecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.plot(results, label =\"pronostico\")\nplt.plot(x_test, label = \"serie\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series[split_time+time-window_size:split_time+time][np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelo 3 : Redes Recurrentes  LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 7\nbatch_size = 64\ntf.keras.backend.clear_session()\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n#(batch_size,timestamp, series_dimensionality)\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]),\n    #tf.keras.layers.LSTM(100, return_sequences=True),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x:x*200)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=30)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = []\nresults = []\nfor time in range(len(time_test)):\n  #print(series[time:time + window_size][np.newaxis] , \" --> \",model.predict(series[time:time + window_size][np.newaxis]))\n    forecast.append(model.predict(series[split_time+time-window_size:split_time+time][np.newaxis]))\n\n#forecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\nplt.plot(results, label =\"pronostico\")\nplt.plot(x_test, label = \"serie\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_test, results).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelo 4 : Redes Recurrentes BiLSTM"},{"metadata":{},"cell_type":"markdown","source":"Modelo 4 : BiLSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 7\nbatch_size = 64\nshuffle_buffer_size = 1000\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]),\n    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x:x*200)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=30)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = []\nresults = []\nfor time in range(len(time_test)):\n  #print(series[time:time + window_size][np.newaxis] , \" --> \",model.predict(series[time:time + window_size][np.newaxis]))\n    forecast.append(model.predict(series[split_time+time-window_size:split_time+time][np.newaxis]))\n\n#forecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.plot(results, label =\"pronostico\")\nplt.plot(x_test, label = \"serie\")\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_test, results).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelo 5 : Red hibrida ( CONV+LSTM )"},{"metadata":{"trusted":true},"cell_type":"code","source":"def windowed_dataset2(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)\n\n\nwindow_size = 7\nbatch_size = 64\nshuffle_buffer_size = 1000\ndataset2 = windowed_dataset2(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x:x*200)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset2,epochs=30)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"forecast = []\nresults = []\nfor time in range(len(time_test)):\n  #print(series[time:time + window_size][np.newaxis] , \" --> \",model.predict(series[time:time + window_size][np.newaxis]))\n    forecast.append(model.predict(series[split_time+time-window_size:split_time+time][np.newaxis]))\n\n#forecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.plot(results, label =\"pronostico\")\nplt.plot(x_test, label = \"serie\")\nplt.legend()"},{"metadata":{},"cell_type":"markdown","source":"Como resultados se tiene:\nPara 64 nodos"},{"metadata":{},"cell_type":"markdown","source":"# Multivariable"},{"metadata":{},"cell_type":"markdown","source":"# Para 2 entradas y 2 salidas"},{"metadata":{"trusted":true},"cell_type":"code","source":"series1 = data_manhattanx10min['PULocationID'][237].to_numpy()[np.newaxis].transpose()\nseries2 = data_manhattanx10min['PULocationID'][90].to_numpy()[np.newaxis].transpose()\nserie_multi = np.append(series1, series2,axis=1)\nplt.figure(figsize=(30,15))\nplt.plot(serie_multi, label = \"serie\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"x = np.array([[10,20,30]]).transpose()\ny = np.array([[100, 200,300]]).transpose()\nprint(np.append(x, y, axis=1))"},{"metadata":{"trusted":true},"cell_type":"code","source":"split_time = 3500\ntime = np.arange(len(series1), dtype=\"float32\")\ntime_train = time[:split_time]\nx_train_multi = serie_multi[:split_time]\ntime_test = time[split_time:]\nx_test_multi = serie_multi[split_time:]\n\nplt.figure(figsize=(30,15))\nplt.plot(time_train,x_train_multi, label = \"train\")\nplt.plot(time_test,x_test_multi, label = \"test\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    #series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[-1]))\n    return ds.batch(batch_size).prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 5\nbatch_size = 2\nshuffle_buffer_size = 1000\nwindow=windowed_dataset(serie_multi, window_size, batch_size, shuffle_buffer_size)\nprint(window)\nfor x, y in window:\n  print(\"x \" ,x.numpy(),\" y \", y.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 7\nbatch_size = 64\nn_features = 2\ntf.keras.backend.clear_session()\ndataset_multi = windowed_dataset(x_train_multi, window_size, batch_size, shuffle_buffer_size)\n#(batch_size,timestamp, n_features)\nmodel = tf.keras.models.Sequential([\n    #tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]),\n    #tf.keras.layers.LSTM(100, return_sequences=True),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(n_features), #n_features\n    tf.keras.layers.Lambda(lambda x:x*100)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset_multi,epochs=20)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = []\nresults = []\nfor time in range(len(time_test)):\n  #print(series[time:time + window_size][np.newaxis] , \" --> \",model.predict(series[time:time + window_size][np.newaxis]))\n    forecast.append(model.predict(serie_multi[split_time+time-window_size:split_time+time][np.newaxis]))\n\nresults = np.array(forecast)[:, 0, :]\nplt.plot(results, label =\"pronostico\")\nplt.plot(x_test_multi, label = \"serie\")\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 7\nbatch_size = 64\nshuffle_buffer_size = 1000\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(2),\n    tf.keras.layers.Lambda(lambda x:x*200)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset_multi,epochs=30)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = []\nresults = []\nfor time in range(len(time_test)):\n    forecast.append(model.predict(serie_multi[split_time+time-window_size:split_time+time][np.newaxis]))\n\nresults = np.array(forecast)[:, 0, :]\nplt.plot(results, label =\"pronostico\")\nplt.plot(x_test_multi, label = \"serie\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Para todos las zonas"},{"metadata":{},"cell_type":"markdown","source":"serie_multi_manhattan= data_manhattanx10min['PULocationID'][4].to_numpy()[np.newaxis].transpose()\n\nfor i in data_manhattanx10min['PULocationID'].columns:\n    if i != 4:\n        serie_i = data_manhattanx10min['PULocationID'][i].to_numpy()[np.newaxis].transpose()\n        serie_multi_manhattan = np.append(serie_multi_manhattan, serie_i,axis=1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"serie_multi_manhattan=[]\nserie_multi_manhattan= data_manhattanx10min['PULocationID'][4].to_numpy()[np.newaxis].transpose()\nzonas_manhattan = data_manhattanx10min['PULocationID'].columns\n\nfor i in zonas_manhattan[1:]:\n    serie_i = data_manhattanx10min['PULocationID'][i].to_numpy()[np.newaxis].transpose()\n    serie_multi_manhattan = np.append(serie_multi_manhattan, serie_i,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"serie_multi_manhattan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"serie_multi_manhattan = []\n#zonas_mas_transitadas = [237,236,161,162,186,48,170, 234,142,239]\nzonas_mas_transitadas = [237,236,161]\nserie_multi_manhattan = data_manhattanx10min['PULocationID'][237].to_numpy()[np.newaxis].transpose()\nfor i in zonas_mas_transitadas[1:]:\n    serie_i = data_manhattanx10min['PULocationID'][i].to_numpy()[np.newaxis].transpose()\n    serie_multi_manhattan = np.append(serie_multi_manhattan, serie_i,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"serie_multi_manhattan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_time = 3500\ntime = np.arange(len(series1), dtype=\"float32\")\ntime_train = time[:split_time]\nx_train_multi_manhattan = serie_multi_manhattan[:split_time]\ntime_test = time[split_time:]\nx_test_multi_manhattan = serie_multi_manhattan[split_time:]\n\nplt.figure(figsize=(30,15))\n#plt.plot(time_train,x_train_multi, label = \"train\")\nplt.plot(time_test,x_test_multi_manhattan, label = \"test\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 7\nbatch_size = 64\nn_features = len(zonas_manhattan)\n\ntf.keras.backend.clear_session()\ndataset_multi_manhattan = windowed_dataset(x_train_multi_manhattan, window_size, batch_size, shuffle_buffer_size)\n#(batch_size,timestamp, n_features)\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(n_features), #n_features\n    tf.keras.layers.Lambda(lambda x:x*200)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset_multi_manhattan,epochs=30)\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediccion del ultimo dia para todas las zonas"},{"metadata":{"trusted":true},"cell_type":"code","source":"tiempo_prediccion = len(time)-1\ntest = serie_multi_manhattan[tiempo_prediccion]\nprediccion = model.predict(serie_multi_manhattan[tiempo_prediccion-window_size:tiempo_prediccion-1][np.newaxis])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = []\nfor i in zonas_manhattan:\n    index.append(zone_dict[i])\ndf = pd.DataFrame({'speed': test,'lifespan': prediccion}, index=index)\nax = df.plot.bar()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}