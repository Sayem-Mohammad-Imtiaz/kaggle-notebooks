{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n%matplotlib inline\n\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nIMG_CHANNEL = 3\nNUM_CLASSES = 32\nBATCH_SIZE = 32\nDROPOUT = 0.3\n\nIMG_PATH = r\"../input/camvid/CamVid/train/\"\nMASK_PATH = r\"../input/camvid/CamVid/train_labels/\"\n\nstrategy = tf.distribute.get_strategy()\n\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LoadImage(name, path, seg_path):\n    img = Image.open(os.path.join(path, name))\n    \n    name_seg = re.sub('\\.', '_L.', name)\n    \n    img = np.array(Image.open(os.path.join(path, name)).convert('RGB').resize((IMG_HEIGHT, IMG_WIDTH), Image.ANTIALIAS))\n    mask = np.array(Image.open(os.path.join(seg_path, name_seg)).convert('RGB').resize((IMG_HEIGHT, IMG_WIDTH), Image.ANTIALIAS))\n\n    return img/255.0, mask/255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nfiles =os.listdir(IMG_PATH)[0:10]\ncolors = []\nfor file in files:\n    img, seg = LoadImage(file, IMG_PATH, MASK_PATH)\n    colors.append(seg.reshape(seg.shape[0]*seg.shape[1], 3))\ncolors = np.array(colors)\ncolors = colors.reshape((colors.shape[0]*colors.shape[1],3))\n\nkm = KMeans(32)\nkm.fit(colors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns \n\ndef LayersToRGBImage(img):\n    \n    colors = [(234,222,218), (153,136,136), (100,44,169),\n             (23,126,137), (8,96,95), (161,205,244),\n             (124,128,155), (61,59,60),(127,121,121),\n             (53,45,57), (255,105,120), (255,255,130),\n             (193,189,179), (95,91,107), (217,187,249), (170,159,177),\n             (230,228,206), (122,48,108), (3,181,170),\n             (78,82,131), (244,172,50),(177,145,255),\n             (82,21,78), (17,19,68), (218,224,242),\n             (84,19,136), (255,212,0), (96,0,71),(217,247,250),\n             (202,240,248), (3,113,113), (255,87,159),\n             (212,170,125), (0,0,0)]\n    \n    nimg = np.zeros((img.shape[0], img.shape[1], 3))\n    for i in range(img.shape[2]):\n        c = img[:,:,i]\n        col = colors[i]\n        \n        for j in range(3):\n            nimg[:,:,j]+=col[j]*c\n    nimg = nimg/255.0\n    return nimg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ColorsToClass(seg):\n    s = seg.reshape((seg.shape[0]*seg.shape[1],3))\n    s = km.predict(s)\n    s = s.reshape((seg.shape[0], seg.shape[1]))\n    \n    n = len(km.cluster_centers_)\n    \n    cls = np.zeros((seg.shape[0], seg.shape[1], n))\n    \n    for i in range(n):\n        m = np.copy(s)\n        m[m!=i] = 0\n        m[m!=0] = 1\n        \n        cls[:,:,i]=m\n        \n    return cls\n\nimg, seg = LoadImage(os.listdir(IMG_PATH)[1], IMG_PATH, MASK_PATH)\nseg2 = ColorsToClass(seg)\nseg2 = LayersToRGBImage(seg2)\ntotal = cv2.addWeighted(img, 0.6, seg2, 0.4, 0)\nplt.imshow(total[:,:,:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Generate(path=IMG_PATH, seg_path=MASK_PATH, batch_size=BATCH_SIZE):\n    \n    files = os.listdir(path)\n    while True:\n        imgs=[]\n        segs=[]\n        \n        for i in range(batch_size):\n            file = random.sample(files,1)[0]\n   \n            img, seg = LoadImage(file, path, seg_path)\n            \n            seg = ColorsToClass(seg)\n            \n            imgs.append(img)\n            segs.append(seg)\n        yield np.array(imgs), np.array(segs)\n        \ngen = Generate()\nimgs, segs = next(gen)\n\nplt.figure(figsize=(10, 10))\nplt.subplot(121)\nplt.imshow(imgs[0])\nplt.subplot(122)\nplt.imshow(LayersToRGBImage(segs[0]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def block(x, n_convs, filters, kernel_size, activation, pool_size, pool_stride, block_name):\n\n    for i in range(n_convs):\n        x = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding='same', name=\"{}_conv{}\".format(block_name, i + 1))(x)\n    \n    x = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_stride, name=\"{}_pool{}\".format(block_name, i+1))(x)\n\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# download the weights\n!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n# assign to a variable\nvgg_weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def VGG_16(image_input):\n\n    # create 5 blocks with increasing filters at each stage. \n    x = block(image_input, n_convs=2, filters=64, kernel_size=(3, 3), activation='relu', pool_size=(2, 2), pool_stride=(2, 2), block_name='block1')\n    p1 = x\n\n    x = block(x, n_convs=2, filters=128, kernel_size=(3, 3), activation='relu', pool_size=(2, 2), pool_stride=(2, 2), block_name='block2')\n    p2 = x \n\n    x = block(x, n_convs=3, filters=256, kernel_size=(3, 3), activation='relu', pool_size=(2, 2), pool_stride=(2, 2), block_name='block3')\n    p3 = x\n\n    x = block(x, n_convs=3, filters=512, kernel_size=(3, 3), activation='relu', pool_size=(2, 2), pool_stride=(2, 2), block_name='block4')\n    p4 = x\n\n    x = block(x, n_convs=3, filters=512, kernel_size=(3, 3), activation='relu', pool_size=(2, 2), pool_stride=(2, 2), block_name='block5')\n    p5 = x\n\n    # create the vgg model\n    vgg = tf.keras.Model(image_input, p5)\n\n    n = 4096\n\n    # our input images are 224x224 pixels so they will be downsampled to 7x7 after the pooling layers above.\n    # we can extract more features by chaining two more convolution layers.\n\n    c6 = tf.keras.layers.Conv2D(n, (7, 7), activation='relu', padding='same', name='conv6')(p5)\n    c7 = tf.keras.layers.Conv2D(n, (1, 1), activation='relu', padding='same', name='conv7')(c6)\n\n    return (p1, p2, p3, p4, c7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fcn8_decoder(convs, n_classes):\n\n    # unpack the output of the encoder\n    f1, f2, f3, f4, f5 = convs\n  \n    # upsample the output of the encoder then crop extra pixels that were introduced\n    o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5)\n    o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n\n    # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n    o2 = f4\n    o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same'))(o2)\n\n    # add the results of the upsampling and pool 4 prediction\n    o = tf.keras.layers.Add()([o, o2])\n\n    # upsample the resulting tensor of the operation you just did\n    o = (tf.keras.layers.Conv2DTranspose( n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False ))(o)\n    o = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n\n    # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n    o2 = f3\n    o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same'))(o2)\n\n    # add the results of the upsampling and pool 3 prediction\n    o = tf.keras.layers.Add()([o, o2])\n  \n    # upsample up to the size of the original image\n    o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False )(o)\n\n    # append a softmax to get the class probabilities\n    o = (tf.keras.layers.Activation('softmax'))(o)\n\n    return o","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def segmentation_model():\n  \n    inputs = tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL))\n    convs = VGG_16(image_input=inputs)\n    outputs = fcn8_decoder(convs, 32)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = segmentation_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = Generate()\nval_gen = Generate(path=\"../input/camvid/CamVid/val/\", seg_path=\"../input/camvid/CamVid/val_labels/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, SGD\n\nadam = Adam(lr=0.001, decay=1e-06)\nsgd = SGD(lr=1E-3, momentum=0.9, nesterov=True)\n\nfilepath = \"best-model-vgg.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=7, verbose=1,\n    mode='min', restore_best_weights=True\n)\n\nreduceLR = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, mode='max', min_lr=1e-15)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nclb = [reduceLR, checkpoint, early_stop]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = BATCH_SIZE\nnum_of_training_samples = len(os.listdir(IMG_PATH))\nnum_of_testing_samples = len(os.listdir(\"../input/camvid/CamVid/val/\"))\n\nhistory = model.fit_generator(train_gen, epochs=200, steps_per_epoch=num_of_training_samples//batch_size,\n                       validation_data=val_gen, validation_steps=num_of_testing_samples//batch_size, callbacks=[clb])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best-model-vgg.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history[\"val_loss\"]\nacc = history.history[\"val_accuracy\"] #accuracy\n\nplt.figure(figsize=(12, 6))\nplt.subplot(211)\nplt.title(\"Val. Loss\")\nplt.plot(loss)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(212)\nplt.title(\"Val. Accuracy\")\nplt.plot(acc)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\n\nplt.tight_layout()\nplt.savefig(\"learn.png\", dpi=150)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def give_color_to_seg_img(seg, n_classes=NUM_CLASSES):\n    \n    seg_img = np.zeros( (seg.shape[0],seg.shape[1],3) ).astype('float')\n    colors = sns.color_palette(\"hls\", n_classes)\n    \n    for c in range(n_classes):\n        segc = (seg == c)\n        seg_img[:,:,0] += (segc*( colors[c][0] ))\n        seg_img[:,:,1] += (segc*( colors[c][1] ))\n        seg_img[:,:,2] += (segc*( colors[c][2] ))\n\n    return(seg_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_gen = DataGenerator(valid_folder)\nmax_show = 1\nimgs, segs = next(val_gen)\npred = model.predict(imgs)\n\nfor i in range(max_show):\n    _p = give_color_to_seg_img(np.argmax(pred[i], axis=-1))\n    _s = give_color_to_seg_img(np.argmax(segs[i], axis=-1))\n\n    predimg = cv2.addWeighted(imgs[i]/255, 0.5, _p, 0.5, 0)\n    trueimg = cv2.addWeighted(imgs[i]/255, 0.5, _s, 0.5, 0)\n    \n    plt.figure(figsize=(12,6))\n    plt.subplot(121)\n    plt.title(\"Prediction\")\n    plt.imshow(predimg)\n    plt.axis(\"off\")\n    plt.subplot(122)\n    plt.title(\"Original\")\n    plt.imshow(trueimg)\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(\"pred_\"+str(i)+\".png\", dpi=150)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}