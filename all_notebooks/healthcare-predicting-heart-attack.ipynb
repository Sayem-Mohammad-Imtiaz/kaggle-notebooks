{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Healthcare: Predicting Heart Attack","metadata":{}},{"cell_type":"markdown","source":"## Description:\n### Cardiovascular disease is one of the top contributors to mortality in the world. Millions of dollars were spent on medical treatment included medications, surgical intervention, and so forth. Instead of spending humongous amount of money in treatment, preventive measures can significantly impact population health in positive way. People can benefit from early diagnosis before heart attack and gives them the opportunity to take preventive actions. ","metadata":{}},{"cell_type":"markdown","source":"## Project Objective:\n### To predict the outcome, if there is a high or less chance of having heart attack. The target feature or y-variable is \"output\", where 0= less chance of heart attack 1= more chance of heart attack.  ","metadata":{}},{"cell_type":"markdown","source":"## Process:\n### This interesting project will start of with basic descriptive analysis, followed by data visualization along with exploratory data analysis, preparing the data, splitting train and test data, and fit into the model. Finally, the model's performance will be evaluated by classification report and confusion matrix. ","metadata":{}},{"cell_type":"markdown","source":"## Potential Impact:\n### Healthier population, less money spent on healthcare, reduce mortality rate due to cardiovascular disease. ","metadata":{}},{"cell_type":"markdown","source":"# Dataset Description:\n\n\n## Age : Age of the patient\n\n## Sex : Sex of the patient\n\n## exang: exercise induced angina (1 = yes; 0 = no)\n\n## ca: number of major vessels (0-3)\n\n## cp : Chest Pain type chest pain type\n\n* Value 1: typical angina \n* Value 2: atypical angina \n* Value 3: non-anginal pain \n* Value 4: asymptomatic\n\n## trtbps : resting blood pressure (in mm Hg)\n\n## chol : cholestoral in mg/dl fetched via BMI sensor\n\n## fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n\n## rest_ecg : resting electrocardiographic results\n\n* Value 0: normal \n* Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n* Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria thalach : maximum heart rate achieved\n\n## target : 0= less chance of heart attack 1= more chance of heart attack","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking for duplicates","metadata":{}},{"cell_type":"code","source":"duplicate_rows = data[data.duplicated()]\nprint(\"Number of duplicate rows: \", duplicate_rows.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicate_rows = data[data.duplicated()]\nprint(\"Number of duplicate rows: \", duplicate_rows.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Noted that the data types are integers and float with no null values","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Basic descriptive analysis","metadata":{}},{"cell_type":"code","source":"data.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### The target feature \"output\" has sorta balanced dataset","metadata":{}},{"cell_type":"code","source":"data[\"output\"].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.countplot(x=data[\"output\"], color=\"red\", alpha=0.4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The min value for age is: \", data[\"age\"].min())\nprint(\"The mean value for age is: \", data[\"age\"].mean())\nprint(\"The max value for age is: \", data[\"age\"].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.distplot(x=data[\"age\"], bins=15, color=\"red\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(x=\"age\", y=\"chol\", data=data, hue=\"output\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Noted outliers in choleterol feature. Will just drop these extreme numbers. ","metadata":{}},{"cell_type":"code","source":"data[\"chol\"].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.distplot(x=data[\"chol\"], bins=10, color=\"green\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing outlier","metadata":{}},{"cell_type":"code","source":"data = data[data[\"chol\"] < 380]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"chol\"].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The gender proportion is imbalanced with male accounts for ~69% and female ~31%.","metadata":{}},{"cell_type":"code","source":"data[\"sex\"].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.set_context(\"paper\", font_scale=1.5)\nsns.countplot(x=\"sex\", palette=\"plasma\", data=data, hue=\"output\")\nplt.title(\"Gender\")\nplt.legend(title=\"Target Variable\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gender_gb = data.groupby(\"output\")[\"sex\"]\ngender_gb.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fun Fact:\n### Given the dataset, the probability of a picking a female patient with having higher chance of heart attack is 23%\n\n### Given the dataset, the probability of a picking a male patient with having higher chance of heart attack is 31%","metadata":{}},{"cell_type":"code","source":"prob_female_total = 69/len(data[\"sex\"])*100\nprob_female_total","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prob_male_total = 92/len(data[\"sex\"])*100\nprob_male_total","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Noted in general, males have higher count than females across all types of chest pain, as shown in the graph below. This is because of the gender proportion in the dataset. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.set_context(\"paper\", font_scale=1.5)\nsns.countplot(x=\"cp\", color=\"gold\", data=data, hue=\"sex\")\nplt.xlabel(\"Chest Pain: 1 typical angina, 2 atypical angina, 3 non-anginal pain, 4 asymptomatic\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(data, col=\"sex\", hue=\"output\")\ng.map(plt.scatter, \"age\", \"chol\").add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drilling down into number of males and females having higher chance of heart attack","metadata":{}},{"cell_type":"code","source":"prob_female = 69/(69+22)*100\nprob_male = 93/(114+93)*100\nprint(\"Given the female population in the dataset, the probability of a female having higher chance of heart attack is: {:.2f}%\".format(prob_female))\nprint(\"\\n\")\nprint(\"Given the male population in the dataset, the probability of a male having higher chance of heart attack is: {:.2f}%\".format(prob_male))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.set_context(\"paper\", font_scale=1.5)\nsns.countplot(x=\"cp\", palette=\"Reds_r\", data=data, hue=\"output\")\nplt.xlabel(\"Chest Pain: 0 typical angina, 1 atypical angina, 2 non-anginal pain, 3 asymptomatic\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.boxplot(x=\"cp\", y=\"trtbps\", data=data, hue=\"output\")\nplt.xlabel(\"Chest Pain: 0 typical angina, 1 atypical angina, 2 non-anginal pain, 3 asymptomatic\")\nplt.ylabel(\"Resting blood pressure (in mm Hg)\")\nsns.set_context(\"paper\", font_scale=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(data, col=\"sex\", hue=\"output\")\ng.map(plt.scatter, \"trtbps\", \"thalachh\").add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.distplot(x=data[\"trtbps\"], bins=10, color=\"blue\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[data[\"trtbps\"] < 180]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"trtbps\"].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.distplot(x=data[\"thalachh\"], bins=15, color=\"red\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[data[\"thalachh\"] > 100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"thalachh\"].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"fbs\"].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.countplot(x=data[\"fbs\"], color=\"red\", alpha=0.4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.boxplot(x=\"exng\", y=\"thalachh\", data=data, hue=\"output\", palette=\"seismic\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.boxplot(x=\"slp\", y=\"oldpeak\", data=data, hue=\"output\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.violinplot(x=\"slp\", y=\"oldpeak\", data=data, hue=\"output\", palette=\"plasma\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"caa\"].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"restecg\"].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"restecg\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ekg_normal = len(data[data[\"restecg\"] == 0])/len(data[\"restecg\"])*100\nekg_normal","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ekg1 = len(data[data[\"restecg\"] == 1])/len(data[\"restecg\"])*100\nekg2 = len(data[data[\"restecg\"] == 2])/len(data[\"restecg\"])*100\nekg_normal = len(data[data[\"restecg\"] == 0])/len(data[\"restecg\"])*100\n\nprint(\"Abnormal EKG: {:.2f}\".format(ekg1))\nprint(\"\\n\")\nprint(\"Hypertrophy by Estes: {:.2f}\".format(ekg2))\nprint(\"\\n\")\nprint(\"Normal EKG: {:.2f}\".format(ekg_normal))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.countplot(x=\"output\", data=data, hue=\"sex\", palette=\"seismic\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()[\"output\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The data correlation in the heatmap below shows us that cp, thalachh, and s/p have the highest correlation with output, our target variable. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,7))\nsns.heatmap(data.corr(method=\"pearson\"), cmap=\"PuRd\", annot=True, lw=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A quick glance using the graph below, age 50 with non-anginal chest type has the highest risk of having a heart attack.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.kdeplot(x=\"age\", y=\"cp\", data=data, hue=\"output\", fill=True, palette=\"Reds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.skew(axis=0, skipna=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The feature fbs has skewness >2. Also noted this feature has the lowest correlation, especially with target feature as shown in the heatmap. Will just drop this feature.\n\n### Features oldpeak and caa will be log-transformed hopefully it can improve the data skewness.","metadata":{}},{"cell_type":"code","source":"data[\"caa\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"caa\"] = data[\"caa\"].replace([2,3,4],1)\ndata[\"caa\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"oldpeak\"].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"oldpeak\"] = data[\"oldpeak\"].replace(0.0, 0.01)\ndata[\"oldpeak\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"log_oldpeak\"] = np.log10(data[\"oldpeak\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.skew(axis=0, skipna=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(\"oldpeak\", axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()[\"output\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop([\"trtbps\", \"chol\", \"restecg\", \"fbs\", \"age\", \"sex\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()[\"output\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating X and y variables for training the model","metadata":{}},{"cell_type":"code","source":"X = data.drop(\"output\", axis=1)\ny = data[\"output\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictive Model: Decision Trees","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_model = DecisionTreeClassifier(max_depth=15, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_predict = tree_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The precision score for 0 and 1 is ~0.80s, which I think is acceptable, but on the low end. ","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, tree_predict))\nprint(\"Confusion Report:\")\nprint(confusion_matrix(y_test, tree_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictive Model: Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_model = RandomForestClassifier(n_estimators=80, bootstrap=True, random_state=42, criterion=\"entropy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_predict = random_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Random Forest yields a slightly better precision score for 0 and 1 at ~0.82s, which I think is acceptable, but on the low end. Too bad my favorite model is not doing so well in this dataset. ","metadata":{}},{"cell_type":"code","source":"print(confusion_matrix(y_test, random_predict))\nprint(\"Confusion Report:\")\nprint(classification_report(y_test, random_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictive Model: Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_model = LogisticRegression(solver=\"liblinear\", random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_predict = log_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The logistic regression model, by far, performs the best with precision score 0.95 on 0, and recall at 0.97 on 1. The f1-score 0.88 for 1 is very good. ","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, log_predict))\nprint(\"Confusion Report:\")\nprint(confusion_matrix(y_test, log_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gaussian_model = GaussianNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gaussian_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gaussian_predict = gaussian_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I had high hope on GaussianNB model but looks like this model yields ~0.84. The recall at 0.90 on 1 is remarkable. ","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, gaussian_predict))\nprint(\"Confusion Report:\")\nprint(confusion_matrix(y_test, gaussian_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = [[\"GaussianNB\", 0.84, 0.84, 0.84, 0.84], [\"Random Forest\", 0.82, 0.83, 0.82, 0.82], \n          [\"DecisionTreeClassifier\", 0.81, 0.81, 0.81, 0.81],\n          [\"LogisticRegression\", 0.88, 0.87, 0.86, 0.86]]\noverall_result = pd.DataFrame(report, columns=[\"Model\", \"Accuracy Score\", \"Precision\", \"Recall\", \"F1-score\"])\noverall_result.sort_values(\"Accuracy Score\", ascending=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overall, Logistic Regression yields the highest score across the board on this dataset. ","metadata":{}}]}