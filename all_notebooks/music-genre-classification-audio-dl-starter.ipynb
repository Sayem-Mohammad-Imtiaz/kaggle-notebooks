{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to my first audio classification Notebook.\nIn this notebook we will classify the genre of music using the GTZAN dataset. GTZAN dataset is a very famous dataset in audio industry. In this dataset we have 10 categories of genres from blues, classical, pop etc. The dataset consist of .wav files and each of the category consist of 100 wav files. \n# Let's Start by loading the dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        print(filename)\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing some comman ML libraries and Some of the python based Audio libraries.\nHere soundfile, Librosa and librosa.display are the python based Audio libraries. \nlibrosa and soundfile both can be used to read the .wav files and librosa.display would help us visualize the .wav file in the form of waveform and IPython.display.Audio would help us listen the audio file","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nimport librosa\nimport librosa.display\nfrom IPython.display import Audio\nfrom tqdm import tqdm \nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, LSTM, Bidirectional, GRU, BatchNormalization, LeakyReLU\nfrom keras.utils import to_categorical\nimport os\nimport math\nimport json\nimport random","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's hear one of the Blues file\nAudio('../input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.00000.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's visualize the same Blues audio file in Wave form\n# For that we must load the audio file \n\n**Loading an audio file returns 2 parameters first is signal(which is a numpy array) and second is the sr(sampling rate)\nSampling rate is the rate at which the wave form is sampled i.e. if we set the sampling rate to be 22050 Hz, Then 22050 points are sampled from the wave file in 1 second. Since here our wave file is 30 seconds long then setting sr=22050 will yeild the signal array of length 22050*30 .**","metadata":{}},{"cell_type":"code","source":"signal, sr = librosa.load('../input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.00000.wav', sr=22050)\nprint('Length of Signal is => ', len(signal))\nprint('Sampling Rate => ', sr)\nprint('Duration of the audio file => ', len(signal)/sr)\n# Then passing the loaded file as the parameter of the below function\nlibrosa.display.waveplot(signal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above waveform is a Amplitude v/s Time plot.\n\nX-axis depicts Amplitude and Y-axis depicts the Time (This is called the wave in Amplitude domain) ","metadata":{}},{"cell_type":"markdown","source":"# Next, we will define some of the terms as variables which we can use further.\n**we will take sample rate as 22050 Hz which is considered as a standard sampling rate.\nThen we would need to fix the duration in order to get the same length of array as signal.\nAlso, Since we are low on training data, we will divide each of the 30 second wav file to smaller wav files to get more training data these smaller wav files will be called num_segments which will be one of the parameter of our function**","metadata":{}},{"cell_type":"code","source":"DATASET_PATH = '/kaggle/input'\nJSON_PATH = './myjson.json'\nSAMPLE_RATE = sr =  22050\nDURATION = 30 #measured in seconds \nSAMPLES_PER_TRACK = SAMPLE_RATE*DURATION","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting MFCC's\nNext up, We will define a high level function which will basically extract us MFCC(Mel Frequency Spectral Coefficients). These MFCC's will act as our input features which we will pass through our model","metadata":{}},{"cell_type":"code","source":"def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=4084, hop_length=1024, num_segments=10):\n    #dictionary to store data\n    data = {\n        'mapping' : [],\n        'mfcc' : [],\n        'labels' : []\n    }\n    \n    count = 0 # To keep track of our progress\n    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments) \n    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment / hop_length)\n    \n    #Loop through all the genres\n    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n        \n        #ensure that we're not at the root level\n        if dirpath not in dataset_path:\n\n            #save the semantic label\n            dirpath_components = dirpath.split('/')\n            semantic_label = dirpath_components[-1]\n            data['mapping'].append(semantic_label)\n            print('\\nProcessing {}'.format(semantic_label))\n            \n            #process files for a specific genre \n            for f in filenames:\n                if f.endswith('.wav') and f != 'jazz.00054.wav': # Since file jazz.00054.wav is an empty file\n                    \n                    file_path = os.path.join(dirpath,f)\n                    \n                    #loading the audio file \n                    # we are using the soundfile library since it is faster than librosa\n                    signal, sr = sf.read(file_path) # len(signal) = 661794  # sr is 22050 by default \n                    #print(signal,sr)\n                    #process segments extracting mfcc and storing data\n                    for s in range(num_segments): \n                        # Since num_segments is defined as 5. Every 30 sec file is divided into 5 segments of length 6sec \n                        # Start sample would keep track of the index of the first element of each 6 second batch\n                        # finish sample would keep track of the index of the last element of each 6 second batch\n                        # And then with the help of python's slice functionality we will extract that 6 second batch from every 30 sec signal\n                        start_sample = num_samples_per_segment * s   \n                        finish_sample = num_samples_per_segment + start_sample\n                        \n                        # Next, we will pass each segment in order to extract MFCC. The parameter n_mfcc defines the number of mfcc \n                        # we need to extract, Usually n_mfcc is set b/w 13 to 40. The other parameters n_fft and hop length are \n                        # indivisual topics of discussion. Will be discussed in later Notbooks. \n                        mfcc = librosa.feature.mfcc(signal[start_sample : finish_sample],\n                                                   sr = sr,\n                                                   n_fft = n_fft,\n                                                   n_mfcc = n_mfcc,\n                                                   hop_length = hop_length)\n\n                        mfcc = mfcc.T\n                        # store mfcc for segment if it has the expected length\n                        if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n                            print(mfcc.shape)\n                            data['mfcc'].append(mfcc.tolist())\n                            data['labels'].append(i)\n                            print('Processing {}, segment:{}'.format(file_path, s))\n                            count += 1\n                            print(count)\n    with open(json_path, 'w') as fp:\n        json.dump(data, fp, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's run the above function \nsave_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the saved Json file\ndef load_data(path):\n    with open(path, 'r') as fp:\n        data = json.load(fp)\n        \n    #Convert lists into numpy arrays\n    inputs = data['mfcc']\n    targets = data['labels'] \n    return np.array(inputs), np.array(targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs, targets = load_data('./myjson.json')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(inputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, is the shape of the inputs obtained.\n\n9986 is for no. of segments of files.\n\n(65,13) are the dimensions of mfcc generated where 13 comes from the n_mfcc parameter\n","metadata":{}},{"cell_type":"code","source":"inputs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(targets, return_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting labels from 15-24 to 0-9\nv = min(np.unique(targets))\nfor i in range(len(targets)):\n    if targets[i] == v:\n        targets[i] = 0\n    else:\n        new = targets[i] - v\n        targets[i] = new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(targets, return_counts=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If you want to apply Convolution NN the remove the comment from the below line\n#inputs = np.reshape(inputs, (inputs.shape[0], inputs.shape[1], inputs.shape[2], 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs, targets, test_size=0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding Noise \nfor i in range(inputs_train.shape[0]):\n    s = np.random.rand(inputs_train.shape[1], inputs_train.shape[2])\n    inputs_train[i] = inputs_train[i] + s\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(GRU(100, return_sequences=True, input_shape=(inputs.shape[1], inputs.shape[2])))\nmodel.add(GRU(500, return_sequences=True))\nmodel.add(GRU(1000))\nmodel.add(LeakyReLU())\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(100))\nmodel.add(LeakyReLU())\nmodel.add(Dense(10, 'softmax'))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n             loss = 'sparse_categorical_crossentropy',\n             metrics=['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(inputs_train, targets_train,\n          validation_data=(inputs_test, targets_test),\n          epochs = 50,\n          batch_size=100)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}