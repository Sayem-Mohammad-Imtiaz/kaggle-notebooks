{"cells":[{"metadata":{},"cell_type":"markdown","source":"**SVM-RANDOM FOREST-DECISION TREE**"},{"metadata":{},"cell_type":"markdown","source":"****\n> \n*In this notebook, I will use svm, random forest and decision tree classification algorithms to classify diabetic and non-diabetic patients using the sklearn library that I made in the initial project in the artificial intelligence course.*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We first read the data set in order to operate on it.****"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We bring the first 5 data to view the properties in the data set.****"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.isnull(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array=data.values\narray\nX=array[:,0:8]\nY=array[:,8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We divide our data into training and test data. We allocate 70% of our series for training data and 30% of our series for test data.Random State = 0 checks the shuffle applied to the data before applying the division and starts shuffling from 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test =train_test_split(X,Y,train_size=0.7,test_size=0.3,\n                                                   random_state=0,stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SUPPORT VECTOR MACHINE CLASSIFIER**"},{"metadata":{},"cell_type":"markdown","source":"> Defining SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#support vector machine algorithm\nfrom sklearn import svm\ndesvek=svm.SVC()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#training model\ndesvek.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> classification of the test result on the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=desvek.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Accuracy classification score calculation and printing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy=accuracy_score(Y_test,y_pred)\nprint(\"svm accuracy\", accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> computing the confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Plotting the confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix as plt_conf\nplt_conf(desvek, X_test,Y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Calculation of the K-Fold cross validation score.We divide the data set into subsets equal to the K value. Of these subsets, k-1 training set 1 is used as the test set. Each time the test data and training data are updated with the unused ones. An accuracy value is created at the end of each operation. At the end of the K process, the average of the total accuracy values gives us the k-Fold cross validation value."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(desvek, X, Y, cv=10)\nprint( score)\nprint(\"SVM KFold cross validation\",score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **RANDOM FOREST CLASSIFICATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train\nrf.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> classification of test result on model and calculation of accuracies"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred=rf.predict(X_test)\nrf_accuracy=accuracy_score(Y_test,rf_pred)\nprint(\"random forest accuracy\", rf_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the confusion matrix\nprint(confusion_matrix(Y_test,rf_pred))\n#plotting the confusion matrix \nplt_conf(rf, X_test, Y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculation the K-Fold cross validation score\nrf_score=cross_val_score(rf,X,Y,cv=10)\nprint(rf_score)\nprint(\"RF KFold cross validation\",rf_score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE CLASSIFIER****"},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train\ndt.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classification of the test result on the model\ndt_pred=dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating accuracy\ndt_accuracy=accuracy_score(Y_test,dt_pred)\nprint(\"decision tree accuracy\", dt_accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing and plotting confusion matrix\nprint(confusion_matrix(Y_test,dt_pred))\nplt_conf(dt, X_test, Y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating Decision Tree K-Fold Cross Validation\ndt_score=cross_val_score(dt,X,Y,cv=10)\nprint(dt_score)\nprint(\"Decision Tree KFold cross validation\",dt_score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Since I used the existing data set instead of cleaning data on the data, the rates turned out like this. The success rates of classifiers can be increased by working on the data."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}