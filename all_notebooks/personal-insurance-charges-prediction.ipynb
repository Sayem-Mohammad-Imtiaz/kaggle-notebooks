{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Personal Insuarance Prediction\nInsurance premium and assured ammount largely depends on lifestyle and existing health condion of an individual. The data contains following information about people and their related insurance charges.\n\nColumns\n\nage: age of primary beneficiary\n\nsex: insurance contractor gender, female, male\n\nbmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\nobjective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n\nchildren: Number of children covered by health insurance / Number of dependents\n\nsmoker: Smoking\n\nregion: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n\ncharges: Individual medical costs billed by health insurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/insurance/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Understanding Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data dimensions\nr,c=df.shape\nprint(f\"The data has {r} rows and {c} columns.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking data types\ncat_cols=df.select_dtypes(exclude=np.number).columns\nnum_cols=df.select_dtypes(include=np.number).columns\nprint(f\"There are {len(cat_cols)} categorical columns in data.\\nThey are:\\n {cat_cols}\\n\")\nprint(f\"There are {len(num_cols)} numerical columns in data.\\nThey are:\\n {num_cols}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Descriptive Statistics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n1. Average age of individuals is about 39 years with a standard deviation of 14 years. Age range being considered is 18-64 years.\n2. Average BMI is nearly 30. \n3. Most individuals have one child.\n4. Distribution of data for Age, BMI & Child is near normal. For charges it is right skewed.\n5. Upto 50% individuals are charged around 9382.9033"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(exclude=np.number)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations: \n\n1. The data is balanced with respect to gender. Number of males is slightly more. \n2. Most individuals are non smokers.\n3. Out of the 4 unique regions covered in the data most belong to the SouthEast region. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=pd.get_dummies(df,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations: \n\n1. Charges Vs Age: We can observe that individuals can be linearly separated based on age and charges. \n2. There seems to be some linearity in relationship of charges with BMI"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.heatmap(df2.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations: \n\n1. There is some correlation between charges and age.\n2. Smoker status yes has high correlation with Insurance charges."},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling Data\nX=df2.drop('charges',axis=1)\ny=df2['charges']\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\ninp_sc=sc.fit_transform(X)\ninp_sc=pd.DataFrame(inp_sc,columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp_sc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building"},{"metadata":{},"cell_type":"markdown","source":"#### Linear Regression: OLS based model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nc=sm.add_constant(inp_sc)\nols=sm.OLS(y,X)\nmod=ols.fit()\nmod.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Checking Assumptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Multicollinearity: \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif=pd.DataFrame()\nvif['VIF']=[variance_inflation_factor(inp_sc.values,i) for i in range(inp_sc.shape[1])]\nvif['Feature']=inp_sc.columns\nvif.sort_values('VIF',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n    No problem of multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linearity:\nfor i in inp_sc.columns:\n    sns.scatterplot(inp_sc[i],y)\n    plt.xlabel(f\"{i}\")\n    plt.xlabel(f\"Charges\")\n    plt.title(f\"{i} Vs Charges\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n1. Linear relationship between age and charges\n2. Some linearity for BMI vs Charges. \n3. Observable difference in charges for smokers and non smokers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normality: \nsns.distplot(mod.resid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \nNear normal distribution of residues."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Autocorrelation\nplt.figure(figsize=(10,5))\nsns.heatmap(df2.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n\nNo/low observable autocorrelation among input features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Homoscadasticity: ypred vs error\nsns.residplot(mod.predict(),mod.resid)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.api import het_goldfeldquandt\nhet_goldfeldquandt(mod.resid,mod.model.exog)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n\nSince the p values > 0.05,  We accept the null hypothesis \nie. The variance among the residues and predicted values are same.\nTherefore the model satisfies the condition of homoscadasticity."},{"metadata":{},"cell_type":"markdown","source":"#### Linear Regression: Sklearn based model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(inp_sc, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(X_train,y_train)\ny_pred_train=lr.predict(X_train)\ny_pred_test=lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\nprint(\"Training stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_train,y_pred_train))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_train,y_pred_train))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_train,y_pred_train))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_train,y_pred_train)))\nprint(\"\\n\\nTesting stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_test,y_pred_test))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_test,y_pred_test))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_test,y_pred_test))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_test,y_pred_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note: Ignore R2 Scores for non-linear models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on relationship between age and charges, trying KNN Regressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nknn=KNeighborsRegressor(n_neighbors=100,weights='distance')\n\nknn.fit(X_train,y_train)\ny_pred_train=knn.predict(X_train)\ny_pred_test=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\nprint(\"Training stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_train,y_pred_train))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_train,y_pred_train))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_train,y_pred_train))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_train,y_pred_train)))\nprint(\"\\n\\nTesting stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_test,y_pred_test))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_test,y_pred_test))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_test,y_pred_test))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_test,y_pred_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying non linear ensemble model to see if there is any improvement in performance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfc=RandomForestRegressor()\nrfc.fit(X_train,y_train)\ny_pred_train=rfc.predict(X_train)\ny_pred_test=rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\nprint(\"Training stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_train,y_pred_train))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_train,y_pred_train))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_train,y_pred_train))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_train,y_pred_train)))\nprint(\"\\n\\nTesting stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_test,y_pred_test))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_test,y_pred_test))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_test,y_pred_test))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_test,y_pred_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n\nWhile the prediction accuracy of the model has increased the resultant model is overfitting in nature. \nNext we will try to improve for this model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nrfc=RandomForestRegressor()\ngrid={'criterion':['mse', 'mae']}\ngc=GridSearchCV(rfc,param_grid=grid, cv=10, scoring='neg_mean_squared_error')\ngc.fit(inp_sc,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=gc.predict(X_train)\ny_pred_test=gc.predict(X_test)\nfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\nprint(\"Training stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_train,y_pred_train))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_train,y_pred_train))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_train,y_pred_train))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_train,y_pred_train)))\nprint(\"\\n\\nTesting stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_test,y_pred_test))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_test,y_pred_test))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_test,y_pred_test))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_test,y_pred_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nab=AdaBoostRegressor()\ngrid={'learning_rate':[0.001,0.01,0.1,1, 2,5,10,30], 'random_state': [20]}\ngc=GridSearchCV(ab,param_grid=grid,cv=10,scoring='neg_mean_squared_error')\ngc.fit(inp_sc,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=gc.predict(X_train)\ny_pred_test=gc.predict(X_test)\nfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\nprint(\"Training stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_train,y_pred_train))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_train,y_pred_train))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_train,y_pred_train))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_train,y_pred_train)))\nprint(\"\\n\\nTesting stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_test,y_pred_test))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_test,y_pred_test))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_test,y_pred_test))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_test,y_pred_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\ngb=GradientBoostingRegressor()\ngrid={'learning_rate':[0.001,0.01,0.1,1, 2,5,10,30], 'random_state': [20]}\ngc=GridSearchCV(gb,param_grid=grid,cv=10,scoring='neg_mean_squared_error')\ngc.fit(inp_sc,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=gc.predict(X_train)\ny_pred_test=gc.predict(X_test)\nfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\nprint(\"Training stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_train,y_pred_train))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_train,y_pred_train))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_train,y_pred_train))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_train,y_pred_train)))\nprint(\"\\n\\nTesting stage characteristics\\n\")\nprint(\"Accuracy of model: \", r2_score(y_test,y_pred_test))\nprint(\"Mean Absolute Error of model: \", mean_absolute_error(y_test,y_pred_test))\nprint(\"Mean Squared Error of model: \", mean_squared_error(y_test,y_pred_test))\nprint(\"Root Mean Squared Error of model: \", np.sqrt(mean_squared_error(y_test,y_pred_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Trying all models at once\nmodels=[]\nmodels.append(('Linear Regression',lr))\nmodels.append(('KNN',knn))\nmodels.append(('RandomForest',rfc))\nmodels.append(('AdaBoost',ab))\nmodels.append(('GradientBoost',gb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name=[]\nscores=[]\nfor name,model in models: \n    kfold=KFold(n_splits=10, shuffle=True, random_state=20)\n    score=cross_val_score(model,inp_sc,y, cv=kfold, scoring='neg_mean_squared_error')\n    model_name.append(name)\n    scores.append(score)\n    print(f\"{name}, bias: {np.mean(1-score)}, Variance error: {np.var(score,ddof=1)}\")\n         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(scores)\nax.set_xticklabels(model_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation: \n\n1. Non-linear models perform better at predicting the insurance charges. \n2. Gradient boosting Algorithm give the lowest bias error at expense of slightly more variance error as compared with other ensemble models. \n3. Random Forest Regressor give the best balance between the bias and variance error. So we will proceed with these algorithms to check if the combinationn of ensemble model gives a better result. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nva=VotingRegressor(estimators=[('RandomForest',rfc),('AdaBoost',ab),('GradientBoost',gb)])\nmodels=[]\nmodels.append(('Linear Regression',lr))\nmodels.append(('KNN',knn))\nmodels.append(('RandomForest',rfc))\nmodels.append(('AdaBoost',ab))\nmodels.append(('GradientBoost',gb))\nmodels.append(('VotingAlgo',va))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name=[]\nscores=[]\nfor name,model in models: \n    kfold=KFold(n_splits=10, shuffle=True, random_state=20)\n    score=cross_val_score(model,inp_sc,y, cv=kfold, scoring='neg_mean_squared_error')\n    model_name.append(name)\n    scores.append(score)\n    print(f\"{name}, bias: {np.mean(1-score)}, Variance error: {np.var(score,ddof=1)}\")\n         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(scores)\nax.set_xticklabels(model_name)\nplt.xticks(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n\nAs expected, the Voting Regressor provides a model with best features from the combination of ensemble model. This model gives comparable performanhce with better bias and variance error balance. We can proceed with this model for deployment."},{"metadata":{},"cell_type":"markdown","source":"### Future scope: \nCan create an app for such prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}