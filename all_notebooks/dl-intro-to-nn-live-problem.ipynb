{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Project - Bank Churn Prediction Case Study\n\n \n### In this project, we aim to predict the churn for a bank, i.e, given a Bank customer, can we build a classifier which can determine whether they will leave or not using Neural networks?\n\n### Objective :\nGiven a Bank customer, build a neural network based classifier that can determine whether they will leave or not in the next 6 months. \n### Context :\nBusinesses like banks which provide service have to worry about problem of 'Churn' i.e. customers leaving and joining another service provider. It is important to understand which aspects of the service influence a customer's decision in this regard. Management can concentrate efforts on improvement of service, keeping in mind these priorities.\n\n### Data Description :\n\nThe case study is from an open-source dataset from Kaggle.The dataset contains 10,000 sample points with 14 distinct features such as CustomerId, CreditScore, Geography, Gender, Age, Tenure, Balance etc.<br>\n\nLink to the Kaggle project site:https://www.kaggle.com/barelydedicated/bank-customer-churn-modelingPoints \n\n### Marks Distribution:\nThe points distribution for this case is as follows:\n1. Read the dataset\n2. Drop the columns which are unique for all users like IDs \n3. Distinguish the feature and target set \n4. Divide the data set into trainingand test sets \n5. Normalize the train and test data\n6. Initialize & build the model. Identify the points of improvement and implement the same the same.\n7. Predict the results using 0.5 as a threshold \n8. Print the Accuracy score and confusion matrix ","metadata":{"id":"DRX9f5_6HUNP"}},{"cell_type":"markdown","source":"## Reference Link - \n\n* Getting Started with Neural Networks - https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks\n* Introduction to PyTorch for Deep Learning - https://courses.analyticsvidhya.com/courses/introduction-to-pytorch-for-deeplearning\n* A Comprehensive Learning Path for Deep Learning in 2020 - https://courses.analyticsvidhya.com/courses/Comprehensive-learning-path-for-deep-learning-in-2020\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"MBVYdw85sEmE","outputId":"80858173-cf0f-4e64-ae36-d4defc32c274","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"id":"j0mRT6iJrkgJ","outputId":"0eb85890-330c-4872-a192-72c416003b03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras import optimizers","metadata":{"id":"IfeZclzIHUNs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Read the dataset","metadata":{"id":"z7ubXtC8HUOA"}},{"cell_type":"code","source":"ds = pd.read_csv(\"/kaggle/input/dl-intro-to-nn/bank.csv\")","metadata":{"id":"1QJLp3P3HUOC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds.head(2)","metadata":{"id":"nCfASHJ8HUOS","outputId":"6f1e10ff-65fc-46dd-cbc9-af007044b636","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds['CustomerId'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop the columns which are unique for all users like IDs","metadata":{"id":"BsBwLHcmHUOg"}},{"cell_type":"code","source":"#RowNumber #CustomerId and #Surname are unique hence dropping it\nds = ds.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\nds","metadata":{"id":"ivF2RMo6HUOr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds['Geography'].value_counts()","metadata":{"id":"2phjlpsmHUOh","outputId":"bbb08363-6bba-4292-82d1-8f425e8b39a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds['Exited'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds['Exited'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(ds['Geography'],ds['Exited'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"?pd.crosstab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(ds['Geography'],ds['Exited'],normalize='index')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds.info()","metadata":{"id":"owLb0T9WHUO0","outputId":"b6d0ba03-a2e3-4344-f6e6-1522db4f42f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=ds['Exited']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds['Exited'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Distinguish the feature and target set","metadata":{"id":"CUXPaUwZHUO8"}},{"cell_type":"code","source":"ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_new=pd.get_dummies(ds,drop_first=True)\nprint(ds_new.shape)\nds_new.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = ds.iloc[:,0:10].values # Credit Score through Estimated Salary\n# y = ds.iloc[:,10].values # Exited\n\nX=ds_new.drop('Exited',axis=1).values\ny=ds_new['Exited'].values","metadata":{"id":"VTb3JwlaHUO-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Encoding categorical (string based) data. Country: there are 3 options: France, Spain and Germany\n# # This will convert those strings into scalar values for analysis\n# print(X[:8,1], '... will now become: ')\n\n# label_X_country_encoder = LabelEncoder()\n# X[:,1] = label_X_country_encoder.fit_transform(X[:,1])\n# print(X[:8,1])","metadata":{"id":"ikg-QAywHUPE","outputId":"e85fd0b5-d4a5-4275-e0df-6fac8e1b6e37","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # We will do the same thing for gender. this will be binary in this dataset\n# print(X[:6,2], '... will now become: ')\n\n# label_X_gender_encoder = LabelEncoder()\n# X[:,2] = label_X_gender_encoder.fit_transform(X[:,2])\n# print(X[:6,2])","metadata":{"id":"wWrBLaE_HUPJ","outputId":"ff012733-31bd-4b89-fdba-2d5f68e7f57b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # The Problem here is that we are treating the countries as one variable with ordinal values (0 < 1 < 2). \n# # Therefore, one way to get rid of that problem is to split the countries into respective dimensions.\n# # Gender does not need this as it is binary\n\n# # Converting the string features into their own dimensions. Gender doesn't matter here because its binary\n# #countryhotencoder = OneHotEncoder(categories = [1]) # 1 is the country column\n# countryhotencoder = ColumnTransformer([(\"countries\", OneHotEncoder(), [1])], remainder=\"passthrough\")\n# X = countryhotencoder.fit_transform(X)\n# #X = countryhotencoder.fit_transform(X).toarray()","metadata":{"id":"9vo-BBy5HUPP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"id":"RwYnvT59HUPT","outputId":"f544f732-3fe4-4076-805f-9dd53bfde79b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"id":"KDvpBBbDHUPX","outputId":"c46d3c67-93a5-461f-81ea-8ee09ccbf2fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A 0 on two countries means that the country has to be the one variable which wasn't included \n# This will save us from the problem of using too many dimensions\n# X = X[:,1:] # Got rid of Spain as a dimension.","metadata":{"id":"td8t2NpPHUPe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Divide the data set into Train and test sets","metadata":{"id":"wyFlF8iBHUPk"}},{"cell_type":"code","source":"# Splitting the dataset into the Training and Testing set.\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)","metadata":{"id":"0YvyE1kXHUPl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalize the train and test data","metadata":{"id":"qlSyq5fNHUPp"}},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"id":"SEcfWvfrHUPr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center> Initialize & Build the Neural N/W Model","metadata":{"id":"m-_JBupNHUPw"}},{"cell_type":"code","source":"# 1. Should perform well on both Train and Test i.e. it shouldn't overfit\n# 2. build a confusion matrix for both train and test - compute recall and precision\n# 3. Use Adam optimizer * lr of 0.005","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing the ANN\nclassifier = Sequential()\n\nclassifier.add(Dense(32, activation = 'relu'))\nclassifier.add(Dense(16, activation = 'relu'))\nclassifier.add(Dense(8, activation = 'relu'))\nclassifier.add(Dense(1,  activation = 'sigmoid'))","metadata":{"id":"aexcSjTuHUPx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd = optimizers.Adam(lr = 0.005)\n\nclassifier.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics=['accuracy'])\n\nclassifier.fit(X_train, y_train, batch_size =500, epochs = 12, verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy Model1: '+ str(classifier.evaluate(X_train,y_train)[1]))\n\nY_pred_cls_train = classifier.predict_classes(X_train, batch_size=200, verbose=0)\nprint('Recall_score: ' + str(recall_score(y_train,Y_pred_cls_train)))\nprint('Precision_score: ' + str(precision_score(y_train, Y_pred_cls_train)))\nprint('F-score: ' + str(f1_score(y_train,Y_pred_cls_train)))\nconfusion_matrix(y_train, Y_pred_cls_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy Model1: '+ str(classifier.evaluate(X_test,y_test)[1]))\nY_pred_cls = classifier.predict_classes(X_test, batch_size=200, verbose=0)\nprint('Recall_score: ' + str(recall_score(y_test,Y_pred_cls)))\nprint('Precision_score: ' + str(precision_score(y_test, Y_pred_cls)))\nprint('F-score: ' + str(f1_score(y_test,Y_pred_cls)))\nconfusion_matrix(y_test, Y_pred_cls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}