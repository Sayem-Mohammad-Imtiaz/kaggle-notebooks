{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import altair as alt\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport requests\nimport seaborn as sns\nfrom sklearn.utils.random import sample_without_replacement\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\n\nalt.data_transformers.disable_max_rows()\nalt.renderers.enable('default')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# small parameter overview"},{"metadata":{},"cell_type":"markdown","source":"\n\n**accousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n\n\n**energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n**valence**: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n\n**instrumentalness**: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n\n\n**key**: The key the track is in. Integers map to pitches using standard Pitch Class notation.\n\n\n**liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n\n\n**loudness**: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n\n\n**mode**: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n\n\n**speechiness**: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n\n[source](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-analysis/)"},{"metadata":{},"cell_type":"markdown","source":"**task**: predict popularity of a song (regression) based solely on other features/characterics of the music track\n\n- impact of the artist's name on the popularity seems quiet obvious but also quiet a complex task (high cardinality data involving of processing to do), so I ll drop this parameter\n    - also see quiet fun to primarily focus parameters charcterizing the music track itself\n- **production year** and **release date** are in most case very near by another (at least at the year resolution), so I ll drop the release date"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/spotify-dataset-19212020-160k-tracks/data.csv')\ndf.drop(['release_date', 'name', 'artists'], axis=1, inplace=True)\ndf, test_df = train_test_split(df, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([df.nunique().rename('nunique'), df.isnull().sum().rename('nnull')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (spotify specific) music track characteristics"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(14,8))\nsns.violinplot(\n    data=df.select_dtypes([int,float])\n            .drop(\n                [\n                    'year', 'duration_ms', \n                    'loudness', 'popularity', \n                    'tempo', 'key', 'mode', 'explicit'\n                ],\n                axis=1\n            ),\n    inner='quartile',\n    ax=ax,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(14,8))\nsns.boxplot(\n    data=df.select_dtypes([int,float])\n            .drop(\n                [\n                    'year', 'duration_ms', \n                    'loudness', 'popularity', \n                    'tempo', 'key', 'mode', 'explicit'\n                ],\n                axis=1\n            ),\n    ax=ax,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the characteric \"features\" of track provided by spotify with (0,1)-range exhibit very strange and mostly highly non-normal distributions.\n\n**instrumentation,speechness,liveness**\n- both show highly skewed distribution (heavy right tailed narrow \"gaussian\")\n- hint of superposition (small bumps in right corners)\n- instrumentation and speechness have a very similar shape\n\n**danceability**\n- very flat symmetric and centered distribution (0.5)\n\n**valence,energy**\n- almost uniform distribution with \n    \n**acousticness**\n- feels like binary entity with two buldges occuring on both ends of the distribution\n- otherwise very flat\n- median and mean have no meaning\n\nunfortunetly I do not have enough computer resource to perform a full bivariate investigation \n(even with random subsampling)"},{"metadata":{},"cell_type":"markdown","source":"# other predictors + target"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = 15000\nidx = sample_without_replacement(df.shape[0], sample_size)\n\nbase = alt.Chart(df.iloc[idx]).mark_bar()\nfig = alt.vconcat()\n\nval = {\n    'continuous predictor': ['tempo', 'loudness', 'duration_ms', 'year'],\n    'discrete predictor': ['key', 'mode', 'explicit'],\n    'target': ['popularity'],\n}\nfor p, cols in val.items():\n    row = alt.hconcat(title=p)\n    for c in cols:\n        row |= base.encode(\n                alt.X(c, bin=alt.Bin(maxbins=50)),\n                alt.Y('count():Q'),\n                tooltip=['count():Q'],\n            ).properties(\n                title=c,\n                width=200,\n                height=500\n        )\n    fig &= row\nfig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **popularity** the target has a very strange anomaly,i.e.\nvery large count in there lowest value 0\n\n- maybe this value encode a different type of information? needs to be investigated -\n  could not find any information in the [spotify api doc](https://developer.spotify.com/documentation/web-api/reference/tracks/get-several-audio-features)\n  \n- **tempo** shows a symmetric distribution centered around 120 bpm with a gap in the appr. range (0,30). music within this range probably doesn't exist. Tempo 0 could be podcasts or audiobooks or music with complex rythms/unclear tempo?\n\n- **loudness** left skewed distribution \n\n- **duration_ms** seems to be poisson distributed (and not a truncated normal distribution)\n     - makes sense since poisson distribution express \"the probabilty of event occuring in specified interval with a known constant mean rate and independent of other events\" wikipedia\n     \n- **year** almost uniformly distributed with a drastic drop occuring around the 1940s"},{"metadata":{},"cell_type":"markdown","source":"# anomaly investigation"},{"metadata":{},"cell_type":"markdown","source":"## bivariate scatterplots/KDEs and univariate KDEs for popularity == 0 "},{"metadata":{"trusted":true},"cell_type":"code","source":" data = (\n     df.where(lambda r: r.popularity == 0)\n      .dropna()\n      .loc[:,['tempo', 'loudness', 'danceability', 'year', 'speechiness', 'instrumentalness']]\n )\n    \nsample_size = 5000\nidx = sample_without_replacement(data.shape[0], sample_size)\n\ng = sns.PairGrid(data.iloc[idx],diag_sharey=False)\ng.map_upper(sns.scatterplot, s=15)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## bivariate scatterplots/KDEs and univariate KDEs for popularity != 0 "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[idx]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data = (\n    df.where(lambda r: r.popularity != 0)\n      .dropna()\n      .loc[:,[\n          'tempo', 'loudness', 'danceability', \n          'year', 'popularity', 'speechiness', \n          'instrumentalness', 'duration_ms',\n          'acousticness',\n      ]]\n)\nsample_size = 5000\nidx = sample_without_replacement(data.shape[0], sample_size)\ng = sns.PairGrid(data.iloc[idx],diag_sharey=False)\ng.map_upper(sns.scatterplot, s=15)\ng.map_lower(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax  = plt.subplots(1,1,figsize=(12,10))\nmin_max_pop_year = (\n    df[df.popularity != 0].groupby('year')\n        .agg({\n            'popularity': [\n                lambda x: x.quantile(0.05),\n                lambda x: x.quantile(0.95),\n                'median',\n                'mean',\n            ]\n        })\n        .rename(columns={\n            '<lambda_0>': 'min',\n            '<lambda_1>': 'max',\n        })\n)\nax.set_title('Popularity vs year')\nax.fill_between(min_max_pop_year.index, min_max_pop_year['popularity', 'min'], min_max_pop_year['popularity', 'max'], label='90%')\nsns.lineplot(x=min_max_pop_year.index, y=min_max_pop_year['popularity','median'], ax=ax, color='r', label='median')\nsns.lineplot(x=min_max_pop_year.index, y=min_max_pop_year['popularity','mean'], ax=ax, color='y', label='mean')\nplt.legend()\nax.set_ylabel('popularity')\n#plt.yscale('log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(\n    (\n        df.iloc[idx].where(lambda r: r.popularity == 0).groupby('year').agg({'id': 'count'}) / \n        df.iloc[idx].groupby('year').agg({'id': 'count'})\n    ).fillna(value=0).reset_index().rename(columns={'id': 'percentile'})\n).mark_bar().encode(\n    x='year',\n    y='percentile',\n).properties(title='percentile zero popularity', width=600, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[idx].where(lambda r: r.popularity == 0).groupby('year').size()\nrow = alt.hconcat()\nbase = (\n    alt.Chart(df.iloc[idx])\n        .mark_bar()\n        .encode(x='year', y='count():Q', tooltip='year')\n)\nrow |= (\n    base\n        .transform_filter(alt.datum.popularity == 0)\n        .properties(width=600, height=500, title='track count with zero popularity vs year')\n)\nrow |= (\n    base\n        .transform_filter(alt.datum.popularity != 0)\n        .properties(width=600, height=500, title='track count with nonzero popularity vs year')\n)\nrow |= (\n    base\n        #.transform_filter(alt.datum.popularity != 0)\n        .properties(width=600, height=500, title='track count with nonzero popularity vs year')\n)\nrow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = alt.vconcat()\nrow &= (\nalt.Chart(df.iloc[idx])\n    .mark_bar()\n    .encode(x='year', y='mean(popularity):Q', tooltip=['year'])\n    .properties(width=800, height=400)   \n)\nrow &= (\nalt.Chart(df.iloc[idx])\n    .mark_bar()\n    .encode(x='year', y='median(popularity):Q', tooltip=['year'])\n    .properties(width=800, height=400)   \n)\nrow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.random import sample_without_replacement\n\nX = df.select_dtypes([int,float]).values\ncols = df.select_dtypes([int, float]).columns\nN = X.shape[0]\n#idx = sample_without_replacement(N, 0.5*N) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr().style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# covariance matrix insights gain\n\n**Directly visible effects on popularity(target variable)**\n\n- popularity of tracks strongly depends on the year the music track was produced\n- positive corr indicates that a trend in increasing popularity with increasing year \n    - the more recent the track is the more popular it is / modern music is popular\n- loudness and energy seems also to have positive effect on popularity\n- whereas acousticness seems to have a negative impact on popularity\n- unexpectedly both instrumentalness and danceability exhibit relatively low corr.\n \n(however we should not forget that covariance solely captures the tendency in linear rel.)\n   \n**Correlation between predictors**\n\n- there seems to be a strong correlation between loudness, energy and acousticness\n- since energetic tracks \"feel fast, **loud**, and noisy\" this leads to an obvious\n  connection between loudness and energy\n- to my knowledge the majority of acoustic songs are not known to 'energetic and loud'\n  (with exception of crazy classical music)\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(max_depth=2)\nclf.fit(df.year.values.reshape(-1,1), df.popularity.values)\n\nfig = plt.figure(figsize=(18,12))\n_ = tree.plot_tree(clf, filled=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first optimal split around the year 1953 via gini shows exactly what where exepecting."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['has_popularity'] = (df.popularity != 0).astype(int)\ntest_df['has_popularity'] = (test_df.popularity != 0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import (\n    train_test_split, cross_validate, KFold\n)\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\nX_train = df.drop(['popularity', 'has_popularity', 'id'], axis=1)\ny_train = df.has_popularity\n\nX_test = test_df.drop(['popularity', 'has_popularity', 'id'], axis=1)\ny_test = test_df.has_popularity\n\n\ndtrain = xgboost.DMatrix(X_train, label=y_train)\ndtest = xgboost.DMatrix(X_test, label=y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import ColumnTransformer\nimport sklearn.preprocessing as prep\n#from sklearn.preprocessing import FunctionTransformer\n\npreprocessor = ColumnTransformer([\n    ('key', prep.OneHotEncoder(handle_unknown='ignore', dtype=int), ['key']),\n    ('year', prep.MinMaxScaler(), ['year']),\n    (\n        'loudness', \n        make_pipeline(\n            prep.FunctionTransformer(func=lambda x: np.log(np.abs(x))),\n            prep.StandardScaler()\n        ),\n        ['loudness'],\n    ),\n    (\n        'duration_ms', \n        make_pipeline(\n            prep.FunctionTransformer(np.log),\n            prep.StandardScaler(),\n        ),\n        ['duration_ms'],\n    ),\n    ('tempo', prep.StandardScaler(), ['tempo']),\n    \n], remainder='passthrough')\n\nclf = Pipeline(steps=[\n    ('preprocess', preprocessor),\n    ('regress', LogisticRegression(max_iter=150))\n])\n\nscores = cross_validate(\n    clf,\n    X_train,\n    y_train,\n    scoring=['accuracy', 'f1', 'precision', 'recall'],\n    cv=5\n)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# xgboost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {}\nnum_boost_round = 999\nmin_mae = float(\"Inf\")\nbest_params = None\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgboost.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'logloss'},\n        early_stopping_rounds=10\n    )\n    # Update best MAE\n    mean_logloss = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\tlogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n    if mean_logloss < min_mae:\n        min_mae = mean_logloss\n        best_params = (max_depth,min_child_weight)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = dict(zip(('max_depth', 'min_child_weight'), best_params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params['eval_metric'] = 'logloss'\nmodel = xgboost.train(params, dtrain)\ny_pred = model.predict(dtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thr = roc_curve(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(15,7.5))\nax1, ax2 = ax\nax1.plot(fpr, tpr)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('ROC curve')\n\nprec, rec, thr = precision_recall_curve(y_test, y_pred)\nidx = np.argmax((2 *  rec * prec) / (prec + rec))\nax2.plot(rec, prec)\nax2.plot(rec[idx], prec[idx], marker='o', label='optimal')\nax2.set_xlabel('recall')\nax2.set_ylabel('precision')\nax2.set_title('precision recall curve')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('roc auc score {:.3f} with threshold {:.3f}'.format(roc_auc_score(y_test, y_pred), thr[idx]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    f1_score,\n    recall_score,\n    plot_confusion_matrix\n)\nprint('accuracy_score: {:.3f} \\nf1_score: {:.3f} \\nrecall_score: {:.3f}'.format(\n    accuracy_score(y_pred > thr[idx], y_test),\n    f1_score(y_pred  > thr[idx],y_test),\n    recall_score(y_pred  > thr[idx],y_test)\n))\n#plot_confusion_matrix(model, X_test, y_test, normalize='true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_pred > thr[idx], y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame(\n    {\n        'features': X_train.columns, \n        'feature_importances': model.feature_importances_,\n    }\n).sort_values('feature_importances', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression"},{"metadata":{},"cell_type":"markdown","source":"## linear regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"#cond = (df.has_popularity.astype(bool))\n#X_train = df[cond].drop(\n#    [\n#        'popularity', 'has_popularity', 'id'\n#    ],\n#    axis=1,\n#)\n#y_train = df[cond].popularity\n#\n#dtrain = xgboost.DMatrix(X_train, label=y_train)\n#dtest = xgboost.DMatrix(X_test, label=y_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# from sklearn import set_config\n# from sklearn.linear_model import LinearRegression, Ridge\n# from sklearn.compose import TransformedTargetRegressor\n# from sklearn.base import BaseEstimator, TransformerMixin\n\n# set_config(display='diagram')  \n# class GaussianMixtureTransformer(BaseEstimator, TransformerMixin):\n#     def __init__(self, n_components):\n#         self.n_components = n_components\n#         #self.column = column\n#         self._model = GaussianMixture(n_components=self.n_components, covariance_type='diag')\n\n#     def fit(self, X, y=None):\n#         self._model.fit(X)\n#         print('fit', self._model.means_)\n#         return self\n    \n#     def transform(self, X, y=None):\n#         pred = self._model.predict(X)\n#         print('transform', self._model.means_, id(self._model))\n#         #for i in range(self.n_components):\n#         #import pdb; pdb.set_trace()\n#             #X['{}_{}'.format(self.column, i)] = pred[:,i]\n#         X.values[:] = pred.reshape(-1,1)\n#         return X\n    \n#     @property\n#     def means(self):\n#         return self._model.means_\n    \n# class IdentityTransformer(BaseEstimator, TransformerMixin):\n\n#     def fit(self, X, y=None):\n#         return self\n    \n#     def transform(self, X, y=None):\n#         if len(X.values.shape) == 1:\n#             return X.values.reshape(-1,1)\n#         return X.values\n    \n# preprocessor = ColumnTransformer([\n#     ('year', prep.MinMaxScaler(), ['year']),\n#     #('key', prep.OneHotEncoder(handle_unknown='ignore', dtype=int), ['key']),\n#     (\n#         'loudness', \n#         make_pipeline(\n#             prep.FunctionTransformer(func=lambda x: np.sqrt(np.abs(x))),\n#             prep.StandardScaler()\n#         ),\n#         ['loudness'],\n#     ),\n#     (\n#         'duration_ms', \n#         make_pipeline(\n#             prep.FunctionTransformer(np.log),\n#             prep.StandardScaler(),\n#         ),\n#         ['duration_ms'],\n#     ),\n#     ('tempo', prep.StandardScaler(), ['tempo']),\n#     (\n#         'acousticness', \n#         Pipeline(steps=[\n#             ('GaussianMixtureTransformer', GaussianMixtureTransformer(2)),\n#             ('OneHotEncoder', prep.OneHotEncoder(handle_unknown='ignore', dtype=int)),\n#         ]),\n#        ['acousticness']\n#     ),\n#     ('instrumentalness', IdentityTransformer(), 'instrumentalness'),\n#     #(\n#     #    'acousticness', \n#     #    IdentityTransformer(),\n#     #   'acousticness'\n#     #),\n#     ('valence', IdentityTransformer(), 'valence'),\n#     ('speechiness', IdentityTransformer(), 'speechiness'),\n#     #(\n#     #    'speechiness', \n#     #    make_pipeline(\n#     #        GaussianMixtureTransformer(3),\n#     #        prep.OneHotEncoder(handle_unknown='ignore', dtype=int),\n#     #    ),\n#     #   ['speechiness']\n#     #)  \n# ], remainder='drop')\n\n# reg = Pipeline(steps=[\n#     ('preprocess', preprocessor),\n#     (\n#         'regress', TransformedTargetRegressor(\n#             Ridge(),\n#             func=lambda x: x/100,\n#             inverse_func=lambda x: x*100,\n#         )\n#     ),\n# ])\n\n# from sklearn.decomposition import PCA\n# from sklearn.manifold import TSNE\n\n# dim_red = Pipeline(steps=[\n#     ('preprocess', preprocessor),\n#     ('tsne', TSNE(n_components=2, perplexity=25)),    \n# ])\n# reg.fit(X_train, y_train)\n\n# #red_data = dim_red.fit_transform(X_train[:5000])\n# #plt.plot(\n# #    red_data[:,0], red_data[:,1], 'o'\n#)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# def get_columns_from_transformer(column_transformer, input_colums, include_remainder=False):    \n#     col_name = []\n\n#     for transformer_in_columns in column_transformer.transformers_[:-1]: #the last transformer is ColumnTransformer's 'remainder'\n#         raw_col_name = transformer_in_columns[2]\n#         if isinstance(transformer_in_columns[1],Pipeline): \n#             transformer = transformer_in_columns[1].steps[-1][1]\n#         else:\n#             transformer = transformer_in_columns[1]\n#         try:\n#             names = transformer.get_feature_names(raw_col_name)\n#         except AttributeError: # if no 'get_feature_names' function, use raw column name\n#             names = raw_col_name\n#         if isinstance(names,np.ndarray): # eg.\n#             col_name += names.tolist()\n#         elif isinstance(names,list):\n#             col_name += names    \n#         elif isinstance(names,str):\n#             col_name.append(names)\n\n#     [_, _, reminder_columns] = column_transformer.transformers_[-1]\n    \n#     if include_remainder:\n#         for col_idx in reminder_columns:\n#             col_name.append(input_colums[col_idx])\n\n#     return col_name\n\n# cols = get_columns_from_transformer(preprocessor, X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plt.subplots(figsize=(12,10))\n# df.groupby('year')['acousticness'].median().plot(label='median acoust')\n# df.groupby('year')['speechiness'].median().plot(label='median speech')\n# df.groupby('year')['instrumentalness'].median().plot(label='median inst')\n# df.groupby('year')['valence'].median().plot(label='median valence')\n# df.groupby('year')['energy'].median().plot(label='median energy')\n# df.groupby('year')['danceability'].median().plot(label='median danceability')\n# df.groupby('year')['liveness'].median().plot(label='median liveness')\n# df.groupby('year')['popularity'].median().apply(lambda x: x/100).plot(style='-o', label='median norm. popularity')\n# plt.legend()\n# plt.title('pseudo track characteristics')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df = df.merge(\n#     df\n#         .groupby(['year'])['popularity']\n#         .agg(lambda x: x.quantile(0.99))\n#         .rename('popularity_q95'),\n#     left_on='year',\n#     right_index=True,\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(12,10))\n# (\n#     df[df.popularity >= df.popularity_q95]\n#         .loc[:, \n#              [\n#                  'acousticness', \n#                  'liveness', \n#                  'speechiness', \n#                  'year',\n#                  'instrumentalness',\n#                  'valence',\n#                  'energy',\n#                  'danceability',\n#                  'popularity',\n#              ]\n#         ]\n#         .assign(popularity=lambda r: r.popularity/100)\n#         .groupby('year')\n#         .median()\n#         .plot(ax=ax)\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df[df.popularity >= df.popularity_q95].groupby('year')['tempo'].median().plot(style='-', label='median tempo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df[df.popularity >= df.popularity_q95].groupby('year')['loudness'].median().plot(label='median inst')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pd.DataFrame(reg['preprocess'].transform(X_train), columns=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reg.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plt.plot(reg.predict(X_train), y_train, 'o')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# cross_validate(\n#     reg, \n#     X_train, \n#     y_train,\n#     cv=5,\n#     scoring=[\n#         'max_error',\n#         'neg_mean_squared_error',\n#         'neg_median_absolute_error',\n#         'explained_variance'\n#     ],\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# dict(zip(cols, reg['regress'].regressor_.coef_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# import statsmodels.api as sm\n\n# model = sm.OLS(y_train, np.squeeze(reg['preprocess'].fit_transform(X_train))).fit()\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reg['preprocess'].fit_transform(X_train).shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# params = {}\n# num_boost_round = 999\n# min_rmse = float(\"Inf\")\n# best_params = None\n# gridsearch_params = [\n#     (max_depth, min_child_weight)\n#     for max_depth in range(9,12)\n#     for min_child_weight in range(5,8)\n# ]\n# for max_depth, min_child_weight in gridsearch_params:\n#     print(\"CV with max_depth={}, min_child_weight={}\".format(\n#                              max_depth,\n#                              min_child_weight))\n#     # Update our parameters\n#     params['max_depth'] = max_depth\n#     params['min_child_weight'] = min_child_weight\n#     # Run CV\n#     cv_results = xgboost.cv(\n#         params,\n#         dtrain,\n#         num_boost_round=num_boost_round,\n#         seed=42,\n#         nfold=5,\n#         metrics={'rmse'},\n#         early_stopping_rounds=10\n#     )\n#     # Update best MAE\n#     mean_rmse = cv_results['test-rmse-mean'].min()\n#     boost_rounds = cv_results['test-rmse-mean'].argmin()\n#     print(\"\\trmse {} for {} rounds\".format(mean_logloss, boost_rounds))\n#     if mean_logloss < min_rmse:\n#         min_rmse = mean_rmse\n#         best_params = (max_depth,min_child_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}