{"cells":[{"metadata":{"trusted":true,"_uuid":"8580784948917aa3e06fa5b64817dd2df169c71a","collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\ndataset = pd.read_csv('turkiye-student-evaluation_generic.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bf49cc191fbcfa585da350752ebd65ae8d6cdc4a"},"cell_type":"code","source":"dataset = pd.read_csv('turkiye-student-evaluation_generic.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ffbd0d5e21b8a527cf604864b4f677405d4e3461"},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3c0f7563e4365258510f246d94f275ac4af83f61"},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a011f1042d99c43aa0f62bd96898ebe58f345046"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # visualize\nimport matplotlib.pyplot as plt\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bc8a7b648ce5b889e5345064ac2ec28571afbfce"},"cell_type":"code","source":"plt.figure(figsize=(18,18))\nsns.heatmap(dataset.corr(),annot = True,fmt = \".2f\",cbar = True)\nplt.xticks(rotation=90)\nplt.yticks(rotation = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"300aeed1127b159b3c9dba152b86052a0db0d000"},"cell_type":"markdown","source":"## To understand which course got most responde"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"579645b681f91c8d892974ebbfa2d66e2f296061"},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.countplot(x='class', data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9a7ec293950908cefd7638f97a195ffc1db76f5"},"cell_type":"markdown","source":"### Below Graph to see how the rating has been given by student for each questions"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ebdebf7325c8c532dfc1bb9abac9cd4cbae0172c"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.boxplot(data=dataset.iloc[:,5:33 ])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c5612bb4db6678bdf0b719526236618b8f03d16"},"cell_type":"markdown","source":"### By above graph, we can see that very less students have given completely disagree (Rating 1) for Question Q14, Q15, Q17, Q19 - Q22, Q25,Q28"},{"metadata":{"_uuid":"e74759bc89fb26381235d8104464938df103b454"},"cell_type":"markdown","source":"## Lets understand the students have responded for the questions against classes"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"40aecb190a99a63d8d136958b8749886f364d038"},"cell_type":"code","source":"# Calculate mean for each question response for all the classes.\nquestionmeans = []\nclasslist = []\nquestions = []\ntotalplotdata = pd.DataFrame(list(zip(classlist,questions,questionmeans))\n                      ,columns=['class','questions', 'mean'])\nfor class_num in range(1,14):\n    class_data = dataset[(dataset[\"class\"]==class_num)]\n    \n    questionmeans = []\n    classlist = []\n    questions = []\n    \n    for num in range(1,14):\n        questions.append(num)\n    #Class related questions are from Q1 to Q12\n    for col in range(5,17):\n        questionmeans.append(class_data.iloc[:,col].mean())\n    classlist += 12 * [class_num] \n    print(classlist)\n    plotdata = pd.DataFrame(list(zip(classlist,questions,questionmeans))\n                      ,columns=['class','questions', 'mean'])\n    totalplotdata = totalplotdata.append(plotdata, ignore_index=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f38a6ce9cae26a437b80ce8fca484ac537278bf9"},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.pointplot(x=\"questions\", y=\"mean\", data=totalplotdata, hue=\"class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cb8a0d229812334bab4bee6d1a66d787e1b2690"},"cell_type":"markdown","source":"### Lets see how rating has been given against instructor wise."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e69ce7da50307abd473b2d2d5253284aa04ae599"},"cell_type":"code","source":"# Calculate mean for each question response for all the classes.\nquestionmeans = []\ninslist = []\nquestions = []\ntotalplotdata = pd.DataFrame(list(zip(inslist,questions,questionmeans))\n                      ,columns=['ins','questions', 'mean'])\nfor ins_num in range(1,4):\n    ins_data = dataset[(dataset[\"instr\"]==ins_num)]\n    questionmeans = []\n    inslist = []\n    questions = []\n    \n    for num in range(13,29):\n        questions.append(num)\n    \n    for col in range(17,33):\n        questionmeans.append(ins_data.iloc[:,col].mean())\n    inslist += 16 * [ins_num] \n    plotdata = pd.DataFrame(list(zip(inslist,questions,questionmeans))\n                      ,columns=['ins','questions', 'mean'])\n    totalplotdata = totalplotdata.append(plotdata, ignore_index=True)\n    plt.figure(figsize=(15, 5))\nsns.pointplot(x=\"questions\", y=\"mean\", data=totalplotdata, hue=\"ins\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56322d35d2a2135e17fbbfa202ad57b6ba11a88d"},"cell_type":"markdown","source":"### Lets begin to cluster the students based on the questionaire data"},{"metadata":{"_uuid":"8416f2d6fd5ac5e78b406f416ff030a4a153a171"},"cell_type":"markdown","source":"#### Lets try to cluster all the students based on the Question responses data."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b3617d1f87dbf340dc337c4991ee61a7987cda32"},"cell_type":"code","source":"dataset_questions = dataset.iloc[:,5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ae3ce0aca908e6f7a85037e0d7839a12123a373d"},"cell_type":"code","source":"dataset_questions.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42011db8c5a5a0808573f2685ab2cacd9e05a3df"},"cell_type":"markdown","source":"# PCA Analysis "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1524d401fd49f7fa84308e95b22a1be28bdaf493"},"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nX=dataset_questions\npca = PCA().fit(scale(X))\nplt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bf7ebbe187cec70dd23cc0aefa8cab44734cf94d"},"cell_type":"code","source":"X=dataset_questions\npca = PCA().fit(scale(X))\nplt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5c20f06e3141b03ff96187fa2bfc1626894ee1b7"},"cell_type":"code","source":"pca = PCA(n_components = 2)\ndataset_questions_pca = pca.fit_transform(dataset_questions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2863d86f26d7aafdab33401aba766ec4e3516d5"},"cell_type":"markdown","source":"### Variance (% cumulative) explained by the principal components"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2741fe9a4710354f3ad938040f2535f51a978853"},"cell_type":"code","source":"print(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100))\nprint(len(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"168889e7d4283a18d7a7700dc28e04ab6cd0232f"},"cell_type":"code","source":"np.cumsum(pca.explained_variance_ratio_)*100","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f334420368790f09bebd592cc728f3e1f2adb4b"},"cell_type":"markdown","source":"###  Eiegenvalues"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3704702ac51fcdffc27ff6909b150ed72e97cd98"},"cell_type":"code","source":"pca.explained_variance_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2e73dde0863635938c09eb0148e820b50349dcd"},"cell_type":"markdown","source":"# Clustring "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c16f523bea9c1039db92e4d9666ddbc65bc5aa91"},"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 7):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(dataset_questions_pca)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 7), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c2605403f3130a531b9706c7950f56cdc5f0ec4"},"cell_type":"markdown","source":"## based on the Elbow graph , we can go for 3 clusters."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4596624114f79975cf07769385f24e7bf8129e8f"},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 3, init = 'k-means++')\ny_kmeans = kmeans.fit_predict(dataset_questions_pca)\n# Visualising the clusters\nplt.scatter(dataset_questions_pca[y_kmeans == 0, 0], dataset_questions_pca[y_kmeans == 0, 1], s = 100, c = 'yellow', label = 'Cluster 1')\nplt.scatter(dataset_questions_pca[y_kmeans == 1, 0], dataset_questions_pca[y_kmeans == 1, 1], s = 100, c = 'green', label = 'Cluster 2')\nplt.scatter(dataset_questions_pca[y_kmeans == 2, 0], dataset_questions_pca[y_kmeans == 2, 1], s = 100, c = 'red', label = 'Cluster 3')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'blue', label = 'Centroids')\nplt.title('Clusters of students')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18976679e3187949778ce51ab5acbdb0b98a5de2"},"cell_type":"markdown","source":"### Looking at the above graph , i see we have 3 clusters of students who have given like Negative, Neutral and Positive feedback"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9d9310b2ce36687f7661dd6b0ad363b1ae79afc3"},"cell_type":"code","source":"import collections\ncollections.Counter(y_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d42ef8442a73f118485a547da81409d6399579c"},"cell_type":"markdown","source":"### So we have 2358 students who have given negative ratings overall , 2222 students with positive ratings and 1240 students with nuetral response"},{"metadata":{"_uuid":"a1360c4dc41c8d7fbafad926d5550a466d83d54a"},"cell_type":"markdown","source":"##  Using the dendrogram to find the optimal number of clusters"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3e72ff067737cc9cfeec48eea8ada626b8f1a75b"},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(dataset_questions_pca, method = 'ward'))\n#plt.figure(figsize=(20, 10))\nplt.title('Dendrogram')\nplt.xlabel('questions')\nplt.ylabel('Euclidean distances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"01482c6842298fe02276c11d6527224e0896a5a3"},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(sch.linkage(dataset_questions_pca, method = 'centroid'))\n#plt.figure(figsize=(20, 10))\nplt.title('Dendrogram')\nplt.xlabel('questions')\nplt.ylabel('Euclidean distances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"58cb47d218351aa25ffda67846fa189fa7f2b850"},"cell_type":"code","source":"# Fitting Hierarchical Clustering to the dataset\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(dataset_questions_pca)\nX = dataset_questions_pca\n# Visualising the clusters\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'yellow', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'red', label = 'Cluster 2')\nplt.title('Clusters of STUDENTS')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"839854fce81aab232f6757767f83a9840a7ed18c"},"cell_type":"code","source":"# Let me check the count of students in each cluster\nimport collections\ncollections.Counter(y_hc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9911267c91ef6fbf5d2532c4930b680a7657d187"},"cell_type":"markdown","source":"# Factor analysis "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"954c72e60d699ecdc253f4e8595f61e123c341a5"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"172db76bbacd07e4bb261685d64187dcf22bcedd"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nimport pandas as pd\nimport numpy as np\n#from sklearn.datasets import load_iris\nfrom sklearn.decomposition import FactorAnalysis\nfactor = FactorAnalysis(n_components=2, random_state=101).fit(X)\npd.DataFrame(factor.components_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b85a6193cef5a56f16f33916c4d302fe505ad41"},"cell_type":"markdown","source":"# Try"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6382bf2ea189bd8252f54c658d1870ebd1dd89db"},"cell_type":"code","source":"dataset.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"31376a6a17bcd9746a1b99f65169464e9a0dedae"},"cell_type":"code","source":"from sklearn.preprocessing import scale\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2332a22a7c502edb7357925eff4e826f878571e3"},"cell_type":"code","source":"data=scale(dataset.iloc[:,3:])\ndata\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8b77ff706a7cdca7db38d7559b4d3c426b107246"},"cell_type":"code","source":"from pylab import * \nimport pandas as pd\nimport numpy as np   \nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB  \nimport sklearn.metrics as sm  \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom sklearn.naive_bayes import GaussianNB  \nfrom sklearn import tree, svm\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.under_sampling import ClusterCentroids\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import RandomOverSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"48379d54160ce281f37b3aa5897e6cda5451d4ce"},"cell_type":"code","source":"seed = 7\ntest_size = 0.15\nData=data\nTarget=dataset.iloc[:,2]\nClasses=Target","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f382587e97ad12fab9a2325278c33c14e3bb5fe2"},"cell_type":"code","source":"Naive_Bayes_classifier = GaussianNB() \nDTC = tree.DecisionTreeClassifier()\nLR = LogisticRegression() \nSVM_Classifier = svm.SVC() \nKNN = KNeighborsClassifier(n_neighbors=5)  \nRF=RandomForestClassifier(n_estimators=25)\nX_train, X_test, y_train, y_test = train_test_split(Data, Target, test_size=test_size, random_state=seed)\nTrained_Classifier  =RF.fit(X_train, y_train)  \n\nTested_Classifier = Trained_Classifier.predict(X_test)  \n  \n# Compute the confusion matrix and calculation the performance creteria   \nP=sm.confusion_matrix(y_test,Tested_Classifier)  \nAccuracy = ((P[0,0] + P[1,1]+P[2,2])/float(sum(P)))*100  \nsensitivity = (P[0,0] / float(sum(P[:,0])))*100  \nspecificity = (P[1,1] / float(sum(P[:,1])))*100  \nPrecision =(P[0,0] / float(sum(P[0,:])))*100 # TP/(TP+FP)\nF_measure=(2*sensitivity*Precision )/(sensitivity+Precision)\n\nprint(\" Accuracy = %s\" % Accuracy, \" Sensitivity = %s\" % sensitivity,\" Specificity = %s\" % specificity)  \nprint(\" Precision = %s\" % Precision ,\"F-measure= %s\"%F_measure)  \n\n             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"46c8c36f00db241a78579dc163a9c2f8f33131d8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"198c0dbed23ce15d9872cee9f5bcf4fd091c5e17"},"cell_type":"code","source":"Naive_Bayes_classifier = GaussianNB() \nDTC = tree.DecisionTreeClassifier()\nLR = LogisticRegression() \nSVM_Classifier = svm.SVC() \nKNN = KNeighborsClassifier(n_neighbors=5)  \nRF=RandomForestClassifier(n_estimators=25)\nX_train, X_test, y_train, y_test = train_test_split(Data, Target, test_size=test_size, random_state=seed)\nTrained_Classifier = LR.fit(X_train, y_train)  \n\nTested_Classifier = Trained_Classifier.predict(X_test)  \n  \n# Compute the confusion matrix and calculation the performance creteria   \nP=sm.confusion_matrix(y_test,Tested_Classifier)  \nAccuracy = ((P[0,0] + P[1,1]+P[2,2])/float(sum(P)))*100  \nsensitivity = (P[0,0] / float(sum(P[:,0])))*100  \nspecificity = (P[1,1] / float(sum(P[:,1])))*100  \nPrecision =(P[0,0] / float(sum(P[0,:])))*100 # TP/(TP+FP)\nF_measure=(2*sensitivity*Precision )/(sensitivity+Precision)\n\nprint(\" Accuracy = %s\" % Accuracy, \" Sensitivity = %s\" % sensitivity,\" Specificity = %s\" % specificity)  \nprint(\" Precision = %s\" % Precision ,\"F-measure= %s\"%F_measure)  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"88652e13adb847ea07695b018bd46345231a3dae"},"cell_type":"code","source":"print('original data check balance: ',sorted(Counter(Target).items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0b3dd18c602c277b74faab5b6525b8350f0d66be"},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.countplot(x=Target, data=dataset)\npercentage=print('Class1=',4909*100/(5820),'Class2=' ,576*100/(5820),'Class3=',335*100/(5820))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cabddc09e3182da7e2c075062e4fd875716d7e5e"},"cell_type":"markdown","source":"# we have biased data, so I am going to resolve it"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ed799607fc14470385c22d3f0c28c72c1c43f4b6"},"cell_type":"code","source":"cc = ClusterCentroids(random_state=0)\nsmote_tomek = SMOTETomek(random_state=0)\nData1, Classes1 = smote_tomek.fit_sample(Data, Classes)\nprint(sorted(Counter(Classes1).items()))\nX_train, X_test, y_train, y_test = train_test_split(Data, Classes, test_size=test_size, random_state=seed)\nprint('modified data check balance using SMOTETomek: ',sorted(Counter(Classes1).items()))\nData2, Classes2 = cc.fit_sample(Data, Classes)\nprint('modified data check balance using undrsampling: ',sorted(Counter(Classes2).items()))\nros = RandomOverSampler(random_state=0)\nData3, Classes3 = ros.fit_sample(Data, Classes)\nprint('modified data check balance using oversampling: ',sorted(Counter(Classes3).items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fa3bd28d43c673956762885e6e65860eee17828d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d33d4b356f0da59dad925e0e4ab780ddfd027ef7"},"cell_type":"code","source":"seed = 7\ntest_size = 0.15\n\n\nNaive_Bayes_classifier = GaussianNB() \nDTC = tree.DecisionTreeClassifier()\nLR = LogisticRegression() \nSVM_Classifier = svm.SVC() \nKNN = KNeighborsClassifier(n_neighbors=5)  \nRF=RandomForestClassifier(n_estimators=25)\nX_train, X_test, y_train, y_test = train_test_split(Data1, Classes1, test_size=test_size, random_state=seed)\nTrained_Classifier  =RF.fit(X_train, y_train)  \nTested_Classifier = Trained_Classifier.predict(X_test)  \n  \n# Compute the confusion matrix and calculation the performance creteria   \nP=sm.confusion_matrix(y_test,Tested_Classifier)  \nAccuracy = ((P[0,0] + P[1,1]+P[2,2])/float(sum(P)))*100  \nsensitivity = (P[0,0] / float(sum(P[:,0])))*100  \nspecificity = (P[1,1] / float(sum(P[:,1])))*100  \nPrecision =(P[0,0] / float(sum(P[0,:])))*100 # TP/(TP+FP)\nF_measure=(2*sensitivity*Precision )/(sensitivity+Precision)\n\nprint(\" Accuracy = %s\" % Accuracy, \" Sensitivity = %s\" % sensitivity,\" Specificity = %s\" % specificity)  \nprint(\" Precision = %s\" % Precision ,\"F-measure= %s\"%F_measure)  \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"}},"nbformat":4,"nbformat_minor":1}