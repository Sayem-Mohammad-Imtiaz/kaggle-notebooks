{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import img_to_array","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv('../input/train-csv/legend.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c1288f883078dc82085f6662483988db73aed29"},"cell_type":"code","source":"train_csv['emotion'] = train_csv['emotion'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f7a89d5822dc981b92d08f56d3083d032d7463f"},"cell_type":"code","source":"train_csv.groupby('emotion').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae405ec8cf82dd5a864f3aab22f3c4f554955e80"},"cell_type":"code","source":"train_csv.replace(\"contempt\", \"anger\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3d14d4d03d23c0d40e19ddc5049e6cb725d237a"},"cell_type":"code","source":"train_csv.groupby('emotion').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e36c1b89f68b736aa4c5179de97537898447493"},"cell_type":"code","source":"mapping_emotion = {'anger': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'neutral': 6, 'sadness': 4, 'surprise': 5}\ntrain_csv['label'] = train_csv['emotion'].map(mapping_emotion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4866e533f18821fc45380fcd3aff47afd93c2661"},"cell_type":"code","source":"train_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de4e9948ceb85a1bb5de3176fd066103ddcb1cdf"},"cell_type":"code","source":"import glob\nimport cv2 as cv\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f02394cc36b771108af2ae8d8a12639cab99ff89"},"cell_type":"code","source":"trained = '../input/trainedimages'\n#os.mkdir(trained)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73ab220270f57bb9ea88c74d89a20437b63f9abb"},"cell_type":"code","source":"face_cascade = cv.CascadeClassifier('../input/haarcascades/haarcascade_frontalface_default.xml')\nimage_train = '../input/trainimages/images_train/images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9686de3f2f503b32dcccc8a1b4846fec407e1262"},"cell_type":"code","source":"deleting = ['Abdul_Majeed_Shobokshi_0001.jpg',\n 'Arsinee_Khanjian_0001.jpg',\n 'Avinash_30.jpg',\n 'Colin_Montgomerie_0004.jpg',\n 'Colin_Powell_0048.jpg',\n 'David_McCullough_0001.jpg',\n 'Donald_Rumsfeld_0117.jpg',\n 'Fernando_Vargas_0004.jpg',\n 'Franz_Muentefering_0003.jpg',\n 'George_HW_Bush_0003.jpg',\n 'George_Pataki_0002.jpg',\n 'Hans_Blix_0016.jpg',\n 'Isaiah_Washington_0002.jpg',\n 'Jeff_Feldman_0001.jpg',\n 'Jiang_Zemin_0002.jpg',\n 'Jiang_Zemin_0007.jpg',\n 'Joe_Vandever_0001.jpg',\n 'John_Wright_0001.jpg',\n 'Kimberly_Bruckner_0001.jpg',\n 'Kimberly_Stewart_0001.jpg',\n 'Kimi_Raikkonen_0001.jpg',\n 'Kimi_Raikkonen_0002.jpg',\n 'Kimi_Raikkonen_0003.jpg',\n 'Kimora_Lee_0001.jpg',\n 'Lin_Yi-fu_0001.jpg',\n 'Luciano_Pavarotti_0002.jpg',\n 'Lynne_Thigpen_0001.jpg',\n 'Michael_Powell_0003.jpg',\n 'Miguel_Contreras_0001.jpg',\n 'Morgan_Freeman_0002.jpg',\n 'Padraig_Harrington_0004.jpg',\n 'Paul_Bremer_0014.jpg',\n 'Pedro_Malan_0003.jpg',\n 'Pierce_Brosnan_0007.jpg',\n 'Pyar_Jung_Thapa_0001.jpg',\n 'Richard_Gephardt_0007.jpg',\n 'Robert_Horan_0002.jpg',\n 'Robert_Zoellick_0005.jpg',\n 'Rob_Moore_0001.jpg',\n 'Scott_McNealy_0001.jpg',\n 'Thomas_Daily_0001.jpg',\n 'Tony_Blair_0090.jpg',\n 'William_Bulger_0002.jpg',\n 'Will_Ferrell_0001.jpg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1609356900307e8287ad2ad66e369f1d2417f213"},"cell_type":"code","source":"data = []\nlabels = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"663271ad971de46256d366adeab34b5bc9e24b6e"},"cell_type":"code","source":"i = 0\nfor img in glob.glob(image_train+\"/*.jpg\"):\n    image = cv.imread(img)\n    name = img.split('/')[-1]\n    \n    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY) # convert to greyscale\n    height, width = image.shape[:2]\n    faces = face_cascade.detectMultiScale(gray_image, 1.3, 1)\n    if isinstance(faces, tuple):\n        resized_image = cv.resize(gray_image, (48, 48))\n        cv.imwrite(trained+'/'+name,resized_image)\n    #print(faces)\n    elif isinstance(faces, np.ndarray):\n        for (x,y,w,h) in faces:\n            if w * h < (height * width) / 3:\n                resized_image = cv.resize(gray_image, (48, 48)) \n                cv.imwrite(trained+'/'+name,resized_image)\n            else:\n                \n                #cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n                roi_gray = gray_image[y:y+h, x:x+w]\n                #print(len(roi_gray))\n                resized_image = cv.resize(roi_gray, (48, 48))\n                cv.imwrite(trained+'/'+name, resized_image)\n    if not name in deleting:\n        data.append(img_to_array(resized_image))\n        label = int(train_csv[ train_csv['image'] == name][['label']].values)\n        #print(label, type(label), name)\n        labels.append(label)\n    \"\"\"if i == 300:\n        break\n    i = i + 1\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1615bbe93992968ebf362d6a09ad6e1255d93c3"},"cell_type":"code","source":"int(train_csv[ train_csv['image'] == 'Al_Sharpton_0004.jpg'][['label']].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67244e626ef4905490d496ab4d3c53a52366dcec"},"cell_type":"code","source":"len(labels), len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"6cd171d4f2b688e1ccf8cbd2be42617260ccd57b"},"cell_type":"code","source":"type(data), type(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"426defbe5de9c7c5e27b39345d5ca83c9053b21e"},"cell_type":"code","source":"# scale the raw pixel intensities to the range [0, 1]\ndata = np.array(data, dtype=\"float\") / 255.0\nlabels = np.array(labels)\nprint(\"[INFO] data matrix: {:.2f}MB\".format(data.nbytes / (1024 * 1000.0)))\nprint(data.shape, labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de640cb4b2e8c71146633908fc13fc9f95d79e51"},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n# binarize the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08f2f04149ca948c39144bc0e467a2d4d8620ee4"},"cell_type":"code","source":"# partition the data into training and testing splits using 70% of\n# the data for training and the remaining 30% for testing\nfrom sklearn.model_selection import train_test_split\n(trainX, valX, trainY, valY) = train_test_split(data,labels, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec84b78e5ec7e24e365267bf5be117734825fa65"},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true,"_uuid":"0bf43ba53daf3d43ffdd746ed9bd47ced59f1a57"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45fb9983f70c77158a37a1c617b5099b0e20e1af"},"cell_type":"code","source":"def buildModel(width, height, depth, classes):\n\t\t# initialize the model along with the input shape to be\n\t\t# \"channels last\" and the channels dimension itself\n\t\tmodel = Sequential()\n\t\tinputShape = (height, width, depth)\n\t\tchanDim = -1\n\n\t\t# if we are using \"channels first\", update the input shape\n\t\t# and channels dimension\n\t\tif K.image_data_format() == \"channels_first\":\n\t\t\tinputShape = (depth, height, width)\n\t\t\tchanDim = 1\n\n\t\t# CONV => RELU => POOL\n\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\",\n\t\t\tinput_shape=inputShape))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# (CONV => RELU) * 2 => POOL\n\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# (CONV => RELU) * 2 => POOL\n\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# first (and only) set of FC => RELU layers\n\t\tmodel.add(Flatten())\n\t\tmodel.add(Dense(1024))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization())\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# softmax classifier\n\t\tmodel.add(Dense(classes))\n\t\tmodel.add(Activation(\"softmax\"))\n\n\t\t# return the constructed network architecture\n\t\treturn model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"771825ca6980a3ad9c3dcd5e17ce298a748bb484"},"cell_type":"code","source":"# initialize the number of epochs to train for, initial learning rate,\n# batch size, and image dimensions\nEPOCHS = 100\nINIT_LR = 1e-3\nBS = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba2b2e03d432921219778a2b04e1ecd736db4b40"},"cell_type":"code","source":"model = buildModel(width=48, height=48,depth=1, classes=len(lb.classes_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"621f465058bc41b972a6a2052c60fc178f9af846"},"cell_type":"code","source":"from keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa8bfd828039f0522c1db8f79c9107ed0baccc01"},"cell_type":"code","source":"opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0beb35e506be223d208038c62b47f095d10d7e6e"},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\naug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n\thorizontal_flip=True, fill_mode=\"nearest\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d387aada491fb78c0800b7b55c5aeed439b7127e"},"cell_type":"code","source":"H = model.fit_generator(\n\taug.flow(trainX, trainY, batch_size=BS),\n\tvalidation_data=(valX, valY),\n\tsteps_per_epoch=len(trainX) // BS,\n\tepochs=EPOCHS, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bb547a9f49229802a889a7fd1b3491b8c8cc70c"},"cell_type":"code","source":"model.save('../input/emotion.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"424db398be3f8cb2b16a6b9c547a6c439d57de42"},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19068cad216e2c289c42ad16428e2324a22f4507"},"cell_type":"code","source":"f = open(\"../input/lb.pickle\", \"wb\")\nf.write(pickle.dumps(lb))\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db55ca95820f3c88a56971d335a1eb6a78cf5997"},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true,"_uuid":"d471d5eee164913cfd4aa771a9f3b7eb2da68f9f"},"cell_type":"code","source":"image_path = '../input/prediction/prediction/57b.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f2668c337619032219a7364ff9f913af4a5e5f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64bcbe3b29b12c2f0521fca17217cb7b59d458bb"},"cell_type":"code","source":"image = cv.imread(image_path)\nimage = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\nimage = cv.resize(image, (48, 48))\nimage = image.astype(\"float\") / 255.0\nimage = img_to_array(image)\nimage = np.expand_dims(image, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb5fcd1f33e241d035baa9bcfdb933be2e691ac7"},"cell_type":"code","source":"image.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee63cbbf95a9259685b278661fbd7663f3f486f2"},"cell_type":"code","source":"df_test = pd.read_csv('../input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"169bb258a3e0dd15313c8184fe159e3e434c561f"},"cell_type":"code","source":"df_test = df_test.sample(frac=0.10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cba5dabc8051a0436735059b8d2345173a6bf33"},"cell_type":"code","source":"X_test = []\ntest_index = df_test.index\nfor item in df_test.index:\n    pixels = df_test.pixels[item]\n    pixels = pixels.split(' ')\n    piarray = np.asarray(pixels, dtype=np.int64)\n    re = piarray.reshape(48,48)\n    X_test.append(re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab576c7cf63620ecf32457d4c17e5384fd80a522"},"cell_type":"code","source":"X_test = np.asarray(X_test)\nX_test = np.expand_dims(X_test, axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c03cee4629db223498a9d4f14ea252b27e2a1ea0"},"cell_type":"code","source":"X_test[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ae0f365eda400d6a3936fa9999f4acc77d93704","scrolled":true},"cell_type":"code","source":"\"\"\"proba = model.predict(image)[0]\nidx = np.argmax(proba)\nlabel = lb.classes_[idx]\n\"\"\"\nY_predict = []\nfor item in X_test:\n    item = np.expand_dims(item, axis = 0)\n    predict = model.predict(item)\n    idx = np.argmax(predict)\n    label = lb.classes_[idx]\n    Y_predict.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44e268b3606ed230c95dc40c6e2bf4d1e6fc1f4a"},"cell_type":"code","source":"Y_true = df_test.emotion.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aa458fb2917a531d98bacb2fbf37ff9811d1b80"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(Y_true, Y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46429b73bc4fd3a5df2a1762cdf41d4349ec8ef4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1437929bfaa7ff39c1f9b5fc81a254ed2b57ca8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}