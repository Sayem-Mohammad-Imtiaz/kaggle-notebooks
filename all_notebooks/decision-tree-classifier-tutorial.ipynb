{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Source of tutorial:\n\nhttps://www.kaggle.com/prashant111/decision-tree-classifier-tutorial"},{"metadata":{},"cell_type":"markdown","source":"# **2. Classification and Regression Trees (CART)** <a class=\"anchor\" id=\"2\"></a>\n\n[Table of Contents](#0.1)\n\n\nNowadays, Decision Tree algorithm is known by its modern name **CART** which stands for **Classification and Regression Trees**. Classification and Regression Trees or **CART** is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification and regression modeling problems.\n\n\nThe CART algorithm provides a foundation for other important algorithms like bagged decision trees, random forest and boosted decision trees. In this kernel, I will solve a classification problem. So, I will refer the algorithm also as Decision Tree Classification problem. \n"},{"metadata":{},"cell_type":"markdown","source":"# **3. Decision Tree algorithm terminology** <a class=\"anchor\" id=\"3\"></a>\n\n[Table of Contents](#0.1)\n\n\n- In a Decision Tree algorithm, there is a tree like structure in which each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The paths from the root node to leaf node represent classification rules.\n\n- We can see that there is some terminology involved in Decision Tree algorithm. The terms involved in Decision Tree algorithm are as follows:-\n\n\n## **Root Node**\n\n- It represents the entire population or sample. This further gets divided into two or more homogeneous sets.\n\n\n## **Splitting**\n\n- It is a process of dividing a node into two or more sub-nodes.\n\n\n## Decision Node\n\n- When a sub-node splits into further sub-nodes, then it is called a decision node.\n\n\n## Leaf/Terminal Node\n\n- Nodes that do not split are called Leaf or Terminal nodes.\n\n\n## Pruning\n\n- When we remove sub-nodes of a decision node, this process is called pruning. It is the opposite process of splitting.\n\n\n## Branch/Sub-Tree\n\n- A sub-section of an entire tree is called a branch or sub-tree.\n\n\n## Parent and Child Node\n\n- A node, which is divided into sub-nodes is called the parent node of sub-nodes where sub-nodes are the children of a parent node. \n\n\nThe above terminology is represented clearly in the following diagram:-"},{"metadata":{},"cell_type":"markdown","source":"### Decision-Tree terminology\n\n![Decision-Tree terminology](https://gdcoder.com/content/images/2019/05/Screen-Shot-2019-05-18-at-03.40.41.png)"},{"metadata":{},"cell_type":"markdown","source":"# **4. Decision Tree algorithm intuition** <a class=\"anchor\" id=\"4\"></a>\n\n[Table of Contents](#0.1)\n\nThe Decision-Tree algorithm is one of the most frequently and widely used supervised machine learning algorithms that can be used for both classification and regression tasks. The intuition behind the Decision-Tree algorithm is very simple to understand.\n\n\nThe Decision Tree algorithm intuition is as follows:-\n\n\n1.\tFor each attribute in the dataset, the Decision-Tree algorithm forms a node. The most important attribute is placed at the root node. \n\n2.\tFor evaluating the task in hand, we start at the root node and we work our way down the tree by following the corresponding node that meets our condition or decision.\n\n3.\tThis process continues until a leaf node is reached. It contains the prediction or the outcome of the Decision Tree.\n"},{"metadata":{},"cell_type":"markdown","source":"# **5. Attribute selection measures** <a class=\"anchor\" id=\"5\"></a>\n\n[Table of Contents](#0.1)\n\n\nThe primary challenge in the Decision Tree implementation is to identify the attributes which we consider as the root node and each level. This process is known as the **attributes selection**. There are different attributes selection measure to identify the attribute which can be considered as the root node at each level.\n\n\nThere are 2 popular attribute selection measures. They are as follows:-\n\n\n- **Information gain**\n\n- **Gini index**\n\n\nWhile using **Information gain** as a criterion, we assume attributes to be categorical and for **Gini index** attributes are assumed to be continuous. These attribute selection measures are described below.\n"},{"metadata":{},"cell_type":"markdown","source":"The ID3 (Iterative Dichotomiser) Decision Tree algorithm uses entropy to calculate information gain. So, by calculating decrease in **entropy measure** of each attribute we can calculate their information gain. The attribute with the highest information gain is chosen as the splitting attribute at the node."},{"metadata":{},"cell_type":"markdown","source":"## **5.2 Gini index** <a class=\"anchor\" id=\"5.2\"></a>\n\n[Table of Contents](#0.1)\n\n\nAnother attribute selection measure that **CART (Categorical and Regression Trees)** uses is the **Gini index**. It uses the Gini method to create split points. \n\n\nGini index can be represented with the following diagram:-"},{"metadata":{},"cell_type":"markdown","source":"## **Gini index**\n\n![Gini index](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRzYHkcmZKKp2sJN1HpHvw-NgqbD9EnapnbXozXRgajrSGvEnYy&s)\n\n\nHere, again **c** is the number of classes and **pi** is the probability associated with the ith class."},{"metadata":{},"cell_type":"markdown","source":"Gini index says, if we randomly select two items from a population, they must be of the same class and probability for this is 1 if the population is pure.\n\nIt works with the categorical target variable “Success” or “Failure”. It performs only binary splits. The higher the value of Gini, higher the homogeneity. CART (Classification and Regression Tree) uses the Gini method to create binary splits.\n\nSteps to Calculate Gini for a split\n\n1.\tCalculate Gini for sub-nodes, using formula sum of the square of probability for success and failure (p^2+q^2).\n\n2.\tCalculate Gini for split using weighted Gini score of each node of that split.\n\n\nIn case of a discrete-valued attribute, the subset that gives the minimum gini index for that chosen is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split-point and point with smaller gini index chosen as the splitting point. The attribute with minimum Gini index is chosen as the splitting attribute."},{"metadata":{},"cell_type":"markdown","source":"# **7. Import libraries** <a class=\"anchor\" id=\"7\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **8. Import dataset** <a class=\"anchor\" id=\"8\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = '/kaggle/input/car-evaluation-data-set/car_evaluation.csv'\n\ndf = pd.read_csv(data, header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **9. Exploratory data analysis** <a class=\"anchor\" id=\"9\"></a>\n\n[Table of Contents](#0.1)\n\n\nNow, I will explore the data to gain insights about the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# view dimensions of dataset\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 1728 instances and 7 variables in the data set."},{"metadata":{},"cell_type":"markdown","source":"### View top 5 rows of dataset"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# preview the dataset\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rename column names\n\nWe can see that the dataset does not have proper column names. The columns are merely labelled as 0,1,2.... and so on. We should give proper names to the columns. I will do it as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n\n\ndf.columns = col_names\n\ncol_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's again preview the dataset\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the column names are renamed. Now, the columns have meaningful names."},{"metadata":{},"cell_type":"markdown","source":"### View summary of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequency distribution of values in variables\n\nNow, I will check the frequency counts of categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n\n\nfor col in col_names:\n    \n    print(df[col].value_counts())   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the `doors` and `persons` are categorical in nature. So, I will treat them as categorical variables."},{"metadata":{},"cell_type":"markdown","source":"### Summary of variables\n\n\n- There are 7 variables in the dataset. All the variables are of categorical data type.\n\n\n- These are given by `buying`, `maint`, `doors`, `persons`, `lug_boot`, `safety` and `class`.\n\n\n- `class` is the target variable."},{"metadata":{},"cell_type":"markdown","source":"### Explore `class` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `class` target variable is ordinal in nature."},{"metadata":{},"cell_type":"markdown","source":"### Missing values in variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values in variables\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing values in the dataset. I have checked the frequency distribution of values previously. It also confirms that there are no missing values in the dataset."},{"metadata":{},"cell_type":"markdown","source":"# **10. Declare feature vector and target variable** <a class=\"anchor\" id=\"10\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['class'], axis=1)\n\ny = df['class']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **11. Split data into separate training and test set** <a class=\"anchor\" id=\"11\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **12. Feature Engineering** <a class=\"anchor\" id=\"12\"></a>\n\n[Table of Contents](#0.1)\n\n\n**Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.\n\n\nFirst, I will check the data types of variables again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data types in X_train\n\nX_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode categorical variables\n\n\nNow, I will encode the categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all  the variables are ordinal categorical data type."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import category encoders\n\nimport category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode variables with ordinal encoding\n\nencoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have training and test set ready for model building. "},{"metadata":{},"cell_type":"markdown","source":"# **13. Decision Tree Classifier with criterion gini index** <a class=\"anchor\" id=\"13\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import DecisionTreeClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the DecisionTreeClassifier model with criterion gini index\n\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_gini.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict the Test set results with criterion gini index"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gini = clf_gini.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check accuracy score with criterion gini index"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, **y_test** are the true class labels and **y_pred_gini** are the predicted class labels in the test-set."},{"metadata":{},"cell_type":"markdown","source":"### Compare the train-set and test-set accuracy\n\n\nNow, I will compare the train-set and test-set accuracy to check for overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train_gini = clf_gini.predict(X_train)\n\ny_pred_train_gini","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for overfitting and underfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting. \n"},{"metadata":{},"cell_type":"markdown","source":"### Visualize decision-trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nfrom sklearn import tree\n\ntree.plot_tree(clf_gini.fit(X_train, y_train)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize decision-trees with graphviz"},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz \ndot_data = tree.export_graphviz(clf_gini, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **14. Decision Tree Classifier with criterion entropy** <a class=\"anchor\" id=\"14\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the DecisionTreeClassifier model with criterion entropy\n\nclf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_en.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict the Test set results with criterion entropy"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_en = clf_en.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check accuracy score with criterion entropy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with criterion entropy: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compare the train-set and test-set accuracy\n\n\nNow, I will compare the train-set and test-set accuracy to check for overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train_en = clf_en.predict(X_train)\n\ny_pred_train_en","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for overfitting and underfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_en.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_en.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the training-set score and test-set score is same as above. The training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting. \n"},{"metadata":{},"cell_type":"markdown","source":"### Visualize decision-trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nfrom sklearn import tree\n\ntree.plot_tree(clf_en.fit(X_train, y_train)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize decision-trees with graphviz"},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz \ndot_data = tree.export_graphviz(clf_en, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. \n\n\nWe have another tool called `Confusion matrix` that comes to our rescue."},{"metadata":{},"cell_type":"markdown","source":"# **17. Results and conclusion** <a class=\"anchor\" id=\"17\"></a>\n\n[Table of Contents](#0.1)\n\n\n1.\tIn this project, I build a Decision-Tree Classifier model to predict the safety of the car. I build two models, one with criterion `gini index` and another one with criterion `entropy`. The model yields a very good performance as indicated by the model accuracy in both the cases which was found to be 0.8021.\n2.\tIn the model with criterion `gini index`, the training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting.\n3.\tSimilarly, in the model with criterion `entropy`, the training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021.We get the same values as in the case with criterion `gini`. So, there is no sign of overfitting.\n4.\tIn both the cases, the training-set and test-set accuracy score is the same. It may happen because of small dataset."},{"metadata":{},"cell_type":"markdown","source":"# **18. References** <a class=\"anchor\" id=\"18\"></a>\n\n[Table of Contents](#0.1)\n\n\nThe work done in this project is inspired from following books and websites:-\n\n1. Hands on Machine Learning with Scikit-Learn and Tensorflow by Aurélién Géron\n\n2. Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido\n\n3. https://en.wikipedia.org/wiki/Decision_tree\n\n4. https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n\n5. https://en.wikipedia.org/wiki/Entropy_(information_theory)\n\n6. https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n\n7. https://stackabuse.com/decision-trees-in-python-with-scikit-learn/\n\n8. https://acadgild.com/blog/decision-tree-python-code\n"},{"metadata":{},"cell_type":"markdown","source":"So, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\t\nYour comments and feedback are most welcome.\n\nThank you\n"},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}