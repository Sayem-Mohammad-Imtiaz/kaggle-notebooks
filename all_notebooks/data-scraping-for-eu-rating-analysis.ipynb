{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Web Scraping\nIn this notebook, I used the carbonculture website in order to scrape building data for all 435 locations reported on this website. This data can be integrated into the Building Data Urban Genome 2 Project as a form of feature engineering in order to improve the accuracy of different models. My goal in this was to improve the performance of a classification model used to predict the energy rating on a scale of A - G for European buildings.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I read in the existing train and test datasets created from performing EDA on the metadata dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/predicting-energy-rating-from-raw-data')\ntrain_data = pd.read_csv('train_rating_eu.csv')\ntest_data = pd.read_csv('test_rating_eu.csv')\n\ntrain_data = train_data.drop(['building_id', 'site_id', 'Unnamed: 0'], axis=1)\ntest_data = test_data.drop(['building_id', 'site_id', 'Unnamed: 0'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the main pages that I will be gathering information from. URL3 is the home page, URL1 is a list of buildings at one of the 12 locations included in this website, and URL2 is a webpage for a specific location","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\n\nURL1 = 'https://platform.carbonculture.net/communities/ucl/30/apps/assets/list/place/'\nURL2 = 'https://platform.carbonculture.net/places/119-torrington-place/1155/'\nURL3 = 'https://platform.carbonculture.net/about/'\npage1 = requests.get(URL1)\npage2 = requests.get(URL2)\npage3 = requests.get(URL3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('/kaggle/working/output1', 'wb')\nf.write(page1.content)\nf = open('/kaggle/working/output2', 'wb')\nf.write(page2.content)\nf = open('/kaggle/working/output3', 'wb')\nf.write(page3.content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\n\nsoup1 = BeautifulSoup(page1.content, 'html.parser')\nsoup2 = BeautifulSoup(page2.content, 'html.parser')\nsoup3 = BeautifulSoup(page3.content, 'html.parser')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following creates a list of urls that correspond to the 12 different locations reported in this website. Each url contains information about all of the buildings reported for each individual location.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"places_elems = soup3.find_all('a', href=True)\nplaces_elems","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nplaces = []\n\nfor place in places_elems:\n    if i > 10 and i < 23:\n        if place.text: \n            places.append(place['href'])\n    i = i + 1\n    \nplaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"urls=[]\nbase = 'https://platform.carbonculture.net'\nend = 'apps/assets/list/place/'\n\nfor p in places:\n    urls.append((base + p + end))\n    \nurls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soups=[]\n\nfor u in urls:\n    p = requests.get(u)\n    soups.append(BeautifulSoup(p.content, 'html.parser'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = []\ntitles_whole = []\n\nfor soup in soups:\n    url_elems = soup.find_all(href=True)\n    for elem in url_elems:\n        if elem.text: \n            if elem['href'].find('places') == 1:\n                titles.append(elem['href'])\n                \nfor t in titles:\n    titles_whole.append(base + t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following gathers data from the above created list of urls. The urls used correspond to the specific buildings and were obtained above by looping through the 12 general location urls and creating a new list containing each individual building url. There are multiple buildings reported for each location. Specifically, it scrapes data pertaining the year built, number of floors, number of occupants, and main heating type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"years = []\nfloors = []\nheating = []\noccupants = []\ni = 0\n\nfor w in titles_whole:\n    test = requests.get(w)\n    soup_test = BeautifulSoup(test.content, 'html.parser')\n    test_elems = soup_test.find_all('li', class_='assets-meta__list-item')\n    if len(test_elems) == 0:\n        print(w)\n        years.append('-')\n        floors.append(-1)\n        heating.append('-')\n        occupants.append(-1)\n    for elem in test_elems:\n        a = str(elem.find('span'))[6:-7]\n        if i == 0:\n            years.append(a)\n        elif i == 1:\n            floors.append(a)\n        elif i == 3:\n            heating.append(a)\n        elif i == 4:\n            occupants.append(a)\n            i = -1\n        i = i + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following gathers data from the table on the general location page (which lists each building reported at that location). From this table I scraped the name of the building, annual energy consumption, annual energy consumption per area, rating, and usable floor area (sqm). The loop runs 12 times (12 locations) and the nested loop runs through each individual building in the table (however many buildings are reported at that location)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nj = 0\ntitle = []\nconsumption = []\nfloor_area = []\nconsumption_area = []\nrating = []\n\nfor soup in soups:\n    url = titles_whole[i]\n    page = requests.get(url)\n    soup_title = BeautifulSoup(page.content, 'html.parser')\n    table_elems = soup.find_all('td')\n    for elem in table_elems:\n        if i != 5 and i != 6 and i != 7:\n            elem = str(elem)\n            elem = elem[4:-5]\n            if i == 0:\n                elem = elem[-11:-8]\n                if elem != 'N/A':\n                    elem = elem[2:]\n                rating.append(elem)\n            if i == 1:\n                title.append(elem)\n            elif i == 2:\n                elem = elem[:-9]\n                if len(elem) == 1:\n                    elem = '-1'\n                elem = elem.replace(',', '')\n                consumption.append(elem)\n            elif i == 3:\n                elem = elem[:-9]\n                if len(elem) == 1:\n                    elem = '-1'\n                elem = elem.replace(',', '')\n                floor_area.append(elem)\n            elif i == 4:\n                elem = elem[:-9]\n                if len(elem) == 1:\n                    elem = '-1'\n                elem = elem.replace(',', '')\n                consumption_area.append(elem)\n            j = j + 1\n        if i == 7:\n            i = -1\n        i = i + 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following block of code changes the 'floors' data into a usable format: getting rid of any unneccessary text and converting any missing values into -1 so that the list can be converted to a float later on.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nnull = occupants[320]\n\nfor o in occupants:\n    if str(o) == null:\n        occupants[i] = -1\n    i = i + 1\n\ni = 0\nnull = floors[33]\nfor f in floors:\n    if str(f) == null:\n        print('here!')\n        floors[i] = -1\n    elif len(str(f)) > 3:\n        seq_type= type(f)\n        f = seq_type().join(filter(seq_type.isdigit, f))\n        floors[i] = f\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting all of the numeric data into float lists:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"consumption = list(map(float, consumption))\nconsumption_area = list(map(float, consumption_area))\nfloor_area = list(map(float, floor_area))\noccupants = list(map(float, occupants))\nfloors = list(map(float, floors))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above scraped data, we can make our new dataframe:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['sqm', 'building', 'energy consumption', 'energy consumption per area', 'year built', 'floors', 'no. occupants', 'main heating type', 'rating2']\ndf = pd.DataFrame(columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sqm'] = floor_area\ndf['building'] = title\ndf['energy consumption'] = consumption\ndf['energy consumption per area'] = consumption_area\ndf['year built'] = years\ndf['floors'] = floors\ndf['no. occupants'] = occupants\ndf['main heating type'] = heating\ndf['rating2'] = rating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sqm'] = df['sqm'].replace(-1, np.nan)\ndf['energy consumption'] = df['energy consumption'].replace(-1, np.nan)\ndf['energy consumption per area'] = df['energy consumption per area'].replace(-1, np.nan)\ndf['rating2'] = df['rating2'].replace('N/A', np.nan)\ndf['floors'] = df['floors'].replace(-1, np.nan)\ndf['no. occupants'] = df['no. occupants'].replace(-1, np.nan)\ndf['year built'] = df['year built'].replace('-', np.nan)\ndf['main heating type'] = df['main heating type'].replace('-', np.nan)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this dataframe, we can split it into a train and test by segmenting the values with and without a rating, respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = df[df['rating2'].notna()]\ntrain1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = df[df['rating2'].isnull()]\ntest1 = test1.drop('rating2', axis=1)\ntest1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I am merging our new train and test dataframes obtained from the above web scraping with our original train and test dataframes from the metadata dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"merge1 = pd.merge(left=train_data, right=train1, how='outer', left_on='sqm', right_on='sqm')\nmerge1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge2 = pd.merge(left=test_data, right=test1, how='outer', left_on='sqm', right_on='sqm')\nmerge2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below analysis of our merge yields the following question: if this dataset supposedly already includes data from this website, why do only 35 of the sqm's match up for the train dataset and 12 for the test dataset?\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sqm1 = train_data['sqm']\nsqm2 = train1['sqm']\nyear1 = train_data['yearbuilt']\nyear2 = train1['year built']\n\ntrain_intersection = list(set(sqm1) & set(sqm2))\nyear_intersection = list(set(year1) & set(year2))\n\n\nprint(\"There are \", len(train_intersection), \" sqm matches!\")\nprint(\"There are \", len(year_intersection), \" year matches!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqm1 = test_data['sqm']\nsqm2 = test1['sqm']\nyear1 = test_data['yearbuilt']\nyear2 = test1['year built']\n\ntrain_intersection = list(set(sqm1) & set(sqm2))\nyear_intersection = list(set(year1) & set(year2))\n\n\nprint(\"There are \", len(train_intersection), \" sqm matches!\")\nprint(\"There are \", len(year_intersection), \" year matches!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}