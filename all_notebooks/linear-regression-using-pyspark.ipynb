{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Installing PySpark package\n\n! pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession\\\n        .builder\\\n        .appName(\"Python Spark regression example\")\\\n        .config(\"spark.some.config.option\", \"some-value\")\\\n        .getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating a spark Dataframe, not to be confused with Pandas Dataframe**\n* format can take values like csv, parquet, json etc..\n* inferSchema - deduces schema from the available data\n* Schema has a StructType that contains many StructFields. Each StructFiels has a name, data type, and Boolean flag to designate whether it accepts null values or not\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf = spark.read.format(\"csv\").\\\n                options(inferSchema = True, header = True).\\\n                load(\"../input/advertisingcsv/Advertising.csv\", header=True)\n\n\nprint(df.show(5, False)) # Shows first five records and does not truncate values\n\nprint(df.printSchema())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe the dataset, the result will be shown for all feature columns\ndf.describe().show()\n\n# Describe the numeriacl statistics for just a signle colums\n\ndf.select(\"TV\").describe().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printitng the column names of the dataframe\ncol_names = df.columns\nprint(col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming the column \"_c0\" to \"Index\" by using withcolumnRenamed method\n\ndf = df.withColumnRenamed(\"_c0\", \"Index\")\n\n# Printing to show the change in column name\ndf.show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Vector can be obtained using different options. Below, we will see three such options to do it\n\n## Creating the feature vector using high level RFormula\n* (~) ---> seperates target labels and individual features\n* (.) ---> All features except the target column\n* (+) ---> Concat the indicated terms\n  \n    \n### We define the formula and call fit & transform on the dataset to yield required features column\n### In the section below, we have separated target term and feature terms. And created a features vector containg values from TV, Newspaper and Radio columns\n### It is mandatory that both the features vector and labels are represeted as double while using Spark ML "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import RFormula\n\nsupervised = RFormula(formula = \"Sales ~  TV + Radio + Newspaper\")\nfittedRF = supervised.fit(df)\npreparedDF = fittedRF.transform(df)\npreparedDF.show(5, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same above transformation can be obtained ina sig\n\ntestDf = RFormula(formula = \"Sales ~  TV + Radio + Newspaper\").fit(df).transform(df)\n\ntestDf.show(5, False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Vector Assembler to create Dense Feature Vector\n\n### It Accepts the name of input columns to be considered for assembling via \"setInputCols\"\n### Gives a features column that is added to the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\n\nva = VectorAssembler()\\\n    .setInputCols([\"TV\",\"Radio\", \"Newspaper\"])\\\n    .setOutputCol(\"va_features\")\n\nva.transform(preparedDF).show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Using RDDs to create feature vector and label"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are defining a functions that takes in a dataframe and converts it to RDD\n# applies a Map function\n# The function passed onto Map function returs All the columns except first & last cloumn to prepare the feature vector\n# and only the last columns (\"Sales\") as the label vector\n\nfrom pyspark.ml.linalg import Vectors\n\ndef vecTransform (data):\n    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])\n\n\n\nvecTransform(df).show(5, False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using StandardScaler to scale the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom pyspark.ml.feature import StandardScaler\n\n\nscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeatures\")\n\nscaledDF = scaler.fit(preparedDF).transform(preparedDF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaledDF.show(5, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preparedDF.show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using VectorIndexer to Index feature Vector\n- Vector indexer takes in a column of Vector and return an indexed Vector column as output\n- This helps in identifying any categorical data present within the feature vector\n- maXCategories help in identifying the categorical that if any present in the feature vector. If the set of values are greater than maxCategories, then the values are treated as continuous\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nfeatureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(preparedDF)\n\ndata = featureIndexer.transform(preparedDF)\n\ndata.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling dataset\n\nscaler = StandardScaler().setInputCol(\"indexedFeatures\").setOutputCol(\"scaledFeatures\")\n\nfinalScaledData = scaler.fit(data).transform(data)\n\nfinalScaledData.show(5, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputData = finalScaledData.select([\"scaledFeatures\", \"label\"])\n\n# inputData = inputData.withColumnRenamed(\"scaledFeatures\", \"features\")\n\ninputData.show(5, False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting a Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting dataset into training and test set\n\ntraining, test = inputData.randomSplit([0.8,0.2])\n\nprint(\"Training set has {} records\".format(training.count()))\nprint(\"Training set has {} records\".format(test.count()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating an instace of LinearRegression class passing on parameters\n\nlin_reg = LinearRegression(featuresCol=\"scaledFeatures\", labelCol = \"label\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n\nprint(lin_reg.explainParams()) # Explains the parameters of the model\n\nmodel = lin_reg.fit(training) # Fitting the Model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('###' * 30)\nprint(\"Coffiecients of the model are: {}\".format(model.coefficients))\nprint('###' * 30)\nprint('###' * 30)\n\nprint(\"Intercept of linear regression equation is: {}\".format(model.intercept))\n\nprint('###' * 30)\nprint('###' * 30)\nsummary = model.summary # creating an instance of model summary\n\nprint('###' * 30)\nprint('###' * 30)\nprint(\"The residual values after the fit are: \")\nsummary.residuals.show() # This returns only the first 20 values\n\nprint('###' * 30)\nprint('###' * 30)\nprint(\"The calculated loss for the objective function at each iteration is: \")\nprint(summary.objectiveHistory)\n\nprint('###' * 30)\nprint('###' * 30)\nprint(\"The RMSE value of the fitted model: {}\".format(summary.rootMeanSquaredError))\n\nprint('###' * 30)\nprint('###' * 30)\nprint(\"The R2 score of the fitted model: {}\".format(summary.r2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.transform(test)\npredictions.show(30, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import RegressionEvaluator\n\nreg_eval = RegressionEvaluator(predictionCol = \"prediction\", labelCol = \"label\", metricName = \"rmse\")\n\nprint(\"The RMSE value on test data: {}\".format(reg_eval.evaluate(predictions)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deifining a ML Pipeline\n\n1. We ingest data and create a Dataframe - df\n2. The dataset is spli into Train and Test set\n3. Instantiate a Vector assemble to create Feature Vector\n4. Use Vector Indexer to automatically index if there are any categorical / discrete values\n5. We scale this Feature Vector created using Vector Indexer\n6. And Finally, instantiate a Linear Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline # Importing Pipeline\n\n\ntrain, test = df.randomSplit([0.8, 0.2]) # splitting datatset into train and test set\nassembler = VectorAssembler(inputCols = [\"TV\", \"Radio\",\"Newspaper\"], outputCol =\"features\")\nindexer = VectorIndexer(inputCol =\"features\", outputCol = \"indexed_features\")\nscaler = StandardScaler(inputCol = \"indexed_features\", outputCol = \"scaledFeatures\")\nlr = LinearRegression(featuresCol = \"scaledFeatures\", labelCol = \"Sales\")\n\npipeline = Pipeline(stages = [assembler, indexer, scaler, lr])\n\nmodel = pipeline.fit(train)\npredictions = model.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.show(10, False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}