{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Objective of the notebook:\n\nTo look at the different open source hyperparameter tuning and automl packages available and to get a quick start with them.\n\n### Introduction\n\nWe will take the telecom customer churn dataset to explore. Let us import the datasets and do some preprocessing. The preprocessing steps are\n1. Converting the `TotalCharges` variable to float from string\n2. Converting the categorical variables to numerical ones through label encoding\n3. Converting the target column `Churn` to 0 and 1. \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf[\"TotalCharges\"] = df[\"TotalCharges\"].apply(lambda x: float(\"0\"+x.strip()))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncat_cols = [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \n            \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \n            \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]\nlbl_enc = LabelEncoder()\nfor col in cat_cols:\n    df[col] = lbl_enc.fit_transform(df[col])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_map_dict = {\"Yes\":1, \"No\":0}\ndf[\"Churn\"] = df[\"Churn\"].map(churn_map_dict)\ndf[\"Churn\"].value_counts()\n\ncols_to_use = [col for col in df.columns if col not in [\"customerID\", \"Churn\"]]\nX = df[cols_to_use].values\ny = df[\"Churn\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the dataset to get train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Optimization"},{"metadata":{},"cell_type":"markdown","source":"Github - https://github.com/fmfn/BayesianOptimization\n\nPure Python implementation of bayesian global optimization with gaussian processes.\n\nLet us use the dataset prepared above to build a Random Forest classifier model. Bayesian Optimization is used to find the best parameters for the Random Forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier as RFC\n\n# function for cross validation\ndef rfc_cv(n_estimators, min_samples_split, max_features, data, targets):\n    estimator = RFC(\n        n_estimators=n_estimators,\n        min_samples_split=min_samples_split,\n        max_features=max_features,\n        random_state=2\n    )\n    cval = cross_val_score(estimator, data, targets,\n                           scoring='neg_log_loss', cv=4)\n    return cval.mean()\n\n# optimization function\ndef optimize_rfc(data, targets):\n    def rfc_crossval(n_estimators, min_samples_split, max_features):\n        return rfc_cv(\n            n_estimators=int(n_estimators),\n            min_samples_split=int(min_samples_split),\n            max_features=max(min(max_features, 0.999), 1e-3),\n            data=data,\n            targets=targets,\n        )\n\n    # evaluation space\n    optimizer = BayesianOptimization(\n        f=rfc_crossval,\n        pbounds={\n            \"n_estimators\": (10, 250),\n            \"min_samples_split\": (2, 25),\n            \"max_features\": (0.1, 0.999),\n        },\n        random_state=1234,\n        verbose=2\n    )\n    optimizer.maximize(n_iter=10)\n\n    print(\"Final result:\", optimizer.max)\n    \n    return optimizer\n    \nresult = optimize_rfc(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(result.max)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HyperOpt"},{"metadata":{},"cell_type":"markdown","source":"Github - https://github.com/hyperopt/hyperopt\n\nLet us use the hyperopt package to get the best set of parameters now. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, tpe, STATUS_OK, Trials\nfrom hyperopt.fmin import fmin\n\nspace ={\n    'n_estimators': hp.choice('n_estimators', np.arange(10, 250, dtype=int)),\n    'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 25, dtype=int)),\n    'max_features': hp.quniform ('max_features', 0.1, 0.99, 0.02)\n    }\n\ndef rfc_cv(space, data=train_X, targets=train_y):\n    estimator = RFC(\n        n_estimators=space[\"n_estimators\"],\n        min_samples_split=space[\"min_samples_split\"],\n        max_features=space[\"max_features\"],\n        random_state=2\n    )\n    cval = cross_val_score(estimator, data, targets,\n                           scoring='neg_log_loss', cv=4)\n    return {'loss':-cval.mean(), 'status': STATUS_OK }\n\ntrials = Trials()\nbest = fmin(fn=rfc_cv,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5, # change\n            trials=trials)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### H2O AutoML\n\nDoc link: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\n\nNow let us look at some of the AutoML packages where both the models and the best hyperparameters are chosen automatically."},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nh2o.init()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a H2OFrame for both train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o_train = h2o.H2OFrame(train_X).cbind(h2o.H2OFrame(train_y))\nh2o_test = h2o.H2OFrame(test_X).cbind(h2o.H2OFrame(test_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Code flow to build a GBM model in H2O"},{"metadata":{"trusted":true},"cell_type":"code","source":"from h2o.estimators.gbm import H2OGradientBoostingEstimator\n\npredictors = h2o_train.columns[:-1]\nresponse = \"C110\"\nh2o_train[response] = h2o_train[response].asfactor()\n\ngbm = H2OGradientBoostingEstimator()\ngbm.train(x=predictors, y=response, training_frame=h2o_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now call the AutoML function in H2O to get the best model for this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from h2o.automl import H2OAutoML\n\naml = H2OAutoML(max_models = 10, \n                max_runtime_secs=100, \n                seed = 1,\n                stopping_metric = \"logloss\",\n                nfolds=4\n               )\naml.train(x=predictors, y=response, training_frame=h2o_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the leaderboard output from the AutoML function. We can see that in addition to individual models, it also builds stacking models. Infact, these stacked models gave good results for this dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"aml.leaderboard","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can check the metrics for the best model in both train and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"aml.leader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Auto Sklearn"},{"metadata":{},"cell_type":"markdown","source":"Github - https://automl.github.io/auto-sklearn/master/\n\nHad some trouble installing this package in kaggle kernels but below is simple code example to run the automl in local machine."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import autosklearn.classification\n\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=100,\n    per_run_time_limit=30,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 4}\n)\nautoml.fit(train_X.copy(), train_y.copy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TPOT"},{"metadata":{},"cell_type":"markdown","source":"Link - https://epistasislab.github.io/tpot/\n\nTPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Let us use TPOT now to get the best pipeline. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tpot import TPOTClassifier\n\nautoml = TPOTClassifier(max_time_mins=1,\n                        max_eval_time_mins=0.5, \n                        scoring='neg_log_loss',\n                        cv=4,\n                        random_state=2020,\n                        verbosity=1\n                       )\nautoml.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best pipeline that comes out of TPOT can be seen using the following command."},{"metadata":{"trusted":true},"cell_type":"code","source":"automl.fitted_pipeline_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Work in progress. Please stay tuned**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}