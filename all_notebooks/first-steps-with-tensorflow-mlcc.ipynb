{"cells":[{"metadata":{"_uuid":"97a1f76f423316ceed2ba102cb67e2198a8e79bf","_cell_guid":"1cfd6248-6ac2-4173-af8d-9ed98519cf69"},"cell_type":"markdown","source":"**First steps with TensorFlow**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# load necessary libraries\nimport math\n\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data from Kaggle datasets (import data using the Data tab, search for california housing data 1990)\ncalifornia_housing_dataframe = pd.read_csv('../input/california_housing_train.csv', sep=',')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"a07283a5ecd65f5366b9fcdc6a868624a4521122","_cell_guid":"d7f14df3-3fdd-4cf1-8599-b4433f7dd7ae","trusted":true},"cell_type":"code","source":"# randomize data to remove any pre-ordering\ncalifornia_housing_dataframe = california_housing_dataframe.reindex(np.random.permutation(california_housing_dataframe.index))\n# scale median house price to be in thousands for easy computation of learning rates\ncalifornia_housing_dataframe['median_house_value'] /= 1000.0\ncalifornia_housing_dataframe","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"9d55710ab147cf13af38ed7a1359450a81278b5d","_cell_guid":"40911baf-43ce-4e12-8583-09d389f5bf93","trusted":true},"cell_type":"code","source":"# examine the data\ncalifornia_housing_dataframe.describe()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"7fc9f2cfda2ea64c8bbcca7f2a4a7a38d7ba9c93","_cell_guid":"3b09842e-243f-47e2-a18a-227083e86e15"},"cell_type":"markdown","source":"Build first model: Using `LinearRegressor` and  `GradientDescentOptimizer` by TensorFlow's Estimator API. \n\nMajorly two types of data\n- categorical\n- numerical\n\nFor TensorFlow we have to describe a feature column which is a description of feature data, not the data itself.\nWe will start with `total_rooms` as the `numeric_column` type feature column."},{"metadata":{"_uuid":"1fbb9b144641ae2ac21fa3fd569dea53454b47fc","collapsed":true,"_cell_guid":"5e94bd62-9f2f-4dc6-a093-7bc2100b64de","trusted":true},"cell_type":"code","source":"# define the input feature: total_rooms\nmy_feature = california_housing_dataframe[['total_rooms']]\n\n# configure a numeric feature column for total_rooms\nfeature_columns = [tf.feature_column.numeric_column('total_rooms')]\n\n# since column shape is 1-d which is the default shape for numeric_column, we don't have to explicitly provide that as an argument","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"21569ac2568651feb4803f4a4fa6b83db1dd8dbd","collapsed":true,"_cell_guid":"0e7dc5cb-ceff-44e8-8a53-466ca547e55f","trusted":true},"cell_type":"code","source":"# define the taget/label\ntargets = california_housing_dataframe['median_house_value']","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"abef59ba27ae618c95980c6072c3a4e81af3c566","collapsed":true,"_cell_guid":"60c59fe3-496a-4e1c-8196-215550f02b5a","trusted":true},"cell_type":"code","source":"# Use gradient descent as the optimizer for training the model\nmy_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n\n# configure the linear regression model with our feature columns and optimizer\n# set a learning rate of 0.0000001 for gradient descent\nlinear_regressor = tf.estimator.LinearRegressor(\n    feature_columns=feature_columns,\n    optimizer=my_optimizer\n)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"45fd15977cc89f4e8e4e4578bf4a2900d774a0f8","_cell_guid":"739fb1fd-8fd7-4c2b-aba3-02dd36af891e"},"cell_type":"markdown","source":"Model is ready, but we need to pass data through it for training. We will define an input function which will instruct TensorFlow to preprocess the data as well as how to batch, shuffle and repeat it during model training\n\nFirst we convert our pandas feature data into a dict of NumPy arrays. Then we can use the TensorFlow Dataset API to construct a dataset object from our data, and then break our data into batches of `data_size` to be repeated for the specified number of epochs (`num_epochs`).\nNOTE: if `num_epochs=None` is passed to `repeat()`, the input data with be repeated indefinitely\n\n`shuffle=True`: shuffle the data so that it's passed to the model randomly\n`buffer_size=X`: size of the dataset from which shuffle will randomly sample\n\nFinally use an iterator for the dataset and return th next batch of data to the LinearRegressor.\n"},{"metadata":{"_uuid":"fd60577e97e1b476874ae9790d0d5fa14d392a55","collapsed":true,"_cell_guid":"22193c55-c0fa-4774-bc26-ded5f6fd3507","trusted":true},"cell_type":"code","source":"def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    \"\"\"Trains a linear regression model of one feature\n    Args:\n        features: pandas DataFrame of features\n        targets: pandas DataFrame of targets\n        batch_size: size of batches to be passed to the model\n        shuffle: True or False. Whether to shuffle the data\n        num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n        Tuple of (features, labels) for next data batch\n    \"\"\"\n    \n    # convert pandas data into a dict of np arrays\n    features = {key: np.array(value) for key, value in dict(features).items()}\n    \n    # construct a dataset and configure batching/repeating\n    ds = Dataset.from_tensor_slices((features, targets)) # beware of limits\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # shuffle the data, if specified\n    if shuffle:\n        ds = ds.shuffle(buffer_size=10000)\n        \n    # returns the next batch of data\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n    ","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"596dccdb211f3e125a6bf37143cc85d8066450d0","collapsed":true,"_cell_guid":"0a822696-28d6-4ee2-9a31-1dbe56b291d7","trusted":true},"cell_type":"code","source":"# train the model\n_ = linear_regressor.train(\n    input_fn = lambda: my_input_fn(my_feature, targets),\n    steps=100\n)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"35882f3a9871a5d86ce697f46aa564079b38dd80","_cell_guid":"865b75ba-bd7a-48a6-bcc2-477f7480ce94","trusted":true},"cell_type":"code","source":"# evaluate the model\n# training error measures how well model fits training data, not how well it generalizes to the new data.\n\n# create an input function for predictions\n# note: since we are making just one prediction for each example, we don't need to repeat or shuffle the data here\nprediction_input_fn = lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n\n# call predict() on the linear_regressor to make predictions\npredictions = linear_regressor.predict(input_fn=prediction_input_fn)\n\n# format predictions as NumPy array, so we can calculate error metrics\npredictions = np.array([item['predictions'][0] for item in predictions])\n\n# print Mean Squared Error and Root Mean Squared Error\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nroot_mean_squared_error = math.sqrt(mean_squared_error)\nprint(\"Mean squared error (on training data): %0.3f\" % mean_squared_error)\nprint(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"6d62ef23c31d9be47dd250a05e74a7b2b952f0f5","_cell_guid":"d5f09dc1-87c1-4d46-91bd-2ceabf992313","trusted":true},"cell_type":"code","source":"# Mean Squared Error (MSE) is hard to interpret, so we look at Root Mean Squared Error (RMSE) instead since it can be interpreted on the same scale as original targets\n\nmin_house_value = california_housing_dataframe['median_house_value'].min()\nmax_house_value = california_housing_dataframe['median_house_value'].max()\nmin_max_difference = max_house_value - min_house_value\n\nprint(\"Min. Median House Value: %0.3f\" % min_house_value)\nprint(\"Max. Median House Value: %0.3f\" % max_house_value)\nprint(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\nprint(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)\n","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"84f4f642b6f9baacc0f9807c86b9cf47ec249f9f","_cell_guid":"8b9deb66-7aaf-4469-9eb6-9816d825545b"},"cell_type":"markdown","source":"Our models spans nearly half the range of target values. Can we do better? This is the question that nags every model developer. Here are some basic strategies to reduce model error"},{"metadata":{"_uuid":"7aa32f2558f68172511be9763e2498283f4a4aa3","_cell_guid":"13bc7e17-a84b-4a1e-bc5e-5c0d9215ec42","trusted":true},"cell_type":"code","source":"# first step is to check how well our predictions match our targets, in terms of overall summary statistics\ncalibration_data = pd.DataFrame()\ncalibration_data['predictions'] = pd.Series(predictions)\ncalibration_data['targets'] = pd.Series(targets)\ncalibration_data.describe()","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"67280d99480e2339937cb51ba400d51ed04d0d95","collapsed":true,"_cell_guid":"aebc34d0-c5dd-498b-8546-ffdeabf8cc7d","trusted":true},"cell_type":"code","source":"# take a uniform random sample to visualize the line that we have learned (since model is linear)\nsample = california_housing_dataframe.sample(n=300)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"e74921c0f396194fd85df542ff658aecc6be2c4c","_cell_guid":"296627a1-f898-4d2a-8aaf-ca6bec787ba7","trusted":true},"cell_type":"code","source":"# Plot the line, drawing from model's bias term and feature weight together with scatter plot\n\n# get the min and max total_room values\nx_0 = sample['total_rooms'].min()\nx_1 = sample['total_rooms'].max()\n\n# retrieve the final weight and bias generated during training\nweight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]\nbias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n\n# get the predicted median_house_Values for the min and max total_rooms values\ny_0 = weight * x_0 + bias\ny_1 = weight * x_1 + bias\n\n# plot our regression line from (x_0, y_0) to (x_1, y_1)\nplt.plot([x_0, x_1], [y_0, y_1], c='r')\n\n# label the graph axes\nplt.ylabel('median_house_value')\nplt.xlabel('total_rooms')\n\n# plot a scatter plot from our data sample\nplt.scatter(sample['total_rooms'], sample['median_house_value'])\n\n# display graph\nplt.show()","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"3b7dff7f833d8c185b5058096abb90105cae9354","_cell_guid":"434fa8af-fd30-488c-b07b-1c5d69cbda8b"},"cell_type":"markdown","source":"> Wayyyy off!!"},{"metadata":{"_uuid":"7ef653b1da652f22084ee2e3254538c36606c360","_cell_guid":"5ce50532-1d3f-4467-8585-b8a65a66e1fc"},"cell_type":"markdown","source":"**Tweak the model hyperparameters**\n\n10 iterations, plotting loss at each iteration to understand convergence. Also plotting feature weight and bias term values learned by model can be used to understand convergence."},{"metadata":{"_uuid":"9e8ac6f5fa0c3542616f8429443150195017e728","collapsed":true,"_cell_guid":"e4c349d9-cf27-4158-8058-31c414182de5","trusted":true},"cell_type":"code","source":"def train_model(learning_rate, steps, batch_size, input_feature=\"total_rooms\"):\n    \"\"\"Trains a linear regression model of one feature.\n    \n    Args:\n        learning_rate: A `float`, the learning rate\n        steps: A non-zero `int`, the total number of training steps. A training step consists of a forward and backward pass using a single batch\n        batch_size: A non-zero `int`, the batch size\n        input_feature: A `string` specifying a column from `california_housing_dataframe` to use as input feature\n    \"\"\"\n    periods = 10\n    steps_per_period = steps/periods\n    \n    my_feature = input_feature\n    my_feature_data = california_housing_dataframe[[my_feature]]\n    my_label = 'median_house_value'\n    targets = california_housing_dataframe[my_label]\n    \n    # create feature columns\n    feature_columns = [tf.feature_column.numeric_column(my_feature)]\n    \n    # create input function\n    training_input_fn = lambda: my_input_fn(my_feature_data, targets, batch_size=batch_size)\n    prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n    \n    # create a linear regressor object\n    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n    linear_regressor = tf.estimator.LinearRegressor(\n        feature_columns=feature_columns,\n        optimizer=my_optimizer\n    )\n    \n    # set up to plot the state of out model's line each period\n    plt.figure(figsize=(15, 6))\n    plt.subplot(1, 2, 1)\n    plt.title('Learned line by period')\n    plt.ylabel(my_label)\n    plt.xlabel(my_feature)\n    sample = california_housing_dataframe.sample(n=300)\n    plt.scatter(sample[my_feature], sample[my_label])\n    colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n    \n    # train the model, but do so inside a loop so tha we can periodically asses loss metrics\n    print('training model...')\n    print('RMSE (on training data):')\n    root_mean_squared_errors = []\n    for period in range(0, periods):\n        # train the model, starting from the prior state\n        linear_regressor.train(\n            input_fn=training_input_fn,\n            steps=steps_per_period\n        )\n\n        # take a break and compute predictions\n        predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n        predictions = np.array([item['predictions'][0] for item in predictions])\n        \n        # compute loss\n        root_mean_squared_error = math.sqrt(metrics.mean_squared_error(predictions, targets))\n        \n        # occasionally print the current loss\n        print(' period %02d: %0.2f' % (period, root_mean_squared_error))\n        \n        # add the loss metrics from this period to our list\n        root_mean_squared_errors.append(root_mean_squared_error)\n        \n        # finally, track the weights and biases over time. Apply some math to ensure that the data and line are plotted neatly\n        y_extents = np.array([0, sample[my_label].max()])\n        \n        weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n        bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n        \n        x_extents = (y_extents - bias) / weight\n        x_extents = np.maximum(np.minimum(x_extents, sample[my_feature].max()), sample[my_feature].min())\n        y_extents = weight * x_extents + bias\n        plt.plot(x_extents, y_extents, color=colors[period])\n        \n    print('Model training finished')\n    \n    # output a graph of loss metrics over periods\n    plt.subplot(1, 2, 2)\n    plt.ylabel('RMSE')\n    plt.xlabel('Periods')\n    plt.title('Root Mean Squared Error vs Periods')\n    plt.tight_layout()\n    plt.plot(root_mean_squared_errors)\n    \n    # output a table with calibration data\n    calibration_data = pd.DataFrame()\n    calibration_data['predictions'] = pd.Series(predictions)\n    calibration_data['targets'] = pd.Series(targets)\n    display.display(calibration_data.describe())\n    \n    print('Final RMSE (on training data): %0.2f' % root_mean_squared_error)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"40f3169fa016177e755ad9acd74951522c79435e","_cell_guid":"fb48150d-1cb3-4b97-8d19-fe5c6b7456d8"},"cell_type":"markdown","source":"**Achieve an RMSE of 180 or below**"},{"metadata":{"_uuid":"490231c4aaddecc8cfee9591b28553ce64911cda","_cell_guid":"e809eb48-ea2c-4250-89b9-2b3043a10ca5","trusted":true},"cell_type":"code","source":"train_model(learning_rate=0.00001, steps=100, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c421d114ab8aeded78c023592eb9d55c9f74972","_cell_guid":"1a590c27-acf5-416c-a9b3-b4808c348ae8","trusted":true},"cell_type":"code","source":"train_model(learning_rate=0.001, steps=100, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aa1e4ea0176ae9c0f31122d5cf517c101adb850","collapsed":true,"_cell_guid":"f744793e-cebd-41a4-9aa5-4434a327a2ad","trusted":true},"cell_type":"code","source":"train_model(learning_rate=0.1, steps=100, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c3eb33e975b0ac0be33c3d2a3b53f095d2bff23","collapsed":true,"_cell_guid":"9145d00e-341b-4e6c-aa56-debb876d2e91","trusted":true},"cell_type":"code","source":"train_model(learning_rate=0.0001, steps=300, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"877002f8e3c149f3f582542561e5006825da1e7f","collapsed":true,"_cell_guid":"21c98444-aab7-4665-aad5-58378f6b484f","trusted":true},"cell_type":"code","source":"train_model(learning_rate=0.00002, steps=500, batch_size=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dd734ed92a0d64eaf0e3846b7caabc328044667","_cell_guid":"23855831-b4df-41be-93dd-cd80f34a1e18"},"cell_type":"markdown","source":"Standard Heuristic for Model Tuning?\n\nEffect of different hyperparameters are data dependent. No hard-and-fast rules, just rules of thumb for guidance\n\n- Training error should steadily decrease, steeply at first, and should eventually plateau as training converges\n- If the training has not converged, try running it for longer\n- If the training error decreases too slowly, increasing the learning rate may help it decrease faster\n    - But sometimes the exact opposite may happen if the learning rate is too high\n- If the training error varies wildly, try decreasing the learning rate\n     - Lower learning rate plus large number of steps or larger batch size is often a good combination\n- Very small batch sizes can also cause instability. First try larger values like 100 or 1000, and decrease until you see degradation.\n\nNever go strictly by these rules of thumb since the effect is data dependent. Always expriment and verify."},{"metadata":{"_uuid":"c208e5f3f5c329ad52fb2dc36c71711385769247","_cell_guid":"627e95c4-29d6-4022-8d81-cf809f370db8"},"cell_type":"markdown","source":"**Try a different feature**"},{"metadata":{"_uuid":"523d4592c88d290d1273fdbe85eeec9af03da75c","collapsed":true,"_cell_guid":"aff136cd-f180-43e8-bac0-3a690502c843","trusted":true},"cell_type":"code","source":"train_model(learning_rate=0.00002, steps=500, batch_size=5, input_feature='housing_median_age')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c0b353d37d86fc399b273975a0b5163b9d74f07","collapsed":true,"_cell_guid":"d676f8e9-1a8a-4187-9395-9f2eeeaa78fb","trusted":true},"cell_type":"code","source":"train_model(learning_rate=0.00002, steps=1000, batch_size=5, input_feature='population')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ce38d340357351fae8689072c991898946f53c9","collapsed":true,"_cell_guid":"0e3b11aa-cdb2-4de2-9f63-b7ab4faca4ed","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}