{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Trying a statistical approach to philosophy :-)\n\n## Table of Contents\n* [Categorical Features](#1)\n* [Numerical Features](#2)\n* [Evaluate by School and Author](#3)\n* [Wordclouds by School](#4)\n* [Word2Vec - Word Embeddings](#5)\n* [Visualize Word Embeddings using UMAP](#6)\n* [GBM model based on word embeddings](#7)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# PACKAGES\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\n\n# plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\n# NLP\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# H2O\nimport h2o\nfrom h2o.estimators import H2OWord2vecEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator\n\n# UMAP\nimport umap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\nt1 = time.time()\ndf = pd.read_csv('../input/history-of-philosophy/phil_nlp.csv')\nt2 = time.time()\nprint('Elapsed time: ', np.round(t2-t1,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# structure of data\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add derived features\ndf['n_tokens'] = list(map(len,map(eval,df.tokenized_txt)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n# Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical features\nfeatures_cat = ['title', 'author', 'school']\n\n# plot distributions\nfor f in features_cat:\n    plt.figure(figsize=(14,5))\n    df[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2'></a>\n# Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentence length\nprint(df.sentence_length.describe())\n\nplt.figure(figsize=(12,5))\ndf.sentence_length.plot(kind='hist', bins=200)\nplt.title('Sentence Length')\nplt.grid()\nplt.show()\n\nplt.figure(figsize=(12,5))\nnp.log10(df.sentence_length).plot(kind='hist', bins=50)\nplt.title('log10(Sentence Length)')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of tokens\nprint(df.n_tokens.describe())\n\nplt.figure(figsize=(12,5))\ndf.n_tokens.plot(kind='hist', bins=200)\nplt.title('Number of Tokens')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By School:"},{"metadata":{"trusted":true},"cell_type":"code","source":"schools = df.school.unique().tolist()\nprint(schools)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sentence length split by school\nplt.figure(figsize=(16,5))\nsns.violinplot(x='school', y='sentence_length', data=df)\nplt.title('Sentence Length - By School')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot number of tokens split by school\nplt.figure(figsize=(16,5))\nsns.violinplot(x='school', y='n_tokens', data=df)\nplt.title('Number of Tokens - By School')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a>\n# Evaluate by School and Author"},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean of numerical features\ndf.groupby(by=['school','author']).mean().round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentence count (wrap in DataFrame to get nicer display)\npd.DataFrame( df.groupby(by=['school','author'])['title'].count() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drilldown further to title level (wrap in DataFrame to get nicer display)\npd.DataFrame( df.groupby(by=['school','author','title'])['title'].count() )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a>\n# Wordclouds by School"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = time.time()\nfor sc in schools:\n    df_temp = df[df.school==sc]\n    \n    print('School = ', sc.upper(), ':')\n    \n    # render wordcloud\n    text = \" \".join(txt for txt in df_temp.sentence_lowered)\n    wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=500,\n                          width = 600, height = 400,\n                          background_color=\"white\").generate(text)\n    plt.figure(figsize=(12,8))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\nt2 = time.time()\nprint('Elapsed time: ', np.round(t2-t1,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n# Word2Vec - Word Embeddings\n#### Using code from: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/word2vec.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# start H2O\nh2o.init()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# utility function for tokenization\ndef tokenize(sentences, stop_word = stopwords): # use stop words from wordcloud package\n    tokenized = sentences.tokenize(\"\\\\W+\")\n    tokenized_lower = tokenized.tolower()\n    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(stop_word)),:]\n    return tokenized_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# upload data to H2O environment\ntext_h2o = h2o.H2OFrame(df[['school','sentence_lowered']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenize text\nt1 = time.time()\nwords = tokenize(text_h2o['sentence_lowered'])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train Word2Vec model\nrandom.seed(1234)\n\nt1 = time.time()\nw2v_model = H2OWord2vecEstimator(vec_size = 50,\n                                 window_size = 5,\n                                 sent_sample_rate = 0.001,\n                                 init_learning_rate = 0.025,\n                                 epochs = 10)\nw2v_model.train(training_frame=words)\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model\nw2v_model.find_synonyms('knowledge', count = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create vector representation for each sentence (as average of the word vectors)\ntext_vec = w2v_model.transform(words, aggregate_method = 'AVERAGE')\n# and add target 'school' to vectors\ntext_vec = text_vec.cbind(text_h2o['school'])\ntext_vec.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vector features (columns w/o the label \"school\")\nfeatures = text_vec.columns\nfeatures.remove('school')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6'></a>\n# Visualize Word Embeddings using UMAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert H2O frame to Pandas data frame\ndf_text_vec = text_vec.as_data_frame();\n\n# drop rows with missing values\ndf_text_vec = df_text_vec.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first make a simple visualization: boxplot for each column\nplt.figure(figsize=(18,6))\ndf_text_vec[features].boxplot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use subset only (for performance and clarity of plot)\ndf_text_vec = df_text_vec.sample(25000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run UMAP algorithm to get a low dimensional (in our case 2D) representation\ndim_reducer = umap.UMAP(random_state=111)\n\nt1 = time.time()\ntext_vec_umap = dim_reducer.fit_transform(df_text_vec[features])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')\n\n# convert result matrix to data frame\ndf_text_vec_umap = pd.DataFrame(text_vec_umap, columns=['x','y'])\n# and add school again\ndf_text_vec_umap['school'] = df_text_vec.school.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show result\ndf_text_vec_umap.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now plot\nplt.figure(figsize=(12,10))\nsns.scatterplot(data=df_text_vec_umap, x='x', y='y', \n                hue='school', alpha=0.5, s=10) # adjust marker size => avoid overplotting\nplt.legend(loc='upper right')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7'></a>\n# GBM model based on word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train/test split\nrandom.seed(1234)\nperc_train = 0.7\ndata_split = text_vec.split_frame(ratios=[perc_train]) # => data_split[0]:training, data_split[1]:validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# export to file - for potential external processing\nh2o.export_file(data_split[0], 'df_train.csv')\nh2o.export_file(data_split[1], 'df_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define gradient boosting model\nn_CV = 5 # number of cross validations\nfit_1 = H2OGradientBoostingEstimator(ntrees=200,\n                                     max_depth=4,\n                                     col_sample_rate=0.5,\n                                     min_rows=10,\n                                     nfolds=n_CV,\n                                     seed=999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nt1 = time.time()\nfit_1.train(x = features,\n                y = 'school',\n                training_frame = data_split[0],\n                validation_frame = data_split[1])\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2), 'secs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show scoring history - training vs cross validation\nfor i in range(n_CV):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_classification_error, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_classification_error, \n                c='darkorange', label='validation')\n    plt.ylim(0,1)\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# variable importance\nfit_1.varimp_plot(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate on validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions on validation set\npred_valid = fit_1.predict(data_split[1])\npred_valid = pred_valid.as_data_frame() # back to pandas\n\n# show preview\npred_valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# actual values on validation set\nactuals = data_split[1]['school'].as_data_frame()\nactuals = actuals.school","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate confusion matrix\nconf_valid = pd.crosstab(pred_valid.predict, actuals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize confusion matrix (validation set)\nplt.figure(figsize=(10,8))\nsns.heatmap(data=conf_valid, annot=True, fmt='g', cmap='Blues')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}