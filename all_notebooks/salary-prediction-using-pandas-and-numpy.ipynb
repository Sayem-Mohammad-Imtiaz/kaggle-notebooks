{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task 0 : Load libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt \nplt.style.use('ggplot')\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 1 : Load Data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/salary-data-simple-linear-regression/Salary_Data.csv')\ndata.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 2 : Visualize Salary Data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x='YearsExperience', y='Salary', data=data)\nax.set_title(\"Salary according to YearsExperience\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 3 : Compute Cost function $J(\\theta)$","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The objective of linear regression is to minimize the cost function\n\n$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)} )^2$$\n\nwhere $h_{\\theta}(x)$ is the hypothesis and given by the linear model\n\n$$h_{\\theta}(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1$$\n\n## Computing : ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost_function(x, y, theta ) : \n    m = len(y)\n    y_pred = x.dot(theta)\n    \n    fn =  ( y_pred - y )**2\n    \n    cost = 1 / ( (2*m)* np.sum(fn))\n    \n    return cost\n\nm = data.YearsExperience.values.size \nx = np.append(np.ones((m,1)), data.YearsExperience.values.reshape(m, 1), axis=1) \ny = data.Salary.values.reshape(m,1)\ntheta = np.zeros((2,1))\n\ncost_function(x,y,theta)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 3 : Computing The Batch Gradient Descent :\n\nMinimize the cost function $J(\\theta)$ by updating the below equation and repeat unitil convergence\n        \n$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update $\\theta_j$ for all $j$).\n\n### Defining the gradient descent function ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(x, y, theta, alpha, iters) :\n    m = len(y)\n    # tracking the history of all Costs in costs list\n    costs = []\n    for i in range(iters) :\n        y_pred = x.dot(theta)\n        fn = np.dot(x.T, (y_pred - y))\n        theta -= alpha*1/m*np.sum(fn)\n        costs.append(cost_function(x,y,theta))\n    return theta,costs\n\ntheta, costs = gradient_descent(x, y, theta, alpha=0.01, iters=1000)\n\nprint(\"h(x) = {} + {}x1\".format(str(round(theta[0, 0], 2)),\n                                str(round(theta[1, 0], 2))))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 4 : Visualizing cost function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ntheta_0 = np.linspace(-20,20,1000)\ntheta_1 = np.linspace(-1,4,1000)\n\ncost_values = np.zeros((len(theta_0), len(theta_1)))\n\nfor i in range(len(theta_0)):\n    for j in range(len(theta_1)):\n        t = np.array([theta_0[i], theta_1[j]])\n        cost_values[i, j] = cost_function(x, y, t)\n        \nfig = plt.figure(figsize = (12, 8))\nax = fig.gca(projection = '3d')\n\nsurf = ax.plot_surface(theta_0, theta_1, cost_values, cmap = \"viridis\", linewidth = 0.2)\nfig.colorbar(surf, shrink=0.5, aspect=5)\n\nplt.xlabel(\"$\\Theta_0$\")\nplt.ylabel(\"$\\Theta_1$\")\nax.set_zlabel(\"$J(\\Theta)$\")\nax.set_title(\"Cost Surface\")\nax.view_init(30,330)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 5 : Plotting Convergence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(costs)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"$J(\\Theta)$\")\nplt.title(\"Values of Cost Function over iterations of Gradient Descent\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 6: Training Data with Linear Regression Fit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"theta = np.squeeze(theta)\nsns.scatterplot(x = \"YearsExperience\", y= \"Salary\", data = data)\n\nx_value=[x for x in range(1, 12)]\ny_value=[(x * theta[1] + theta[0]) for x in x_value]\nsns.lineplot(x_value,y_value)\n\nplt.xlabel(\"Years Experience\")\nplt.ylabel(\"Salary\")\nplt.title(\"Linear Regression Fit\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 7 : Final Prediction\n\n$h_\\theta(x) = \\theta^Tx$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(x, theta):\n    y_pred = np.dot(theta.transpose(), x)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = 8.7\ny_pred_1 = predict(np.array([1, x1]),theta) \n\nprint(\"Salary [ for experience {x1} ] = \".format(x1=x1) + str(round(y_pred_1, 5)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}