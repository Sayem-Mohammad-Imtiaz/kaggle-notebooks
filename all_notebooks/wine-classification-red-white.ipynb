{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libraries\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler as ss\n\n# Dimensionality Reduction\nfrom sklearn.decomposition import PCA\n\n# Data Splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# For modelling\nfrom xgboost.sklearn import XGBClassifier\n\n# For model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n# For model evaluation\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import confusion_matrix\n\n# For plotting\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nimport seaborn as sns\n\n# For Bayes Optimization\nfrom sklearn.model_selection import cross_val_score\n\nfrom bayes_opt import BayesianOptimization\n\n# For finding feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Miscellaneous\nimport time\nimport os\nimport gc\nimport random\nfrom scipy.stats import uniform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set option to dislay many rows\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set file directory\nos.chdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data has 6497 rows \ntr_f = \"winequalityN.csv\"\n\n# Total number of lines and lines to read:\ntotal_lines = 6497\nnum_lines = 6487\n\n# Read randomly 'p' fraction of files\np = num_lines/total_lines  # fraction of lines to read (99% approximately)\np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick up random rows from hard-disk\ndata = pd.read_csv(\n         tr_f,\n         header=0,   \n         skiprows=lambda i: (i>0) and (random.random() > p)\n         )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore data\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting the rows with null values\ndata.dropna(axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = data.quality, data=data, hue='type', palette=\"rocket\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"ticks\")\ndef hide_current_axis(*args, **kwds):\n    plt.gca().set_visible(False)\n\np = sns.pairplot(data, vars = ['fixed acidity','free sulfur dioxide', 'total sulfur dioxide', 'volatile acidity', 'residual sugar','chlorides','density','citric acid'], diag_kind = 'kde', \n             hue='type',\n             height = 4,\n             palette=\"rocket\")\np.map_upper(hide_current_axis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,14))\nsns.heatmap(data.iloc[:,0:13].corr(), cbar = True,  square = True, annot=True, cmap= 'BuGn_r')\n\n#Free sulfur dioxide and total sulfar dioxide, and Density and alcohol are the most correlated features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(22,10))\nfeatures = [\"total sulfur dioxide\", \"residual sugar\", \"volatile acidity\", \"total sulfur dioxide\", \"chlorides\", \"fixed acidity\", \"density\",\"sulphates\"]\n\nfor i in range(8):\n    ax1 = fig.add_subplot(2,4,i+1)\n    sns.boxplot(x=\"type\", y=features[i],data=data, palette=\"rocket\");\n    \n# Fixed Acidity: acid that contributes to the conservation of wine.\n# Volatile Acidity: Amount of acetic acid in wine at high levels can lead to an unpleasant taste of vinegar.\n# Citric Acid: found in small amounts, can add “freshness” and flavor to wines.\n# Residual sugar: amount of sugar remaining after the end of the fermentation.\n# Chlorides: amount of salt in wine.\n# Free Sulfur Dioxide: it prevents the increase of microbes and the oxidation of the wine.\n# Total Sulfur Dioxide: it shows the aroma and taste of the wine.\n# Density: density of water, depends on the percentage of alcohol and amount of sugar.\n# pH: describes how acid or basic a wine is on a scale of 0 to 14.\n# Sulfates: additive that acts as antimocrobian and antioxidant.\n# Alcohol: percentage of alcohol present in the wine.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(24,10))\nfeatures = [\"total sulfur dioxide\", \"residual sugar\", \"volatile acidity\", \"total sulfur dioxide\", \"chlorides\", \"fixed acidity\", \"citric acid\",\"sulphates\"]\n\nfor i in range(8):\n    ax1 = fig.add_subplot(2,4,i+1)\n    sns.barplot(x='quality', y=features[i],data=data, hue='type', palette='rocket')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <font color=blue>The volatile acidity of red wine decreases with better quality.</font>\n* <font color=blue>Better quality red and white wines have shown decreased level of chlorides in them, meaning less amount of salt.</font>\n* <font color=blue>There is an increase in the levels of Citric acid and sulphates in higher quality of red wine, which could mean that good quality red wines have more freshness/flavor and antioxidants in them.</font>"},{"metadata":{},"cell_type":"markdown","source":"# Splitting data into predictors and target, then to train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide data into predictors and target\nX = data.iloc[ :, 1:13]\nX.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1st index or 1st column is target\ny = data.iloc[ : , 0]\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Transform type data to '1' and '0'\ny = y.map({'white':1, 'red' : 0})\ny.dtype           # int64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store column names somewhere for use in feature importance\ncolnames = X.columns.tolist()\n\ncolnames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataset into train and validation parts\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.30,\n                                                    shuffle = True\n                                                    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Create pipeline ####\n#### Pipe using XGBoost\n\nsteps_xg = [('sts', ss() ),\n            ('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        # Specify other parameters here\n            )\n            ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Pipeline object\npipe_xg = Pipeline(steps_xg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameter tuning using Grid Search, Random Search and Bayesian Optimization\n\n## <font color=\"brown\">Grid Search</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"##################### Grid Search #################\n\n#   Specify xgboost parameter-range\n#   Dictionary of parameters (16 combinations)\n#     Syntax: {\n#              'transformerName_parameterName' : [ <listOfValues> ]\n#              }\n\n\nparameters = {'xg__learning_rate':  [0.3, 0.05],\n              'xg__n_estimators':   [50,  100],\n              'xg__max_depth':      [3,5],\n              'pca__n_components' : [5,7]\n              }                               # Total: 2 * 2 * 2 * 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#    Grid Search (16 * 2) iterations\n#    Create Grid Search object first with all necessary\n#    specifications. Note that data, X, as yet is not specified\nclf = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 2,         # USe parallel cpu threads\n                   cv =5 ,             # No of folds\n                   verbose =2,         # Higher the value, more the verbosity\n                   scoring = ['accuracy', 'roc_auc'],  # Metrics for performance\n                   refit = 'roc_auc'   # Refitting final model on what parameters?\n                                       # Those which maximise auc\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start fitting data to pipeline\nstart = time.time()\nclf.fit(X_train, y_train)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Best score: {clf.best_score_} \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Best parameter set {clf.best_params_}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gs = clf.predict(X_test)\n# Accuracy\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\nf\"Accuracy: {accuracy_gs * 100.0}\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='orange'>Finding feature importance - Grid Search</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Find feature importance of any BLACK Box model\n\n# Instantiate the importance object\nperm = PermutationImportance(\n                            clf,\n                            random_state=1\n                            )\n\n# fit data & learn\nstart = time.time()\nperm.fit(X_test, y_test)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Conclude: Get feature weights\n\neli5.show_weights(\n                  perm,\n                  feature_names = colnames      # X_test.columns.tolist()\n                  )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fw = eli5.explain_weights_df(\n                  perm,\n                  feature_names = colnames      # X_test.columns.tolist()\n                  )\n\n# Print importance\nfw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"brown\">Random Search</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#####################  Randomized Search #################\n\n# Tune parameters using randomized search\n# Hyperparameters to tune and their ranges\nparameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,100),\n              'xg__max_depth':      range(3,5),\n              'pca__n_components' : range(5,7)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#     Tune parameters using random search\n#     Create the object first\nrs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          # Max combination of\n                                            # parameter to try. Default = 10\n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          # Use parallel cpu threads\n                        cv = 2               # No of folds.\n                                             # So n_iter * cv combinations\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run random search for 25 iterations. 21 minutes\nstart = time.time()\nrs.fit(X_train, y_train)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate\nf\"Best score: {rs.best_score_} \"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Best parameter set: {rs.best_params_} \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\ny_pred_rs = rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\nf\"Accuracy: {accuracy_rs * 100.0}\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"brown\">Bayesian Optimization</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"###############  Tuning using Bayes Optimization ############\n# Which parameters to consider and what is each one's range\npara_set = {\n           'learning_rate':  (0, 1),                 \n           'n_estimators':   (50,100),               \n           'max_depth':      (3,5),                 \n           'n_components' :  (5,7)          \n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#    Create a function that when passed some parameters\n#    evaluates results using cross-validation\n#    This function is used by BayesianOptimization() object\n\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    #  Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        # Why repeat this here for each evaluation?\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # Now fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1'\n                                ).mean()             # take the average of all results\n\n\n    #  Finally return maximum/average value of result\n    return cv_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#      Instantiate BayesianOptimization() object\n#      This object  can be considered as performing an internal-loop\n#      i)  Given parameters, xg_eval() evaluates performance\n#      ii) Based on the performance, set of parameters are selected\n#          from para_set and fed back to xg_eval()\n#      (i) and (ii) are repeated for given number of iterations\n#\nxgBO = BayesianOptimization(\n                             xg_eval,     # Function to evaluate performance.\n                             para_set     # Parameter set from where parameters will be selected\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#     Gaussian process parameters\n#     Modulate intelligence of Bayesian Optimization process\n#     This parameters controls how much noise the GP can handle,\n#     so increase it whenever you think that extra flexibility is needed.\ngp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian\n                                 # process.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Fit/train (so-to-say) the BayesianOptimization() object\n#     Start optimization. 25minutes\n#     Our objective is to maximize performance (results)\nstart = time.time()\nxgBO.maximize(init_points=5,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=25,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n              **gp_params\n               )\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get values of parameters that maximise the objective\nxgBO.res\nxgBO.max","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting parameters in model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model with parameters of grid search\nmodel_gs = XGBClassifier(\n                    learning_rate = clf.best_params_['xg__learning_rate'],\n                    max_depth = clf.best_params_['xg__max_depth'],\n                    n_estimators=clf.best_params_['xg__n_estimators']\n                    )\n\n#  Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__n_estimators']\n                    )\n\n#  Model with parameters of Bayesian Optimization\nmodel_bo = XGBClassifier(\n                    learning_rate = xgBO.max['params']['learning_rate'],\n                    max_depth = int(xgBO.max['params']['max_depth']),\n                    n_estimators= int(xgBO.max['params']['n_estimators'])\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modeling with all the parameters\nstart = time.time()\nmodel_gs.fit(X_train, y_train)\nmodel_rs.fit(X_train, y_train)\nmodel_bo.fit(X_train, y_train)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions with all the models\ny_pred_gs = model_gs.predict(X_test)\ny_pred_rs = model_rs.predict(X_test)\ny_pred_bo = model_bo.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9.4 Accuracy from all the models\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\naccuracy_bo = accuracy_score(y_test, y_pred_bo)\nprint(\"Grid Search\",accuracy_gs)\nprint(\"Random Search\",accuracy_rs)\nprint(\"Bayesian Optimization\",accuracy_bo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Get feature importances from all the models\nmodel_gs.feature_importances_\nmodel_rs.feature_importances_\nmodel_bo.feature_importances_\nplot_importance(model_gs)\nplot_importance(model_rs)\nplot_importance(model_bo)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for all the models\n\ncm_gs = confusion_matrix(y_test,y_pred_gs)\ncm_rs = confusion_matrix(y_test,y_pred_rs)\ncm_bo = confusion_matrix(y_test,y_pred_bo)\n\ncms = [cm_gs, cm_rs, cm_bo]\nclassifiers = [\"Grid Search\",\"Random Search\",\"Bayesian Optimization\"]\n\ndef plot_confusion_matrix(cms):\n   \n    fig = plt.figure(figsize=(20,12))\n    plt.subplots_adjust( hspace=0.5, wspace=0.4)\n    for i in range(3):\n        j = i+1\n        ax = fig.add_subplot(1,3,j)\n        plt.imshow(cms[i], interpolation='nearest', cmap=plt.cm.Pastel1)\n\n        classNames = ['Red','White']\n\n        plt.ylabel('Actual', size='large')\n\n        plt.xlabel('Predicted', size='large')\n\n        tick_marks = np.arange(len(classNames))\n        plt.xticks(tick_marks, classNames, size='x-large')\n\n        plt.yticks(tick_marks, classNames, size='x-large')\n\n        s = [['TN','FP'], ['FN', 'TP']]\n    \n    \n        plt.text(-0.23,0.05, str(s[0][0])+\" = \"+str(cms[i][0][0]), size='x-large')\n        plt.text(0.8,0.05, str(s[0][1])+\" = \"+str(cms[i][0][1]), size='x-large')\n        plt.text(-0.23,1.05, str(s[1][0])+\" = \"+str(cms[i][1][0]), size='x-large')\n        plt.text(0.8,1.05, str(s[1][1])+\" = \"+str(cms[i][1][1]), size='x-large')\n        plt.title(classifiers[i], fontsize=15)\n\nplot_confusion_matrix(cms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get probability of occurrence of each class\ny_pred_prob_gs = model_gs.predict_proba(X_test)\ny_pred_prob_rs = model_rs.predict_proba(X_test)\ny_pred_prob_bo = model_bo.predict_proba(X_test)\n\n# Draw ROC curve\nfpr_gs, tpr_gs, thresholds = roc_curve(y_test,\n                                 y_pred_prob_gs[: , 0],\n                                 pos_label= 0\n                                 )\n\nfpr_rs, tpr_rs, thresholds = roc_curve(y_test,\n                                 y_pred_prob_rs[: , 0],\n                                 pos_label= 0\n                                 )\n\nfpr_bo, tpr_bo, thresholds = roc_curve(y_test,\n                                 y_pred_prob_bo[: , 0],\n                                 pos_label= 0\n                                 )\n# AUC\nauc_gs = auc(fpr_gs,tpr_gs)\nauc_rs = auc(fpr_rs,tpr_rs)\nauc_bo = auc(fpr_bo,tpr_bo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"performance = pd.DataFrame({ \"Classifiers\":[\"Grid Search\",\"Random Search\",'Bayesian Optimization'],\n                             \"Accuracy\": [accuracy_score(y_test,y_pred_gs),accuracy_score(y_test,y_pred_rs),accuracy_score(y_test,y_pred_bo)],\n                             \"Precision\": [precision_score(y_test,y_pred_gs),precision_score(y_test,y_pred_rs),precision_score(y_test,y_pred_bo)],\n                             \"AUC\":[auc_gs,auc_rs,auc_bo],\n                             \"Recall\":[recall_score(y_test,y_pred_gs),recall_score(y_test,y_pred_rs),recall_score(y_test,y_pred_bo)],\n                             \"f1_score\":[f1_score(y_test,y_pred_gs),f1_score(y_test,y_pred_rs),f1_score(y_test,y_pred_bo)]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))   # Create window frame\nax = fig.add_subplot(111)   # Create axes\n\n#8.1 Connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n\n#8.2 Labels \nax.set_xlabel('False Positive Rate')  # Final plot decorations\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for models')\n\n#8.3 Set graph limits\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\n\n#8.4 Plot each graph now\nax.plot(fpr_gs, tpr_gs, label = \"gs\")\nax.plot(fpr_rs, tpr_rs, label = \"rs\")\nax.plot(fpr_bo, tpr_bo, label = \"bo\")\n\n\n#8.5 Set legend and show plot\nax.legend(loc=\"lower right\")\nplt.show()\n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}