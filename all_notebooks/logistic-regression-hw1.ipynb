{"cells":[{"metadata":{"_uuid":"c9fafc5f18551abee0cf53af8f6c6750f70d79d8"},"cell_type":"markdown","source":"# This kernel is about Logistic Regression of 'Pima Indians Diabetes Database'\n\n1.[Exploratory Data analysis](#1)\n\n2.[Seaborn Heatmap](#15)\n\n3.[Supervised Learning:](#16)\n\n4.[Logistic Regression with sklearn](#2) \n\n5.[Gridsearch with Logistic Regression](#9)\n\n6.[K-Nearest Neighbors (KNN)](#3)\n\n7.[Support Vector Machine (SVM) Classification](#4)\n\n8.[Naive Bayes Classification](#5)\n\n9.[Decision Tree Classification](#6)\n\n10.[Random Forest Classification](#7)\n\n11.[Confusion Matrix](#8)\n\n12.[Class-Accuracy Barplot](#17)\n\n13.[Class-Accuracy Diagram](#18)\n\n13.[Conclusion:](#18)\n\n\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom sklearn.metrics import accuracy_score\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data= pd.read_csv(\"../input/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"919c1c6b3c5258404057ee932a9e51ffac52c03e"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Exploratory Data analysis"},{"metadata":{"trusted":true,"_uuid":"b71d3d2c4a972de512cd813cb46898ebe5748722"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b32e805f5bb4f2f212cf94107a62f71b3d63e0f"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbb3498e38ec6ec168254e859a079dcb0050bddc"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a77644f405ad006922d2cd2b85ded07cfdc98957"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f190dc47838835a1272d9094b40d7ab3d0b7989"},"cell_type":"code","source":"# correlation between features\ndata.Outcome =[\"D\" if each == 1 else \"ND\" for each in data.Outcome]\n\nsns.pairplot(data=data,palette=\"Set2\",hue=\"Outcome\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ec621ba92f193e20e8ee07f4f4bd2709539f2e6"},"cell_type":"code","source":"data.Outcome =[\"1\" if each == \"D\" else \"0\" for each in data.Outcome]\n# we find out number of zeros in each feature\nzeros = (data == 0)\nzeros.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1178da312f4b749adf6ce98027656de071ba5ff8"},"cell_type":"code","source":"# we replace zeros of each column ex. pregnancies ,age  and outcome with their column's mean \nfor each in data.columns[1:6]:\n    data[each] = data[each].replace(0, data[each].median())\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"833be24e846604d22d55a830c260d073d621305e"},"cell_type":"code","source":"y=data.Outcome.values                                  \nx_data=data.drop(['Outcome'],axis=1)\nprint(y.shape,x_data.shape)\nx_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"061b4dd397e85c847ef9fd8f97c8fb9382af3529"},"cell_type":"code","source":"#normalization: to get a value between 0 and 1 for each feature to prevent  some features from being dominant\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d7c8a94aee477bb73385b55308b8a1d87bdb4d8"},"cell_type":"code","source":"diabet = data[\"BMI\"]\nsimilarity_with_other_col = data.corrwith(diabet) #correlation of BMI with other features\nsimilarity_with_other_col","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd1dccdfc3be47e4b95fa597b5e5473cd59776f4"},"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n# Seaborn Heatmap"},{"metadata":{"trusted":true,"_uuid":"cf5adbbd0b0fede462a6274bd5b8b30a235c9d8a"},"cell_type":"code","source":"#Seaborn Heatmap to find out correlation between each feature\nf,ax = plt.subplots(figsize=(12,10))\ncmap=sns.diverging_palette(150, 275, s=80, l=55,n=9)\nsns.heatmap(\ndata.corr(), \nannot=True, annot_kws={'size':12},\nlinewidths=.8,linecolor=\"blue\", fmt= '.2f',ax=ax,square=True,cmap=cmap)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8936f4027818a00a4132eb02d38965a9f62ec55"},"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n# Supervised Learning:"},{"metadata":{"trusted":true,"_uuid":"10a8243aaffea578f05ad40d86e04c819a4c1260"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#we split our data in 80% train and 20% test data\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n#random state is important to get the same values after each forward_backward_propagation\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d81fb5cf35ea3e865a278c2c10fbf01773558b52"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# Logistic Regression with sklearn"},{"metadata":{"trusted":true,"_uuid":"462f46266950a4f286bf31544d20f3c3ce45b501"},"cell_type":"code","source":"#Logistic Regression with sklearn\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(C=100,penalty=\"l2\",solver=\"saga\",class_weight=None)\n#C : float, default value: 1.0\n#Inverse of regularization strength should be  positive, \n#small values>stronger regularization.\n#solver : For small datasets choose ‘liblinear’ ,‘sag’ and ‘saga’ are faster for large datasets.\n#For multiclass problems choose only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ for multinomial loss\n#‘newton-cg’, ‘lbfgs’ and ‘sag’ with L2 penalty,‘liblinear’ and ‘saga’ with L1 penalty.\nlogreg.fit(x_train, y_train)\ny_pre_lr = logreg.predict(x_test)\n\ntest_acc= logreg.score(x_test,y_test) \n\nprint(\"LR accuracy :  \",test_acc)\nlr_acc=logreg.score(x_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6774e559d03c8036eeb6097a19a1310405aa83cb"},"cell_type":"markdown","source":"<a id=\"9;\"></a> <br>\n# Gridsearch with Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"2487207acd6c25dd46299184f912af861fb986b8"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  \n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv = 10)\nlogreg_cv.fit(x_train,y_train)\ny_pre_lrcv = logreg_cv.predict(x_test)\nprint(\"tuned hyperparameters: \",logreg_cv.best_params_)\nprint(\"lr_accuracy: \",logreg_cv.best_score_)\n\nlogreg2 = LogisticRegression(C=100.0,penalty=\"l1\")\nlogreg2.fit(x_train,y_train)\nprint(\"lr_score: \", logreg2.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2cdf36f03473ea40f1c1cdf7c76eb999dd6ec42"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n#  K-Nearest Neighbors (KNN)\n"},{"metadata":{"trusted":true,"_uuid":"fe6d422f9d141655cfcb5bdb19f027c38b64463b"},"cell_type":"code","source":"# knn \nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=16, p=2,\n           weights='uniform')\n#‘distance’ : weight points, closer neighbors  have  greater influence than neighbors far away.\n#‘uniform’ : uniform weights, all points are weighted equally\n#algorithm :‘auto’  to decide the most appropriate algorithm \n# n_neighbors = k\nknn.fit(x_train,y_train)\ny_pre_knn = knn.predict(x_test)\nprint(\" With KNN (K= {}) accuracy is: {} \".format(16,knn.score(x_test,y_test)))\nknn_acc=knn.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c34154d516d6357db11e7fe0fb35742b3494211","trusted":true},"cell_type":"code","source":"# find k value\nk_list = []\nfor each in range(1,25):\n    knn_2 = KNeighborsClassifier(n_neighbors = each)\n    knn_2.fit(x_train,y_train)\n    k_list.append(knn_2.score(x_test,y_test))\n    \n\nf = plt.subplots(figsize=(18,8))\nplt.plot(range(1,25),k_list)\n   \nplt.xlabel('k values',fontsize = 15,color='black')             \nplt.ylabel('accuracy',fontsize = 15,color='black')\nplt.title('K values-Accuracy Plot',fontsize = 20,color='black')\nplt.xticks(range(1,25))\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(k_list),1+k_list.index(np.max(k_list))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3c6a8b61ad820789852d7d98949b9a77ead7f30"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# Support Vector Machine (SVM) Classification"},{"metadata":{"trusted":true,"_uuid":"2b5868cc0542e45dd8e224739efc30705c9a9a6a"},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm = SVC(C=100.0, cache_size=200, class_weight=\"balanced\", kernel='rbf',max_iter=-1)\n#Penalty parameter C ,default = 1.0\nsvm.fit(x_train,y_train)\ny_pre_svm = svm.predict(x_test)\nprint(\"SVM accuracy is: \",accuracy_score(y_test, y_pre_svm))\nsvm_acc=accuracy_score(y_test, y_pre_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4c64d61ae84eddd5ff543b3aaeaad86add73f42"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# Naive Bayes Classification\n"},{"metadata":{"trusted":true,"_uuid":"597a583b2a3bac4cbc741238b00e38708fe074a6"},"cell_type":"code","source":" # %% Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pre_nb = nb.predict(x_test)\n\nprint(\"NB accuracy is: \",nb.score(x_test,y_test))\nnb_acc=nb.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f08180d42d494e5a66472a0532de7e51e5c8de9"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n# Decision Tree Classification"},{"metadata":{"trusted":true,"_uuid":"09eab2cf0f9386dd7b0c2626a2b7730493fb747c"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(class_weight=\"balanced\",max_leaf_nodes=100)\ndt.fit(x_train,y_train)\n\ny_pre_dt = dt.predict(x_test)\n\nprint(\"DT accuracy is: \", dt.score(x_test,y_test))\ndt_acc= dt.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94d5a72cce98ba2170ec29962619d430570dc42c"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Random Forest Classification"},{"metadata":{"trusted":true,"_uuid":"8248138d847c032da440e684aaaa2bd7c7290344"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 500,max_depth=200)\nrf.fit(x_train,y_train)\n\ny_pre_rf = rf.predict(x_test)\nprint(\"RF accuracy is: \",rf.score(x_test,y_test))\nrf_acc=rf.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d49461ebaf5925106cb5f1b5e1f1f1189f9292d"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# Confusion Matrix"},{"metadata":{"trusted":true,"_uuid":"d2ddf99359622d9e53085d0c3e2281af8c5e9ce6","scrolled":false},"cell_type":"code","source":"import numpy  as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm_lr = confusion_matrix(y_test,y_pre_lr)        #logistic regression\ncm_dt = confusion_matrix(y_test,y_pre_dt)        #decision tree\ncm_knn = confusion_matrix(y_test,y_pre_knn)      #nearest neighbors\ncm_nb = confusion_matrix(y_test,y_pre_nb)        #naine bayes\ncm_rf = confusion_matrix(y_test,y_pre_rf)        #random forest\ncm_svm = confusion_matrix(y_test,y_pre_svm)      #support vector machine\n\ncm= np.array([cm_lr,cm_dt,cm_knn,cm_nb,cm_rf,cm_svm])\n  \n\nplt.figure(figsize=(20,10))\nplt.suptitle(\"Confusion Matrix\",fontsize=24,color=\"b\") \nclassification = np.array([\"Logistic Regression\",\"Decision Tree\",\"Random Forest\",\"K Nearest Neighbors\",\"Naive Bayes\",\"Support Vector Machine\"])\n\ni=0\nk=1\nwhile i < len(classification):\n    plt.subplot(2,3,k)\n    plt.title(classification[i],fontsize=14,color=\"b\")\n    sns.heatmap(cm[i],cbar=False,annot=True,cmap=\"PuBuGn\",fmt=\"d\",linewidths=.8,linecolor=\"red\")\n    i=i+1\n    k=k+1\n\nplt.show()\n \n        \n\n      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bbb7b3abd2087a0b4dafc1d93b62528d5262ed8"},"cell_type":"code","source":"dictionary = {\"Class\":[\"Logistic Regression\",\"Decision Tree\",\"Random Forest\",\"K Nearest Neighbors\",\"Naive Bayes\",\"Support Vector Machine\"],\n              \"Accuracy\":[lr_acc,dt_acc,rf_acc,knn_acc,nb_acc,svm_acc]} \ndataFrame1 = pd.DataFrame(dictionary)\ndataFrame1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d3feafb1f96389e1f114f45fe6588b89000823d"},"cell_type":"markdown","source":"<a id=\"17\"></a> <br>\n# Class-Accuracy Barplot"},{"metadata":{"trusted":true,"_uuid":"ee658dbf531bbb9516b57f724172021378b053b8"},"cell_type":"code","source":"\nfig, ax = plt.subplots(figsize=(15,10))\nN = 6  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.2  # bar width\n\nsns.barplot(x=dataFrame1['Class'], y=dataFrame1[\"Accuracy\"])\n\nax.set_xticks(ind + width)\nax.set_xticklabels(['LogisticRegression\\n',\n                    \"Decision Tre\\n\",\n                    'RandomForest\\n',\n                    \"K Nearest Neighbors\\n\",\n                    'Naive Bayes\\n',\n                    'Support Vector Machine\\n'],\n                   rotation=40,\n                   ha='right',fontsize = 13,color='magenta')\nplt.xlabel('Class',fontsize = 18,color='blue')\nplt.ylabel('Accuracy',fontsize = 18,color='blue')\nplt.ylim(0.65,0.85)\nplt.title('Class-Accuracy Diagram',fontsize = 20,color='blue')\n\nplt.savefig('graph.png')\nplt.grid()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8352b9b88c2a7849fe111c6bdd3a252f25ed9eb4"},"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n# Class-Accuracy Diagram"},{"metadata":{"trusted":true,"_uuid":"02f612ff0e13ee759f00733057b9e5de0c3ca255"},"cell_type":"code","source":"\ntrace1 = go.Bar(\n                x = dataFrame1['Class'],\n                y = dataFrame1[\"Accuracy\"],\n                name = \"Accuracy\",\n                marker = dict(color = ['rgba(160, 200, 155, 0.7)','rgba(60, 20, 155, 0.7)','rgba(16, 200, 55, 0.7)','rgba(90, 2, 155, 0.7)',\n                              'rgba(33, 234, 155, 0.7)','rgba(67, 56, 155, 0.7)'],\n                             line=dict(color='rgba(0,0,0)',width=2)))\ndt = [trace1]\nlayout = go.Layout(barmode = \"relative\",title = 'Class-Accuracy Diagram',hovermode='closest',font=dict(family='Arial', size=14,color=\"rgba(123,34,121,0.7)\"),\n         xaxis= dict(title= 'Class',ticklen= None,zeroline= False,gridwidth=2,tickangle=-20), \n         yaxis= dict(title= 'Accuracy',ticklen= None,zeroline= False,gridwidth=2))\n\n\n\nfig = go.Figure(data = dt, layout = layout)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9469ff97ad957d77c26bec18bc84e84fba53e664"},"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n# Conclusion:\n* After adjusting each method with their parameters we get K-Nearest Neighbors as the best  regression method for our dataset ! \nbut as we see in our Confusion Matrix none of the methods achieved a satisfactory result !"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}