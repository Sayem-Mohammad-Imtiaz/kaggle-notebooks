{"cells":[{"metadata":{},"cell_type":"markdown","source":"**In this notebook I'm using two approaches to determine if a bank's clients will leave or not, by using information like credit score, number of products or salary. The two ways to determine this is to make a classification with the help of 1) Logistic Regression and 2) Random Forests.**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\ndb = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv')\ndb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what data we have in here:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"db.info() #no missings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I noticed there are some variables that are not useful, like the customer's ID or name, so I'll delete those:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"db = db.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look to see how the two groups are different:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.options.display.max_rows = 200\ndb.groupby('Exited').describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, there are two important variables that are categorical, Gender and the country of origin, expressed by the Geography variable; in the following steps I will encode those ones into 1 and 0 and merge them to the initial database.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\ndb_cat = db[['Gender', 'Geography']] \ndb_cat_encoded = ordinal_encoder.fit_transform(db_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\ndb_cat_encoded_1hot = cat_encoder.fit_transform(db_cat_encoded)\ndb_cat_encoded_1hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array = db_cat_encoded_1hot.toarray()\n\ndb_cat_encoded_1hot_df = pd.DataFrame(array)\ndb_cat_encoded_1hot_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_cat_encoded_1hot_df.columns = ['M', 'F','France', 'Germany', 'Spain' ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_final = db.merge(db_cat_encoded_1hot_df, left_index = True, right_index = True)\ndb_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some important variables that I want to use when making the train/test split; for this, I will make some groups that wll be used for stratification: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#making bins to be able to split correctly, but using in regression/forest the original variable\ndb_final['Credit_Score'] = pd.cut(db_final['CreditScore'], bins = [0,500,650,720,np.inf], labels = [1,2,3,4])\n\ndb_final['Estimated_Salary'] = pd.cut(db_final['EstimatedSalary'], bins = [0,50000,100000,150000,np.inf], labels = [1,2,3,4])\n\ndb_final['Age_new'] = pd.cut(db_final['Age'], bins = [0,30,35,45,60,np.inf], labels = [1,2,3,4,5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll be making the train/test split by considering salary, credit score, country and gender as criterias:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(db_final, test_size = 0.2,\n                               stratify = db_final[['Estimated_Salary','Credit_Score',\n                                                    'M','F', 'France','Germany', 'Spain','Exited']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Isolating the target variables:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label = train['Exited']\ntrain = train.drop(['Exited','Gender', 'Geography'], axis = 1)\n\ntest_label = test['Exited']\ntest = test.drop(['Exited','Gender', 'Geography'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will be using Grid Search, so below I'm giving some various values for the C parameter and see what parameter is the best:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'C': [0.01, 0.05,0.1,0.15,0.3,0.5,0.8,1,5,3,5,5,10,20,50,100,150,200,300,400,1000],\n    'max_iter': [50, 100, 150]\n    }\n\nlog_reg = LogisticRegression()\ngrid_search = GridSearchCV(estimator = log_reg , param_grid = param_grid, \n                          cv = 10)\ngrid_search.fit(train,train_label)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid = grid_search.best_estimator_\nbest_grid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the classification report and accuracy:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_label = best_grid.predict(test)\n\nfrom sklearn.metrics import classification_report\nclassification_report(test_label, predict_label)\ntarget_names = ['0','1']\nprint(classification_report(test_label, predict_label, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(test_label, predict_label) #an accuracy of 0.79 ( for the first run)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Firstly, I'll be using Randomized Search to get a hint of what parameter options would be best, and then I'll use them and other closer values in a Grid Search:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint \n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 5, stop = 800, num = 30)]\n\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(5, 2000, num = 30)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 4, 6, 10]\n\nmin_samples_leaf = [1,2, 4, 6, 10]\n\n\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==== Randomized Search ====","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_reg = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = forest_reg, param_distributions = random_grid, n_iter = 100, cv = 3, \n                               verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(train,train_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==== Grid Search ====","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [1500,2000, 2200],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [8,10,12],\n    'min_samples_split': [4,6,8],\n    'n_estimators': [360,550,600,650,700]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(train,train_label)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid = grid_search.best_estimator_\n\nbest_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_label_2 = best_grid.predict(test)\n\nfrom sklearn.metrics import classification_report\nclassification_report(test_label, predict_label_2)\ntarget_names = ['0','1']\nprint(classification_report(test_label, predict_label_2, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(test_label, predict_label_2) # nice, 0.86 accuracy for the first run :)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In conclusion, The Random Forest algorithm got a better accuracy, of 0.86, compared to the Logistic Regression, with 0.79.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}