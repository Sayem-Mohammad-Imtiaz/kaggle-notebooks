{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport time \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport sklearn.model_selection as skl\n\n\n# from clean import *\n# from control import *","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_j = \"diagnosis\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dtype = torch.float\n# device = torch.device(\"cpu\")\ndevice = torch.device(\"cuda:0\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# organize dataset"},{"metadata":{},"cell_type":"markdown","source":"drops & converet"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = data.drop([\"Unnamed: 32\", \"id\"], axis=1)\n\ndata.loc[data.diagnosis == \"M\", \"diagnosis\"] = 1\ndata.loc[data.diagnosis == \"B\", \"diagnosis\"] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fill na "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Control.check_na_all_cols(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"normaliztion"},{"metadata":{"trusted":false},"cell_type":"code","source":"def norm(xij, mean_j, min_j, max_j):\n    result1 = xij- mean_j\n    result2  =   max_j - min_j\n    return result1/result2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"columns_x =list(data.columns)\ncolumns_x.remove(Y_j )\n\nfor j in data.columns :\n    data[j] = data[j].astype(np.float64)\n    \nfor j in columns_x:\n    j_max = data[j].max()\n    j_min = data[j].min()\n    j_mean = data[j].mean()\n\n    data[j] = data[j].apply(lambda xi :norm(xi , j_mean, j_min , j_max))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# val test vs train"},{"metadata":{"trusted":false},"cell_type":"code","source":"for j in data.columns :\n    data[j] = data[j].astype(np.float64)\n\nX = np.array(data.drop(Y_j, axis=1) )\ny = np.array(data[Y_j])\n\nX = torch.from_numpy(X).float()\ny = torch.from_numpy(y).float()\n\nX= X.to(device)\ny = y.to(device)\nX_train, X_test, y_train, y_test = skl.train_test_split(X, y, test_size=0.3, random_state=30)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## model"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.is_cuda","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class Module(torch.nn.Module):\n    def __init__(self):\n        super( Module,  self).__init__()\n        self.linear1 = torch.nn.Linear(30, 10)\n        self.linear2 = torch.nn.Linear(10, 1)\n\n\n    def forward(self, x):\n        x = F.prelu(self.linear1(x), weight=torch.Tensor([0.2]).to(device))\n        y_pred = F.prelu(self.linear2(x), weight=torch.Tensor([0.2]).to(device))\n        return y_pred\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%time \n# model instance\nmodel = Module()\nmodel.to(device)\n\n# var\nlearning_rate = 1e-6\nregularization_num = 2\nm = y_train.size()[0]\nD_in, H, D_out = X_train.shape[0], 10 , 1\n\n# func\nloss_fn = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\nstart = time.time()\nloss_hold = []\nfor epoch in range (261560):\n    for i in range(m):\n        optimizer.zero_grad()\n        y_pre = model(X_train[i].view(-1,30))\n        loss_ = loss_fn(y_pre, y_train[i].view(1,1))\n        \n        r = 0 \n        for param in model.parameters():\n            e= torch.sum(abs(param))\n            r +=e\n\n        loss= loss_+ r*(regularization_num/m)\n\n        loss.backward()\n        optimizer.step()\n    loss_hold.append(loss_)\n    print(loss_, epoch)\n    \nend = time.time()   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(np.array(loss_hold))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_neg = 0\ntrue_pos = 0\nfalse_pos = 0\nfalse_neg = 0 \ntotal = 0\nprint(\"Accuracy scores\")\nwith torch.no_grad():\n    for i  in range(len(X_test)):\n        # Calculate Accuracy\n\n        # Load images to a Torch Variable\n        images = X_test[i].view(-1, 30)\n\n        # Forward pass only to get logits/output\n        outputs = model(images)\n\n        # Get predictions from the maximum value\n        predicted = [1 if outputs[0]> 0.42 else 0]\n\n        \n        \n        # Total correct predictions\n        if 0 == int(predicted[0]) == y_test[i]:\n            true_neg +=1\n\n        elif 1 == int(predicted[0]) == y_test[i]:\n            true_pos +=1\n\n        elif 1== int(predicted[0]):\n            false_pos +=1\n\n        elif 0 == int(predicted[0]):\n            false_neg +=1\n \n\n    accuracy =  (true_pos + true_neg)/  len(X_test)\n    precision = true_pos/(true_pos + false_pos)\n    recall = true_pos/(true_pos + false_neg)\n    F1score = precision*recall*2 /(recall+precision)\n    print(\"accuracy \", str(accuracy))\n    print(\"precision \",str(precision))\n    print(\"recall \",str(recall))\n    print(\"F1score \", str(F1score) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"save_model = True\nif save_model is True:\n    # Saves only parameters\n    torch.save(model.state_dict(), '3_model_Breast_Cancer.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Research question:\nCan we predict a type of lump in the breast (cancerous or benign) by the X independent variables?\n\nData assembly:\nThe data is provided by the University of Wisconsin\n\nData manipulations:\nNormalization -\nxij_new = xij- mean_j / max_j - min_j\nTraining preparation:\n30% of the data - test (171 cases)\n70% of the data - were allocated for training\n\nModel type\nTwo-layer model\nLinear1 (prelu (p = 0.2), in_features = 30, out_features = 20, bias = True)\nLinear2 (prelu (p = 0.2), in_features = 10 out_features = 1, bias = True)\n\nLOSS function - MSELoss\n\nPunishment function - Adam\n\nTraining:\nepoch- 261560"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}