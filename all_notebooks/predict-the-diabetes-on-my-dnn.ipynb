{"cells":[{"metadata":{"collapsed":true,"trusted":true,"_uuid":"4801834872a59f2e65bb9e694e9cb85ae518deb6"},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport random\nimport time\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.contrib.layers import batch_norm\nfrom tensorflow.contrib.framework import arg_scope\nimport sys\ntf.reset_default_graph()","execution_count":23,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b2f7ab027c2863e2fd38127a6497f688d77ed19d"},"cell_type":"code","source":"\nDATASET_PATH      = \"../input/diabetes.csv\"\n\nLOGS_PATH\t= \"logs/\"\n\nNUM_DIM=8\nNUM_LABELS\t\t= 2\n\nTRAIN_BATCH_SIZE\t= 0\nVALID_BATCH_SIZE\t= 0\nTEST_BATCH_SIZE\t\t= 0\n#epoch \nMAX_EPOCH=70","execution_count":24,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f3929b2454a850fb7484d53f593f5e9dc68a11bd"},"cell_type":"code","source":"\ndef read_label_file(file='./diabetes.csv',train_split_ratio=0.6,valid_split_ratio=0.2,test_split_ratio=0.2):\n\tdf = pd.read_csv(file, encoding='latin-1')\n\tlabel=df.Outcome.tolist()\n\tcontents=[]\n\tfor row in df.iterrows():\n\t\tindex, data = row\n\t\tcontents.append(data[0:8].tolist())\n\n\tsize=len(label)\n\n\tsum_ratio=train_split_ratio+valid_split_ratio+test_split_ratio\n\t\n    \n\ttrain_size=int(size*train_split_ratio)\n\ttrain_split_range=train_size\n\ttrain_contents=contents[0:train_split_range]\n\ttrain_label=label[0:train_split_range]\n\t\n\tvalid_size=int(size*valid_split_ratio)\n\tvalid_split_range=valid_size+train_split_range\n\tvalid_contents=contents[train_split_range:valid_split_range]\n\tvalid_label=label[train_split_range:valid_split_range]\n\t\n\ttest_size=int(size*test_split_ratio)\n\ttest_split_range=test_size+valid_split_range\n\ttest_contents=contents[valid_split_range:test_split_range]\n\ttest_label=label[valid_split_range:test_split_range]\n\t\n\t\n\treturn train_size,valid_size,test_size,train_contents,train_label,valid_contents,valid_label,test_contents,test_label\n\ndef preprocess(data,label,num_class):\n\n\n\tcontents=data\n\t\n\n\tlabel = tf.one_hot(label,depth=num_class,on_value=1,off_value=0,axis=-1)\n\treturn contents,label\n\t\t\ndef create_queue(contents,labels):\n\n\n\tinput_queue = tf.train.slice_input_producer(\n\t\t\t\t\t\t\t\t\t\t\t\t[contents, labels],\n\t\t\t\t\t\t\t\t\t\t\t\tshuffle=True)\n\n\tcontent = input_queue[0]\n\tlabel = input_queue[1]\n\t\n\treturn content,label\n\n","execution_count":25,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"60e3cacaef64e294052cdc1eeb3a3ea91da4c715"},"cell_type":"code","source":"\ntotal_train_size,total_valid_size,total_test_size,train_contents,train_labels,valid_contents,valid_labels,test_contents,test_labels = read_label_file(DATASET_PATH,0.6,0.2,0.2)\n\nif(TRAIN_BATCH_SIZE==0):\n    TRAIN_BATCH_SIZE=(int)(total_train_size*0.1)\nif(VALID_BATCH_SIZE==0):\n    VALID_BATCH_SIZE=(int)(total_valid_size*0.1) \nif(TEST_BATCH_SIZE==0):\n    TEST_BATCH_SIZE=(int)(total_test_size*0.1) \n\n\ntrain_contents,train_labels=preprocess(train_contents,train_labels,NUM_LABELS)\nvalid_contents,valid_labels=preprocess(valid_contents,valid_labels,NUM_LABELS)\ntest_contents,test_labels=preprocess(test_contents,test_labels,NUM_LABELS)\n\n\n\n\ntrain_content, train_label=create_queue(train_contents,train_labels)\nvalid_content, valid_label=create_queue(valid_contents,valid_labels)\ntest_content, test_label=create_queue(test_contents,test_labels)\n\n\n\n\n\ntrain_batch = tf.train.batch(\n                        [train_content, train_label],\n                        batch_size=TRAIN_BATCH_SIZE\n                    )\n\nvalid_batch = tf.train.batch(\n                        [valid_content, valid_label],\n                        batch_size=VALID_BATCH_SIZE\n                    )\t\t\n\ntest_batch = tf.train.batch(\n                        [test_content,test_label],\n                        batch_size=TEST_BATCH_SIZE\n                    )\t\t\t\nprint (\"input pipeline ready\")","execution_count":26,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3289bb52e80ad536e5af00b22858ca4364fdb22b"},"cell_type":"code","source":"\ndef Batch_Norm(x, training, scope=\"bn\"):\n    with arg_scope([batch_norm],\n                    scope=scope,\n                    updates_collections=None,\n                    decay=0.9,\n                    center=True,\n                    scale=True,\n                    zero_debias_moving_mean=True):\n        return tf.cond(training,\n                        lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n                        lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n                       \n#BR layer(= ReLU + Batch Norm)\ndef BR_Layer(x,name,output_num,training):\n    xavier_initializer = tf.contrib.layers.xavier_initializer()\n    shape = x.get_shape().as_list()\n    input_num= shape[1]\n    W = tf.Variable(xavier_initializer([input_num, output_num]))\n    b = tf.Variable(xavier_initializer([output_num]))\n    _x = tf.matmul(x, W) + b\n    _x = Batch_Norm(_x,training, scope=\"bn_\"+name)\n    _x = tf.nn.relu(_x)\n    return _x\n\n\n\n# placeholder is used for feeding data.\nx = tf.placeholder(tf.float32, shape=[None, NUM_DIM], name = 'x') # none represents variable length of dimension. 784 is the dimension of MNIST data.\ny_target = tf.placeholder(tf.float32, shape=[None, NUM_LABELS], name = 'y_target') # shape argument is optional, but this is useful to debug.\ntraining=tf.placeholder(tf.bool)\n\nxavier_initializer = tf.contrib.layers.xavier_initializer()\n\n# reshape input data\n_x = tf.reshape(x,[-1,NUM_DIM],name=\"x_data\")\n\n#create a model\n_x=BR_Layer(_x,\"layer1\",32,training)\n_x=BR_Layer(_x,\"layer2\",16,training)\n_x=BR_Layer(_x,\"layer3\",32,training)\n\nshape = _x.get_shape().as_list()\ninput_num= shape[1]                       \nW = tf.Variable(xavier_initializer([input_num, NUM_LABELS]))\nb = tf.Variable(xavier_initializer([NUM_LABELS]))\n_x = tf.matmul(_x, W) + b\n                       \npred=_x\nprob_y=tf.nn.softmax(pred, name=\"prob_y\")\n\n# define the Loss function\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_target))\n\n# define optimization algorithm\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n\n\n# correct_prediction is list of boolean which is the result of comparing(model prediction , data)\ncorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y_target, 1))\n\n#define accuracy\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n\n#save weight\nt_vars = tf.trainable_variables()\nsaver = tf.train.Saver(max_to_keep=None,var_list=t_vars)\nsaver_def = saver.as_saver_def()","execution_count":27,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ef051667ad4ac598795bca48c1dcf17a7eef0ec1"},"cell_type":"code","source":"#gpu config\nconfigure=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True))\n\nsess = tf.Session(config=configure) \n\n#define a tensorboard\ntraining_loss=tf.summary.scalar('training_loss', cost)\nvalidation_loss=tf.summary.scalar('validation_loss', cost)\n\ntraining_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy)\nvalidation_accuracy = tf.summary.scalar(\"validation_accuracy\", accuracy)\n\n\n# Merge all summaries into a single op\nmerged_op = tf.summary.merge_all()\nwriter=tf.summary.FileWriter(LOGS_PATH, sess.graph)\n\n# initialization\ninit_op = tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e9b0218130cff02e96ae579d3987182c20394f89"},"cell_type":"code","source":"print(\"Session started!\")\nstart_session_time = time.time()\nsess.run(init_op)\n\n\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess,coord=coord)\nmax_epoch=MAX_EPOCH\ndisplay_step=1\nmax_accuracy=0\nmax_index=0\n#train \nfor epoch in range(0,max_epoch+1):\n    avg_cost = 0\n    total_batch=(int)(total_train_size/TRAIN_BATCH_SIZE)\n    for step in range(total_batch):\n        _train_batch=sess.run([tf.cast(train_batch[0], tf.float32),tf.cast(train_batch[1], tf.float32)])\n        if epoch!=0:\n            _,c=sess.run([train_step,cost] , feed_dict={x: _train_batch[0], y_target: _train_batch[1], training: True})\n            avg_cost += c / total_batch\n        else:\n            c=sess.run(cost , feed_dict={x: _train_batch[0], y_target: _train_batch[1], training: False})\n            avg_cost += c / total_batch\n\n    if epoch % display_step == 0:\n\n        print (\"Epoch:\", '%04d' % (epoch), \"cost=\",\"{:.9f}\".format(avg_cost))\n\n        _valid_batch=sess.run([tf.cast(valid_batch[0], tf.float32),tf.cast(valid_batch[1], tf.float32)])\n\n        # traininig accuracy\n        train_cos,train_acc, train_summ_acc,train_summ_loss = sess.run(\n        [cost, accuracy, training_accuracy,training_loss], \n        feed_dict={x : _train_batch[0],  y_target : _train_batch[1],training: False})\n        writer.add_summary(train_summ_acc, epoch) \n        writer.add_summary(train_summ_loss, epoch) \n\n\n        # validation accuracy\n        valid_cos, valid_acc, valid_summ_acc,valid_summ_loss  = sess.run(\n        [cost,accuracy, validation_accuracy,validation_loss],\n        feed_dict={x: _valid_batch[0], y_target: _valid_batch[1],training: False})\n        writer.add_summary(valid_summ_acc, epoch)\n        writer.add_summary(valid_summ_loss, epoch)\n        \n        #save weight\n        saver.save(sess,\"./weight/w\",epoch)\n        print(\"Train-Accuracy:\", train_acc,\"Train-Loss:\", train_cos, \"Validation-Accuracy:\", valid_acc,\"Val-Loss:\", valid_cos,\"\\n\")\n        #model selection = vaildation accuracy\n        if valid_acc > max_accuracy:\n            max_accuracy=valid_acc\n            max_index=epoch\n\n\n#test accuracy\nsaver.restore(sess,\"./weight/w-%d\"%(max_index))\n_test_batch=sess.run([tf.cast(test_batch[0],tf.float32),tf.cast(test_batch[1], tf.float32)])\n_accuracy=sess.run(accuracy, feed_dict={x: _test_batch[0], y_target:  _test_batch[1],training: False})\nprint(\"(result)test accuracy: %g / weight-%d\"%(_accuracy,max_index))    \n\n# close thread qeueue, writer, session\ncoord.request_stop()\ncoord.join(threads)\nwriter.close()\nsess.close()\n\nprint(\"close\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"4309a6b214bd16087f58aa69d58a676a9687cebe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c5a3e1a004016ea6e00303609fe90c12c3fdb2cf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c43b80db1072b4b0baabf1713b346240053601c3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"4ddcb130f4417f71e04838efa937604a0059f901"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}