{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## Credit Card Customer Attrition Prediction  \n\nGiven *data about credit card customers*, let's try to predict whether a given customer will **close their account**.  \n  \nWe will use a few different models to make our predictions."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/credit-card-customers/BankChurners.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Attrition_Flag'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{column: list(data[column].unique()) for column in data.select_dtypes('object').columns}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_encode(df, column, positive_value):\n    df = df.copy()\n    df[column] = df[column].apply(lambda x: 1 if x == positive_value else 0)\n    return df\n\ndef ordinal_encode(df, column, ordering):\n    df = df.copy()\n    df[column] = df[column].apply(lambda x: ordering.index(x))\n    return df\n\ndef onehot_encode(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop last two columns (unneeded)\n    df = df.drop(df.columns[-2:], axis=1)\n    \n    # Drop CLIENTNUM columns\n    df = df.drop('CLIENTNUM', axis=1)\n    \n    # Encode unknown values as np.NaN\n    df = df.replace('Unknown', np.NaN)\n    \n    # Fill ordinal missing values with modes (Education_Level and Income_Category columns)\n    df['Education_Level'] = df['Education_Level'].fillna('Graduate')\n    df['Income_Category'] = df['Income_Category'].fillna('Less than $40K')\n    \n    # Encode binary columns\n    df = binary_encode(df, 'Attrition_Flag', positive_value='Attrited Customer')\n    df = binary_encode(df, 'Gender', positive_value='M')\n    \n    # Encode ordinal columns\n    education_ordering = [\n        'Uneducated',\n        'High School',\n        'College',\n        'Graduate',\n        'Post-Graduate',\n        'Doctorate'\n    ]\n    income_ordering = [\n        'Less than $40K',\n        '$40K - $60K',\n        '$60K - $80K',\n        '$80K - $120K',\n        '$120K +'\n    ]\n    df = ordinal_encode(df, 'Education_Level', ordering=education_ordering)\n    df = ordinal_encode(df, 'Income_Category', ordering=income_ordering)\n    \n    # Encode nominal columns\n    df = onehot_encode(df, 'Marital_Status', prefix='MS')\n    df = onehot_encode(df, 'Card_Category', prefix='CC')\n    \n    # Split df into X and y\n    y = df['Attrition_Flag'].copy()\n    X = df.drop('Attrition_Flag', axis=1).copy()\n    \n    # Scale X with a standard scaler\n    scaler = StandardScaler()\n    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n    \n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = preprocess_inputs(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluding categorical columns\neda_df = pd.concat([X.loc[:, ['Customer_Age', 'Months_on_book']], X.loc[:,'Credit_Limit':'Avg_Utilization_Ratio']], axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24, 15))\n\nfor i in range(len(eda_df.columns) - 1):\n    plt.subplot(3, 3, i + 1)\n    sns.boxplot(eda_df[eda_df.columns[i]])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24, 15))\n\nfor i in range(len(eda_df.columns) - 1):\n    plt.subplot(3, 3, i + 1)\n    sns.distplot(eda_df[eda_df.columns[i]])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nsns.pairplot(eda_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = pd.concat([eda_df, y], axis=1).corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr, annot=True, vmin=-1.0, cmap='mako')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    LogisticRegression(),\n    SVC(),\n    DecisionTreeClassifier(),\n    MLPClassifier(),\n    RandomForestClassifier()\n]\n\nfor model in models:\n    model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_names = [\n    \"   Logistic Regression\",\n    \"Support Vector Machine\",\n    \"         Decision Tree\",\n    \"        Neural Network\",\n    \"         Random Forest\"\n]\n\nfor model, name in zip(models, model_names):\n    print(name + \": {:.4f}%\".format(model.score(X_test, y_test) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/pyeUcmmLeiM"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}