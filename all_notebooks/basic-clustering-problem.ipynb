{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n### This notebook was completed in accordance to [THIS TUTORIAL](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a) on Towards Data Science.\n___\n\nThe main objective is to see if there is a relationship between the time you'd wait for a geyser eruption and the how long the duration of the eruption is for Ol' Faithful geyser at Yellowstone National Park.\n\nTL;DR\n\nThere seems to be a corellation between ['waitng', 'eruption] points of data. The data suggests there is a cyclic activity that happens below Yellowstone that allows for a short waiting time and a short eruption followed by a longer waiting time with a longer eruption that could especially be seen on the dist and scatter plots. If the dataset is equipped with the date, time and also the height of the eruption, a more precise conclusion could be made to either support or rebut the hypothesis. "},{"metadata":{},"cell_type":"markdown","source":"### Process\n---\n\n1. Evaluate and explore the data to figure out additional information that could be useful for analysis\n2. Prepare the data through scaling and selecting only relevant features in the dataset.\n3. Process data through clustering methods with scaled data.\n4. Visualise the data that is processed to gain insight.\n5. Make what you will of the data :) (Ethically, you should tell the story of the data, not make the data fit your story)\n"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom sklearn.cluster import KMeans, SpectralClustering\nfrom sklearn.datasets.samples_generator import(make_blobs, make_circles, make_moons)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_samples, silhouette_score, pairwise_distances_argmin\n\n\ndf = pd.read_csv(\"../input/old-faithful/faithful.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (12,6))\nplt.subplot(1,2,1)\nsns.distplot(df.waiting, bins = 10)\n\nplt.subplot(1,2,2)\nsns.distplot(df.eruptions, bins = 10)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Graphs above shows that there are two different sets of data that could be clustered together. Since this is two dimentional data, it is possible to plot these points on a graph using a scatter plot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df.eruptions, df.waiting)\nplt.xlabel('Eruption time (min)')\nplt.ylabel('Waiting interval (min)')\nplt.title('Ol\\' Faithful Geyser Eruption')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This reveals that there are two clustering points, ie., the data can be categorised into two distinctive segments. But, that is what's seen to us as humans, sometimes there are hidden patterns that we are not able to see. \n\nTo prove this, we will deploy Machine Learning methods, specifically, K-Means clustering, the most common form of clustering."},{"metadata":{"trusted":true},"cell_type":"code","source":"elbow = []\nx = StandardScaler().fit_transform(df)\nfor i in range(1,10):\n    km = KMeans(n_clusters = i, max_iter = 20, random_state = 20)\n    km.fit(x)\n    elbow.append(km.inertia_)\n\n#Plot cluster\nplt.plot(range(1,10), elbow)\nplt.xlabel('Num of cluster')\nplt.title('Elbow Method')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [Elbow Method]((https://uc-r.github.io/kmeans_clustering#elbow)\n---\n\nDetermines the optimal number of clusters in the data set which can be seen at the sharpest curve of the graph, hence the name. \n\nK-Means in general utilises the Within Cluster Sum of Squares(WCSS) algorithm = distance of data point with centroid, the closer it is, the more similar they are.\n\nIn this data, it is clear that 2 is the best K as it has the sharpest curve. This will be evaluated furthur below. For now with K = 2, the below scatterplot is acheived."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['eruptions', 'waiting']]\nx = StandardScaler().fit_transform(df)\n\n#kmeans\nkm = KMeans(n_clusters = 2, max_iter = 20, random_state = 20)\nkm.fit(x)\n#Plot cluster\nkmCenter = km.cluster_centers_\nfig, ax = plt.subplots(figsize = (6,6))\nplt.scatter(x[km.labels_ == 0,0], x[km.labels_ == 0,1], c = 'green', label = 'Cluster 1')\nplt.scatter(x[km.labels_ == 1,0], x[km.labels_ == 1,1], c = 'blue', label = 'Cluster 2')\nplt.scatter(kmCenter[:, 0], kmCenter[:,1], c = 'r', label = 'Centroid', marker = '*')\n\nplt.legend()\nplt.xlim([-2, 2])\nplt.ylim([-2, 2])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of clustered data')\nax.set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# very long way to visualise k-means scatterplot\nfig, ax = plt.subplots(2, 2, figsize = (12,12), sharex = True, sharey = True)\n\n#kmeans\nkm = KMeans(n_clusters = 1, max_iter = 20, random_state = 20)\nkm.fit(x)\nkmCenter = km.cluster_centers_\n#Plot cluster\nax[0,0].set_title('K = 1')\nax[0,0].scatter(x[km.labels_ == 0,0], x[km.labels_ == 0,1], c = 'green', label = 'Cluster 1')\nax[0,0].scatter(kmCenter[:, 0], kmCenter[:,1], c = 'r', label = 'Centroid', marker = '*')\n\nkm = KMeans(n_clusters = 2, max_iter = 20, random_state = 20)\nkm.fit(x)\nkmCenter = km.cluster_centers_\n#Plot cluster\nax[0,1].set_title('K = 2')\nax[0,1].scatter(x[km.labels_ == 0,0], x[km.labels_ == 0,1], c = 'green', label = 'Cluster 1')\nax[0,1].scatter(x[km.labels_ == 1,0], x[km.labels_ == 1,1], c = 'blue', label = 'Cluster 2')\nax[0,1].scatter(kmCenter[:, 0], kmCenter[:,1], c = 'r', label = 'Centroid', marker = '*')\n\nkm = KMeans(n_clusters = 3, max_iter = 20, random_state = 20)\nkm.fit(x)\nkmCenter = km.cluster_centers_\n#Plot cluster\nax[1,0].set_title('K = 3')\nax[1,0].scatter(x[km.labels_ == 0,0], x[km.labels_ == 0,1], c = 'green', label = 'Cluster 1')\nax[1,0].scatter(x[km.labels_ == 1,0], x[km.labels_ == 1,1], c = 'blue', label = 'Cluster 2')\nax[1,0].scatter(x[km.labels_ == 2,0], x[km.labels_ == 2,1], c = 'yellow', label = 'Cluster 3')\nax[1,0].scatter(kmCenter[:, 0], kmCenter[:,1], c = 'r', label = 'Centroid', marker = '*')\n\nkm = KMeans(n_clusters = 4, max_iter = 20, random_state = 20)\nkm.fit(x)\nkmCenter = km.cluster_centers_\n#Plot cluster\nax[1,1].set_title('K = 4')\nax[1,1].scatter(x[km.labels_ == 0,0], x[km.labels_ == 0,1], c = 'green', label = 'Cluster 1')\nax[1,1].scatter(x[km.labels_ == 1,0], x[km.labels_ == 1,1], c = 'blue', label = 'Cluster 2')\nax[1,1].scatter(x[km.labels_ == 2,0], x[km.labels_ == 2,1], c = 'yellow', label = 'Cluster 3')\nax[1,1].scatter(x[km.labels_ == 3,0], x[km.labels_ == 3,1], c = 'purple', label = 'Cluster 4')\nax[1,1].scatter(kmCenter[:, 0], kmCenter[:,1], c = 'r', label = 'Centroid', marker = '*')\n\n\nplt.xlim([-2, 2])\nplt.ylim([-2, 2])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nfig.suptitle('Visualization of clustered data')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [Silhouette Analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n---\nUsed for K-Means clustering evaluation, checks the distance between centroid and data points. Values in this graph ranges from [-1, 1]:\n\nSilhouette Coefficients(silCo) = values calculated\n\n* silCo nearing +1 = samples are far away from their neighbor clusters\n* silCo 0 = very close to decision boundary between neighboring clusters\n* silCo nearing -1, negative values = smaple maybe assigned to wrong clusters\n\n\nIt should also be noted that using this method to code and iterate through the data is a more precise and elegant way to create scatterplots compated to the one above. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conciese and eficient method of iterating through differenct k-means along with it's silhouette analysis\nc = [2,3,4]\nfor n in c:\n    fig, (ax, ay) = plt.subplots(1,2,figsize = (12,8))\n    ax.set_xlim([-0.1, 1])\n    cluster = KMeans(n_clusters = n, max_iter = 20, random_state = 20)\n    clusterLabel = cluster.fit_predict(x)\n    silAvg = silhouette_score(x, clusterLabel)\n    print(\"K = \", n,\" average silhouette scoe : \", silAvg)\n    sampleSilVal = silhouette_samples(x, clusterLabel)\n    \n    yLower = 10\n    for i in range(n): \n        clusterSilVal = sampleSilVal[clusterLabel == i]\n        clusterSilVal.sort()\n        \n        iClusterSize = clusterSilVal.shape[0]\n        yUpper = yLower + iClusterSize\n        \n        color = cm.nipy_spectral(float(i) / n)\n        ax.fill_betweenx(np.arange(yLower, yUpper), 0, clusterSilVal, facecolor = color, edgecolor = color, alpha = 0.7)\n        \n        ax.text(-0.05, yLower + 0.5 * iClusterSize, str(i))\n        yLower = yUpper + 10\n    ax.axvline(x = silAvg, color = 'red', linestyle = '--')\n    \n    colors = cm.nipy_spectral(clusterLabel.astype(float) / i)\n    ay.scatter(x[:,0] ,x[:,1] ,c = colors, edgecolor='k')\n\n    \n    centers = cluster.cluster_centers_\n    # Draw white circles at cluster centers\n    ay.scatter(centers[:, 0], centers[:, 1], marker='o',c=\"white\", alpha=1, s=400, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ay.scatter(c[0], c[1], marker='$%d$' % i, cmap = 'winter')\n\n    ay.set_title(\"Ol' Faithful\")\n    ay.set_xlabel(\"Eruption Time\")\n    ay.set_ylabel(\"Waiting Time\")\n\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the sample above k = 3 and 4 are bad picks due to some samples having negative values, this means that there are data values that are clustered wrongly. Therefore, according to the elbow method, the evaluation using silhouette analysis proves correct as the best cluster for this dataset is k = 2. What can we infer from this?"},{"metadata":{},"cell_type":"markdown","source":"### Refferences\n1. Everything in General\n    https://uc-r.github.io/kmeans_clustering#elbow\n2. Subplots\n    https://jakevdp.github.io/PythonDataScienceHandbook/04.08-multiple-subplots.html\n    https://stackoverflow.com/questions/31726643/how-do-i-get-multiple-subplots-in-matplotlib\n3. Silhouette Analysis\n    https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}