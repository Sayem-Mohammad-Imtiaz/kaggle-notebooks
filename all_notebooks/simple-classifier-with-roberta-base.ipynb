{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport transformers\nfrom transformers import RobertaModel, RobertaTokenizer\nimport torch\n\nfrom torch.utils.data import DataLoader, Dataset\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding='latin1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[['OriginalTweet', 'Sentiment']]\ntest = test[['OriginalTweet', 'Sentiment']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Taking a look at data****"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(train['OriginalTweet'])\ntext[:1500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Let's do some cleaning.(Roberta is good enough for this dataset even if we don't clean the dataset. You can try it.)****"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: x.lower())\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: re.sub('\\r', '', x))\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: re.sub('\\r', '', x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: re.sub('\\n', '', x))\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: re.sub('\\n', '', x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: re.sub(\"\\'\", \"\", x))\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: re.sub(\"\\'\", \"\", x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(train['OriginalTweet'])\ntext[:1500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text[-1500:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Let's map the Sentiments into something that machine can understand****"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sentiment'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapped = {'Extremely Negative': 0,'Negative': 0,'Neutral': 1,'Positive': 2,'Extremely Positive': 2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sentiment'] = train['Sentiment'].map(mapped)\ntest['Sentiment'] = test['Sentiment'].map(mapped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Initializing the tokenizer****"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.OriginalTweet\n        self.targets = self.data.Sentiment\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TRAIN Dataset: {}\".format(train.shape))\nprint(\"TEST Dataset: {}\".format(test.shape))\n\ntraining_set = SentimentData(train, tokenizer, max_len)\ntesting_set = SentimentData(test, tokenizer, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = SentimentData(train, tokenizer, max_len= max_len)\ntest_set = SentimentData(test, tokenizer, max_len= max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loader = DataLoader(training_set, batch_size = 16)\ntesting_loader = DataLoader(test_set, batch_size= 16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Creating the network****"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 5)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RobertaClass()\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params= model.parameters(), lr = 2e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calcuate_accuracy(preds, targets):\n    n_correct = (preds==targets).sum().item()\n    return n_correct","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Trainng****"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epoch):\n    tr_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask, token_type_ids)\n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calcuate_accuracy(big_idx, targets)\n\n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n        \n        if _%5000==0:\n            loss_step = tr_loss/nb_tr_steps\n            accu_step = (n_correct*100)/nb_tr_examples \n            print(f\"Training Loss per 5000 steps: {loss_step}\")\n            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        # # When using GPU\n        optimizer.step()\n\n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 3\nfor epoch in range(EPOCHS):\n    train(epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Evaluation****"},{"metadata":{"trusted":true},"cell_type":"code","source":"def testing(model, testing_loader):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask, token_type_ids).squeeze()\n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calcuate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n            \n            if _%5000==0:\n                loss_step = tr_loss/nb_tr_steps\n                accu_step = (n_correct*100)/nb_tr_examples\n                print(f\"Test Loss per 100 steps: {loss_step}\")\n                print(f\"Test Accuracy per 100 steps: {accu_step}\")\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Test Loss Epoch: {epoch_loss}\")\n    print(f\"Test Accuracy Epoch: {epoch_accu}\")\n    \n    return epoch_accu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = testing(model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Upvote if you learn something. Next I'll try to use XLNet for this dataset.****"},{"metadata":{},"cell_type":"markdown","source":"****Thanks****"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}