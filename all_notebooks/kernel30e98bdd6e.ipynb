{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load the data from .csv into a data-frame\nimport pandas as pd\ninitial_df = pd.read_csv('/kaggle/input/datasetucimlairquality/AirQualityUCI.csv')\ninitial_df.head()\ninitial_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print the null values of Date column to observe how the data looks.\ninitial_df[initial_df['Date'].isnull()]\n#Observing the data, each data has a data point per hour\n#However all the column of Date NaN also have a NaN\n#Hence it makes sense to drop all NaN rows which amount to 114 data points out of 9471 data points = 0.11%\ninitial_df.dropna(subset=['Date'],inplace=True)\ninitial_df.info()\ninitial_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column 15 and 16 has no valid data, so need to dop them as well.\ninitial_df.drop(initial_df.filter(regex=\"Unnamed\"),axis=1, inplace=True)\ninitial_df.describe()\ninitial_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copy into a new Pandas Data-frame for clarity purposes only\ncleaned_up_df = initial_df\ninitial_df.shape\ncleaned_up_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove date and time for plotting purposes\ntemp_df = cleaned_up_df.copy()\ntemp_df.drop(columns=['Date','Time']) #This line causes error if executed mutiple times as \n#'Date' and 'Time' no longer esist after\n#first run\ntemp_df.plot()\ncleaned_up_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From above graph it is clear that -200 is an invalid value which needs to be got rid of.\n#The idea is to drop if -200 is found in more than two columns\n#An attempt was made to drop all -200 values, but then the number of data points drop to from 9357 to 827, hence the logic\n#of dropping rows where two columns have -200.\n#filterinfDataframe = dfObj[(dfObj['Sale'] > 30) & (dfObj['Sale'] < 33) ]\nsub_set_df = cleaned_up_df[(cleaned_up_df['CO_GT'] != -200) & (cleaned_up_df['PT08_S1_CO'] != -200)]\nsub_set_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count the remaining -200s in the data-set to decide how to proceed\nprint(\"In column CO(GT),  % of invalid values\", (sub_set_df['CO_GT'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column PT08.S1(CO),  % of invalid values\", (sub_set_df['PT08_S1_CO'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column NMHC(GT),  % of invalid values\", (sub_set_df['NMHC_GT'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column C6H6(GT) % of invalid values\", (sub_set_df['C6H6_GT'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column PT08.S2(NMHC) % of invalid values\", (sub_set_df['PT08_S2_NMHC'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column NOx(GT) % of invalid values\", (sub_set_df['Nox_GT'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column PT08.S3(NOx) % of invalid values\", (sub_set_df['PT08_S3_Nox'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column NO2(GT) % of invalid values\", (sub_set_df['NO2_GT'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column PT08.S4(NO2) % of invalid values\", (sub_set_df['PT08_S4_NO2'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column PT08.S5(O3) % of invalid values\", (sub_set_df['PT08_S5_O3'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column T % of invalid values\", (sub_set_df['T'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column RH % of invalid values\", (sub_set_df['RH'] == -200).sum(axis=0)/len(sub_set_df)*100)\nprint(\"In column AH % of invalid values\", (sub_set_df['AH'] == -200).sum(axis=0)/len(sub_set_df)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the column NMHC(GT)\nsub_set_with_minimal_minus_200 = sub_set_df.drop(['NMHC_GT'],axis=1)\nsub_set_with_minimal_minus_200.shape\nsub_set_with_minimal_minus_200.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Common -200 values in NOx(GT) and NO2(GT) can be removed as well.\nfully_clean_df = sub_set_with_minimal_minus_200[(sub_set_with_minimal_minus_200['Nox_GT'] != -200) & (sub_set_with_minimal_minus_200['NO2_GT'] != -200)]\nfully_clean_df.plot()\nfully_clean_df .shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop all the GT values for ML model building. \n#Note C6H6 is kept as is, as this there is no sensor value for the same.\nprint(fully_clean_df.shape)\nml_data_set = fully_clean_df.drop(['CO_GT','Nox_GT','NO2_GT'],axis=1)\nprint(ml_data_set.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the date and time so that ML models can use them to learn.\n#For e.g. months are needed to take into account seasonality in Italy\n#Spring Mar-May\n#Summer June-Aug\n#Autumn Sep-Nov\n#Winter Dec-Feb\n#The format of date is M/DD/YYYY or M-DD-YY\n#First is to make all date formats uniform\nimport pandas as pd\nml_data_set['Month'] = pd.to_datetime(ml_data_set['Date']).dt.month\nml_data_set['Year'] =  pd.to_datetime(ml_data_set['Date']).dt.year\n#Drop the Date column as month which is needed is extracted.\nml_data_set.drop(['Date'],axis=1,inplace = True)\n\n#Now extract the hour from Time column and drop the same.\nml_data_set['Hour'] = pd.to_datetime(ml_data_set['Time']).dt.hour\nml_data_set.drop(['Time'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CO_level is categorical, converth this using one_hot encoding\none_hot_features = ['CO_level']\none_hot_encoded_training_predictors = pd.get_dummies(ml_data_set['CO_level'])\nml_data_set = pd.concat([ml_data_set, one_hot_encoded_training_predictors] ,axis=1)\nml_data_set.drop(['CO_level'],axis=1,inplace=True)\nml_data_set.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First ML model for this problem, going with Linear Regression to predict RH\nml_data_set.describe()\nX = ml_data_set.drop(['RH'],axis = 1)\ny = ml_data_set['RH']\nX.describe()\nX.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into training and testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1234,test_size=0.3)\nprint(X_train.head())\ny_train\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\nprint(linreg.coef_)                                            # Coefficients for Logistic Regression\nprint(linreg.intercept_)\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict using the linear regression model\ny_pred = linreg.predict(X_test)\ny_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate these metrics by hand!\nfrom sklearn import metrics\nimport numpy as np\ndef typical_linear_model_performance(y_pred):\n    print ('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n    print ('MSE:', metrics.mean_squared_error(y_test, y_pred))\n    print ('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the R2 value\nfrom sklearn.model_selection import cross_val_score\ndef get_cross_value_score(model):\n    scores = cross_val_score(model, X_train, y_train,cv=5,scoring='r2')\n    print('CV Mean: ', np.mean(scores))\n    print('STD: ', np.std(scores))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_cross_value_score(linreg)\ntypical_linear_model_performance(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add an eli5 to understand how the model behaves\nimport eli5\neli5.show_weights(linreg,feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot a SHAP as well\nimport shap\nind = 4\nexplainer = shap.LinearExplainer(linreg,data=X_test.values)\nshap_values = explainer.shap_values(X_test)\nshap.initjs()\nshap.force_plot(\n    explainer.expected_value, shap_values[ind,:], X_test.iloc[ind,:],\n    feature_names=X_test.columns.tolist()\n)\nshap.summary_plot(shap_values,X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build KNN model for the data-set for fun and profit\nfrom sklearn.neighbors import KNeighborsRegressor\n\nclf_knn = KNeighborsRegressor(n_neighbors=10)\nclf_knn = clf_knn.fit(X_train,y_train)\n\ny_pred = clf_knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Measure up how KNN is doing\nget_cross_value_score(clf_knn)\ntypical_linear_model_performance(y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}