{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Автоматизируем выбор лучших признаков для модели","metadata":{}},{"cell_type":"markdown","source":"Имопорт библиотек","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Загружаем данные","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/mobile-price-classification/train.csv')\ndf_test = pd.read_csv('/kaggle/input/mobile-price-classification/test.csv')\nf'df_train shape: {df_train.shape}!    df_test shape: {df_test.shape}!'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* battery_power: Total energy a battery can store in one time measured in mAh\n* blue: Has Bluetooth or not\n* clock_speed: the speed at which microprocessor executes instructions\n* dual_sim: Has dual sim support or not\n* fc: Front Camera megapixels\n* four_g: Has 4G or not\n* int_memory: Internal Memory in Gigabytes\n* m_dep: Mobile Depth in cm\n* mobile_wt: Weight of mobile phone\n* n_cores: Number of cores of the processor\n* pc: Primary Camera megapixels\n* px_height\n* Pixel Resolution Height\n* px_width: Pixel Resolution Width\n* ram: Random Access Memory in MegaBytes\n* sc_h: Screen Height of mobile in cm\n* sc_w: Screen Width of mobile in cm\n* talk_time: the longest time that a single battery charge will last when you are\n* three_g: Has 3G or not\n* touch_screen: Has touch screen or not\n* wifi: Has wifi or not\n* price_range: This is the target variable with a value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).","metadata":{}},{"cell_type":"markdown","source":"## Приступаем к изучению методов селекции признаков","metadata":{}},{"cell_type":"markdown","source":"**Выбор признаков на статистической близости к целевой переменной**","metadata":{}},{"cell_type":"code","source":"'''функция для отбора К лучших признаков по их статистической близости к целевой переменной'''\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2 # для выбора категориальных входных признаков\nfrom sklearn.feature_selection import f_regression # числовые признаки и числовой выходной признак\nfrom sklearn.feature_selection import f_classif # числовые входные признаки и категориальный выходной признак","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.iloc[:,0:20]  # выбираем признаки для обучения\ny = df_train.iloc[:,-1]    # выбираем целевой признак\n'''обучаем функцию выбора на основе критерия кси-квадрат и извлекаем лучшие 10 признаков'''\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nbestfeatures2 = SelectKBest(score_func=f_regression, k=10)\nfit = bestfeatures.fit(X,y)\nfit2 = bestfeatures2.fit(X,y)\n'''создаем набор данных признаков с их весом и выбираем 10 лучших'''\nfeatureScores =  pd.DataFrame({'Specs':X.columns, 'Score1': fit.scores_, 'Score2': fit2.scores_})\nfeatureScores = featureScores.set_index('Specs')\nprint(featureScores.nlargest(10,'Score1'))\nprint(featureScores.nlargest(10,'Score2'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выбор лучших признаков на основе рекурсивной перекрестной проверки на выбранной модели","metadata":{}},{"cell_type":"code","source":"'''рекурсивное удаление признаков. Значимость признаков расчитывается на основе перекрестной проверки'''\nfrom sklearn.feature_selection import RFECV\n'''выбираем разные модели для оценщика'''\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import ElasticNet, ARDRegression, HuberRegressor, Lasso, \\\n                                 LogisticRegression, LinearRegression, RANSACRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = DecisionTreeClassifier()#SVC(kernel=\"linear\", C=1)\nrfe = RFECV(estimator=dt, step=1, cv=3, scoring='accuracy')\nrfe.fit(X, y)\nprint(\"Optimal number of features : %d\" % rfe.n_features_)\n\nrfe_support = rfe.get_support()\nfeatureScores['rfe'] = rfe_support\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выбор признаков на основе обучения модели. Без рекурсивного уменьшения количества признаков на каждом следующем этапе.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(), max_features=10)\nembeded_lr_selector.fit(X, y)\nembeded_svc_selector = SelectFromModel(SVC(kernel=\"linear\", C=1), max_features=10)\nembeded_svc_selector.fit(X, y)\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(), max_features=10)\nembeded_rf_selector.fit(X, y)\n\nfeatureScores[\"LogReg\"] = embeded_lr_selector.get_support()\nfeatureScores[\"SVC\"] = embeded_svc_selector.get_support()\nfeatureScores[\"rf\"] = embeded_rf_selector.get_support()\nfeatureScores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Подход на основе отдельной модели со встроенным методом важности признаков**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeatureScores[\"ET\"] = model.feature_importances_\nfeatureScores[\"ET\"].nlargest(10).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featureScores[\"ET\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\ncorrmat = df_train.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[top_corr_features].corr().loc['price_range', df_train[top_corr_features].corr().loc['price_range', :]>0.1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2\n\nhttps://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/","metadata":{}},{"cell_type":"code","source":"featureScores.sort_values(\"ET\", ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}