{"cells":[{"metadata":{"_uuid":"df8336a35f9871b6a1a6e78f6d2bfd0a0c9b18be","_cell_guid":"69fad06f-902d-48fc-b518-5e4576936b8e"},"cell_type":"markdown","source":"## 用决策树来分类贷款是否优良\n\n### 1. unbalanced samples. downsampling data\n### 2. 拼接column: data = data[cols + [target]]\n### 3. apply/transform lambda, then generate new column\n### 4. pd.concat([risky_loans, safe_loans], axis=0)  (行上面合并)"},{"metadata":{"_uuid":"c55f3a38832fa0dab94b65e415a1f9ef584b2a66","_cell_guid":"c6172972-1254-4543-8569-ea1033424043"},"cell_type":"markdown","source":"[LendingClub](https://www.lendingclub.com/) 是一家贷款公司. 在本次作业中,我们需要手动实现决策树来预测一份贷款是否安全，并对比不同复杂度下决策树的表现"},{"metadata":{"_uuid":"fb53c64fd342f2f1fd9101f70a72308581aef437","_cell_guid":"42453ae5-6138-43ce-bb7d-4e347f501cc1","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cb76d39b5d8af5873c0a7f07ab84a6b5e9dd83b","_cell_guid":"aa86acb1-6055-4134-a32b-79d597698c8c"},"cell_type":"markdown","source":"## 读取数据"},{"metadata":{"_uuid":"507c44676d13e998070c41e3f8839a7d82724bc7","_cell_guid":"7724afbc-2b38-4c70-80c7-e6312224ef54","trusted":true},"cell_type":"code","source":"#data = pd.read_csv(os.path.join(\"data\", \"loan_sub.csv\"), sep=',')\ndata = pd.read_csv(os.path.join(\"../input\", \"loan_sub.csv\"), sep=',')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3a4fe78a0b66ca9a1a1c42f33ab63df1465d645","_cell_guid":"412ed8a3-dd5b-4846-aa60-888b143306c5"},"cell_type":"markdown","source":"## 打印可用特征"},{"metadata":{"trusted":true,"_uuid":"24cf1892f93ff1fb062021265ee3f0b7dc0a1773"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e23f44db8c114fb10cf2d02b68d2bf6dc8e3aac7","_cell_guid":"c8884197-6cc9-4236-a28f-8b3506f6c637","trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efa74e142e0ca3b930ea9b29ec318f679fac2486"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40d1e9007e379dffa4c5e74a7b84941d65268868","_cell_guid":"0a13bb8a-8446-4b02-b393-8af665a7d3e9"},"cell_type":"markdown","source":"## 预处理预测目标\n\n预测目标是一列'bad_loans'的数据。其中**1**表示的是不良贷款，**0**表示的是优质贷款。\n\n将预测目标处理成更符合直觉的标签，创建一列 `safe_loans`. 并且: \n\n* **+1** 表示优质贷款, \n* **-1** 表示不良贷款. "},{"metadata":{"_uuid":"4bbc7f1c25b1ca7fe1002330eef24c8556513297","_cell_guid":"820bf879-5629-4913-9cf1-3c9ec31d6283","trusted":true},"cell_type":"code","source":"# safe_loans =  1 => safe\n# safe_loans = -1 => risky\n# original data only has column \"bad_loans\". \"safe_loans\" will be newly added column\ndata['safe_loans'] = data['bad_loans'].apply(lambda x : +1 if x==0 else -1)\ndata['safe_loans_jun'] = data['bad_loans'].transform(lambda x : +1 if x==0 else -1)\ndata = data.drop('bad_loans', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"203ca8f9fbc79311e1759f8bf5d1c5f84403041e"},"cell_type":"code","source":"np.where(data['safe_loans'] == data['safe_loans_jun']) # apply 和 transform 效果一样","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7089d17a76c0d2130919895b354ae81212e68055"},"cell_type":"code","source":"np.where(data['safe_loans'] != data['safe_loans_jun']) # apply 和 transform 效果一样","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d874fda861807e0f8140ebde7a482b4e61a75bb"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aafe75e22c05b0f357df7c941f635c28b107184","_cell_guid":"f5051fe5-cd49-4c7e-88d3-f8f0c4ae11c3"},"cell_type":"markdown","source":"## 打印优质贷款与不良贷款的比例"},{"metadata":{"_uuid":"41729addb33e2bbf2c818e9bb241defbd2911c15","_cell_guid":"82033730-45db-46da-b7ce-7d69e37e2d6f","trusted":true},"cell_type":"code","source":"data['safe_loans'].value_counts(normalize=True) # normalize output percentage. otherwise, print counts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e21249f249e98930e592512fa6fd37613136cbc","_cell_guid":"801fb7e3-601c-402e-9c74-54f3270d5e1a"},"cell_type":"markdown","source":"#### **这是一个不平衡数据， 好的贷款远比坏的贷款要多. **"},{"metadata":{"_uuid":"ccd24d899b1664cbc7f2c77fe5904c1092795bc7","_cell_guid":"3e9fdf3f-fb0a-4752-b8f4-3cea7cecaa02"},"cell_type":"markdown","source":"## 选取用于预测的特征"},{"metadata":{"_uuid":"77f4953e4cffc3ce8b1d03cf0638d743a5c0648f","_cell_guid":"e06eaf86-d27b-49c2-9c58-d06fe5d96495","trusted":true},"cell_type":"code","source":"cols = ['grade', 'term','home_ownership', 'emp_length']\ntarget = 'safe_loans'\n\ndata = data[cols + [target]]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7986f5fa1a00465580d5fc0ef35c7e486c3e8cbd","_cell_guid":"2608a6d3-ce0c-4654-b03e-7818b27a286e"},"cell_type":"markdown","source":"## 创建更为平衡的数据集  \n\n* 对占多数的标签进行下采样  \n* 注意有很多方法处理不平衡数据，下采样只是其中之一"},{"metadata":{"_uuid":"b4b53ace05296e20088141891e1c530b6729f1b5","_cell_guid":"276f6cff-07ed-47a6-a6d5-3e908d0324f4","trusted":true},"cell_type":"code","source":"data['safe_loans'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b80ca04b085d025bfd1599734309a08062913c1b","_cell_guid":"6471a511-e850-4e05-aa69-aec543d41605","trusted":true},"cell_type":"code","source":"# target = 'safe_loans'\n# use the percentage of bad and good loans to undersample the safe loans. undersample 减少 traning data 里 good loans 数量 使得数据集更平衡\nbad_ones = data[data[target] == -1]\nsafe_ones = data[data[target] == 1]\npercentage = len(bad_ones)/float(len(safe_ones)) # 2 / 10\nprint(percentage)\nsafe_ones.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14a21e1a9a57d236dddcb4c280ccee28383d617d"},"cell_type":"code","source":"risky_loans = bad_ones\nsafe_loans = safe_ones.sample(frac=percentage, random_state=33) # we got percentage in above step\nsafe_loans.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db38a3b2f693938f941cccdb5b06ce5d94d93790"},"cell_type":"code","source":"# combine two kinds of loans\ndata_set = pd.concat([risky_loans, safe_loans], axis=0) \ndata_set.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae851c795b1904acd75893497c3a2c937dc515b9","_cell_guid":"cfff49b7-80fd-46ed-9b61-192fcf8fb0eb"},"cell_type":"markdown","source":"Now, let's verify that the resulting percentage of safe and risky loans are each nearly 50%."},{"metadata":{"_uuid":"0723e88e0bdeb98cb7627208a833bce81b66b25d","_cell_guid":"c1ecc879-0726-43eb-b4a0-53fb3632b548","trusted":true},"cell_type":"code","source":"data_set[target].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7876e40a701b88b8b429a8cf2b4a9c0758f0a7a4","_cell_guid":"6bb3bfec-2acd-452e-8595-d28d7fcfbc4e"},"cell_type":"markdown","source":"## Preprocessing your features"},{"metadata":{"_uuid":"0fe13ceba9133e64d651e7fe39225d6b2d727022","_cell_guid":"a2e4d526-5e8d-4d88-a843-14b78b421c0a","trusted":true},"cell_type":"code","source":"def dummies(data, columns=['pclass','name_title','embarked', 'sex']):\n    for col in columns:\n        data[col] = data[col].apply(lambda x: str(x))\n        new_cols = [col + '_' + i for i in data[col].unique()]\n        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)[new_cols]], axis=1)\n        del data[col]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb04a0fc3b56dc8df8a8a55122c1c43387a06942","_cell_guid":"abeb6576-735b-4344-9ade-9e6f8cd53bef","trusted":true},"cell_type":"code","source":"#grade, home_ownership, target\ncols = ['grade', 'term','home_ownership', 'emp_length']\n# dmatrices 与 pandas.get_dummies 区别. dmatrices 会通过新产生的截距列消化掉其中一种类别，而get_dummies 不会，所有类别都会变成一列\ndata_set = dummies(data_set, columns=cols)\ndata_set.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d243b2e10f964060cd9530717a53d7532a2b1853","_cell_guid":"16882dc4-18b5-49b4-8148-479dfb58b925"},"cell_type":"markdown","source":"## 将数据分成训练集和测试集"},{"metadata":{"_uuid":"1e5b4b8bd2b0e377d8ddfff2ff38c24f5c434bff","_cell_guid":"4911ffb5-2c05-4fdd-be98-1e5c9fff46a6"},"cell_type":"markdown","source":"重要的事情说三遍!!  **把你的爪子从TEST DATA上拿开!!**   "},{"metadata":{"_uuid":"6db47bd106f3aa8b15ae5bd27e60c6d0afa335cb","_cell_guid":"4e50f571-4d2a-4bd9-8c52-e14dc455bd85","trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(data_set, test_size=0.2, random_state=33)\ntrainX, trainY = train_data[train_data.columns[1:]], pd.DataFrame(train_data[target])\ntestX, testY = test_data[test_data.columns[1:]], pd.DataFrame(test_data[target])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6fe564cf722378daa55e1e911602a0779bc209d","_cell_guid":"4fbbdee5-74a7-47a2-8813-f107fc31f9a1"},"cell_type":"markdown","source":"## 建自己的决策树!  \n\n任务：  \n1 实现根据error来选择最佳划分特征的函数best_split()  \n2 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n3 实现树节点的类TreeNode  \n4 实现模型的类MyDecisionTree  "},{"metadata":{"_uuid":"dd29bf8f9df88e51a03b8bba345851a6a051c47d","_cell_guid":"45e9cc4b-2976-4859-9e9b-d5668f629f94"},"cell_type":"markdown","source":"#### 任务1， 实现根据error来选择最佳划分特征的函数best_split()  也就是老师课件里的 “Gain in error reduction”\n约定树的左边对应target == 0， 树的右边对应target == 1"},{"metadata":{"_uuid":"849af04e01574d383a32c00fa0e461ad4ab2686b","collapsed":true,"_cell_guid":"147d0027-076d-45e6-90a9-fcc5f02077ab","trusted":false},"cell_type":"code","source":"# Week4.Session2.DecisionTree_Theory_V1.pdf \"Gain in error reduction = 1\" page 理解 为什么 positive_ones negative_ones min 就表示error counts\ndef count_errors(labels_in_node): # labels_in_node 就是label那一个column\n    if len(labels_in_node) == 0: # i.e. no labels\n        return 0\n    \n    positive_ones = labels_in_node.apply(lambda x: x==1).sum()\n    negative_ones = labels_in_node.apply(lambda x: x==-1).sum()\n    \n    return min(positive_ones, negative_ones) # 由于全民公投，占多数的那个类作为正确的类， 少的那个就当预测错了\n\n\ndef best_split(data, features, target):\n    # return the best feature\n    best_feature = None\n    best_error = 2.0 \n    num_data_points = float(len(data))  # i.e. training data size\n\n    for feature in features: # try use each feature to split\n        \n        # 左分支对应当前特征为0的数据点, 因为上面左右用pad.get_dummies做过one hot encoding了，所以feature 的值就是0/1\n        left_split_data = data[data[feature] == 0]\n        \n        # 右分支对应当前特征为1的数据点\n        right_split_data = data[data[feature] == 1] \n        \n        # 计算左边分支里犯了多少错 . left_split_data[target] 拿出来真实的label\n        left_misses = count_errors(left_split_data[target])            \n\n        # 计算右边分支里犯了多少错\n        right_misses = count_errors(right_split_data[target])\n            \n        # 计算当前划分之后的分类犯错率\n        error = (left_misses + right_misses) * 1.0 / num_data_points\n\n        # 更新应选特征和错误率，注意错误越低说明该特征越好\n        if error < best_error:\n            best_error = error\n            best_feature = feature\n    return best_feature","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdd890b070aad00c6e1e8fa05994c7b3fe4c16ef","_cell_guid":"c7e7535f-4bd8-4f21-b4d0-9f73f58c7824"},"cell_type":"markdown","source":"#### 任务2， 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n"},{"metadata":{"_uuid":"efe668080fa6d6014e8577755e1bcf9a486f00a6","collapsed":true,"_cell_guid":"8d2c523e-87c9-4bc5-bc4d-3f91523770dd","trusted":false},"cell_type":"code","source":"# input e. [0,0,1...]\n# output the entropy of the array\ndef entropy(labels_in_node):\n    # 二分类问题: 0 or 1\n    n = len(labels_in_node)\n    s1 = (labels_in_node==1).sum()\n    if s1 == 0 or s1 == n:\n        return 0\n    # 根据 Week4.Session2.DecisionTree_Theory_V1.pdf 里的公式\n    p1 = float(s1) / n\n    p0 = 1 - p1\n    return -p0 * np.log2(p0) - p1 * np.log2(p1)\n\n\ndef best_split_entropy(data, features, target):\n    \n    best_feature = None\n    best_info_gain = float('-inf') # 求对大值，所以先定义一个极小值\n    num_data_points = float(len(data))\n    # 计算划分之前数据集的整体熵值\n    entropy_original = entropy(data[target])\n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split_data = data[data[feature] == 0]\n        \n        # 右分支对应当前特征为1的数据点\n        right_split_data = data[data[feature] == 1] \n        \n        # 计算左边分支的熵值\n        left_entropy = entropy(left_split_data[target])            \n\n        # 计算右边分支的熵值\n        right_entropy = entropy(right_split_data[target])\n            \n        # 计算左边分支与右分支熵值的加权和（数据集划分后的熵值）\n        # please refer to: Week4.Session2.DecisionTree_Theory_V1.pdf  \"房产 vs 工作\" page 中的 公式 E（划分后）= (3/5) * E(有房产) + (2/5) * E（没有房产）\n        # 3/5 就是对应划分后有房产的人占的比率，2/5 无房产的人占的比列\n        entropy_split = len(left_split_data) / num_data_points * left_entropy + len(right_split_data) / num_data_points * right_entropy\n        \n        # 计算划分前与划分后的熵值差得到信息增益\n        info_gain = entropy_original - entropy_split\n\n        # 更新最佳特征和对应的信息增益的值\n        if info_gain > best_info_gain:\n            best_info_gain = info_gain\n            best_feature = feature\n    return best_feature\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0811bc86ebf5a0d00370c1aa789d04b7fc9957e","_cell_guid":"3b00c9a6-e15d-4e91-88e8-2fbd38f47eff"},"cell_type":"markdown","source":"#### 任务3，实现树节点的类TreeNode，每个树节点应该包含如下信息:  \n\n   3.1 is_leaf: True/False  表示当前节点是否为叶子节点  \n   \n   3.2 prediction: 当前节点做全民公投的预测结果\n   \n   3.3 left: 左子树  \n   \n   3.4 right: 右子树 \n   \n   3.5 split_feature: 当前节点用来划分数据时所采用的特征"},{"metadata":{"_uuid":"9547b5667dc89bfc04c3fc01867cc75605d31068","collapsed":true,"_cell_guid":"467829d6-2cd3-4cf0-9a8c-3a30ea0df025","trusted":false},"cell_type":"code","source":"class TreeNode:\n    def __init__(self, is_leaf, prediction, split_feature):\n        self.is_leaf = is_leaf\n        self.prediction = prediction\n        self.split_feature = split_feature\n        self.left = None\n        self.right = None\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de7d60a2ff797c586765d57c329de8d38cc73913","_cell_guid":"80fcae51-ed63-4657-b853-0b946c68c524"},"cell_type":"markdown","source":"#### 任务4，实现模型的类MyDecisionTree， 实现如下主要函数  \n  \n  \n   4.1 fit(): 模型在训练集上的学习  \n   \n   4.2 predict(): 模型在数据集上的预测\n   \n   4.3 score(): 模型在测试集上的得分   \n   \n   \n   \n   \n   为了实现4.1 - 4.3的方法， 需要实现如下辅助函数  \n   4.4 create_tree(): 创建一棵树  \n   \n   4.5 create_leaf(): 创建叶子节点  \n   \n   4.6 predict_single_data(): 模型预测单个数据  \n   \n   4.7 count_leaves(): 统计模型的叶子数"},{"metadata":{"_uuid":"3a9aa2c3839b0dec9d49655d272bf4ccad6e4357","_cell_guid":"a4771e61-5b2b-45ec-8061-c8db8c8e9653","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.metrics import accuracy_score\nclass MyDecisionTree(BaseEstimator):\n    \n    def __init__(self, max_depth, min_error):\n        self.max_depth = max_depth\n        self.min_error = min_error\n    \n    def fit(self, X, Y, data_weights = None):\n        \n        data_set = pd.concat([X, Y], axis=1)\n        features = X.columns\n        target = Y.columns[0]\n        # decision tree learning 的过程 就是建立树的过程\n        self.root_node = self.create_tree(data_set, features, \n                               target, current_depth = 0, max_depth = self.max_depth, min_error=self.min_error)\n        \n        \n    def predict(self, X):\n        prediction = X.apply(lambda row: self.predict_single_data(self.root_node, row), axis=1)\n        return prediction\n        \n        \n    def score(self, testX, testY):\n        target = testY.columns[0]\n        result = self.predict(testX)\n        return accuracy_score(testY[target], result)\n    \n    \n    def create_tree(self, data, features, target, current_depth = 0, max_depth = 10, min_error=0):\n        \"\"\"\n        Input\n        data: (pandas data frame) the input data\n        features: (pandas series/dataframe/numpy array) availabe features\n        target: (pandas series/dataframe numpy array) the target to predict\n        current_depth: (Int) current depth of the tree\n        max_depth: (Int) the maxmium depth of the tree\n        min_error: (Float) the manimum error reduction\n        \"\"\"\n        \n        \"\"\"\n        探索三种不同的终止划分数据集的条件  \n  \n        termination 1, 当错误率降到min_error以下, 终止划分并返回叶子节点  \n        termination 2, 当特征都用完了, 终止划分并返回叶子节点  \n        termination 3, 当树的深度等于最大max_depth时, 终止划分并返回叶子节点\n        \"\"\"\n        \n    \n        # 拷贝以下可用特征\n        remaining_features = features[:]\n\n        target_values = data[target]\n\n        \n        #############################\n        # section 1: 递归出口\n        #############################\n        # termination 1  bonus task\n        #if count_errors(target_values) <= min_error:\n        #    print(\"Termination 1 reached.\")     \n        #    return self.create_leaf(target_values)\n\n        # termination 2\n        if len(remaining_features) == 0:\n            print(\"Termination 2 reached.\")    \n            return self.create_leaf(target_values)    \n\n        # termination 3\n        if current_depth >= max_depth: \n            print(\"Termination 3 reached.\")\n            return self.create_leaf(target_values)\n\n        #############################\n        # section 2：如果继续划分， 划分数据集\n        #############################\n        # 选出最佳当前划分特征\n        #split_feature = best_split(data, features, target)   #根据正确率划分 bonus task\n        split_feature = best_split_entropy(data, features, target)  # 根据信息增益来划分\n\n        # 选出最佳特征后，该特征为0的数据分到左边，该特征为1的数据分到右边\n        left_split = data[data[split_feature] == 0]\n        right_split = data[data[split_feature] == 1]\n\n        # 剔除已经用过的特征\n        remaining_features = remaining_features.drop(split_feature)\n        print(\"Split on feature %s. (%s, %s)\" % (split_feature, str(len(left_split)), str(len(right_split))))\n\n        # 如果当前数据全部划分到了一边，直接创建叶子节点返回即可\n        if len(left_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(left_split[target])\n        if len(right_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(right_split[target])\n\n        # 递归上面的步骤\n        left_tree = self.create_tree(left_split, remaining_features, target, current_depth + 1, max_depth, min_error)        \n        right_tree = self.create_tree(right_split,remaining_features,target, current_depth + 1, max_depth, min_error)\n\n\n        #生成当前的树节点\n        result_node = TreeNode(False, None, split_feature)\n        result_node.left = left_tree\n        result_node.right = right_tree\n        return result_node    \n    \n    \n    \n    def create_leaf(self, target_values):\n        \"\"\"\n        Input\n        target_values: (pd frame/numpy array) e.g.[1, 1, -1, 1, -1, -1, 1]\n        ###\n        output\n        leaf : class Treenode(is_leaf, prediction, split_features)\n        \"\"\"\n        # 用于创建叶子的函数\n\n        # 初始化一个树节点， None: prediction, None: split_feature\n        leaf = TreeNode(True, None, None)\n\n        # 统计当前数据集里标签为+1和-1的个数，较大的那个即为当前节点的预测结果\n        num_positive_ones = len(target_values[target_values == +1])\n        num_negative_ones = len(target_values[target_values == -1])\n\n        # 全民公投\n        if num_positive_ones > num_negative_ones:\n            leaf.prediction = 1\n        else:\n            leaf.prediction = -1\n\n        # 返回叶子        \n        return leaf \n    \n    \n    \n    def predict_single_data(self, tree, x, annotate = False):   \n        # 如果已经是叶子节点直接返回叶子节点的预测结果\n        \"\"\"\n        Input:\n        tree: class TreeNode (is_leaf, prediction, split_feature)\n        x: pd dataframe\n        ####\n        Output:\n        prediction result: (Int) 1 or -1\n        \"\"\"\n        if tree.is_leaf:\n            if annotate: \n                print(\"leaf node, predicting %s\" % tree.prediction)\n            return tree.prediction \n        else:\n            # 查询当前节点用来划分数据集的特征\n            split_feature_value = x[tree.split_feature]\n\n            if annotate: \n                print(\"Split on %s = %s\" % (tree.split_feature, split_feature_value))\n            if split_feature_value == 0:\n                #如果数据在该特征上的值为0，交给左子树来预测\n                return self.predict_single_data(tree.left, x, annotate)\n            else:\n                #如果数据在该特征上的值为0，交给右子树来预测\n                return self.predict_single_data(tree.right, x, annotate)    \n    \n    def count_leaves(self):\n        return self.count_leaves_helper(self.root_node)\n    \n    def count_leaves_helper(self, tree):\n        if tree.is_leaf:\n            return 1\n        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9ca0219e3364102e75b39deabe4f77d04955a09","_cell_guid":"e66041da-6a65-4e35-9ac1-2e8ac3347b4a","trusted":false,"collapsed":true},"cell_type":"code","source":"#my_decesion_tree = create_tree(train_data, features, target, current_depth = 0, max_depth = 10)\nm = MyDecisionTree(max_depth = 10, min_error = 1e-15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4d1ebbb87e20249b0e0693252046026c5317277","_cell_guid":"78d4b9d1-788d-4d18-89e7-615f453e09b9","trusted":false,"collapsed":true},"cell_type":"code","source":"m.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f045de63eb6fd14c12af69947ca4acc8e2c0ea5","_cell_guid":"173b653e-2d06-41d7-839c-bb0cdda81392","trusted":false,"collapsed":true},"cell_type":"code","source":"m.score(testX, testY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1be26e9ef4d294dc5cca4978f6d438377012ba1e","collapsed":true,"_cell_guid":"fdfe7991-cbc4-4f69-b2ff-c86b5d634309"},"cell_type":"markdown","source":"### 决策树复杂度的探讨"},{"metadata":{"_uuid":"a6931555ca22d61cddc1d1f1414a7ddbbc40d2d7","_cell_guid":"976155ff-7a13-455a-9e9e-cc3ce4361727","trusted":false,"collapsed":true},"cell_type":"code","source":"m.count_leaves()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eaa6838a12272e0f7477fccb264abf4083a9db8d","_cell_guid":"8023c19c-517d-4e19-b551-b9be96fe3b25"},"cell_type":"markdown","source":"#### 探索不同树深度对决策树的影响  \n\n1 max_depth = 3  \n2 max_depth = 7  \n3 max_depth = 15\n"},{"metadata":{"_uuid":"004882eea9485db53ec13dbe66a40bc15d251553","_cell_guid":"70bc8cd9-8c22-47b7-ac9a-b2c5c8536878","trusted":false,"collapsed":true},"cell_type":"code","source":"model_1 = MyDecisionTree(max_depth = 3, min_error = 1e-15)\nmodel_2 = MyDecisionTree(max_depth = 7, min_error = 1e-15)\nmodel_3 = MyDecisionTree(max_depth = 15, min_error = 1e-15)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa55871d41d3df20f56560b6522dd188bb736698","_cell_guid":"cf1f1323-d0b3-42b4-ba2f-eacdacad3c23","trusted":false,"collapsed":true},"cell_type":"code","source":"model_1.fit(trainX, trainY)\nmodel_2.fit(trainX, trainY)\nmodel_3.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43d8ca8f1b6c2a8e5fa1cf8220fe7c86502c2bf1","_cell_guid":"f7fc22f7-9f16-4f4d-ad6e-f21a1cb48ceb","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"model_1 training accuracy :\", model_1.score(trainX, trainY))\nprint(\"model_2 training accuracy :\", model_2.score(trainX, trainY))\nprint(\"model_3 training accuracy :\", model_3.score(trainX, trainY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a94c9f3dbeee5faa1c28a308e61eb64a4585e975","_cell_guid":"200b9223-cb1a-40e2-9bc0-97cd3235859b","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"model_1 testing accuracy :\", model_1.score(testX, testY))\nprint(\"model_2 testing accuracy :\", model_2.score(testX, testY))\nprint(\"model_3 testing accuracy :\", model_3.score(testX, testY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27d934f546eaab51afa88c5096d638c79889719b","_cell_guid":"75357dec-a872-452d-ac02-08d6fb8ebdf3","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"model_1 complexity is: \", model_1.count_leaves())\nprint(\"model_2 complexity is: \", model_2.count_leaves())\nprint(\"model_3 complexity is: \", model_3.count_leaves())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f13d3823bf3878db93f850f8bcafbe26f9a569ec","collapsed":true,"_cell_guid":"15514b40-c5da-4c11-be2d-fee9a791b03d","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}