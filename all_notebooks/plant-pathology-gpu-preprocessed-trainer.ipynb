{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os, shutil\n!pip install -q efficientnet\nimport efficientnet.keras as efn\nimport tensorflow_probability as tfp\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfd = tfp.distributions\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nIMG_SIZE = 299\nBATCH_SIZE = 16\nEPOCHS = 7\nFOLDS = 6\nFILES_LIST = tf.io.gfile.glob('../input/plant-pathology-512x512-augmented/*.tfrecords')\nFILES_LIST.sort()\nFOLDS_LIST=dict()\nSTEPS_PER_EPOCH = dict()\nfor i in range(FOLDS):\n    FOLDS_LIST['fold_'+str(i)] = dict({'train':FILES_LIST[:i]+FILES_LIST[i+1:], 'val':FILES_LIST[i]})\n\ntotal_rows = 0\nfor k in FILES_LIST:\n    df = pd.read_csv(k[:-9]+'csv')\n    total_rows += df.shape[0]\n\nSTEPS_PER_EPOCH = int((total_rows*0.7)/BATCH_SIZE)    \nTFRECS_FORMAT={'image': tf.io.FixedLenFeature([], tf.string),\n                      'image_name': tf.io.FixedLenFeature([], tf.string),\n                      'target': tf.io.FixedLenFeature([6,], tf.float32)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixup(entry1,entry2):\n    image1,label1 = entry1\n    image2,label2 = entry2\n    \n    alpha = [0.2]\n    dist = tfd.Beta(alpha, alpha)\n    l = dist.sample(1)[0][0]\n    \n    img = l*image1+(1-l)*image2\n    lab = l*label1+(1-l)*label2\n    \n    return img, lab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_entry(entry):\n    return tf.cast(tf.reshape(tf.io.decode_raw(entry['image'],tf.uint8),[512,512,3]),tf.float32)/255.,entry['target']\n\n@tf.function\ndef resize(image):\n    return tf.image.resize(image,[IMG_SIZE,IMG_SIZE])\n\ndef make_datasets(fold_num):\n    train_TFRecDataset=tf.data.TFRecordDataset(FOLDS_LIST['fold_'+str(fold_num)]['train'])\n    train_Dataset = train_TFRecDataset.map(lambda example:tf.io.parse_example(example,TFRECS_FORMAT))\n    train_Dataset = train_Dataset.map(lambda entry: decode_entry(entry))\n    train_Dataset = train_Dataset.map(lambda image,target: (resize(image),target))\n    train_Dataset1 = train_Dataset.shuffle(128)\n    train_Dataset2 = train_Dataset.shuffle(128)\n    train_Dataset = tf.data.Dataset.zip((train_Dataset1, train_Dataset2))\n    train_Dataset = train_Dataset.map(lambda entry1,entry2: mixup(entry1,entry2))\n    \n    val_TFRecDataset=tf.data.TFRecordDataset([FOLDS_LIST['fold_'+str(fold_num)]['val']])\n    val_Dataset = val_TFRecDataset.map(lambda example:tf.io.parse_example(example,TFRECS_FORMAT))\n    val_Dataset = val_Dataset.map(lambda entry: decode_entry(entry))\n    val_Dataset = val_Dataset.map(lambda image,target: (resize(image),target) )\n    \n    return train_Dataset.batch(BATCH_SIZE).repeat().prefetch(AUTOTUNE),val_Dataset.batch(BATCH_SIZE).repeat().prefetch(AUTOTUNE) ####","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model():\n    model = tf.keras.Sequential()\n    efn_model = efn.EfficientNetB5(weights='noisy-student',input_shape=(IMG_SIZE,IMG_SIZE,3))\n    base_model = tf.keras.models.Model(inputs=efn_model.input, outputs=efn_model.layers[-3].output)\n    base_model.trainable = True\n#     base_model = tf.keras.applications.Xception(include_top=False,\n#                                             weights=\"imagenet\",\n#                                             input_tensor=None,\n#                                             input_shape=(299,299,3),\n#                                             pooling='max')\n    base_model.trainable = True\n    model.add(tf.keras.Input((299,299,3)))\n    model.add(base_model)\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dense(10,activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dense(6,activation='softmax'))\n\n    model.compile(optimizer='nadam',\n                 loss=tf.keras.losses.CategoricalCrossentropy(),\n                 metrics=[tf.keras.metrics.CategoricalAccuracy()])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold_num in range(FOLDS):\n    \n    print('#'*10)\n    print('Training fold ',fold_num)\n    print('#'*10)\n    \n    train_ds,val_ds = make_datasets(fold_num)\n    \n    filepath = \"fold_%i_effnet5_mixup_aug.hdf5\"%fold_num\n    save_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n                                                    factor=0.1,\n                                                    patience=2,\n                                                    verbose=1,)\n\n    \n    model = make_model()\n    \n    model.fit(train_ds,\n             validation_data = val_ds,\n             epochs = EPOCHS,\n             steps_per_epoch = STEPS_PER_EPOCH,\n             validation_steps = STEPS_PER_EPOCH,\n             callbacks = [save_checkpoint],\n             verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}