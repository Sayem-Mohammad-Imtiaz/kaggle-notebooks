{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom os import system\n\nimport sklearn\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n\ndf=pd.read_csv(path)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shape of the data\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Null Check\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Duplicate CHeck\ndf.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Column Level Information\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping 11 records - Total Charges has Blank values\ndf[df.TotalCharges==' '].TotalCharges","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df[df.TotalCharges!=' ']\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Object data type to Float Conversion for TotalCharges column\ndf.TotalCharges=df.TotalCharges.astype('float64')\nprint (df.dtypes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5 Point summary on the data\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dependent variable data distribution\ndf.Churn.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Churn.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot = df.Churn.value_counts().plot.pie(y='Churn', figsize=(5, 5), autopct='%1.0f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns[df.dtypes!='object']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns[df.dtypes=='object']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Categorical Unique values:\nfor i in df.columns[df.dtypes=='object']:\n    print((i),':')\n    print(df[i].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data distribution of each columns\ndata_plot=df[df.columns[df.dtypes!='object']]\nfig=plt.figure(figsize=(20,30))\nfor i in range(0,len(data_plot.columns)):\n    ax=fig.add_subplot(6,3,i+1)\n    skew =data_plot[data_plot.columns[i]].skew()\n    sns.distplot(data_plot[data_plot.columns[i]],hist=True,label='Skew = %.3f' %(skew))\n    ax.set_title(data_plot.columns[i],color='Red')\n    plt.legend(loc='best')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total Charges : Right Skewed\n\nTenure is distribued over 0 to 80 months\n\nMonthly charges are varying from 20 to 120  and highest data concentration is near 20 to 40\n\nTotal Charges - data is varying from 0 to 10000 and right skewed","metadata":{}},{"cell_type":"code","source":"data_plot=df[df.columns[df.dtypes!='object']]\ncol=df.columns[df.dtypes!='object']\nfig=plt.figure(figsize=(20,20))\nfor i in range(0,len(data_plot.columns)):\n    ax=fig.add_subplot(5,4,i+1)\n    sns.stripplot(df.Churn,df[col[i]],jitter=True)\n    ax.set_title(data_plot.columns[i],color='Red')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Monthly Charges of Churned customers are varing in lower range of 20 to 60\n\nTotal charges of churned customers having density near 0-4000","metadata":{}},{"cell_type":"code","source":"data_plot=df[df.columns[df.dtypes=='object']]\ncol=df.columns[df.dtypes=='object']\nfig=plt.figure(figsize=(20,20))\nfor i in range(0,len(data_plot.columns)):\n    ax=fig.add_subplot(5,4,i+1)\n    sns.stripplot(df.Churn,df[col[i]],jitter=True)\n    ax.set_title(data_plot.columns[i],color='Red')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.pairplot(df,hue='Churn' , diag_kind = 'kde')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Tenure and Total Charges shows very good correlation from the graph\n\nMonthly charges and tenure also having little good concentration","metadata":{}},{"cell_type":"code","source":"corr=df.corr()\ncorr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,25)) \np=sns.heatmap(corr, annot=True,square=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns[df.dtypes=='object']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=df.columns[df.dtypes=='object']\nfig=plt.figure(figsize=(20,15))\nfor i in range(0,len(col)):\n    ax=fig.add_subplot(6,3,i+1)\n    sns.countplot(x=col[i], hue='Churn', data=df)\n    ax.set_title(data_plot.columns[i],color='Green')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gender concentration - 50-50\n\nCustomers not having partner have churned more than having partners\n\nCustomers not having dependent have churned most\n\nCustomers having phone services have churned most \n\nCustomers having multiple lines and fibre obtip services have churned more\n\nCustomers have churned more when they are not given with tech support\n\nCustomers having month to month contract have chruned most\n\nCustomers having paperless billing, electronic check payments have churned most","metadata":{}},{"cell_type":"code","source":"#Categorical Encoding\nfor feature in df.columns: \n    if df[feature].dtype == 'object': \n        print('\\n')\n        print('feature:',feature)\n        print(pd.Categorical(df[feature].unique()))\n        print(pd.Categorical(df[feature].unique()).codes)\n        df[feature] = pd.Categorical(df[feature]).codes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping columns which are not required from dataset\ndf=df.drop('customerID', axis=1)\ndf.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Outlier Check\ndata_plot=df[df.columns[df.dtypes!='object']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(20,20))\nfor i in range(0,len(data_plot.columns)):\n    ax=fig.add_subplot(5,4,i+1)\n    sns.boxplot(x='Churn',y=data_plot[data_plot.columns[i]],data=data_plot)\n    ax.set_title(data_plot.columns[i],color='Green')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(20,20))\nfor i in range(0,len(data_plot.columns)):\n    ax=fig.add_subplot(5,4,i+1)\n    sns.boxplot(data_plot[data_plot.columns[i]])\n    ax.set_title(data_plot.columns[i],color='Green')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # **Train and Test Split**","metadata":{}},{"cell_type":"code","source":"# Copy all the predictor variables into X dataframe\nX = df.drop(['Churn'], axis=1)\n\n# Copy target into the y dataframe.  \ny = df['Churn']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split X and y into training and test set in 70:30 ratio\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler() \nX_train = sc.fit_transform(X_train) \nX_test = sc.transform (X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp=MLPClassifier(hidden_layer_sizes=(200),random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(mlp, X_train, y_train, cv=10)\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Performance Matrix on train data set\ny_train_predict = mlp.predict(X_train)\nmodel_score =mlp.score(X_train, y_train)\nprint(model_score)\nprint(metrics.confusion_matrix(y_train, y_train_predict))\nprint(metrics.classification_report(y_train, y_train_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#SENSITIVITY/RECALL=TP/TP+FN\n\n#SPECIFICITY=TN/TN+FP\n\n#ACCURACY=(TP+TN)/(TP+TN+FP+FN)\n\n#PRECISION=TP/TP+FP\n","metadata":{}},{"cell_type":"code","source":"# predict probabilities\nprobs = mlp.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANN_metrics=classification_report(y_train, y_train_predict,output_dict=True)\ndf1=pd.DataFrame(ANN_metrics).T\nANN_train_precision=round(df1.loc[\"1\"][0],2)\nANN_train_recall=round(df1.loc[\"1\"][1],2)\nANN_train_f1=round(df1.loc[\"1\"][2],2)\nANN_train_acc=round(df1.loc[\"accuracy\"][0],2)\nprint ('ANN_train_precision ',ANN_train_precision)\nprint ('ANN_train_recall ',ANN_train_recall)\nprint ('ANN_train_f1 ',ANN_train_f1)\nprint('ANN_train_acc',ANN_train_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Performance Matrix on test data set\ny_test_predict = mlp.predict(X_test)\nmodel_score = mlp.score(X_test, y_test)\nprint(model_score)\nprint(metrics.confusion_matrix(y_test, y_test_predict))\nprint(metrics.classification_report(y_test, y_test_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nprobs = mlp.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANN_metrics=classification_report(y_test, y_test_predict,output_dict=True)\ndf1=pd.DataFrame(ANN_metrics).T\nANN_test_precision=round(df1.loc[\"1\"][0],2)\nANN_test_recall=round(df1.loc[\"1\"][1],2)\nANN_test_f1=round(df1.loc[\"1\"][2],2)\nANN_test_acc=round(df1.loc[\"accuracy\"][0],2)\nprint ('ANN_test_precision ',ANN_test_precision)\nprint ('ANN_test_recall ',ANN_test_recall)\nprint ('ANN_test_f1 ',ANN_test_f1)\nprint('ANN_test_acc',ANN_test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL TUNING","metadata":{}},{"cell_type":"code","source":"param_grid = {\n    'hidden_layer_sizes': [200,300], \n    'max_iter': [5000],\n    'solver': ['adam'], \n    'tol': [0.0001], \n}\n\nnncl = MLPClassifier()\n\ngrid_search = GridSearchCV(estimator = nncl, param_grid = param_grid, cv =5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(X_train, y_train)\ngrid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_grid = grid_search.best_estimator_\nbest_grid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Performance Matrix on train data set\ny_train_predict = best_grid.predict(X_train)\nmodel_score =best_grid.score(X_train, y_train)\nprint(model_score)\nprint(metrics.confusion_matrix(y_train, y_train_predict))\nprint(metrics.classification_report(y_train, y_train_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nprobs = best_grid.predict_proba(X_train)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_train, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(y_train, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(train_fpr, train_tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANN_metrics=classification_report(y_train, y_train_predict,output_dict=True)\ndf1=pd.DataFrame(ANN_metrics).T\nANN_train_precision_M=round(df1.loc[\"1\"][0],2)\nANN_train_recall_M=round(df1.loc[\"1\"][1],2)\nANN_train_f1_M=round(df1.loc[\"1\"][2],2)\nANN_train_acc_M=round(df1.loc[\"accuracy\"][0],2)\nprint ('ANN_train_precision_M ',ANN_train_precision_M)\nprint ('ANN_train_recall_M ',ANN_train_recall_M)\nprint ('ANN_train_f1_M ',ANN_train_f1_M)\nprint('ANN_train_acc_M',ANN_train_acc_M)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Performance Matrix on test data set\ny_test_predict = best_grid.predict(X_test)\nmodel_score = best_grid.score(X_test, y_test)\nprint(model_score)\nprint(metrics.confusion_matrix(y_test, y_test_predict))\nprint(metrics.classification_report(y_test, y_test_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nprobs = best_grid.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntest_auc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\ntest_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(test_fpr, test_tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ANN_metrics=classification_report(y_test, y_test_predict,output_dict=True)\ndf1=pd.DataFrame(ANN_metrics).T\nANN_test_precision_M=round(df1.loc[\"1\"][0],2)\nANN_test_recall_M=round(df1.loc[\"1\"][1],2)\nANN_test_f1_M=round(df1.loc[\"1\"][2],2)\nANN_test_acc_M=round(df1.loc[\"accuracy\"][0],2)\nprint ('ANN_test_precision_M ',ANN_test_precision_M)\nprint ('ANN_test_recall_M ',ANN_test_recall_M)\nprint ('ANN_test_f1_M ',ANN_test_f1_M)\nprint('ANN_test_acc_M',ANN_test_acc_M)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Insights from EDA:\n\n*Dependent variable concentration is 73:27 percentage\n    \n* Most of the Customers churn happened because of not getting the technical support\n    \n*Customer churn for Male and Female customers are almost 50:50 percent\n    \n*Tenure is distribued over 0 to 80 months\n    \n* Monthly charges are varying from 20 to 120  and highest data concentration is near 20 to 40\n    \n*Total Charges - data is varying from 0 to 10000 and right skewed\n    \n*Tenure and Total Charges shows very good correlation from the graph\n    \n*Customers not having partner have churned more than having partners\n    \n*Customers not having dependent have churned most\n    \n*Customers having phone services have churned most \n    \n*Customers having multiple lines and fibre obtip services have churned more\n    \n*Customers have churned more when they are not given with tech support\n    \n*Customers having month to month contract have chruned most\n    \n*Customers having paperless billing, electronic check payments have churned most","metadata":{}}]}