{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bank_Marketing"},{"metadata":{},"cell_type":"markdown","source":"## Classification problem - will client subscribe a deposit"},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import data and modules\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_palette('husl')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import datasets\n\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom catboost import Pool, CatBoostClassifier\n\n\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom scipy.stats import skew \nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/bank-marketing/bank-additional-full.csv\", delimiter=';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imbalanced data\nprint('Imbalanced data','\\n',data['y'].value_counts())\nprint('Null',data.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlabel=LabelEncoder()\ndata['y']=label.fit_transform(data['y'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(data, hue=\"y\", height=5) \\\n   .map(sns.distplot, \"age\") \\\n   .add_legend();\nbins=[0,29,32,37,43,52,58,62,100]\nfor i in bins:\n    plt.axvline(i,c='green',linewidth=1,linestyle=\"--\")  #vertical line\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlabels = [1,2,3,4,5,6,7,8]\ndata['age_range'] = (pd.cut(data.age, bins, labels = labels)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Duration"},{"metadata":{"trusted":true},"cell_type":"code","source":"list([1,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(data, hue=\"y\", height=5) \\\n   .map(sns.distplot, \"duration\") \\\n   .add_legend();\nbins=[-1,30,100,180,319,650,1000,1800,5500]\nfor i in bins:\n    plt.axvline(i,c='green',linewidth=1,linestyle=\"--\")  #vertical line\nlabels = [1,2,3,4,5,6,7,8]\ndata['dur_range'] = (pd.cut(data.duration, bins, labels = labels)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dur_range.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pdays - days after 1st Call (999 if 0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['1st_call'] = data['pdays'].map(lambda x: 1 if x == 999 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['1st_call'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['pdays'] = data['pdays'].map(lambda x: 0 if x == 999 else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Num and Cat data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data['y'].copy()\ndata=data.drop(['y'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['campaign'] = data['campaign'].astype('object')\nfeat=data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_cat = np.where(data[feat].dtypes == np.object)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dum object column\ndata= pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_mass_calc(X,y):\n\n    #Some parameters\n\n    svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n\n    #Split\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n    #Standartize\n\n    sc = StandardScaler()\n    sc.fit(X_train)\n    X_train_std = sc.transform(X_train)\n    X_test_std = sc.transform(X_test)\n    a=[]\n   \n    #Search knn_param\n    a_index=list(range(1,11))\n    knn=[1,2,3,4,5,6,7,8,9,10]\n    a=[]\n    for i in knn:\n        model=KNeighborsClassifier(n_neighbors=i) \n        model.fit(X_train_std, y_train)\n        prediction=model.predict(X_test_std)\n        a.append(pd.Series(metrics.accuracy_score(prediction,y_test)))\n\n\n    #Max_Score_KNN\n    knn=pd.DataFrame(knn)\n    a=pd.DataFrame(a)\n    knn_data=pd.concat([knn,a],axis=1)\n    knn_data.columns=['Neig','Score']\n    knn_take=int(knn_data[knn_data['Score']==knn_data['Score'].max()][:1]['Neig'])\n\n    #model\n    #SolveLater How to write names automat\n    x=['CatB','XGB','RandomF','NB','svm.SVC','Log','DTr',str('KN='+str(knn_take))]\n    #Form for cycle\n\n    models=[CatBoostClassifier(),XGBClassifier(),RandomForestClassifier(),GaussianNB(),svm,LogisticRegression(),DecisionTreeClassifier(),KNeighborsClassifier(n_neighbors=knn_take)]\n    a_index=list(range(1,len(models)+1))\n    a=[]\n    for model in models:\n\n        model.fit(X_train_std, y_train)\n        prediction=model.predict(X_test_std)\n        a.append(pd.Series(metrics.accuracy_score(prediction,y_test)))\n    plt.plot(x, a)\n    #plt.xticks(x)\n    #MAX_Score+Model\n    x=pd.DataFrame(x)\n    a=pd.DataFrame(a)\n    all_scores=pd.concat([x,a],axis=1)\n    all_scores.columns=['model','Score']\n    print('Max_score:',all_scores[all_scores['Score']==all_scores['Score'].max()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_mass_calc(data,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The best XGB  0.92 by accuracy"},{"metadata":{},"cell_type":"markdown","source":"### AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_mass_calc(X,y,Score):\n\n    #Some parameters\n\n    svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n\n    #Split\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n    #Standartize\n\n    sc = StandardScaler()\n    sc.fit(X_train)\n    X_train_std = sc.transform(X_train)\n    X_test_std = sc.transform(X_test)\n    a=[]\n   \n    #Search knn_param\n    a_index=list(range(1,11))\n    knn=[1,2,3,4,5,6,7,8,9,10]\n    a=[]\n    for i in knn:\n        model=KNeighborsClassifier(n_neighbors=i) \n        model.fit(X_train_std, y_train)\n        prediction=model.predict(X_test_std)\n        a.append(pd.Series(Score(prediction,y_test)))\n\n\n    #Max_Score_KNN\n    knn=pd.DataFrame(knn)\n    a=pd.DataFrame(a)\n    knn_data=pd.concat([knn,a],axis=1)\n    knn_data.columns=['Neig','Score']\n    knn_take=int(knn_data[knn_data['Score']==knn_data['Score'].max()][:1]['Neig'])\n\n    #model\n    #SolveLater How to write names automat\n    x=['CatB','XGB','RandomF','NB','svm.SVC','Log','DTr',str('KN='+str(knn_take))]\n    #Form for cycle\n\n    models=[CatBoostClassifier(),XGBClassifier(),RandomForestClassifier(),GaussianNB(),svm,LogisticRegression(),DecisionTreeClassifier(),KNeighborsClassifier(n_neighbors=knn_take)]\n    a_index=list(range(1,len(models)+1))\n    a=[]\n    for model in models:\n\n        model.fit(X_train_std, y_train)\n        prediction=model.predict(X_test_std)\n        a.append(pd.Series(Score(prediction,y_test)))\n    plt.plot(x, a)\n    #plt.xticks(x)\n    #MAX_Score+Model\n    x=pd.DataFrame(x)\n    a=pd.DataFrame(a)\n    all_scores=pd.concat([x,a],axis=1)\n    all_scores.columns=['model','Score']\n    print('Max_score:',all_scores[all_scores['Score']==all_scores['Score'].max()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_mass_calc(data,y,metrics.roc_auc_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The best XGB  0.81 by auc_roc"},{"metadata":{},"cell_type":"markdown","source":"# Check XGB stability using Cross_Validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}