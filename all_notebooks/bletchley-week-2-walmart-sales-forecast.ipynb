{"cells":[{"metadata":{"_cell_guid":"2ae681dd-2393-4126-99d1-04cce9005baa","_uuid":"cd3296c40587f51a36e1818ae4a310ad05b658b5"},"cell_type":"markdown","source":"# A simple linear baseline for the Walmart challenge\nThis notebook shows how you load the data, prepare it for usage with Keras and then create a submission file. The model is a simple linear regression.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"24a8815e-b41c-46a9-87cd-aa9e832d6355","_uuid":"5d52d47ae5511641e3848fc37356f291e9a9166b"},"cell_type":"markdown","source":"## Data fields\n* Store - the store number\n* Dept - the department number\n* Date - the week\n* Weekly_Sales - sales for the given department in the given store\n* IsHoliday - whether the week is a special holiday week\n* Temperature - average temperature in the region\n* Fuel_Price - cost of fuel in the region\n* MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n* CPI - the consumer price index\n* Unemployment - the unemployment rate\n* IsHoliday - whether the week is a special holiday week\n* Weekly_Sales: The weekly department wide sales (train set only)\n* Type: An anonymized description on which type of store it is","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"7c399d8c-5531-44a8-a758-ef7785518f28","_uuid":"3f34f97043b07eb2c0cca8fdc317a199ace93a6c","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2f4f632d-6c37-435b-a330-62364d8a9091","_uuid":"f668e5a9e6c7462e8050c97c298bef888a13f6aa"},"cell_type":"markdown","source":"## Loading the data\nIn Kaggle, data that can be accessed by a Kernel is saved under ``../inputs/``\nFrom there we can load it with pandas:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"ecb87ead-f34a-4449-95f9-2c9c0764247d","_uuid":"03a0a3ff6dfdb3d53a037d727f6260c0412990a8","trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"028afb4f-3f91-482a-a91d-a7d16737f356","_uuid":"cca3344c7d9c45a2c0dc41fe90e88083e15fb8de"},"cell_type":"markdown","source":"We are going to do some data preparation. It is easiest to do this for training and test set combined so we have to do all these steps only once. It is good to know where to split the set afterwards though!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c813405d-10a0-45ba-891e-e4e494ab34fc","_uuid":"e6c57c708a9db7fc01447b6b00908b67cb7f4b09","trusted":false,"collapsed":true},"cell_type":"code","source":"len(train) # Get number of training examples","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8bca733e-3ba3-4db2-bcc1-66b371f702ab","_uuid":"adac71f84a15958f3ec618c5c49f8f86acbb9d06","trusted":false,"collapsed":true},"cell_type":"code","source":"len(test) # Get number of test examples","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"21486f31-1a32-4471-ae71-d791097547eb","_uuid":"98271a862054ac1b445e38a613b7b500c12e55ba","trusted":false},"cell_type":"code","source":"df = pd.concat([train,test],axis=0) # Join train and test","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7094d024-8782-4d1c-8b46-ddf7140f46ab","_uuid":"b2d03ef65c79c4144d743e6ab70f2de365a26c61","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head() # Get an overview of the data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1af8adf9-b554-4dd8-b58a-1b1d522a89fe","_uuid":"540ed3ffa79d92489943b96b0d40a9fdea4254d5","trusted":false,"collapsed":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cd280406-fb91-4a9a-bc08-27acbe6c8808","_uuid":"ecc8a8f449625c1ab493d380c5ea8fbd06a3e89e"},"cell_type":"markdown","source":"There seem to be some missing values in the data. We have to make sure to deal with them before feeding anything into the network.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4c2f78ee-4bf8-4d90-9e61-611aed761c1b","_uuid":"69a906f1e30491b5e20eadc6a807862ee4c4a344","trusted":false,"collapsed":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c1bc2f25-a609-4b1f-a9d4-a81f7b41919e","_uuid":"51e0995047f008d796329e30e1b8d38e8040c124"},"cell_type":"markdown","source":"We will do a bit of very basic feature engineering here by creating a feature which indicates whether a certain markdown was active at all.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"050a7853-86d7-4713-b336-456e2e216926","_uuid":"74c6dce0279cc94fe4ea748e6c9676908c94b431","trusted":false,"collapsed":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2331bf3-e2c3-4cef-b173-fc55097bdb9f","_uuid":"9f8d42ccef4c37705e8dc64fafb64e3a45104473"},"cell_type":"markdown","source":"We can probably safely fill all missing values with zero. For the markdowns this means that there was no markdown. For the weekly sales, the missing values are the ones we have to predict, so it does not really matter what we fill in there.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"690b5760-b2c9-4471-8355-fc9b3b0e0d73","_uuid":"5868b63dfc643d6d8015267c10c8105357de18fc","trusted":false},"cell_type":"code","source":"df.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ac690137-bb19-46b8-b75a-b1354222a161","_uuid":"57faefa432d4cd9d3624f9b5d2cd83c7be7af048","trusted":false,"collapsed":true},"cell_type":"code","source":"df.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5fbcdb3-1730-4781-a315-4382e09944cb","_uuid":"fcf57b3abc047cb210cff24f1f6d6f06d46c26b5","trusted":false,"collapsed":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"80cbd990-4c9a-4042-add5-bf2903c76504","_uuid":"2401b7681253b94de41c9cbbda369591cc68918e"},"cell_type":"markdown","source":"Now we have to create some dummy variables for categorical data.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c8f056ff-5f66-4b1c-b8a7-3bb5e8e3f189","_uuid":"0817f62638cbf975f9c2e1e87b398bcf60090e19","trusted":false,"collapsed":true},"cell_type":"code","source":"def get_holiday_feature(date):\n    super_bowl = ['2010-02-12','2011-02-11','2012-02-10','2013-02-08']\n    labor = ['2010-09-10','2011-09-09','2012-09-07','2013-09-06']\n    thanksgiving = ['2010-11-26','2011-11-25','2012-11-23','2013-11-29']\n    christmas = ['2010-12-31','2011-12-30','2012-12-28','2013-12-27']\n    if date in super_bowl:\n        return [0,0,0,1]\n    elif date in labor:\n        return [0,0,1,0]\n    elif date in thanksgiving:\n        return [0,1,0,0]\n    elif date in christmas:\n        return [1,0,0,0]\n    else:\n        return [0,0,0,0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e9e33d4-fa96-4d43-beb1-be291da09ea5","_uuid":"be494e5973e31cfd001e01a658b4d0eb030e0207","trusted":false,"collapsed":true},"cell_type":"code","source":"def dates(datelist):\n    x = []\n    for date in datelist:\n        temp = 0\n        temp = get_holiday_feature(date)\n        x.append(temp)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ad72749-e0a6-4e26-b87a-9ccf86c45a47","_uuid":"991111dd5e6f9cde4336d3b4eccb272e7849de70","trusted":false,"collapsed":true},"cell_type":"code","source":"x = dates(df['Date'])\nx[:100]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8d6e6b44-caee-4228-af7e-3432e0f4a483","_uuid":"704569e39146ff717fc0ade2a540bafc9166642d","trusted":false},"cell_type":"code","source":"df['Week'] = pd.to_datetime(df.Date).dt.week\ndf['Year'] = pd.to_datetime(df.Date).dt.year","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"689f5f9f-0e86-48ba-b7fb-e0f53ef7c7d8","_uuid":"f15828f3008a13624e832c406dfaea081f0e994c","trusted":false,"collapsed":true},"cell_type":"code","source":"lastweek = df.sort_values(by = ['Store', 'Dept', 'Date'])\nsales = lastweek['Weekly_Sales'].values\navg = df['Weekly_Sales'].mean()\nfor i in range(1,len(sales)):\n    avg.append((z[i-1]))\nfor j in range(len(avg)):\n    if avg[j] == 0:\n        avg[j] = Prev[j-1]\nlastweek = lastweek.assign(np.array(avg))\ndf = lastweek","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"75c044d2-4552-4371-91ce-563e8f2f305d","_uuid":"98780a9a5fe53e4944b96cc6417b3c23331fa6ce","trusted":false},"cell_type":"code","source":"# Make sure we can later recognize what a dummy once belonged to\ndf['Type'] = 'Type_' + df['Type'].map(str)\ndf['Store'] = 'Store_' + df['Store'].map(str)\ndf['Dept'] = 'Dept_' + df['Dept'].map(str)\ndf['Week'] = 'Week_' + df['Week'].map(str)\ndf['Year'] = 'Year_' + df['Year'].map(str)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"141ad9c1-1b6c-40b3-9286-200455fda866","_uuid":"c891ff438b9531dbdae3b87f954d22c13126eea3","trusted":false},"cell_type":"code","source":"# Create dummies\ntype_dummies = pd.get_dummies(df['Type'])\nstore_dummies = pd.get_dummies(df['Store'])\ndept_dummies = pd.get_dummies(df['Dept'])\nweek_dummies = pd.get_dummies(df['Week'])\nyear_dummies = pd.get_dummies(df['Year'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6b9b33b5-6511-4069-a1e2-8058494e30b3","_uuid":"5824ea5127d26b5174cabf1c33942e008e3d12b4","trusted":false},"cell_type":"code","source":"# Add dummies\ndf = pd.concat([df,type_dummies,store_dummies,dept_dummies, week_dummies, year_dummies],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"976808b7-58ae-4260-85c9-d8e4ec60cc2f","_uuid":"7a229ea27c65ad24f8a69699bff78337d0f71ad2","trusted":false},"cell_type":"code","source":"# Remove originals\ndel df['Type']\ndel df['Store']\ndel df['Dept']\ndel df['Week']\ndel df['Year']\ndel df['Date']\n#del df['CPI']\n#del df['Fuel_Price']\n#del df['MarkDown1']\n#del df['MarkDown2']\n#del df['MarkDown3']\n#del df['MarkDown4']\n#del df['MarkDown5']\n#del df['Size']\n#del df['Temperature']\n#del df['Unemployment']\n#del df['IsHoliday']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"380a4882-7bf1-443b-bf7f-904839a25181","_uuid":"e57d087a410140af3fb150f325b1c8a63ec43545","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8f6d0f4-ba49-4cbc-94ec-3c0546b22000","_uuid":"2880be4b30dad2cef5c6d92d0cbefe1cf8dde843"},"cell_type":"markdown","source":"Now we can split train test again.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"e5843e97-a2f0-499f-bb5a-f76b5892cf67","_uuid":"a8b39cd5493a5d870050f7138f67564d8a39e223","trusted":false},"cell_type":"code","source":"# smaller training set just to test out different models\ntrain_fake = df.iloc[:15000]\ntrain = df.iloc[:282451]\n\ntest_fake = df.iloc[15000:20000]\ntest = df.iloc[282451:]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e5694f11-94b0-49f7-bd5c-03c6ca42b306","_uuid":"be7ce247dc836e58a262eaa65829e78a8bb6ac97","trusted":false},"cell_type":"code","source":"test = test.drop('Weekly_Sales',axis=1) # We should remove the nonsense values from test","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"05480287-8f52-4a0c-90b4-77a7ef542966","_uuid":"4ba90bd3dced7bfd9fc47b7d7d2b87ba7f72da4f"},"cell_type":"markdown","source":"To get numpy arrays out of the pandas data frame, we can ask for a columns, or dataframes values","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ee4bd27f-60f9-451a-8800-8834a129ba68","_uuid":"d718d37849a08665b1a649c342b27b0115105ae2","trusted":false,"collapsed":true},"cell_type":"code","source":"y = train['Weekly_Sales'].values\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e24d360-2654-4993-ad4c-6a62c2581002","_uuid":"06c17700a046b415745a37bbbc524d722c64190b","trusted":false,"collapsed":true},"cell_type":"code","source":"X = train.drop('Weekly_Sales',axis=1).values\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"646df720-0d93-4a28-8248-59ee8e0979ab","_uuid":"dd252e577ea69f0d9e2d187c00db6150388860e0"},"cell_type":"markdown","source":"Now we create the baseline model","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"2432e904-f354-4d14-97e8-fd51482daa3a","_uuid":"e7f30cbfb0fdadcd71966f0bd9a21cbf8e6d97af","trusted":false},"cell_type":"code","source":"from keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97950a3e-f649-45f5-8c1d-b3037e7e5ba0","_uuid":"2482223a55b2ab4d23969b50de7c7706ff5f2ea5"},"cell_type":"markdown","source":"# Testing of different models starts here\n\nWe will train this model using batch gradient descent, that is we will process all of our training examples at once. We can do this since we do not have very many training examples and the size of each individual example is quite small, just a 64 number per row. If you have a computer with little RAM you might consider using a smaller batch size than the whole trainings set.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"c26cfc2b-8e40-4a9d-9727-54dc67398d95","_uuid":"f49110995254b426299240b1cbdab4a084290c5f","trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(1,input_dim=196,\n                activation ='relu',\n                kernel_regularizer= regularizers.l2(0.01)))\nmodel.compile(optimizer='adam', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbe954e4-16c0-434e-a643-3aa01badd61b","_uuid":"bce5fb2f92f19945db32e085034f2cf7918ef3e7","trusted":false,"collapsed":true},"cell_type":"code","source":"model.fit(X, y, epochs=5, batch_size= 2048)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0196e4f7-c7e2-48cf-80d4-30d230658ea0","_uuid":"30f0718d03aa9f509e60e1d9a82b66217fd20c58","trusted":false,"collapsed":true},"cell_type":"code","source":"model.evaluate(x=X,y=y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c1e90c20-b79d-4f27-aab2-2bc0fa29c0a9","_uuid":"e3c30e44178c49e849d2bc10931a3cce1492b55b"},"cell_type":"markdown","source":"After we have created our model, we can predict things with it on the test set","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"477db2ed-eac9-41b0-8183-25d3f51fbd82","_uuid":"1acc40800a9009b01df3516c0c450d9de95c0a76","trusted":false,"collapsed":true},"cell_type":"code","source":"y_pred = model.predict(test.values, batch_size = X.shape[0])\ny_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d91cfd14-1d0c-40f6-bf3c-29d46f4d49e1","_uuid":"8cbf4dc2be08983bc71bb9228ec43f72c0bb7b5a","trusted":false},"cell_type":"code","source":"X_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7a3ed9dd-2026-4b98-aef4-7a8f66bd3c48","_uuid":"0c6f1f23a472e6ea928911d0aed5b15c55e14f47","trusted":false},"cell_type":"code","source":"y_pred = model.predict(X_test,batch_size=2048)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e05b28a-fd62-40d0-b5d1-d2ead9a2fa1e","_uuid":"def1162bf32a3f7ae4bb2ac095f563cd3aa2faf7"},"cell_type":"markdown","source":"To create the ids required for the submission we need the original test file one more time","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"88d548cd-1646-4083-a54b-74a42901eb5c","_uuid":"61f90e8fbc75fd2d62b0546d4a7772a25f2799c3","trusted":false},"cell_type":"code","source":"testfile = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"02f1a056-c082-4603-9b8c-6ed2001e2ec5","_uuid":"7c8c55cc85f67e658a6ec5a9f1725bef665212ee"},"cell_type":"markdown","source":"Now we create the submission. Once you run the kernel you can download the submission from its outputs and upload it to the Kaggle InClass competition page.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"145421d0-6f8e-4545-bc14-c6fe8dea4d0f","_uuid":"f74aaf3d356cbf771e6a1c701831ac96a3b8a4d5","trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({'id':testfile['Store'].map(str) + '_' + testfile['Dept'].map(str) + '_' + testfile['Date'].map(str),\n                         'Weekly_Sales':y_pred.flatten()})","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1bd4f1fe-f16f-4a35-8320-2baa3e33e385","_uuid":"a5549a653482844d45f3812edf87050c08b82cde","trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2d7c93a0-3885-433d-a25e-87fe8a0c7b07","_uuid":"ed0064926ffecd2c12f5c6917ce99a2508ee3832","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}