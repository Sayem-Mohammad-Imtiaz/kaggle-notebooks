{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Churn Prediction \n### A Machine Learning Model That Can Predict Customers Who Will Leave The Company\n\nThe aim is to predict whether a bank's customers leave the bank or not. If the Client has closed his/her bank account, he/she has left.\n\n## Dataset\n\n- **RowNumber:** corresponds to the record (row) number and has no effect on the output.\n- **CustomerId:** contains random values and has no effect on customer leaving the bank.\n- **Surname:** the surname of a customer has no impact on their decision to leave the bank.\n- **CreditScore:** can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n- **Geography:** a customer’s location can affect their decision to leave the bank.\n- **Gender:** it’s interesting to explore whether gender plays a role in a customer leaving the bank.\n- **Age:** this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n- **Tenure:** refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n- **Balance:** also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n- **NumOfProducts:** refers to the number of products that a customer has purchased through the bank.\n- **HasCrCard:** denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n- **IsActiveMember:** active customers are less likely to leave the bank.\n- **EstimatedSalary:** as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n- **Exited:** whether or not the customer left the bank.  (0=No,1=Yes)\n\n\n\n### The model created as a result of LightGBM hyperparameter optimization (0.867300)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, \\\n    classification_report\n\nfrom scipy.stats import shapiro\n\nimport warnings\nimport missingno as msno\n\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option(\"display.float_format\", lambda x: '%.2f' % x)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nlow_q1 = 0.05\nupper_q3 = 0.95\ncorrelation_limit = 0.60\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndef cat_summary(dataframe, categorical_columns, target, plot=False):\n    \"\"\"\n    -> Kategorik değişkenlerin sınıflarının oranını ve targettaki medyanı gösterir.\n\n    :param dataframe: İşlem yapılacak dataframe\n    :param categorical_columns: Kategorik değişkenlerin adları\n    :param target: Dataframe'de ilgilendiğimiz değişken.\n    :param plot: Grafik çizdirmek için argüman : True/False\n\n    \"\"\"\n    for col in categorical_columns:\n        print(col, \" : \", dataframe[col].nunique(), \" unique classes.\\n\")\n\n        print(col, \" : \", dataframe[col].value_counts().sum(), \"\\n\")\n\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO ( % )\": 100 * dataframe[col].value_counts() / len(dataframe),\n                            \"TARGET_MEDIAN\": dataframe.groupby(col)[target].median(),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n        if plot:\n            sns.countplot(x=col, data=dataframe)\n\n            plt.show()\n\n\ndef hist_for_numeric_columns(dataframe, numeric_columns):\n    \"\"\"\n    -> Sayısal değişkenlerin histogramını çizdirir.\n\n    :param dataframe: İşlem yapılacak dataframe.\n    :param numeric_columns: Sayısal değişkenlerin adları\n\n    \"\"\"\n    col_counter = 0\n\n    data = dataframe.copy()\n\n    for col in numeric_columns:\n        data[col].hist(bins=20)\n\n        plt.xlabel(col)\n\n        plt.title(col)\n\n        plt.show()\n\n        col_counter += 1\n\n    print(col_counter, \"variables have been plotted!\")\n\n\ndef find_correlation(dataframe, numeric_columns, target, corr_limit=correlation_limit):\n    \"\"\"\n    -> Sayısal değişkenlerin targetla olan korelasyonunu inceler.\n\n    :param dataframe: İşlem yapılacak dataframe\n    :param numeric_columns: Sayısal değişken adları\n    :param target: Korelasyon ilişkisinde bakılacak hedef değişken\n    :param corr_limit: Korelasyon sınırı. Sınırdan aşağısı düşük, yukarısı yüksek korelasyon\n    :return: İlk değer düşük korelasyona sahip değişkenler, ikinci değer yüksek korelasyona sahip değişkenler\n    \"\"\"\n    high_correlations = []\n\n    low_correlations = []\n\n    for col in numeric_columns:\n        if col == target:\n            pass\n\n        else:\n            correlation = dataframe[[col, target]].corr().loc[col, target]\n\n            if abs(correlation) > corr_limit:\n                high_correlations.append(col + \" : \" + str(correlation))\n\n            else:\n                low_correlations.append(col + \" : \" + str(correlation))\n\n    return low_correlations, high_correlations\n\n\ndef outlier_thresholds(dataframe, variable, low_quantile=low_q1, up_quantile=upper_q3):\n    \"\"\"\n    -> Verilen değerin alt ve üst aykırı değerlerini hesaplar ve döndürür.\n\n    :param dataframe: İşlem yapılacak dataframe\n    :param variable: Aykırı değeri yakalanacak değişkenin adı\n    :param low_quantile: Alt eşik değerin hesaplanması için bakılan quantile değeri\n    :param up_quantile: Üst eşik değerin hesaplanması için bakılan quantile değeri\n    :return: İlk değer olarak verilen değişkenin alt sınır değerini, ikinci değer olarak üst sınır değerini döndürür\n    \"\"\"\n    quantile_one = dataframe[variable].quantile(low_quantile)\n\n    quantile_three = dataframe[variable].quantile(up_quantile)\n\n    interquantile_range = quantile_three - quantile_one\n\n    up_limit = quantile_three + 1.5 * interquantile_range\n\n    low_limit = quantile_one - 1.5 * interquantile_range\n\n    return low_limit, up_limit\n\n\ndef has_outliers(dataframe, numeric_columns, plot=False):\n    \"\"\"\n    -> Sayısal değişkenlerde aykırı gözlem var mı?\n\n    -> Varsa isteğe göre box plot çizdirme görevini yapar.\n\n    -> Ayrıca aykırı gözleme sahip değişkenlerin ismini göndürür.\n\n    :param dataframe:  İşlem yapılacak dataframe\n    :param numeric_columns: Aykırı değerleri bakılacak sayısal değişken adları\n    :param plot: Boxplot grafiğini çizdirmek için bool değer alır. True/False\n    :return: Aykırı değerlere sahip değişkenlerin adlarını döner\n    \"\"\"\n    variable_names = []\n\n    for col in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, col)\n\n        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]\n\n            print(col, \" : \", number_of_outliers, \" aykırı gözlem.\")\n\n            variable_names.append(col)\n\n            if plot:\n                sns.boxplot(x=dataframe[col])\n                plt.show()\n\n    return variable_names\n\n\ndef replace_with_thresholds(dataframe, numeric_columns):\n    \"\"\"\n    Baskılama yöntemi\n\n    Silmemenin en iyi alternatifidir.\n\n    Loc kullanıldığından dataframe içinde işlemi uygular.\n\n    :param dataframe: İşlem yapılacak dataframe\n    :param numeric_columns: Aykırı değerleri baskılanacak sayısal değişkenlerin adları\n    \"\"\"\n    for variable in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n\n        dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n\n        dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\n\n\n\ndef one_hot_encoder(dataframe, categorical_columns, nan_as_category=False):\n    \"\"\"\n    Drop_first doğrusal modellerde yapılması gerekli\n\n    Ağaç modellerde gerekli değil ama yapılabilir.\n\n    dummy_na eksik değerlerden değişken türettirir.\n\n    :param dataframe: İşlem yapılacak dataframe\n    :param categorical_columns: One-Hot Encode uygulanacak kategorik değişken adları\n    :param nan_as_category: NaN değişken oluştursun mu? True/False\n    :return: One-Hot Encode yapılmış dataframe ve bu işlem sonrası oluşan yeni değişken adlarını döndürür.\n    \"\"\"\n    original_columns = list(dataframe.columns)\n\n    dataframe = pd.get_dummies(dataframe, columns=categorical_columns,\n                               dummy_na=nan_as_category, drop_first=False)\n\n    new_columns = [col for col in dataframe.columns if col not in original_columns]\n\n    return dataframe, new_columns\n\n\ndef rare_analyser(dataframe, categorical_columns, target, rare_perc):\n    \"\"\"\n     Data frame değişkenlerinin herhangi bir sınıfı, verilen eşik değerden düşük frekansa sahipse bu değişkenleri gösterir.\n\n    :param dataframe: İşlem yapılacak dataframe\n    :param categorical_columns: Rare analizi yapılacak kategorik değişken adları\n    :param target: Analizi yapılacak hedef değişken adı\n    :param rare_perc: Rare için sınır değer. Altında olanlar rare kategorisine girer.\n    :return:\n    \"\"\"\n    rare_columns = [col for col in categorical_columns\n                    if (dataframe[col].value_counts() / len(dataframe) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        print(var, \" : \", len(dataframe[var].value_counts()))\n\n        print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n                            \"RATIO\": dataframe[var].value_counts() / len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(var)[target].mean(),\n                            \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}),\n              end=\"\\n\\n\\n\")\n\n    print(len(rare_columns), \" adet rare sınıfa sahip değişken var.\")\n\n\n\ndef robust_scaler(variable):\n    var_median = variable.median()\n    quartile1 = variable.quantile(0.01)\n    quartile3 = variable.quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    if int(interquantile_range) == 0:\n        quartile1 = variable.quantile(0.05)\n        quartile3 = variable.quantile(0.95)\n        interquantile_range = quartile3 - quartile1\n        if int(interquantile_range) == 0:\n            quartile1 = variable.quantile(0.25)\n            quartile3 = variable.quantile(0.75)\n            interquantile_range = quartile3 - quartile1\n            z = (variable - var_median) / interquantile_range\n            return round(z, 3)\n\n        z = (variable - var_median) / interquantile_range\n        return round(z, 3)\n    else:\n        z = (variable - var_median) / interquantile_range\n    return round(z, 3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the data\ndf = pd.read_csv(\"../input/churn-analysis/churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verinin baştan ilk 5 gözlemi\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verinin sondan ilk 5 gözlemi\n\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kaç farklı müşteri var?\ndf[\"CustomerId\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Anlamsız değişkenlerin düşürülmesi\nneed_drops = [\"RowNumber\", \"CustomerId\", \"Surname\"]\ndf.drop(need_drops, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eksik gözlem kontrolü\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Betimsel istatistiklere bakalım\ndf.describe([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kategorik değişkenlerin hedefle ilişkisinin incelenmesi\ntemp_categorical = [\"Geography\", \"Gender\", \"Tenure\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\"]\ncat_summary(df, temp_categorical, \"Exited\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sayısal değişkenler için histogram incelenmesi\ntemp_numeric = [\"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\"]\nhist_for_numeric_columns(df, temp_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sayısal değişkenlerin normallik varsayımları\n# H0: Normal dağılım varsayımı sağlanmaktadır.\n# H1:... sağlanmamaktadır.\n\n# p - value < ise 0.05'ten HO RED.\n# p - value < değilse 0.05 H0 REDDEDİLEMEZ.\ncreditscore = df[[\"CreditScore\"]]\nbalance = df[[\"Balance\"]]\nsalary = df[[\"EstimatedSalary\"]]\nages = df[[\"Age\"]]\n\nshapiro(creditscore[\"CreditScore\"])[1] < 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro(balance[\"Balance\"])[1] < 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro(salary[\"EstimatedSalary\"])[1] < 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro(ages[\"Age\"])[1] < 0.05","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Bütün H0'lar reddedildi"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Korelasyonlara bakalım\nlow_corr_list, up_corr_list = find_correlation(df, temp_numeric, \"Exited\")\nprint(\"Low Corr List\")\nfor i in low_corr_list:\n    print(i)\n\nprint(\"High Corr List\")\nfor i in up_corr_list:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering\n\nbins = [15, 25, 40, 55, 100]\nnames = ['Young', 'Adult', 'Mature', 'Old']\ndf[\"NEW_Age_Range\"] = pd.cut(df['Age'], bins, labels=names)\n\n\n\nbins = [0, 2, 4, 6, 14]\nnames = ['New', 'Accustomed', 'Loyal', 'Constant']\ndf[\"NEW_Tenure_Status\"] = pd.cut(df['Tenure'], bins, labels=names)\n\n\nnames = ['CAT1', 'CAT2', 'CAT3', 'CAT4', 'CAT5']\ndf[\"NEW_CreditScore_Status\"] = pd.qcut(df['CreditScore'], 5, labels=names)\n\nnames = ['CAT1', 'CAT2', 'CAT3', 'CAT4', 'CAT5']\ndf[\"NEW_EstimatedSalary_Status\"] = pd.qcut(df['EstimatedSalary'], 5, labels=names)\n\n\ndf[\"NEW_Card_Member_Score\"] = df[\"HasCrCard\"] * df[\"IsActiveMember\"]\n\ndf[\"NEW_MemberStarts_Age\"] = df[\"Age\"] - df[\"Tenure\"]\n\n\nbins = [5, 25, 40, 55, 100]\nnames = ['Young', 'Adult', 'Mature', 'Old']\ndf[\"NEW_MemberStarts_Age_Range\"] = pd.cut(df[\"NEW_MemberStarts_Age\"], bins, labels=names)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategorical_columns = [\"Geography\", \"Gender\", \"NumOfProducts\",\n                       \"NEW_Age_Range\", \"NEW_Tenure_Status\",\n                       \"NEW_CreditScore_Status\", \"NEW_EstimatedSalary_Status\",\n                       \"NEW_MemberStarts_Age_Range\"]\n\nnumerical_columns = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\",\n                     \"EstimatedSalary\",\n                     \"NEW_Card_Member_Score\", \"NEW_MemberStarts_Age\"]\n\n\nrare_analyser(df, categorical_columns, \"Exited\", 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nadir sınıflar silinecek\ndf = df.loc[~((df[\"NumOfProducts\"] == 3) | (df[\"NumOfProducts\"] == 4))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot Encode\ndf, one_hot_columns = one_hot_encoder(df, categorical_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Robust Scale\nneed_scale_cols = [\"Balance\", \"EstimatedSalary\"]\nfor col in need_scale_cols:\n    df[col] = robust_scaler(df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe([0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ****************************************************************#\n# Models\nX = df.drop(\"Exited\", axis=1)\ny = np.ravel(df[[\"Exited\"]])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n\nrf_params = {\"max_depth\": [3, 5, 8],\n             \"max_features\": [8, 15, 25],\n             \"n_estimators\": [200, 500, 1000],\n             \"min_samples_split\": [2, 5, 10]}\n\nlgbm_params = {\"learning_rate\": [0.01, 0.1],\n               \"n_estimators\": [200, 500, 1000],\n               \"max_depth\": [3, 5, 8],\n               \"colsample_bytree\": [1, 0.8, 0.5],\n               \"num_leaves\": [32, 64, 128]}\n\nxgb_params = {\"learning_rate\": [0.1, 0.01],\n              \"max_depth\": [3, 5, 8],\n              \"n_estimators\": [200, 500, 1000],\n              \"colsample_bytree\": [0.7, 1]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state=123)\nlgbm = LGBMClassifier(random_state=123)\nxgb = XGBClassifier(random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_cv_rf = GridSearchCV(rf,\n                        rf_params,\n                        cv=10,\n                        n_jobs=-1,\n                        verbose=2).fit(X_train, y_train)\n\ngs_cv_lgbm = GridSearchCV(lgbm,\n                          lgbm_params,\n                          cv=10,\n                          n_jobs=-1,\n                          verbose=2).fit(X_train, y_train)\n\ngs_cv_xgb = GridSearchCV(xgb,\n                         xgb_params,\n                         cv=10,\n                         n_jobs=-1,\n                         verbose=2).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned = RandomForestClassifier(**gs_cv_rf.best_params_, random_state=123).fit(X_train, y_train)\n\nlgbm_tuned = LGBMClassifier(**gs_cv_lgbm.best_params_, random_state=123).fit(X_train, y_train)\n\nxgb_tuned = XGBClassifier(**gs_cv_xgb.best_params_, random_state=123).fit(X_train, y_train)\n\n# Accuracy results\nmodels = [(\"RF\", rf_tuned),\n          (\"LGBM\", lgbm_tuned),\n          (\"XGB\", xgb_tuned)]\n\nfor name, model in models:\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    msg = \"%s: (%f)\" % (name, acc)\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}