{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error\n\nimport time\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LabelEncoder + handle unknowns\n\nThis is an extended version of sklearn.preprocessing.LabelEncoder class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabelEncoderExt(object):\n    def __init__(self):\n        \"\"\"\n        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n        \"\"\"\n        self.label_encoder = LabelEncoder()\n        # self.classes_ = self.label_encoder.classes_\n\n    def fit(self, data_list):\n        \"\"\"\n        This will fit the encoder for all the unique values and introduce unknown value\n        :param data_list: A list of string\n        :return: self\n        \"\"\"\n        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n\n        return self\n\n    def transform(self, data_list):\n        \"\"\"\n        This will transform the data_list to id list where the new values get assigned to Unknown class\n        :param data_list:\n        :return:\n        \"\"\"\n        new_data_list = list(data_list)\n        for unique_item in np.unique(data_list):\n            if unique_item not in self.label_encoder.classes_:\n                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n\n        return self.label_encoder.transform(new_data_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train dataset\n\nThis is the train set - input for our model to be trained"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/black-friday-sales-prediction/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-processing\n\nProcessing and cleaning the data before using it for training the model.\n\n1. Data Imputation\n2. Data Cleaning\n3. One-Hot encoding - using pd.get_dummies()\n"},{"metadata":{},"cell_type":"markdown","source":"## Handling Missing Value\n\nIterativeImputer is an experimental feature in sklearn module.\nIt studies the other columns in the dataset and intelligently populates the missing values.\nThis is a smarter way to fill the missing values. Instead of filling with a single value in all the empty cells, this is a better approach to fill in considering various proportions."},{"metadata":{"trusted":true},"cell_type":"code","source":"ii = IterativeImputer(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def DataCleaning(df):\n    df['Product_Category_2'].fillna(0,inplace = True)\n    df['Product_Category_3'].fillna(0,inplace = True)\n    df['Product_Category_2'] = df['Product_Category_2'].astype(int) \n    df['Product_Category_3'] = df['Product_Category_3'].astype(int) \n\n    df['Gender'] = np.where(df['Gender']=='M', 1, 0)\n\n    df = pd.get_dummies(df, columns=['Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years',\n                                     'Product_Category_1', 'Product_Category_2', 'Product_Category_3'])\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = DataCleaning(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Product_Category_x fields"},{"metadata":{},"cell_type":"markdown","source":"### Some of my Observations\n\n* If you select a specific Product_ID and look up for its other occurrences in the dataset, you will notice each of the rows will have the same values in Product_Category_1, 2, and 3.\n* Product_Category_1 will be filled first. Only if Product_Category_1 is filled and there's need for more room, Product_Category_2 is used. Same for Product_Category_3. In other words, Product_Category_3 will never be filled keeping Product_Category_2 or Product_Category_1 empty.\n* These values here, are masked and represented numerically since we do not need to know the exact values. "},{"metadata":{},"cell_type":"markdown","source":"### Understanding through an analogy\n\nLet's look at a scenario where there's a User Details Dataset, and in the contact details section.\n- Many users will have only 1 mobile number. The next 2 fields shall be kept null.\n- However, if required, a single user-id can have more than one mobile numbers. That's where the next 2 fields come into picture.\n- And, Mobile_Number_3 will only be used if Mobile_Number_2 field is already populated.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Product_Category_2_1'] = 0\ndf['Product_Category_2_19'] = 0\ndf['Product_Category_2_20'] = 0\n\ndf['Product_Category_3_1'] = 0\ndf['Product_Category_3_2'] = 0\ndf['Product_Category_3_7'] = 0\ndf['Product_Category_3_19'] = 0\ndf['Product_Category_3_20'] = 0\n\n\ndf = df.drop('Product_Category_2_0', axis=1)\ndf = df.drop('Product_Category_3_0', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myL = ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myL2 = ['Product_Category_1_1', 'Product_Category_1_2', 'Product_Category_1_3', \n        'Product_Category_1_4', 'Product_Category_1_5', 'Product_Category_1_6',\n        'Product_Category_1_7', 'Product_Category_1_8', 'Product_Category_1_9', \n        'Product_Category_1_10', 'Product_Category_1_11', 'Product_Category_1_12',\n        'Product_Category_1_13', 'Product_Category_1_14', 'Product_Category_1_15',\n        'Product_Category_1_16', 'Product_Category_1_17', 'Product_Category_1_18',\n        'Product_Category_1_19', 'Product_Category_1_20', 'Product_Category_2_1',\n        'Product_Category_2_2', 'Product_Category_2_3', 'Product_Category_2_4',\n        'Product_Category_2_5', 'Product_Category_2_6', 'Product_Category_2_7',\n        'Product_Category_2_8', 'Product_Category_2_9', 'Product_Category_2_10',\n        'Product_Category_2_11', 'Product_Category_2_12', 'Product_Category_2_13',\n        'Product_Category_2_14', 'Product_Category_2_15', 'Product_Category_2_16',\n        'Product_Category_2_17', 'Product_Category_2_18', 'Product_Category_2_19',\n        'Product_Category_2_20', 'Product_Category_3_1', 'Product_Category_3_2',\n        'Product_Category_3_3', 'Product_Category_3_4', 'Product_Category_3_5',\n        'Product_Category_3_6', 'Product_Category_3_7', 'Product_Category_3_8',\n        'Product_Category_3_9', 'Product_Category_3_10', 'Product_Category_3_11',\n        'Product_Category_3_12', 'Product_Category_3_13', 'Product_Category_3_14',\n        'Product_Category_3_15', 'Product_Category_3_16', 'Product_Category_3_17',\n        'Product_Category_3_18', 'Product_Category_3_19', 'Product_Category_3_20']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in myL:\n    df4['Product_Category_'+i] = df['Product_Category_1_'+i] + df['Product_Category_2_'+i] + df['Product_Category_3_'+i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(myL2, axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.merge(df4, left_index=True, right_index=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = LabelEncoderExt()  \nlabel.fit(df['Product_ID'])\ndf['Product_ID'] = label.transform(df['Product_ID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heatmap\n\n... to show correlation factor"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = ( 20 , 15 )) \nsns.heatmap(df.corr(), cmap='cubehelix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.Purchase.values\n\nfeatures = ['User_ID', 'Product_ID', 'Gender', 'Marital_Status', 'Age_0-17',\n            'Age_18-25', 'Age_26-35', 'Age_36-45', 'Age_46-50', 'Age_51-55', 'Age_55+',\n            'Occupation_0', 'Occupation_1', 'Occupation_2', 'Occupation_3', 'Occupation_4',\n            'Occupation_5', 'Occupation_6', 'Occupation_7', 'Occupation_8', 'Occupation_9', 'Occupation_10',\n            'Occupation_11', 'Occupation_12', 'Occupation_13', 'Occupation_14', 'Occupation_15', \n            'Occupation_16', 'Occupation_17', 'Occupation_18', 'Occupation_19', 'Occupation_20',\n            'City_Category_A', 'City_Category_B', 'City_Category_C',\n            'Stay_In_Current_City_Years_0', 'Stay_In_Current_City_Years_1', 'Stay_In_Current_City_Years_2',\n            'Stay_In_Current_City_Years_3', 'Stay_In_Current_City_Years_4+',\n            'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Product_Category_4',\n            'Product_Category_5', 'Product_Category_6', 'Product_Category_7', 'Product_Category_8',\n            'Product_Category_9', 'Product_Category_10', 'Product_Category_11', 'Product_Category_12',\n            'Product_Category_13', 'Product_Category_14', 'Product_Category_15', 'Product_Category_16',\n            'Product_Category_17', 'Product_Category_18', 'Product_Category_19', 'Product_Category_20']\n\nX = df[features].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling Down\n\nUsing sklearn.preprocessing.StandardScaler to reduce the data items to smaller numeric values which in turn helps in conducting faster calculations for huge matrices."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nscaler = sc.fit(X)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomizedSearchCV\n\nUsing RandomizedSearchCV to effectively tune the hyperparamets and choose the best estimator configuration."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [500, 800, 1300]\n\n# Number of features to consider at every split\n# max_features = ['auto', 'sqrt', 80] # auto is best\n\n# Maximum number of levels in tree\n# max_leaf_nodes = [100, 1000, 2000, 5000]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [40, 60, 100]\n\n# # Minimum number of samples required at each leaf node\nmin_samples_leaf = [8, 10, 15]\n\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf\n              }\n\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Base Estimator configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr = RandomForestRegressor(random_state=42, verbose=True,\n                                     max_leaf_nodes=5000, \n#                                      min_samples_leaf=4, bootstrap=True, min_samples_split=15, \n#                                      max_depth=50, max_features=90, min_samples_split=2,\n#                                      n_estimators=400, max_features='auto',\n                                     n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\nCV_rfr = RandomizedSearchCV(estimator=rfr, param_distributions=random_grid, cv=2)\nCV_rfr.fit(X_train, y_train)\n\nprint(\"--- %s min ---\" % ((time.time() - start_time)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"best_estimator_\", CV_rfr.best_estimator_) \n# print(\"best_index_\", CV_rfr.best_index_) \nprint(\"best_params_\", CV_rfr.best_params_) \n# print(\"cv_results_\", CV_rfr.cv_results_) \nprint(\"get_params\", CV_rfr.get_params) \n# print(\"n_features_in_\", CV_rfr.n_features_in_) \n# print(\"n_splits_\", CV_rfr.n_splits_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr = CV_rfr.best_estimator_\n\n# Below is the best estimator achieved so far. \n# Feel free to throw in some comments if you think you can help us improve! \n# Would love to hear from you all.\n\n# rfr = RandomForestRegressor(max_leaf_nodes=5000, min_samples_split=60,\n#                             n_estimators=1300, n_jobs=4, random_state=42,\n#                             verbose=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nrfr.fit(X_train, y_train)\ny_pred = rfr.predict(X_test)\nprint(mean_squared_error(y_test, y_pred, squared=False))\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test dataset\n\nreading the Test dataset and preprocessing it in similar steps as the train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv(r'../input/black-friday-sales-prediction/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = DataCleaning(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Product_ID'] = label.transform(test['Product_ID']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Product_Category_x fields"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Product_Category_1_19'] = 0\ntest['Product_Category_1_20'] = 0\n\ntest['Product_Category_2_1'] = 0\ntest['Product_Category_2_19'] = 0\ntest['Product_Category_2_20'] = 0\n\ntest['Product_Category_3_1'] = 0\ntest['Product_Category_3_2'] = 0\ntest['Product_Category_3_7'] = 0\ntest['Product_Category_3_19'] = 0\ntest['Product_Category_3_20'] = 0\n\n\ntest = test.drop('Product_Category_2_0', axis=1)\ntest = test.drop('Product_Category_3_0', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in myL:\n    df4['Product_Category_'+i] = test['Product_Category_1_'+i] + test['Product_Category_2_'+i] + test['Product_Category_3_'+i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop(myL2, axis=1)\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.merge(df4, left_index=True, right_index=True)\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling down\n\nscaling down the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test2 = test[features].values\ntest2 = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting\n\nPredicting Purchase values for Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"output=pd.read_csv(r'../input/black-friday-sales-prediction/test.csv',usecols=['User_ID','Product_ID'])\noutput['Purchase']=rfr.predict(test2)\noutput2 = output[['Purchase','User_ID','Product_ID']]\noutput2.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### submission.csv is ready!\n\nSo far, we've achieved 2679 error (RMSE) on test dataset.\nWe appreciate if you can share your comments and help us improve on our model.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}