{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Clustering is a collection of unsupervised machine learning algorithms in which parts of\nthe data are grouped based on similarity. For example, clusters might consist of data that is\nclose together in n-dimensional Euclidean space. Clustering is useful in cybersecurity for\ndistinguishing between normal and anomalous network activity, and for helping to classify\nmalware into families","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/file-pe-headers/file_pe.csv\", sep=\",\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets plot the dataset","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_3d(\n    df,\n    x = \"SuspiciousImportFunctions\",\n    y = \"SectionsLength\",\n    z = \"SuspiciousNameSection\",\n    color = \"Malware\"\n)\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"extract the features and targe labels","metadata":{}},{"cell_type":"code","source":"y = df[\"Malware\"]\nX = df.drop([\"Name\", \"Malware\"], axis=1).to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"importing scikit-learns clustering module and fit a K means model with two clusters to the data","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nestimator = KMeans(n_clusters=len(set(y)))\nestimator.fit(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"predict the cluster using our trained algorithm","metadata":{}},{"cell_type":"code","source":"y_pred = estimator.predict(X)\ndf[\"pred\"] = y_pred\ndf[\"pred\"] = df[\"pred\"].astype(\"category\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets plot the clusters","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_3d(\n    df,\n    x = \"SuspiciousImportFunctions\",\n    y = \"SectionsLength\",\n    z = \"SuspiciousNameSection\",\n    color = \"pred\"\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start by importing our dataset of PE header information from a collection of samples\n(step 1). This dataset consists of two classes of PE files: malware and benign. We then use\nplotly to create a nice-looking interactive 3D graph (step 1). We proceed to prepare our\ndataset for machine learning. Specifically, in step 2, we set X as the features and y as the\nclasses of the dataset. Based on the fact that there are two classes, we aim to cluster the data\ninto two groups that will match the sample classification. We utilize the K-means algorithm\n(step 3), about which you can find more information at: https://en.wikipedia.org/wiki/\nK-means_clustering. With a thoroughly trained clustering algorithm, we are ready to\npredict on the testing set. We apply our clustering algorithm to predict to which cluster\neach of the samples should belong (step 4). Observing our results in step 5, we see that\nclustering has captured a lot of the underlying information, as it was able to fit the data\nwell","metadata":{}},{"cell_type":"markdown","source":"## Lets train an XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating one instance of an XGBooset model and training it on training set","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nXGB_model_instance = XGBClassifier()\n\nXGB_model_instance.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_test_pred = XGB_model_instance.predict(X_test)\naccuracy = accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We begin by reading in our data (step 1). We then create a train-test split (step 2). We\nproceed to instantiate an XGBoost classifier with default parameters and fit it to our\ntraining set (step 3). Finally, in step 4, we use our XGBoost classifier to predict on the\ntesting set. We then produce the measured accuracy of our XGBoost model's predictions","metadata":{}}]}