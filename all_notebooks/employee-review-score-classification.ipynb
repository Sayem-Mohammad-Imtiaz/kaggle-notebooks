{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk import pos_tag\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# add sentiment anaylsis columns\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n# create doc2vec vector columns\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('../input/train.csv')\ndf_test=pd.read_csv('../input/test.csv')\ndf_subm=pd.read_csv('../input/sample_submission.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":4,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30336 entries, 0 to 30335\nData columns (total 17 columns):\nID                30336 non-null int64\nPlace             30336 non-null object\nlocation          19082 non-null object\ndate              30336 non-null object\nstatus            30336 non-null object\njob_title         30336 non-null object\nsummary           30284 non-null object\npositives         30336 non-null object\nnegatives         30336 non-null object\nadvice_to_mgmt    17059 non-null object\nscore_1           27150 non-null float64\nscore_2           24286 non-null float64\nscore_3           27167 non-null float64\nscore_4           27145 non-null float64\nscore_5           26851 non-null float64\nscore_6           30336 non-null int64\noverall           30336 non-null float64\ndtypes: float64(6), int64(2), object(9)\nmemory usage: 3.9+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(5)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"          ID      Place   ...   score_6 overall\n23894  53353  startup_6   ...         2     1.0\n20760  46463  startup_2   ...         0     5.0\n6935   15710  startup_4   ...         0     4.0\n26263  58664  startup_6   ...         0     4.0\n17098  38421  startup_2   ...        10     4.0\n\n[5 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Place</th>\n      <th>location</th>\n      <th>date</th>\n      <th>status</th>\n      <th>job_title</th>\n      <th>summary</th>\n      <th>positives</th>\n      <th>negatives</th>\n      <th>advice_to_mgmt</th>\n      <th>score_1</th>\n      <th>score_2</th>\n      <th>score_3</th>\n      <th>score_4</th>\n      <th>score_5</th>\n      <th>score_6</th>\n      <th>overall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23894</th>\n      <td>53353</td>\n      <td>startup_6</td>\n      <td>Garden City, NY</td>\n      <td>May 21, 2016</td>\n      <td>Former Employee</td>\n      <td>Product Advisor</td>\n      <td>Started out great then became a miserable envi...</td>\n      <td>Play Xbox at work Part timers get vacation day...</td>\n      <td>No room to grow Products that don't work Drive...</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>20760</th>\n      <td>46463</td>\n      <td>startup_2</td>\n      <td>NaN</td>\n      <td>Dec 27, 2008</td>\n      <td>Current Employee</td>\n      <td>Systems Eng</td>\n      <td>Great place but forgettiing there roots!</td>\n      <td>Great place great people love to work here can...</td>\n      <td>hmmm work hard but heck what job is there you ...</td>\n      <td>Keep up the good work but please listen to the...</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>6935</th>\n      <td>15710</td>\n      <td>startup_4</td>\n      <td>NaN</td>\n      <td>May 19, 2017</td>\n      <td>Current Employee</td>\n      <td>Anonymous Employee</td>\n      <td>It's okay</td>\n      <td>Its not a bad company. Great benefits and pay....</td>\n      <td>It can be hostile and unprofessional. I dislik...</td>\n      <td>Learn how to effectively and professionally co...</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>26263</th>\n      <td>58664</td>\n      <td>startup_6</td>\n      <td>North Lauderdale, FL</td>\n      <td>Jul 17, 2013</td>\n      <td>Former Employee</td>\n      <td>Group Manager</td>\n      <td>This is a very good company to work for.</td>\n      <td>learning new things with the products</td>\n      <td>there aren't any cons for this company</td>\n      <td>Listen and take in to consideration what the e...</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>17098</th>\n      <td>38421</td>\n      <td>startup_2</td>\n      <td>Cupertino, CA</td>\n      <td>Aug 28, 2017</td>\n      <td>Former Employee</td>\n      <td>Anonymous Employee</td>\n      <td>Once-in-a-lifetime experience, but you pay for it</td>\n      <td>Good pay, benefits, and employee perks. Get to...</td>\n      <td>Big company with many different groups, so exp...</td>\n      <td>See Cons. Focus and efficiency is lacking in s...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>10</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"((30336, 17), (29272, 16))"},"metadata":{}}]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# for i in range(len(df_train.columns)):\n#     if i in [0,2,3,6,7,8,9]:\n#         pass\n#     else:\n#         print(df_train.iloc[:,i].value_counts())","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"ID                  int64\nPlace              object\nlocation           object\ndate               object\nstatus             object\njob_title          object\nsummary            object\npositives          object\nnegatives          object\nadvice_to_mgmt     object\nscore_1           float64\nscore_2           float64\nscore_3           float64\nscore_4           float64\nscore_5           float64\nscore_6             int64\noverall           float64\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"ID                    0\nPlace                 0\nlocation          11254\ndate                  0\nstatus                0\njob_title             0\nsummary              52\npositives             0\nnegatives             0\nadvice_to_mgmt    13277\nscore_1            3186\nscore_2            6050\nscore_3            3169\nscore_4            3191\nscore_5            3485\nscore_6               0\noverall               0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5']\nfor c in col:\n    df_train[c].fillna(df_train[c].dropna().median(), inplace=True)\n    df_test[c].fillna(df_train[c].dropna().median(), inplace=True)\n\ncol1 = ['negatives', 'summary', 'advice_to_mgmt']\nfor c in col1:\n    df_train[c].fillna('', inplace=True)\n    df_test[c].fillna('', inplace=True)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"((30336, 17), (29272, 16))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum().any(), df_test.isnull().sum().any()\n","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(True, True)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_test.isnull().sum()\n# location column has lots of Nan values. lets drop it.","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_col = ['ID', 'location', 'date']\ndf_train.drop(columns=drop_col, inplace=True)\ndf_test.drop(columns=drop_col, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Place'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OEncoder  = OrdinalEncoder()\nEnc_train = OEncoder.fit_transform(df_train[['Place', 'status']])\nEnc_test  = OEncoder.transform(df_test[['Place', 'status']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Enc_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Enc_train.shape, df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Enc_mapped = map(lambda x: x[0], Enc_train.tolist())\n# print(list(Enc_mapped))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_ENC(df, Enc):\n#   Create empty arrays with random elements with dimensions of the encoded column\n    Place_enc = np.empty((len(Enc),))  \n    Status_enc = np.empty((len(Enc),))\n    for i in range(len(Enc)):\n        Place_enc[i] = Enc[i][0]\n        Status_enc[i] = Enc[i][1]\n    df['place_enc'] = Place_enc\n    df['status_enc'] = Status_enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Create_ENC(df_train, Enc_train)\nCreate_ENC(df_test,  Enc_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum().any(), df_test.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('overall').Place.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('Place').overall.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.groupby('job_title').overall.count()\n# no information from this","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Review_len(df):\n    df['len_pos'] = df['positives'].str.len()\n    df['len_neg'] = df['negatives'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Review_len(df_train)\nReview_len(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ChangeToInt(df,col):\n    df[col]=df[col].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label='overall'\nChangeToInt(df_train,label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_wordcloud(data, title = None):\n    V_wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 200,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 7\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize = (20, 20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(V_wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print positive wordcloud\nshow_wordcloud(df_train[\"positives\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print negatives wordcloud\nshow_wordcloud(df_train[\"negatives\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print summary wordcloud\nshow_wordcloud(df_train[\"summary\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the lemmas of words from wordnet corpus reader\ndef get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    # lower text\n    text = text.lower()\n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    # remove stop words\n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    # remove empty and less than 3 length tokens\n    text = [t for t in text if len(t) >= 3]\n    # pos tag text\n    pos_tags = pos_tag(text)\n    # lemmatize text\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    # remove words with less than 3 letters\n    text = [t for t in text if len(t) >= 3]\n    # join all\n    text = \" \".join(text)\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Clean_positives\"] = df_train[\"positives\"].apply(lambda x: clean_text(x))\ndf_train[\"Clean_negatives\"] = df_train[\"negatives\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[\"Clean_positives\"] = df_test[\"positives\"].apply(lambda x: clean_text(x))\ndf_test[\"Clean_negatives\"] = df_test[\"negatives\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum().any(), df_test.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Clean_reviews\"] = df_train[\"Clean_positives\"]+' '+df_train[\"Clean_negatives\"]\ndf_test[\"Clean_reviews\"] = df_test[\"Clean_positives\"]+' '+df_test[\"Clean_negatives\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum().any(), df_test.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6']\n# for col in scores:\n#     print(df_train[col].value_counts())\n# # score_6 column doesn't have uniform realistic values. Ignore this column in analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_words(df):\n    df['num_words_pos'] = df['positives'].apply(lambda x: len(x.split()))\n    df['num_words_neg'] = df['negatives'].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words(df_train)\nnum_words(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def SIAscores(df):\n    SIA = SentimentIntensityAnalyzer()\n    df[\"sentiments\"] = df[\"Clean_reviews\"].apply(lambda x: SIA.polarity_scores(x))\n    return pd.concat([df.drop(['sentiments'], axis=1), df['sentiments'].apply(pd.Series)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SIA.polarity_scores returns a dictionary of 4 scores for each sentence.\n# {'neg': 0.0, 'neu': 0.404, 'pos': 0.596, 'comp': 0.7096}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = SIAscores(df_train)\ndf_test = SIAscores(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_cols=['place_enc','status_enc']\ntrain_d = pd.get_dummies(data=df_train, columns=dummy_cols)\ntest_d  = pd.get_dummies(data=df_test, columns=dummy_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_d.columns), len(test_d.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_D2V(df):\n    D2V = [col for col in df.columns if 'D2V_' in col]\n    df.drop(columns=D2V, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_D2V(train_d)\ndrop_D2V(test_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create doc2vec vector columns\ndef Create_Doc2Vec(df):\n    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df[\"Clean_reviews\"].apply(lambda x: x.split(\" \")))]\n    # train a Doc2Vec model with our text data\n    model = Doc2Vec(documents, size=10, window=2, min_count=2, workers=4)\n    # transform each document into a vector data\n    doc2vec_df = df[\"Clean_reviews\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n    doc2vec_df.columns = [\"D2V_\" + str(x) for x in doc2vec_df.columns]\n    return pd.concat([df, doc2vec_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d = Create_Doc2Vec(train_d)\ntest_d  = Create_Doc2Vec(test_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tfidf = TfidfVectorizer(min_df = 3)\n# tfidf_train = tfidf.fit_transform(df_train[\"Clean_reviews\"]).toarray()\n# tfidf_test  = tfidf.transform(df_test[\"Clean_reviews\"]).toarray()\n\n# def Create_TFIDF(df, tfidf_result):\n#     tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n#     tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n#     tfidf_df.index = df.index\n#     return pd.concat([df, tfidf_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train = Create_TFIDF(df_train, tfidf_train)\n# df_test  = Create_TFIDF(df_test,  tfidf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_d.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature selection\nlabel = \"overall\"\nignore_cols = [label, 'Place', 'status', 'job_title', 'summary', 'positives', 'negatives', 'advice_to_mgmt', \n               'score_6', 'Clean_positives', 'Clean_negatives', 'Clean_reviews']\nfeatures = [c for c in train_d.columns if c not in ignore_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(features), train_d.shape, test_d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_d[features]\ny = train_d[label]\nX_test = test_d[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del df_train, df_test\n# # , tfidf, tfidf_test, tfidf_train\n# # deleted to save RAM\n# import gc\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# who_ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, train_x, val_x, train_y, val_y):\n    model.fit(train_x,train_y)\n    pred_y = model.predict(val_x)\n    train_acc = model.score(train_x, train_y)\n    test_acc = accuracy_score(val_y, pred_y)\n    f1_sc = f1_score(val_y, pred_y, average='weighted')\n    return train_acc, test_acc, f1_sc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# StratifiedKFold and Model Training & Evaluation\nkfold = 6\nskf = StratifiedKFold(n_splits=kfold,shuffle=True,random_state=7)\nmodels = [\n          LogisticRegression(n_jobs=-1, random_state=6), \n          XGBClassifier(random_state=5, n_jobs=-1),\n          ExtraTreesClassifier(random_state=97, n_estimators=100, n_jobs=-1),\n          LGBMClassifier(objective='multiclass', random_state=5)\n         ]\nscores_df = pd.DataFrame(index=range(kfold * len(models)))\ndf_row = []\nfor i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n    X_train, X_val = X.loc[train_idx], X.loc[test_idx]\n    y_train, y_val = y.loc[train_idx], y.loc[test_idx]\n    print('[Fold: {}/{}]'.format(i + 1, kfold))\n    for model in models:\n        model_name = model.__class__.__name__\n        trn, acc, f1 = evaluate(model, X_train, X_val, y_train, y_val)\n        df_row.append((model_name, i, f1, acc, trn))\n        \nprint('Training Done!')\n\n#SVM model\n#     model_SVC = LinearSVC(random_state=7)\n# worse metric evaluation for SVM. Do not use it. \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df = pd.DataFrame(df_row, columns=['model_name', 'fold_idx', 'F1_score', 'Test_acc', 'Train_acc'])\nscores_df.sort_values(by=['model_name', 'fold_idx'], inplace=True)\nscores_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df.groupby(['model_name'])['F1_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Default parameters LogReg:\n>     Avg_CV_score: 0.4040081841492168, Avg_F1_score: 0.3763991552269393\n\nDefault parameters XGB:\n>     Avg_CV_score: 0.41343607919726366, Avg_F1_score: 0.39661000929134393\n\nDefault parameters DTC:\n>     Avg_CV_score: 0.3374539112115183, Avg_F1_score: 0.3376255378164512"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [ LogisticRegression(n_jobs=-1, random_state=0), XGBClassifier(random_state=5, n_jobs=-1) ]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n    model_name = model.__class__.__name__\n    f1_scores = cross_val_score(model, X, y, scoring='f1_weighted', cv=CV)\n    for cv_idx, f1 in enumerate(f1_scores):\n        entries.append((model_name, cv_idx, f1))\n\ncv_df = pd.DataFrame(entries, columns=['model_name', 'cv_idx', 'F1_score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_df.groupby(['model_name'])['F1_score'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> References: \n> > https://towardsdatascience.com/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e\n> > https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n> > https://towardsdatascience.com/mapping-word-embeddings-with-word2vec-99a799dc9695\n> > https://medium.com/explore-artificial-intelligence/word2vec-a-baby-step-in-deep-learning-but-a-giant-leap-towards-natural-language-processing-40fe4e8602ba\n> > https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n> > https://blog.insightdatascience.com/using-nlp-to-gain-insights-from-employee-review-data-da15687f311a"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}