{"cells":[{"metadata":{"_uuid":"ec300a456ca30079a79031fa520f677513f70660","_cell_guid":"f9e46493-c945-4388-bd19-cb43b7394293"},"cell_type":"markdown","source":"<h1 class='font-effect-3d' style='font-family:Akronim; color:#ffcc33;'>✍️ Styling, Links, Helpful Functions, and Modules</h1>\n\n---\n#### [GitHub Jupyter Notebook](https://github.com/OlgaBelitskaya/kaggle_notebooks/blob/master/kaggle_image_generator.ipynb) & [Online Version](https://olgabelitskaya.github.io/kaggle_image_generator.html)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%html\n<style> \n@import url('https://fonts.googleapis.com/css?family=Akronim|Roboto&effect=3d'); \na,h4 {color:slategray; font-family:Roboto; text-shadow:4px 4px 4px #aaa;}\nspan {color:black; font-family:Roboto; text-shadow:4px 4px 4px #aaa;}\ndiv.output_prompt,div.output_area pre {color:slategray;}\ndiv.input_prompt,div.output_subarea {color:#ffcc33;}      \ndiv.output_stderr pre {background-color:gainsboro;}  \ndiv.output_stderr {background-color:slategrey;}     \n</style>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import warnings; warnings.filterwarnings('ignore')\nimport pandas as pd,numpy as np,pylab as pl\nfrom PIL import ImageFile,Image\nimport math,tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"31030ea4b167df7f73e899d3e6d2bf102dff242d","_cell_guid":"6266e090-79ed-4116-a728-e151358349b8","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_batches(images,batch_size):\n    current_index=0\n    while current_index+batch_size<=images.shape[0]:\n        data_batch=images[current_index:current_index+batch_size]\n        current_index+=batch_size\n        yield data_batch/255-.5 \ndef images_square_grid(images,mode):\n    save_size=math.floor(np.sqrt(images.shape[0]))\n    images=(((images-images.min())*255)/\\\n            (images.max()-images.min())).astype(np.uint8)\n    images_in_square=np.reshape(images[:save_size*save_size],\n                                (save_size,save_size,images.shape[1],\n                                 images.shape[2],images.shape[3]))\n    if mode=='L':\n        images_in_square=np.squeeze(images_in_square,4)\n    new_im=Image.new(mode,(images.shape[1]*save_size,\n                           images.shape[2]*save_size))\n    for col_i,col_images in enumerate(images_in_square):\n        for image_i,image in enumerate(col_images):\n            im=Image.fromarray(image, mode)\n            new_im.paste(im,(col_i*images.shape[1],\n                             image_i*images.shape[2]))\n    return new_im","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24143aa868bae50d36b3d308ea1b8defb7bf060d","_cell_guid":"f891f962-348e-4aa5-888b-4ecee68070dc"},"cell_type":"markdown","source":"<h1 class='font-effect-3d' style='font-family:Akronim; color:#ffcc33;'>✍️  Data</h1>"},{"metadata":{"collapsed":true,"_uuid":"bdc484bac0b16c98cb632c9a4328b79939ffe78a","_cell_guid":"c2eadd37-f21e-462d-b6cb-0ea54b29f728","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/digit-recognizer/train.csv\")\nid_images=[\"%s%s\"%(\"pixel\",pixel_no) \n           for pixel_no in range(0,784)]\nimages=np.array(df_train[id_images])\nimages=images.astype('float32').reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f478257eacc8474185a4561a35cb36e9e3208e72","_cell_guid":"3c4a8358-1d49-4938-8f96-17c60ade3d81"},"cell_type":"markdown","source":"<h1 class='font-effect-3d' style='font-family:Akronim; color:#ffcc33;'>✍️  NN Model Construction </h1>"},{"metadata":{"collapsed":true,"_uuid":"941cffd7c67d16c841a06af414ec29338c1b5e8d","_cell_guid":"b2ac883a-5c80-49dc-a514-afff79db99d7","trusted":true},"cell_type":"code","source":"stddev=.01; alpha=0.1\nlearning_rate=.0002; beta1=.5 \nz_dim=128; batch_size=32; epochs=3","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1a38b2e91493640cb2165eb57404d89484be34fa","_cell_guid":"a490ac20-2481-4b05-9989-1e93e2ccc9dd","trusted":true},"cell_type":"code","source":"def nn_inputs(image_width,image_height,image_channels,z_dim):\n    input_real=tf.placeholder(tf.float32,name=\"Real_Input\",\n                              shape=[None,image_width,\n                                     image_height,image_channels]) # rank 4  \n    input_z=tf.placeholder(tf.float32,shape=[None,z_dim],\n                           name=\"Z_Input\") # rank 2    \n    input_learning_rate=tf.placeholder(tf.float32,shape=[],\n                                       name=\"Learning_Rate\") # rank 0    \n    return input_real,input_z,input_learning_rate\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"068c1737830aeb89d7279810bb31ca041a6ad9f2","_cell_guid":"6cdb7925-1560-4e6f-85fa-ae7cb348ae14","trusted":true},"cell_type":"code","source":"def nn_discriminator(images,reuse=False,alpha=alpha):\n    with tf.variable_scope('discriminator',reuse=reuse):\n        # image shape [28,28,3]\n        x=tf.layers.conv2d(images,32,5,strides=2,padding='same',\n                           kernel_initializer=\\\n                           tf.random_normal_initializer(stddev=stddev))\n        x=tf.maximum(x*alpha,x)         \n        # input shape [14,14,32]       \n        x=tf.layers.conv2d(x,96,5,strides=2,padding='same',\n                           kernel_initializer=\\\n                           tf.random_normal_initializer(stddev=stddev))\n        x=tf.maximum(x*alpha,x)        \n        x=tf.layers.batch_normalization(x,training=True)\n        # input shape [7,7,96]        \n        x=tf.layers.conv2d(x,128,5,strides=2,padding='same',\n                           kernel_initializer=\\\n                           tf.random_normal_initializer(stddev=stddev))\n        x=tf.maximum(x*alpha,x)       \n        x=tf.layers.batch_normalization(x,training=True)\n        # input shape [4,4,128]       \n        x=tf.reshape(x,(-1,4*4*128))        \n        discriminator_logits=tf.layers.dense(x,1)\n        discriminator_outputs=tf.sigmoid(discriminator_logits)        \n    return discriminator_outputs,discriminator_logits","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6a5b7b72c20a736f90325f8b56ae95f3b0739fac","_cell_guid":"741a0490-d690-4fef-86a0-0cf6eebac1e3","trusted":true},"cell_type":"code","source":"def nn_generator(z,out_channel_dim,is_train=True,alpha=alpha):\n    with tf.variable_scope('generator',reuse=(not is_train)):        \n        x=tf.layers.dense(z,7*7*128)\n        x=tf.reshape(x,(-1,7,7,128))\n        x=tf.maximum(x*alpha,x)          \n        x=tf.layers.batch_normalization(x,training=is_train)      \n        # input shape [7,7,128]       \n        x=tf.layers.conv2d_transpose(x,96,5,strides=2,padding='same',\n                                     kernel_initializer=\\\n                                     tf.random_normal_initializer(stddev=stddev))\n        x=tf.maximum(x*alpha,x)          \n        x=tf.layers.batch_normalization(x,training=is_train)\n        # input shape [14,14,96]\n        x=tf.layers.conv2d_transpose(x,32,5,strides=2,padding='same',\n                                     kernel_initializer=\\\n                                     tf.random_normal_initializer(stddev=stddev))\n        x=tf.maximum(x*alpha,x)              \n        x=tf.layers.batch_normalization(x,training=is_train)\n        # input shape [28,28,32]        \n        generator_logits=\\\n        tf.layers.conv2d_transpose(x,out_channel_dim,3,strides=1,padding='same', \n                                   kernel_initializer=\\\n                                   tf.random_normal_initializer(stddev=stddev))\n        # input shape [28,28,3]         \n        generator_outputs=tf.tanh(generator_logits)   \n    return generator_outputs    ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"817121ef4c9bfcf78fa6df02e49171af69cb2c58","_cell_guid":"81193775-7b59-4bc1-8226-c87e671ef1c4","trusted":true},"cell_type":"code","source":"def nn_loss(input_real,input_z,out_channel_dim,\n            alpha=alpha,index=\"leaky_relu\"):    \n    if index==\"relu\":\n        img_generator=generator(input_z,out_channel_dim,\n                                is_train=True,alpha=alpha)\n        real_outputs,real_logits=\\\n        discriminator(input_real,reuse=False,alpha=alpha)\n        generator_outputs,generator_logits=\\\n        discriminator(img_generator,reuse=True,alpha=alpha)\n    elif index==\"leaky_relu\":\n        img_generator=nn_generator(input_z,out_channel_dim,\n                                   is_train=True,alpha=alpha)\n        real_outputs,real_logits=\\\n        nn_discriminator(input_real,alpha=alpha)\n        generator_outputs,generator_logits=\\\n        nn_discriminator(img_generator,reuse=True,alpha=alpha)          \n    real_labels=tf.ones_like(real_outputs)*(1-alpha)\n    zeros_labels=tf.zeros_like(generator_outputs)\n    ones_labels=tf.ones_like(generator_outputs)\n    real_loss=tf.reduce_mean(tf.nn\\\n    .sigmoid_cross_entropy_with_logits(logits=real_logits,\n                                       labels=real_labels))\n    zeros_loss=tf.reduce_mean(tf.nn\\\n    .sigmoid_cross_entropy_with_logits(logits=generator_logits,\n                                       labels=zeros_labels))\n    ones_loss=tf.reduce_mean(tf.nn\\\n    .sigmoid_cross_entropy_with_logits(logits=generator_logits,\n                                       labels=ones_labels))\n    return real_loss+zeros_loss,ones_loss","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b167d7d88f039fb3f23061213da0aa8dd1c7e396","_cell_guid":"b64c87e0-c651-4282-8352-f1c52f150185","trusted":true},"cell_type":"code","source":"def nn_optimizer(discriminator_loss,generator_loss,\n                 learning_rate,beta1):\n    trainable_variables=tf.trainable_variables()\n    discriminator_trainable_variables=\\\n    [v for v in trainable_variables \n     if v.name.startswith('discriminator')]\n    generator_trainable_variables=\\\n    [v for v in trainable_variables \n     if v.name.startswith('generator')]   \n    update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    discriminator_update_ops=\\\n    [u for u in update_ops \n     if u.name.startswith('discriminator')]\n    generator_update_ops=\\\n    [u for u in update_ops \n     if u.name.startswith('generator')]\n    with tf.control_dependencies(discriminator_update_ops):\n        discriminator_training_operations=\\\n        tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1)\\\n        .minimize(discriminator_loss,\n                  var_list=discriminator_trainable_variables)\n    with tf.control_dependencies(generator_update_ops):\n        generator_training_operations=\\\n        tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1)\\\n        .minimize(generator_loss,\n                  var_list=generator_trainable_variables)           \n    return discriminator_training_operations,generator_training_operations","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4445b521910dd3fa70f7a9b79bd473b77ec111dd","_cell_guid":"edc430b1-1df9-4e05-a433-bd3b1f14572a","trusted":true},"cell_type":"code","source":"def train(images,epoch_count,batch_size,\n          z_dim,learning_rate,beta1, \n          data_shape,data_image_mode,\n          print_step,show_step):    \n    input_real,input_z,input_learning_rate = \\\n    nn_inputs(data_shape[1],data_shape[2],\n              data_shape[3],z_dim)   \n    discriminator_loss,generator_loss=\\\n    nn_loss(input_real,input_z,data_shape[3],\n            alpha=alpha,index=\"leaky_relu\")  \n    discriminator_training_operations,generator_training_operations=\\\n    nn_optimizer(discriminator_loss,generator_loss,\n                 learning_rate,beta1)   \n    train_step=0; DTL,GTL=[],[]    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for epoch_i in range(epoch_count):\n            for batch_images in get_batches(images,batch_size):\n                train_step+=1\n                batch_images*=2.0                \n                batch_z=np.random.uniform(-1,1,size=(batch_size,z_dim))               \n                _=sess.run(discriminator_training_operations, \n                           feed_dict={input_real:batch_images,\n                                      input_z:batch_z})\n                _=sess.run(generator_training_operations, \n                           feed_dict={input_z:batch_z})\n                if train_step%print_step==0:\n                    discriminator_training_loss=\\\n                    discriminator_loss.eval({input_real:batch_images,\n                                             input_z:batch_z})\n                    generator_training_loss=\\\n                    generator_loss.eval({input_z:batch_z})\n                    print(\"Epoch {}/{}| Step {}|\"\\\n                          .format(epoch_i+1,epochs,train_step),\n                          \"Discriminator Loss:{:.5f}|\"\\\n                          .format(discriminator_training_loss),\n                          \"Generator Loss:{:.5f}|\"\\\n                          .format(generator_training_loss),\n                          \"Discriminator Loss>Generator Loss: {}\"\\\n                          .format(discriminator_training_loss>generator_training_loss))                   \n                    DTL.append(discriminator_training_loss) \n                    GTL.append(generator_training_loss)                    \n                if train_step%show_step==0:\n                    cmap=None if data_image_mode=='RGB' else 'gray'\n                    z_dim=input_z.get_shape().as_list()[-1]\n                    example_z=np.random.uniform(-1,1,size=[25,z_dim])\n                    samples=sess.run(nn_generator(input_z,data_shape[3],False),\n                                     feed_dict={input_z:example_z})\n                    images_grid=images_square_grid(samples,data_image_mode)\n                    pl.figure(figsize=(5,5))\n                    pl.imshow(np.array(images_grid),cmap=cmap); pl.show()                            \n    pl.figure(figsize=(14,6))\n    pl.plot(DTL,label='discriminator')\n    pl.plot(GTL,label='generator')\n    pl.legend(); pl.title('Loss Function'); pl.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7967d8bf2d8672c37bab45b7cfc8e6aa1d529f00","_cell_guid":"3ae8d2de-ca05-4254-864b-8189f3377156"},"cell_type":"markdown","source":"<h1 class='font-effect-3d' style='font-family:Akronim; color:#ffcc33;'>✍️ Image Generating </h1>"},{"metadata":{"_uuid":"8318064726bc30f187eba52bb0932a858f64d5f6","_cell_guid":"8b57295d-a8b9-4424-81f8-d63a6bb21c17","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"with tf.Graph().as_default():\n    train(images,epochs,batch_size, \n          z_dim,learning_rate,beta1, \n          images.shape,\"L\",100,1000)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"file_extension":".py","version":"3.6.3","name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}