{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install wordninja\nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport emoji\nimport spacy\nimport wordninja #Someone else implemented this ->  https://github.com/keredson/wordninja\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport xgboost\nfrom xgboost import XGBClassifier\npd.set_option('display.max_colwidth', 135)\n#!python -m spacy download en\nnlp =spacy.load('en_core_web_sm')\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_file = '../input/airline-sentiment/Tweets.csv'\ndata=pd.read_csv(tweets_file)\nprint (\"data_shape: \", data.shape)\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean and tokenize tweet texts"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def clean_and_tokenize_tweets(input_text):\n    \n    #remove_mentions, urls, hash_sign:\n    mention_words_removed= re.sub(r'@\\w+','',input_text)\n    hash_sign_removed=re.sub(r'#','',mention_words_removed)\n    url_removed=' '.join(word for word in hash_sign_removed.split(\" \") if not word.startswith('http'))\n    \n    #Transform emoji to text\n    demoj=emoji.demojize(url_removed)\n    \n    #Split compound words coming from hashtags\n    splitted=wordninja.split(demoj)\n    splitted=\" \".join(word for word in splitted)\n    \n    # Implement lemmatization & remove punctuation\n    lem = nlp(splitted)\n    punctuations = string.punctuation\n    punctuations=punctuations+'...'\n\n    sentence=[]\n    for word in lem:\n        word = word.lemma_.lower().strip()\n        if ((word != '-pron-') & (word not in punctuations)):\n            sentence.append(word)    \n            \n    #Remove stopwords\n    stop_words=set(stopwords.words('english'))\n    stop_words_removed=[word for word in sentence if word not in stop_words]\n    \n    return stop_words_removed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"text_\"]=data[\"text\"].apply(clean_and_tokenize_tweets)\ndata[[\"text\",\"text_\"]].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create tfidf vectorizer for text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"text_\"]=[\" \".join(word) for word in data[\"text_\"]]\nX_train, X_test=data[\"text_\"][:10000],data[\"text_\"][10000:]\ntfidf_vector = TfidfVectorizer()\nX_train=tfidf_vector.fit_transform(X_train)\nX_test = tfidf_vector.transform(X_test)\nylabels=data[\"airline_sentiment\"].map({\"negative\":-1,\"neutral\":0,\"positive\":1})\ny_train, y_test=ylabels[:10000],ylabels[10000:] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"## First model- Multinomial logistics regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_classifier=LogisticRegression(random_state=0, solver='lbfgs',max_iter=500,multi_class='multinomial').fit(X_train,y_train)\npred_train_base=base_classifier.predict(X_train)\npred_test_base=base_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression Train Accuracy:\",np.round(metrics.accuracy_score(y_train, pred_train_base),4))\nprint(\"Logistic Regression Test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_base),4))\nprint(\"\")\nprint(\"Logistic Regression Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_base,labels=[-1, 0, 1]),sep=\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------------------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"* ## Second model- GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = GradientBoostingClassifier(n_estimators=180, max_depth=6, random_state=0,learning_rate=0.1)\ngbm.fit(X_train, y_train)\npred_train_gbm=gbm.predict(X_train)\npred_test_gbm=gbm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LGBM Train Accuracy:\",np.round(metrics.accuracy_score(y_train, pred_train_gbm),4))\nprint(\"LGBM Test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_gbm),4))\nprint(\"\")\nprint(\"LGBM Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_gbm,labels=[-1, 0, 1]),sep=\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------------------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"## Third model- XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_classifier = XGBClassifier(n_estimators=200,random_state=0,learning_rate=0.7,objective='multi:softprob',num_class=3)\nxgb_classifier.fit(X_train, y_train)\npred_train_xgb=xgb_classifier.predict(X_train)\npred_test_xgb=xgb_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"XGBoost train Accuracy:\",metrics.accuracy_score(y_train, pred_train_xgb))\nprint(\"XGBoost test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_xgb),4))\nprint(\"\")\nprint(\"XGBoost Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_xgb,labels=[-1, 0, 1]),sep=\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------------------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"## Fourth Model - CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"catboost_classifier = CatBoostClassifier(iterations=500, learning_rate=0.5, l2_leaf_reg=3.5, depth=8, rsm=0.98, eval_metric='AUC',use_best_model=True,random_seed=42,loss_function='MultiClass')\ncatboost_classifier.fit(X_train,y_train,eval_set=(X_test,y_test))\npred_train_catb = catboost_classifier.predict(X_train)\npred_test_catb = catboost_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"CatBoost train Accuracy:\",metrics.accuracy_score(y_train, pred_train_catb))\nprint(\"CatBoost test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_catb),4))\nprint(\"\")\nprint(\"CatBoost Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_catb,labels=[-1, 0, 1]),sep=\"\\n\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}