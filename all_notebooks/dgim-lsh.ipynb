{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom collections import deque\nimport time\nfrom copy import deepcopy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Look up data","metadata":{}},{"cell_type":"code","source":"# Read data and transform to integer\nfile = \"../input/coding2/stream_data.txt\"\nf = open(file, \"r\")\norg_data = f.read()\nprint(f\"type of data: {type(org_data)}\")\norg_data = org_data.split('\\t')\norg_data = [int(d) for d in org_data if d != '']\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(org_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define DGIM","metadata":{}},{"cell_type":"code","source":"class DGIM:\n    def __init__(self, window_length=1000, bucket_size=2):\n        self.window_length = window_length\n        self.bucket_size = bucket_size    # allow either s-1 or s buckets\n        # self.key_number = np.ceil(np.log2(self.window_length))\n        self.key_number = np.floor( np.log2(self.window_length / self.bucket_size) ) + 1\n        self.buckets_keys = np.arange(start=0, stop=self.key_number, step=1).tolist()\n        self.buckets_keys_reversed = np.arange(start=self.key_number-1, stop=-1, step=-1).tolist()\n        \n        self.reset()\n        \n    \n    def reset(self):\n        # initialize the buckets\n        self.buckets = {}                 # buckets to store indexes\n        for key in self.buckets_keys:\n            self.buckets[key] = deque( maxlen = self.bucket_size + 1 )  # +1 to tolerate s+1 buckets\n        self.buckets[self.key_number-1] = deque( maxlen = self.bucket_size )   # last bucket only tolerate s buckets\n    \n        self.current_stamp = -1    # record time stamp\n    \n    \n    def stream(self, bit):\n        self.current_stamp = (self.current_stamp + 1) % self.window_length\n        \n        # remove bits out of windows\n        for key in self.buckets_keys_reversed:\n            if len(self.buckets[key]) > 0:\n                # first_stamp = self.buckets[key][-1]\n                # normally, last_bucket_end_stamp small than last_bucket_start_stamp\n                # last_bucket_end_stamp = self.buckets[key][-1]\n                \"\"\"\n                last_bucket_start_stamp = (self.buckets[key][-2] + 1) % self.window_length\n                if last_bucket_start_stamp == self.current_stamp:\n                    self.buckets[key].pop()\n                \"\"\"\n                last_bucket_end_stamp = self.buckets[key][-1]\n                if last_bucket_end_stamp == self.current_stamp:\n                    self.buckets[key].pop()\n                    # print(f\"removed: {last_bucket_end_stamp}\")\n                \n                break\n        \n        if bit == 1:\n            self.append(self.current_stamp)\n        \n    \n    def append(self, stamp):\n        self.buckets[0].appendleft(stamp)\n        for key in self.buckets_keys:\n            if (len(self.buckets[key]) == self.bucket_size + 1) and (key + 1 < self.key_number):\n                self.buckets[key+1].appendleft(self.buckets[key].pop())\n                self.buckets[key].pop()    # throw away\n            else:\n                break\n                \n    def count(self, window_length=None):\n        if window_length is None:\n            window_length = self.window_length\n        \"\"\"\n        if len(self.buckets[0]) == 0:\n            print(\"The buckets is empty now!\")\n            return 0\n        \"\"\"\n        \n        cnt = 0\n        for key in self.buckets_keys:\n            for stamp in self.buckets[key]:\n                index = (self.current_stamp - stamp + self.window_length) % self.window_length   # in case that current stamp start from beginning again\n                if index < window_length:\n                    cnt += 2**key\n                else:\n                    cnt += 2**key * 0.5    # number estimated for cnt\n                    cnt = min(cnt, window_length)\n                    return cnt\n        cnt = min(cnt, window_length)\n        return cnt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiate DGIM","metadata":{}},{"cell_type":"code","source":"window_length = 1000\nbucket_size = 2\nDGIM_handler = DGIM(window_length=window_length, bucket_size=bucket_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test DGIM","metadata":{}},{"cell_type":"code","source":"n = 560\nfor _ in range(n):\n    DGIM_handler.stream(1)\nDGIM_cnt = DGIM_handler.count()\nprint(f\"number of bit 1 actually: {n}, number of bit 1 DGIM counts: {DGIM_cnt}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DGIM_handler.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare data\ndata = deepcopy(org_data)\n\nstart_time = time.time()\n\nwhile data != []:\n    bit = data.pop(0)\n    DGIM_handler.stream(bit)\n    if DGIM_handler.current_stamp == window_length - 1:\n        DGIM_cnt = DGIM_handler.count()\n        print(f\"number of bit 1 DGIM counts: {DGIM_cnt}\")\n        \nend_time = time.time()\ntotal_time = end_time - start_time\nprint (f\"total time normal counter spends: {total_time}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"window_size = 1000\nwindows = deque(maxlen=window_size)\ncurrent_stamp = -1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare data\ndata = deepcopy(org_data)\n\nstart_time = time.time()\n\nwhile data != []:\n    bit = data.pop(0)\n    windows.appendleft(bit)\n    current_stamp = (current_stamp + 1) % window_size\n    if current_stamp == window_size - 1:\n        NORMAL_cnt = np.sum(windows)\n        print(f\"number of bit 1 normal counter counts: {NORMAL_cnt}\")\n\nend_time = time.time()\ntotal_time = end_time - start_time\nprint (f\"total time normal counter spends: {total_time}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Increase bucket size","metadata":{}},{"cell_type":"code","source":"window_length = 1000\nbucket_size = 10\nDGIM_handler = DGIM(window_length=window_length, bucket_size=bucket_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare data\ndata = deepcopy(org_data)\n\nstart_time = time.time()\n\nwhile data != []:\n    bit = data.pop(0)\n    DGIM_handler.stream(bit)\n    if DGIM_handler.current_stamp == window_length - 1:\n        DGIM_cnt = DGIM_handler.count()\n        print(f\"number of bit 1 DGIM counts: {DGIM_cnt}\")\n        \nend_time = time.time()\ntotal_time = end_time - start_time\nprint (f\"total time normal counter spends: {total_time}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"可以看到，增加 bucket_size 后，误差确实变小了很多！","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"markdown","source":"## Input libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = \"../input/coding2/docs_for_lsh.csv\"\ndata = pd.read_csv(file)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(data.columns)\nprint(f\"Before delete: {data.shape}\")\ndata = data.drop('doc_id', axis=1)      # NOTE: 不是 in-place 操作；axis=1 表示列，axis=0 表示行\nprint(f\"After delete: {data.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The min_hashing function\ndef minHashing(shingles,  signature_number):\n    shingle_number, file_number = shingles.shape\n    # shingles， signatures\n    signatures = np.ones(shape=[signature_number, file_number]) * shingle_number\n    \n    for signature in range(signature_number):\n        hash_a = np.random.randint(1000)\n        hash_b = np.random.randint(1000)\n        hash_p = 10000019    # a prime number\n        print(f\"Process signature: {signature}\\t\", end=\"\\t\")\n        if signature % 5 == 4:\n            print(f\"\\n\", end='')\n        for file in range(file_number):\n            # for shingle in range(shingle_number):\n            \"\"\"\n            for shingle in np.where(shingles[str(file)])[0]:\n                index = ((hash_a * shingle + hash_b) % hash_p) % shingle_number\n                if signatures[signature, file] < index:\n                    signatures[signature, file] = shingle\n            \"\"\"\n            #  本质上就是只计算为 1 的位置，不是 1 的位置不计算\n            indexes = np.where(shingles[str(file)])[0]\n            hash_indexes = ((hash_a * indexes + hash_b) % hash_p) % shingle_number\n            signatures[signature, file] = hash_indexes.min()\n            \n    return signatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The LSH hashing for bands\ndef LSHHash(signatures, row_number):\n    signature_number, file_number = signatures.shape\n    band_number = signature_number // row_number\n    LSH_hash = np.zeros(shape=[band_number, file_number])\n    hash_add = np.random.randint(low=1, high=3)\n    \n    for band in range(band_number):\n        for file in range(file_number):\n            band_signature = signatures[band*row_number : band*row_number+row_number, file]\n            # product_item = round(np.product(band_signature + hash_add) ** (1/row_number))    # hashing\n            # product_item = int(np.cumproduct(band_signature**2 + hash_add).mean() ** (1/row_number))\n            sum_item = int(np.cumsum(band_signature**2).mean())\n            # LSH_hash[band, file] = product_item + sum_item\n            LSH_hash[band, file] = sum_item\n    return LSH_hash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_Jaccard_similarity(fileA, fileB):\n    # AandB = dA and dB\n    AandB = np.bitwise_and(fileA, fileB).sum()\n    # AorB = dA or dB\n    AorB = np.bitwise_or(fileA, fileB).sum()\n    Jaccard_similarity_A_B = AandB / AorB\n    # print(f\"Jaccard similarity for file A and file B: {Jaccard_similarity_A_B}\")\n    # print(AandB, AorB)\n    return Jaccard_similarity_A_B","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signature_number = 200\nstart_time = time.time()\nsignatures = minHashing(data,  signature_number)\nend_time = time.time()\nprint(f\"total time: {end_time - start_time}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_number = 5    # band_number = signature_number // row_number\nstart_time = time.time()\nLSH_hash =  LSHHash(signatures, row_number)\nend_time = time.time()\nprint(f\"total time: {end_time - start_time}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LSH_hash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"similar_bools = np.any(np.repeat(a=LSH_hash[:,0].reshape(-1, 1), repeats=LSH_hash.shape[1]-1, axis=1) == LSH_hash[:,1:], axis=0)\nsimilar_numbers = similar_bools.sum()\nprint(f\"similar numbers: {similar_numbers}\")\nsimilar_file_indexes = np.where(similar_bools)[0] + 1\nprint(f\"similar files: {similar_file_indexes}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fileA_index = 0\nfileA = data[str(fileA_index)]\n# for fileB_index in data.columns:\nfor fileB_index in similar_file_indexes:\n    fileB = data[str(fileB_index)]\n    Jaccard_similarity_A_B = get_Jaccard_similarity(fileA, fileB)\n    # Jaccard_similarity_A_B = jaccard_score(fileA, fileB)    # 速度很慢，不如自己算\n    print(f\"Jaccard similarity for file {fileA_index} and file {fileB_index}: {round(Jaccard_similarity_A_B, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fileA_index = 0\nfileA = data[str(fileA_index)]\nfor fileB_index in data.columns:\n# for fileB_index in similar_file_indexes:\n    fileB = data[str(fileB_index)]\n    Jaccard_similarity_A_B = get_Jaccard_similarity(fileA, fileB)\n    # Jaccard_similarity_A_B = jaccard_score(fileA, fileB)    # 速度很慢，不如自己算\n    print(f\"Jaccard similarity for file {fileA_index} and file {fileB_index}: {round(Jaccard_similarity_A_B, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fileA_index = 0\nfileA = signatures[:, fileA_index]\n# for fileB_index in data.columns:\nfor fileB_index in similar_file_indexes:\n    fileB = signatures[:, fileB_index]\n    Jaccard_similarity_A_B = (fileA == fileB).sum() / len(fileA)\n    print(f\"Jaccard similarity for file {fileA_index} and file {fileB_index}: {round(Jaccard_similarity_A_B, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fileA_index = 0\nfileA = signatures[:, fileA_index]\nfor fileB_index in range(1, signatures.shape[1]):\n# for fileB_index in similar_file_indexes:\n    fileB = signatures[:, fileB_index]\n    Jaccard_similarity_A_B = (fileA == fileB).sum() / len(fileA)\n    print(f\"Jaccard similarity for file {fileA_index} and file {fileB_index}: {round(Jaccard_similarity_A_B, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}