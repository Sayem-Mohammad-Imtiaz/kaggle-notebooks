{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:08:52.164245Z","iopub.execute_input":"2021-07-09T16:08:52.164752Z","iopub.status.idle":"2021-07-09T16:08:52.633531Z","shell.execute_reply.started":"2021-07-09T16:08:52.164639Z","shell.execute_reply":"2021-07-09T16:08:52.632286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\n\nnltk.download('punkt') # one time execution\nnltk.download('stopwords')# one time execution","metadata":{"id":"WeptlGXN2MnF","outputId":"781281df-51c3-4729-fe6c-7c8401925582","execution":{"iopub.status.busy":"2021-07-09T16:08:52.635375Z","iopub.execute_input":"2021-07-09T16:08:52.63581Z","iopub.status.idle":"2021-07-09T16:08:54.752838Z","shell.execute_reply.started":"2021-07-09T16:08:52.635766Z","shell.execute_reply":"2021-07-09T16:08:54.752032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file\ndf = pd.read_csv('/kaggle/input/nlp-specialization-data/tennis_articles_v4.csv')\nprint(df.shape)\ndf.head(3)","metadata":{"id":"_5FvQ9LHGtja","execution":{"iopub.status.busy":"2021-07-09T16:08:54.754566Z","iopub.execute_input":"2021-07-09T16:08:54.754863Z","iopub.status.idle":"2021-07-09T16:08:54.80874Z","shell.execute_reply.started":"2021-07-09T16:08:54.754819Z","shell.execute_reply":"2021-07-09T16:08:54.807745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the the text in the articles into sentences\nsentences = []\nfor s in df['article_text']:\n    sentences.append(sent_tokenize(s))  ","metadata":{"id":"eZVoc3R6G9a8","execution":{"iopub.status.busy":"2021-07-09T16:08:54.810613Z","iopub.execute_input":"2021-07-09T16:08:54.811027Z","iopub.status.idle":"2021-07-09T16:08:54.839255Z","shell.execute_reply.started":"2021-07-09T16:08:54.810984Z","shell.execute_reply":"2021-07-09T16:08:54.83811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flatten the list\nsentences = [y for x in sentences for y in x]","metadata":{"id":"V_lwimHsHB9l","execution":{"iopub.status.busy":"2021-07-09T16:08:54.840849Z","iopub.execute_input":"2021-07-09T16:08:54.841168Z","iopub.status.idle":"2021-07-09T16:08:54.845969Z","shell.execute_reply.started":"2021-07-09T16:08:54.841134Z","shell.execute_reply":"2021-07-09T16:08:54.844558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove punctuations, numbers and special characters\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","metadata":{"id":"iHvW8lKFHV1x","execution":{"iopub.status.busy":"2021-07-09T16:08:54.849309Z","iopub.execute_input":"2021-07-09T16:08:54.849629Z","iopub.status.idle":"2021-07-09T16:08:54.864587Z","shell.execute_reply.started":"2021-07-09T16:08:54.849601Z","shell.execute_reply":"2021-07-09T16:08:54.863571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')","metadata":{"id":"UKk_3HZ-Idjm","execution":{"iopub.status.busy":"2021-07-09T16:08:54.8662Z","iopub.execute_input":"2021-07-09T16:08:54.866603Z","iopub.status.idle":"2021-07-09T16:08:54.880104Z","shell.execute_reply.started":"2021-07-09T16:08:54.866573Z","shell.execute_reply":"2021-07-09T16:08:54.879006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to remove stopwords\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","metadata":{"id":"RX_NFApzIkmC","execution":{"iopub.status.busy":"2021-07-09T16:08:54.884433Z","iopub.execute_input":"2021-07-09T16:08:54.884879Z","iopub.status.idle":"2021-07-09T16:08:54.891557Z","shell.execute_reply.started":"2021-07-09T16:08:54.884814Z","shell.execute_reply":"2021-07-09T16:08:54.890697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","metadata":{"id":"OcFap_w9Ivob","execution":{"iopub.status.busy":"2021-07-09T16:08:54.892963Z","iopub.execute_input":"2021-07-09T16:08:54.893258Z","iopub.status.idle":"2021-07-09T16:08:54.911643Z","shell.execute_reply.started":"2021-07-09T16:08:54.893232Z","shell.execute_reply":"2021-07-09T16:08:54.91069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(clean_sentences))\nclean_sentences[:15]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:15:50.247641Z","iopub.execute_input":"2021-07-09T16:15:50.248185Z","iopub.status.idle":"2021-07-09T16:15:50.256233Z","shell.execute_reply.started":"2021-07-09T16:15:50.248134Z","shell.execute_reply":"2021-07-09T16:15:50.255113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = '/kaggle/input/glove6b/glove.6B.100d.txt'","metadata":{"id":"a5Skjq6DJUtQ","outputId":"eeba2eed-d0ed-42f8-f45f-dea8a99bcb5b","execution":{"iopub.status.busy":"2021-07-09T16:08:54.912975Z","iopub.execute_input":"2021-07-09T16:08:54.913258Z","iopub.status.idle":"2021-07-09T16:08:55.580335Z","shell.execute_reply.started":"2021-07-09T16:08:54.913232Z","shell.execute_reply":"2021-07-09T16:08:55.579263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract word vectors\nword_embeddings = {}\nf = open(glove_input_file, encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","metadata":{"id":"TsXIa7CBKsWQ","execution":{"iopub.status.busy":"2021-07-09T16:08:55.581641Z","iopub.execute_input":"2021-07-09T16:08:55.58198Z","iopub.status.idle":"2021-07-09T16:09:17.31766Z","shell.execute_reply.started":"2021-07-09T16:08:55.581944Z","shell.execute_reply":"2021-07-09T16:09:17.316628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(word_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:12:07.514947Z","iopub.execute_input":"2021-07-09T16:12:07.515313Z","iopub.status.idle":"2021-07-09T16:12:07.521469Z","shell.execute_reply.started":"2021-07-09T16:12:07.515283Z","shell.execute_reply":"2021-07-09T16:12:07.520369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_embeddings['the'].shape","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:12:17.726263Z","iopub.execute_input":"2021-07-09T16:12:17.72661Z","iopub.status.idle":"2021-07-09T16:12:17.731716Z","shell.execute_reply.started":"2021-07-09T16:12:17.72658Z","shell.execute_reply":"2021-07-09T16:12:17.730815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_vectors = []\nfor i in clean_sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","metadata":{"id":"N3VtdSPyKxUZ","execution":{"iopub.status.busy":"2021-07-09T16:09:17.319211Z","iopub.execute_input":"2021-07-09T16:09:17.319531Z","iopub.status.idle":"2021-07-09T16:09:17.330543Z","shell.execute_reply.started":"2021-07-09T16:09:17.319503Z","shell.execute_reply":"2021-07-09T16:09:17.329208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sentence_vectors)","metadata":{"id":"e3Iww3I9LYhJ","outputId":"362a7dd8-b1bf-4478-ef59-34728fbbd8b2","execution":{"iopub.status.busy":"2021-07-09T16:09:17.33199Z","iopub.execute_input":"2021-07-09T16:09:17.332327Z","iopub.status.idle":"2021-07-09T16:09:17.351994Z","shell.execute_reply.started":"2021-07-09T16:09:17.332285Z","shell.execute_reply":"2021-07-09T16:09:17.351039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to find similarities among the sentences. We will use cosine similarity to find similarity between a pair of sentences. Let's create an empty similarity matrix for this task and populate it with cosine similarities of the sentences.","metadata":{"id":"1bh9L2pqL3gp"}},{"cell_type":"code","source":"# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])\nsim_mat.shape","metadata":{"id":"mm_fNZpOLxbM","execution":{"iopub.status.busy":"2021-07-09T16:13:37.972159Z","iopub.execute_input":"2021-07-09T16:13:37.972522Z","iopub.status.idle":"2021-07-09T16:13:37.978223Z","shell.execute_reply.started":"2021-07-09T16:13:37.972491Z","shell.execute_reply":"2021-07-09T16:13:37.977443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity","metadata":{"id":"oVeHkvf0MO1j","execution":{"iopub.status.busy":"2021-07-09T16:09:17.367569Z","iopub.execute_input":"2021-07-09T16:09:17.367899Z","iopub.status.idle":"2021-07-09T16:09:17.379501Z","shell.execute_reply.started":"2021-07-09T16:09:17.36787Z","shell.execute_reply":"2021-07-09T16:09:17.378361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(sentences)):\n    for j in range(len(sentences)):\n        if i != j:\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n\nprint(sim_mat.shape)\nsim_mat[:5,:5]  ","metadata":{"id":"xTAAe-q3L4xM","execution":{"iopub.status.busy":"2021-07-09T16:14:27.893807Z","iopub.execute_input":"2021-07-09T16:14:27.894226Z","iopub.status.idle":"2021-07-09T16:14:31.308922Z","shell.execute_reply.started":"2021-07-09T16:14:27.894189Z","shell.execute_reply":"2021-07-09T16:14:31.307666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)","metadata":{"id":"CAQUnNRWL_tA","execution":{"iopub.status.busy":"2021-07-09T16:14:46.322425Z","iopub.execute_input":"2021-07-09T16:14:46.322765Z","iopub.status.idle":"2021-07-09T16:14:46.602297Z","shell.execute_reply.started":"2021-07-09T16:14:46.322734Z","shell.execute_reply":"2021-07-09T16:14:46.601516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","metadata":{"id":"aQCcvT3yO5Xj","execution":{"iopub.status.busy":"2021-07-09T16:15:09.480047Z","iopub.execute_input":"2021-07-09T16:15:09.480567Z","iopub.status.idle":"2021-07-09T16:15:09.485245Z","shell.execute_reply.started":"2021-07-09T16:15:09.48052Z","shell.execute_reply":"2021-07-09T16:15:09.484207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify number of sentences to form the summary\nsn = 10\n\n# Generate summary\nfor i in range(sn):\n    print(ranked_sentences[i][1])","metadata":{"id":"jwxtPBlgO_Gk","outputId":"94f7a32b-fcd3-4295-ec49-4fb69e49342e","execution":{"iopub.status.busy":"2021-07-09T16:09:21.645635Z","iopub.execute_input":"2021-07-09T16:09:21.646031Z","iopub.status.idle":"2021-07-09T16:09:21.666868Z","shell.execute_reply.started":"2021-07-09T16:09:21.646001Z","shell.execute_reply":"2021-07-09T16:09:21.664154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Further Readings : \n* Text summarization using TextRank in NLP - https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390\n* Find the original article here https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n* link for Cosine Similarity explanation\nhttps://medium.com/datadriveninvestor/cosine-similarity-cosine-distance-6571387f9bf8","metadata":{}}]}