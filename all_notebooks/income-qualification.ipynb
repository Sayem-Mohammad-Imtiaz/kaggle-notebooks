{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n#Reading train and test data\ntrain_data = pd.read_csv(\"/kaggle/input/income-qualification/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/income-qualification/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Analyzing the train data\nprint(train_data.head)\n\n#Target variable has 4 values corresponding to:\n#1 = extreme poverty\n#2 = moderate poverty\n#3 = vulnerable households\n#4 = non vulnerable households","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if the data is imbalanced\ntarget_count = train_data['Target'].value_counts()\ntarget_count.plot(kind=\"bar\", title=\"Target count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since data is biased and consists of more data corresponding to non-vulnerable households, we need to balance the data.\nI'm using oversampling technique to balance the data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data preprocessing\n\n#check for null values\ntrain_data.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the ID column\ntrain_data.drop(\"Id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting string columns to integer columns\n#from sklearn import preprocessing\n\n#encoder = preprocessing.LabelEncoder()\n\nprint(train_data.dtypes)\nfor columns in train_data.columns:\n    if (train_data[columns].dtype == \"object\"):\n        (train_data[columns], uniques) = pd.factorize(train_data[columns])\n\nprint(train_data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.fillna(train_data.mean())\nprint(train_data.isnull().any().any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn import preprocessing\n\n#dividing X and Y in data\nX = np.array(train_data.iloc[:, train_data.columns != 'Target'])\nY = np.array(train_data.iloc[:, train_data.columns == 'Target'])\nprint(\"X_data size is \", X.shape, \" Y_data size is \", Y.shape)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_normalized = min_max_scaler.fit_transform(X)\nX = pd.DataFrame(X_normalized)\n\nros = RandomOverSampler()\nX_oversampled, Y_oversampled = ros.fit_sample(X, Y)\nprint(\"X_data size is \", X_oversampled.shape, \" Y_data size is \", Y_oversampled.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)\nprincipalComponents = pca.fit_transform(X_oversampled)\nX_oversampled = principalComponents\nprincipalDf = pd.DataFrame(principalComponents)\nprint(principalDf.head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Splitting training and testing data\n(X_train, X_Test, Y_train, Y_Test) = train_test_split(X_oversampled, Y_oversampled, test_size = 0.33, stratify = Y_oversampled, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Random Forest as Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=10, random_state=0)\nclf.fit(X_train, Y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred = clf.predict(X_Test)\naccuracy_score(Y_Test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Randomforest using cross validation\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42)\n# Fit the random search model\nrf_random.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best parameters are:\nrf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = RandomForestClassifier(n_estimators = 500,min_samples_split = 2,min_samples_leaf = 1, max_features= 'sqrt',max_depth=35, bootstrap= False, random_state = 42)\nbase_model.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred_regres = base_model.predict(X_Test)\naccuracy_score(Y_Test, y_pred_regres)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred)\nprint(y_pred_regres)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}