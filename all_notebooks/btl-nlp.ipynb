{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preprocessing Data"},{"metadata":{},"cell_type":"markdown","source":"Import neccesary libararies in our code"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.layers import *\nimport xgboost\nfrom tqdm import tqdm\nimport numpy as np\nimport gensim\nfrom tqdm import tqdm\nfrom time import time\nfrom gensim.models import KeyedVectors\nimport pickle\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nimport numpy as np\nimport keras as kr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\ncategories = None\nremove = ('headers', 'footers', 'quotes')\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories,\n                                     shuffle=True, random_state=42)\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories,\n                                     shuffle=True, random_state=42)\ntarget_names = newsgroups_train.target_names\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A simple preprocessing: removing the stop words and lemmatization "},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.utils import simple_preprocess \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english')) \nwordnet_lemmatizer = WordNetLemmatizer()\ndef preprocessing(corpus):\n    res = []\n    for doc in corpus:\n        words = []\n        for word in simple_preprocess(doc):\n            if word not in stop_words:\n                word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n                word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n                word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n                words.append(word3)\n                pass\n            pass\n        res.append(' '.join(words))        \n        pass\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newsgroups_train.data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = preprocessing(newsgroups_train.data)\nX_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_data = newsgroups_train.target\ny_data[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = preprocessing(newsgroups_test.data)\ny_test = newsgroups_test.target","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"type_name = ''\nn_class = 20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF Vectors"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# word level - max number of words equal to 30000 except all words (100k+ words)\ntfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\ntfidf_vect.fit(X_data) # learn vocabulary and idf from training set\nX_data_tfidf =  tfidf_vect.transform(X_data)\n# assume that we don't have test set before\nX_test_tfidf =  tfidf_vect.transform(X_test)\npickle.dump(tfidf_vect, open(type_name + \"tfidf.pickle\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# ngram level - max number of words equal to 30000 except all words (100k+ words)\ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(1, 2))\ntfidf_vect_ngram.fit(X_data)\nX_data_tfidf_ngram =  tfidf_vect_ngram.transform(X_data)\n# assume that we don't have test set before\nX_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\npickle.dump(tfidf_vect_ngram, open(type_name + \"tfidf_ngram.pickle\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform by SVD to decrease number of dimensions"},{"metadata":{},"cell_type":"markdown","source":"##### Word Level"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components= 2000, random_state=42)\nsvd.fit(X_data_tfidf)\npickle.dump(svd, open(type_name + \"tfidf_svd.pickle\", \"wb\"))\nX_data_tfidf_svd = svd.transform(X_data_tfidf)\nX_test_tfidf_svd = svd.transform(X_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### ngram Level"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"svd_ngram = TruncatedSVD(n_components=2000, random_state=42)\nsvd_ngram.fit(X_data_tfidf_ngram)\npickle.dump(svd_ngram, open(type_name + \"tfidf_ngram_svd.pickle\", \"wb\"))\nX_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\nX_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert y to categorical"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nencoder = preprocessing.LabelEncoder()\ny_data_n = encoder.fit_transform(y_data)\ny_test_n = encoder.fit_transform(y_test)\nencoder.classes_\nresults = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"Plot function of NN model's learning curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training function"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def train_model(name, classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3): \n    print('_' * 80)\n    print(\"Training: \" + name)\n    X_train, y_train = X_data, y_data\n    t0 = time()\n    train_time = time() - t0\n    test_time = -1\n    if is_neuralnet:\n        es_callback = kr.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n        history = classifier.fit(X_train, y_train, epochs=n_epochs, validation_split = 0.2, batch_size=512, callbacks=[es_callback], verbose = 0)\n        train_time = time() - t0\n        plot_history(history)\n        t0 = time()\n        test_probas = classifier.predict(X_test)\n        test_time = time() - t0\n        test_predictions = test_probas.argmax(axis=-1)\n        if name != '': classifier.save(name + '.h5')\n        skplt.metrics.plot_roc(y_test, test_probas, figsize = (10,8), title = name,)\n        skplt.metrics.plot_confusion_matrix(y_test, test_predictions, figsize = (10,9), title = name, normalize=True)\n    else:\n        classifier.fit(X_train, y_train)\n        train_time = time() - t0\n        t0 = time()\n        test_predictions = classifier.predict(X_test)\n        test_time = time() - t0\n        if name != '':\n            f = open(name + '.pickle', 'wb')\n            pickle.dump(classifier, f)\n            f.close()\n        y_probas = classifier.predict_proba(X_test)\n        skplt.metrics.plot_roc(y_test, y_probas, figsize = (10,8), title = name, )\n        skplt.estimators.plot_learning_curve(classifier, X_train, y_train, title='Learning Curve (' + name + ')')\n        skplt.metrics.plot_confusion_matrix(y_test,test_predictions, figsize = (10,9), title = name, normalize=True)\n    acc_score = metrics.accuracy_score(y_test, test_predictions)\n    F1_score = metrics.f1_score(y_test, test_predictions, average = None)\n\n    print(\"train time: %0.3fs\" % train_time)\n    print(\"test time:  %0.3fs\" % test_time)\n    print(\"accuracy:   %0.3f\" % acc_score)\n    print(\"classification report:\")\n    print(metrics.classification_report(y_test, test_predictions,\n                                        target_names=target_names))\n    clf_descr = str(classifier).split('(')[0]\n    return name, acc_score, train_time, test_time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn import naive_bayes","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'naive_bayes', naive_bayes.MultinomialNB(alpha = 0.1), X_data_tfidf, y_data,\n            X_test_tfidf, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'naive_bayes_ngram', naive_bayes.MultinomialNB(alpha = 0.1), X_data_tfidf_ngram, y_data,\n            X_test_tfidf_ngram, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Other type Naive Bayes"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'naive_bayes_bernoulli', naive_bayes.BernoulliNB(alpha = 0.1), X_data_tfidf, y_data, \n            X_test_tfidf, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'naive_bayes_bernoulli_ngram', naive_bayes.BernoulliNB(alpha = 0.1), X_data_tfidf_ngram, y_data,\n            X_test_tfidf_ngram, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'naive_bayes_complement', naive_bayes.ComplementNB(alpha = 0.1), X_data_tfidf, y_data, \n            X_test_tfidf, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'naive_bayes_complement', naive_bayes.ComplementNB(alpha = 0.1), X_data_tfidf_ngram, y_data, \n            X_test_tfidf_ngram, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Classifier"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'linear_model',linear_model.LogisticRegression(penalty = 'l2', C = 3.0), X_data_tfidf, y_data,\n            X_test_tfidf, y_test, is_neuralnet=False))\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'linear_model_ngram',linear_model.LogisticRegression(penalty = 'l2', C = 3.0), X_data_tfidf_ngram, y_data,\n            X_test_tfidf_ngram, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM Model"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'svm', svm.SVC(kernel = 'linear', C = 3.0), X_data_tfidf_svd, y_data,\n            X_test_tfidf_svd, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'svm_ngram',svm.SVC(kernel = 'linear', C = 3.0), X_data_tfidf_ngram_svd, y_data,\n            X_test_tfidf_ngram_svd, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging Model"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'bagging',ensemble.RandomForestClassifier(criterion = 'entropy'), X_data_tfidf_svd, y_data,\n            X_test_tfidf_svd, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'bagging_ngram',ensemble.RandomForestClassifier(criterion = 'entropy'), X_data_tfidf_ngram_svd, y_data, \n            X_test_tfidf_ngram_svd, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting Model"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'boosting',xgboost.XGBClassifier(), X_data_tfidf_svd, y_data,\n            X_test_tfidf_svd, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results.append(train_model(type_name + 'boosting_ngram',xgboost.XGBClassifier(), X_data_tfidf_ngram_svd, y_data, \n            X_test_tfidf_ngram_svd, y_test, is_neuralnet=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep Neural Network"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def create_dnn_model(n_class):\n    input_layer = Input(shape=(600,))\n    layer = Dense(128, activation='relu')(input_layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(64, activation='relu')(layer)\n    layer = Dropout(0.4)(layer)\n\n    output_layer = Dense(n_class, activation='softmax')(layer)\n    \n    classifier = models.Model(input_layer, output_layer)\n    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy',\n                       metrics=['accuracy'])\n    \n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"classifier = create_dnn_model(n_class)\nresults.append(train_model(name = type_name + 'dnn', classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n,\n            X_test=X_test_tfidf_svd, y_test=y_test_n, n_epochs= 200, is_neuralnet=True))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"classifier = create_dnn_model(n_class)\nresults.append(train_model(name = type_name + 'dnn_ngram', classifier=classifier, X_data=X_data_tfidf_ngram_svd,\n            y_data=y_data_n, X_test=X_test_tfidf_ngram_svd, y_test=y_test_n, n_epochs= 200,\n            is_neuralnet=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recurrent Convolutional Neural Network "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def create_rcnn_model(n_class):\n    input_layer = Input(shape=(600,))\n    \n    layer = Reshape((20, 30))(input_layer)\n    layer = Bidirectional(GRU(128, activation='relu', return_sequences=True))(layer)    \n    layer = Convolution1D(100, 3, activation=\"relu\")(layer)\n    layer = Flatten()(layer)\n    layer = Dense(128, activation='relu')(layer)\n    layer = Dropout(0.2)(layer)\n    \n    output_layer = Dense(n_class, activation='softmax')(layer)\n    \n    classifier = models.Model(input_layer, output_layer)\n    classifier.summary()\n    classifier.compile(optimizer=optimizers.Adam(),\n                       loss='sparse_categorical_crossentropy',\n                       metrics=['accuracy'])\n    \n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"classifier = create_rcnn_model(n_class)\nresults.append(train_model(name = type_name + 'rcnn', classifier=classifier, \n            X_data=X_data_tfidf_svd, y_data=y_data_n,\n            X_test=X_test_tfidf_svd, y_test=y_test_n, \n            is_neuralnet=True, n_epochs=200))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"classifier = create_rcnn_model(n_class)\nresults.append(train_model(name = 'rcnn_ngram', classifier=classifier,\n            X_data=X_data_tfidf_ngram_svd, y_data=y_data_n,\n            X_test=X_test_tfidf_ngram_svd, y_test=y_test_n,\n            is_neuralnet=True, n_epochs=200))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot"},{"metadata":{},"cell_type":"markdown","source":"Sumary and compare the performance between models"},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.arange(len(results))\n\nresults = [[x[i] for x in results] for i in range(4)]\n\n#clf_descr, acc_score, F1_score, train_time, test_time\nclf_names, acc_score, training_time, test_time = results\ntraining_time = np.array(training_time) / np.max(training_time)\ntest_time = np.array(test_time) / np.max(test_time)\n\nplt.figure(figsize=(12, 8))\nplt.title(\"Accuracy  and time (normalize)\")\nplt.barh(indices, acc_score, .2, label=\"accuracy\", color='navy')\nplt.barh(indices + .3, training_time, .2, label=\"training time\", color='c')\nplt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\nplt.yticks(())\nplt.legend(loc='best')\nplt.subplots_adjust(left=.25)\nplt.subplots_adjust(top=.95)\nplt.subplots_adjust(bottom=.05)\n\nfor i, c in zip(indices, clf_names):\n    plt.text(-.3, i, c)\nplt.savefig('acc_time.png')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}