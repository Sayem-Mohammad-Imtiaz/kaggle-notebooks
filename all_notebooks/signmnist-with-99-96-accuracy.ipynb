{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SignMNIST","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## DataSet Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So, we will start by visualizing the input data. Let's import all the required libraries and get started.\nI'll be using the following libraries to provide you some insights on this dataset:\n* ***Pandas*** to import the csv as a dataframe and perform the required operations.\n* ***Matplotlib*** to give some graphical data for better understanding.\n* ***Seaborn*** just to have a fancy look [XD XD].\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I'll read the dataset into a Pandas Dataframe and show you the information related to the data which contains things like **Total no. of rows**, **Total no. of cloumns**, **types of values**, etc..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\")\ndf_test = pd.read_csv(\"../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can see the Total number of rows and columns here\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the SignMNIST dataset consists of 27455 rows and each row in turn has 785 columns.\nOut of the 785 columns, the first column is to identify the sign or we can say it contains all the *labels*.  \n\nTo have a better understanding, I'll show you exactly the number of examples each label contains. This can be done in the following manner: \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count = df['label'].value_counts()\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also, just for verification\ncount.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, Using the Matplotlib Library of Python, you can see the graphical distribution of the given data for each label.\n\nThis can be done in the following manner:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = plt.figure(figsize=(5,3),dpi=100)\naxes = fig1.add_axes([1,1,1,1])\naxes.set_ylim([0, 1300])\naxes.set_xlabel('classes')\naxes.set_ylabel('No. of Examples available')\naxes.set_xlim([0,24])\nfor i in range(24):\n    axes.axvline(i)\naxes.bar(count.index,count.values,color='purple',ls='--') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, the dataset has no data for the class label **9**. This is because in order to express the word 'J', you need to provide rotation to your hand whose data can't really be captured in 2-D arrays. Hence, the empty bar in the graph above.\n\nNow, just to be a bit fancy, I'll plot the same thing via Seaborn too. Seaborn is another library of Python which is built on top of Matplotlib. It provides some fancy customizations (like palettes and all) and also is easier to use, hence, you can try your hands on that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I like the palette 'magma' too much. You can go with others like 'coolwarm','hsl' or 'husl'\nsns.countplot(x=df['label'], palette='magma')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Have a look at few of the examples in the dataset.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_labels = df_test['label']\ndel df['label']\nfig, axes = plt.subplots(3,4,figsize=(10,10),dpi=150)\nk = 1\nfor i in range(3):\n    for j in range(4):\n        axes[i,j].imshow(df.values[k].reshape(28,28))\n        axes[i,j].set_title(\"image \"+str(k))\n        k+=1\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, I'll import all the necessary libraries I'm gonna need to get started.","execution_count":null},{"metadata":{"_uuid":"a462a6bd-986e-4c2e-8aa0-6cee68a7cc62","_cell_guid":"8b7a5636-62ba-48a1-9027-58b1db139b95","trusted":true,"id":"wYtuKeK0dImp"},"cell_type":"code","source":"import csv\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom os import getcwd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing the dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'm using the csv library as I used it when I was learning via Coursera. Also, You can use a combination of 'os', 'numpy' and 'pandas' to achieve the same as well. ","execution_count":null},{"metadata":{"_uuid":"f7859b3d-12ff-4ac2-b648-ff3dab6db697","_cell_guid":"31b253e3-085c-4d64-a6be-b2112f7054f7","trusted":true,"id":"4kxw-_rmcnVu"},"cell_type":"code","source":"def get_data(filename):\n    with open(filename) as training_file:\n        csv_reader = csv.reader(training_file, delimiter=',')\n        first_line = True\n        temp_images = []\n        temp_labels = []\n        for row in csv_reader:\n            if first_line:\n                # print(\"Ignoring first line\")\n                first_line = False\n            else:\n                temp_labels.append(row[0])\n                image_data = row[1:785]\n                image_data_as_array = np.array_split(image_data, 28)\n                temp_images.append(image_data_as_array)\n        images = np.array(temp_images).astype('float')\n        labels = np.array(temp_labels).astype('float')\n        #print(labels)\n    return images, labels\n\npath_sign_mnist_train = f\"{getcwd()}/../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\"\npath_sign_mnist_test = f\"{getcwd()}/../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\"\ntraining_images, training_labels = get_data(path_sign_mnist_train)\ntesting_images, testing_labels = get_data(path_sign_mnist_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, in short, what I've done is the following: \n* Read the CSV file via the csv.reader() method provided by csv library.\n* Created to simple arrays which contain 1. Labels 2. Image Data\n* Appended the data from the CSV file in the respective arrays\n* Converted the arrays to numpy arrays and changed the datatype to float(from String)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Applying Image Augmentation\nSimply, followed the most commonly used Image Augmentation techniques like:\n* height/width shit which in simple terms is Cropping the image\n* rescaling to normalize the data and centre is between 0 and 1\n* rotation of images by 10% and zooming the images by 20%\n\nNormalization has been done for both Training as well as Validation Data.","execution_count":null},{"metadata":{"_uuid":"e875c0a6-6be4-4e8b-b5a4-f294f39a3041","_cell_guid":"982a6bd2-aa83-4647-900f-03918ce53b6c","trusted":true,"id":"awoqRpyZdQkD"},"cell_type":"code","source":"training_images = np.expand_dims(training_images, axis=-1)\ntesting_images = np.expand_dims(testing_images, axis=-1)\n\n# Create an ImageDataGenerator and do Image Augmentation\ntrain_datagen = ImageDataGenerator(rescale=1.0/255,\n                                   rotation_range=10,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.1,\n                                   zoom_range=0.2)\n\nvalidation_datagen = ImageDataGenerator(rescale=1.0/255)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is how i made my model. \n\nYou can freely experiment with it by increasing/decreasing layers, adding more features like Activity Regularization, Callbacks, Strides, etc etc..  \nI did not include Batch normalization as I didn't understand that concept very well. Surely after developing a better understanding, I'll include it. Or you guys can suggest me somethings anytime.\n\nI mean, there's infinite scope of improvement in this discipline, no model can be termed perfect. ","execution_count":null},{"metadata":{"_uuid":"dc2baf76-0d40-4931-a73f-4b4fe6f2451e","_cell_guid":"157fdf24-fa80-4aee-9c22-c66d056748fd","trusted":true},"cell_type":"code","source":"# Define the model\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nankitz = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.75, min_lr=0.00001)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(128, (4, 4), activation='relu', input_shape=(28, 28, 1),padding='same'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same',bias_regularizer=regularizers.l2(1e-4),),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (2, 2), activation='relu', padding='same',bias_regularizer=regularizers.l2(1e-4)),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(1,1), padding='same'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(384, activation=tf.nn.relu, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4)),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(25, activation=tf.nn.softmax,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=regularizers.l2(1e-4))])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compiling the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So, I'm gonna use a batch size of 128 and Adam as the optimizer. In future, Ill surely improve by going much deeper, But as of now I'm also learning and so, do suggest the improvements in the comments.\n\n**Some of the compiling definitions I used are: **\n* ADAM(Adaptive Moment) Optimizer\n* No. of epochs is 30\n* A callback has been used which will reduce the learning rate on plateaus by a factor of 0.75\n* loss used is 'sparse_categorical_crossentropy'.  \n(You can use 'categorical_crossentropy' as well but with a few changes in the above code.)","execution_count":null},{"metadata":{"_uuid":"68332bf7-875b-4eb8-9db0-a7844118b1e4","_cell_guid":"cda90a85-0086-447b-8ecf-9926ee383d17","trusted":true,"id":"Rmb7S32cgRqS"},"cell_type":"code","source":"# Compile Model. \nmodel.compile(optimizer=tf.optimizers.Adam(),\n             loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])\n\n# Train the Model\nhistory = model.fit_generator(train_datagen.flow(training_images, training_labels, batch_size=128),\n                              epochs = 30,\n                              validation_data=validation_datagen.flow(testing_images, testing_labels),\n                             callbacks = [ankitz])\n\nprint(\"\\n\\nThe accuracy for the model is: \"+str(model.evaluate(testing_images, testing_labels, verbose=1)[1]*100)+\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll be honest here. The accuracy isn't fixed even for the identical process repeated twice. This is because your machine need not face the exact same complications every single time it runs. Sometimes, all goes well and you obtain the desired results. But, this truly is \"random\". So, the value above is one of the entire range of accuracy along with some(+-) error.  \nAlso, the accuracy for the above model, when i tested it four time came out to be 99.9%, 99.2%, 99.7% and 99.9%. So, I'd say the accuracy will mostly come out to be greater than 99%.\nIf not, as i said above, feel free to experiment..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Analysis of results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So, first, we will start by simply comparing and visualizing the loss and accuracy over training and validation respectively.  \n\nI'm doin this using the Matplotlib Library. The variable 'history' will provide you with all the necessary values to plot and analyze the results.","execution_count":null},{"metadata":{"_uuid":"4665e0d2-f6d2-4a9b-a129-d1ec9715765f","_cell_guid":"d77f5f26-46a3-45fb-bc6d-0fa61525e972","trusted":true,"id":"_Q3Zpr46dsij"},"cell_type":"code","source":"# Plot the chart for accuracy and loss on both training and validation\n%matplotlib inline\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So, both the graphs came out pretty smooth I'd say. You can further make it better by having fun with the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(testing_images)\nprint(predictions[:15])\nprint(image_labels.values[:15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clearly, first 15 of our results have been correctly Classified. Below is the text report showing the main classification metrics.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nclasses_report = [\"Class \" + str(i) for i in range(25) if i != 9]\nprint(classification_report(image_labels, predictions, target_names = classes_report))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank You!!\nThanks a lot guys. This is my **first** (basically my first activity XD) on Kaggle. I hope I was able to provide you with insights of the problem. If you have any constructive criticism, please let me know in comments. \nAlso, I don't know if there is any option to message other people on kaggle, if there is, feel free to ask any doubt and also to share your knowledge with me.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}