{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nimport statsmodels.api as sm\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score\n\ndef adj_r2_score(r2, n, k):\n    return 1-((1-r2)*((n-1)/(n-k-1)))\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras.backend as K\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import and transform Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"uber_raw_apr14 = pd.read_csv(\"../input/uber-raw-data-apr14.csv\")\nuber_raw_may14 = pd.read_csv(\"../input/uber-raw-data-may14.csv\")\nuber_raw_jun14 = pd.read_csv(\"../input/uber-raw-data-jun14.csv\")\nuber_raw_jul14 = pd.read_csv(\"../input/uber-raw-data-jul14.csv\")\nuber_raw_aug14 = pd.read_csv(\"../input/uber-raw-data-aug14.csv\")\nuber_raw_sep14 = pd.read_csv(\"../input/uber-raw-data-sep14.csv\")\n\n#Combining dataset of 6 months into 1 dataset\nuber_2014 = [uber_raw_apr14, uber_raw_may14, uber_raw_jun14, uber_raw_jul14,uber_raw_aug14, uber_raw_sep14]\nuber_data_2014 = pd.concat(uber_2014,axis=0,ignore_index=True)\nuber_data_2014.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the dataset has 4 columns:\n\n    Date/Time : The date and time of the Uber pickup.\n    Lat : The latitude of the Uber pickup\n    Lon : The longitude of the Uber pickup\n    Base : The TLC base company code affiliated with the Uber pickup"},{"metadata":{"trusted":false},"cell_type":"code","source":"uber_data_2014.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dividing the above Date/Time columns into several columns for visualising and analysing the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"uber_data_2014.Timestamp = pd.to_datetime(uber_data_2014['Date/Time'],format='%m/%d/%Y %H:%M:%S') \nuber_data_2014['Date_only'] = uber_data_2014.Timestamp.dt.date\nuber_data_2014['Date'] = uber_data_2014.Timestamp\nuber_data_2014['Month'] = uber_data_2014.Timestamp.dt.month\nuber_data_2014['DayOfWeekNum'] = uber_data_2014.Timestamp.dt.dayofweek\nuber_data_2014['DayOfWeek'] = uber_data_2014.Timestamp.dt.weekday_name\nuber_data_2014['MonthDayNum'] = uber_data_2014.Timestamp.dt.day\nuber_data_2014['HourOfDay'] = uber_data_2014.Timestamp.dt.hour\n\nuber_data_2014= uber_data_2014.drop(columns = ['Lat','Lon'])\nuber_data_2014.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{},"cell_type":"markdown","source":"#### Peak Days"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"uber_data_2014.groupby(pd.Grouper(key='DayOfWeek')).count()\n\nuber_weekdays = uber_data_2014.pivot_table(index=['DayOfWeekNum','DayOfWeek'],\n                                  values='Base',\n                                  aggfunc='count')\nuber_weekdays.plot(kind='bar', figsize=(15,8))\nplt.ylabel('Total Journeys')\nplt.xlabel('Day')\nplt.title('Journeys by Week Day');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the larest number of uber pickups were done on Thurdays and Fridays"},{"metadata":{},"cell_type":"markdown","source":"#### Analysing peek hours"},{"metadata":{"trusted":false},"cell_type":"code","source":"uber_hour = uber_data_2014.pivot_table(index=['HourOfDay'],\n                                  values='Base',\n                                  aggfunc='count')\nuber_hour.plot(kind='bar', figsize=(8,6))\nplt.ylabel('Total Journeys')\nplt.title('Journeys by Hour');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can see that peak hours of booking a cab are in evening from 4pm to 6pm.\nWe can also see that more cabs are booked in evenings compared to mornings"},{"metadata":{},"cell_type":"markdown","source":"#### Base"},{"metadata":{"trusted":false},"cell_type":"code","source":"uber_data_2014.groupby(pd.Grouper(key='Base')).count()\n\nuber_monthdays = uber_data_2014.pivot_table(index=['Base'], values='Date' ,\n                                  aggfunc='count')\nuber_monthdays.plot(kind='bar', figsize=(8,6))\nplt.ylabel('Total Journeys')\nplt.title('Journeys by Month Day');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that Base B02617 provided most cabs. Closely followed by B02598"},{"metadata":{},"cell_type":"markdown","source":"# Splitting the dataset"},{"metadata":{},"cell_type":"markdown","source":"As we have a very large dataset of 4.5 million+ values. I have used 90-10 split"},{"metadata":{"trusted":false},"cell_type":"code","source":"uber_data_2014= uber_data_2014.drop(columns = ['Month','DayOfWeekNum','Base', 'DayOfWeek', 'MonthDayNum', 'HourOfDay'])\n#uber_data_2014.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\nThe df uber_count is the grouping of the above dataset on hourly basis with time stamp of both date and time.\nThis df is used mostly for ANN analysis.\n'''\nuber_count=uber_data_2014.groupby(pd.Grouper(key='Date')).count()\nuber_count= uber_count.drop(columns = ['Date_only'])\nprint(uber_count.info())\n\ntrain = uber_count[:][:234083]             #90% of 260093\ntest = uber_count[:][234084:]\ndisplay(train.tail())\ntest.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Date/Time'].plot(kind='line',figsize=(15,8), title= 'Hourly Ridership', fontsize=14)\ntest['Date/Time'].plot(figsize=(15,5), title= 'Hourly Ridership', fontsize=14)\nplt.ylabel('Total Journeys')\nplt.xlabel('Month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\nThe df uber_dates is the grouping of the above dataset on daily basis with time stamp of onlu date.\nThis df is used to for univariate Time Series Forecasting.\n'''\nuber_dates=uber_data_2014.groupby(pd.Grouper(key='Date_only')).count()\nuber_dates= uber_dates.drop(columns = ['Date'])\nprint(uber_dates.info())\nuber_dates_d= uber_dates.drop(columns = ['Date/Time'])\n\ntrain_ts = uber_dates[:][:163]                     #split is 90-10\ntest_ts = uber_dates[:][164:]\ntest_ts_d = uber_dates_d[:][164:]\ntest_ts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_ts['Date/Time'].plot(kind='line',figsize=(15,8), title= 'Daily Ridership', fontsize=14)\ntest_ts['Date/Time'].plot(figsize=(15,5), title= 'Daily Ridership', fontsize=14)\nplt.ylabel('Total Journeys')\nplt.xlabel('Month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time Series Forcasting"},{"metadata":{},"cell_type":"markdown","source":"### Holt’s Winter seasonal method"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_avg = test_ts.copy()\nfit1 = ExponentialSmoothing(np.asarray(train_ts['Date/Time']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(test_ts))\nplt.figure(figsize=(15,5))\nplt.plot( train_ts['Date/Time'], label='Train')\nplt.plot(test_ts['Date/Time'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')\nplt.ylabel('Total Journeys')\nplt.xlabel('Months')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model also gives best accuracy so far and we can see from the above plot that the predicted Holt winter graph is almost overlapping with the actual test dataset available to us"},{"metadata":{},"cell_type":"markdown","source":"### SARIMA Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_avg = test_ts.copy()\nfit1 = sm.tsa.statespace.SARIMAX(train_ts['Date/Time'], order=(2, 1, 4),seasonal_order=(1,1,1,7)).fit()\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-09-11\", end=\"2014-09-30\", dynamic=True)\nplt.figure(figsize=(15,6))\nplt.plot( train_ts['Date/Time'], label='Train')\nplt.plot(test_ts['Date/Time'], label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.ylabel('Total Journeys')\nplt.xlabel('Months')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model also predicts with comparable accuracy as the above holt winter season method as we can see here as well that the predicted sarima graph is almost overlapping with the actual test dataset available to us"},{"metadata":{},"cell_type":"markdown","source":"### Holt’s Linear Trend method"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.style.use('default')\nplt.figure(figsize = (16,8))\nimport statsmodels.api as sm\nsm.tsa.seasonal_decompose(train_ts['Date/Time'].values,freq=30).plot()\nresult = sm.tsa.stattools.adfuller(uber_dates['Date/Time'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_avg = test_ts.copy()\n\nfit1 = Holt(np.asarray(train_ts['Date/Time'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(test_ts))\n\nplt.figure(figsize=(16,5))\nplt.plot(train_ts['Date/Time'], label='Train')\nplt.plot(test_ts['Date/Time'], label='Test')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that Holt Linear Trend is hardly a good model for our dataset."},{"metadata":{},"cell_type":"markdown","source":"### ARIMA"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationary(timeseries):\n    #Determine rolling statistics\n    #rolmean = pd.rolling_mean(timeseries,window = 24)\n    #rolstd = pd.rolling_std(timeseries, window = 24)\n    \n    rolmean = timeseries.rolling(24).mean()\n    rolstd = timeseries.rolling(24).std()\n    \n    \n    #Plot rolling Statistics\n    orig = plt.plot(timeseries, color = \"blue\", label = \"Original\")\n    mean = plt.plot(rolmean, color = \"red\", label = \"Rolling Mean\")\n    std = plt.plot(rolstd, color = \"black\", label = \"Rolling Std\")\n    plt.legend(loc = \"best\")\n    plt.title(\"Rolling Mean and Standard Deviation\")\n    plt.show(block = False)\n    \n    #Perform Dickey Fuller test\n    print(\"Results of Dickey Fuller test: \")\n    dftest = adfuller(timeseries, autolag = 'AIC')\n    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistics', 'p-value', '# Lag Used', 'Number of Observations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)' %key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize']=(20,10)\ntest_stationary(uber_count['Date/Time'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove Trend"},{"metadata":{"trusted":false},"cell_type":"code","source":"Train_log = np.log(train_ts['Date/Time'])\nvalid_log = np.log(test_ts['Date/Time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"moving_avg = Train_log.rolling(24).mean()\nplt.plot(Train_log)\nplt.plot(moving_avg, color = 'red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_log_moving_diff = Train_log - moving_avg\ntrain_log_moving_diff.dropna(inplace = True)\ntest_stationary(train_log_moving_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Differncing can help to make series stable and eliminate trend"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_log_diff = Train_log - Train_log.shift(1)\ntest_stationary(train_log_diff.dropna())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removing Seasonailty"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nplt.figure(figsize = (16,10))\ndecomposition = seasonal_decompose(pd.DataFrame(Train_log)['Date/Time'].values, freq = 24)\nplt.style.use('default')\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(Train_log, label = 'Original')\nplt.legend(loc = 'best')\nplt.subplot(412)\nplt.plot(trend, label = 'Trend')\nplt.legend(loc = 'best')\nplt.subplot(413)\nplt.plot(seasonal, label = 'Seasonal')\nplt.legend(loc = 'best')\nplt.subplot(414)\nplt.plot(residual, label = 'Residuals')\nplt.legend(loc = 'best')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking stationarity of residuals"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (16,8))\ntrain_log_decompose = pd.DataFrame(residual)\ntrain_log_decompose['date'] = Train_log.index\ntrain_log_decompose.set_index('date', inplace = True)\ntrain_log_decompose.dropna(inplace = True)\ntest_stationary(train_log_decompose[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(train_log_diff.dropna(), nlags = 25)\nlag_pacf = pacf(train_log_diff.dropna(), nlags = 25, method= \"ols\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nplt.style.use(\"fivethirtyeight\")\nplt.plot(lag_acf)\nplt.axhline( y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline( y= -1.96/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline(y = 1.96 /np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Autocorrelation Function\")\nplt.show()\n# PACF\nplt.figure(figsize = (15,8))\nplt.plot(lag_pacf)\nplt.axhline(y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline(y = -1.96/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline( y = 1.96/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Partial Autocorrelation Function\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AR model"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nplt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (2,1,0))  #here q value is zero since it is just AR Model\nresults_AR = model.fit(disp=-1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_AR.fittedvalues, color = 'red', label = 'Predictions')\nplt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"AR_predict = results_AR.predict(start=\"2014-09-11\", end=\"2014-09-30\")\nAR_predict = AR_predict.cumsum().shift().fillna(0)\nAR_predict1 = pd.Series(np.ones(test_ts.shape[0])* np.log(test_ts['Date/Time'])[0], index = test_ts_d)\nAR_predict = np.exp(AR_predict1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Moving Average Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (0,1,2)) # here the p value is 0 since it is moving average model\nresults_MA = model.fit(disp = -1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_MA.fittedvalues, color = \"red\", label = \"Prediction\")\nplt.legend(loc = \"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"MA_predict = results_MA.predict(start=\"2014-09-11\", end=\"2014-09-30\")\nMA_predict=MA_predict.cumsum().shift().fillna(0)\nMA_predict1=pd.Series(np.ones(test_ts.shape[0]) * np.log(test_ts['Date/Time'])[0], index = test_ts_d)\n#MA_predict1=MA_predict1.add(MA_predict,fill_value=0)\nMA_predict = np.exp(MA_predict1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plt.figure(figsize = (15,8))\nplt.plot(test_ts['Date/Time'], label = \"Valid\")\nplt.plot(MA_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, test_ts['Date/Time']))/test_ts.shape[0]))\nplt.show()"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Combined Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nmodel = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='Original')\nplt.plot(results_ARIMA.fittedvalues, color='red', label='Predicted')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to scale model to original scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Date/Time'])[0], index = given_set.index)\n    #predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_base)\n    \n    plt.plot(given_set['Date/Time'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Date/Time']))/given_set.shape[0]))\n    plt.show()\n\ndef check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['Date/Time'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Date/Time']))/given_set.shape[0]))\n    plt.show()\n\nARIMA_predict_diff=results_ARIMA.predict(start=\"2014-09-11\", end=\"2014-09-30\")\n\nplt.figure(figsize = (16,8))\ncheck_prediction_diff(ARIMA_predict_diff, test_ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ARIMA_predict_diff.shape \n\n\ntest_ts.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The huge RMSE value in AR model shows that this model is not suitable for our dataset"},{"metadata":{},"cell_type":"markdown","source":"### Simple Exponential Smoothing"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat = test_ts.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train_ts['Date/Time'])).fit(smoothing_level = 0.6,optimized = False)\ny_hat['SES'] = fit2.forecast(len(test_ts))\nplt.figure(figsize =(15,8))\nplt.plot(train_ts['Date/Time'], label = 'Train')\nplt.plot(test_ts['Date/Time'], label = 'Validation')\nplt.plot(y_hat['SES'], label = 'Simple Exponential Smoothing')\nplt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RMSE Error for Simple Exponential Smoothing"},{"metadata":{"trusted":false},"cell_type":"code","source":"abc=y_hat['SES'].values.tolist()\nrmse = sqrt(mean_squared_error(test_ts['Date/Time'],abc))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast'] = train_ts['Date/Time'].rolling(10).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date/Time'], label = 'Train')\nplt.plot(test_ts['Date/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = 'Moving Average Forecast with 10 Observations')\nplt.legend(loc = 'best')\nplt.show()\ny_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast'] = train_ts['Date/Time'].rolling(20).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date/Time'], label = 'Train')\nplt.plot(test_ts['Date/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'],label = 'Moving Average Forecast with 20 Observations')\nplt.legend(loc = 'best')\nplt.show()\ny_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast']= train_ts['Date/Time'].rolling(50).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date/Time'], label = 'Train')\nplt.plot(test_ts['Date/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = \"Moving Average Forecast with 50 Observations\")\nplt.legend(loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rmse = sqrt(mean_squared_error(test_ts['Date/Time'], y_hat_avg['moving_average_forecast']))\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This model gives huge RMSE value too showing this model isn't good for our dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat = test_ts.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train_ts['Date/Time'])).fit(smoothing_level = 0.6,optimized = False)\ny_hat['SES'] = fit2.forecast(len(test_ts))\nplt.figure(figsize =(15,8))\nplt.plot(train_ts['Date/Time'], label = 'Train')\nplt.plot(test_ts['Date/Time'], label = 'Validation')\nplt.plot(y_hat['SES'], label = 'Simple Exponential Smoothing')\nplt.legend(loc = 'best')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"rmse = sqrt(mean_squared_error(train_ts['Date/Time'], y_hat['SES']))\nrmse"},{"metadata":{},"cell_type":"markdown","source":"#### Simple Exponential Smoothing is not the right model for our dataset"},{"metadata":{},"cell_type":"markdown","source":"# ANN"},{"metadata":{"trusted":false},"cell_type":"code","source":"sc = MinMaxScaler()\ntrain_sc = sc.fit_transform(train)\ntest_sc = sc.transform(test)\n\nX_train = train_sc[:-1]\ny_train = train_sc[1:]\n\nX_test = test_sc[:-1]\ny_test = test_sc[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"K.clear_session()\n\nmodel = Sequential()\nmodel.add(Dense(9, input_dim=1, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nearly_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_test_ann = model.predict(X_test)\ny_train_pred_ann = model.predict(X_train)\nrmse = sqrt(mean_squared_error(y_train,y_train_pred_ann))\nprint(\"Train : {:0.3f}\".format(rmse))\n\nrmse = sqrt(mean_squared_error(y_test,y_pred_test_ann))\nprint(\"Test : {:0.3f}\".format(rmse))\n\nmodel.save('Uber_ANN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we can see the Root Mean Square Error values are less. \nThis is the least when compared with other standard splits of 70-30, 80-20 and 95- 05"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_ann = load_model('Uber_ANN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_test_ANN = model_ann.predict(X_test)\nplt.plot(y_test, label='True')\nplt.plot(y_pred_test_ANN, label='ANN')\nplt.title(\"ANN's_Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('INR_Scaled')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"score_ann= model_ann.evaluate(X_test, y_test, batch_size=1)\nprint('ANN: %f'%score_ann)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Error Score for this model is very low, hence ANN model gives good results"},{"metadata":{},"cell_type":"markdown","source":"## Result\n\n#### From the abve analysis we can see that Holt's Winter Season and SABRIMA and ANN gave the best reults with small RMSE values compared to other models used."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}