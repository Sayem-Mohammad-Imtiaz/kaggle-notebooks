{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport os\npath = \"/kaggle/input/papers-for-biobert/testset/\"\nfiles = os.listdir(path)\npapers = []\nfor f in files:\n    papers += torch.load(path + f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from ipywidgets import interact_manual, widgets\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\ndef clean_text(text):\n    tokens = word_tokenize(text)\n\n    start = -1\n    for i, token in enumerate(tokens):\n        if start<0:\n            if not(token.isdigit()) and not (token in (string.punctuation)):\n                start = i\n    tokens = tokens[start:]\n    sentence = \" \".join(tokens)\n    return sentence","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%capture\n# BioBERT dependencies\nimport subprocess\n# Tensorflow 2.0 didn't work with the pretrained BioBERT weights\n!pip install tensorflow==1.15\n# Install bert-as-service\n!pip install bert-serving-server==1.10.0\n!pip install bert-serving-client==1.10.0\n\n# We need to rename some files to get them to work with the naming conventions expected by bert-serving-start\n!cp /kaggle/input/biobert-pretrained /kaggle/working -r\n%mv /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/model.ckpt-1000000.index /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/bert_model.ckpt.index\n%mv /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/model.ckpt-1000000.data-00000-of-00001 /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/bert_model.ckpt.data-00000-of-00001\n%mv /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/model.ckpt-1000000.meta /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed/bert_model.ckpt.meta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nbert_command = 'bert-serving-start -model_dir /kaggle/working/biobert-pretrained/biobert_v1.1_pubmed -max_seq_len=None -max_batch_size=32 -num_worker=2'\nprocess = subprocess.Popen(bert_command.split(), stdout=subprocess.PIPE)\n\n# Start the BERT client. It takes about 10 seconds for the bert server to start, which delays the client\nfrom bert_serving.client import BertClient\n\nbc = BertClient()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\nfrom scipy.spatial.distance import cosine\nimport pickle\nimport numpy as np\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport string\nimport string \n\n\nwith open('/kaggle/input/summary-embeddings/summary_embs_df.pkl','rb') as f:\n    emb_df = pickle.load(f)\n\n\ndef cosine_distance(v1, v2):\n    distance = 1 - cosine(v1, v2)\n    return distance\n\ndef answer_query(query,num_summaries=5):\n    ##Encode the query with biobert\n    qemb = bc.encode([query])\n    \n    relevant_embeddings = emb_df\n\n    ## Compute similarities with relevant embeddings and querry\n    a = np.array([cosine_distance(qemb[0],relevant_embeddings['embedding'][i]) for i in range(relevant_embeddings.shape[0])])\n    asort = np.argsort(a)\n    \n    ## Print everything\n#     print('')\n    print('Generated summaries of '+str(num_summaries)+' most relevant papers for query:')\n    print('\"'+query+'\"')\n    for i in range(1,num_summaries):\n        print('-----------------')\n        print(\"From paper with Title : \"+relevant_embeddings['paper_id'][asort[-i]])\n        print(\"Important sentences : \")\n        soum = relevant_embeddings['summary'][asort[-i]]\n        sents = soum.split('.')      \n        for s in sents:\n            print (clean_text(s))    \n            print('...')\n        print('With average sentence importance score : '+str(relevant_embeddings['sum_score'][asort[-i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Given a query, returns the answer :)\ndef answer_query(query,bc,filter_dataset=False,use_synonyms=False,count=5):\n    print (bcolors.OKCYAN + \"Answers for the query: \" + bcolors.ENDC)\n    print (bcolors.OKBLUE + query + bcolors.ENDC)\n    print(bcolors.OKGREEN  + \"----------------------------------------------------------------------------------\" + bcolors.ENDC)\n    #model = create_bert()\n    \n    #print(emb_df.head())\n    #print(\"IN ANSWER QUERY COUNT IS \"+str(count))\n    if use_synonyms:\n        queries = get_synonymous_queries(query)\n    else:\n        queries = list()\n        queries.append(query)\n\n\n    if filter_dataset:\n        relevant_embeddings = snomed_filter(query=query,count=count)\n        print(\"Retrieved \"+str(relevant_embeddings.shape[0])+ ' papers with more than '+str(count)+' appearances of relevant SNOMED terms to the query.')\n    else:\n        relevant_embeddings = emb_df\n\n    #print(type(queries))\n    similar_texts = get_similar_texts(queries,[relevant_embeddings['embedding'][i] for i in range(relevant_embeddings.shape[0])],bc = bc)\n    \n#     print(similar_texts)\n#     print(type(similar_texts))\n    for i in range(5):\n        pid = relevant_embeddings['paper_id'][similar_texts[i]]\n#         title = md[(md['s2_id']==pid) | (md['who_covidence_id']==pid) | (md['cord_uid']==pid) | (md['pubmed_id']==pid) | (md['arxiv_id']==pid) | (md['pmcid']==pid) | (md['sha']==pid) | (md['mag_id']==pid)]['title'].reset_index(drop=True)\n        print(bcolors.OKCYAN + 'Paper ID : ' + bcolors.ENDC + bcolors.FAIL + str(pid) + bcolors.ENDC)\n#         if len(title)<1:\n#             print(\"NO TITLE !?\")\n#         else:\n#             print('Title: '+str(title[0]))\n        print(bcolors.OKCYAN + 'Summary/Abstract: ' + bcolors.ENDC)\n        print(relevant_embeddings['summary'][similar_texts[i]])\n        print(bcolors.OKGREEN  + \"----------------------------------------------------------------------------------\" + bcolors.ENDC)\n\n\n\n## Given a query, returns embeddings of papers which contain relevant terms to the query according to SNOMED\ndef snomed_filter(query,count=1):\n    print(\"IN SNOMED FILTER COUNT IS \"+str(count))\n    ## split_query into words and remove stopwords\n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(query)\n    words = [w for w in word_tokens if w not in stop_words]\n    \n    words=[word.lower() for word in words if word.isalpha()]\n    words = [w for w in words if w not in ['is','are']]\n    ## search all words for relevant snomed ids\n    rel_ids = [find_relevant_papers(word) for word in words]\n\n    ## check for intersection of paper ids with more than 20 papers\n    rel_ids_flat = [item for sublist in rel_ids for item in sublist]\n    relevant_papers = list(set([i for i in rel_ids_flat if rel_ids_flat.count(i)>count]))\n    \n\n    ## if intersection is very small just return the union\n    if len(relevant_papers)<20:\n        #print(\"WARNING: The intersection of snomed ids was too small, searching in union\")\n        relevant_papers = list(set(rel_ids_flat))\n        \n    #return relevant_papers \n    ## Finally, if union is still small, relevant embeddings are all papers\n    if len(relevant_papers)<20:\n        print(\"WARNING: The union of snomed ids was too small, searching all papers\")\n        relevant_embeddings = emb_df\n    else:\n        relevant_embedding_indexes = [i for i in range(emb_df.shape[0]) if emb_df['paper_id'][i] in relevant_papers]\n        relevant_embeddings = emb_df.iloc[relevant_embedding_indexes].reset_index(drop=True)\n\n    return relevant_embeddings\n\n\n## Given a single term, searches for papers in which a term or a relevant term appears\ndef find_relevant_papers(search_term):\n    \n    ## Get rows with relevant term in 'snomed_superclasses' column\n    relevant_words = snomed_words[snomed_words['snomed_superclasses'].str.contains(search_term,case=False)].reset_index(drop=True)\n    ## Get rows with relevant term in 'snomed_descriptions' column\n    relevant_words = relevant_words.append(snomed_words[snomed_words['snomed_descriptions'].str.contains(search_term,case=False)]).reset_index(drop=True)\n    #print(\" relevant snomed IDs to word '\"+search_term+\"'\")\n#     print(\"\\t\"+str(len(relevant_words))+\" terms from SNOMED are relevant to '\"+search_term+\"' and appear in the corpus:\")\n    for r in relevant_words['word']:\n        print(\"\\t\\t\"+r)\n    ## Get the SNOMED ids contained in retrieved rows\n    relevant_ids = [s[2:-2].replace('\"','').replace(\"'\",'').split(', ') for s in relevant_words['snomed_ids']]\n    ## Flatten list\n    relevant_ids = [item for sublist in relevant_ids for item in sublist]\n    ## Remove duplicates\n    relevant_ids = list(set(relevant_ids))\n    #print(\"These appear in \"+str(len(relevant_ids))+\" papers in the corpus.\")\n\n    ## Search for appearances of found SNOMED ids in our annotation set\n    if len(relevant_ids)>0:\n        #print(\"\\tFinding papers annotated with relevant IDs\")\n        r = relevant_ids[0]\n        relevant_papers = id_dict[r]\n        for i in range(1,len(relevant_ids)):\n            r = relevant_ids[i]\n            relevant_papers = relevant_papers + id_dict[r]\n            \n        print(\"\\t\"+str(len(list(set(relevant_papers))))+\" papers contain at least one of the above terms, based on the keyword: \"+search_term)\n    else:\n        relevant_papers=[]\n        print('\\tNo relevant papers')\n    return list(set(relevant_papers))\n\n\ndef get_similar_texts(input_texts,target_embeddings,bc):\n\n    #print('input texts type : '+str(type(input_texts)))\n    #input_embeddings = [bc.encode(i) for i in input_texts]\n    input_embeddings = bc.encode(input_texts)\n    #input_embeddings = bc.predict(input_texts)\n    similarities = np.zeros((len(input_embeddings),len(target_embeddings)))\n\n    for i in range(len(input_embeddings)):\n    \tsimilarities[i] = np.array([cosine_distance(input_embeddings[i],t) for t in target_embeddings])\n\n    cumulative_similarities = np.sum(similarities,axis=0)\n#     print(cumulative_similarities.shape)\n    #print(cumulative_similarities)\n\n    return np.argsort(-1*cumulative_similarities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def_query = \"What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?\"\n\n@interact_manual\ndef search_articles(query=def_query, \n                    filter_dataset = [False, True],\n                    use_synonyms = [False, True],\n                    count = widgets.IntSlider(min=0, max=6, step=1, value=4)):\n    answer_query(query, bc, filter_dataset, use_synonyms, count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}