{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **COVID-19 Open Research Dataset Challenge (CORD-19)**"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.ovpm.org/wp-content/uploads/2020/03/chla-what-you-should-know-covid-19-1200x628-01.jpg)"},{"metadata":{},"cell_type":"markdown","source":"This is a joint work by [Moshe Hazoom](https://www.kaggle.com/hazoom), [Sarah June Sachs](https://www.kaggle.com/sarahjune) and [Kevin Benassuly](https://www.kaggle.com/kevinbenassuly)."},{"metadata":{},"cell_type":"markdown","source":"# **Goal**\nOur goal is to build an infrastructure that can serve whoever fights the novel COVID-19 virus (researches, doctors, health care workers, etc.) by finding the most useful information using state-of-the-art NLP tools and algorithms. We hope this project will be useful and that our efforts will yield fruits to make our world without the COVID-19 virus. \n"},{"metadata":{},"cell_type":"markdown","source":"# **Project Description**\nThis project consists of different modules that serve together as a full pipeline in order to extract relevant, useful and accurate information from the the scholarly articles.\nWe believe that our solution is capable to mine information that answers the research question accurately, flexible enough in order to support future research questions and easy to understand. \nWe will describe in details the different modules below:\n1. **Data Preprocessing** - ETL, Keyword Extraction & Word Embeddings.\n2. **Topic Modeling** - LDA model.\n3. **Search Engine for Seed Sentences** - Simple but useful search engine to find seed sentences using keywords.\n4. **Semantic Search for Relevant Sentences** - Find relevant answers from seed sentences from #3 using sentence embedding techniques.\n5. **Answer Summarization** - Generate abstractive summary for answers using Facebook's BART model.\n\nAll the code and notebooks are availabe in the Github [repo](https://github.com/Hazoom/covid19)."},{"metadata":{},"cell_type":"markdown","source":"# **Pros/Cons**\n\n## Pros\n1. Our solution finds information to all the research questions in accuracte, consice and informative manner.\n2. It's simple to understand, read and reproduce results.\n3. Easy to expand to other research questions and domains.\n\n## Cons\n1. The keyword generation for each task can be improved to better use the search engine for seed sentences.\n2. There is some manual work needed to pick the best seed sentences for each sub-task in each task. In most cases, we took the first 1-3 sentences that the search engine returned, but it wasn't the case for all search queries. At this point, after that we marked for each query the best results out of 10, we can better improve the search engine against gold-data set with a defined evaluation metric for information retrieval tasks (e.g. [nDCG - Normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)) . We believe this can be further improved and might be an interesting direction for future research."},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing**\n\n## Article Filtering\nIn order to be focusing on relevant articles only, we filtered out articles that are not related specificaly to COVID-19 by using [@ajrwhite](https://www.kaggle.com/ajrwhite)'s list of keywords (thanks!) and took only articles that contains at least one word from the following list:"},{"metadata":{"trusted":true},"cell_type":"code","source":"[\"2019-ncov\", \"2019 novel coronavirus\", \"coronavirus 2019\", \"coronavirus disease 19\", \"covid-19\", \"covid 19\", \"ncov-2019\", \"sars-cov-2\", \"wuhan coronavirus\", \"wuhan pneumonia\", \"wuhan virus\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ETL\nFor each article we did the following:\n1. Parsed its full text using [scispacy](https://allenai.github.io/scispacy/) and split it into sentence. The sentence segmentation part was done using Microsoft's [BlingFire](https://github.com/microsoft/BlingFire) library since we've noticed that scispacy had difficulties to split some text into sentences and kept a very long text. The Code for sentence segmentation is available in [blingfire_sentence_splitter.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/blingfire_sentence_splitter.py) and [common_sentence_splitter.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/common_sentence_splitter.py).\n2. Cleaned the text by normalizing non ASCII characters, fixing contractions, removing URLs, removing punctuations, removing stop-words, etc. The cleaning code is available in [cleaning.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/cleaning.py).\n3. Transformed the sentence to contains meaningful bi-grams and tri-grams. Detailed explanation below.\n4. Created a metadata CSV file such that each row contains a sentence, its cleaned version, the section it came from (abstract, body) and the article metadata it came from. The code is available in [preprocess.py](https://github.com/Hazoom/covid19/blob/master/src/preprocessing/preprocess.py)."},{"metadata":{},"cell_type":"markdown","source":"The parsing method with scispacy (for demonstration purposes, it doesn't include our custom sentence segmantation):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install scispacy package\n!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport scispacy\n\nnlp = spacy.load(\"../input/scispacymodels/en_core_sci_sm/en_core_sci_sm-0.2.4\")\nnlp.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cleaning method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nCURRENCIES = {'$': 'USD', 'zł': 'PLN', '£': 'GBP', '¥': 'JPY', '฿': 'THB',\n              '₡': 'CRC', '₦': 'NGN', '₩': 'KRW', '₪': 'ILS', '₫': 'VND',\n              '€': 'EUR', '₱': 'PHP', '₲': 'PYG', '₴': 'UAH', '₹': 'INR'}\n\nRE_NUMBER = re.compile(\n    r\"(?:^|(?<=[^\\w,.]))[+–-]?\"\n    r\"(([1-9]\\d{0,2}(,\\d{3})+(\\.\\d*)?)|([1-9]\\d{0,2}([ .]\\d{3})+(,\\d*)?)|(\\d*?[.,]\\d+)|\\d+)\"\n    r\"(?:$|(?=\\b))\")\n\nRE_URL = re.compile(\n    r'((http://www\\.|https://www\\.|http://|https://)?' +\n    r'[a-z0-9]+([\\-.][a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(/.*)?)')\n\n# English Stop Word List (Standard stop words used by Apache Lucene)\nSTOP_WORDS = {\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\",\n              \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\",\n              \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom typing import List\nimport ftfy\nimport contractions\n\ndef clean_tokenized_sentence(tokens: List[str],\n                             unicode_normalization=\"NFC\",\n                             unpack_contractions=False,\n                             replace_currency_symbols=False,\n                             remove_punct=True,\n                             remove_numbers=False,\n                             lowercase=True,\n                             remove_urls=True,\n                             remove_stop_words=True) -> str:\n    if remove_stop_words:\n        tokens = [token for token in tokens if token not in STOP_WORDS]\n\n    sentence = ' '.join(tokens)\n\n    if unicode_normalization:\n        sentence = ftfy.fix_text(sentence, normalization=unicode_normalization)\n\n    if unpack_contractions:\n        sentence = contractions.fix(sentence, slang=False)\n\n    if replace_currency_symbols:\n        for currency_sign, currency_tok in CURRENCIES.items():\n            sentence = sentence.replace(currency_sign, f'{currency_tok} ')\n\n    if remove_urls:\n        sentence = RE_URL.sub('_URL_', sentence)\n\n    if remove_punct:\n        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n\n    # strip double spaces\n    sentence = re.sub(r' +', ' ', sentence)\n\n    if remove_numbers:\n        sentence = RE_NUMBER.sub('_NUMBER_', sentence)\n\n    if lowercase:\n        sentence = sentence.lower()\n\n    return sentence\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting it all together:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_sentence(sentence) -> str:\n    doc = nlp(sentence)\n    tokens = [str(token) for token in doc]\n    return clean_tokenized_sentence(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clean_sentence(\"Let's clean this sentence!\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the output of ETL process:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_df = pd.read_csv('../input/covid19sentencesmetadata/sentences_with_metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Sentence count: {len(sentences_df)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training bi-gram Model\nSome words on its own doesn't give a lot of information, but when coming together, the meaning is changing to something else. Our goal was to transform meaningful bi-gram phrases to one token, for example: `fake news` to `fake_news`. For that, we used [Gensim](https://radimrehurek.com/gensim/)'s Phrases package, that has two implementations: 1. Data-Driven approach and 2. NPMI (Normalized Pointwise Mutual Information) score. We won't show here the training, since it takes times, but we will load our trained model and see some examples. The trainind code is in our [notebook](https://github.com/Hazoom/covid19/blob/master/notebooks/Taxonomy/Topic_Model_LDA.ipynb).\nWe used a threshold of 10 and minimum count of 5 that worked best for our use case, to build a search engine. One can play with the hyper-parameters for his own use-case, depending on the tradeoff between large number (and less meaningful) of phrases to a smaller (but more meaningful) amount of phrases."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.phrases import Phraser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_model = Phraser.load(\"../input/covid19phrasesmodels/covid_bigram_model_v0.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_model[\"despite social media often vehicle fake news boast news hype also worth noting tremendous effort scientific community provide free uptodate information ongoing studies well critical evaluations\".split()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training tri-gram model\nWe created a tri-gram model, in addition to the bi-gram model, in order to catch more meaningful phrases, like `Epidemic Preparedness Innovations` for example. We transformed the cleaned sentences (from all articles) with bi-grams using the model above and trained a Phrases model with Gensim again, but with a lower threshold this time. Please note that this method can also crerate 4-grams if the model connects between two bi-grams."},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_model = Phraser.load(\"../input/covid19phrasesmodels/covid_trigram_model_v0.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add phrases model to the ETL process and change the clean function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_sentence(sentence, bigram_model=None, trigram_model=None) -> str:\n    doc = nlp(sentence)\n    tokens = [str(token) for token in doc]\n    cleaned_sentence = clean_tokenized_sentence(tokens)\n    \n    if bigram_model and trigram_model:\n        sentence_with_bigrams = bigram_model[cleaned_sentence.split(' ')]\n        sentence_with_trigrams = trigram_model[sentence_with_bigrams]\n        return ' '.join(sentence_with_trigrams)\n    \n    return cleaned_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clean_sentence(\"On 23 January 2020, the Coalition for Epidemic Preparedness Innovations (CEPI) announced that they will fund vaccine development programmes with Inovio\", bigram_model, trigram_model))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FastText Word Embeddings\nWe trained word embeddings model on the full corpus (without filtering out articles) using Facebook's [FastText](https://github.com/facebookresearch/fastText) library. This will serve us later in the Sentence Similarity model to find relevant answers for each question. After training, we also created word counts that serves us in the sentence encoder when calculating the smooth weighted average of the word embeddings of all words in the sentence.\nThe code for training word embeddings using FastText is availabe at [train_fasttext.py](https://github.com/Hazoom/covid19/blob/master/src/w2v/train_fasttext.py)\n\nLet's visualize our word embeddings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.manifold import TSNE\nfrom matplotlib import pylab\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fasttext_model_dir = '../input/fasttext-no-subwords-trigrams'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the 400 most frequent word vectors. The vectors in the file are in descending order of frequency."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_points = 400\n\nfirst_line = True\nindex_to_word = []\nwith open(os.path.join(fasttext_model_dir, \"word-vectors-100d.txt\"),\"r\") as f:\n    for line_num, line in enumerate(f):\n        if first_line:\n            dim = int(line.strip().split()[1])\n            word_vecs = np.zeros((num_points, dim), dtype=float)\n            first_line = False\n            continue\n        line = line.strip()\n        word = line.split()[0]\n        vec = word_vecs[line_num-1]\n        for index, vec_val in enumerate(line.split()[1:]):\n            vec[index] = float(vec_val)\n        index_to_word.append(word)\n        if line_num >= num_points:\n            break\nword_vecs = normalize(word_vecs, copy=False, return_norm=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train t-SNE in order to reduce embeddings to 2-dimension for visualization purpose:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=10000)\ntwo_d_embeddings = tsne.fit_transform(word_vecs[:num_points])\nlabels = index_to_word[:num_points]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the most frequent 400 word vectors in a 2-dimensions plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(embeddings, labels):\n    pylab.figure(figsize=(20,20))\n    for i, label in enumerate(labels):\n        x, y = embeddings[i,:]\n        pylab.scatter(x, y)\n        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n                       ha='right', va='bottom')\n    pylab.show()\n\nplot(two_d_embeddings, labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The visualization of the word vectors of the 400 most frequent words makes sense.\nWe can use those vectors to find synonyms, or related terms, for each input word (or phrase) by comparing its word vectors to the whole corpus vectors using cosine similarity, or any other vectors similarity functions.\nLet's see some examples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\nimport gensim.models.keyedvectors as word2vec\n\nfasttext_model = word2vec.KeyedVectors.load_word2vec_format(os.path.join(fasttext_model_dir, \"word-vectors-100d.txt\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_most_similar(search_term):\n    print(f\"Synonyms of '{search_term}':\")\n    synonyms = fasttext_model.most_similar(search_term)\n    pprint(synonyms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_most_similar(\"new_coronavirus\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_most_similar(\"fake_news\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_most_similar(\"pathogen\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Topic Modeling & Keyword Extraction**\nIn order to understand the content of the corpus and how the text might relate to each task, we extracted relevant topics with the Latent Dirichlet Allocation (LDA) algorithm. We used the [Gensim](https://radimrehurek.com/gensim/) package on the clean version of the sentences within the filtered subset of the corpus.\n\n[Notebook with code for topic modeling](https://github.com/Hazoom/covid19/blob/master/notebooks/Taxonomy/Topic_Model_LDA.ipynb)\n\nSteps taken:\n\n1. Read in cleaned sentences\n2. Build Gensim dictionary with id2word\n3. Structure corpus with doc2bow\n4. Calculate term document frequency\n5. Train the LDA model\n6. Using the full filtered subset of articles did not result in very distinctive topics, below is a snippet with keywords from the 10 topics which were derived:"},{"metadata":{"trusted":true},"cell_type":"code","source":"[(0, '0.079\"•\" + 0.019\"blood\" + 0.015\"associated\" + 0.013\"cells\" + ' '0.012\"ace2\" + 0.012\"protein\" + 0.011\"important\" + 0.011\"levels\" + ' '0.010\"diseases\" + 0.010\"cell\"'), (1, '0.110\"who\" + 0.088\"it\" + 0.056\"response\" + 0.043\"could\" + 0.036\"under\" ' '+ 0.035\"available\" + 0.032\"major\" + 0.032\"as\" + 0.030\"without\" + ' '0.024\"muscle\"'), (2, '0.173\"■\" + 0.020\"some\" + 0.013\"drugs\" + 0.010\"transmission\" + ' '0.009\"surgery\" + 0.009\"must\" + 0.009\"drug\" + 0.009\"there\" + ' '0.008\"increased\" + 0.008\"high\"'), (3, '0.071\"de\" + 0.036\"were\" + 0.025\"patient\" + 0.023\"1\" + 0.022\"after\" + ' '0.018\"a\" + 0.018\"more\" + 0.015\"all\" + 0.015\"when\" + 0.014\"cause\"'), (4, '0.044\"the\" + 0.035\"from\" + 0.028\"should\" + 0.019\"other\" + 0.018\"risk\" ' '+ 0.017\"oral\" + 0.017\"which\" + 0.017\"in\" + 0.013\"use\" + 0.013\"cases\"'), (5, '0.069\"may\" + 0.033\"can\" + 0.031\"have\" + 0.029\"disease\" + 0.028\"dental\" ' '+ 0.022\"also\" + 0.020\"has\" + 0.020\"been\" + 0.018\"health\" + ' '0.016\"virus\"'), (6, '0.051\"la\" + 0.031\"en\" + 0.025\"2\" + 0.023\"3\" + 0.016\"que\" + 0.016\"el\" ' '+ 0.016\"y\" + 0.014\"los\" + 0.014\"4\" + 0.013\"les\"'), (7, '0.045\"s\" + 0.041\"et\" + 0.031\"during\" + 0.023\"al\" + 0.022\"had\" + ' '0.021\"people\" + 0.020\"à\" + 0.018\"local\" + 0.017\"days\" + 0.016\"2020\"'), (8, '0.062\"patients\" + 0.030\"treatment\" + 0.028\"care\" + 0.020\"used\" + ' '0.014\"clinical\" + 0.014\"infection\" + 0.013\"common\" + 0.013\"severe\" + ' '0.013\"respiratory\" + 0.012\"dentistry\"'), (9, '0.030\"using\" + 0.020\"areas\" + 0.018\"ct\" + 0.014\"described\" + ' '0.014\"performed\" + 0.013\"lesions\" + 0.013\"above\" + 0.012\"day\" + ' '0.011\"learning\" + 0.011\"reactions\"')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The lack of distinctive topics is likely due to the corpus range of content which contained a lot of noise. If we run the same LDA on the output from the semantic search for relevant sentences, hopefully clearer topics will emerge. This is an interesting research direction for the future."},{"metadata":{},"cell_type":"markdown","source":"# **Search Engine for Seed Sentences**\nNow, our goal was to find the best (accurate and informative) sentences for each sub-task for each task in the CORD-19 challenge.\nFor that goal, we did the following:\n1. Created a search engine for finding relevant sentences using input keywords. The search engine transforms the input to phrases using our phrases model above and performs Query Expansion technique by adding synonyms (above certain similarity threshold) to the input keywords and ranks sentences by the number of keyword matches and the date of the article the sentence came from (the newest will be ranked higer). In order to focus on articles about COVID-19, the search engine can get optional keywords that boost the sentences containing them. We've tried to use TF-IDF and some other weighting techniques, but our simple method worked best for us.\n2. For each sub-task we created list of keywords the retrieves for us the best result set of sentences that answer the resaerch question in the sub-task.\n3. Picked 1-3 sentences (out of 10) that summarizes the answer in the most accurate, consice and informative manner. Those sentences are considered \"seed sentences\" and will serve us in the next module of sentence similarity. This part was done partially manually and we belive this approach can be automatic with better keyword extraction and a better search engine. This is an interesting area for future research.\n\nAll the examples are listed in the `notebooks` folder in our repo."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_articles_metadata_mapping(sentences_df: pd.DataFrame) -> dict:\n    sentence_id_to_metadata = {}\n    for row_count, row in sentences_df.iterrows():\n        sentence_id_to_metadata[row_count] = dict(\n            paper_id=row['paper_id'],\n            cord_uid=row['cord_uid'],\n            source=row['source'],\n            url=row['url'],\n            publish_time=row['publish_time'],\n            authors=row['authors'],\n            section=row['section'],\n            sentence=row['sentence'],\n        )\n    return sentence_id_to_metadata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_id_to_metadata = create_articles_metadata_mapping(sentences_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\nfrom datetime import datetime\n\nclass SearchEngine:\n    def __init__(self,\n                 sentence_id_to_metadata: dict,\n                 sentences_df: pd.DataFrame,\n                 bigram_model,\n                 trigram_model,\n                 fasttext_model):\n        self.sentence_id_to_metadata = sentence_id_to_metadata\n        self.cleaned_sentences = sentences_df['cleaned_sentence'].tolist()\n        print(f'Loaded {len(self.cleaned_sentences)} sentences')\n\n        self.bigram_model = bigram_model\n        self.trigram_model = trigram_model\n        self.fasttext_model = fasttext_model\n\n    def _get_search_terms(self, keywords, synonyms_threshold):\n        # clean tokens\n        cleaned_terms = [clean_tokenized_sentence(keyword.split(' ')) for keyword in keywords]\n        # remove empty terms\n        cleaned_terms = [term for term in cleaned_terms if term]\n        # create bi-grams\n        terms_with_bigrams = self.bigram_model[' '.join(cleaned_terms).split(' ')]\n        # create tri-grams\n        terms_with_trigrams = self.trigram_model[terms_with_bigrams]\n        # expand query with synonyms\n        search_terms = [self.fasttext_model.most_similar(token) for token in terms_with_trigrams]\n        # filter synonyms above threshold (and flatten the list of lists)\n        search_terms = [synonym[0] for synonyms in search_terms for synonym in synonyms\n                        if synonym[1] >= synonyms_threshold]\n        # expand keywords with synonyms\n        search_terms = list(terms_with_trigrams) + search_terms\n        return search_terms\n\n    def search(self,\n               keywords: List[str],\n               optional_keywords=None,\n               top_n: int = 10,\n               synonyms_threshold=0.7,\n               keyword_weight: float = 3.0,\n               optional_keyword_weight: float = 0.5) -> List[dict]:\n        if optional_keywords is None:\n            optional_keywords = []\n\n        search_terms = self._get_search_terms(keywords, synonyms_threshold)\n\n        optional_search_terms = self._get_search_terms(optional_keywords, synonyms_threshold) \\\n            if optional_keywords else []\n\n        print(f'Search terms after cleaning, bigrams, trigrams and synonym expansion: {search_terms}')\n        print(f'Optional search terms after cleaning, bigrams, trigrams and synonym expansion: {optional_search_terms}')\n\n        date_today = datetime.today()\n\n        # calculate score for each sentence. Take only sentence with at least one match from the must-have keywords\n        indexes = []\n        match_counts = []\n        days_diffs = []\n        for sentence_index, sentence in enumerate(self.cleaned_sentences):\n            sentence_tokens = sentence.split(' ')\n            sentence_tokens_set = set(sentence_tokens)\n            match_count = sum([keyword_weight if keyword in sentence_tokens_set else 0\n                               for keyword in search_terms])\n            if match_count > 0:\n                indexes.append(sentence_index)\n                if optional_search_terms:\n                    match_count += sum([optional_keyword_weight if keyword in sentence_tokens_set else 0\n                                       for keyword in optional_search_terms])\n                match_counts.append(match_count)\n                article_date = self.sentence_id_to_metadata[sentence_index][\"publish_time\"]\n\n                if article_date == \"2020\":\n                    article_date = \"2020-01-01\"\n\n                article_date = datetime.strptime(article_date, \"%Y-%m-%d\")\n                days_diff = (date_today - article_date).days\n                days_diffs.append(days_diff)\n\n        # the bigger the better\n        match_counts = [float(match_count)/sum(match_counts) for match_count in match_counts]\n\n        # the lesser the better\n        days_diffs = [(max(days_diffs) - days_diff) for days_diff in days_diffs]\n        days_diffs = [float(days_diff)/sum(days_diffs) for days_diff in days_diffs]\n\n        index_to_score = {}\n        for index, match_count, days_diff in zip(indexes, match_counts, days_diffs):\n            index_to_score[index] = 0.7 * match_count + 0.3 * days_diff\n\n        # sort by score descending\n        sorted_indexes = sorted(index_to_score.items(), key=operator.itemgetter(1), reverse=True)\n\n        # take only the sentence IDs\n        sorted_indexes = [item[0] for item in sorted_indexes]\n\n        # limit results\n        sorted_indexes = sorted_indexes[0: min(top_n, len(sorted_indexes))]\n\n        # get metadata for each sentence\n        results = []\n        for index in sorted_indexes:\n            results.append(self.sentence_id_to_metadata[index])\n        return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_engine = SearchEngine(sentence_id_to_metadata, sentences_df, bigram_model, trigram_model, fasttext_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(keywords, optional_keywords=None, top_n=10, synonyms_threshold=0.8, only_sentences=False):\n    print(f\"\\nSearch for terms {keywords}\\n\\n\")\n    results = search_engine.search(\n        keywords, optional_keywords=optional_keywords, top_n=top_n, synonyms_threshold=synonyms_threshold\n    )\n    print(\"\\nResults:\\n\")\n    \n    if only_sentences:\n        for result in results:\n            print(result['sentence'] + \"\\n\")\n    else:\n        pprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search(keywords=[\"spillover\", \"bats\", \"snakes\", \"exotic animals\", \"seafood\"],\n       optional_keywords=[\"new coronavirus\", \"coronavirus\", \"covid19\"],\n      top_n=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the end of this stage, we had seed sentences for each sub-task."},{"metadata":{"trusted":true},"cell_type":"code","source":"task_id = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nwith open(f\"../input/covid19seedsentences/{task_id}.json\") as in_fp:\n    seed_sentences_json = json.load(in_fp)\n\nprint(seed_sentences_json['taskName'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Semantic Search for Relevant Sentences**\nAfter finding seed sentences, we wanted to expand our evidences by finding sentences with similar semantic meaning in the whole corpus (that haven't came up in the search engine from previous phase) in order to collect more information and evidence to support the research question.\n\nThere are many methods and techniques for sentence embedding in the NLP litretature and from our vast experience in the field we chose to use to implement 2 and to ty both\n1. The techniques from the paper [\"A Simple but Tough-to-Beat Baseline for Sentence Embeddings\"](https://openreview.net/forum?id=SyK00v5xx) (Sanjeev Arora, Yingyu Liang, Tengyu Ma). In their work, they use a pretrained word embedding model on unsupervised large corpus and in order to create embedding for the whole sentence, they use a smooth weighted average on the word embeddings of the the words in the sentence, and remove the 1st principal component from the vector after performing a dimension reduction technique (e.g. SVD or PCA). The latter improves to reduce noise from the sentence embeddings. We found that using phrases (bi-grams and tri-grams) in the semantic search engine, using this technique, performed worser than with them so we decided to not using them for the semantic search engine part.\n2. Using fine-tuned BERT on Stanford Natural Language Inference (SNLI) task that predicts if two sentences are semantically related or not. For the implementation of BERT encoder, we used UKP's sentence-transformers library. We chose to use the model called `bert-base-nli-stsb-mean-tokens`.\n\nWe developed a method that gets list of sentences and retrieves similar semantic sentences to them. Because the input is not just one sentence, we had to aggerage them in different manngers. We did research one some aggregation techniques:\n1. Union\n2. Mean\n3. 1st Principal Component (pc_1)\n4. 2nd Principal Component (pc_2)\n\nThe method that worked best was fine-tuned BERT (#2) with Union aggregation.\n\nWe iterate through all the sentences in our filtered corpus and encoded the sentences using the techniques mentioned above. When giving a new sentence, we encoded it and compared it to all existing pre-encoded sentences we have in our index (using Cosine Similarity function between the vectors) in order to get the most similar sentences for each input sentence. We used `nmslib` for storing the pre-encoded sentences and to perform the similarity measures. All code is available in our repo in [corpus_indx](https://github.com/Hazoom/covid19/tree/master/src/corpus_index) and [encoders](https://github.com/Hazoom/covid19/tree/master/src/encoders). \n\nCredit to [Tal Almagor](https://github.com/talmago) for helping to develop this module."},{"metadata":{},"cell_type":"markdown","source":"# **Answer Summarization**\nThe final module in our project is the abstractive answer summarization. The goal is to build informative and consice answer for each sub-task using the relevant sentences we found from previous task. We chose to use Facebook's [BART](https://arxiv.org/abs/1910.13461) model. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. BART is really useful for text generation tasks, hence we chose it for this task. We use HuggingFace's great library [transformers](https://github.com/huggingface/transformers) for that end. A future improvement here can be to fine-tune BART model with COVID-19 articles. Due to lack of resources, we took the pretrained BART model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# This will take time on the first time since it downloads the model\ntokenizer_summarize = BartTokenizer.from_pretrained('bart-large-cnn')\nmodel_summarize = BartForConditionalGeneration.from_pretrained('bart-large-cnn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BartSummarizer:\n    def __init__(self, tokenizer_summarize, model_summarize):\n        self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.tokenizer_summarize = tokenizer_summarize\n        self.model_summarize = model_summarize\n        self.model_summarize.to(self.torch_device)\n        self.model_summarize.eval()\n\n    def create_summary(self, text: str,\n                       repetition_penalty=1.0) -> str:\n        text_input_ids = self.tokenizer_summarize.batch_encode_plus(\n            [text], return_tensors='pt', max_length=1024)['input_ids']\n        \n        min_length = min(text_input_ids.size()[1], 128)\n        max_length = min(text_input_ids.size()[1], 1024)\n\n        print(f\"Summary Min length: {min_length}\")\n        print(f\"Summary Max length: {max_length}\")\n\n        text_input_ids = text_input_ids.to(self.torch_device)\n\n        summary_ids = self.model_summarize.generate(text_input_ids,\n                                                    num_beams=4,\n                                                    length_penalty=1.4,\n                                                    max_length=max_length,\n                                                    min_length=min_length,\n                                                    no_repeat_ngram_size=4,\n                                                    repetition_penalty=repetition_penalty)\n        summary = self.tokenizer_summarize.decode(summary_ids.squeeze(), skip_special_tokens=True)\n        return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bart_summarizer = BartSummarizer(tokenizer_summarize, model_summarize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Putting it all together**\nNow that we have all the components ready, we can visualize the results for the task.\nWe will iterate the different sub-tasks, for each one we will find relevant sentences and create an abstractive summary."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(f\"../input/covid19seedsentences/{task_id}_relevant_sentences.json\") as in_fp:\n    relevant_sentences_json = json.load(in_fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answers_results = []\nfor idx, sub_task_json in enumerate(relevant_sentences_json[\"subTasks\"]):\n    sub_task_description = sub_task_json[\"sub_task_description\"]\n    print(f\"Working on task: {sub_task_description}\")\n    best_sentences = seed_sentences_json[\"subTasks\"][idx][\"bestSentences\"]\n    relevant_sentences = sub_task_json[\"relevant_sentences\"]\n    relevant_sentences_texts = [result[\"sentence\"] for result in relevant_sentences]\n    sub_task_summary = bart_summarizer.create_summary(\" \".join(best_sentences + relevant_sentences_texts))\n    answers_results.append(dict(sub_task_description=sub_task_description, relevant_sentences=relevant_sentences, sub_task_summary=sub_task_summary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's visualize the results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, HTML\npd.set_option('display.max_colwidth', 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_summary(summary: str):\n    return display(HTML(f\"<div>{summary}</div>\"))\n\ndef display_sub_task_description(sub_task_description):\n    return display(HTML(f\"<h2>{sub_task_description}</h2>\"))\n\ndef display_task_name(task_name):\n    return display(HTML(f\"<h1>{task_name}</h1>\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_output(sub_task_json):\n    \"\"\"\n    Prints output for each sub-task\n    \"\"\"\n    # print description\n    display_sub_task_description(sub_task_json.get(\"sub_task_description\"))\n    display_summary(sub_task_json.get(\"sub_task_summary\"))\n\n    # print output sentences\n    results = sub_task_json.get('relevant_sentences')\n    sentence_output = pd.DataFrame(sub_task_json.get('relevant_sentences'))\n    sentence_output.rename(columns={\"sentence\": \"Relevant Sentence\",\"cord_id\": \"CORD UID\",\n                                    \"publish_time\": \"Publish Time\", \"url\": \"URL\",\n                                    \"source\": \"Source\"}, inplace=True)\n    \n    display(HTML(sentence_output[['cord_uid', 'Source', 'Publish Time', 'Relevant Sentence', 'URL']].to_html(render_links=True, escape=False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_task_name(seed_sentences_json[\"taskName\"])\nfor sub_task_json in answers_results:\n    visualize_output(sub_task_json)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the results looks informative and relevant to the research topic."},{"metadata":{},"cell_type":"markdown","source":"# Extracting Topics From Answers"},{"metadata":{},"cell_type":"markdown","source":"## Aggregate all sentences into a dataframe to use LDA on relevant sentences to see if distincitve topics emerge"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_output(seed_sentences, sub_task_json):\n    \"\"\"\n    Saves output for each sub-task\n    \"\"\"\n    sentence_output = pd.DataFrame(sub_task_json.get('relevant_sentences'))\n    sentence_output.rename(columns={\"sentence\": \"Relevant Sentence\",\"cord_id\": \"CORD UID\",\n                                    \"publish_time\": \"Publish Time\", \"url\": \"URL\",\n                                    \"source\": \"Source\"}, inplace=True)\n    \n    return sentence_output[['cord_uid', 'Source', 'Publish Time', 'Relevant Sentence', 'URL']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_sentences = []\nfor idx, sub_task_json in enumerate(answers_results):\n    task_sentences = save_output(seed_sentences_json[\"subTasks\"][idx][\"bestSentences\"], sub_task_json)\n    relevant_sentences.append(task_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_relevant_sentences = pd.concat(relevant_sentences).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_relevant_sentences.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean relevant sentences and add bigrams and trigrams:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_sentences = []\nfor i in range(len(all_relevant_sentences['Relevant Sentence'])):\n    cleaned_sentences.append(clean_sentence(all_relevant_sentences['Relevant Sentence'][i], bigram_model, trigram_model).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_sentences[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare our corpus for LDA model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import corpora\n\n# Create Dictionary\nid2word = corpora.Dictionary(cleaned_sentences)\n\n# Create Corpus\ntexts = cleaned_sentences\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the Keyword in the topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe hope this notebook will help researches in the medical domain to better gain information about the new COVID-19 virus.\nPlease feel free to to use this notebook for your own needs.\n\nAny comments and upvotes will be much appreciated."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}