{"cells":[{"metadata":{},"cell_type":"markdown","source":"In my [previous notebook][1], I provided some background on ammonium, performed exploratory data analysis (EDA), and attempted to use a linear regression to predict ammonium levels in a stream in the Ukraine. Here, we're going to explore the usefulness of decision trees, random forests, and support vector regression as predictive models for the same dataset. \n\n[1]: https://www.kaggle.com/jessicaleger/ammonium-predictions-in-river-water-model","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the previous notebook included the EDA, I'll proceed to loading both the training and testing datasets, cleaning up and splitting the data before moving on to creating the models. ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ammonium_train=pd.read_csv('/kaggle/input/ammonium-prediction-in-river-water/train.csv')\nammonium_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ammonium_test=pd.read_csv('/kaggle/input/ammonium-prediction-in-river-water/test.csv')\nammonium_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ammonium_train.drop(ammonium_train[['3','4','5','6','7']], axis=1, inplace=True)\nammonium_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ammonium_train.dropna(inplace=True)\nammonium_train.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=ammonium_train[['1','2']]\ny=ammonium_train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first model we will assess is a simple regression tree:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree=DecisionTreeRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ammonium_train_predictions=dtree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the mean squared error for the decision tree is\", mse(y_test, ammonium_train_predictions))\nprint(\"the r2 score for the decision tree is\", r2_score(y_test, ammonium_train_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The decision tree has a lower MSE than just a simple linear regression (line 29 in [this notebook][1]), which means that this model better fits the test data. A better fit might not always be a good thing, as overfitting may occur and therefore lead to inaccurate predictions on new data. This model also has a lower $r^2$ score, which means that a lower proportion of the variance can be explained by this model. Therefore, we can't say that this model performs better than the baseline model. Here is what the regression tree looks like:\n\n[1]: https://www.kaggle.com/jessicaleger/ammonium-predictions-in-river-water-model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"export_graphviz(dtree, out_file = 'tree.dot', rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')\nfrom IPython.display import Image\nImage('tree.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try boosting the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_boosted= AdaBoostRegressor(dtree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_boosted.fit(X_train, y_train)\nammonium_train_predictions_boost=dtree_boosted.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the mean squared error for the decision tree is\", mse(y_test, ammonium_train_predictions_boost))\nprint(\"the r2 score for the decision tree is\", r2_score(y_test, ammonium_train_predictions_boost))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MSE and $r^2$ appear to have improved with boosting. One concern about a decreasing MSE could be that as models increase in complexity, the risk of overfitting increases. However, boosting has proven to be robust against overfitting. While the MSE is lower on this data using decision tree models, the $r^2$ is still lower than using the baseline model, meaning that more of the variance can be explained by using a simple linear model alone. \n\nvisualize one tree from the boosted decision tree:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_tree_5=dtree_boosted.estimators_[5]\n\nexport_graphviz(sub_tree_5, out_file = 'tree.dot', rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')\nfrom IPython.display import Image\nImage('tree.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision trees tend not to have very good predictive accuracy due to high variance. Random forests reduce this variance and should thus improve predictive accuracy. Let's see if this is the case for our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ammonium_train_pred_rf=rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the mean squared error for the random forest regression is\", mse(y_test, ammonium_train_pred_rf))\nprint(\"the r2 score for the random forest regression is\", r2_score(y_test, ammonium_train_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The r2 of the random forest is in fact higher than both the non-boosted and boosted regression trees. Its MSE is lower, which may lead to concerns of overfitting. At this stage, a simple linear regression is still able to explain more of the variance than any of the models in this notebook, and would probably generalize better too.\n\nLet's visualize one tree from the random forest:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_tree_2=rf.estimators_[2]\n\nexport_graphviz(sub_tree_2, out_file = 'tree.dot', rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')\nfrom IPython.display import Image\nImage('tree.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we're going to boost the random forest. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_boosted= AdaBoostRegressor(rf)\n\nrf_boosted.fit(X_train, y_train)\nammonium_train_pred_rf_b=rf_boosted.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the mean squared error for the decision tree is\", mse(y_test, ammonium_train_pred_rf_b))\nprint(\"the r2 score for the decision tree is\", r2_score(y_test, ammonium_train_pred_rf_b))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're starting to see diminishing returns when we boost the random forest. The $r^2$ is starting to go down and the mse has gone up. This means that the boosted forest could generalize better than the random forest alone, but less of the variance can be explained by the model. While boosting is considered a good tool, it can sometimes perform worse than a simpler model that uses less computing power. Let's visualize a tree from this model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_tree_3=rf_boosted.estimators_[3].estimators_[3]\n\nexport_graphviz(sub_tree_3, out_file = 'tree.dot', rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')\nfrom IPython.display import Image\nImage('tree.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's take a look at how support vector regression performs on this data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVR_rbf=SVR(kernel='rbf')\nSVR_rbf.fit(X_train, y_train)\nSVR_pred=SVR_rbf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the mean squared error for SVR with rbf kernel is\", mse(y_test, SVR_pred))\nprint(\"the r2 score for SVR with rbf kernel is\", r2_score(y_test, SVR_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVR_lin=SVR(kernel='linear')\nSVR_lin.fit(X_train, y_train)\nSVR_lin_pred=SVR_lin.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the mean squared error for SVR with linear kernel is\", mse(y_test, SVR_lin_pred))\nprint(\"the r2 score for SVR with linear kernel is\", r2_score(y_test, SVR_lin_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVR_poly=SVR(kernel='poly')\nSVR_poly.fit(X_train, y_train)\nSVR_poly_pred=SVR_poly.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the mean squared error for SVR with polynomial kernel is\", mse(y_test, SVR_poly_pred))\nprint(\"the r2 score for SVR with polynomial kernel is\", r2_score(y_test, SVR_poly_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import timeit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark_results = pd.DataFrame(columns=[\"Code\", \"Trial 1 (ms)\", \"Trial 2 (ms)\", \"Trial 3 (ms)\", \"Mean (ms)\"])\nbenchmark_codes = ['dtree', 'dtree_boosted','rf', 'rf_boosted', 'SVR_rbf', 'SVR_lin', 'SVR_poly']\n\nfor index, code in enumerate(benchmark_codes):\n    row = [code]\n    results = timeit.repeat(f'{code}.predict(X_test)', f'from __main__ import {\",\".join(globals())}', repeat=3, number=10)\n    row.extend(results)\n    row.append(sum(results)/len(results))\n    benchmark_results.loc[index] = row\n\nbenchmark_results.round(decimals=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, the most suitable candidate model for predicting ammonium downstream for this area would be the random forest model. Support vector regression with a polynomial kernel had the second highest MSE after the baseline model (simple linear regression), but it also had the lowest $r^2$.  The boosted forest was a step down for the random forest, and it also took the most time to compute. The random forest and SVR with a linear kernel performed similarly, however the SVR performed the regression more quickly. In addition, the baseline model had the highest $r^2$ and MSE, meaning it could explain the most variance and it would probably generalize the best. It is also the simplest model and would be the easiest to explain. The issue with using linear models on this data is that the data are not normally distributed, and therefore violate one of the assumptions used in linear regression. Future work might include running a weighted least squares regression on the data, as this model works with heteroskedastic data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}