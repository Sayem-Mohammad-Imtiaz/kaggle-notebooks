{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T19:43:41.004772Z","iopub.execute_input":"2021-07-27T19:43:41.00539Z","iopub.status.idle":"2021-07-27T19:43:41.015308Z","shell.execute_reply.started":"2021-07-27T19:43:41.005339Z","shell.execute_reply":"2021-07-27T19:43:41.01441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction\n\nThe data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units on diverse product categories The dataset is available at UCI machine learning repository.\n\nData Set Information:\n\nProvide all relevant information about your data set.\n\n\n### Attribute Information:\n\n1) FRESH: annual spending on fresh products (Continuous);\n2) MILK: annual spending on milk products (Continuous);\n3) GROCERY: annual spending on grocery products (Continuous);\n4) FROZEN: annual spending on frozen products (Continuous)\n5) DETERGENTS_PAPER: annual spending on detergents and paper products (Continuous)\n6) DELICATESSEN: annual spending on and delicatessen products (Continuous);\n7) CHANNEL: customers™ Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)\n8) REGION: customers™ Region“ Lisnon, Oporto or Other (Nominal)\n\n### Descriptive Statistics:\n\n(Minimum, Maximum, Mean, Std. Deviation)\nFRESH ( 3, 112151, 12000.30, 12647.329)\nMILK (55, 73498, 5796.27, 7380.377)\nGROCERY (3, 92780, 7951.28, 9503.163)\nFROZEN (25, 60869, 3071.93, 4854.673)\nDETERGENTS_PAPER (3, 40827, 2881.49, 4767.854)\nDELICATESSEN (3, 47943, 1524.87, 2820.106)\n\nREGION Frequency\nLisbon 77\nOporto 47\nOther Region 316\nTotal 440\n\nCHANNEL Frequency\nHoreca 298\nRetail 142\nTotal 440\n\n### Implementation:\n\n    EDA and any data cleaning\n    Implemented Feature Scaling to Normalize the data(compare the histogram and KDE for MinMaxScaler and StandardScaler)\n    Finding optimal number of features using RFECV and shown the plot between Number of features selected vs Cross validation score (used channel as target variable)\n    Implemented KMeans Clustering for K=2 to K=15 and based on elbow method identify what is the optimum number of clusters\n    Implemented PCA with number of original features to answer how much variance is explained by first 2 components and by first 4 components and visualize the clusters in the data\n    Implemented XGBoost Classifier with 5 Fold CV and report the performance metrics\n\n### Task:\nGoal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with.\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # for plotting facilities\nimport seaborn as sns; \nsns.set(color_codes=True)\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.023264Z","iopub.execute_input":"2021-07-27T19:43:41.023951Z","iopub.status.idle":"2021-07-27T19:43:41.032231Z","shell.execute_reply.started":"2021-07-27T19:43:41.0239Z","shell.execute_reply":"2021-07-27T19:43:41.03139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/wholesale-customers-data-set/Wholesale customers data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.037152Z","iopub.execute_input":"2021-07-27T19:43:41.037768Z","iopub.status.idle":"2021-07-27T19:43:41.051917Z","shell.execute_reply.started":"2021-07-27T19:43:41.037716Z","shell.execute_reply":"2021-07-27T19:43:41.050938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Shape of Data set","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.053509Z","iopub.execute_input":"2021-07-27T19:43:41.054051Z","iopub.status.idle":"2021-07-27T19:43:41.061592Z","shell.execute_reply.started":"2021-07-27T19:43:41.054Z","shell.execute_reply":"2021-07-27T19:43:41.06059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"440 instances and 8 attributes ","metadata":{}},{"cell_type":"markdown","source":"##  Preview dataset ","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.069231Z","iopub.execute_input":"2021-07-27T19:43:41.069606Z","iopub.status.idle":"2021-07-27T19:43:41.084817Z","shell.execute_reply.started":"2021-07-27T19:43:41.069571Z","shell.execute_reply":"2021-07-27T19:43:41.083479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Channel unique values:\",df['Channel'].unique())\nprint(\"Region unique values\",df['Region'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.087472Z","iopub.execute_input":"2021-07-27T19:43:41.087947Z","iopub.status.idle":"2021-07-27T19:43:41.095733Z","shell.execute_reply.started":"2021-07-27T19:43:41.0879Z","shell.execute_reply":"2021-07-27T19:43:41.094552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that Channel variable contains values as 1 and 2.\n\nThese two values classify the customers from two different channels as\n\n1 for Horeca (Hotel/Retail/Café) customers and\n2 for Retail channel (nominal) customers.\n\nRegion - 3 unique values\nLisnon, Oporto or Other (Nominal)","metadata":{}},{"cell_type":"markdown","source":"## Summary of dataset","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.103397Z","iopub.execute_input":"2021-07-27T19:43:41.103915Z","iopub.status.idle":"2021-07-27T19:43:41.120787Z","shell.execute_reply.started":"2021-07-27T19:43:41.103866Z","shell.execute_reply":"2021-07-27T19:43:41.119686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that no null value and all the data inputs are having numeric datatype","metadata":{}},{"cell_type":"markdown","source":"## Summary statistics of dataset ","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.122548Z","iopub.execute_input":"2021-07-27T19:43:41.123149Z","iopub.status.idle":"2021-07-27T19:43:41.162376Z","shell.execute_reply.started":"2021-07-27T19:43:41.123104Z","shell.execute_reply":"2021-07-27T19:43:41.161448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check missing value","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.163674Z","iopub.execute_input":"2021-07-27T19:43:41.164106Z","iopub.status.idle":"2021-07-27T19:43:41.17402Z","shell.execute_reply.started":"2021-07-27T19:43:41.164075Z","shell.execute_reply":"2021-07-27T19:43:41.172785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are no missing values in the dataset. Thus, there is no need to use the dropna() function.","metadata":{}},{"cell_type":"markdown","source":" 6 continuous types of feature ('Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen')\n 2 categoricals features ('Channel', 'Region')","metadata":{}},{"cell_type":"code","source":"categorical_features = ['Channel', 'Region']\ncontinuous_features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.175635Z","iopub.execute_input":"2021-07-27T19:43:41.175986Z","iopub.status.idle":"2021-07-27T19:43:41.185115Z","shell.execute_reply.started":"2021-07-27T19:43:41.175946Z","shell.execute_reply":"2021-07-27T19:43:41.184187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Channel Count","metadata":{}},{"cell_type":"code","source":"df['Channel'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.186569Z","iopub.execute_input":"2021-07-27T19:43:41.187091Z","iopub.status.idle":"2021-07-27T19:43:41.207921Z","shell.execute_reply.started":"2021-07-27T19:43:41.187053Z","shell.execute_reply":"2021-07-27T19:43:41.206691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def categorical_multi(i,j):\n    pd.crosstab(df[i],df[j]).plot(kind='bar')\n    plt.show()\n    print(pd.crosstab(df[i],df[j]))\n\ncategorical_multi(i='Channel',j='Region')    ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.209401Z","iopub.execute_input":"2021-07-27T19:43:41.209697Z","iopub.status.idle":"2021-07-27T19:43:41.451914Z","shell.execute_reply.started":"2021-07-27T19:43:41.209668Z","shell.execute_reply":"2021-07-27T19:43:41.450702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.454331Z","iopub.execute_input":"2021-07-27T19:43:41.454779Z","iopub.status.idle":"2021-07-27T19:43:41.476877Z","shell.execute_reply.started":"2021-07-27T19:43:41.454733Z","shell.execute_reply":"2021-07-27T19:43:41.476101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation analysis\ncorrMatt = df.corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:41.478094Z","iopub.execute_input":"2021-07-27T19:43:41.478366Z","iopub.status.idle":"2021-07-27T19:43:42.103101Z","shell.execute_reply.started":"2021-07-27T19:43:41.478339Z","shell.execute_reply":"2021-07-27T19:43:42.102098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the plot above, there are a few pairs of features that exhibit some degree of correlation. They include:\n\n#### Grocery and Detergents_Paper are highly correlated  - 0.92\n#### Milk and Groceries - 0.73\n#### Milk and Detergents_Paper - 0.66","metadata":{}},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.MinMaxScaler()\nscaled = scaler.fit_transform(df)\nscaled_data = pd.DataFrame(scaled, columns = [name for name in list(df)])\nfig = plt.figure(figsize = (20,10))\nscaled_data.boxplot(column=[name for name in list(scaled_data)], grid=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:42.104363Z","iopub.execute_input":"2021-07-27T19:43:42.104697Z","iopub.status.idle":"2021-07-27T19:43:42.496543Z","shell.execute_reply.started":"2021-07-27T19:43:42.104665Z","shell.execute_reply":"2021-07-27T19:43:42.493051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows that no outliers present in the dataset","metadata":{}},{"cell_type":"code","source":"# No obvious trend in data set as sequence\n%matplotlib inline\ndf.plot(figsize=(12,6), style='.');","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:42.498314Z","iopub.execute_input":"2021-07-27T19:43:42.498747Z","iopub.status.idle":"2021-07-27T19:43:42.888937Z","shell.execute_reply.started":"2021-07-27T19:43:42.498703Z","shell.execute_reply":"2021-07-27T19:43:42.887792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q. Implement Feature Scaling to Normalize the data(compare the histogram/KDE for MinMaxScaler \nand StandardScaler). Choose one of the Scaler to proceed ahead and provide reasoning as to \nwhy it was selected?\n\nSome features in our data might have high-magnitude values (for example, annual salary), while others might have relatively low values (for example, the number of years worked at a company). Just because some data has smaller values does not mean it is less significant. So, to make sure our prediction does not vary because of different magnitudes of features in our data, we can perform feature scaling, standardization, or normalization\n\n","metadata":{}},{"cell_type":"markdown","source":"Implementing Scaling Using the\n## Standard Scaler Method","metadata":{}},{"cell_type":"code","source":"df1 = df.copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:42.891392Z","iopub.execute_input":"2021-07-27T19:43:42.891748Z","iopub.status.idle":"2021-07-27T19:43:42.896376Z","shell.execute_reply.started":"2021-07-27T19:43:42.891713Z","shell.execute_reply":"2021-07-27T19:43:42.895307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstd_scale = StandardScaler().fit_transform(df1)\n\nscaled_frame = pd.DataFrame(std_scale, columns=df1.columns)\n\nscaled_frame.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:42.898461Z","iopub.execute_input":"2021-07-27T19:43:42.899053Z","iopub.status.idle":"2021-07-27T19:43:42.930883Z","shell.execute_reply.started":"2021-07-27T19:43:42.89901Z","shell.execute_reply":"2021-07-27T19:43:42.929621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_frame.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:42.932267Z","iopub.execute_input":"2021-07-27T19:43:42.932624Z","iopub.status.idle":"2021-07-27T19:43:42.979215Z","shell.execute_reply.started":"2021-07-27T19:43:42.932583Z","shell.execute_reply":"2021-07-27T19:43:42.977965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(scaled_frame).plot(kind='kde')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:42.981024Z","iopub.execute_input":"2021-07-27T19:43:42.981559Z","iopub.status.idle":"2021-07-27T19:43:43.461012Z","shell.execute_reply.started":"2021-07-27T19:43:42.981519Z","shell.execute_reply":"2021-07-27T19:43:43.46011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(scaled_frame).plot(kind='hist', bins=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:43.462115Z","iopub.execute_input":"2021-07-27T19:43:43.462526Z","iopub.status.idle":"2021-07-27T19:43:44.036904Z","shell.execute_reply.started":"2021-07-27T19:43:43.462497Z","shell.execute_reply":"2021-07-27T19:43:44.035785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the StandardScaler method, we have scaled the data into a uniform unit over all the columns. As you can see in the preceding table, the values of all the features have been converted into a uniform range of the same scale. Because of this, it becomes easier for the model to make predictions.\nKind kde is seems to be better then hist because in hist, data seems to be overlapped and unable to see the value for each field, whereas for kde, data visualization is finer.","metadata":{}},{"cell_type":"markdown","source":" Implementing Scaling Using the \n ## MinMax Scaler Method","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nminmax_scale = MinMaxScaler().fit_transform(df)\n\nscaled_frame2 = pd.DataFrame(minmax_scale,columns=df.columns)\n\nscaled_frame2.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:44.038239Z","iopub.execute_input":"2021-07-27T19:43:44.038627Z","iopub.status.idle":"2021-07-27T19:43:44.06257Z","shell.execute_reply.started":"2021-07-27T19:43:44.038589Z","shell.execute_reply":"2021-07-27T19:43:44.061329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_frame2.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:44.064153Z","iopub.execute_input":"2021-07-27T19:43:44.064574Z","iopub.status.idle":"2021-07-27T19:43:44.108016Z","shell.execute_reply.started":"2021-07-27T19:43:44.064529Z","shell.execute_reply":"2021-07-27T19:43:44.106916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(scaled_frame2).plot(kind='kde')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:44.109679Z","iopub.execute_input":"2021-07-27T19:43:44.110139Z","iopub.status.idle":"2021-07-27T19:43:44.581641Z","shell.execute_reply.started":"2021-07-27T19:43:44.110086Z","shell.execute_reply":"2021-07-27T19:43:44.580628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(scaled_frame2).plot(kind='hist', bins=30)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:44.583005Z","iopub.execute_input":"2021-07-27T19:43:44.583292Z","iopub.status.idle":"2021-07-27T19:43:45.869239Z","shell.execute_reply.started":"2021-07-27T19:43:44.583265Z","shell.execute_reply":"2021-07-27T19:43:45.868182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the MinMaxScaler method, we have again scaled the data into a uniform unit over all the columns. As you can see in the preceding table, the values of all the features have been converted into a uniform range of the same scale[0,1].","metadata":{}},{"cell_type":"markdown","source":"Ans: In Standardscaler, it assumes that data has normally distributed features and it has scaled them to zero mean and 1 standard deviation.\n\nAll the features are of the same scale after applying the scaler.\n\nWhile in Minmaxscaler, it shrinks the data within the range of -1 to 1(if there are negative values) and responds well if standard deviation is small and is used when distribution is not Gaussian.This scaler is sensitive to outliers.\n\n\nIn Standard scaler, centered curves are there with no outliers and in minmax, outliers are there.\n\nSo here, we will continue with standard scaler.","metadata":{}},{"cell_type":"code","source":"df_Con=df.drop(['Channel','Region'], axis=1)  # drop Categorical features","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:45.871934Z","iopub.execute_input":"2021-07-27T19:43:45.87246Z","iopub.status.idle":"2021-07-27T19:43:45.87931Z","shell.execute_reply.started":"2021-07-27T19:43:45.872413Z","shell.execute_reply":"2021-07-27T19:43:45.878098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_Con.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:45.88094Z","iopub.execute_input":"2021-07-27T19:43:45.881401Z","iopub.status.idle":"2021-07-27T19:43:45.901172Z","shell.execute_reply.started":"2021-07-27T19:43:45.881358Z","shell.execute_reply":"2021-07-27T19:43:45.899938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram\nplt.figure(figsize=(10,8))\ndf_Con.plot(kind='hist', alpha=0.8,bins=60, subplots=True,layout=(3,2),legend=True,figsize=(12,10))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:45.90312Z","iopub.execute_input":"2021-07-27T19:43:45.903582Z","iopub.status.idle":"2021-07-27T19:43:48.102829Z","shell.execute_reply.started":"2021-07-27T19:43:45.903529Z","shell.execute_reply":"2021-07-27T19:43:48.101721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histograms show an exponential decline in the number of orders for the respected products. hence this could be a cluster consisting of larger size companies with higher purchase quanitites for these particular items.","metadata":{}},{"cell_type":"code","source":"plot1=sns.pairplot(df_Con, diag_kind='kde')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:48.107187Z","iopub.execute_input":"2021-07-27T19:43:48.107732Z","iopub.status.idle":"2021-07-27T19:43:55.643724Z","shell.execute_reply.started":"2021-07-27T19:43:48.107687Z","shell.execute_reply":"2021-07-27T19:43:55.642935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_df=df.describe().loc['mean',:]\nmean_df","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:55.645786Z","iopub.execute_input":"2021-07-27T19:43:55.64622Z","iopub.status.idle":"2021-07-27T19:43:55.671455Z","shell.execute_reply.started":"2021-07-27T19:43:55.64619Z","shell.execute_reply":"2021-07-27T19:43:55.670759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q. Find optimal number of features using RFECV and show the plot between Number of features \nselected vs Cross validation score (use channel as target variable)\n","metadata":{}},{"cell_type":"code","source":"X = scaled_frame.drop(['Channel'], axis=1)\ny = df['Channel'] # Channel has 2 values so we will use channel here from main table\n\n# convert channel into binary values\ny[y == 2] = 0\ny[y == 1] = 1\n\ny.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:55.672493Z","iopub.execute_input":"2021-07-27T19:43:55.672913Z","iopub.status.idle":"2021-07-27T19:43:55.686868Z","shell.execute_reply.started":"2021-07-27T19:43:55.672881Z","shell.execute_reply":"2021-07-27T19:43:55.685895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split\n\nlr, knn = LinearRegression(), KNeighborsClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:55.688318Z","iopub.execute_input":"2021-07-27T19:43:55.688645Z","iopub.status.idle":"2021-07-27T19:43:55.703119Z","shell.execute_reply.started":"2021-07-27T19:43:55.688604Z","shell.execute_reply":"2021-07-27T19:43:55.702163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:55.704472Z","iopub.execute_input":"2021-07-27T19:43:55.705038Z","iopub.status.idle":"2021-07-27T19:43:55.720364Z","shell.execute_reply.started":"2021-07-27T19:43:55.704984Z","shell.execute_reply":"2021-07-27T19:43:55.719109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Fit raw features:\")\nprint(\" LR:\", lr.fit(X_train, y_train).score(X_test, y_test))\nprint(\"KNN:\", knn.fit(X_train, y_train).score(X_test, y_test))\nprint(\"GBC: \", GradientBoostingClassifier().fit(X_train, y_train).score(X_test, y_test))\nprint(\"RFC: \", RandomForestClassifier().fit(X_train, y_train).score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:55.722191Z","iopub.execute_input":"2021-07-27T19:43:55.722581Z","iopub.status.idle":"2021-07-27T19:43:56.101084Z","shell.execute_reply.started":"2021-07-27T19:43:55.722542Z","shell.execute_reply":"2021-07-27T19:43:56.099999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier()\nrfecv = RFECV(estimator=rf)\nrfecv.fit(X, y)\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,5))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.title(\"Optimal number of features : %d\" % rfecv.n_features_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:43:56.102359Z","iopub.execute_input":"2021-07-27T19:43:56.102667Z","iopub.status.idle":"2021-07-27T19:44:04.713477Z","shell.execute_reply.started":"2021-07-27T19:43:56.102635Z","shell.execute_reply":"2021-07-27T19:44:04.712283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we will use RFECV using GradientBoostingClassifier with pipleline","metadata":{}},{"cell_type":"code","source":"model = GradientBoostingClassifier()\nrfecv = RFECV(estimator=GradientBoostingClassifier())","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:44:04.715116Z","iopub.execute_input":"2021-07-27T19:44:04.715665Z","iopub.status.idle":"2021-07-27T19:44:04.721757Z","shell.execute_reply.started":"2021-07-27T19:44:04.715553Z","shell.execute_reply":"2021-07-27T19:44:04.720686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature selection is an important task and it is crucial when the data in question has many features. The optimal number of features also leads to improved model accuracy. Obtaining the most important features and the number of optimal features can be obtained via feature importance or feature ranking.","metadata":{}},{"cell_type":"markdown","source":"## Recursive Feature Elimination","metadata":{}},{"cell_type":"markdown","source":"Instead of manually configuring the number of features(RFE), it would be very nice if we could automatically select them. This can be achieved via recursive feature elimination and cross-validation. This is done via the RFECV.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import RFE\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:44:04.723682Z","iopub.execute_input":"2021-07-27T19:44:04.724129Z","iopub.status.idle":"2021-07-27T19:44:04.735606Z","shell.execute_reply.started":"2021-07-27T19:44:04.724084Z","shell.execute_reply":"2021-07-27T19:44:04.734443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Pipeline — since we’ll perform some cross-validation. It’s best practice in order to avoid data leakage.\n    RepeatedStratifiedKFold — for repeated stratified cross-validation.\n    cross_val_score — for evaluating the score on cross-validation.\n    GradientBoostingClassifier — the estimator we’ll use.\n    numpy — so that we can compute the mean of the scores.","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline([('Feature Selection', rfecv), ('Model', model)])\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=42)\nn_scores = cross_val_score(pipeline, X,y, scoring='accuracy', cv=cv, n_jobs = -1)\nnp.mean(n_scores)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:44:04.737297Z","iopub.execute_input":"2021-07-27T19:44:04.737691Z","iopub.status.idle":"2021-07-27T19:45:27.430137Z","shell.execute_reply.started":"2021-07-27T19:44:04.737648Z","shell.execute_reply":"2021-07-27T19:45:27.428976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s fit the pipeline and then obtain the optimal number of features.","metadata":{}},{"cell_type":"code","source":"pipeline.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:27.43143Z","iopub.execute_input":"2021-07-27T19:45:27.431908Z","iopub.status.idle":"2021-07-27T19:45:31.698357Z","shell.execute_reply.started":"2021-07-27T19:45:27.431798Z","shell.execute_reply":"2021-07-27T19:45:31.697497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Optimal number of features : %d\" % rfecv.n_features_)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:31.699468Z","iopub.execute_input":"2021-07-27T19:45:31.699921Z","iopub.status.idle":"2021-07-27T19:45:31.705146Z","shell.execute_reply.started":"2021-07-27T19:45:31.699872Z","shell.execute_reply":"2021-07-27T19:45:31.70413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfecv.support_","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:31.706468Z","iopub.execute_input":"2021-07-27T19:45:31.70675Z","iopub.status.idle":"2021-07-27T19:45:31.725235Z","shell.execute_reply.started":"2021-07-27T19:45:31.706724Z","shell.execute_reply":"2021-07-27T19:45:31.724011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once fitted, the following attributes can be obtained:\n  \n    ranking_ — the ranking of the features.\n    n_features_ — the number of features that have been selected.\n    support_ — an array that indicates whether or not a feature was selected.\n    grid_scores_ — the scores obtained from cross-validation.","metadata":{}},{"cell_type":"markdown","source":"Lets put that into a dataframe and check the result","metadata":{}},{"cell_type":"code","source":"rfecv_df = pd.DataFrame(rfecv.ranking_, index=X.columns, columns = ['Rank']).sort_values(by='Rank', ascending=True)\nrfecv_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:31.726568Z","iopub.execute_input":"2021-07-27T19:45:31.727123Z","iopub.status.idle":"2021-07-27T19:45:31.743446Z","shell.execute_reply.started":"2021-07-27T19:45:31.727019Z","shell.execute_reply":"2021-07-27T19:45:31.74254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the grid_scores_ we can plot a graph showing the cross-validated scores","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score ( nb of correct classification)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:31.74463Z","iopub.execute_input":"2021-07-27T19:45:31.745113Z","iopub.status.idle":"2021-07-27T19:45:31.980549Z","shell.execute_reply.started":"2021-07-27T19:45:31.745066Z","shell.execute_reply":"2021-07-27T19:45:31.979717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have concluded that Milk, Grocery, Frozen, Detergents_Paper and Delicassen are optimal features","metadata":{}},{"cell_type":"markdown","source":"Q. Implement KMeans Clustering for K=2 to K=15 and based on elbow method identify what is the optimum number of clusters","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n# from sklearn.metrics import silhouette_score","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:31.981748Z","iopub.execute_input":"2021-07-27T19:45:31.982257Z","iopub.status.idle":"2021-07-27T19:45:32.066469Z","shell.execute_reply.started":"2021-07-27T19:45:31.982205Z","shell.execute_reply":"2021-07-27T19:45:32.065454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First we need to convert our categorical features (region and channel) to dummy variable:\n# df2 = pd.get_dummies(df)\n# X1 = df2.iloc[:,:].values","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:32.067698Z","iopub.execute_input":"2021-07-27T19:45:32.068021Z","iopub.status.idle":"2021-07-27T19:45:32.071912Z","shell.execute_reply.started":"2021-07-27T19:45:32.067991Z","shell.execute_reply":"2021-07-27T19:45:32.070905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wcss = []\nfor i in range(2, 16):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(scaled_frame) # Standard scaler \n    wcss.append(kmeans.inertia_)\n    cluster_labels = kmeans.labels_\n    # silhouette score\n#     silhouette_avg = silhouette_score(df, cluster_labels)\n#     print(\"For n_clusters={0}, the silhouette score is {1}\".format(i, silhouette_avg))\nplt.plot(range(2, 16), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('n: Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:32.07339Z","iopub.execute_input":"2021-07-27T19:45:32.073799Z","iopub.status.idle":"2021-07-27T19:45:33.66634Z","shell.execute_reply.started":"2021-07-27T19:45:32.073756Z","shell.execute_reply":"2021-07-27T19:45:33.665369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph looks like elbow and we have to(can) determine that elbow point.\nHere the elbow point comes at around 6 and this our optimal number of clusters for the above data which we should choose.\nIf we look at the figure carefully after 6 when we go on increasing the number of cluster WCSS reduces slightly.","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 6,random_state = 111)\ny_means = kmeans.fit(scaled_frame)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:33.667693Z","iopub.execute_input":"2021-07-27T19:45:33.668015Z","iopub.status.idle":"2021-07-27T19:45:33.733352Z","shell.execute_reply.started":"2021-07-27T19:45:33.667975Z","shell.execute_reply":"2021-07-27T19:45:33.732468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_means","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:33.734615Z","iopub.execute_input":"2021-07-27T19:45:33.735213Z","iopub.status.idle":"2021-07-27T19:45:33.742003Z","shell.execute_reply.started":"2021-07-27T19:45:33.735171Z","shell.execute_reply":"2021-07-27T19:45:33.740965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('SSE: ', kmeans.inertia_)\nprint('\\nCentroids: \\n', kmeans.cluster_centers_)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:33.743519Z","iopub.execute_input":"2021-07-27T19:45:33.743945Z","iopub.status.idle":"2021-07-27T19:45:33.756368Z","shell.execute_reply.started":"2021-07-27T19:45:33.743903Z","shell.execute_reply":"2021-07-27T19:45:33.75526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count number of records in every cluster\npd.Series(kmeans.labels_).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:33.7577Z","iopub.execute_input":"2021-07-27T19:45:33.758065Z","iopub.status.idle":"2021-07-27T19:45:33.771804Z","shell.execute_reply.started":"2021-07-27T19:45:33.758031Z","shell.execute_reply":"2021-07-27T19:45:33.77056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q. Implement PCA with number of original features to answer how much variance is explained by first 2 components and by first 4 components and visualize the clusters in the data","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=400, n_init=100, random_state=0)\ny_means = kmeans.fit(scaled_frame)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:33.773365Z","iopub.execute_input":"2021-07-27T19:45:33.773702Z","iopub.status.idle":"2021-07-27T19:45:34.299447Z","shell.execute_reply.started":"2021-07-27T19:45:33.773672Z","shell.execute_reply":"2021-07-27T19:45:34.298427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca2 = PCA(n_components=2).fit(scaled_frame)\npca2d = pca2.transform(scaled_frame)\n\nprint(\"Explained variance is:\",pca2.explained_variance_)\nprint(\"Explained variance ratio\",pca2.explained_variance_ratio_)\nprint(\"Variance for 1st component is 38.75% & 2nd component is 22.37%\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.300868Z","iopub.execute_input":"2021-07-27T19:45:34.301212Z","iopub.status.idle":"2021-07-27T19:45:34.319596Z","shell.execute_reply.started":"2021-07-27T19:45:34.301178Z","shell.execute_reply":"2021-07-27T19:45:34.318395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above output, we can observe that the principal component 1 holds 38.75% of the information while the principal component 2 holds only 22.37% of the information.","metadata":{}},{"cell_type":"code","source":"pca4 = PCA(n_components=4).fit(scaled_frame)\npca4d = pca4.transform(scaled_frame)\n\nprint(\"Explained variance is:\",pca4.explained_variance_)\nprint(\"Explained variance ratio is:\",pca4.explained_variance_ratio_)\n\nplt.figure(figsize = (8,8))\nsns.scatterplot(pca4d[:,0], pca4d[:,1], \n                hue=y_means.labels_, \n                palette='Set1',\n                s=100, alpha=0.2).set_title('KMeans Clusters (4) Derived from Elbow Method', fontsize=15)\nplt.legend()\nplt.ylabel('PC2')\nplt.xlabel('PC1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.32096Z","iopub.execute_input":"2021-07-27T19:45:34.321255Z","iopub.status.idle":"2021-07-27T19:45:34.768319Z","shell.execute_reply.started":"2021-07-27T19:45:34.321227Z","shell.execute_reply":"2021-07-27T19:45:34.767239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above output, we can observe that the principal component 1 holds 38.75% of the information while the principal component 2 holds only 22.37% , 3 holds 12% and 4 holds 9%of the information.","metadata":{}},{"cell_type":"markdown","source":"Q. Implement XGBoost Classifier with 5 Fold CV and report the performance metrics","metadata":{}},{"cell_type":"code","source":"X = df.drop('Channel', axis=1)\n\ny = df['Channel']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.769708Z","iopub.execute_input":"2021-07-27T19:45:34.770049Z","iopub.status.idle":"2021-07-27T19:45:34.776119Z","shell.execute_reply.started":"2021-07-27T19:45:34.770018Z","shell.execute_reply":"2021-07-27T19:45:34.774854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.777662Z","iopub.execute_input":"2021-07-27T19:45:34.778146Z","iopub.status.idle":"2021-07-27T19:45:34.79953Z","shell.execute_reply.started":"2021-07-27T19:45:34.778104Z","shell.execute_reply":"2021-07-27T19:45:34.798483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.800858Z","iopub.execute_input":"2021-07-27T19:45:34.801405Z","iopub.status.idle":"2021-07-27T19:45:34.808271Z","shell.execute_reply.started":"2021-07-27T19:45:34.801371Z","shell.execute_reply":"2021-07-27T19:45:34.807231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"y label contain values as 1 and 2\nWe will convert it into 0 and 1 for further analysis.","metadata":{}},{"cell_type":"code","source":"# convert labels into binary values\n\ny[y == 2] = 0\n\ny[y == 1] = 1","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.809485Z","iopub.execute_input":"2021-07-27T19:45:34.809808Z","iopub.status.idle":"2021-07-27T19:45:34.828065Z","shell.execute_reply.started":"2021-07-27T19:45:34.809777Z","shell.execute_reply":"2021-07-27T19:45:34.82703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.829202Z","iopub.execute_input":"2021-07-27T19:45:34.829488Z","iopub.status.idle":"2021-07-27T19:45:34.846349Z","shell.execute_reply.started":"2021-07-27T19:45:34.829461Z","shell.execute_reply":"2021-07-27T19:45:34.845262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains.","metadata":{}},{"cell_type":"code","source":"# import sys\n# !{sys.executable} -m pip install xgboost\n# to install xgboost","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.850114Z","iopub.execute_input":"2021-07-27T19:45:34.850597Z","iopub.status.idle":"2021-07-27T19:45:34.858282Z","shell.execute_reply.started":"2021-07-27T19:45:34.850539Z","shell.execute_reply":"2021-07-27T19:45:34.857203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the XGBoost Classifier ","metadata":{}},{"cell_type":"markdown","source":"### k-fold Cross Validation using XGBoost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.860069Z","iopub.execute_input":"2021-07-27T19:45:34.860729Z","iopub.status.idle":"2021-07-27T19:45:34.968866Z","shell.execute_reply.started":"2021-07-27T19:45:34.860677Z","shell.execute_reply":"2021-07-27T19:45:34.967271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBClassifier(eval_metric='mlogloss')\nkfold = KFold(n_splits=5)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.970135Z","iopub.execute_input":"2021-07-27T19:45:34.97044Z","iopub.status.idle":"2021-07-27T19:45:34.975011Z","shell.execute_reply.started":"2021-07-27T19:45:34.970411Z","shell.execute_reply":"2021-07-27T19:45:34.974024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = cross_val_score(model, X, y, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\nAccuracy = results.mean()*100","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:34.976668Z","iopub.execute_input":"2021-07-27T19:45:34.977353Z","iopub.status.idle":"2021-07-27T19:45:35.289962Z","shell.execute_reply.started":"2021-07-27T19:45:34.977308Z","shell.execute_reply":"2021-07-27T19:45:35.28906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this kernel, I have implemented XGBoostclassifier with Python and Scikit-Learn to classify the customers from two different channels as Horeca (Hotel/Retail/Café) customers or Retail channel (nominal) customers.\n\nThe y labels contain values as 1 and 2. We have converted them into 0 and 1 for further analysis.\n\nWe have performed k-fold cross-validation with XGBoost classifier.\nThis example summarizes the performance of the default model configuration on the dataset including both the mean and standard deviation classification accuracy.","metadata":{}},{"cell_type":"code","source":"# max_depth = 5, alpha = 10, n_estimators = 10)\n\nmodel = XGBClassifier(eval_metric='mlogloss')\nkfold = KFold(n_splits=5)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:46:17.130228Z","iopub.execute_input":"2021-07-27T19:46:17.130567Z","iopub.status.idle":"2021-07-27T19:46:17.134781Z","shell.execute_reply.started":"2021-07-27T19:46:17.130536Z","shell.execute_reply":"2021-07-27T19:46:17.133987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PW = cross_val_score(model, X, y, cv=kfold, scoring='precision_weighted')\nRscore = cross_val_score(model, X, y, cv=kfold, scoring='recall_weighted')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:35.305998Z","iopub.execute_input":"2021-07-27T19:45:35.306469Z","iopub.status.idle":"2021-07-27T19:45:35.885462Z","shell.execute_reply.started":"2021-07-27T19:45:35.306422Z","shell.execute_reply":"2021-07-27T19:45:35.884599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Recall = (round((Rscore.mean()*100),3))\nPrecision = (round((PW.mean()*100),3))\n\nf1score=round(2*((Recall*Precision)/(Recall+Precision)),3)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:35.894043Z","iopub.execute_input":"2021-07-27T19:45:35.897018Z","iopub.status.idle":"2021-07-27T19:45:35.904174Z","shell.execute_reply.started":"2021-07-27T19:45:35.894573Z","shell.execute_reply":"2021-07-27T19:45:35.903264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy= %f, Recall=%f, Precision=%f, f1score=%f\" % (Accuracy,Recall,Precision,f1score))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T19:45:35.905379Z","iopub.execute_input":"2021-07-27T19:45:35.905786Z","iopub.status.idle":"2021-07-27T19:45:35.919436Z","shell.execute_reply.started":"2021-07-27T19:45:35.905757Z","shell.execute_reply":"2021-07-27T19:45:35.918467Z"},"trusted":true},"execution_count":null,"outputs":[]}]}