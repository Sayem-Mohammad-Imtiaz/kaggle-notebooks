{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Health Insurance Lead Prediction - Kaggle Competition\n## Job-A-Thon - Analytics Vidhya, Health Insurance\nAuthor: Miguel Santana\n\n### Project Methodology\nFinMan Company is looking to leverage their client base by cross selling insurance products to existing customers. Insurance policies are offered to prospective and existing clients based on website landing and consumer election to fill out additional information forms. FinMan company would like to leverage their acquired information to classify positive leads for outreach programs using machine learning classifiers.\n\n### Data and Analytical Structure\nThe project dataset is provided by Analytics Vidhya via Kaggle. Data includes demographic features, policy features (for current customers) and example positive classifications for ML model validation and interpretation. The source can be found [here](https://www.kaggle.com/imsparsh/jobathon-analytics-vidhya?select=sample_submission.csv). The project analysis will follow the OSEMN framework: Obtain, Scrub, Explore, Model and Interpret.\n\n# Data & Packages | Obtain","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\npd.set_option('display.max_columns',50)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pycaret\nfrom pycaret.datasets import get_data\nfrom pycaret.classification import *\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_directory = '../input/jobathon-analytics-vidhya'\n\ntrain_path = os.path.join(data_directory, 'train.csv')\ntest_path = os.path.join(data_directory, 'test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv(train_path)\ntestdf = pd.read_csv(train_path)\n\nprint(traindf.shape)\ntraindf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning | Scrub\n### Null Values","metadata":{}},{"cell_type":"code","source":"traindf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nonclients = ['Holding_Policy_Duration','Holding_Policy_Type']\nfor col in nonclients:\n    traindf[col] = traindf[col].fillna(0)\n    testdf[col] = testdf[col].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf['Health Indicator'] = traindf['Health Indicator'].fillna(traindf['Health Indicator'].mode()[0])\ntestdf['Health Indicator'] = testdf['Health Indicator'].fillna(testdf['Health Indicator'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With many of these prospects not clearly identified as current clients, its safe to assume that null values in the 'Policy Duration' and 'Policy Type' columns are tied to non existing accounts and may be filled with zeros.\n\n## Feature Engineering\nConvert to numerical: Holding_Policy_Duration\n* Feature engineer long term customers\n* Convert '14+' to '15' / convert to numerical\n\nNote: (after EDA) Convert to binary | Accomodation_Type, Reco_Insurance_Type, Is_Spouse\n\n#### Categorical Features","metadata":{}},{"cell_type":"code","source":"traindf['Long_Term_Cust'] = traindf['Holding_Policy_Duration'].apply(lambda x: 'Yes' if x == '14+' else 'No')\ntestdf['Long_Term_Cust'] = testdf['Holding_Policy_Duration'].apply(lambda x: 'Yes' if x == '14+' else 'No')\n\ntraindf['Holding_Policy_Duration'] = traindf['Holding_Policy_Duration'].replace('14+',15).astype(float).astype(int)\ntestdf['Holding_Policy_Duration'] = testdf['Holding_Policy_Duration'].replace('14+',15).astype(float).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Renaming Features","metadata":{}},{"cell_type":"code","source":"traindf.rename(columns={'Is_Spouse':'Married','Health Indicator':'Health_Indicator'},inplace=True)\ntestdf.rename(columns={'Is_Spouse':'Married','Health Indicator':'Health_Indicator'},inplace=True)\n\ntraindf['Avg_Age'] = (traindf['Upper_Age'] + traindf['Lower_Age']) / 2\ntestdf['Avg_Age'] = (testdf['Upper_Age'] + testdf['Lower_Age']) / 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Typically, insurance products are priced and underwritten based on the age of the applicant or applicants. This is especially the case in most health insurance pricing. To reflect this and retain data, an average age feature will be created and the original two features will be dropped.","metadata":{}},{"cell_type":"code","source":"# feature engineering\ntraindf['Prim_Prem_Ratio'] = traindf['Reco_Policy_Premium'] / traindf['Upper_Age']\ntestdf['Prim_Prem_Ratio'] = testdf['Reco_Policy_Premium'] / testdf['Upper_Age']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"traindf.drop(['ID','Region_Code','Upper_Age','Lower_Age'],axis=1,inplace=True)\ntestdf2 = testdf.copy()\ntestdf.drop(['ID','Region_Code','Upper_Age','Lower_Age'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The unique 'ID' and 'Region Code' columns will be dropped in order to simplify the data. 'Region Code' consists of far too many categorical values which would need to be one hot encoded. The feature is dropped as the data still retains the 'City Code' feature to capture some level of geographical distinction. In addition, the upper and lower age features will be dropped being represented by average age.","metadata":{}},{"cell_type":"code","source":"numcols = testdf.select_dtypes('number').columns\nfor col in numcols:\n    traindf[col] = traindf[col].astype(int)\n    testdf[col] = testdf[col].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy for final analysis\ndf = traindf.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vals = {'Rented':1,'Owned':2,'Individual':1,'Joint':2,'No':0,'Yes':1}\ncols = ['Accomodation_Type','Reco_Insurance_Type','Married','Long_Term_Cust']\n\nfor col in cols:\n    traindf[col] = traindf[col].replace(vals)\n    testdf[col] = testdf[col].replace(vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features 'Accommodation Type', 'Reco Insurance Type', 'Is Spouse' will be converted to binary (0 and 1).","metadata":{}},{"cell_type":"code","source":"ordinal = ['Holding_Policy_Type','Reco_Policy_Cat']\nfor col in ordinal:\n    traindf[col] = traindf[col].astype('O')\n    testdf[col] = testdf[col].astype('O')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe two feature that stand out are 'Holding Policy Type' and 'Reco Policy Cat' which are listed under numerical but most likely correspond to type and category of policy in existing customers.\n\n# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"corr = traindf.corr() # analyzing correlation\nfig, ax = plt.subplots(figsize=(12,10))\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nsns.heatmap(corr, mask=mask, square=True, annot=True, cmap='YlGnBu')\nax.patch.set_edgecolor('black')  \nax.patch.set_linewidth('1')\nax.set_title('Correlation & Heat Map', fontsize=15, fontfamily='serif')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf.drop(['Married'],axis=1,inplace=True)\ntestdf.drop(['Married'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targetdf = df.groupby('Response').mean().head()\ntargetdf.style.background_gradient(cmap='Reds')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Customers who elect to receive additional information typically hold existing policies longer and are classified under a larger policy category with a slightly larger premium.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(data=df[df['Holding_Policy_Type']!=0],x='Holding_Policy_Type',hue='Response',palette='mako');\nfor p in ax.patches:\n        ax.annotate(p.get_height(),(p.get_x()+0.09, p.get_height()+75))\nfig.savefig('policytypecount.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Holding Policy Type three has the highest number of positive responses but all four of the categories have approximately 30% positive to negative client responses.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,6))\nsns.violinplot(data=df[df['Holding_Policy_Type']!=0],x='Holding_Policy_Type',y='Avg_Age',hue='Response',palette='mako');\nfig.savefig('policytypexage.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The violin plot gives an interesting take on Average Age versus Holding Policy Type. HPT 3 shows a pretty even distribution across age groups while HPT 1 is heavily made up of younger individuals.","metadata":{}},{"cell_type":"code","source":"traincat_vars = [var for var in traindf.columns if traindf[var].dtype == 'O']\ntestcat_vars = [var for var in testdf.columns if testdf[var].dtype == 'O']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Transformations","metadata":{}},{"cell_type":"code","source":"def replace_categories(df, var, target):\n    # Order variable categories | lowest to highest against target (price)\n    ordered_labels = df.groupby([var])[target].mean().sort_values().index\n    # Dictionary of ordered categories to integer values\n    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n    # Replace the categorical strings by integers using dictionary\n    df[var] = df[var].map(ordinal_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in traincat_vars:\n    replace_categories(traindf, var, 'Avg_Age')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in testcat_vars:\n    replace_categories(testdf, var, 'Avg_Age')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With each of the categorical values mapped to values with respect to average age, the resulting values will end up on a similar scale as the rest of the dataset. In order to minimize data manipulation for modeling, no label encoding or standard scaling will occur.","metadata":{}},{"cell_type":"code","source":"# labelencoder = preprocessing.LabelEncoder()\n# scaler = preprocessing.StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# traindf['City_Code'] = labelencoder.fit_transform(traindf['City_Code'])\n# testdf['City_Code'] = labelencoder.fit_transform(testdf['City_Code'])\n# traindfscaled = scaler.fit_transform(traindf)\n# testdfscaled = scaler.fit_transform(testdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n## Pycaret","metadata":{}},{"cell_type":"code","source":"dataset = traindf.copy()\ndata = dataset.sample(frac=0.80, random_state=786)\ndata_unseen = dataset.drop(data.index).reset_index(drop=True)\nprint('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = setup(data=data,target='Response',session_id=123,numeric_features=['Long_Term_Cust','Health_Indicator','Accomodation_Type','Reco_Insurance_Type','Holding_Policy_Duration','Holding_Policy_Type'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scikit-Learn","metadata":{}},{"cell_type":"code","source":"def model_visuals (model, X_test, y_test):\n    '''Plots the confusion matrix and ROC-AUC plot'''\n    fig, axes = plt.subplots(figsize = (12, 6), ncols = 2)  # confusion matrix\n    metrics.plot_confusion_matrix(model, X_test, y_test, normalize = 'true', \n                          cmap = 'Blues', ax = axes[0])\n    axes[0].set_title('Confusion Matrix');\n    # ROC-AUC Curve\n    roc_auc = metrics.plot_roc_curve(model, X_test, y_test,ax=axes[1])\n    axes[1].plot([0,1],[0,1],ls=':')\n    axes[1].set_title('ROC-AUC Plot')\n    axes[1].grid()\n    axes[1].legend()\n    fig.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(traindf.drop(columns=['Response'],axis=1),traindf['Response'],test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### GridSearchCV","metadata":{}},{"cell_type":"code","source":"gbclf = GradientBoostingClassifier(random_state=42)\ngbclf.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    'learning_rate': [0.1,0.2],\n    'max_depth': [6],\n    'subsample': [0.5,0.7,1],\n    'n_estimators': [100]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_clf = GridSearchCV(gbclf,param_grid,scoring='roc_auc',cv=None,n_jobs=1)\ngrid_clf.fit(X_train,y_train)\n\nbest_parameters = grid_clf.best_params_\n\nprint('Grid search found the following optimal parameters: ')\nfor param_name in sorted(best_parameters.keys()):\n    print('%s: %r' % (param_name,best_parameters[param_name]))\n    \ntraining_preds = grid_clf.predict(X_train)\ntest_preds = grid_clf.predict(X_test)\ntraining_accuracy = accuracy_score(y_train,training_preds)\ntest_accuracy = accuracy_score(y_test,test_preds)\n\nprint('')\nprint('Training Accuracy: {:.4}%'.format(training_accuracy*100))\nprint('Validation Accuracy: {:.4}%'.format(test_accuracy*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar accuracy in the training and test sets suggests minimal under/over fitting.\n\n### Final Model","metadata":{}},{"cell_type":"code","source":"gbclf = GradientBoostingClassifier(max_depth=6,learning_rate=0.1,n_estimators=100,subsample=1,random_state=42)\ngbclf.fit(X_train,y_train)\n# predict\ntraining_preds = gbclf.predict(X_train)\ntest_preds = gbclf.predict(X_test)\n# accuracy\ntraining_accuracy = accuracy_score(y_train,training_preds)\ntest_accuracy = accuracy_score(y_test,test_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, test_preds), '\\n\\n')\nmodel_visuals (gbclf, X_test, y_test) # class report / plots","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpret Results\n### Feature Importance","metadata":{}},{"cell_type":"code","source":"# Feature Importance\nX = traindf.drop(columns=['Response'],axis=1)\nclf_feature = pd.DataFrame({'Importance':gbclf.feature_importances_,'Column':X.columns})\nclf_feature = clf_feature.sort_values(by='Importance',ascending=False) \nclf_feature = clf_feature[:8] # top 8 features\nclf_feature.plot(kind='barh',x='Column',y='Importance',figsize=(20, 10))\nplt.title('Gradient Boosting Feature Importance \\n',fontsize=16)\nplt.savefig('featureimportance.jpg',dpi=200,bbox_inches='tight')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reco Policy Category","metadata":{}},{"cell_type":"code","source":"df['Reco_Policy_Cat'] = df['Reco_Policy_Cat'].astype('O')\n\nfig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(data=df,x='Reco_Policy_Cat',hue='Response',palette='mako');\nfig.savefig('policycategoryxresponse.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph may be misleading as each policy category caries a significantly different client count. Lets break down the top five categories based on positive over total responses.","metadata":{}},{"cell_type":"code","source":"RPC = df.groupby(['Reco_Policy_Cat','Response'])['Response'].count().unstack()\nRPC['PositiveRatio'] = RPC[1] / (RPC[1] + RPC[0])\nRPC = RPC.sort_values(by='PositiveRatio', ascending=False)[:5].reset_index()\n# RPC\nfig, ax = plt.subplots(figsize=(10,6))\nplt.title('Top 5 Reco Policy Categories', fontdict={'fontsize': 14})\nsns.barplot(data=RPC,x='Reco_Policy_Cat',y='PositiveRatio',palette='mako');\nfig.savefig('top5categoryxresponse.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reco Policy Category 15 is the clear front runner\n\n### Reco Policy Premium","metadata":{}},{"cell_type":"code","source":"# Binning Ages for Visualizations\ndf['Premium(bin)'] = df['Reco_Policy_Premium'].apply(lambda x: '0-9999' if x < 10000\n                                                     else '10000-14999' if x < 15000 \n                                                     else '15000-19999' if x < 20000 \n                                                     else '20000-24999' if x < 25000 \n                                                     else '25000-29999' if x < 30000 \n                                                     else '30000+')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.sort_values(['Premium(bin)'], ascending=True)\nfig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(data=df,x='Premium(bin)',hue='Response',palette='mako');\nfig.savefig('premiumbin.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets confirm the positive to total response ratios before we make a recommendation.","metadata":{}},{"cell_type":"code","source":"PRE = df.groupby(['Premium(bin)','Response'])['Response'].count().unstack()\nPRE['PositiveRatio'] = PRE[1] / (PRE[1] + PRE[0])\nPRE = PRE.sort_values(by='PositiveRatio', ascending=False)\nPRE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRE = PRE.reset_index()\nfig, ax = plt.subplots(figsize=(10,6))\nplt.title('Top 5 Premium Bins', fontdict={'fontsize': 16})\nsns.barplot(data=PRE,x='Premium(bin)',y='PositiveRatio',palette='mako');\nfig.savefig('top5premiumbin.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, most of the ratios are extremely close so the recommendation would be to focus on individuals who pay an annual premium between 15,000 and 19,999 as they convert at approximately the same rate as the front runner but represent a much larger group of clients. The high conversion along with the larger client volume will lead to higher profit.\n\n### City Code","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,6))\nsns.countplot(data=df,x='City_Code',hue='Response',palette='mako');\nfig.savefig('citycode.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CITY = df.groupby(['City_Code','Response'])['Response'].count().unstack()\nCITY['PositiveRatio'] = CITY[1] / (CITY[1] + CITY[0])\nCITY = CITY.sort_values(by='PositiveRatio', ascending=False)[:11]\nCITY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CITY = CITY.reset_index()\nfig, ax = plt.subplots(figsize=(10,6))\nplt.title('Top 11 City Categories', fontdict={'fontsize': 16})\nsns.barplot(data=CITY,x='City_Code',y='PositiveRatio',palette='mako');\nfig.savefig('top11citycode.jpg',dpi=200,bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this particular case I would recommend focusing on the top 11 scoring positive ratios. Ranks 10 and 11 are exponentially larger in volume than the first 9 and have the potential to yield high ROI with positive to total ratios close to 25%.","metadata":{}},{"cell_type":"markdown","source":"#### Submission","metadata":{}},{"cell_type":"code","source":"features = testdf.columns\ntarget = ['Response']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing submission\ngbclf.fit(traindf[features], traindf[target].values.ravel())\npredictions = gbclf.predict_proba(testdf[features])[:,1]\nsubmission = pd.DataFrame({'ID': testdf2['ID'],'Response': predictions})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Response'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Response'] = submission['Response'].apply(lambda x: 0 if x < 0.3 else 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In following the theme of the test and train datasets as well as presenting a client list of a respectable size, the cut off for positive response predictions will be 30%.","metadata":{}},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recommendations\nThe model's top 3 features were Reco Policy Category, Reco Policy Premium and City Code. Within those three categories, subcategories yielded the highest positive to total response ratios. It is recommended to focus on clients in/with:\n\n* City Codes: C1, C2, C13, C23\n* Reco Policy Categories: 15, 22\n* Reco Policy Premiums between: 15,000 & 19,999.\n\n### Limitations\nThe project was limited by the anonymity of the data. Specifically the geographic data that could have been used for additional feature engineering leading to higher scores.\n\n### Future Work\nFuture models can be created using more complicated feature engineering and analysis such as clustering of the geographic features. For the purposes of this project, doing so would have complicated the output and made it difficult to implement within a real workplace.\nFor any additional questions, please reach out via email at santana2.miguel@gmail.com, on [LinkedIn](https://www.linkedin.com/in/miguel-angel-santana-ii-mba-51467276/) or on [Twitter](https://twitter.com/msantana_ds).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}