{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Semantic Segmentation of High-Resolution Aerial Images","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hello everyone!\n\n\n\n![](https://uavid.nl/UAVid_files/imgs/UAVid_example.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we will work with [UAVid](https://uavid.nl/#home) dataset, which focusing on urban scenes. Our goal is to predict per-pixel semantic labeling.\n\nAnd the evaluation metric is IoU. \n![](https://www.oreilly.com/library/view/deep-learning-for/9781788295628/assets/63fb2c41-8e83-49c5-ad3a-fee59e8a178b.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook we’ll use the following notable libraries: \n\n* [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) has a lot of encoders for each model architecture.\n* [albumentations](https://github.com/albumentations-team/albumentations) has spatial-level transforms that change both an input image and mask simultaneously.\n* [catalyst](https://github.com/catalyst-team/catalyst) PyTorch framework that helps with reproducibility, fast experimentation and has a lot of useful utils.\n* [wandb](https://www.wandb.com/) Logger to track metrics, save hyper-parameters, gradients and model checkpoints.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data overview","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, we’ll install `segmentation-models-pytorch` and `torch nightly` for native amp support. And import libraries.","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install segmentation-models-pytorch\n!pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"WANDB_SILENT\"] = 'True'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom catalyst import utils\nimport cv2\nimport glob\nfrom PIL import Image\nimport os.path as osp\nfrom tqdm import tqdm\nfrom typing import Callable, List, Tuple\nimport torch\nimport catalyst\nimport wandb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def example(image_path = '../input/uavid-semantic-segmentation-dataset/train/train/seq33/Images/000200.png', \n             mask_path ='../input/uavid-semantic-segmentation-dataset/train/train/seq33/Labels/000200.png'):\n    image = Image.open(image_path)\n    mask = Image.open(mask_path)    \n    plt.figure(figsize=(18, 24))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(mask)\nexample()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset comprises 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes.\nThe image resolution is mostly 3840×2160 or 4096×2160.\n\n![](https://i.ibb.co/r73CD9M/colors.png)\n\n1. building: living houses, garages, skyscrapers, security booths, and buildings under construction.\n2. road: road or bridge surface that cars can run on legally. Parking lots are not included.\n3. tree: tall trees that have canopies and main trunks.\n4. low vegetation: grass, bushes and shrubs.\n5. static car: cars that are not moving, including static buses, trucks, automobiles, and tractors. Bicycles and motorcycles are not included.\n6. moving car: cars that are moving, including moving buses, trucks, automobiles, and tractors. Bicycles and motorcycles are not included.\n7. human: pedestrians, bikers, and all other humans occupied by different activities.\n8. clutter: all objects not belonging to any of the classes above.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\ncls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation', 'Static Car', 'Moving Car', 'Human']\npixels = [30.436, 25.977, 17.120, 14.322, 9.464, 1.405, 1.115, 0.162]\npix = pd.DataFrame({'Classes': cls, 'Pixel Number': pixels})\n\ndef plot_pixel():\n    plt.figure(figsize=(14,12))\n    sns.set_palette(['#800000','#008000','#000000', '#804080', '#808000', '#C000C0', '#400080', '#404000'])\n    sns.barplot(x='Classes', y='Pixel Number', data=pix)\n    plt.title('Pixel Number Histogram')\n    plt.ylabel('%')\n    plt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_pixel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the pixels are from classes like building, tree, clutter, road, and low vegetation. Fewer pixels are from moving\ncar and static car classes, which are both fewer than 1.5% of\nthe total pixels. For human class, it is almost zero, fewer\nthan 0.2% of the total pixels.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We’ll need helper functions for label image conversion from 3 channel RGB color image to 1 channel label index image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class UAVidColorTransformer:\n    def __init__(self):\n    # color table.\n        self.clr_tab = self.createColorTable()\n    # id table.\n        id_tab = {}\n        for k, v in self.clr_tab.items():\n            id_tab[k] = self.clr2id(v)\n        self.id_tab = id_tab\n\n    def createColorTable(self):\n        clr_tab = {}\n        clr_tab['Clutter'] = [0, 0, 0]\n        clr_tab['Building'] = [128, 0, 0]\n        clr_tab['Road'] = [128, 64, 128]\n        clr_tab['Static_Car'] = [192, 0, 192]\n        clr_tab['Tree'] = [0, 128, 0]\n        clr_tab['Vegetation'] = [128, 128, 0]\n        clr_tab['Human'] = [64, 64, 0]\n        clr_tab['Moving_Car'] = [64, 0, 128]\n        return clr_tab\n\n    def colorTable(self):\n        return self.clr_tab\n   \n    def clr2id(self, clr):\n        return clr[0]+clr[1]*255+clr[2]*255*255\n\n  #transform to uint8 integer label\n    def transform(self,label, dtype=np.int32):\n        height,width = label.shape[:2]\n    # default value is index of clutter.\n        newLabel = np.zeros((height, width), dtype=dtype)\n        id_label = label.astype(np.int64)\n        id_label = id_label[:,:,0]+id_label[:,:,1]*255+id_label[:,:,2]*255*255\n        for tid,val in enumerate(self.id_tab.values()):\n            mask = (id_label == val)\n            newLabel[mask] = tid\n        return newLabel\n\n  #transform back to 3 channels uint8 label\n    def inverse_transform(self, label):\n        label_img = np.zeros(shape=(label.shape[0], label.shape[1],3),dtype=np.uint8)\n        values = list(self.clr_tab.values())\n        for tid,val in enumerate(values):\n            mask = (label==tid)\n            label_img[mask] = val\n        return label_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clrEnc = UAVidColorTransformer()\ndef prepareTrainIDForDir(gtDirPath, saveDirPath):\n    gt_paths = [p for p in os.listdir(gtDirPath) if p.startswith('seq')]\n    for pd in tqdm(gt_paths):\n        lbl_dir = osp.join(gtDirPath, pd, 'Labels')\n        lbl_paths = os.listdir(lbl_dir)\n        if not osp.isdir(osp.join(saveDirPath, pd, 'TrainId')):\n            os.makedirs(osp.join(saveDirPath, pd, 'TrainId'))\n            assert osp.isdir(osp.join(saveDirPath, pd, 'TrainId')), 'Fail to create directory:%s'%(osp.join(saveDirPath, pd, 'TrainId'))\n        for lbl_p in lbl_paths:\n            lbl_path = osp.abspath(osp.join(lbl_dir, lbl_p))\n            trainId_path = osp.join(saveDirPath, pd, 'TrainId', lbl_p)\n            gt = np.array(Image.open(lbl_path))\n            trainId = clrEnc.transform(gt, dtype=np.uint8)\n            Image.fromarray(trainId).save(trainId_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we use joblib.Parallel, we can speedup ~3x","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prepareTrainIDForDir('../input/uavid-semantic-segmentation-dataset/train/train', './trainlabels/')\nprepareTrainIDForDir('../input/uavid-semantic-segmentation-dataset/valid/valid', './validlabels/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After creating label images, we’ll define lists of images and labels for our Dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_list = sorted(glob.glob(pathname='../input/uavid-semantic-segmentation-dataset/train/train/*/Images/*.png', recursive=True))\ntrain_mask_list =  sorted(glob.glob(pathname='./trainlabels/*/TrainId/*.png', recursive=True))\nvalid_image_list = sorted(glob.glob(pathname='../input/uavid-semantic-segmentation-dataset/valid/valid/*/Images/*.png', recursive=True))\nvalid_mask_list =  sorted(glob.glob(pathname='./validlabels/*/TrainId/*.png', recursive=True))\nprint(train_image_list[42])\nprint(train_mask_list[42])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Set seed and mix precision training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nutils.set_global_seed(SEED)\nutils.prepare_cudnn(deterministic=True)\nis_fp16_used = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our Dataset, we can read images, extract values of classes from segmentation mask, apply augmentation and pre-processing transformations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\n\nclass Dataset(BaseDataset):\n\n    CLASSES = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\n    \n    def __init__(\n            self, \n            images_list, \n            masks_list, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.images_list = images_list\n        self.masks_list = masks_list\n        self.classes = classes\n        \n        # convert str names to class values on masks\n        if self.classes is not None:\n            self.class_values = np.array([self.CLASSES.index(cls.lower()) for cls in classes]) / 255\n\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_list[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.masks_list[i], 0)\n        mask = mask.astype('float') / 255\n        \n        # extract certain classes from mask (e.g. cars)\n        if self.classes is not None:\n            masks = [(mask == v) for v in self.class_values]\n            mask = np.stack(masks, axis=-1).astype('float')\n        else:\n            mask = np.expand_dims(mask, 2)\n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n\n    def __len__(self):\n        return len(self.images_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper functions for visualization.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def visualize(image, mask, label=None, truth=None,  augment=False):\n    if truth is None:\n        plt.figure(figsize=(14, 20))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        if augment == False:\n            plt.title(f\"{'Original Image'}\")\n        else:\n            plt.title(f\"{'Augmented Image'}\")\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(mask)\n        if label is not None:\n            plt.title(f\"{label.capitalize()}\")\n        \n    else:\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 3, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")\n        \n        plt.subplot(1, 3, 3)\n        plt.imshow(truth)\n        plt.title(f\"{'Ground Truth'}\")\n        \ndef visualize_overlay(image, mask, truth_path=None):\n    if truth_path is None:\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")\n        \n    else:\n        truth = Image.open(truth_path)\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 3, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")\n        \n        plt.subplot(1, 3, 3)\n        plt.imshow(truth)\n        plt.title(f\"{'Ground Truth'}\")\n        \ndef visualize_prediction(image, mask):\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing segmentation masks for all classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\nfor label in labels:\n    dataset = Dataset(train_image_list, train_mask_list, classes=[label])\n\n    image, mask = dataset[4]\n    visualize(\n        image=image, mask=mask.squeeze(),\n        label = label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll resize images to `576*1024` to keep 9:16 ratio. \n\nAugmentation list:\n* *HorizontalFlip*\n* *OneOf(RandomBrightnessContrast, CLAHE, HueSaturationValue)*\n* *IAAAdditiveGaussianNoise* with 0.2 probability\n\n*Note:* For better result we could crop each image into 16 evenly distributed smaller(1280*720) overlapped images that cover the whole image for training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import albumentations as albu\n\ndef get_training_augmentation():\n    train_transform = [\n\n        albu.Resize(576, 1024, p=1),\n        albu.HorizontalFlip(p=0.5),\n\n        albu.OneOf([\n            albu.RandomBrightnessContrast(\n                  brightness_limit=0.4, contrast_limit=0.4, p=1),\n            albu.CLAHE(p=1),\n            albu.HueSaturationValue(p=1)\n            ],\n            p=0.9,\n        ),\n\n        albu.IAAAdditiveGaussianNoise(p=0.2),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    test_transform = [albu.Resize(576, 1024, p=1),\n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing augmented images and masks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\nfor label in labels:\n    augmented_dataset = Dataset(\n        train_image_list, \n        train_mask_list, \n        augmentation=get_training_augmentation(), \n        classes=[label],\n    )\n\n# same image with different random transforms\n    image, mask = augmented_dataset[8]\n    visualize(\n        image=image, mask=mask.squeeze(),\n        label = label, augment=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Single Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In our experiments, we'll use FPN model with EfficientnetB3 encoder. The motivation was to select best model in Memory Consumption, Accuracy trade-off.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\nENCODER = 'efficientnet-b3'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'sigmoid'\nCLASSES = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\n\n# create segmentation model with pretrained encoder\nmodel = smp.FPN(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’ll define Dataloaders and set batch size to 6, because of memory limitation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 6\n\ntrain_dataset = Dataset(\n    train_image_list, \n    train_mask_list, \n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\nvalid_dataset = Dataset(\n    valid_image_list, \n    valid_mask_list, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, drop_last=False)\n\nloaders = {\n    \"train\": train_loader,\n    \"valid\": valid_loader\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting optimization level: **01** - Mixed Precision (recommended for typical use)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if is_fp16_used:\n    fp16_params = dict(opt_level=\"O1\")\nelse:\n    fp16_params = None\n\nprint(f\"FP16 params: {fp16_params}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Experiment settings:\n* Loss: BCEDiceLoss with 0.5 contibution of BCE and Dice\n* Optimizer: Lookahead(improves the learning stability and lowers the variance of its inner optimizer)\n* Scheduler: OneCycleLRWithWarmup with 2 warmup steps\n* Initial learning rate is set to 1e-3, and 1e-4 on encoder. Number of epochs to 30.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.contrib.nn import BCEDiceLoss, RAdam, Lookahead, OneCycleLRWithWarmup\nfrom catalyst.dl import SupervisedRunner\n\nlogdir = \"./logs\"\nnum_epochs = 30\nlearning_rate = 1e-3\nbase_optimizer = RAdam([\n    {'params': model.decoder.parameters(), 'lr': learning_rate}, \n    {'params': model.encoder.parameters(), 'lr': 1e-4},\n    {'params': model.segmentation_head.parameters(), 'lr': learning_rate},\n])\noptimizer = Lookahead(base_optimizer)\ncriterion = BCEDiceLoss(activation=None)\nrunner = SupervisedRunner()\nscheduler = OneCycleLRWithWarmup(\n    optimizer, \n    num_steps=num_epochs, \n    lr_range=(0.0016, 0.0000001),\n    init_lr = learning_rate,\n    warmup_steps=2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing callbacks for metrics and logging","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.dl.callbacks import IouCallback, WandbLogger, EarlyStoppingCallback, ClasswiseIouCallback\n\ncallbacks = [\n    IouCallback(activation = 'none'),\n    ClasswiseIouCallback(classes=CLASSES, activation = 'none'),\n    EarlyStoppingCallback(patience=7, metric='iou', minimize=False),\n    WandbLogger(project='Project_Name', name='Run_Name'),\n    \n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model training, set main_metric to **'iou'**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"runner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=callbacks,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    # save our best checkpoint by IoU metric\n    main_metric=\"iou\",\n    # IoU needs to be maximized.\n    minimize_metric=False,\n    # for FP16. It uses the variable from the very first cell\n    fp16=fp16_params,\n    # prints train logs\n    verbose=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/qjGFnbw/single.png)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation', 'Static Car', 'Moving Car', 'Human']\ns_iou = [85.64, 72.85, 55.86, 69.78, 54.93, 0, 0, 0]\nsing_iou = pd.DataFrame({'Classes': cls, 'IoU': s_iou})\nsingle_iou = sing_iou.pivot_table(values='IoU', columns='Classes')\nsingle_iou","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = next(iter(loaders['valid']))\ndataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation())\nimage, _ = dataset[0]\ntruth_path='../input/uavid-semantic-segmentation-dataset/valid/valid/seq16/Labels/000000.png'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from catalyst.utils import mask_to_overlay_image\n\npred = mask_to_overlay_image(image=image, masks=single[0], threshold=0.4)\nvisualize_overlay(image, pred, truth_path=truth_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we’ve shown strong results in most pixels classes. But we’re not able to identify cars and people. \n\nTo address this problem, we create 3 additional models for static car, moving car and human classes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Clutter, Building, Road, Tree, Vegetation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"New experiment settings:\n* Loss: The same\n* Optimizer: The same\n* Scheduler: ReduceLROnPlateau with patience 3 and factor 0.3\n* Number of epochs changed to 40. Added weight decay. Learning rate is set to 1.5e-3. Batch size reduced to 5.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\nENCODER = 'efficientnet-b3'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'sigmoid'\nCLASSES = ['clutter', 'building', 'road', 'tree', 'vegetation']\n\n# create segmentation model with pretrained encoder\nmodel = smp.FPN(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES),\n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n\nlogdir = \"./logs\"\nnum_epochs = 40\nlearning_rate = 1.5e-3\nbase_optimizer = RAdam([\n    {'params': model.decoder.parameters(), 'lr': learning_rate, 'weight_decay': 1e-3}, \n    {'params': model.encoder.parameters(), 'lr': 1e-4, 'weight_decay': 1e-4},\n    {'params': model.segmentation_head.parameters(), 'lr': learning_rate},\n])\noptimizer = Lookahead(base_optimizer)\nscheduler = ReduceLROnPlateau(optimizer, factor=0.3, patience=3, mode='max')\ncriterion = BCEDiceLoss(activation=None)\nrunner = SupervisedRunner()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/hRhzVBf/cls5.png)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation']\ns_iou = [85.64, 72.85, 55.86, 69.78, 54.93]\ncls5_iou = [86.79, 74.16, 58.01, 71.08, 53.59]\nsing_iou = pd.DataFrame([s_iou, cls5_iou], columns=cls, index=['single', 'top5'])\nsing_iou","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the table, we’ve improved previous results in 4 of 5 classes.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pred5 = mask_to_overlay_image(image=image, masks=cls5[0], threshold=0.4)\nvisualize_overlay(image, pred5, truth_path=truth_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Moving Car","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we try several learning rates: 3e-4, 7e-4, 1e-3, 1.5e-3, 3e-3","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/XphB6s2/mc.png)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation(), classes=['moving_car'])\nimage, ground_truth = dataset[0]\n\ngrs2 = mask_to_overlay_image(image=image, masks=mc[0], threshold=0.4)\nvisualize(image, grs2, truth=ground_truth.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Static Car","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/cXKMJRS/sc.png)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation(), classes=['static_car'])\nimage, ground_truth = dataset[0]\n\ngrs3 = mask_to_overlay_image(image=image, masks=sc[0], threshold=0.4)\nvisualize(image, grs3, truth=ground_truth.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Human","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/VVDpYxt/hum.png)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation(), classes=['human'])\nimage, ground_truth = dataset[0]\n\ngrs4 = mask_to_overlay_image(image=image, masks=hum[0], threshold=0.4)\nvisualize(image, grs4, truth=ground_truth.squeeze())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation', 'Static Car', 'Moving Car', 'Human', 'mIoU']\nensemble = [86.79, 74.16, 58.01, 71.08, 53.59, 51.34, 39.27, 22.21, 57.06]\nfinal = pd.DataFrame([ensemble], columns=cls, index=['ensemble'])\nfinal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation())\nimage, _ = dataset[0]\nimage1, _ = dataset[1]\nimage2, _ = dataset[2]\nimage3, _ = dataset[3]\nimage4, _ = dataset[4]\nimages = [image, image1, image2, image3, image4]\nfull = [[cls5[i][0], cls5[i][1], cls5[i][2], sc[i][0], cls5[i][3], cls5[i][4], hum[i][0], mc[i][0]]  for i in range(5)]\nfull_truth1 = [mask_to_overlay_image(image=images[i], masks=full[i], threshold=0.4) for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for i in range(5):\n    visualize_prediction(images[i], full_truth1[i])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}