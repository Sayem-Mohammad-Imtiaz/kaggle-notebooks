{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import LinearSVC\nfrom stop_words import get_stop_words\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get stop words\nstop_words = get_stop_words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenise and filter \ndef tokenise(myStr):\n\n    # r is for python raw string, to avoid backslashes issues\n    cleanText = re.compile(r'[^0-9a-zA-Z:\\-|\\/\\\\_@#]')\n    tokens = cleanText.split(myStr)\n\n    wordList = []\n\n    for t in tokens:\n        if (t not in stop_words) and (len(t)>1):\n            wordList.append(t)\n\n    return wordList","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data, remove whitespace, and lower case\ndata = pd.read_csv('/kaggle/input/tweetsentiments/tweets_sentiment.csv', encoding = 'ISO-8859-1', usecols=[0, 1])\ndata['SentimentText'] = data['SentimentText'].str.strip()\ndata['SentimentText'] = data['SentimentText'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenise and filter each tweet\nlistOfLists = []\nfor i in range(len(data)):\n    listOfLists.append(tokenise(data['SentimentText'][i]))\ndata['Tokens'] = pd.DataFrame({'Tokens': listOfLists})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counts to see the dataset's ratio balance\ntotal_positive_count = (data['Sentiment']==1).sum()\ntotal_negative_count = (data['Sentiment']==0).sum()\n\ntotal_prob_positive_count = total_positive_count/len(data)\ntotal_prob_negative_count = total_negative_count/len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total number of tweets\nlen(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive and negative tweets count\nprint(\"Positive:\", total_positive_count, \"\\nNegative:\", total_negative_count)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# We can see that the data is balanced between the positive and negative tweets\nprint(\"Positive:\", total_prob_positive_count, \"\\nNegative:\", total_prob_negative_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The average length of tweets\nprint('The mean number of words in a tweet is',(data['Tokens'].str.len()).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The average length of tweets\nprint('The standard deviation of words in a tweet is',(data['Tokens'].str.len()).std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the longest tweetsk\ndata['Tokens'].str.len().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This converts the list of words into space-separated strings\ndata['Tokens'] = data['Tokens'].apply(lambda x: ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the data into occurrences\ncount_vect = CountVectorizer()  \ncounts = count_vect.fit_transform(data['Tokens'])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['Negative', 'Positive']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap='coolwarm'):\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    \n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model(model, name):\n\n    # Train the model using the training sets \n    model.fit(train_x, train_y)\n    \n    # Training score output\n    print('Training score:', model.score(train_x, train_y))\n\n    # Predict Output \n    predicted = model.predict(test_x)\n\n    # Testing score output\n    print('Testing score:', model.score(test_x, test_y))\n    \n    # Mean square error\n    print('Mean-squared error:', mean_squared_error(test_y, predicted))\n    \n    # Confusion matrix\n    cnf_matrix = confusion_matrix(test_y, predicted)\n    print('Confusion Matrix without normalisation:\\n', cnf_matrix)  \n    \n    # Draw non-normalised confusion matrix\n    #plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False, title='Confusion matrix without normalisation')\n    \n    # Plot normalized confusion matrix\n    #plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Confusion matrix with normalisation')\n    \n    \n    # Save plot\n    plt.savefig(name+'.png', bbox_inches='tight', dpi=100)\n    \n    # Show\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithms "},{"metadata":{},"cell_type":"markdown","source":"## Independent Treatment"},{"metadata":{},"cell_type":"markdown","source":"### Naive-Bayes"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"make_model(MultinomialNB(fit_prior=False), 'MNBNoPrior')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_model(MultinomialNB(fit_prior=True), 'MNBWithPrior')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nmake_model(SGDClassifier(loss='hinge', penalty='l2', random_state=42), 'LinearSGD')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###Â Linear Classifier (Logistic Regression)"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"make_model(LogisticRegression(), 'LogisticalReg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dependent Treatment"},{"metadata":{},"cell_type":"markdown","source":"### SVD / LSA\n\nSVD is essentially doing a variation of a PCA, and is reducing the number of columns you have (100k), to a chose n_components)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\nsvd.fit(train_x)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(svd.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(svd.explained_variance_ratio_.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(svd.singular_values_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 2"},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machines"},{"metadata":{},"cell_type":"markdown","source":"### 1,2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This converts the list of words into space-separated strings\n# data['Tokens'] = data['Tokens'].apply(lambda x: ' '.join(x))\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,2))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"make_model(LinearSVC(), 'SVC_1_2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1,3"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,3))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"make_model(LinearSVC(), 'SVC_1_3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1,4"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,4))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"make_model(LinearSVC(), 'SVC_1_4')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2, 2"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(2,2))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_2_2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2,3"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(2,3))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_2_3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1, 5"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,5))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_1_5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1, 6"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do counts again\n\n# Transform the data into occurrences\ncount_vect = CountVectorizer(ngram_range=(1,6))  \ncounts = count_vect.fit_transform(data['Tokens'])  \n\n# Transform into inverted indice\ntransformer = TfidfTransformer().fit(counts)\ncounts = transformer.transform(counts)\n\n# Split data train:test 80:20\ntrain_x, test_x, train_y, test_y = train_test_split(counts, data['Sentiment'],\n                                                    train_size=0.8, test_size=0.2, random_state=6)\n\nmake_model(LinearSVC(), 'SVC_1_6')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot ngram_max value vs score"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Data for plotting\nt = np.arange(2, 7)\ns = [0.79292, 0.79574, 0.79589, 0.79508, 0.79439]\n\nfig, ax = plt.subplots()\nax.plot(t, s)\n\nax.set(xlabel='Max n_gram', ylabel='Score',\n       title='Max n_gram vs score')\nax.grid()\n\nplt.savefig('Plot.png', bbox_inches='tight', dpi=100)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}