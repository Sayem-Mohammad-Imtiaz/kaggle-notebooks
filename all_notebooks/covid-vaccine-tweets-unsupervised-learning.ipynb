{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Steps:**\n- Import Libraries and Load Data into Memory\n- K-Means Clustering Using HashingVectorizer\n    - Clustering using Scikit-Learn's HashingVectorizer and KMeans modules\n    - PCA dimension reduction to 2D in order to visualize the data points\n    - Making WordClouds to show the clusters\n        - Thanks to Vadim Nareyko's Kaggle Notebook: Google word2vec, KMeans, PCA https://www.kaggle.com/nareyko/google-word2vec-kmeans-pca Got inspired by his post\n- TextBlob Clustering\n    - Output 1: Clustering and TextBlob output\n- Finding the best K for K-means Using SpaCy, TF-IDF, and Elbow Method\n    - Output 2: Clustering 10 Out","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries and Load Data into Memory","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport random\nfrom sklearn.utils import shuffle\n\n#for text cleaning\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# clustering\nfrom gensim.models import word2vec, KeyedVectors\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KDTree\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\n\nimport re;\nimport logging;\nimport sqlite3;\nimport time;\nimport sys;\nimport multiprocessing;\nfrom wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\nimport matplotlib.pyplot as plt;\nfrom itertools import cycle;\n\nfrom tqdm import tqdm\ntqdm.pandas()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 2021","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import DataFrame into Memory","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n#Importing the dataset \ndata = pd.read_csv(\"../input/covidvaccine-tweets/covidvaccine.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at the Data Frame:","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See a random data entry:","metadata":{}},{"cell_type":"code","source":"random.seed(SEED)\ndata.iloc[random.randint(0, len(data))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape #183494 entries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning","metadata":{}},{"cell_type":"code","source":"def clean_text(txt):\n    '''\n    cleans the input text in the following steps:\n    1 - replace contractions\n    2 - removing punctuation\n    3 - spliting into words\n    4 - removing stopwords\n    5 - removing leftover punctuations\n    6 - lower-case everything\n    '''\n    contraction_dict = {   \n        \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n        \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n        \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n        \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n        \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n        \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n        \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n        \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n        \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n        \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n        \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n        \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n        \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n        \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n        \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n        \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n        \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n        \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n        \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n        \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n        \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n        \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n        \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n        \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n        \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n        \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n        \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n        \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n    def _get_contractions(contraction_dict):\n        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n        return contraction_dict, contraction_re\n\n    def replace_contractions(text):\n        contractions, contractions_re = _get_contractions(contraction_dict)\n        def replace(match):\n            return contractions[match.group(0)]\n        return contractions_re.sub(replace, text)\n\n    # replace contractions\n    txt = replace_contractions(txt)\n    \n    #remove punctuations\n    txt  = \"\".join([char for char in txt if char not in string.punctuation])\n    txt = re.sub('[0-9]+', '', txt)\n    \n    # split into words\n    words = word_tokenize(txt)\n    \n    # remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    \n    # removing leftover punctuations\n    words = [word for word in words if word.isalpha()]\n    \n    # lower-case everything\n    words = [w.lower() for w in words]\n    \n    # stem the words\n    porter = PorterStemmer()\n    words = [porter.stem(w) for w in words]\n    \n    \n    cleaned_text = ' '.join(words)\n    return cleaned_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[['text', 'hashtags']].fillna('')\ndata.head()\n\ndata['raw_tweet'] = data['text'] + ' ' + data['hashtags']\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['tweet'] = data['raw_tweet'].progress_apply(lambda txt: clean_text(txt))\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"clean_df = clean_df.str.split()\nclean_df.head()","metadata":{}},{"cell_type":"code","source":"data.sort_values(by=['tweet']).head()\n# still a bunch of empty lists in the dataset. We will have to remove them before clustering.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove the empty strings\ndf = data[data['tweet'].astype(bool)]\ndf.sort_values(by=['tweet']).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I only want to keep hashtags and tweet\ndf = df[['hashtags', 'tweet']]\ndf.sort_values(by=['tweet']).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet_split'] = df.tweet.str.split()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering using HushingVectorizer in sklearn\n\nThis part is reference by Vadim Nareyko's Kaggle Notebook: Google word2vec, KMeans, PCA.\nHere is the link to his notebook. https://www.kaggle.com/nareyko/google-word2vec-kmeans-pca\nThank you.","metadata":{}},{"cell_type":"code","source":"# find out the longest tweet\nmax_len = df.tweet.str.len().max()\nprint(max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\"]\n# create the transform\nvectorizer = HashingVectorizer(n_features=20)\n# encode document\nvector = vectorizer.transform(text)\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = HashingVectorizer (n_features = max_len)\ndf['vector'] = df['tweet'].progress_apply(lambda t: vectorizer.fit_transform([t]).toarray())\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.concatenate(df['vector'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 4)\ndf['cluster'] = kmeans.fit_predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PCA for visualization","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2)\npca_result = pca.fit_transform(X)\ndf['x'] = pca_result[:, 0]\ndf['y'] = pca_result[:, 1]\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_colors = pd.np.array(['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'])\ndf['color'] = cluster_colors[df.cluster.values]\ndf['text'] = df.tweet.str[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bokeh.io\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, LabelSet\n\n# from bokeh.charts import Donut, HeatMap, Histogram, Line, Scatter, show, output_notebook, output_file\nbokeh.io.output_notebook()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualize the data using bokeh\n#output_file(\"top_artists.html\", title=\"top artists\")\n# TOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n\nsource = ColumnDataSource.from_df(df[['x', 'y', 'color', 'text']])\nTOOLTIPS = [(\"text\", \"@text\")]\nTOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,save\"\n\nplot = figure(plot_width=800, plot_height=450, tooltips=TOOLTIPS, tools=TOOLS)\n\n#draw circles\nplot.circle(y='y', x='x', source=source, size=15, fill_color='color')\nshow(plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WordClouds","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.query('cluster == 4').tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster0 = df.query('cluster == 0').tweet\ncluster1 = df.query('cluster == 1').tweet\ncluster2 = df.query('cluster == 2').tweet\ncluster3 = df.query('cluster == 3').tweet\n\n\ntext0 = ' '.join(tweet for tweet in cluster0)\ntext1 = ' '.join(tweet for tweet in cluster1)\ntext2 = ' '.join(tweet for tweet in cluster2)\ntext3 = ' '.join(tweet for tweet in cluster3)\n\nprint(f'There are {len(text0)} words in the combination of all cells in column \"tweet\" labeled as cluster 0.')\nprint(f'There are {len(text1)} words in the combination of all cells in column \"tweet\" labeled as cluster 1.')\nprint(f'There are {len(text2)} words in the combination of all cells in column \"tweet\" labeled as cluster 2.')\nprint(f'There are {len(text3)} words in the combination of all cells in column \"tweet\" labeled as cluster 3.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create stopwords\nstopwords = set(STOPWORDS)\nstopwords.update(['amp', 'covid', 'covidvaccin', 'vaccin', 'today', 'peopl', 'coronaviru', 'say', 'day', 'one']) \n# I want to see what's left after filtered out the common keywords.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud0 = WordCloud(stopwords=stopwords, background_color='white').generate(text0)\nwordcloud1 = WordCloud(stopwords=stopwords, background_color='white').generate(text1)\nwordcloud2 = WordCloud(stopwords=stopwords, background_color='white').generate(text2)\nwordcloud3 = WordCloud(stopwords=stopwords, background_color='white').generate(text3)\n\nwordcloud = [wordcloud0, wordcloud1, wordcloud2, wordcloud3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cluster 0\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud0, interpolation='bilinear')\nplt.title('Cluster 0 Key words', size=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cluster 1\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud1, interpolation='bilinear')\nplt.title('Cluster 1 Key words', size=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cluster 2\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.title('Cluster 2 Key words', size=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cluster 3\nplt.figure(figsize=(10,5))\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud3, interpolation='bilinear')\nplt.title('Cluster 3 Key words', size=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using TextBlob for Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\ndef get_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef get_polarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndef get_sentiment(score):\n    if score > 0:\n        return 'Positive'\n    elif score == 0:\n        return 'Neutral'\n    else:\n        return 'Negative'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['subjectivity'] = df['text'].apply(get_subjectivity)\ndf['polarity'] = df['text'].apply(get_polarity)\ndf['sentiment'] = df['polarity'].apply(get_sentiment)\ndf.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View sentiment counts\n\ndf.sentiment.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizae Polarity and Subjectivity of Tweets","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nplt.scatter(df['polarity'], df['subjectivity'], s=4)\n\nplt.ylabel('Subjectivity')\nplt.xlabel('Polarity')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Upload Output 1: TextBlob Sentiments","metadata":{}},{"cell_type":"code","source":"textblob_output = df.loc[:,['hashtags', 'tweet','subjectivity', 'polarity', 'sentiment']].sort_values('sentiment')\ntextblob_output.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textblob_output.to_csv('textblog.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF Vectorizer and Finding the Best K with Elbow Method","metadata":{}},{"cell_type":"markdown","source":"### Read the Data","metadata":{}},{"cell_type":"code","source":"#df=pd.read_csv(\"../input/covidvaccine-tweets/covidvaccine.csv\")\ndf = pd.read_csv(\"../input/covidvaccine-tweets/covidvaccine.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.is_retweet.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"#We create a pandas dataframe as follows:\ndata = pd.DataFrame(data=df.text)\ndata = data.rename(columns={'text' : 'Tweets'})\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We display the first 10 elements of the dataframe:\npd.set_option('max_colwidth',170)\ndisplay(data.head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs=df.text.head(1000).values\ntype(docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs_clean = []\nfor doc in docs:\n    doc_2 = re.sub(r':.*$', \":\", doc)\n    docs_clean.append(doc_2)\n\ndocs_clean[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs2=docs_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove punctuations\npunctuationChars = '!@#$%^&*(){}{}|;:\",./<>?' # you might choose different charcters to drop\nfor i in punctuationChars:\n    docs2 = np.char.replace(docs2, i, ' ')\n# remove apostrophe's (single quotes)\ndocs2 = np.char.replace(docs2,\"'\",' ')\n# remove line feeds\ndocs2 = np.char.replace(docs2,\"\\n\",' ')\n# remove 'http:'\ndocs2 = np.char.replace(docs2,\"https:\",' ')\ndocs2 = np.char.replace(docs2,\"https\",' ')\n\n# make lower case\nfor i,s in enumerate(docs2):\n    docs2[i] = s.lower()\n    \n# Show the cleaned data\n# Show the beginning of each document\n\n#for i in range(len(docs2)):\n#        print(f'\\ndoc{i}: {docs2[i]}') \ndocs2[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define SpaCy Tokenizer","metadata":{}},{"cell_type":"code","source":"def spacy_tokenizer(document):\n    tokens = nlp(document)\n    tokens = [token for token in tokens if (\n        token.is_stop == False and \\\n        token.is_punct == False and \\\n        token.lemma_.strip()!= '')]\n    tokens = [token.lemma_ for token in tokens]\n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data to see what spacy tokenizer can do.\nexample_corpus = [\n    \"Monsters are bad. They likes to eat geese. I saw one goose flying away\", \\\n    \"I saw a monster yesterday. The meaning is so obvious!\", \\\n    \"Why are we talking about bad monsters? They are meanness.\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\ntfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\n# test\ncorpus=example_corpus\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult_test = tfidf_vector.fit_transform(corpus)\nresult_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Successfully extraxt intended meaning of the words. 14 tokens for the example corpus.","metadata":{}},{"cell_type":"code","source":"dense = result_test.todense()\ndenselist = dense.tolist()\ndf_test = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply Spacy tokenizer, TF-IDF, K-means for the first 100 tweets.","metadata":{}},{"cell_type":"code","source":"tfidf_vector = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)\ncorpus = docs2\n\n# fit: learns vocabulary and idf\n# transform: transforms documents into document-term matrix\nresult = tfidf_vector.fit_transform(corpus)\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It’s a sparse matrix with 1000 reviews and 3191 terms, out of those 3191000 possible numbers there are 9169 non-zero TF-IDF values. We can check which terms are actually considered from the sentences with the get_feature_names method:","metadata":{}},{"cell_type":"code","source":"# We can check which terms are actually considered from the sentences with the get_feature_names method:\ntfidf_vector.get_feature_names()[1:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sparse matrix format is an efficient way to store this information, but you might want to convert it to a more readable, dense matrix format using the todense method. To create a pandas DataFrame from the results, you can use the following code:","metadata":{}},{"cell_type":"code","source":"dense = result.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(\n    denselist,columns=tfidf_vector.get_feature_names())\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see the weights for words contained in the first tweet:","metadata":{}},{"cell_type":"code","source":"df[[\"australia\", \"manufacture\", \"covid-19\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check cosine similarity","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import linear_kernel\ncos_df = pd.DataFrame(columns=df.index)\nfor i in range(999):\n    curr_cos_sim = linear_kernel(result[i:i+1], result).flatten()\n    cos_df[i] = curr_cos_sim\n    \ncos_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the clustering table","metadata":{}},{"cell_type":"code","source":"kmeans_models = {}\nfor i in range(2,13+1):\n    current_kmean = KMeans(n_clusters=i).fit(result)\n    kmeans_models[i] = current_kmean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_df = pd.DataFrame()\ncluster_df['Review Texts'] = docs\nfor i in range(2, 13+1):\n    col_name = str(i) +'means_label'\n    cluster_df[col_name] = kmeans_models[i].labels_\ncluster_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Elbow Method to determine the best K","metadata":{}},{"cell_type":"code","source":"Sum_of_squared_distances = []\nK = range(1,18)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(result)\n    Sum_of_squared_distances.append(km.inertia_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choose K=10 to experiment","metadata":{}},{"cell_type":"code","source":"cluster10 = cluster_df.iloc[:,[0,9]]\ncluster10_0 = cluster10.loc[cluster10[\"10means_label\"] == 0]\ncluster10_0.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster10_1 = cluster10.loc[cluster10[\"10means_label\"] == 1]\ncluster10_1.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cluster_2 focus on topics related to Russia Vaccine","metadata":{}},{"cell_type":"code","source":"cluster10_2 = cluster10.loc[cluster10[\"10means_label\"] == 2]\ncluster10_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cluster 3 contains more rumors and negative reactions","metadata":{}},{"cell_type":"code","source":"cluster10_3 = cluster10.loc[cluster10[\"10means_label\"] == 3]\ncluster10_3.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster10_4 = cluster10.loc[cluster10[\"10means_label\"] == 4]\ncluster10_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster10_5 = cluster10.loc[cluster10[\"10means_label\"] == 5]\ncluster10_5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Upload Output 2: 10 clusters Tweets","metadata":{}},{"cell_type":"code","source":"cluster10 = cluster10.sort_values(by='10means_label')\ncluster10.to_csv('cluster10.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}