{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Created by: Sangwook Cheon\n\nDate: Dec 23, 2018\n\nThis is step-by-step guide to Regression using scikit-learn, which I created for reference. I added some useful notes along the way to clarify things. I am excited to move onto more advanced concepts, such as deep learning, using frameworks like Keras and Tensorflow.\nThis notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn about the basics of regression.\n\n# Content:\n\n### 1. Simple Linear Regression\n### 2. Multiple Linear Regression\n### 3. Polynomial Regression\n### 4. Supper Vector Regression (SVR)\n### 5. Decision Tree Regression\n### 6. Random Forest Regression\n### 7. R-squared/Adjusted R-squared\n_______________________________________________________\n_______\n\n# Simple Linear Regression  \n\n![i7](https://i.imgur.com/LEgEZqA.png)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#data preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('../input/salary-data/Salary_Data.csv')\nx = data.iloc[:, :-1].values\ny = data.iloc[:, 1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 1/3, random_state = 42)\n\n#No need to do feature scaling, as the library automatically takes care of this.\n\n#fitting regressor\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, Y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1af79df49b0877e2618e952192fbf98a632ba8b9"},"cell_type":"code","source":"#predicting the test set results\ny_pred = regressor.predict(X_test) #vector of all predictions of the dependent variable\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6454339f168f6db54b1e52fada5822e230fb9ea3"},"cell_type":"code","source":"#visualize the training set results\nplt.scatter(X_train, Y_train, color = 'red')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue')\nplt.title(\"years vs salary\")\nplt.xlabel(\"number of years\")\nplt.ylabel(\"salary (dollars)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"488b18947fa210bddb0959d3603e0e0ec14a0f52"},"cell_type":"code","source":"#visualizing test set results\nplt.scatter(X_test, Y_test, color = 'red')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue') #this should not change as the regressor fitted to train set should be shown\nplt.title(\"years vs salary\")\nplt.xlabel(\"number of years\")\nplt.ylabel(\"salary (dollars)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55bc033a28772092c422a5b794c11dcd1fbab13d"},"cell_type":"markdown","source":"# Multiple linear regression\n![i8](https://i.imgur.com/1YjjMvT.png)\n\n## Dummy variables\nNever include all dummy variable columns. Always omit one column (ex: if 9 columns, include 8), because including all leads to\n--> Dummy variable trap, which will disrupt the learning of the machine learning model.\n\n## 5 methods of building models:\n\n**1. All-in**\n\n*     • Putting all the variables into the equation.\n*     • Prepare the Backward Elimination\n\n**2. Backward Elimination**\n\n*     • Step 1: Select a significant level to stay in the model \n*     • Step 2: Fit the full model with all possible predictors\n*     • Step 3: Consider the predictor with the highest P-value. If P > Significance level, go to Step 4, otherwise go to FIN\n*     • Step 4: Remove the predictor\n*     • Step 5: Fit model without this variable. Go back to Step 3\n\n**3. Forward Selection**\n\n*     • Step 1: Select a significant level to enter the model \n*     • Step 2: Fit all simple regression models y ~ xn  Select the one with the lowest P-value.\n*     • Step 3: Keep this variavle and fit all possible models with one extra predictor added to the one(s) you already have\n*     • Step 4: Consider the predictor with the lowest P-value. If P > SL, go to STEP 3, other waise go to FIN\n\n**4. Bidirectional Elimination**\n\n*     • Step 1: Select a significance level to enter and to stay in the model (e.g: SLENTER = 0.05, SLSTAY = 0.05\n*     • Step 2: Perform the next step of Forward Selection (new variables must have P < SLENTER to enter)\n*     • Step 3: Perform ALL steps of Backward Elimination (old variables must have P < SLSTAY to stay)\n*     • Step 4: No new variables can enter and no old variables can exit. Until this happens,  repeat Step 2 and 3.\n\n**5. All Possible Models**\n\n*     • Step 1: Select a criterian of goodness of fit \n*     • Step 2: Construct All possible regression models 2^n - 1 total combinations\n*     • Step 3: Select the one with the best criterion.\n"},{"metadata":{"trusted":true,"_uuid":"98269e88ba87a9bcf5667e30d4d6e9db7e510c63"},"cell_type":"code","source":"data2 = pd.read_csv('../input/m-50-startups/50_Startups.csv')\nx = data2.iloc[:, :-1]\ny = data2.iloc[:, 4]\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\nxtemp = x.iloc[:, 3]\nlabelencoder = LabelEncoder()\nxtemp = labelencoder.fit_transform(xtemp)\nxtemp = pd.DataFrame(to_categorical(xtemp))\n\nx = x.drop(['State'], axis = 1) #In pandas axis = 1 --> column\nx = pd.concat([x, xtemp], axis = 1)\n\n# x = x.iloc[:, :-1] This is to avoid dummay variable trap. However, the library already takes care of this, so no need.\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dfae627f34087c73cc832073d3751cb4a4f8980"},"cell_type":"code","source":"#Linear Regression\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, Y_train)\n\ny_pred = regressor.predict(X_test)\ny_pred, Y_test\n\nprint(x.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df71bb8424ef5a0e8ba1cfc4375f3dbbaef20e79"},"cell_type":"code","source":"#Find an optimal team of independent variables, so that each variable has significant impact on the prediction. \n# --> Backward elimination\n\nimport statsmodels.formula.api as sm\nx = np.array(x) #use numpy arrays instead of DataFrames for more useful functions. DataFrames are useful for preparing dataset\nx = np.append(np.ones((x.shape[0], 1), dtype = 'int'), x, axis = 1) #(x.shape[0], 1).astype(int) does not work\n#Above is done to add constant to the model, which is necessary for Ordinary Least Squares to work","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ca877ac3bea99b9e7209500df98edd39ea1e010"},"cell_type":"code","source":"x_opt = x[:, [0,1,2,3,4,5]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b23fc94bac356ad2a87731285d8b5ded27de8547"},"cell_type":"code","source":"'''The significance value is set to 0.05 If P-value is lower than this, \nthen it is significant. If higher, it is less significant. Therefore, variables\nwith higher P_values need to be removed, as they do not have large impact. \nThis is called backward elimination. \n\nIn this case, as x4 has 0.990, it needs to be removed'''\n\nx_opt = x[:, [0,1,2,3,4,5]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d15873f96fc7919026751d737347aa27e79e55f"},"cell_type":"code","source":"\n\n\n#repeat the step --> remove the insignificant variable, fit it, repeat it.\n\nx_opt = x[:, [0,1,3,5]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32c9027b477f76d88d4f8fc1975c370e319cf1cd"},"cell_type":"code","source":"x_opt = x[:, [0,1,3]] #needs to specify all the indexes, so that individual index is evaluated.\nregressor_OLS = sm.OLS(endog = y, exog = x_opt).fit()\nregressor_OLS.summary() #shows statistical summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a06f19099a6f13e69473d4891f5fcf28ae7b6d6b"},"cell_type":"code","source":"#To check if the team of variables are correct\nx_show = pd.DataFrame(x)\n# x_show\n\n#Therefore, only using these variables ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77d4e7cc6851dcd72bbd090875af6c225c6ea82d"},"cell_type":"markdown","source":"# Polynomial regression\ny = b0 + b1x1 + b2x1^2 --- bnx1^n"},{"metadata":{"trusted":true,"_uuid":"183f0ae7fe329ab7111c270e8ec02fff5c139ed3"},"cell_type":"code","source":"data3 = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\n\n# X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n# Use whole dataset to train\n\ndata3\nplt.scatter(data3.iloc[:, 1], y)\nplt.title('Level vs Salary')\nplt.xlabel(\"Level\")\nplt.ylabel(\"Salary (dollrs)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"534988dbfd1f2488275855f513d8b91168f8601a"},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n#transforms the x matrix into a new matrix that has x1, x2, x3 --- columns\npoly_reg = PolynomialFeatures(degree = 2) #specify the degree -> how many terms\nx_poly = poly_reg.fit_transform(x)\nx_poly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"109b52fcb7ae963c3e23af34221bdd94afa082fa"},"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_reg.fit(x_poly, y)\n\ny_pred = lin_reg.predict(poly_reg.fit_transform(x)) #this is used instead of x_poly, so that this model will work for any matrix input x \n\nplt.figure(2)\nplt.scatter(x, y)\nplt.plot(x, y_pred)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f66567423299bf42f6454f189ba7d695ca19a30b"},"cell_type":"code","source":"#improving the model --> add degrees to make it more complex\n\npoly_reg = PolynomialFeatures(degree = 4) #specify the degree -> how many terms\nx_poly = poly_reg.fit_transform(x)\nx_poly\n\nlin_reg = LinearRegression()\nlin_reg.fit(x_poly, y)\n\ny_pred = lin_reg.predict(poly_reg.fit_transform(x)) #this is used instead of x_poly, so that this model will work for any matrix input x \n\n#This is to get a more continuous curve, by plotting more x values.\nx_grid = np.arange(min(x), max(x), 0.1) #0.1 --> increment \nx_grid = x_grid.reshape(x_grid.shape[0], 1)\n\nplt.figure(2)\nplt.scatter(x, y)\nplt.plot(x_grid, lin_reg.predict(poly_reg.fit_transform(x_grid)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de8487db3e61ada92c71d2ef97dcf61a8a95636d"},"cell_type":"markdown","source":"# Support Vector Regression (SVR)\n![i8](https://i.imgur.com/QDhMroy.png)"},{"metadata":{"_uuid":"abd394b987373944e27cab221ae03d0cc8975b4c"},"cell_type":"markdown","source":"In SVR, the objective is to make sure errors do not exceed the threshold, while in linear regression it is to minimize the error between prediction and data."},{"metadata":{"trusted":true,"_uuid":"aa375fb50bae6bd4409800f7ed3221b8eb228596"},"cell_type":"code","source":"data3 = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\ny = y.reshape(y.shape[0], 1)\nx = x.reshape(x.shape[0], 1)\n\nprint(y.shape)\n# X_train, X_test, Y_train, Y_test = train_test_split()\n\n#svr does not have feature scaling built in\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nx = sc_x.fit_transform(x)\ny = sc_y.fit_transform(y)\n\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(x, y)\n\ny_pred = sc_y.inverse_transform(regressor.predict(sc_x.transform(np.array([[6.5]])))) #input scaled value, then inverse scale the predicted value\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"473e68710a4235d60b087b225ddb934de83c0b7b"},"cell_type":"markdown","source":"# Decision tree Regression\n![i9](https://i.imgur.com/JZkzrXt.png)\n\nInformation Entropy --> tries to find an optimal way to split the dataset into leaves (each section is called a leaf)."},{"metadata":{"_uuid":"909d847ca37f3f77214ba33ec247a8d5610d08d0"},"cell_type":"markdown","source":"![i10](https://i.imgur.com/CdxakvJ.png)\nTake the average of each leaf, and assign that value to any coordinate that falls under any leaf."},{"metadata":{"trusted":true,"_uuid":"be31380270931a622bbb95e8fd413ce5fb80146e"},"cell_type":"code","source":"data3 = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\n\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(x, y)\n\ny_pred = regressor.predict(np.array([[6.5]]))\n\nx_grid = np.arange(min(x), max(x), 0.01)\nx_grid = x_grid.reshape(len(x_grid), 1)\nplt.figure(3)\nplt.plot(x_grid, regressor.predict(x_grid))\nplt.show()\n\n#Notice how average is used to represent each interval.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9f3ef6a82d1203de0b8d0840568f8b66799025b"},"cell_type":"markdown","source":"# Random Forest Regression\n\n* Step 1: Pick at random K points from the training set\n* Step 2: Build the Decision Tree associated to these K points\n* Step 3: Choose the number of Decision trees to build, and repeat step 1 and 2\n* Step 4: For a new data point, make each Decision tree output a prediction, and assign the average of these values.\n\n## Forest --> A team of trees"},{"metadata":{"trusted":true,"_uuid":"83ab215bcf6ac5107c10da930e29a3dad98143f2"},"cell_type":"code","source":"data3 = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\nx = data3.iloc[:, 1:2].values #1:2 is done instead of only 1, because independent variable should be a matrix.\ny = data3.iloc[:, 2].values\n\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 5000, criterion = 'mse', random_state = 0) #can tune the n_estimators\nregressor.fit(x, y)\n\ny_pred = regressor.predict([[6.5]])\n\nx_grid = np.arange(min(x), max(x), 0.01)\nx_grid = x_grid.reshape(len(x_grid), 1)\nplt.plot(x_grid, regressor.predict(x_grid))\nplt.show()\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78317cecd1cc52de60aa60959cdae4a01d2597cb"},"cell_type":"markdown","source":"# R-Squared\n\nr^2 = 1 - SSres /SStot\n\nSSres = SUM (yi - yihat)^2 --> Sum of residuals\n\nSStot = SUM (yi - yavg)^2 \n\nFind a best line that minizes R^2, and make it best compared to the average line. \n\n**Closer to 1, better. If smaller, worse**\n\n![i11](https://i.imgur.com/4KGJmle.png)\n\n# Adjusted R-Squared\n\nThis takes care of adding non-meaningful regressors. It penalizes the model for adding a variable.  This metric can be used to assess the model accurately.\n![i12](https://i.imgur.com/bqZZu05.png)\n\n## When doing backward elimination, check Adjusted R-Squared to see if removing a variable is beneficial to the model. If the Adjusted R-Squared grows, then removing it is a good idea.\n\n\tcoef\tstd err\tt\tP>|t|\t[0.025\t0.975]\nconst\t4.698e+04\t2689.933\t17.464\t0.000\t4.16e+04\t5.24e+04\nx1\t0.7966\t0.041\t19.266\t0.000\t0.713\t0.880\nx2\t0.0299\t0.016\t1.927\t0.060\t-0.001\t0.061\n\nWhen interpreting these coeffients, which is shown in the Statsmodel library, needs to be careful about units.\n\nThe coefficient part of the table shows how much impact a variable has on the independent variable **per unit**. If the variables are of same unit, then they can be compared, but if they are not, it is only valid to say \"one has more impact then the other per unit\""},{"metadata":{"trusted":true,"_uuid":"5e20716585d15af8914d77b7b37edb3f04b8d103"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}