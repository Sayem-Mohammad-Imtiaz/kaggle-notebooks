{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle\nimport pandas as pd\nimport pickle\nfrom matplotlib.pyplot import MultipleLocator","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:22:56.619447Z","iopub.execute_input":"2021-05-21T05:22:56.619927Z","iopub.status.idle":"2021-05-21T05:23:03.901345Z","shell.execute_reply.started":"2021-05-21T05:22:56.619831Z","shell.execute_reply":"2021-05-21T05:23:03.900131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:23:03.902639Z","iopub.execute_input":"2021-05-21T05:23:03.90303Z","iopub.status.idle":"2021-05-21T05:23:09.49678Z","shell.execute_reply.started":"2021-05-21T05:23:03.902999Z","shell.execute_reply":"2021-05-21T05:23:09.495032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.TPUStrategy(resolver)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:23:09.498711Z","iopub.execute_input":"2021-05-21T05:23:09.499029Z","iopub.status.idle":"2021-05-21T05:23:09.508264Z","shell.execute_reply.started":"2021-05-21T05:23:09.499Z","shell.execute_reply":"2021-05-21T05:23:09.5068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"读入数据\"\"\"\ntrain_data = pd.read_csv(r'../input/blood-pressure-datasets/Train_Merge_Data.csv')\ntrain_dev = pd.read_csv(r'../input/ppg-dev-new/train.csv')\n\nvalidation_data = pd.read_csv(r'../input/blood-pressure-datasets/Validation_Merge_Data.csv')\nvalidation_dev = pd.read_csv(r'../input/ppg-dev-new/validation.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:23:09.510083Z","iopub.execute_input":"2021-05-21T05:23:09.510556Z","iopub.status.idle":"2021-05-21T05:25:12.457108Z","shell.execute_reply.started":"2021-05-21T05:23:09.510517Z","shell.execute_reply":"2021-05-21T05:25:12.456136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"组合训练数据和一阶导、二阶导数据\"\"\"\ntrain_data = pd.concat([train_data,train_dev],axis=1)\nvalidation_data = pd.concat([validation_data,validation_dev],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:12.458666Z","iopub.execute_input":"2021-05-21T05:25:12.459252Z","iopub.status.idle":"2021-05-21T05:25:13.430967Z","shell.execute_reply.started":"2021-05-21T05:25:12.459194Z","shell.execute_reply":"2021-05-21T05:25:13.429711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"打乱数据\"\"\"\ntrain_data = shuffle(train_data)\nvalidation_data = shuffle(validation_data)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:13.432475Z","iopub.execute_input":"2021-05-21T05:25:13.432828Z","iopub.status.idle":"2021-05-21T05:25:17.08956Z","shell.execute_reply.started":"2021-05-21T05:25:13.432793Z","shell.execute_reply":"2021-05-21T05:25:17.088398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载训练集\"\"\"\ntrain_label_Sbp = train_data.iloc[:,610]\ntrain_label_Sbp = train_label_Sbp.values\n\ntrain_label_Dbp = train_data.iloc[:,611]\ntrain_label_Dbp = train_label_Dbp.values\n\ntrain_dev1 = train_data.iloc[:,662:1262]\ntrain_dev1 = train_dev1.values\n\ntrain_dev2 = train_data.iloc[:,1262:]\ntrain_dev2 = train_dev2.values\n\ntrain_data = train_data.iloc[:,:600]\ntrain_data = train_data.values\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:17.091406Z","iopub.execute_input":"2021-05-21T05:25:17.091905Z","iopub.status.idle":"2021-05-21T05:25:17.101808Z","shell.execute_reply.started":"2021-05-21T05:25:17.09185Z","shell.execute_reply":"2021-05-21T05:25:17.100709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载验证集\"\"\"\nvalidation_label_Sbp = validation_data.iloc[:,610]\nvalidation_label_Sbp = validation_label_Sbp.values\n\nvalidation_label_Dbp = validation_data.iloc[:,611]\nvalidation_label_Dbp = validation_label_Dbp.values\n\nvalidation_dev1 = validation_data.iloc[:,662:1262]\nvalidation_dev1 = validation_dev1.values\n\nvalidation_dev2 = validation_data.iloc[:,1262:]\nvalidation_dev2 = validation_dev2.values\n\nvalidation_data = validation_data.iloc[:,:600]\nvalidation_data = validation_data.values","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:17.104679Z","iopub.execute_input":"2021-05-21T05:25:17.105142Z","iopub.status.idle":"2021-05-21T05:25:17.118195Z","shell.execute_reply.started":"2021-05-21T05:25:17.105104Z","shell.execute_reply":"2021-05-21T05:25:17.11733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"数据格式、尺寸\"\"\"\nprint(\"train_data_information:\")\nprint(train_data.shape)\nprint(train_dev1.shape)\nprint(train_dev2.shape)\nprint(train_label_Sbp.shape)\nprint(train_label_Dbp.shape)\n\nprint(\"validation_data_information:\")\nprint(validation_data.shape)\nprint(validation_dev1.shape)\nprint(validation_dev2.shape)\nprint(validation_label_Sbp.shape)\nprint(validation_label_Dbp.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:17.119766Z","iopub.execute_input":"2021-05-21T05:25:17.120237Z","iopub.status.idle":"2021-05-21T05:25:17.143236Z","shell.execute_reply.started":"2021-05-21T05:25:17.120191Z","shell.execute_reply":"2021-05-21T05:25:17.142373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"注意力机制模块\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n# 注意力机制：Relu6方式\n# 激活函数 relu6\ndef relu6(x):\n    return tf.keras.activations.relu(x, max_value=6)\n#   利用relu函数乘上x模拟sigmoid\ndef hard_swish(x):\n    return x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n\n#---------------------------------------#\n#   通道注意力机制单元\n#   利用两次全连接算出每个通道的比重\n#   可以连接在任意特征层后面\n#---------------------------------------#\n\ndef squeeze(inputs):\n    input_channels = int(inputs.shape[-1])\n    \n    x = layers.GlobalAveragePooling1D()(inputs)\n\n    x = layers.Dense(int(input_channels/4))(x)\n    x = relu6(x)\n\n    x = layers.Dense(input_channels)(x)\n    x = hard_swish(x)\n\n    x = layers.Reshape((1, input_channels))(x)\n    #print(x)\n    #print(inputs)\n    x = layers.Multiply()([inputs, x])\n    return x\n\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n#注意力机制：Sigmoid方式\n# def squeeze(inputs):\n#     input_channels = int(inputs.shape[-1])\n    \n#     x = layers.GlobalAveragePooling1D()(inputs)\n\n#     x = layers.Dense(input_channels//4)(x)\n#     x = layers.Activation('relu')(x)\n\n#     x = layers.Dense(input_channels)(x)\n#     x = layers.Activation('softmax')(x)\n\n#     x = layers.Reshape((1, input_channels))(x)\n#     #print(x)\n#     #print(inputs)\n#     x = layers.Multiply()([inputs, x])\n#     return x\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:45.752664Z","iopub.execute_input":"2021-05-21T05:25:45.753078Z","iopub.status.idle":"2021-05-21T05:25:45.763839Z","shell.execute_reply.started":"2021-05-21T05:25:45.753045Z","shell.execute_reply":"2021-05-21T05:25:45.76191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"下采样\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n#基础层—3x3_3x3—下采样——改变卷积层步长的方式\n#ps:目前试验结果中最好的下采样方式\ndef DownSampling(inputs,filters,stride_padding='same',Drop=False):\n    layer = layers.Conv1D(filters,3,padding='same')(inputs)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    \n    res_layer = layers.Conv1D(filters,1,padding='same')(inputs)\n    res_layer = layers.BatchNormalization()(res_layer)\n    layer = layers.add([layer,res_layer])\n    \n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    if Drop==True:\n        layer = layers.Dropout(0.25)(layer)\n    \n    pool_inputs = layers.Conv1D(filters,3,padding=stride_padding,strides=2)(layer)\n    pool = layers.BatchNormalization()(pool_inputs)\n    pool = layers.Activation(tf.nn.relu)(pool)\n\n    pool = layers.Conv1D(filters,3,padding='same')(pool)\n    pool = layers.BatchNormalization()(pool)\n    \n    res_pool = layers.Conv1D(filters,1,padding='same')(pool_inputs)\n    res_pool = layers.BatchNormalization()(res_pool)\n    pool = layers.add([pool,res_pool])\n    \n    pool = layers.Activation(tf.nn.relu)(pool)\n    \n    \n    return layer,pool\n# ---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n#基础层—3x3_3x3—下采样——池化的方式\n# def DownSampling(inputs,filters,stride_padding='same',Drop=False):\n#     layer = layers.Conv1D(filters,3,padding='same')(inputs)\n#     layer = layers.BatchNormalization()(layer)\n#     layer = layers.Activation(tf.nn.relu)(layer)\n\n#     layer = layers.Conv1D(filters,3,padding='same')(layer)\n#     layer = layers.BatchNormalization()(layer)\n    \n#     res_layer = layers.Conv1D(filters,1,padding='same')(inputs)\n#     res_layer = layers.BatchNormalization()(res_layer)\n#     layer = layers.add([layer,res_layer])\n    \n#     layer_inputs = layers.Activation(tf.nn.relu)(layer)\n\n#     layer = layers.Conv1D(filters,3,padding=stride_padding,strides=1)(layer_inputs)\n#     layer = layers.BatchNormalization()(layer)\n#     layer = layers.Activation(tf.nn.relu)(layer)\n\n#     layer = layers.Conv1D(filters,3,padding='same')(layer)\n#     layer = layers.BatchNormalization()(layer)\n    \n#     res_layer = layers.Conv1D(filters,1,padding='same')(layer_inputs)\n#     res_layer = layers.BatchNormalization()(res_layer)\n#     layer = layers.add([layer,res_layer])\n#     layer = layers.Activation(tf.nn.relu)(layer)\n    \n#     if Drop==True:\n#         layer = layers.Dropout(0.25)(layer)\n    \n#     #平均池化\n#     pool = layers.AveragePooling1D(pool_size=2)(layer)\n#     #最大池化\n#     #pool = layers.MaxPooling1D(pool_size=2)(layer)\n#     return layer,pool\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n# 每层单个残差块\n# def DownSampling(inputs,filters,stride_padding='same',Drop=False):\n#     layer = layers.Conv1D(filters,3,padding=stride_padding)(inputs)\n#     layer = layers.BatchNormalization()(layer)\n#     layer = layers.Activation(tf.nn.relu)(layer)\n\n#     layer = layers.Conv1D(filters,3,padding='same')(layer)\n#     layer = layers.BatchNormalization()(layer)\n    \n#     res_layer = layers.Conv1D(filters,1,padding='same')(inputs)\n#     res_layer = layers.BatchNormalization()(res_layer)\n#     layer = layers.add([layer,res_layer])\n    \n#     layer = layers.Activation(tf.nn.relu)(layer)\n    \n#     if Drop==True:\n#         layer = layers.Dropout(0.25)(layer)\n    \n#     #平均池化\n#     pool = layers.AveragePooling1D(pool_size=2)(layer)\n#     #最大池化\n#     #pool = layers.MaxPooling1D(pool_size=2)(layer)\n#     return layer,pool\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:45.982885Z","iopub.execute_input":"2021-05-21T05:25:45.983409Z","iopub.status.idle":"2021-05-21T05:25:45.997769Z","shell.execute_reply.started":"2021-05-21T05:25:45.983356Z","shell.execute_reply":"2021-05-21T05:25:45.99649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"上采样\"\"\"\n#基础层上采样\ndef UpSampling(inputs,con_input,filters,need_zero=False):\n    \n    \n    up_layer = layers.UpSampling1D(size=2)(inputs)\n    \n    if need_zero==True:\n        up_layer = layers.ZeroPadding1D((0,1))(up_layer)\n    \n    layer = layers.Conv1D(filters,2,padding='same')(up_layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.Concatenate(axis=2)([layer,con_input])\n    \n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    output = layers.Activation(tf.nn.relu)(layer)\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2021-05-21T05:25:46.511503Z","iopub.execute_input":"2021-05-21T05:25:46.512049Z","iopub.status.idle":"2021-05-21T05:25:46.521947Z","shell.execute_reply.started":"2021-05-21T05:25:46.512009Z","shell.execute_reply":"2021-05-21T05:25:46.521051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"上采样结束的卷积操作\"\"\"\ndef FinalConv1D(inputs,filters):\n    layer = layers.Conv1D(filters,3,padding='same')(inputs)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.MaxPooling1D(pool_size=2)(layer)\n    \n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.MaxPooling1D(pool_size=2)(layer)\n    return layer","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:14:46.595717Z","iopub.execute_input":"2021-05-21T06:14:46.596315Z","iopub.status.idle":"2021-05-21T06:14:46.603608Z","shell.execute_reply.started":"2021-05-21T06:14:46.596265Z","shell.execute_reply":"2021-05-21T06:14:46.602795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_model():\n#     #双输入模型\n#     inputs_1 = keras.Input(shape=(600,1),name='inputs_1')\n#     inputs_dev1 = keras.Input(shape=(600,1),name='inputs_dev1')\n#     inputs_dev2 = keras.Input(shape=(600,1),name='inputs_dev2')\n    \n#     inputs = layers.Concatenate(axis=2)([inputs_1,inputs_dev1,inputs_dev2])\n    \n#     #下采样\n#     layer1,pool1 = DownSampling(inputs,filters=64)\n    \n#     layer2,pool2 = DownSampling(pool1,filters=128)\n        \n#     layer3,pool3 = DownSampling(pool2,filters=256)\n    \n#     layer4,pool4 = DownSampling(pool3,filters=512,stride_padding='valid',Drop=False)\n    \n#     #最后一层不需要Pooling\n#     layer51,pool51 = DownSampling(pool4,filters=1024,Drop=False)\n#     layer52,pool52 = DownSampling(pool4,filters=1024,Drop=False)\n#     layer53,pool53 = DownSampling(pool4,filters=1024,Drop=False)\n    \n#     layer51 = squeeze(layer51)\n#     layer52 = squeeze(layer52)\n#     layer53 = squeeze(layer53)\n    \n#     layer51 = layers.BatchNormalization()(layer51)\n#     layer52 = layers.BatchNormalization()(layer52)\n#     layer53 = layers.BatchNormalization()(layer53)\n#     layer5 = layers.add([layer51,layer52])\n#     layer5 = layers.add([layer5,layer53])\n    \n#     #layer5 = layers.BatchNormalization()(layer5)\n    \n#     #上采样\n    \n#     #attention\n#     layer4 = squeeze(layer4)\n    \n#     layer44 = UpSampling(layer5,layer4,512,need_zero=True)\n    \n#     #attention\n#     layer3 = squeeze(layer3)\n                     \n#     layer33 = UpSampling(layer44,layer3,256)\n    \n#     #attention\n#     layer2 = squeeze(layer2)\n    \n#     layer22 = UpSampling(layer33,layer2,128)\n    \n#     #attention\n#     layer1_1 = squeeze(layer1)\n    \n#     layer1_2 = squeeze(layer1)\n    \n#     layer1_3 = squeeze(layer1)\n    \n#     layer111 = UpSampling(layer22,layer1_1,64)\n    \n#     layer112 = UpSampling(layer22,layer1_2,64)\n    \n#     layer113 = UpSampling(layer22,layer1_3,64)\n    \n#     layer111 = FinalConv1D(layer111,filters=64)\n#     layer112 = FinalConv1D(layer112,filters=64)\n#     layer113 = FinalConv1D(layer113,filters=64)\n    \n#     layer1111 = squeeze(layer111)\n    \n#     layer1112 = squeeze(layer112)\n    \n#     layer1113 = squeeze(layer113)\n    \n    \n#     layer1111 = layers.BatchNormalization()(layer1111)\n#     layer1112 = layers.BatchNormalization()(layer1112)\n#     layer1113 = layers.BatchNormalization()(layer1113)\n#     layer1111 = layers.add([layer1111,layer1112])\n#     layer11 = layers.add([layer1111,layer1113])\n    \n#     layer = layers.GlobalAveragePooling1D()(layer11)\n\n#     outputs_sbp = layers.Dense(1,name='Sbp')(layer)\n#     outputs_dbp = layers.Dense(1,name='Dbp')(layer)\n\n#     model = keras.Model(inputs=[inputs_1,inputs_dev1,inputs_dev2],outputs=[outputs_sbp,outputs_dbp])\n    \n#     return model","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:14:46.778371Z","iopub.execute_input":"2021-05-21T06:14:46.778935Z","iopub.status.idle":"2021-05-21T06:14:46.799202Z","shell.execute_reply.started":"2021-05-21T06:14:46.778897Z","shell.execute_reply":"2021-05-21T06:14:46.797364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #ARU-Net原版\n# def create_model():\n#     #双输入模型\n#     inputs_1 = keras.Input(shape=(600,1),name='inputs_1')\n#     inputs_dev1 = keras.Input(shape=(600,1),name='inputs_dev1')\n#     inputs_dev2 = keras.Input(shape=(600,1),name='inputs_dev2')\n    \n#     inputs = layers.Concatenate(axis=2)([inputs_1,inputs_dev1,inputs_dev2])\n    \n#     #下采样\n#     layer1,pool1 = DownSampling(inputs,filters=64)\n    \n#     layer2,pool2 = DownSampling(pool1,filters=128)\n        \n#     layer3,pool3 = DownSampling(pool2,filters=256)\n    \n#     layer4,pool4 = DownSampling(pool3,filters=512,stride_padding='valid',Drop=False)\n    \n#     #最后一层不需要Pooling\n#     layer5,pool5 = DownSampling(pool4,filters=1024,Drop=False)\n    \n#     #上采样\n    \n#     #attention\n#     layer4 = squeeze(layer4)\n    \n#     layer44 = UpSampling(layer5,layer4,512,need_zero=True)\n    \n#     #attention\n#     layer3 = squeeze(layer3)\n                     \n#     layer33 = UpSampling(layer44,layer3,256)\n    \n#     #attention\n#     layer2 = squeeze(layer2)\n    \n#     layer22 = UpSampling(layer33,layer2,128)\n    \n#     #attention\n#     layer1 = squeeze(layer1)\n    \n#     layer11 = UpSampling(layer22,layer1,64)\n    \n    \n#     layer = layers.GlobalAveragePooling1D()(layer11)\n\n#     outputs_sbp = layers.Dense(1,name='Sbp')(layer)\n#     outputs_dbp = layers.Dense(1,name='Dbp')(layer)\n\n#     model = keras.Model(inputs=[inputs_1,inputs_dev1,inputs_dev2],outputs=[outputs_sbp,outputs_dbp])\n    \n#     return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#W-Net\ndef create_model():\n    #双输入模型\n    inputs_1 = keras.Input(shape=(600,1),name='inputs_1')\n    inputs_dev1 = keras.Input(shape=(600,1),name='inputs_dev1')\n    inputs_dev2 = keras.Input(shape=(600,1),name='inputs_dev2')\n    \n    inputs = layers.Concatenate(axis=2)([inputs_1,inputs_dev1,inputs_dev2])\n    #11111111111111111111111111111111111111111111111\n    #下采样\n    layer_1_1,pool_1_1 = DownSampling(inputs,filters=64)\n    \n    layer_1_2,pool_1_2 = DownSampling(pool_1_1,filters=128)\n        \n    layer_1_3,pool_1_3 = DownSampling(pool_1_2,filters=256)\n    \n    layer_1_4,pool_1_4 = DownSampling(pool_1_3,filters=512,stride_padding='valid',Drop=False)\n    \n    #最后一层不需要Pooling\n    layer_1_5,pool_1_5 = DownSampling(pool_1_4,filters=1024,Drop=False)\n    \n    #上采样\n    \n    #attention\n    layer_1_4 = squeeze(layer_1_4)\n    \n    layer_1_44 = UpSampling(layer_1_5,layer_1_4,512,need_zero=True)\n    \n    #attention\n    layer_1_3 = squeeze(layer_1_3)\n                     \n    layer_1_33 = UpSampling(layer_1_44,layer_1_3,256)\n    \n    #attention\n    layer_1_2 = squeeze(layer_1_2)\n    \n    layer_1_22 = UpSampling(layer_1_33,layer_1_2,128)\n    \n    #attention\n    layer_1_1 = squeeze(layer_1_1)\n    \n    layer_1_11 = UpSampling(layer_1_22,layer_1_1,64)\n    \n    layer_dbp = layers.GlobalAveragePooling1D()(layer_1_11)\n    \n    \n    #222222222222222222222222222222222222222222222\n     #下采样\n    layer_2_1,pool_2_1 = DownSampling(layer_1_11,filters=64)\n    \n    layer_2_2,pool_2_2 = DownSampling(pool_2_1,filters=128)\n        \n    layer_2_3,pool_2_3 = DownSampling(pool_2_2,filters=256)\n    \n    layer_2_4,pool_2_4 = DownSampling(pool_2_3,filters=512,stride_padding='valid',Drop=False)\n    \n    #最后一层不需要Pooling\n    layer_2_5,pool_2_5 = DownSampling(pool_2_4,filters=1024,Drop=False)\n    \n    #上采样\n    \n    #attention\n    layer_2_4 = squeeze(layer_2_4)\n    \n    layer_2_44 = UpSampling(layer_2_5,layer_2_4,512,need_zero=True)\n    \n    #attention\n    layer_2_3 = squeeze(layer_2_3)\n                     \n    layer_2_33 = UpSampling(layer_2_44,layer_2_3,256)\n    \n    #attention\n    layer_2_2 = squeeze(layer_2_2)\n    \n    layer_2_22 = UpSampling(layer_2_33,layer_2_2,128)\n    \n    #attention\n    layer_2_1 = squeeze(layer_2_1)\n    \n    layer_2_11 = UpSampling(layer_2_22,layer_2_1,64)\n    \n    \n    \n    \n    \n    \n    layer_sbp = layers.GlobalAveragePooling1D()(layer_2_11)\n\n    outputs_sbp = layers.Dense(1,name='Sbp')(layer_sbp)\n    outputs_dbp = layers.Dense(1,name='Dbp')(layer_dbp)\n\n    model = keras.Model(inputs=[inputs_1,inputs_dev1,inputs_dev2],outputs=[outputs_sbp,outputs_dbp])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:31.424344Z","iopub.execute_input":"2021-05-21T08:13:31.424981Z","iopub.status.idle":"2021-05-21T08:13:31.443824Z","shell.execute_reply.started":"2021-05-21T08:13:31.424941Z","shell.execute_reply":"2021-05-21T08:13:31.442716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"自定义评价指标模块\"\"\"\ndef standard_deviation(y_true, y_pred):\n    u = keras.backend.mean(y_pred-y_true)\n    return keras.backend.sqrt(keras.backend.mean(keras.backend.square((y_pred-y_true) - u)))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:31.669439Z","iopub.execute_input":"2021-05-21T08:13:31.669852Z","iopub.status.idle":"2021-05-21T08:13:31.67785Z","shell.execute_reply.started":"2021-05-21T08:13:31.669815Z","shell.execute_reply":"2021-05-21T08:13:31.676761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"回调函数\"\"\"\n#保存迭代周期内最好的模型\ncheckpoint_filepath = r'./model_struction.h5'\nSave_epochs = 100 #迭代多少层保存一次模型\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    # save_weights_only=True,\n    monitor='val_Sbp_mean_absolute_error',\n    mode='min',\n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:31.839669Z","iopub.execute_input":"2021-05-21T08:13:31.840067Z","iopub.status.idle":"2021-05-21T08:13:31.846Z","shell.execute_reply.started":"2021-05-21T08:13:31.840029Z","shell.execute_reply":"2021-05-21T08:13:31.844773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"建立模型\"\"\"\nwith strategy.scope():\n    model = create_model()\n    \n    model.compile(loss={'Sbp':\"mse\",'Dbp':\"mse\"}, optimizer=keras.optimizers.Adam(lr=0.0001),metrics=[tf.keras.metrics.MeanAbsoluteError(),standard_deviation])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:32.115651Z","iopub.execute_input":"2021-05-21T08:13:32.116057Z","iopub.status.idle":"2021-05-21T08:13:42.848121Z","shell.execute_reply.started":"2021-05-21T08:13:32.116019Z","shell.execute_reply":"2021-05-21T08:13:42.846802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:42.849706Z","iopub.execute_input":"2021-05-21T08:13:42.850017Z","iopub.status.idle":"2021-05-21T08:13:43.009025Z","shell.execute_reply.started":"2021-05-21T08:13:42.849987Z","shell.execute_reply":"2021-05-21T08:13:43.007764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"保存模型结构图片\"\"\"\ntf.keras.utils.plot_model(model, to_file=r'./model_graph.png', show_shapes=True, show_layer_names=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:43.011576Z","iopub.execute_input":"2021-05-21T08:13:43.011918Z","iopub.status.idle":"2021-05-21T08:13:49.469684Z","shell.execute_reply.started":"2021-05-21T08:13:43.011885Z","shell.execute_reply":"2021-05-21T08:13:49.468657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit({'inputs_1':train_data,'inputs_dev1':train_dev1,'inputs_dev2':train_dev2},{'Sbp':train_label_Sbp,'Dbp':train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=({'inputs_1':validation_data,'inputs_dev1':validation_dev1,'inputs_dev2':validation_dev2},{'Sbp':validation_label_Sbp,'Dbp':validation_label_Dbp})\n                    )","metadata":{"execution":{"iopub.status.busy":"2021-05-21T08:13:49.471428Z","iopub.execute_input":"2021-05-21T08:13:49.471972Z","iopub.status.idle":"2021-05-21T10:42:22.369635Z","shell.execute_reply.started":"2021-05-21T08:13:49.471922Z","shell.execute_reply":"2021-05-21T10:42:22.367333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r'./last_model.h5.pickle', 'wb') as file_pi:\n \tpickle.dump(history.history, file_pi)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:42:22.372021Z","iopub.execute_input":"2021-05-21T10:42:22.372395Z","iopub.status.idle":"2021-05-21T10:42:22.396033Z","shell.execute_reply.started":"2021-05-21T10:42:22.37236Z","shell.execute_reply":"2021-05-21T10:42:22.394689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit({'inputs_1':train_data,'inputs_dev1':train_dev1,'inputs_dev2':train_dev2},{'Sbp':train_label_Sbp,'Dbp':train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=({'inputs_1':validation_data,'inputs_dev1':validation_dev1,'inputs_dev2':validation_dev2},{'Sbp':validation_label_Sbp,'Dbp':validation_label_Dbp})\n                    )","metadata":{"execution":{"iopub.status.busy":"2021-05-21T10:42:22.397799Z","iopub.execute_input":"2021-05-21T10:42:22.398119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit({'inputs_1':train_data,'inputs_dev1':train_dev1,'inputs_dev2':train_dev2},{'Sbp':train_label_Sbp,'Dbp':train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=({'inputs_1':validation_data,'inputs_dev1':validation_dev1,'inputs_dev2':validation_dev2},{'Sbp':validation_label_Sbp,'Dbp':validation_label_Dbp})\n                    )","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:13:40.577056Z","iopub.execute_input":"2021-05-20T13:13:40.577389Z","iopub.status.idle":"2021-05-20T14:51:40.080007Z","shell.execute_reply.started":"2021-05-20T13:13:40.577354Z","shell.execute_reply":"2021-05-20T14:51:40.078698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}