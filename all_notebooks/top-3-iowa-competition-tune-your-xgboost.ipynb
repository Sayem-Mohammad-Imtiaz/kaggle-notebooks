{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/GNiaWNs.png\">\n<center><h1>Predict House Prices in Iowa</h1></center>\n<center><h2>CORGI Workshop</h2></center>\n\n# Introduction\n\n### What is a Regression Problem?\n\nA Regression problem is a **supervised machine learning technique**, which has the aim of predicting a *real* or *continuous* variable (e.g.: salary, height, house prices üòâ, etc.) by looking at other *helper* variables (which can be also called *features*).\n\nHence, we're trying to:\n* predict a TARGET\n* by looking at FEATURES\n\n<img src=\"https://i.imgur.com/eXHXYdZ.png\" width = 600>\n\n\n### Predicting House Prices in Iowa (Kaggle Competition) üè°\n\nThis competition aims to predict the Sales Price of Houses in Iowa, by looking at various aspects of the house or area, such as *number of rooms, number of floors, garage, the street, pool, utilities etc.*\n\nThere are 2 Chapters that we'll cover to achieve our goal:\n1. **Data Preprocessing**\n2. **Data Modeling**\n\n\n# 1. Data Preprocessing üõ†\n\nIn this Chpater we'll learn how to do the following:\n\n1. Import and analyse the data\n2. Clean the data from missing values\n3. Encode categorical variables\n\n### Libraries üìö"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Data Libraries\nimport pandas as pd\nimport numpy as np\n\n# Import Visualization Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Importing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the datasets\ntrain = pd.read_csv(\"../input/iowa-house-prices/train.csv\")\ntest = pd.read_csv(\"../input/iowa-house-prices/test.csv\")\n\n# Drop ID columns\ntrain.drop(columns=\"Id\", axis=1, inplace=True)\ntest.drop(columns=\"Id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 2 datasets that we need to import:\n* `train` : it's the TRAINING data. The model *learns* by looking at it and understanding patterns through different algorithms. It is **labeled** (meaning that we have the houses *price* column in there).\n* `test` : it's the data for TESTING. Once we create our model and we know are working, we can test it on a NEW completely unseen dataset, to see if it can properly generalise the information learned during testing.\n\n> Splitting the data into TRAIN and TEST ensures that the model is **robust**, meaning that it is able to **generalise**. \n\n<img src=\"https://i.imgur.com/WrMWQT3.png\" width=600>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore the format\nprint(\"Train shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))\n\n# Explore the head of the dataframe\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Other helpful commands\n# train.dtypes          # check the datatypes of the columns\n# train.columns         # check name of columns\n# train.shape           # check the shape of the columns\n# train.isna().sum()    # check how much missing data in each column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Check for Leakages üöø\n\nA **leakage** is when you are using *features* that you wouldn't have available if a **new case** comes up to train your model.\n\nImagine trying to predict the price you will sell your house with, but your model all of the sudden expects you to input the *day you sold the house*, or if the transaction was *cash or through a bank order*. You can't possibly know these aspects, as you **haven't sold the house yet**.\n\nHence, we need to look for leakages in our data before proceeding with the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Leaked columns\nleaked_columns = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n\n# Remove in BOTH datasets\ntrain.drop(labels = leaked_columns, axis = 1, inplace = True)\ntest.drop(labels = leaked_columns, axis = 1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Check if the columns have dissapeared\n# train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Data Preprocessing üßπ\n\nThere are many techniques that can be performed during this phase. Some of them are:\n* checking for **missing** data\n* analysis of the distributions and **patterns** in the data\n* creating **new features** from the existing data (feature engineering)\n* **encoding** the categorical features\n* etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the data types of the columns\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #I. Looking at the Target column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing values in the target column\ntrain[\"SalePrice\"].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of the target column\nplt.figure(figsize = (16, 4))\nsns.distplot(a = train[\"SalePrice\"], color = \"#FF7F50\")\nplt.title(\"Distribution of Sales Price\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing the target variable separately\ny = train[\"SalePrice\"]\ntrain.drop(columns=[\"SalePrice\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #II. Numerical Data\n\n> Numerical columns are the ones that are of type `int` or `float`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select ONLY numerical columns\nnumerical_cols = [col for col in train.columns if \n                  train[col].dtype in [\"int64\", \"float64\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if there are any missing values between these columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the columns that have null values\nna_count_n = train[numerical_cols].isna().sum()   # this becomes a Pandas Series\nna_count_n[na_count_n > 0]                          # filter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How should we make the imputation?**\n\n> **Imputation** is a technique that replaces missing values in the data with another value (like the mean, median, mode, or other more complex operations). *Be weary of the bias*!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select columns with NAs\nna_numeric_columns = na_count_n[na_count_n > 0].index\nna_numeric_data = train[na_numeric_columns].dropna(axis=0)\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Plot the distribution of these variables\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nplt.figure(figsize = (16, 4))\nsns.distplot(a = na_numeric_data[na_numeric_columns[0]], color = \"blue\")\nplt.title(f\"{na_numeric_columns[0]}\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 4))\nsns.distplot(a = na_numeric_data[na_numeric_columns[1]], color = \"purple\")\nplt.title(f\"{na_numeric_columns[1]}\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 4))\nsns.distplot(a = na_numeric_data[na_numeric_columns[2]], color = \"orange\")\nplt.title(f\"{na_numeric_columns[2]}\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Introducing \"SimpleImputer\"ü•Å"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Simple Imputer and from SkLearn\nfrom sklearn.impute import SimpleImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the Imputation Objects\nmedian_impute = SimpleImputer(strategy = 'median')\nmode_impute = SimpleImputer(strategy = 'most_frequent')\nmean_impute = SimpleImputer(strategy = 'mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_imputation(impute_object, column):\n    '''Function that applies the imputation to the desired column.\n    Returns the values for train and test.'''\n\n    ### attention at the difference between fit_transform and transform!\n    imputed_train = impute_object.fit_transform(X = train[[column]])\n    imputed_test = impute_object.transform(X = test[[column]])\n    \n    return imputed_train, imputed_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the Imputation\ntrain['LotFrontage'] = apply_imputation(median_impute, 'LotFrontage')[0]\ntest['LotFrontage'] = apply_imputation(median_impute, 'LotFrontage')[1]\n\ntrain['MasVnrArea'] = apply_imputation(mode_impute, 'MasVnrArea')[0]\ntest['MasVnrArea'] = apply_imputation(mode_impute, 'MasVnrArea')[1]\n\ntrain['GarageYrBlt'] = apply_imputation(mean_impute, 'GarageYrBlt')[0]\ntest['GarageYrBlt'] = apply_imputation(mean_impute, 'GarageYrBlt')[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scale the Data\n\nYou'll also need to be scaling the data, so that all the variables are normalised. It can also speed up the model computations.\n\n#### Introducing \"StandardScaler\"ü•Å"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Import the Standard Scaler\n# from sklearn.preprocessing import StandardScaler\n\n# # Scale the data\n# scaler = StandardScaler()\n# scaled_matrix_train = pd.DataFrame(scaler.fit_transform(train[numerical_cols]),\n#                                    columns=numerical_cols)\n# scaled_matrix_test = pd.DataFrame(scaler.transform(test[numerical_cols]),\n#                                    columns=numerical_cols)\n\n# # Erase old data and append scaled one\n# train.drop(columns=numerical_cols, axis=1, inplace=True)\n# test.drop(columns=numerical_cols, axis=1, inplace=True)\n\n# train = pd.concat([train, scaled_matrix_train], axis=1)\n# test = pd.concat([test, scaled_matrix_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #III. Categorical Data\n\n> Categorical data is the one stored in object datatypes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select ONLY categorical columns\ncateg_cols = [col for col in train.columns if \n              train[col].dtype in [\"object\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if there are any missing values between these columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the columns that have null values\nna_count_s = train[categ_cols].isna().sum()     # this becomes a Pandas Series\nna_count_s[na_count_s > 0]                        # filter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop all columns with more than 50% missing data\nto_drop = na_count_s[na_count_s > train.shape[0]*0.5].index\n\ntrain.drop(labels=to_drop, axis=1, inplace=True)\ntest.drop(labels=to_drop, axis=1, inplace=True)\n\n# Update the categ_cols\ncateg_cols = list(set(categ_cols) - set(to_drop))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all we have to do is **impute** the rest of the data and **encode** it (transform it from data type `object` to data type `int`)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The computer doesn't know how to read letters :)\ntrain[categ_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Introducing \"One Hot Encoder\" ü•Å\n\nThere are different types of encoding methodologies:\n* **Label Encoding**: when you convert the categories (e.g.: day, night, noon) into numbers/labels (e.g.: 1, 2, 3).\n* **One Hot Encoding**: when you create a *flag* for each category, with 1 where that category appears and 0 otherwise.\n\n<img src=\"https://i.imgur.com/3725Gwc.png\" width=600>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import OneHotEncoder from sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the imputation object\nc_mode_impute = SimpleImputer(strategy = 'most_frequent')\n# Create the encoder object\nencoder = OneHotEncoder(handle_unknown='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the Imputation\nfor column in categ_cols:\n    train[column] = apply_imputation(c_mode_impute, column)[0]\n    test[column] = apply_imputation(c_mode_impute, column)[1]\n    \n\n# Perform One Hot Encoding\nencoded_train = pd.DataFrame(encoder.fit_transform(train[categ_cols]).toarray())\nencoded_test = pd.DataFrame(encoder.transform(test[categ_cols]).toarray())\n\n# Drop old columns and replace with encoded ones\ntrain.drop(columns=categ_cols, axis=1, inplace=True)\ntest.drop(columns=categ_cols, axis=1, inplace=True)\n\ntrain = pd.concat([train, encoded_train], axis=1)\ntest = pd.concat([test, encoded_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Model Training üíª‚è∞\n\nIn this Chapter we'll learn how to do the following:\n\n1. **Prepare** the data to properly feed to the model\n2. Create multiple **models** and assess which one is the best\n\n### Libraries üìö"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Splitting\nfrom sklearn.model_selection import train_test_split\n\n# Models (or Algorithms)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# Model Evaluation\nfrom sklearn.metrics import mean_absolute_error\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Data Validation\n\nThis step is extremely important. It ensures that during the training session you have some sort of **indicator on how your model is performing**.\n\nIt also takes care of model **overfitting**: when a model overfits it means that the model isn't learning patterns and generalities from the data. Fitting the points too well might lead to a very high accuracy of the model during training, but a very low score when you actually deploy it into production:\n\n<img src=\"https://i.imgur.com/7632QAP.png\" width=600>\n\nOne solution to this is to *split* the training data into a **training** part and **validation** part.\n\nHence, you can **train** the model on the Training Data and then **predict** on the Validation Data. This way, you can use you labeled data not only for training, but also for validating how your model performs.\n\n<img src=\"https://miro.medium.com/max/1552/1*Nv2NNALuokZEcV6hYEHdGA.png\" width=600>\n\nThere are many more other options to this technique, such as K Fold or Stratified K Fold, but we won't get into them in this notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train -> training data\n# test -> testing data (unlabeled)\n# y -> target variable (we stored it from the train data)\n\n# Target Variable: y\n# Features -> X\nX = train\n\n# Split data further\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, \n                                                      random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Model Selection: trial and error\n\nNow we start to try 1 by 1 the 4 algorithms we imported earlier. Remember, there are many more other algorithms that you can try for this problem. Head to the [sklearn documentation](https://scikit-learn.org/stable/) to find more.\n\n### #I. Linear Model\n\n<img src=\"https://backlog.com/wp-blog-app/uploads/2019/12/Nulab-Gradient-descent-for-linear-regression-using-Golang-Blog.png\" width=300>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ~~~~~~~~~~~~~~~~\n# LinearRegression\n# ~~~~~~~~~~~~~~~~\n\nlinear_model = LinearRegression()\n\n# Train the model on training data\nlinear_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = linear_model.predict(X_valid)\n\n# Get how well it performed\nmae_linear = mean_absolute_error(y_valid, predictions)\n\nprint(\"Linear: {:,}\".format(mae_linear))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linear model looks like it's **underfitting** big time.\n\n### #II. Decission Tree Regressor\n\n<img src=\"https://miro.medium.com/max/2000/1*WerHJ14JQAd3j8ASaVjAhw.jpeg\" width=300>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ~~~~~~~~~~~~~~~~~~~~~\n# DecisionTreeRegressor\n# ~~~~~~~~~~~~~~~~~~~~~\n\ntree_model = DecisionTreeRegressor()\n\n# Train the model on training data\ntree_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = tree_model.predict(X_valid)\n\n# Get how well it performed\nmae_tree = mean_absolute_error(y_valid, predictions)\n\nprint(\"Tree: {:,}\".format(mae_tree))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Decission Tree is MUCH better, with an error of only 24,823.\n\n### #III. Random Forest Regressor\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png\" width=300>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ~~~~~~~~~~~~~~~~~~~~~\n# RandomForestRegressor\n# ~~~~~~~~~~~~~~~~~~~~~\n\nrf_model = RandomForestRegressor()\n\n# Train the model on training data\nrf_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = rf_model.predict(X_valid)\n\n# Get how well it performed\nmae_rf = mean_absolute_error(y_valid, predictions)\n\nprint(\"Random Forest: {:,}\".format(mae_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, a very nice improvement. The Random Forest is performing better than the Decission Tree.\n\n### #IV. XGBoost\n\n> Still an ensemble, but more complicated. You can find [documentation here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ~~~~~~~~~~~~\n# XGBRegressor\n# ~~~~~~~~~~~~\n\nxgb_model = XGBRegressor(n_estimators=600)\n\n# Train the model on training data\nxgb_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = xgb_model.predict(X_valid)\n\n# Get how well it performed\nmae_xgb = mean_absolute_error(y_valid, predictions)\n\nprint(\"XGBoost: {:,}\".format(mae_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. There's more\n\nThere's soooooo much more to this. I'll leave here some **names** of techniques or agorithms that you might want to check out in order to further improve this score:\n\n* Feature Engineering ([check out this notebook](https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial))\n* Fancy Impute (or any other Imputation technique)\n* PCA before model training\n* K Fold or Stratified K Fold\n* Model Selection (try other models)\n* Hyperparameter Tunning\n* many many many more, you just need to be curious üôÉ\n\n# 4. Submit to Competition\n\nOnce you're ready, you can use your model to predict on the `test` dataset and submit to the [Iowa Housing Prices Competition](https://www.kaggle.com/c/home-data-for-ml-course).\n\nIf you want to learn mode on Machine Learning or just take a deep dive into this tutorial, check out the [Kaggle Courses](https://www.kaggle.com/learn/overview) in the Machine Learning Series:\n* Intro to ML\n* Intermediate ML\n* ML Explainability"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This model is created after Grid Search\n### check out version 4 of this notebook to see how I did it :)\nmy_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                        colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n                        importance_type='gain', learning_rate=0.005, \n                        max_delta_step=0, max_depth=4, min_child_weight=1, \n                        missing=None, n_estimators=5000, n_jobs=1, \n                        nthread=4, objective='reg:squarederror', random_state=0,\n                        reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=27,\n                        silent=None, subsample=0.8, verbosity=1)\n\n# Fit on the entire training data\nmy_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict and Submit\nfinal_predictions = my_model.predict(test)\n\n# Import test to get ID\nX_test = pd.read_csv(\"../input/iowa-house-prices/test.csv\", index_col ='Id')\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice' : final_predictions})\noutput.to_csv('submission_final.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Happy Data Sciencin'!**\n\n<img src=\"https://i.imgur.com/cUQXtS7.png\">\n\n# Specs on how I prepped & trained ‚å®Ô∏èüé®\n### (*locally*)\n* Z8 G4 Workstation üñ•\n* 2 CPUs & 96GB Memory üíæ\n* NVIDIA Quadro RTX 8000 üéÆ\n* RAPIDS version 0.17 üèÉüèæ‚Äç‚ôÄÔ∏è"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}