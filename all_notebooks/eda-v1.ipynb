{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_df = pd.read_json('/kaggle/input/github-bugs-prediction/embold_train.json')\ntrain_df[\"text\"] = train_df.title + \" \" + train_df.body\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attribute Description:\n\n    -Title - the title of the GitHub bug, feature question\n    -Body - the body of the GitHub bug, feature question\n    -Label - Represents various classes of Labels\n        Bug - 0\n        Feature - 1\n        Question - 2\n        \n    - Text - we combined Title and Body to have whole text feature"},{"metadata":{},"cell_type":"markdown","source":"### Data quick glance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'We have {train_df.shape[0]} rows and {train_df.shape[-1]} columns')\nprint(f'\\n')\nprint(f'Remember we combined Title and Body to create new column \"Text\"')\nprint(f'columns {train_df.columns}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes  # check data types ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encode(data, from_numeric= True):\n    \n    '''wrappen function to label to numberic code and vice versa'''\n    \n    if from_numeric:\n        if data== 0:\n            return 'Bug'\n        elif data == 1:\n            return 'Feature'\n        elif data == 2:\n            return 'Question'\n        \n    else:\n        if data== 'Bug':\n            return 0\n        elif data == 'Feature':\n            return 1\n        elif data == 'Question':\n            return 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets convert 'label' to its classification label for better visualization \ntrain_df['label'] = train_df.label.apply(label_encode)\n\ntrain_df.label = train_df.label.astype('category')  # convert in category data types\n\ntrain_df.label.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statistical Analysis-I\nOkay lets do some basic descriptive statitical insights on raw training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.label.value_counts()  # .plot(kind= \"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='label',data=train_df)\n            \nprint(f'There are too many request for bug  and feature, less on Questioin \\nClearly our class is imbalanced')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets see how character and words play role in different class"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text_len'] = train_df['text'].astype(str).apply(len)\ntrain_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,5],frameon=True)\n\nplt.subplot(1,2,1)\norder_index = train_df.word_count.value_counts().index\nsns.distplot(train_df.word_count,kde = False)\nplt.title('Overall number of words used')\n\nplt.subplot(1,2,2)\n\norder_index = train_df.text_len.value_counts().index\nsns.distplot(train_df.text_len,kde = False)\nplt.title('Overall number characters used')\n\n\n# plt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Majority used words less than 200\n- Number of characters used are fairly in a range\n\nbelow plotly version of above visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.offline import iplot\nimport seaborn as sns\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='Text  length',\n    linecolor='black',\n    yTitle='count',\n    title='Text Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='word count',\n    linecolor='black',\n    yTitle='count',\n    title='word count Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So how they (words used , characater used) do across category ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"label\", y=\"text_len\", data= train_df)\n\ng = sns.FacetGrid(train_df, col=\"label\")\ng.map(sns.distplot, \"text_len\",kde = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"label\", y=\"word_count\", data= train_df)\n\ng = sns.FacetGrid(train_df, col=\"label\")\ng.map(sns.distplot, \"word_count\",kde = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Punctuation used in across class ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count_Bug_punctuations      = train_df[train_df.label == 'Bug']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Feature_punctuations  = train_df[train_df.label == 'Feature']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Question_punctuations = train_df[train_df.label == 'Question']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['count_punctuations'] = train_df.text.apply(lambda z: len([c for c in str(z) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_df, col=\"label\" , height = 4, aspect = 1 , sharex = True , sharey = True)\ng.map(sns.distplot, \"count_punctuations\",kde = False)\n\n# print(f'X and Y range are different, so better look figure carefully !!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Bug\" level has max punctuations - It make sense as report a bug would certainly contains more punctuations if code or code logs are mention "},{"metadata":{},"cell_type":"markdown","source":"### Stopwords usage across class ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['stop_words'] = train_df.text.apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n\ng = sns.FacetGrid(train_df, col=\"label\" , height = 4, aspect = 1 , sharex = True , sharey = True )\ng.map(sns.distplot, \"stop_words\",kde = False)\n\n# print(f'X and Y range are different, so better look figure carefully !!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar insight for punctuations \n- \"Bug\" level has max stopwods - It make sense as report a bug would certainly contains more stopwords if code or code logs are mention "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['check_url'] = train_df.text.apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\ng = sns.FacetGrid(train_df, col=\"label\" , height = 4, aspect = 1 , sharex = True , sharey = True)\ng.map(sns.distplot, \"check_url\",kde = False)\n\n# print(f'X and Y range are different, so better look figure carefully !!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This also makes match with our comman sense - People will tend to give reference link for adding feature or for tentative reference solutions to a bug"},{"metadata":{},"cell_type":"markdown","source":"WordCloud Visualizations\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import wordcloud\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom PIL import Image\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_cloud(data,color):\n    plt.subplots(figsize=(15,15))\n    mask = None\n    wc = WordCloud(stopwords=STOPWORDS, \n                   mask=mask, background_color=\"white\", contour_width=2, contour_color=color,\n                   max_words=2000, max_font_size=256,\n                   random_state=42)\n    wc.generate(' '.join(data))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_cloud(train_df['text'],'red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'For Bug class')\ndisplay_cloud(train_df[train_df.label == 'Bug']['text'],'red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Bug\" class wordcloud\n- make sense to have \"error\" \"issue\" occuring frequently\n- if reported image, most reported in 'png' image format\n- Doe you notice python file is frequently used ? The reason I love python "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f'For Feature class')\ndisplay_cloud(train_df[train_df.label == 'Feature']['text'],'red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Feature\" class wordcloud\n- make sense to have \"add\" ad occure tokens. Mostly feature to be added right ? Or are you feature deletion guy :) \n- So you already will give reason for \"github\" words occur too !\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'For Question class')\ndisplay_cloud(train_df[train_df.label == 'Question']['text'],'red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most occuring words distribution ?\n\nHey !! , in below bar plot, what we are seeing so many unknown character ?\n-  Remember we are not cleaning data yet\n-  hmm I just want to show you , text analysis is not a clean job :) \n\nWe`ll clean stopwords later on"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Simplified counter function\n\nfrom collections import Counter\n\ndef create_corpus(word):\n    corpus=[]\n    \n    for x in train_df[train_df['label']==word]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stops=set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus('Bug')\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx=[]\ny=[]\nfor word,count in most[:100]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus('Feature')\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx=[]\ny=[]\nfor word,count in most[:100]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus('Question')\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx=[]\ny=[]\nfor word,count in most[:100]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference so far - raw data\n\n- Balance class : \"Questions\" class is fairly less compared to \"Bug\" and \"Feature\" counts, where last two class is almost same counts\n- Stopwords : Stopwords contribute a major junk in \"Bug\" and \"Feature\" class . Thought they are the majority among class distribution\n- Unusual usage of long text length\n- Cleaning of text is recommended - Not only stopwords , presence of noise inclusing html ect"},{"metadata":{},"cell_type":"markdown","source":"## Statistical Analysis-II\nN-gram analysis - to be continue"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def gram_analysis(data,gram):\n    \n#     token= tokenizer.tokenize(data.lower()) \n#     token = [tok for tok in token if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()]\n#     ngrams=zip(*[token[i:] for i in range(gram)])\n#     final_tokens=[\" \".join(z) for z in ngrams]\n#     return final_tokens\n\n\n# def create_dict(data,grams):\n#     freq_dict=defaultdict(int)\n#     for sentence in data:\n#         for tokens in gram_analysis(sentence,grams):\n#             freq_dict[tokens]+=1\n#     return freq_dict\n\n# def horizontal_bar_chart(df, color):\n#     trace = go.Bar(\n#         y=df[\"n_gram_words\"].values[::-1],\n#         x=df[\"n_gram_frequency\"].values[::-1],\n#         showlegend=False,\n#         orientation = 'h',\n#         marker=dict(\n#             color=color,\n#         ),\n#     )\n#     return trace\n\n# def create_new_df(freq_dict,):\n#     freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n#     freq_df.columns=['n_gram_words','n_gram_frequency']\n#     trace=horizontal_bar_chart(freq_df[:20],'orange')\n#     return trace","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def plot_grams(df1,df2,df3):\n#     fig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.1,\n#                           subplot_titles=[\"Frequent words of lable 0\", \n#                                           \"Frequent words of lable 1\",\n#                                           \"Frequent words of lable 2\"])\n#     fig.append_trace(df1, 1, 1)\n#     fig.append_trace(df2, 1, 2)\n#     fig.append_trace(df3, 1, 3)\n#     fig['layout'].update(height=800, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n#     py.iplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for gram in range(2,4):\n    \n#     if(gram == 2):\n#         print(\"Bi-gram analysis\")\n#     else:\n#         print(\"Tri-gram analysis\")\n\n#     freq_label_0_zero=create_dict(label_0_df['text'][:400],gram)\n#     trace_zero=create_new_df(freq_label_0_zero)\n    \n#     freq_label_1_ones=create_dict(label_1_df['text'][:400],gram)\n#     trace_ones=create_new_df(freq_label_1_ones)\n    \n#     freq_label_2_ones=create_dict(label_2_df['text'][:400],gram)\n#     trace_secs=create_new_df(freq_label_2_ones)\n    \n#     plot_grams(trace_zero,trace_ones,trace_secs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count_Bug_punctuations      = train_df[train_df.label == 'Bug']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Feature_punctuations  = train_df[train_df.label == 'Feature']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Question_punctuations = train_df[train_df.label == 'Question']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regex cleaning\nimport re\n\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndef clean_data(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text = train_df.text.apply(lambda z : remove_url(z))\ntrain_df.text = train_df.text.apply(lambda z: clean_data(z))\ntrain_df.text = train_df.text.apply(lambda z: remove_html(z))\ntrain_df.text = train_df.text.apply(lambda z: remove_punctuations(z))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}