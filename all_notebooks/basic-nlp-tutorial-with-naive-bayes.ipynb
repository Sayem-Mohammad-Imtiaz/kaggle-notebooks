{"cells":[{"metadata":{"_uuid":"4e792dbb86778dece9a0696c8f4e08772fdadea5"},"cell_type":"markdown","source":"# Tutorial for NLP - Text classification using Naive Bayes and NLP\n\nNaive Bayes is a popular algorithm, widely used for text classification.  Along with the powerful NLTK library,we will see the standard procedures used for sentiment classification of labelled text.\nIn this kernel we will see on how to create a model to classify a text input using"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\ntrain = pd.read_csv('../input/train.csv',encoding='iso-8859-1')","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.shape","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"51ce0ba5-8105-4f7d-9aad-4c91613d6771","_uuid":"176df0b3af87cec80e109ba2d1883a66e6d478e0","trusted":true},"cell_type":"code","source":"train.head(20)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"54207e50-3840-4adc-86a3-8871abf6fc8c","_uuid":"93025a17cf1691ca6617a8ee02e18254e7ea417c","trusted":true},"cell_type":"code","source":"train['SentimentText'][400]","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"f5780b19-175a-4058-8d86-0cf9e3efc360","_uuid":"f714172a78eb5c445848ce084a257b3c9cb800ad","trusted":true},"cell_type":"code","source":"lens = train.SentimentText.str.len()\nlens.mean(), lens.std(), lens.max()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"d2c99a8e-4bee-4d94-ad4a-ae850b788916","_uuid":"8724d8045acae449249d23c1a69281b4d2d6e0ce","trusted":true},"cell_type":"code","source":"lens.hist();\nplt.show()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"5da4e4f6-6d8e-4353-abc5-682f7cc6d895","_uuid":"2f951d383ba0f58372bea92ed94e503917f361c2","trusted":true},"cell_type":"code","source":"labels = ['0', '1']\nsizes = [train['Sentiment'].value_counts()[0],\n         train['Sentiment'].value_counts()[1]\n        ]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True)\nax1.axis('equal')\nplt.title('Sentiment Proportion', fontsize=20)\nplt.show()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"e05ee63a-9940-45a0-93d7-7a7b115153b2","_uuid":"691dbf65bf380f7105b8a9be18a1cd476ff87297"},"cell_type":"markdown","source":"So this data is a fairly balanced dataset, with not much skewness, for eductaional purpose we will use it as is.\n\n## StopWords\n\nWe can ignore words of no importance like conjunctions, adjective, etc, to make our input data much more meaningful to the algorithm. NLTK provides inbuilt corpus with stopwords to filer out them."},{"metadata":{"_cell_guid":"8b463c3e-54bd-43ef-bc81-8a57a61de3d9","_uuid":"b0c9a0f97d2aa013a99aa77c91339a8e68c2e660","trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\n \ndata = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\nstopWords = set(stopwords.words('english'))\nwords = word_tokenize(data)\nwordsFiltered = []\n \nfor w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)\n \nprint(wordsFiltered)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"f984062c-769d-4612-ae20-1aa290d934a5","_uuid":"dfcca89cf6e510132bd66c9c7523e7c7dbe5c99d","trusted":true},"cell_type":"code","source":"stopwords.words('english')","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"1a16521291c631c4b89ab79952966a2978f706f8","_cell_guid":"f9aa8e0a-c4d2-4f22-8976-cb2f65d4e322","collapsed":true,"trusted":true},"cell_type":"code","source":"#nltk.download(\"stopwords\") \nfrom nltk.corpus import stopwords\ntrain.SentimentText = [w for w in train.SentimentText if w.lower() not in stopwords.words('english')]","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"46e3f0cc-972b-4b04-977d-1789ad9f0053","_uuid":"32fbd728cb931dd3c3d8697a750cced1a242506c"},"cell_type":"markdown","source":"# Stemmer\nA word stem is part of a word. It is sort of a normalization idea, but linguistic. Given words, NLTK can find the stems.\n![title](https://pythonspot-9329.kxcdn.com/wp-content/uploads/2016/08/word-stem.png)"},{"metadata":{"_cell_guid":"d61a400b-524e-4c68-b6e5-dcaaf5bd251e","_uuid":"c339a02e00824528317f96ba10588b10257d0e9a","trusted":false,"collapsed":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n \nwords = [\"game\",\"gaming\",\"gamed\",\"games\"]\nstemmer = PorterStemmer()\n \nfor word in words:\n    print(stemmer.stem(word))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d9c08b8-0af2-4eb6-8551-4d0c04821f11","_uuid":"f3ee50f273fa9ad128cf080782c158cd11607ed8","trusted":false,"collapsed":true},"cell_type":"code","source":"plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n            'died', 'agreed', 'owned', 'humbled', 'sized',\n            'meeting', 'stating', 'siezing', 'itemization',\n            'sensational', 'traditional', 'reference', 'colonizer',\n            'plotted'] \nfor word in plurals:\n    print(stemmer.stem(word))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe835d742627f1e54b629517a393e4be23212438","_cell_guid":"1fc913b7-c607-482d-9457-0c0a70fe528d","collapsed":true,"trusted":false},"cell_type":"code","source":"#nltk.download(\"wordnet\")\nps = nltk.PorterStemmer()\ntrain.SentimentText = [ps.stem(l) for l in train.SentimentText]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"374001b9-67af-4dc1-a7c9-de9c7d114426","_uuid":"ddcd1b11a0f5024e8f7139178258174897b6ffb1"},"cell_type":"markdown","source":"# Split Test and Train"},{"metadata":{"_uuid":"f771df9dd289c37b86ac94c8d6346c65af4d3e0a","_cell_guid":"6ab58457-d111-4cf5-8f59-39e2a59e8c3f","collapsed":true,"trusted":false},"cell_type":"code","source":"X = train.SentimentText\ny = train.Sentiment\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d489503-ece3-4e0f-87a0-8770589e5e12","_uuid":"ef42378a076dd94b23d2a7758c2d135b6753f3b3","trusted":false,"collapsed":true},"cell_type":"code","source":"train1=pd.concat([X_train,y_train], axis=1)\ntrain1.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16c5f837-aa84-4e09-84fd-d4533d887762","_uuid":"d0b6303a49944b63168f3a556ae6d85b400edb65"},"cell_type":"markdown","source":"# Tokenization\n\nThe goal of tokenization is to break up a sentence or paragraph into specific tokens or words. We basically want to convert human language into a more abstract representation that computers can work with.\n\nSometimes you want to split sentence by sentence and other times you just want to split words."},{"metadata":{"_uuid":"041eb13f1f3fb0f179a723f834838ce43147ec88","_cell_guid":"f8fbefc2-2c35-4957-b2f4-743b53a4e1be","collapsed":true,"trusted":false},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96bedc82-a7a0-4693-9daf-eb3220923ba1","_uuid":"611c0c88838de18edd5f1e82c24aea67dbc550bd"},"cell_type":"markdown","source":"# Ngrams"},{"metadata":{"_cell_guid":"60fd8495-edc9-4505-8903-224595f57e57","_uuid":"e3888e5b31c8998e1acc7e622825b6e6e7fb5afa","trusted":false,"collapsed":true},"cell_type":"code","source":"from nltk import ngrams\nsentence = 'this is a foo bar sentences and i want to ngramize it'\nn = 2\nbigrams = ngrams(sentence.split(), n)\nfor grams in bigrams:\n  print (grams)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33e072b7-be7e-4729-8160-1dfdc2175cbd","_uuid":"dc32e86f32d828d45f36d149251fab83dc352ee9"},"cell_type":"markdown","source":"# Term Frequency-Inverse Document Frequency (TF-IDF)\nTerm-frequency-inverse document frequency (TF-IDF) is another way to judge the topic of an article by the words it contains. With TF-IDF, words are given weight – TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\n\nFirst, TF-IDF measures the number of times that words appear in a given document (that’s term frequency). But because words such as “and” or “the” appear frequently in all documents, those are systematically discounted. That’s the inverse-document frequency part. The more documents a word appears in, the less valuable that word is as a signal. That’s intended to leave only the frequent AND distinctive words as markers. Each word’s TF-IDF relevance is a normalized data format that also adds up to one.\n\n![title](https://deeplearning4j.org/img/tfidf.png)"},{"metadata":{"_uuid":"c49eaddda25fedb1f514e1b9a62f94178e8aab5c","_cell_guid":"ab1f002a-f03c-45a8-a36c-8d07dab13df1","collapsed":true,"trusted":false},"cell_type":"code","source":"n = train1.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), #The lower and upper boundary of the range of n-values for different n-grams\n                      tokenizer=tokenize,\n                      min_df=3,      # ignore terms that have a df strictly lower than threshold\n                      max_df=0.9,    #ignore terms that have a df strictly higher than threshold (corpus-specific stop words)\n                      strip_accents='unicode', #Remove accents during the preprocessing step\n                      use_idf=1,\n                      smooth_idf=1,  #Smooth idf weights by adding one to document frequencies, \n                                     #as if an extra document was seen containing every term in \n                                     #the collection exactly once. Prevents zero divisions.\n                      sublinear_tf=1, #Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n                      max_features=40000\n                     )\ntrn_term_doc = vec.fit_transform(train1['SentimentText'])\ntest_term_doc = vec.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59fea1c4-c89d-4db2-acca-3fce8b55e02e","_uuid":"a910522306431b4e84e6cfc5a76444cb086501b5","trusted":false,"collapsed":true},"cell_type":"code","source":"#This creates a sparse matrix with only a small number of non-zero elements (stored elements in the representation below).\ntrn_term_doc, test_term_doc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8714f7fc36270d88f1eb41655ce7b7d0a9825b5b","_cell_guid":"4d1393fd-a049-439c-8803-3a4b95fb3adc","collapsed":true,"trusted":false},"cell_type":"code","source":"#Here's the basic naive bayes feature equation:\ndef pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0653bbcec932e6c0d8205d30b06e0b27defee951","_cell_guid":"04b9028b-66af-41e3-be06-73167e5bf532","collapsed":true,"trusted":false},"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=3,solver='newton-cg')\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"603dcb8f-af06-4236-a809-2135b53159f7","_uuid":"a865d1dd5b65b20771f71cfb2f778dabddb9d170","trusted":false,"collapsed":true},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc\n\nlabel_cols=['Sentiment']\npreds = np.zeros((len(X_test), len(label_cols)))\npreds","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a037db5-a62f-499f-a1dd-feacdc44b708","_uuid":"81404111a3fee3013258c04b5cd48d2b485512ee","trusted":false,"collapsed":true},"cell_type":"code","source":"for i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train1[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3a7196de2498a9c8a1baad0166e6e7d5b1f632d","_cell_guid":"bfd7f214-245e-4053-8c97-0b0a19fb6b45","collapsed":true,"trusted":false},"cell_type":"code","source":"y_pred=pd.DataFrame(preds.round(decimals=0), columns = label_cols)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7b6b7d70-646f-49b1-81a2-9651935ab80c","_uuid":"6e0544dec560182a707deb48f1d8768ace9f823f","trusted":false,"collapsed":true},"cell_type":"code","source":"accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fef25b01-a8c4-4ab9-8968-92a6d46928bc","_uuid":"3309cea755a4b19788fd54def8c418e8751afb86"},"cell_type":"markdown","source":"## References:\n- https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python\n- http://billchambers.me/tutorials/2014/12/21/tf-idf-explained-in-python.html\n- http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html\n- https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\n- https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}