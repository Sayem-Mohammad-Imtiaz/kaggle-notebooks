{"cells":[{"metadata":{"_uuid":"dfbe3e48319960c2d42c296b30c22840b3359d6c"},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I will apply classification algorithms with scikit-learn library. Firstly, EDA(Exploratory Data Analysis) will be applied to dataset. Then, different algorithms will classify dataset.\n\n[0. EDA(Exploratory Data Analysis)](#1)\n\n[1. K-Nearest Neighbor Algorithm](#2)\n\n[2. Support Vector Machine(SVM)](#3)\n\n[3. Naive-Bayes Classification](#4)\n\n[4. Decision Tree Classification](#5)\n\n[5. Random Forest Classification](#6)\n\n[6.  Conclusion](#7)\n\n[7.  References](#8)\n"},{"metadata":{"_uuid":"310140792cd4e1daf19873d7739990517cdb72e9"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## EDA (Exploratory Data Analysis)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/breastCancer.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48c5e6150e9df0e2a005f7ac4dba524d9101309b"},"cell_type":"code","source":"# Clear the noisy attributes\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"049f9cb3b0d7ce0bd4c27158a90f3fe6beacaf84"},"cell_type":"code","source":"M = data[data.diagnosis=='M']\nB = data[data.diagnosis=='B']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89c1c1ff83dce83689da9742d7660bd4049b130e"},"cell_type":"code","source":"plt.scatter(M.radius_mean,M.texture_mean,color='red',label='Malignant',alpha=0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color='green',label='Benign',alpha=0.3)\nplt.xlabel('Malignant')\nplt.ylabel('Benign')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47f2f1d6f7450be3240ef0aaa673a0403f44f2d7"},"cell_type":"code","source":"# Change M and B values to 0 and 1\n# Prepare x and y values for KNN algorithm\ndata.diagnosis= [1 if each==\"M\" else 0 for each in data.diagnosis]\ny=data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"145c9529c3eceb5b8a1871af8dedd314b0aafbbf"},"cell_type":"code","source":"# Normalization\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbb694de40513d75fdd9d1d600e2e5f08e19fda3"},"cell_type":"code","source":"# Train-Test Split for Learning\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdcc281f2649872abfd76eda23c50c54978389f7"},"cell_type":"markdown","source":"<a id=\"2\"></a><br>\n## KNN(K-Nearest Neighbor) Classification\n\n![KNN](https://www.kdnuggets.com/wp-content/uploads/knn2.jpg)"},{"metadata":{"trusted":true,"_uuid":"77d9fcedeca1df96bb927e91cee05e9e4a09e99e"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3) #k=3\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\"{} nn score: {}\".format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c04cba9b9905d76e8578e3606d8472979d2bbc9a"},"cell_type":"code","source":"# Hyperparameter Tuning\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2bb912547674387883d51c22bf600d568b16b4e"},"cell_type":"markdown","source":"<a id=\"3\"></a><br>\n## SUPPORT VECTOR MACHINE(SVM)"},{"metadata":{"_uuid":"5dbfb820d688ed9f222d92a6fd163a7c4dca475d"},"cell_type":"markdown","source":"![](http://www.saedsayad.com/images/SVM_2.png)\n\nI will not explain how SVM algorithm works. However, I found a great page for understand SVM algorithm. \nSo you can look detailed information about SVM in there : [Support Vector Machine](http://www.saedsayad.com/support_vector_machine.htm)\n\n**Note : Our x,y, train-test split(x_data,y_data) values are prepared from the previous algorithm**"},{"metadata":{"trusted":true,"_uuid":"53bf665db78a6858070694fb40506e4d67f5c9be"},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state=1) # Return the same value every time\nsvm.fit(x_train,y_train)\n\n# test\nprint(\"primy accuracy of SVM algorithm : \",svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"745ad114c9a5ba42100ce9ef0b8a152644c327ea"},"cell_type":"markdown","source":"<a id=\"4\"></a><br>\n## Naive-Bayes Classification"},{"metadata":{"_uuid":"e542be892ffee204000df058d5d1476170abeda9"},"cell_type":"markdown","source":"Here is the another probabilstic approach for machine learning. You can look the Bayes Theorem from this page.\n![Naive-Bayes](http://www.saedsayad.com/images/Bayes_rule.png)\n[Naive-Bayes Classification](http://www.saedsayad.com/naive_bayesian.htm)\n\n**Note : Our x,y, train-test split(x_data,y_data) values prepared from the previous algorithm**"},{"metadata":{"trusted":true,"_uuid":"f1d2b97db7951c88b374b5431295160bd1fa6328"},"cell_type":"code","source":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\n#test\nprint(\"Accuracy of Naive-Bayes Algorithm\",nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba253736d5db8b4fc58a533c3dc873a48954cca2"},"cell_type":"markdown","source":"<a id=\"5\"></a><br>\n## Decision Tree Classification"},{"metadata":{"trusted":true,"_uuid":"963cafe91c7d63ec50cae5e446a99b6e717c7c55"},"cell_type":"markdown","source":"<img src=\"https://tr.akinator.com/bundles/elokencesite/images/akinator.png?v95\" alt=\"drawing\" height=\"300\" width=\"300\"/>"},{"metadata":{"trusted":true,"_uuid":"8077e25e6a52adc2aab6edaac9a365b8c2202511"},"cell_type":"markdown","source":"Have you ever played Akinator Game? It is a great example of decision trees. If you haven't yet, let me explain. The goal of the game is predict a famous character, based of bunch of questions which asked to us. After the each answer, questions are getting more relevant to famous person which is in our mind. Finally, akinator shows his predict. Mostly, he justifies about the prediction.\n\nDecision tree algorithm works behind the akinator. There are splits which decides to person to based on the answers. For example if we are looking for a person which is blonde then it redirects the future predictions to \"blonde\" people. \"Blonde\" input is given by the user as an answer for a question.\n\nIf you want to play the game, the link is below.\n\nhttps://en.akinator.com/"},{"metadata":{"trusted":true,"_uuid":"b603b6a3268d661e4144e99723aec87f010166e9"},"cell_type":"markdown","source":"![](https://annalyzin.files.wordpress.com/2016/07/decision-trees-titanic-tutorial.png?w=616&h=342)\nSource : https://annalyzin.files.wordpress.com/2016/07/decision-trees-titanic-tutorial.png?w=616&h=342"},{"metadata":{"trusted":true,"_uuid":"8fc543d19fe5c3d564e1e4f99a5c3ba8c96f2c34"},"cell_type":"markdown","source":"You can see the splits. It can be more than two splits. In this example, we have a binary tree. The bottom nodes known as \"leaf\". These are our predictions. The top node is \"root\" node. In a nutshell, let's start the code.\n\n* EDA and Normalization have already done in the previous sections. \n* Train-Test split is obtained in the previous sections. But I want to change percentage of test split to 15%\n\nLet's start at the algorithm."},{"metadata":{"trusted":true,"_uuid":"234092e6f39dc083e518e941ceb403a447649b8b"},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.15,random_state=42)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n# Accuracy\nprint(\"Accuracy of Decision Tree Algorithm\",dt.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3068a791936c8c45e5213ffc7487489871d8abf"},"cell_type":"markdown","source":"<a id=\"6\"></a><br>\n# Random Forest Classification"},{"metadata":{"trusted":true,"_uuid":"3d3e56fe87e438b4656d4ee8ac6f0a9c1ed3f504"},"cell_type":"markdown","source":"* The random forest is a \"ensemble learning\" algorithm. So, it includes more than one classification algorithm. The \"forest\" name comes from to our trees. In a nutshell, random forest classification is includes 'n' trees in itself. \n\n* While we make predictions, different results(classes) can occur. In order to reduce our prediction to a singe class, we will use \"Majority Voting\". It is simple, which count of class is bigger than to other it will be final result.\n\n* Estimator = Number of trees for our random forest classification model"},{"metadata":{"trusted":true,"_uuid":"883f12ca11dc0d289c49d0a813799f2d80632575"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100,random_state=1) # Number of tree = 100\nrf.fit(x_train,y_train)\nprint(\"Accuracy of Random Forest Algorithm\",rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afd66870fce491cf874305f0d7814206e8f53809"},"cell_type":"markdown","source":"We can see that, accuracy is increased. However, we can't be sure about the number of estimators. In order to choose best number of estimators:"},{"metadata":{"trusted":true,"_uuid":"f30690248d1e38d2eb8a9d8970f29404b6e2e45f"},"cell_type":"code","source":"accuracy_list=[]\nfor i in range(1,11,1):\n    rf = RandomForestClassifier(n_estimators=i,random_state=1) # Number of tree = 100\n    rf.fit(x_train,y_train)\n    accuracy_list.append(rf.score(x_test,y_test))\n    #print(\"Accuracy of Random Forest Algorithm for {} trees: {}\".format(i,rf.score(x_test,y_test)))\nplt.plot(range(1,11),accuracy_list)\nplt.xlabel(\"Number of estimators\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d422507f0f2a286d1d62fc7ce107b8732b06db6"},"cell_type":"markdown","source":"We can see that, optimum number of estimators is 4. There is no need to 100 trees. Hence, we can obtain same result with less trees."},{"metadata":{"_uuid":"82adb89dd98ef252cc99e48d0411590169889671"},"cell_type":"markdown","source":"<a id=\"7\"></a><br>\n## Conclusion\n\n* You have seen which algorithm is better. Of course **none of them**. All algorithms have trade-offs. You should pick your algorithm for your scenario. \n\n* Do not forget tuning **hyperparameters** for the better results.\n"},{"metadata":{"trusted":true,"_uuid":"cf2431a8cc90a9db55dba6dba786b7104c3a50e0"},"cell_type":"markdown","source":"<a id=\"8\"></a><br>\n## References\n\nhttps://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners\n\nhttp://www.saedsayad.com/\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}