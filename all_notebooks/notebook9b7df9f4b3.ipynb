{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nimport fastai \nfrom pathlib import Path\nfrom fastai.vision.all import *\nfrom fastai.basics import *\n\nimport albumentations as A\n\n# Input data files are available in the read-only \"../input/\" directory\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('../input/hpa-cell-tiles-sample-balanced-dataset')\ndf = pd.read_csv(path/'cell_df.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the the total number of target labels\nlabels = [str(i) for i in range(19)]\nfor x in labels: df[x] = df['image_labels'].apply(lambda r: int(x in r.split('|')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here a sample of the dataset has been taken, change frac to 1 to train the entire dataset!\ndfs = df.sample(frac=1, random_state=42)\ndfs = dfs.reset_index(drop=True)\nlen(dfs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# obtain the input images.\ndef get_x(r): \n    return path/'cells'/(r['image_id']+'_'+str(r['cell_id'])+'.jpg')\n\n# obtain the targets.\ndef get_y(r): \n    return r['image_labels'].split('|')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''AlbumentationsTransform will perform different transforms over both\n   the training and validation datasets ''' \nclass AlbumentationsTransform(RandTransform):\n    \n    '''split_idx is None, which allows for us to say when we're setting our split_idx.\n       We set an order to 2 which means any resize operations are done first before our new transform. '''\n    split_idx, order = None, 2\n    \n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    # Inherit from RandTransform, allows for us to set that split_idx in our before_call.\n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    # If split_idx is 0, run the trainining augmentation, otherwise run the validation augmentation. \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_aug(size): \n    \n    return A.Compose([\n            # allows to combine RandomCrop and RandomScale\n            A.RandomResizedCrop(size,size),\n            \n            # Transpose the input by swapping rows and columns.\n            A.Transpose(p=0.5),\n        \n            # Flip the input horizontally around the y-axis.\n            A.HorizontalFlip(p=0.5),\n        \n            # Flip the input horizontally around the x-axis.\n            A.VerticalFlip(p=0.5),\n        \n            # Randomly apply affine transforms: translate, scale and rotate the input.\n            A.ShiftScaleRotate(p=0.5),\n        \n            # Randomly change hue, saturation and value of the input image.\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        \n            # Randomly change brightness and contrast of the input image.\n            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        \n            # CoarseDropout of the rectangular regions in the image.\n            A.CoarseDropout(p=0.5),\n        \n            # CoarseDropout of the square regions in the image.\n            A.Cutout(p=0.5) ])\n\ndef get_valid_aug(size): \n    \n    return A.Compose([\n    # Crop the central part of the input.   \n    A.CenterCrop(size, size, p=1.),\n    \n    # Resize the input to the given height and width.    \n    A.Resize(size,size)], p=1.)\n\nget_train_aug(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''The first step item_tfms resizes all the images to the same size (this happens on the CPU) \n   and then batch_tfms happens on the GPU for the entire batch of images. '''\n# Transforms we need to do for each image in the dataset\nitem_tfms = [Resize(224), AlbumentationsTransform(get_train_aug(224), get_valid_aug(224))]\n\n# Transforms that can take place on a batch of images\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]\n\nbs=6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock(vocab=labels)),\n                  splitter = RandomSplitter(seed=42),\n                  get_x=get_x,\n                  get_y=get_y,\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\ndls = dblock.dataloaders(dfs, bs=bs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#learn = cnn_learner(dls, ('../resnet50/resnet50.pth'), metrics=accuracy_multi)\n#learn.lr_find()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#learn.fine_tune(4,0.00144)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#learn.export()\nlearn = load_learner('../input/hpamodelmvd/export.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('../input/hpa-cell-tiles-test-with-enc-dataset')\ndf = pd.read_csv(path/'cell_df.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('cell_df.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dl = learn.dls.test_dl(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dl.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds, _ = learn.get_preds(dl=test_dl)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('preds.pickle', 'wb') as handle:\n    pickle.dump(preds, handle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_prds = torch.argmax(preds, dim=-1)\nlen(cls_prds), cls_prds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/hpa-single-cell-image-classification/sample_submission.csv')\nsample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['cls'] = cls_prds\ndf['pred'] = df[['cls', 'enc']].apply(lambda r: str(r[0]) + ' 1 ' + r[1], axis=1)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = df.groupby(['image_id'])['pred'].apply(lambda x: ' '.join(x)).reset_index()\n# subm = subm.loc[3:]\nsubm.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.merge(\n    sample_submission,\n    subm,\n    how=\"left\",\n    left_on='ID',\n    right_on='image_id',\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def isNaN(num):\n    return num != num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, row in sub.iterrows():\n    if isNaN(row['pred']): continue\n    sub.PredictionString.loc[i] = row['pred']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = sub[sample_submission.columns]\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cell_df = pd.read_csv('cell_df.csv')\ncell_df.head()\ncell_df['cls'] = ''\n\nthreshold = 0.0\n\nfor i in range(preds.shape[0]): \n    p = torch.nonzero(preds[i] > threshold).squeeze().numpy().tolist()\n    if type(p) != list: p = [p]\n    if len(p) == 0: cls = [(preds[i].argmax().item(), preds[i].max().item())]\n    else: cls = [(x, preds[i][x].item()) for x in p]\n    cell_df['cls'].loc[i] = cls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine(r):\n    cls = r[0]\n    enc = r[1]\n    classes = [str(c[0]) + ' ' + str(c[1]) + ' ' + enc for c in cls]\n    return ' '.join(classes)\n\ncombine(cell_df[['cls', 'enc']].loc[24])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cell_df['pred'] = cell_df[['cls', 'enc']].apply(combine, axis=1)\ncell_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/hpa-single-cell-image-classification/sample_submission.csv')\nsample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.merge(\n    sample_submission,\n    subm,\n    how=\"left\",\n    left_on='ID',\n    right_on='image_id',\n)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def isNaN(num):\n    return num != num\n\nfor i, row in sub.iterrows():\n    if isNaN(row['pred']): continue\n    sub.PredictionString.loc[i] = row['pred']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = sub[sample_submission.columns]\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}