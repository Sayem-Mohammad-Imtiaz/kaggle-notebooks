{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Installing Dependencies"},{"metadata":{},"cell_type":"markdown","source":"appending torchvision reference scripts for detection to path for importing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/torchvisionreferencedetection/torchvision-reference-derection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Installing pycocotools for evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycocotools","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"importing required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom engine import train_one_epoch, evaluate\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting Data ready"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_csv_path = \"../input/yolo-animal-detection-small/train.csv\"\ntest_csv_path = \"../input/yolo-animal-detection-small/test.csv\"\ntrain_images = \"../input/yolo-animal-detection-small/yolo-animal-detection-small/train\"\ntest_images = \"../input/yolo-animal-detection-small/yolo-animal-detection-small/test\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv(train_csv_path)\ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv = pd.read_csv(test_csv_path)\ntest_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = train_csv[\"class\"].unique()\nprint(categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"encoding classes to integers.\n\n- 0 is for background by default"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabelMap:\n    def __init__(self, categories):\n        self.map_dict = {}\n        self.reverse_map_dict={}\n        for i, cat in enumerate(categories):\n            self.map_dict[cat] = i + 1\n            self.reverse_map_dict[i] = cat\n    def fit(self, df, column):\n        df[column] = df[column].map(self.map_dict)\n        return df\n    def inverse(self, df, column):\n        df[column] = df[column].map(self.map_dict)\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_map = LabelMap(categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = label_map.fit(train_csv, \"class\")\ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv = label_map.fit(test_csv, \"class\")\ntest_csv.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"creating torch dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AnimalDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_path, categories, transforms=None,**kwargs):\n        super().__init__(**kwargs)\n        self.df = df\n        self.image_path = image_path\n        self.categories = categories\n        self.images = self.df[\"filename\"].unique()\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image_file = os.path.join(self.image_path, self.images[idx])\n        img = cv2.imread(image_file)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = img.astype(np.float32)\n        img = img/255.0\n        image_data = self.df[self.df['filename'] == self.images[idx]]\n        labels = torch.as_tensor(image_data[\"class\"].values, dtype=torch.int64)\n        xmins = image_data[\"xmin\"].values\n        ymins = image_data[\"ymin\"].values\n        xmaxs = image_data[\"xmax\"].values\n        ymaxs = image_data[\"ymax\"].values\n        boxes = torch.as_tensor(np.stack([xmins, ymins, xmaxs, ymaxs], axis=1), dtype=torch.float32)\n        areas = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n        image_id = torch.tensor([idx])\n        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = areas\n        target[\"iscrowd\"] = iscrowd\n        if self.transforms is not None:\n            transformed = self.transforms(image=img, bboxes=boxes, labels=labels)\n            img = transformed[\"image\"]\n            target[\"boxes\"] = torch.as_tensor(transformed[\"bboxes\"],dtype=torch.float32)\n        return torch.as_tensor(img, dtype=torch.float32), target\n    def get_height_and_width(self, image):\n        image_data = self.df.loc[self.df['filename'] == image]\n        return image_data[\"width\"].values[0], image_data[\"height\"].values[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"defining augmentations and transforms for training and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_train = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    ToTensorV2(p=1)\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_test = A.Compose([\n    ToTensorV2(p=1)\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"function called after we get data from data loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"initiating datsets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = AnimalDataset(train_csv, train_images, categories, transform_train)\ntest_dataset = AnimalDataset(test_csv, test_images, categories, transform_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"creating dataloaders from datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader_train = torch.utils.data.DataLoader(\n        train_dataset, batch_size=4, shuffle=True, num_workers=4,\n        collate_fn=collate_fn)\n    \ndata_loader_test = torch.utils.data.DataLoader(\n    test_dataset, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plotting images from dataloader and verifying"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_images(images, targets):\n    for image, target in zip(images, targets):\n        sample = image.permute(1,2,0).cpu().numpy()\n        fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n        boxes = target[\"boxes\"].cpu().numpy().astype(np.int32)\n        for box in boxes:\n            cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (220, 0, 0), 3)\n        ax.set_axis_off()\n        ax.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images, targets = next(iter(data_loader_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(images, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images, targets = next(iter(data_loader_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_images(images, targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Validation"},{"metadata":{},"cell_type":"markdown","source":"loading faster rcnn model from torchvision"},{"metadata":{"trusted":true},"cell_type":"code","source":"detection_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"changing classification head for fine-tuning based on our dataset including background class"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = len(categories)+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_features = detection_model.roi_heads.box_predictor.cls_score.in_features\ndetection_model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"load model to gpu or cpu based on device"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detection_model.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"training and validating model in each epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training(model, train_loader, val_loader, epochs=10):\n    # construct an optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n    # and a learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=3,\n                                                   gamma=0.1)\n    for epoch in range(epochs):\n        # train for one epoch, printing every 10 iterations\n        train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n        # update the learning rate\n        lr_scheduler.step()\n        # evaluate on the test dataset\n        evaluate(model, val_loader, device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training(detection_model, data_loader_train, data_loader_test, epochs=10 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"saving model state for further use"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(detection_model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}