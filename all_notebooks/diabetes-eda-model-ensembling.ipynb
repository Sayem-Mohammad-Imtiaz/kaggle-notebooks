{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media.nature.com/lw800/magazine-assets/d41586-020-01891-8/d41586-020-01891-8_18111016.jpg\" height=\"800px\" width=\"800px\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"People with type 1 diabetes are unable to produce the hormone insulin. Photo Credit: Bernard Chantal/Alamy","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# About the Data\n\nThis dataset was sourced from the National Institute of Diabetes and Digestive and Kidney Diseases. The dataset is used to predict whether or not a patient has diabetes, based on certain diagnostic measurements. Several constraints were placed on the selection criteria from a larger database, meaning that the observations are female patients at least 21 years old and of Pima Native American heritage.\n\n## Objective\nConduct EDA on the diabetes dataset and build an ensemble classifier to improve the AUC score.\n\n## Dataset\nThe dataset consists of several features and a target variable --> Outcome. \n\n* **Pregnancies**: Number of times pregnant\n* **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* **BloodPressure**: Diastolic blood pressure (mm Hg)\n* **SkinThickness**: Triceps skin fold thickness (mm)\n* **Insulin**: 2-Hour serum insulin (mu U/ml)\n* **BMI**: Body mass index (weight in kg/(height in m)^2)\n* **DiabetesPedigreeFunction**: provides some data on diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient\n* **Age**: Age (years)\n* **Outcome**: Class variable (0 or 1) indicating whether or not a patient has diabetes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Data & Summary Statistics","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import packages for EDA and Data Visualization\nimport pandas as pd\npd.set_option('display.max_columns', 50)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nsns.set(style=\"white\")\n\nSEED = 42  # random seed for modeling\n\n# Import Classes for Model Traning and Feature Exploration / Engineering \nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom scipy.stats import mstats\nfrom sklearn.preprocessing import (StandardScaler, LabelEncoder, PolynomialFeatures)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import (cross_val_score, RepeatedStratifiedKFold,\n                                     train_test_split, GridSearchCV)\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import several other Classes for Ensembling / Stacking \nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier, RandomForestClassifier, VotingClassifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"diabetesDF = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metadata","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display(diabetesDF.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Descriptive Stats","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display(diabetesDF.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First 5 observations","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display(diabetesDF.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last 5 observations","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display(diabetesDF.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It's clear that we have zeroes in the BloodPressure, Skin Thickness & BMI attributes.  Other attributes also have zeroes, however, without knowing information about how the data was collected I am hesitant to impute zeroes in other features such as Insulin, for example.\n* You can think of zeroes as you would missing values.\n* Let's address these zeroes with Imputation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Missing Value Imputation - Iterative Imputer\n\nWe are going to use Scikit-Learn's Iterative Imputer to impute values that are currently set to zero.  Iterative Imputer imputes missing values by modeling each feature as a function of other features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Before Imputation","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Define columns with missing values\nmissing_colsDF = diabetesDF[['BloodPressure','SkinThickness','BMI']]\n\n# show the pairplot\nsns.pairplot(missing_colsDF)\nplt.show()\n\n# replace zeroes with np.nan\nmissing_colsDF = missing_colsDF.replace(0, np.nan)\nprint(missing_colsDF.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear from the visualizations that these 3 attributes are multi-modal, due to the presence of missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### After Imputation","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Iteratively impute\nimp_iter = IterativeImputer(max_iter=5, sample_posterior=True, random_state=SEED)\ndiabetes_imp_iter = imp_iter.fit_transform(missing_colsDF)\n\n# Convert returned array to DataFrame\ndiabetes_imp_iterDF = pd.DataFrame(diabetes_imp_iter, columns=missing_colsDF.columns)\n\n# show the pairplot\nsns.pairplot(diabetes_imp_iterDF)\nplt.show()\n\n# Check the DataFrame's info\nprint(diabetes_imp_iterDF.info())\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing Values have been successfully imputed and as a result the distributions are more normal, compared to before imputation.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# drop original columns from dataframe \ndiabetesDF = diabetesDF.drop(['BloodPressure','SkinThickness','BMI'], axis=1)\n\n# and add new columns containing imputations\ndiabetesDF = diabetesDF.join(diabetes_imp_iterDF)\n\n# Check the DataFrame's info\nprint(diabetesDF.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will examine the distrubtion of all the attributes in the dataset by Outcome.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.pairplot(diabetesDF, hue='Outcome', palette=\"Blues\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are less younger women that have diabetes, compared to women in their 30s-50s.\n* Women that have diabetes have higher BMI.\n* Diabetes pedigree function is lower for women that have diabetes; Same thing with insulin & glucose.\n* Pregnacies are also lower among women that have diabetes.\n* There are outliers present in almost all of the attributes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Outlier Detection & Replacement\n\nLet's take a closer look at the outliers by visualing box plots by Outcome.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def boxplot(df: pd.DataFrame) -> None:\n    \"\"\"\n    Visualize a boxplot for each feature for each class.\n    \"\"\"\n    fig, axis = plt.subplots()\n\n    for col in df.columns:\n        if col != 'Outcome':\n            sns.boxplot(x='Outcome', y=col, data=df, palette='Blues')\n            plt.show()\n            \n# call the function to display the boxplot\nboxplot(df=diabetesDF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's replace the outliers with the 5th and 95th percentile values.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def winsorizeDF(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Replace outliers with 5th and 95th percentile values.\n    \"\"\"\n    return df.apply(lambda x: mstats.winsorize(x, limits=[.05, .05]))\n\n# apply winsorize and show summary stats\ndiabetes_winDF = winsorizeDF(df=diabetesDF.drop('Outcome', axis=1))\ndiabetesDF = diabetes_winDF.join(diabetesDF['Outcome'])\ndisplay(diabetesDF.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers have been replaced!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Class Imbalance\nNext, let's take a look at the proportion of classes.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot(diabetesDF['Outcome'], palette=\"Blues\")\nplt.show()\n\n# Show the % share of classes\nprint(round(diabetesDF['Outcome'].value_counts(normalize=True)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 35% diabetes prevalence rate for Pima Native American women.\n* The class imbalance is not too bad; A little less than a 2:1 ratio for the negative to positive diabetes diagnoses.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation Heatmap","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# # Create the correlation matrix\ncorr = diabetesDF.corr()\n\n# Generate a mask for the upper triangle \nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Add the mask to the heatmap\nplt.figure(figsize = (10,5))\nsns.heatmap(corr, mask=mask, center=0, linewidths=1, annot=True, fmt=\".2f\", cmap=\"Blues\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* SkinThickness has a moderately positive relationship with BMI.\n* Age has a moderately positive relationship with Pregancies.\n* Glucose has a moderate to low positive relationship with Outcome.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## PCA for Feature Exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can use Principal Component Analysis as a means to look at the separation of the features in a 2-dimensional space by Outcome.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# create the feature set: X\nX = diabetesDF.drop('Outcome', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Build the PCA pipeline\npipe = Pipeline([('scaler', StandardScaler()),\n        \t\t ('reducer', PCA(n_components=2, random_state=SEED))])\n\n# Fit it to the dataset and extract the component vectors\npc = pipe.fit_transform(X)\nvectors = pipe.steps[1][1].components_.round(2)\n\n# Print feature effects\nprint('PC 1 effects = ' + str(dict(zip(X.columns, vectors[0]))))\nprint('PC 2 effects = ' + str(dict(zip(X.columns, vectors[1]))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The first PC has a moderate to low postive relationship with the SkinThickness, BMI and BloodPressure\n* The second PC has a moderately positive relationship with Pregnancies and Age","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Add the 2 components to X\nX['PC 1'] = pc[:, 0]\nX['PC 2'] = pc[:, 1]\n\n# join outcome variable from original DF\nX = X.join(diabetesDF['Outcome'])\n\n# Use the Outcome feature to color the PC 1 vs PC 2 in the scatterplot\nsns.scatterplot(data=X, x='PC 1', y='PC 2', hue='Outcome', palette=\"Blues\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There isn't a clear separation of classes when we visualize the PCs by Outcome.  This means we'll have to take special care when we engineer features for modeling..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Training & Evaluation\n\nWe're going to use AUC as the metric to evaluate our model since we noticed the presence of class imbalance earlier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Create a Naive Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's start by defining our features and target variables and evaluating a Naive Classifier. This is a Dummy Classifier that uses the modal value for its predictions.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def define_X_y(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Create X and y variables for ML Classification\n    \"\"\"\n    # create the feature set: X\n    X = df.drop('Outcome', axis=1)\n\n    # create the target variable: y\n    y = df['Outcome']\n    \n    # Convert X matrix data types to 'float32' for consistency using .astype()\n    X = X.astype('float32')\n\n    # Convert y (target) array to 'str' using .astype()\n    y = y.astype('str')\n\n    # Encode class labels in y array using dot notation with LabelEncoder().fit_transform()\n    # Hint: y goes in the fit_transform function call\n    y = LabelEncoder().fit_transform(y)\n    \n    return X, y\n\n# call the function to return features & target\nX, y = define_X_y(df=diabetesDF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Instantiate a DummyClassifier with 'most_frequent' strategy\nnaive = DummyClassifier(strategy='most_frequent', random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def evaluate_model(model, X, y) -> np.ndarray:\n    \"\"\"\n    Cross-validate model training and print out AUC.\n    \"\"\"\n    # Create RepeatedStratifiedKFold cross-validator with 10 folds, 3 repeats and a seed of 1.\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=SEED)\n\n    # Calculate AUC for each training round using `cross_val_score()` with model instantiated, data to fit, target variable, 'AUC' scoring, cross validator, n_jobs=-1, and error_score set to 'raise'\n    auc_scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n\n    # Print mean and standard deviation of n_scores: \n    print('Model AUC: %.4f (%.4f)' % (np.mean(auc_scores), np.std(auc_scores)))\n    print()\n    \n    # return auc\n    return auc_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# evaluate Naive Classifier\nnaive_auc = evaluate_model(naive, X=X, y=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Naive Classifier has 50% AUC score.  This is as good as a random guess. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Create a Baseline Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next, we will create a baseline classifier to generate predictions.  We will use a Decision Tree for our baseline classifier.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Instantiate a DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=SEED)\n\n# evaluate Baseline Classifier\ndt_auc = evaluate_model(dt, X=X, y=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! We see a ~19 point increase in AUC by using a Decision Tree. Let's use some more robust models now and see how they perform on the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering - Polynomial Interactions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's start the feature engineering process by creating polynomial interaction features.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# create the transformer\npoly = PolynomialFeatures(include_bias=False, interaction_only=True)\n\n# fit the transformer\npoly_X = poly.fit_transform(X)\n\n# Create a list of features\nfeature_list = poly.get_feature_names(X.columns)\n\n# Create DF\npolyDF = pd.DataFrame(data=poly_X, columns=feature_list)\nprint(f\"New DataFrame columns with Polynomial Features! \\n \\n {polyDF.columns}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Better Models + Hyper-Parameter Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next, let's define some higher-level learners, which include: SVM, Guassian Naive Bayes, Random Forest, XGBoost, and Logistic Regression.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# instantiate list of models to be evaluated: model_list\nmodel_list = [\n    ('SVM', SVC()),\n    ('Bayes', GaussianNB()),\n    ('RF', RandomForestClassifier()),\n    ('XGB', xgb.XGBClassifier()),\n    ('LR', LogisticRegression())\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def best_model(model, param_grid, X, y) -> None:\n    \"\"\"\n    Identify best model by running a Grid Search on model hyper-parameters.\n    \"\"\"\n    pipe = Pipeline([('scaler', StandardScaler()), \n                   ('classifier', model)])\n    \n    # Create random search object using k-fold cv\n    clf = GridSearchCV(pipe, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n\n    # Fit on data\n    best_clf = clf.fit(X, y)\n    best_hyperparams = best_clf.best_estimator_.get_params()['classifier']\n\n    # Print the values used for both Parameters & Score\n    print(best_hyperparams) \n    print()\n    print(\"Best Hyper-parameters: \", best_clf.best_params_)\n    print()\n    print(\"Best AUC Score: \", round(best_clf.best_score_, 4))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def best_model_hyp_params(model_list: list, X: pd.DataFrame, y: np.ndarray) -> None:\n    for model in model_list:\n\n        if model[0] == 'SVM':\n            param_grid = {'classifier__kernel' : ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']} \n\n            # perform hyper-parameter tuning and evaluate model\n            best_model(model=model[1], param_grid=param_grid, X=X, y=y)\n\n        if model[0] == 'Bayes': \n            param_grid = {'classifier__var_smoothing' : np.array([1e-09, 1e-08])} \n\n            # perform hyper-parameter tuning and evaluate model\n            best_model(model=model[1], param_grid=param_grid, X=X, y=y)\n\n        if model[0] == 'RF': \n            param_grid = {'classifier__criterion' : np.array(['gini', 'entropy']),\n                          'classifier__max_depth' : np.arange(3,8)} \n\n            # perform hyper-parameter tuning and evaluate model\n            best_model(model=model[1], param_grid=param_grid, X=X, y=y)\n\n        if model[0] == 'XGB':\n            param_grid = {'classifier__learning_rate' : np.arange(0.022,0.04,.01),\n                          'classifier__max_depth' : np.arange(3,8)} \n\n            # perform hyper-parameter tuning and evaluate model\n            best_model(model=model[1], param_grid=param_grid, X=X, y=y)\n\n        if model[0] == 'LR':\n            param_grid = {'classifier__penalty' : np.array(['l1', 'l2', 'elasticnet', 'none']),\n                          'classifier__C' : np.array([0.001,0.01,0.1,1,10,100])} \n\n            # perform hyper-parameter tuning and evaluate model\n            best_model(model=model[1], param_grid=param_grid, X=X, y=y)\n            \n# run the Grid Search\nbest_model_hyp_params(model_list=model_list, X=poly_X, y=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest & Logistic Regression have the best AUC scores among the models that were trained.  Let's eliminate the Gaussian Naive Bayes Classifier from the competition since it was one of the weakest models.  We still have use for XGBoost when we perform model ensembling.  Let's spend some time to understand these models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will look at the Feature Importance for the top 2 performing models: Random Forest & Logistic Regression.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def rf_feature_importance() -> pd.DataFrame:\n    \"\"\"\n    Plot feature importance for Random Forest Classifier.\n    \"\"\"\n    # fit RandomForestClassifier\n    rfc = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=6, random_state=SEED, n_jobs=-1).fit(poly_X, y)\n\n    # Calculate feature importances\n    feature_importances = rfc.feature_importances_\n\n    # Create a list of features\n    feature_list = poly.get_feature_names(X.columns)\n\n    # Save the results inside a DataFrame using feature_list as an index\n    feature_importances = pd.DataFrame(\n        index=feature_list, \n        data=feature_importances, \n        columns=[\"Feature Importance\"]) \\\n    .sort_values(by=[\"Feature Importance\"], ascending=False)\n    \n    # reset index\n    feature_importances = feature_importances.reset_index()\n\n    # plot barplot\n    pal = sns.color_palette(\"Blues\")\n    plt.figure(figsize=(10,8))\n    sns.barplot(x=\"Feature Importance\", y=\"index\", data=feature_importances,\n                label=\"Total\", color=pal[3])\n    plt.ylabel('')\n    plt.title('Random Forest Feature Importance \\n')\n    plt.tight_layout()\n    plt.show()\n    \n    return feature_importances\n\n# show RF feature importance\nrfc_feat_importanceDF = rf_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def lr_feature_importance() -> pd.DataFrame:\n    \"\"\"\n    Plot feature importance for SVM.\n    \"\"\"\n    # fit LogisticRegression Classifier\n    lr = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', random_state=SEED).fit(poly_X, y)\n\n    # Calculate feature importances\n    feature_importances = abs(lr.coef_[0])\n\n    # Create a list of features\n    feature_list = poly.get_feature_names(X.columns)\n\n    # Save the results inside a DataFrame using feature_list as an index\n    feature_importances = pd.DataFrame(\n        index=feature_list, \n        data=feature_importances, \n        columns=[\"Coefficients\"]) \\\n    .sort_values(by=[\"Coefficients\"], ascending=False)\n    \n    # reset index\n    feature_importances = feature_importances.reset_index()\n\n    # plot barplot\n    pal = sns.color_palette(\"Blues\")\n    plt.figure(figsize=(10,8))\n    sns.barplot(x=\"Coefficients\", y=\"index\", data=feature_importances,\n                label=\"Total\", color=pal[3])\n    plt.ylabel('')\n    plt.title('Logistic Regression Feature Importance  \\n')\n    plt.tight_layout()\n    plt.show()\n    \n    return feature_importances\n\n# show RF feature importance\nlr_feat_importanceDF = lr_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's interesting to see how Logistic Regression & Random Forest rank features with varying levels of importance:\n\n* Random Forest ranks Glucose BMI, Glucose, Glucose Age, Age BMI, & Glucose BloodPressure as the top 5 important features.\n* LogisticRegression ranks Pregancies DiabetesPedigreeFunction, BloodPressure, Pregancies, Age & BMI as the top 5 important features. Logistic Regression also has a number of zero-value coefficients due to L2 regularization.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Modeling - Stacking vs. Voting\n\nNow we will create various types of baseline models, including Logistic Regression using Scikit-Learn, for comparison to ensemble methods.\nWe will build layers and stack them up; We will also use soft voting classifiers. Lastly, we will calculate and visualize the AUC score.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def model_stacking():\n    \"\"\"\n    Create a stacked ML classifier that uses Random Foreset and XGBoost for first layer predictions.\n    Then it uses the Logistic Regression Model to make predictions, based on the predictions from the first layer as features.\n    \"\"\"\n    # Create an empty list for the base models called layer1\n    layer1 = list()\n\n    # Append tuple with classifier name and instantiations (no arguments) for RF, and SVC\n    # Hint: layer1.append(('ModelName', Classifier()))\n    layer1.append(('RF', \n                   RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=6, random_state=SEED)))\n    layer1.append(('LR',\n                   LogisticRegression(C=1.0, penalty='l2', solver='liblinear', random_state=SEED)))\n                   \n\n    # Instantiate Logistic Regression as meta learner model called layer2\n    layer2 = xgb.XGBClassifier(base_score=0.5, booster='gbtree',importance_type='gain',\n                                     learning_rate=0.032, max_depth=3, n_estimators=100, n_jobs=-1, random_state=SEED)\n\n    # Define StackingClassifier() called model passing layer1 model list and meta learner with 5 cross-validations\n    model = StackingClassifier(estimators=layer1, final_estimator=layer2, cv=5, n_jobs=-1)\n\n    # return model\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def model_voting():\n    \"\"\"\n    Create a voting ML classifier that averages the predictions of Random Foreset, XGBoost, and Logistic Regression through a soft vote.\n    \"\"\"\n    # instantiate base learners\n    rf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=6, n_jobs=-1, random_state=SEED)\n    xgb_clf = xgb.XGBClassifier(base_score=0.5, booster='gbtree',importance_type='gain',\n              learning_rate=0.032, max_depth=3, n_estimators=100, n_jobs=-1, random_state=SEED)\n    lr = LogisticRegression(C=1.0, penalty='l2', random_state=SEED, n_jobs=-1)\n    \n    # instantiate Voting Classifier\n    model = VotingClassifier([('RF', rf),\n                            ('XGB', xgb_clf),\n                            ('LR', lr)],\n                           voting='soft', n_jobs=-1)\n    \n    # return model\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_models():\n    \"\"\"\n    Add key:value pairs to dictionary with key as ModelName and value as instantiations for Random Forest, SVM, XGBoost & LogisticRegression\n    as base models, and Stacked models created with custom functions.\n    \"\"\"\n    # Create empty dictionary called models\n    models = dict()\n\n    # Hint: models['ModelName'] = Classifier()\n    models['RF'] = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=6, n_jobs=-1, random_state=SEED)\n    models['XGB'] = xgb.XGBClassifier(base_score=0.5, booster='gbtree',importance_type='gain',\n              learning_rate=0.032, max_depth=3, n_estimators=100, n_jobs=-1, random_state=SEED)\n    models['LR'] = LogisticRegression(random_state=SEED, n_jobs=-1)\n\n    # Add key:value pair to dictionary with key called Stacking and value that calls model_stacking() function\n    models['Stacking'] = model_stacking()\n    \n    # Add key:value pair to dictionary with key called Voting and value that calls model_voting() function\n    models['Voting'] = model_voting()\n    \n    # return dictionary\n    return models\n\n# Assign get_models() to a variable called models\nmodels = create_models()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def evaluate_stacked_models(models: dict) -> None:\n    \"\"\"\n    Evaluate the models and store results.\n    \"\"\"\n    # Create an empty list for the results\n    results = list()\n\n    # Create an empty list for the model names\n    names = list()\n\n    # Create a for loop that iterates over each name, model in models dictionary \n    for name, model in models.items():\n        \n        print(f'{name}: {model}')\n        print()\n\n        # Call evaluate_model(model) and assign it to variable called scores\n        scores = evaluate_model(model=Pipeline([\n            ('scaler', StandardScaler()), \n            ('classifier', model)\n        ]), X=poly_X, y=y)\n        \n        # Append output from scores to the results list\n        results.append(scores)\n        \n        # Append name to the names list\n        names.append(name)\n\n    # Plot model performance for comparison using names for x and results for y and setting showmeans to True\n    fig = plt.figure(figsize=(8,4))\n    \n    sns.boxplot(x=names, y=results, showmeans=True, palette=\"Blues\")\n    plt.title('Model AUC Comparison')\n    plt.show()\n    \n# evaluate stacked models \nevaluate_stacked_models(models=models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix & Classification Report","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Finally let's look at the confusion matrix and classification report.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def classfication_metrics(models: dict) -> None:\n    \"\"\"\n    Evaluate the other classification metrics for the voting classifier\n    \"\"\"\n    # Create a for loop that iterates over each name, model in models dictionary \n    for name, model in models.items():\n\n        print(f'{name}: {model}')\n        print()\n\n        if name == 'Voting':\n            \n            # instantiate the pipeline\n            pipe = Pipeline([\n                ('scaler', StandardScaler()), \n                ('classifier', model)\n            ])\n\n            # split into train & test sets\n            X_train, X_test, y_train, y_test = train_test_split(poly_X, y, test_size=0.2, stratify=y, random_state=SEED)\n\n            # fit the pipeline\n            pipe.fit(X_train, y_train)\n\n            # make predictions\n            y_pred = pipe.predict(X_test)\n\n            # print the confusion matrix\n            print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n            print()\n\n            # print the classification report\n            print(classification_report(y_true=y_test, y_pred=y_pred))\n            print()\n            \n# evaluate stacked models \nclassfication_metrics(models=models)         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Observation\n* Our Voting Classifier got ~84% AUC, which was .004 higher than Logistic Regression. Even if it was a negligible difference, model ensembling still paid off!\n* The voting classifier predicted 82 true positives and 29 true negatives.\n* It also predicted 18 false positives and 25 false negatives.\n\n## Next Steps\n* We could try other models in stacking layer 1\n* We could try other models in stacking layer 2.\n* We could add more hyperparameters to our parameter grid, but this will slow the computation time and add additional expense.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}