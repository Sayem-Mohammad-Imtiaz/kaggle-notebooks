{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Vector Error Correction Model Configuration & Analysis :: original story was published [here](https://sarit-maitra.medium.com/vector-error-correction-model-configuration-analysis-95770699d6a5)**\n\nError correction model (ECM)is important in time-series analysis to better understand long-run dynamics. ECM can be derived from auto-regressive distributed lag model as long as there is a cointegration relationship between variables. In that context, each equation in the vector auto regressive (VAR) model is an autoregressive distributed lag model; therefore, it can be considered that the vector error correction model (VECM) is a VAR model with cointegration constraints."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime, pickle, copy, warnings\nfrom pandas import DataFrame, merge, concat\nimport glob\nfrom matplotlib import pyplot as plt\nplt.style.use('dark_background')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fx-min-data/April_data_6series.csv')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\ndf = df.set_index('timestamp')\ndf.index = pd.to_datetime(df.index)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[:15000] # subset of data\nplt.style.use('dark_background')\ndef plot_vars(train, levels, color, leveltype):\n    \n    \"\"\"\n    Displays historical trends of variables\n    And see if it's sensible to just select levels instead of differences\n    \"\"\"\n    fig, ax = plt.subplots(1, 6, figsize=(16,3), sharex=True)\n    for col, i in dict(zip(levels, list(range(6)))).items():\n        X[col].plot(ax=ax[i], legend=True, linewidth=1.0, color=color, sharex=True)     \n    \n    fig.suptitle(f\"Historical trends of {leveltype} variables\", \n                 fontsize=12, fontweight=\"bold\")\n    \nplot_vars(X.values, levels = X.columns, color=\"red\", leveltype=\"levels\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef adfuller_test(series, signif=0.05, name='', verbose=False):\n    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n    r = adfuller(series, autolag='AIC')\n    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n    p_value = output['pvalue'] \n    def adjust(val, length= 6): return str(val).ljust(length)\n\n    # Print Summary\n    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n    print(f' Significance Level    = {signif}')\n    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n\n    for key,val in r[4].items():\n        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n\n    if p_value <= signif:\n        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n        print(f\" => Series is Stationary.\")\n    else:\n        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n        print(f\" => Series is Non-Stationary.\")\n        \n# ADF Test on each column\nfor name, column in X.iteritems():\n    adfuller_test(column, name=column.name)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import kpss\ndef kpss_test(x, h0_type='c'):\n    indices = ['Test Statistic', 'p-value', '# of Lags']\n    kpss_test = kpss(x, regression=h0_type, nlags ='auto')\n    results = pd.Series(kpss_test[0:3], index=indices)\n    for key, value in kpss_test[3].items():\n        results[f'Critical Value ({key})'] = value\n        return results\nprint('KPSS-EURUSD:')\nprint(kpss_test(X.eurusd))\nprint('___________________')\nprint('KPSS-GBPUSD:')\nprint(kpss_test(X.gbpusd))\nprint('___________________')\nprint('KPSS-USDJPY:')\nprint(kpss_test(X.usdjpy))\nprint('___________________')\nprint('KPSS-GC:')\nprint(kpss_test(X.gc))\nprint('___________________')\nprint('KPSS-NQ:')\nprint(kpss_test(X.nq))\nprint('___________________')\nprint('KPSS-ES:')\nprint(kpss_test(X.es))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nstat,p = stats.normaltest(X.eurusd)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('EURUSD Data looks Gaussian (fail to reject H0)')\nelse:\n    print('EURUSD Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.gbpusd)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('GBPUSD Data looks Gaussian (fail to reject H0)')\nelse:\n    print('GBPUSD Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.usdjpy)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('USDJPY Data looks Gaussian (fail to reject H0)')\nelse:\n    print('USDJPY Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.es)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('ES Data looks Gaussian (fail to reject H0)')\nelse:\n    print('ES Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.nq)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('NQ Data looks Gaussian (fail to reject H0)')\nelse:\n    print('NQ Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.gc)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('GC Data looks Gaussian (fail to reject H0)')\nelse:\n    print('GC Data do not look Gaussian (reject H0)')\nprint('______________')\nprint('EURUSD: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.eurusd)))\nprint('EURUSD: Skewness of normal distribution: {}'. format(stats.skew(X.eurusd)))\nprint('************')\nprint('GBPUSD: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.gbpusd)))\nprint('GBPUSD: Skewness of normal distribution: {}'. format(stats.skew(X.gbpusd)))\nprint('************')\nprint('USDJPY: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.usdjpy)))\nprint('USDJPY: Skewness of normal distribution: {}'. format(stats.skew(X.usdjpy)))\nprint('************')\nprint('ES: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.es)))\nprint('ES: Skewness of normal distribution: {}'. format(stats.skew(df.es)))\nprint('************')\nprint('NQ: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.nq)))\nprint('NQ: Skewness of normal distribution: {}'. format(stats.skew(X.nq)))\nprint('************')\nprint('GC: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.gc)))\nprint('GC: Skewness of normal distribution: {}'. format(stats.skew(X.gc)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization (EURUSD)\npd.options.display.float_format = \"{:.2f}\".format\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['eurusd'].hist(bins=50)\nplt.title('EURUSD')\nplt.subplot(1,2,2)\nstats.probplot(df['eurusd'], plot=plt);\nX['eurusd'].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization (GBPUSD)\npd.options.display.float_format = \"{:.2f}\".format\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['gbpusd'].hist(bins=50)\nplt.title('GBPUSD')\nplt.subplot(1,2,2)\nstats.probplot(df['gbpusd'], plot=plt);\nX['gbpusd'].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization (USDJPY)\npd.options.display.float_format = \"{:.2f}\".format\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['usdjpy'].hist(bins=50)\nplt.title('USDJPY')\nplt.subplot(1,2,2)\nstats.probplot(df['usdjpy'], plot=plt);\nX['usdjpy'].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization (ES)\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['es'].hist(bins=50)\nplt.title('ES')\nplt.subplot(1,2,2)\nstats.probplot(df['es'], plot=plt);\nX['es'].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization (GC)\n\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['gc'].hist(bins=50)\nplt.title('gc')\nplt.subplot(1,2,2)\nstats.probplot(df['gc'], plot=plt);\nX['gc'].describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n# Compute the correlation matrix\ncorr = X.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n# Heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, annot=True, fmt = '.4f', mask=mask, center=0, square=True, linewidths=.5)\nprint(\"value > 0.5 is considerred correlated, > 0.8 is highly correlated\")\nplt.show()\nprint('Correlation matrix:')\ncorr = X.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import grangercausalitytests\nmax_lag = 6\ntest = 'ssr_chi2test'\ndef causation_matrix(data, variables, test='ssr_chi2test', verbose=False):\n    X = DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in X.columns:\n        for r in X.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag = max_lag, verbose = False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(max_lag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            X.loc[r, c] = min_p_value\n    X.columns = [var + '-x axis' for var in variables]\n    X.index = [var + '-y axis' for var in variables]\n    return X\ncausation_matrix(X, variables = X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.vector_ar.vecm import VECM, select_order\nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen\nfrom statsmodels.tsa.vector_ar.vecm import select_coint_rank\nfrom statsmodels.tsa.vector_ar.vecm import CointRankResults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nobs = 15\ntrain_ecm, test_ecm = X[0:-nobs], X[-nobs:]\n\n# Check size\nprint(train_ecm.shape)  \nprint(test_ecm.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VECM model fitting\nfrom statsmodels.tsa.vector_ar import vecm\n# pass \"1min\" frequency\ntrain_ecm.index = pd.DatetimeIndex(train_ecm.index).to_period('1min')\nmodel = vecm.select_order(train_ecm, maxlags=8)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Johansen co-integration\npd.options.display.float_format = \"{:.2f}\".format\n\"\"\"definition of det_orderint:\n-1 - no deterministic terms; 0 - constant term; 1 - linear trend\"\"\"\npd.options.display.float_format = \"{:.2f}\".format\nmodel = coint_johansen(endog = train_ecm, det_order = 1, k_ar_diff = 3)\nprint('Eigen statistic:')\nprint(model.eig) \nprint()\nprint('Critical values:')\nd = DataFrame(model.cvt)\nd.rename(columns = {0:'90%', 1: '95%', 2:'99%'}, inplace=True)\nprint(d); print()\nprint('Trace statistic:')\nprint(DataFrame(model.lr1)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cointegration rank determination\nfrom statsmodels.tsa.vector_ar.vecm import select_coint_rank\nrank1 = select_coint_rank(train_ecm, det_order = 1, k_ar_diff = 3,\n                                   method = 'trace', signif=0.01)\nprint(rank1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rank2 = select_coint_rank(train_ecm, det_order = 1, k_ar_diff = 3, \n                              method = 'maxeig', signif=0.01)\n\nprint(rank2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VECM fitting\n# VECM\nvecm = VECM(train_ecm, k_ar_diff=3, coint_rank = 3, deterministic='ci')\n\"\"\"estimates the VECM on the prices with 3 lags, 3 cointegrating relationship, and \na constant within the cointegration relationship\"\"\"\nvecm_fit = vecm.fit()\nprint(vecm_fit.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Residual auto-correlation\nfrom statsmodels.stats.stattools import durbin_watson\nout = durbin_watson(vecm_fit.resid)\nfor col, val in zip(train_ecm.columns, out):\n    print((col), ':', round(val, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impulse-response plot\nfrom statsmodels.tsa.vector_ar import irf\nirf = vecm_fit.irf(15)\nirf.plot(orth = False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nirf.plot(impulse='eurusd')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nirf.plot(impulse='usdjpy', orth = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nirf.plot(impulse='es')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nirf.plot(impulse='gc')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nirf.plot(impulse='nq')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\npd.options.display.float_format = \"{:.2f}\".format\nforecast, lower, upper = vecm_fit.predict(nobs, 0.05)\nprint(\"lower bounds of confidence intervals:\")\nprint(DataFrame(lower.round(2)))\nprint(\"\\npoint forecasts:\")\nprint(DataFrame(forecast.round(2)))\nprint(\"\\nupper bounds of confidence intervals:\")\nprint(DataFrame(upper.round(2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = \"{:.2f}\".format\nforecast = DataFrame(forecast, index= test_ecm.index, columns= test_ecm.columns)\nforecast.rename(columns = {'eurusd':'eurusd_pred', 'gbpusd':'gbpusd_pred', 'usdjpy':'usdjpy_pred',\n                    'gc':'gc_pred', 'nq':'nq_pred', 'es':'es_pred'}, inplace = True)\nforecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = concat([test_ecm, forecast], axis=1)\npred = combine[['eurusd', 'eurusd_pred', 'gbpusd', 'gbpusd_pred', 'usdjpy', \n                   'usdjpy_pred', 'gc', 'gc_pred', 'nq', 'nq_pred', 'es', 'es_pred']]\ndef highlight_cols(s):\n    color = 'yellow'\n    return 'background-color: %s' % color\n\npred.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_pred', 'gbpusd_pred', 'usdjpy_pred',\n                                                               'gc_pred', 'nq_pred', 'es_pred']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# score eur_usd\nmae = mean_absolute_error(pred.eurusd, pred['eurusd_pred'])\nmse = mean_squared_error(pred.eurusd, pred.eurusd_pred)\nrmse = np.sqrt(mse)\nsum = DataFrame(index = ['Mean Absolute Error', 'Mean squared error', 'Root mean squared error'])\nsum['Accuracy metrics :    EURUSD'] = [mae, mse, rmse]\n\n# score gbp_usd\nmae = mean_absolute_error(pred.gbpusd, pred['gbpusd_pred'])\nmse = mean_squared_error(pred.gbpusd, pred.gbpusd_pred)\nrmse = np.sqrt(mse)\nsum['GBPUSD'] = [mae, mse, rmse]\n\n# score usd_jpy\nmae = mean_absolute_error(pred.usdjpy, pred['usdjpy_pred'])\nmse = mean_squared_error(pred.usdjpy, pred.usdjpy_pred)\nrmse = np.sqrt(mse)\nsum['USDJPY'] = [mae, mse, rmse]\n\n# score nq\nmae = mean_absolute_error(pred.nq, pred['nq_pred'])\nmse = mean_squared_error(pred.nq, pred.nq_pred)\nrmse = np.sqrt(mse)\nsum['NQ'] = [mae, mse, rmse]\n\n# score usd_jpy\nmae = mean_absolute_error(pred.es, pred['es_pred'])\nmse = mean_squared_error(pred.es, pred.es_pred)\nrmse = np.sqrt(mse)\nsum['ES'] = [mae, mse, rmse]\n\n# score usd_jpy\nmae = mean_absolute_error(pred.gc, pred['gc_pred'])\nmse = mean_squared_error(pred.gc, pred.gc_pred)\nrmse = np.sqrt(mse)\nsum['GC'] = [mae, mse, rmse]\nsum","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}