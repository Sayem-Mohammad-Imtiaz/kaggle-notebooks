{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3347dcc4-8290-1db6-b8ac-6a496db0e465"},"source":"# Attempt to predict game sales in NA"},{"cell_type":"markdown","metadata":{"_cell_guid":"63482499-12a3-6d1c-9423-f80cc3b2f8e9"},"source":"# Attempt to predict game sales in North America\n## Overview\nThe following code is a predictive analysis of the data in the 'vgsales.csv' file. The file is available on Kaggle.com.\nUsing a set of features of interest (namely, 'Rank', 'Genre', 'Platform', 'Year', 'Publisher', 'EU_Sales', and 'JP_Sales', 'Other_Sales') a predictive model has been built to estimate the value of an outcome of interest (namely, 'NA_Sales).\n## Possible applications of similar models\nSimilar models could be used by manufacturers and vendors to estimate the number of games copies to be produced and stocked respectively, and what the profits of sales may be."},{"cell_type":"markdown","metadata":{"_cell_guid":"32a96120-df5d-f782-f6c1-833cd3d188ae"},"source":"### The following block of code loads the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5afb06fa-f175-7037-9511-0bf62ea2ae0d"},"outputs":[],"source":"import numpy as np \nimport pandas as pd\nfrom IPython.display import display\n%matplotlib inline\n\ntry:\n    data = pd.read_csv('../input/vgsales.csv')\n    print ('Dataset loaded...')\nexcept:\n    print ('Unable to load dataset...') "},{"cell_type":"markdown","metadata":{"_cell_guid":"04ffb368-46a0-9701-2902-b6d68753e7de"},"source":"### Display the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23255d18-6936-9df7-cba2-89128f4ca763"},"outputs":[],"source":"display(data[:10])   "},{"cell_type":"markdown","metadata":{"_cell_guid":"1babc10a-ff72-1903-5f97-1d779f419b34"},"source":"### Processing data\nTo simplify the analysis process, entries whose 'Year' feature value is missing have been deleted from the DataFrame. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba736dc5-f418-9631-155a-89da93a44b82"},"outputs":[],"source":"data = data[np.isfinite(data['Year'])]"},{"cell_type":"markdown","metadata":{"_cell_guid":"6f1ad498-86fd-c965-b02f-9c4fba6e94a6"},"source":"### Setting our Y-value (to-be-predicted) and our X-value (features)\nIn the following block of code, I set our 'y-value' column under the variable name 'naSales'. We are interested in predicting this value. Additionally, I set the 'x-value' columns under the variable name 'features'. It is these features that we will use to predict our naSales values. The 'features' variable will store the following columns of the dataframe: 'Rank', 'Genre', 'Platform', 'Year', 'Publisher', 'EU_Sales', and 'JP_Sales', 'Other_Sales'. I am not including the 'Global_Sales' column in 'features' b/c its inclusion would reduce our problem of predicting naSales to a simple subtraction problem. naSales would simply equal 'Global_Sales' - 'EU_Sales' - 'JP_Sales' - 'Other_Sales'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64ed24c1-3e8d-8092-da2e-f4191ce7c674"},"outputs":[],"source":"naSales = data['NA_Sales']\nfeatures = data.drop(['Name', 'Global_Sales', 'NA_Sales'], axis = 1)\n\n# Displaying our features and target columns... \ndisplay(naSales[:5])\ndisplay(features[:5])"},{"cell_type":"markdown","metadata":{"_cell_guid":"d3727154-2400-ffd6-6be0-75d107b6778f"},"source":"### Principal Component Analysis\nThe 'EU_Sales', 'JP_Sales', 'Other_Sales' likely are observables driven by an underlying latent feature. I am herein performing a Principal Component Analysis on these three features to obtain one underlying latent feature."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a38bbcc-b5b5-bede-4ff2-4d7bb9d5fb11"},"outputs":[],"source":"# Firstly, I am dividing the features data set into two as follows. \n\nsalesFeatures = features.drop(['Rank', 'Platform', 'Year', 'Genre', 'Publisher'], \n                              axis = 1)\notherFeatures = features.drop(['EU_Sales', 'JP_Sales', 'Other_Sales', 'Rank'], \n                              axis = 1)\n\n# Secondly, I am obtaining the PCA transformed features...\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 1)\npca.fit(salesFeatures)\nsalesFeaturesTransformed = pca.transform(salesFeatures)\n\n# Finally, I am merging the new transfomed salesFeatures \n# (...cont) column back together with the otherFeatures columns...\n\nsalesFeaturesTransformed = pd.DataFrame(data = salesFeaturesTransformed, \n                                        index = salesFeatures.index, \n                                        columns = ['Sales'])\nrebuiltFeatures = pd.concat([otherFeatures, salesFeaturesTransformed], \n                            axis = 1)\n\ndisplay(rebuiltFeatures[:5])"},{"cell_type":"markdown","metadata":{"_cell_guid":"cb1ae49e-a3e3-bdf6-6a92-04250582d6ac"},"source":"### Processing our data\nMost Machine Learning models expect numeric values. The following block of code converts non-numeric values into numeric values by adding dummy variable columns. For example, the 'Genre' feature with say, 2 values, namely 'a' and 'b' would be divided into 2 features: 'Genre_a' and 'Genre_b', each of which would take binary values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2aebf5a1-2d1f-a908-54bc-88e0b69e0acf"},"outputs":[],"source":"# This code is inspired by udacity project 'student intervention'.\ntemp = pd.DataFrame(index = rebuiltFeatures.index)\n\nfor col, col_data in rebuiltFeatures.iteritems():\n    \n    if col_data.dtype == object:\n        col_data = pd.get_dummies(col_data, prefix = col)\n        \n    temp = temp.join(col_data)\n    \nrebuiltFeatures = temp\ndisplay(rebuiltFeatures[:5])"},{"cell_type":"markdown","metadata":{"_cell_guid":"5508b6c2-f6cf-8f89-a527-e8165972ce4f"},"source":"### Dividing our data into Training and Testing sets. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f574a89-aeb3-f8aa-f61c-564053583e22"},"outputs":[],"source":"# Dividing the data into training and testing sets...\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(rebuiltFeatures, \n                                                    naSales, \n                                                    test_size = 0.2, \n                                                    random_state = 2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7568fcfb-866e-f518-9d7e-e418e721ad7c"},"source":"### Model Selection\nI believe Decision Tree Regression and K-Neighbors Regression will fit the data well. I herein build both these models and analyze the results to ascertain the better of the two. The metric I am using to guage the 'goodness' of the model is the R-squared score. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"642c64a9-782b-c982-568b-348dcbeef95a"},"outputs":[],"source":"# Creating & fitting a Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nregDTR = DecisionTreeRegressor(random_state = 4)\nregDTR.fit(X_train, y_train)\ny_regDTR = regDTR.predict(X_test)\n\nfrom sklearn.metrics import r2_score\nprint ('The following is the r2_score on the DTR model...')\nprint (r2_score(y_test, y_regDTR))\n\n# Creating a K Neighbors Regressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nregKNR = KNeighborsRegressor()\nregKNR.fit(X_train, y_train)\ny_regKNR = regKNR.predict(X_test)\n\nprint ('The following is the r2_score on the KNR model...')\nprint (r2_score(y_test, y_regKNR))"},{"cell_type":"markdown","metadata":{"_cell_guid":"17b15bd5-fe4c-ca16-8d7c-a28bb7277613"},"source":"The above results show that the Decision Tree Regression model is the better of the two with a superior R-squared score. "},{"cell_type":"markdown","metadata":{"_cell_guid":"4d21f83a-0fba-b427-32d3-44de09e87a90"},"source":"### Optimizing the Decision Tree Regression Model\nThe following block of code optimizes the parameters of the DTR model. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"799c02da-878d-eab6-b741-e478ae007396"},"outputs":[],"source":"# This code is inspired by udacity project 'student intervention'\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.cross_validation import ShuffleSplit\ncv_sets = ShuffleSplit(X_train.shape[0], n_iter = 10, \n                       test_size = 0.2, random_state = 2)\nregressor = DecisionTreeRegressor(random_state = 4)\nparams = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n          'splitter': ['best', 'random']}\nscoring_func = make_scorer(r2_score)\n    \ngrid = GridSearchCV(regressor, params, cv = cv_sets, \n                    scoring = scoring_func)\ngrid = grid.fit(X_train, y_train)\n\noptimizedReg = grid.best_estimator_\ny_optimizedPrediction = optimizedReg.predict(X_test)\n\nprint ('The r2_score of the optimal regressor is:')\nprint (r2_score(y_test, y_optimizedPrediction))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7702a0a-1597-e014-c6d4-a3451eb2e085"},"source":"Strangely enough, the optimization code does not yield better results than the default model. It could be that the model parameters I have selected to optimize are not the right ones."},{"cell_type":"markdown","metadata":{"_cell_guid":"db433dc7-73f1-153a-0d0f-3e71ecfd73eb"},"source":"## Conclusion\n\nIn conclusion, we were able to build a model that estimates the target values given theselected features set. Our Decision Tree Regression model performed fairly well with an R squared score of 0.65 ~ 0.7."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"007c523a-2bb6-6f29-465a-c1f1e777eb9c"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}