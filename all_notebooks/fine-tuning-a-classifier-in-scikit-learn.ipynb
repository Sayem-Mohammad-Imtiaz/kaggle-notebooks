{"cells":[{"metadata":{"_cell_guid":"ea8ecc6c-5022-46b8-a7e9-7410b38c1cb2","_uuid":"0c55a74a598b766c5c5732cdbac49c82ac288197"},"cell_type":"markdown","source":"Itâ€™s easy to understand that many machine learning problems benefit from either precision or recall as their optimal performance metric but implementing the concept requires knowledge of a detailed process. My first few attempts to fine tune models for recall (sensitivity) were difficult, so I decided to share my experience.\n\nThis is my first Kaggle kernel, my aim wasn't to build the most robust classifier, I just wanted to show the practicality of optimizing for sensitivity.  In figure A below, I'd like to move the decision threshold to left to minimize the amount of false negatives, which would be especially important in cancer diagnoses."},{"metadata":{"_cell_guid":"607a29eb-08ce-45b4-ad4f-0c83f47010f5","_uuid":"8c4e6d9df09bce759312b32a198c9533d7a2baed"},"cell_type":"markdown","source":"![](https://c1.staticflickr.com/5/4340/37157583241_7cc603070c_z_d.jpg)"},{"metadata":{"_cell_guid":"9a923f87-32dc-411e-869f-3a4ce886f76e","_uuid":"c0d309adce2ddc1e80af7c3cc18f8bf6aed71900"},"cell_type":"markdown","source":"Tuning a classifier for maximum sensitivity or specificity can be achieved in (at least) two main steps. The first is using `GridSearchCV` to fine tune your model and keep the classifier with the highest recall score. The second step is to adjust the decision threshold using the precision recall curve and the roc curve."},{"metadata":{"_cell_guid":"57410405-55b5-4b5a-88ef-8fe0d33e72f3","_uuid":"ab79ce36de0bf9c4156ed0ff4044ac9c9d679110","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\ndf = pd.read_csv('../input/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bda92e56-7348-4d0c-a7a8-bc541b67ac52","_uuid":"485654bc4b58061c390bd2f578422f7949caddc0","trusted":true},"cell_type":"code","source":"# class distribution\n# diagnosis: B = 0, M = 1\ndf['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8e586b28-8d91-453a-9418-c7d07d4b95ac","_uuid":"8cc1fd44af27055f852a2142c0203026c2f6ee23","trusted":true},"cell_type":"code","source":"# by default majority class (benign) will be negative\nlb = LabelBinarizer()\ndf['diagnosis'] = lb.fit_transform(df['diagnosis'].values)\ntargets = df['diagnosis']\n\ndf.drop(['id', 'diagnosis', 'Unnamed: 32'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"429b11e6-7d16-4546-9ed7-48babbb5800f","_uuid":"fe87b1941c481b23f6acadf58a4148ac14d6d354","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df, targets, stratify=targets)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d4a5198-d32f-4956-a46a-f3702361b4e2","_uuid":"d886519b75bcc7eee4cabbaaee6c0da8a8b65475"},"cell_type":"markdown","source":"`train_test_split` with `stratify=True` results in consistent class distribution betwen training and test sets."},{"metadata":{"_cell_guid":"5dd6b7bf-11f0-481f-b123-8798d62eed6a","_uuid":"61d123734c5a86d4b83d71475f3c12db9e6f584d","trusted":true},"cell_type":"code","source":"print('y_train class distribution')\nprint(y_train.value_counts(normalize=True))\n\nprint('y_test class distribution')\nprint(y_test.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"318d6dd7-1f22-413f-9148-8083febc41da","_uuid":"6fd5b08d87bbd5f06c0f8a8d5d274426d544bf28"},"cell_type":"markdown","source":"## First strategy: Optimize for sensitivity using GridSearchCV and scoring."},{"metadata":{"_cell_guid":"c36e38fb-df36-4740-ad49-3ca2d8fafb3b","_uuid":"767cc162a12d482aec2e6be24ad3d212f82190f0"},"cell_type":"markdown","source":"First build a generic classifier and setup a parameter grid; random forests have many tunable parameters, which make it suitable for `GridSearchCV`."},{"metadata":{"_cell_guid":"cb16feea-423e-43ed-9583-a577d499142a","_uuid":"9e4c633290fe806cb8194070ea196441abc835ab","trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_jobs=-1)\n\nparam_grid = {\n    'min_samples_split': [3, 5, 10], \n    'n_estimators' : [100, 300],\n    'max_depth': [3, 5, 15, 25],\n    'max_features': [3, 5, 10, 20]\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92668296-155e-4e28-aa58-72c448f83834","_uuid":"27057b01b48a5747f94f96efa209421ea4884563"},"cell_type":"markdown","source":"The `scorers` dictionary can be used as the `scoring` argument in `GridSearchCV`. When multiple scores are passed, `GridSearchCV.cv_results_` will return scoring metrics for each of the score types provided."},{"metadata":{"_cell_guid":"bfb096b1-6288-487d-98a8-815ad61d40a9","_uuid":"11117105496cb381cbde94a0349d01598603b950","trusted":true},"cell_type":"code","source":"scorers = {\n    'precision_score': make_scorer(precision_score),\n    'recall_score': make_scorer(recall_score),\n    'accuracy_score': make_scorer(accuracy_score)\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2b400f7-957d-480f-b142-d43fe2cc4eb2","_uuid":"2a4221d83434d34b3f455f3cb6f0f340958d5811"},"cell_type":"markdown","source":"The function below uses  [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to fit several classifiers according to the combinations of parameters in the `param_grid`.  The scores from `scorers` are recorded and the best model (as scored by the `refit` argument) will be selected and \"refit\" to the full training data for downstream use.  This also makes predictions on the held out `X_test` and prints the confusion matrix to show performance.\n\nThe point of the wrapper function is to quickly reuse the code to fit the best classifier according to the type of scoring metric chosen. First, try `precision_score`, which should limit the number of false positives. This isn't well-suited for the goal of maxium sensitivity, but allows us to quickly show the difference between a classifier optimized for `precision_score` and one optimized for `recall_score`."},{"metadata":{"_cell_guid":"3038eab0-7e9b-4759-b790-02624dddeb04","_uuid":"08ce5c5a78ba5256d4bccb6bf94118dcbbbfeebc","trusted":true},"cell_type":"code","source":"def grid_search_wrapper(refit_score='precision_score'):\n    \"\"\"\n    fits a GridSearchCV classifier using refit_score for optimization\n    prints classifier performance metrics\n    \"\"\"\n    skf = StratifiedKFold(n_splits=10)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n                           cv=skf, return_train_score=True, n_jobs=-1)\n    grid_search.fit(X_train.values, y_train.values)\n\n    # make the predictions\n    y_pred = grid_search.predict(X_test.values)\n\n    print('Best params for {}'.format(refit_score))\n    print(grid_search.best_params_)\n\n    # confusion matrix on the test data.\n    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n    return grid_search","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9960594-ace8-4994-9382-bd4ffb3e72d0","_uuid":"be2d45f775d6b910381ec822c75baf49d4341f0f","trusted":true},"cell_type":"code","source":"grid_search_clf = grid_search_wrapper(refit_score='precision_score')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"520c9595-79e3-42b6-9059-0705c91b8823","_uuid":"4a16d957e79a15c7444da720188fb1e2f56e1172"},"cell_type":"markdown","source":"The precision, recall, and accuracy scores for every combination of the parameters in `param_grid` are stored in `cv_results_`.  Here, a pandas DataFrame helps visualize the scores  and parameters for each classifier iteration. This is included to show that although accuracy may be relatively consistent across classifiers, it's obvious that precision and recall have a trade-off. Sorting by precision, the best scoring model should be the first record. This can be checked by looking at the parameters of the first record and comparing them to `grid_search.best_params_` above."},{"metadata":{"_cell_guid":"4fc2f434-af40-41e2-be22-fee13f58ab9f","_uuid":"7d7c77701efe3ba95c595c0585f641172a1c5a08","trusted":true},"cell_type":"code","source":"results = pd.DataFrame(grid_search_clf.cv_results_)\nresults = results.sort_values(by='mean_test_precision_score', ascending=False)\nresults[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score',\n         'param_max_depth', 'param_max_features', 'param_min_samples_split',\n         'param_n_estimators']].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0060d701-d6e6-47e4-9015-87689106ef16","_uuid":"36de528cb44a0a5b1bfc064e93c6addf77d92476"},"cell_type":"markdown","source":"That classifier was optimized for precision. For comparison, to show how `GridSearchCV` selects the best classifier, the function call below returns a classifier optimized for recall. The grid might be similar to the grid above, the only difference is that the classifer with the highest recall will be refit. This will be the most desirable metric in the cancer diagnosis classification problem, there should be less false negatives on the test set confusion matrix."},{"metadata":{"_cell_guid":"1a25b8bd-1b99-4fd6-bf4a-c7322f442563","_uuid":"d7c2d22bc8cae2deb66ff094857a679854441bbd","trusted":true},"cell_type":"code","source":"grid_search_clf = grid_search_wrapper(refit_score='recall_score')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6cc02e6c-f783-4a30-9aea-ba0e4d756b95","_uuid":"f1c15aef1aa6882da79861bac17aa929edd4facd","trusted":true},"cell_type":"code","source":"results = pd.DataFrame(grid_search_clf.cv_results_)\nresults = results.sort_values(by='mean_test_recall_score', ascending=False)\nresults[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score',\n         'param_max_depth', 'param_max_features', 'param_min_samples_split',\n         'param_n_estimators']].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce2d4b72-3ad9-4ab6-af8a-b3f21de4e586","_uuid":"b433327a3ceb89328524cbcd87fc6ab24e1f5fde"},"cell_type":"markdown","source":"The first strategy doesn't yield impressive results for `recall_score`, it doesn't significantly reduce (if at all) the number of false negatives compared to the classifier optimized for `precision_score`. Ideally, when designing a cancer diagnosis test, the classifier should strive for the fewest false negatives as possible.\n\n## Strategy 2: Adjust the decision threshold -- Identify the operating point\n\nThe `precisoin_recall_curve` and `roc curve` are useful tools to visualize the sensitivity-specificty tradeoff in the classifier. They can help inform a data scientist where to set the decision threshold of the model to maximize either sensitivity or specificity. This is called the \"operating point\" of the model.\n\nAn important point to make this method generalizable to all classifiers in scikit-learn is to understand that some classifiers (like RandomForest) use `.predict_proba()` while others (like SVC) use `.decision_function()`. The idea is to get the \"probability\" that a sample is predicted to be in a class, not just the class returned from `.predict()`. The default threshold for `RandomForestClassifier` is 0.5"},{"metadata":{"_cell_guid":"9c98e87e-ecab-4f97-a599-a10cfd85f6f1","_uuid":"22122155a870d1e8d2114960e0b95f9626eee985","trusted":true},"cell_type":"code","source":"# this gives the probability [0,1] that each sample belongs to class 1\ny_scores = grid_search_clf.predict_proba(X_test)[:, 1]\n\n# for classifiers with decision_function, this achieves similar results\n# y_scores = classifier.decision_function(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"413c2785-506f-4ec7-9984-1d57f3237395","_uuid":"8a608213559fa37769c302574ca8ae3e1221cf89","trusted":true},"cell_type":"code","source":"def adjusted_classes(y_scores, t):\n    \"\"\"\n    This function adjusts class predictions based on the prediction threshold (t).\n    Will only work for binary classification problems.\n    \"\"\"\n    return [1 if y >= t else 0 for y in y_scores]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f55727f6-0bea-4acd-aa21-62722b8462db","scrolled":false,"_uuid":"c9659575b6b0c74e2c99347831ee6f6acbae0e64","trusted":true},"cell_type":"code","source":"# generate the precision recall curve\np, r, thresholds = precision_recall_curve(y_test, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a3471b20-89df-411e-96e7-5daa2f7d5d78","_uuid":"d7104daa6f106bbb6a04f92ec12b68d45f6ccf5b","trusted":true},"cell_type":"code","source":"def precision_recall_threshold(t=0.5):\n    \"\"\"\n    plots the precision recall curve and shows the current value for each\n    by identifying the classifier's threshold (t).\n    \"\"\"\n    \n    # generate new class predictions based on the adjusted_classes\n    # function above and view the resulting confusion matrix.\n    y_pred_adj = adjusted_classes(y_scores, t)\n    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),\n                       columns=['pred_neg', 'pred_pos'], \n                       index=['neg', 'pos']))\n    \n    # plot the curve\n    plt.figure(figsize=(8,8))\n    plt.title(\"Precision and Recall curve ^ = current threshold\")\n    plt.step(r, p, color='b', alpha=0.2,\n             where='post')\n    plt.fill_between(r, p, step='post', alpha=0.2,\n                     color='b')\n    plt.ylim([0.5, 1.01]);\n    plt.xlim([0.5, 1.01]);\n    plt.xlabel('Recall');\n    plt.ylabel('Precision');\n    \n    # plot the current threshold on the line\n    close_default_clf = np.argmin(np.abs(thresholds - t))\n    plt.plot(r[close_default_clf], p[close_default_clf], '^', c='k',\n            markersize=15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c1a856c8-35b7-44d6-8050-13d78d383806","_uuid":"e2478a15b2c4ff94eed73eb28889a3885b886606"},"cell_type":"markdown","source":"Re-execute this cell to tune the threshold until there are 0 False Negatives. On this particular run, I had to go all the way down to 0.0 before reducing the false negatives to 0. Unfortunately this means I predicted everything positive!\n"},{"metadata":{"_cell_guid":"98ea772b-5bcd-41b6-91ef-c5b538d5a0ca","_uuid":"f49acf4e2a6ef8d369afd633304e283bdb42e2ed","trusted":true},"cell_type":"code","source":"# The best I could do with 1 FN was 0.17, but re-execute to watch the confusion matrix change.\nprecision_recall_threshold(0.17)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"235abd58-698e-4f2a-8056-3c5ee37b0454","_uuid":"ff3b6ed0c6f9adb9a465378a9ddea851160db3d8"},"cell_type":"markdown","source":"Another way to view the tradeoff between precision and recall is to plot them together as a function of the decision threshold."},{"metadata":{"_cell_guid":"0cd85cbc-26b2-43e7-ba6b-5226f245c6af","_uuid":"66958866b24f4c2c0810620340627e16165f0c0c","trusted":true},"cell_type":"code","source":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    \"\"\"\n    Modified from:\n    Hands-On Machine learning with Scikit-Learn\n    and TensorFlow; p.89\n    \"\"\"\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3cd3e430-4a96-4c33-b522-e83661f042c6","_uuid":"2331540f9da13c541941a68a8963c4eb95cc1451","trusted":true},"cell_type":"code","source":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de395609-cb7b-474d-b687-cbe2d61cf38a","_uuid":"4a6dbd8267039122b8a616a9e1eaa647cee08ef0"},"cell_type":"markdown","source":"Finally, the ROC curve shows that to achieve a 1.0 recall, we must accept some false positive rate > 0.0."},{"metadata":{"_cell_guid":"e1169250-1f5e-4ceb-a289-a28cc81b3ed1","_uuid":"24fad58f761cb0c40e434a97d4365822bfc8afad","trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    \"\"\"\n    The ROC curve, modified from \n    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n    \"\"\"\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n    plt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3cebf16-40b0-4dc2-bb41-74436a7aa695","_uuid":"b1144608c9dbc583b5fd72744dfa37072a5bc964","trusted":true},"cell_type":"code","source":"fpr, tpr, auc_thresholds = roc_curve(y_test, y_scores)\nprint(auc(fpr, tpr)) # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2872637b-6240-4e40-86cd-0adbfc5470cc","_uuid":"173b72ee14b1d9543e0f4bb1d0a2bb2630a38051"},"cell_type":"markdown","source":"Thanks for following along. I'm interested to hear suggestions to improve the code and/or the classifiers."}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}