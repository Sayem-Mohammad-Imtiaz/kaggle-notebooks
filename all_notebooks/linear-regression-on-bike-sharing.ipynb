{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Bike Sharing Case Study"},{"metadata":{},"cell_type":"markdown","source":"#### Problem Statement:\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \nEssentially, the company wants â€”\n\n\n- To identify the variables which are significant in predicting the demand for shared bikes.\n\n- To create a linear model that quantitatively relates how well those variables describe the bike demands\n\n- To know the accuracy of the model, i.e. how well these variables can predict bikes demand.\n\n**So interpretation is important!**"},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Reading and Understanding the Data\n\nLet us first import NumPy and Pandas and read the housing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day = pd.read_csv(\"../input/boom-bike-dataset/bike_sharing_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspect the various aspects of the housing dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"day.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Visualising the Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(day)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualising Categorical Variables\n\nAs you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = day)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'season', y = 'cnt', data = day)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'mnth', y = 'cnt', data = day)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = day)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = day)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Dropping unnecessary variables - 'instant', 'dteday', 'casual', 'registered'"},{"metadata":{"trusted":true},"cell_type":"code","source":"day.drop(['instant','dteday', 'casual', 'registered'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy Variables"},{"metadata":{},"cell_type":"markdown","source":"The variable 'season', 'mnth', 'weekday', 'weathersit' has multiple levels.\n\nFor this, we will use dummy variables."},{"metadata":{},"cell_type":"markdown","source":"The variable 'season' has four levels\n- `1` will correspond to `spring`\n- `2` will correspond to `summer`\n- `3` will correspond to `fall`\n- `4` will correspond to `winter`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop the first column from season using 'drop_first = True'\nseason_status = pd.get_dummies(day['season'], drop_first = True)\n\nseason_status = season_status.rename(columns ={ 1:'spring',2:'summer',\n                                                3:'fall',\n                                                4:'winter'})\n# Add the results to the original housing dataframe\nday = pd.concat([day, season_status], axis = 1)\n\n# Now let's see the head of our dataframe.\nday.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping 'season' as we have created the dummies for it\nday.drop(['season'], axis = 1, inplace = True)\n\nday.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable 'mnth' has 12 levels from 1 to 12\n\n- 1 will correspond to January .....\n- 12 will correspond to December\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop the first column from month using 'drop_first = True'\nmonth = pd.get_dummies(day['mnth'], drop_first = True)\n\nmonth = month.rename(columns ={ 1:'January',2:'February',3:'March',4:'April',5:'May',6:'June',7:'July',8:'August',9:'September',10:'October',11:'November',12:'December'})\n# Add the results to the original housing dataframe\nday = pd.concat([day, month], axis = 1)\n\n# Dropping 'mnth' as we have created the dummies for it\nday.drop(['mnth'], axis = 1, inplace = True)\n\n# Now let's see the head of our dataframe.\nday.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable 'weekday' has 7 levels from 0 to 6\n\n- 0 will correspond to Sunday \n- 1 will correspond to Monday\n.\n.\n.\n- 6 will correspond to Saturday\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop the first column from weekday using 'drop_first = True'\nnew_weekday = pd.get_dummies(day['weekday'], drop_first = True)\n\nnew_weekday = new_weekday.rename(columns ={ 0:'Sunday',1:'Monday',2:'Tuesday',3:'Wednesday',4:'Thursday',5:'Friday',6:'Saturday'})\n\n# Add the results to the original housing dataframe\nday = pd.concat([day, new_weekday], axis = 1)\n\n# Dropping 'weekday' as we have created the dummies for it\nday.drop(['weekday'], axis = 1, inplace = True)\n\n# Now let's see the head of our dataframe.\nday.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable 'weathersit' has 4 levels\n\n- 1 will correspond to Clear, Few clouds, Partly cloudy, Partly cloudy \n- 2 will correspond to Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3 will correspond to Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4 will correspond to Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop the first column from weekday using 'drop_first = True'\nweathersituation = pd.get_dummies(day['weathersit'], drop_first = True)\n\nweathersituation = weathersituation.rename(columns ={ 1:'Clear',2:'Mist',3:'Light Snow',4:'Heavy Rain'})\n\n# Add the results to the original housing dataframe\nday = pd.concat([day, weathersituation], axis = 1)\n\n# Dropping 'weekday' as we have created the dummies for it\nday.drop(['weathersit'], axis = 1, inplace = True)\n\n# Now let's see the head of our dataframe.\nday.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Splitting the Data into Training and Testing Sets\n\nThe first basic step for regression is performing a train-test split."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(day, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the Features\nWe will use MinMax scaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['temp','atemp', 'hum', 'windspeed', 'cnt' ]\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (26, 15))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As noticed here, 'atemp' seems to be correlated to 'cnt' the most. Let's see a pairplot for 'atemp' vs 'cnt'."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[6,6])\nplt.scatter(df_train.atemp, df_train.cnt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we pick 'atemp' as the first variable and we'll try to fit a regression line to that."},{"metadata":{},"cell_type":"markdown","source":"### Dividing into X and Y sets for the model building\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('cnt')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Building a model using RFE\n\nUsing the LinearRegression function from SciKit Learn for its compatibility with RFE(Recursive feature elimination)\n\n### RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with output number of variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train,y_train)\n\nrfe = RFE(lm,10)    # running RFE\nrfe = rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model using statsmodel, for the detailed statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable\nimport statsmodels.api as sm\nX_train_rfe = sm.add_constant(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_rfe).fit()  # Running the Linear Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the Linear Model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by=\"VIF\", ascending = False)\nvif\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_cnt = lm.predict(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6 - Making Predictions"},{"metadata":{},"cell_type":"markdown","source":"#### Applying the scaling on the test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['temp','atemp','hum', 'windspeed', 'cnt' ]\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dividing into X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('cnt')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's use our model to make predictions.\n\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test)\n\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test_new[X_train_rfe.columns]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_pred = lm.predict(X_test_new)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 7: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can see that the equation of our best fitted line is:\n\n$ cnt = 0.2265  \\times  yr - 0.0897  \\times  holiday + 0.5666  \\times temp - 0.2864 \\times hum - 0.2014 \\times windspeed + 0.1002 \\times summer + 0.1521 \\times winter + 0.0494 \\times August + 0.1187 \\times September - 0.1917 \\times Light Snow $\n"},{"metadata":{},"cell_type":"markdown","source":"'temp','yr','winter' are some of the  variables which help to increse the count significantly."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}