{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Tabular Playground Series - Feb 2021](https://www.kaggle.com/c/tabular-playground-series-feb-2021/overview)","metadata":{}},{"cell_type":"markdown","source":"# Learn Boosting Techniques through TPS Feb 2021 ([Link](https://www.kaggle.com/c/tabular-playground-series-feb-2021/submit))","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we will learn everything about the popular Gradient Boosting Technique known as XGBoost. We will see how far up the leaderboard can just an XGBoost model take us. We will focus on understanding the hyperparameters  how to tune them. ","metadata":{}},{"cell_type":"markdown","source":"# How is XGBoost different from vanilla Gradient Boosting?","metadata":{}},{"cell_type":"markdown","source":"WIP","metadata":{}},{"cell_type":"markdown","source":"# Import Libraires & Read Data","metadata":{}},{"cell_type":"code","source":"# Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport altair as alt\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn import compose\nfrom sklearn import decomposition\nfrom sklearn import pipeline\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\n\n# Settings\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 1000)\n\n# Read Data\nPATH = '/kaggle/input/tabular-playground-series-feb-2021'\ntrain = pd.read_csv(f'{PATH}/train.csv')\ntest = pd.read_csv(f'{PATH}/test.csv')\nss = pd.read_csv(f'{PATH}/sample_submission.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T08:07:51.947726Z","iopub.execute_input":"2021-07-22T08:07:51.948077Z","iopub.status.idle":"2021-07-22T08:08:01.892076Z","shell.execute_reply.started":"2021-07-22T08:07:51.948049Z","shell.execute_reply":"2021-07-22T08:08:01.890693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Code","metadata":{}},{"cell_type":"code","source":"# Function to fit and evaluate the model on validation set.\ndef mfe(model, X_train, X_val, y_train, y_val, oob_score=False):\n    model.fit(X_train, y_train) \n    \n    y_train_pred = model.predict(X_train)\n    train_error = np.sqrt(metrics.mean_squared_error(y_train, np.where(y_train_pred < 0, 0, y_train_pred))) \n    \n    y_val_pred = model.predict(X_val) \n    val_error = np.sqrt(metrics.mean_squared_error(y_val, np.where(y_val_pred < 0, 0, y_val_pred)))\n    \n    if oob_score:\n        result = {'Train Error': train_error, 'Validation. Error': val_error, 'oob_score': model.oob_score_} \n    else:\n        result = {'Train Error': train_error, 'Validation. Error': val_error} \n    print(result)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:05:41.567748Z","iopub.execute_input":"2021-07-21T17:05:41.568071Z","iopub.status.idle":"2021-07-21T17:05:41.575564Z","shell.execute_reply.started":"2021-07-21T17:05:41.568039Z","shell.execute_reply":"2021-07-21T17:05:41.574541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to plot feature importances\ndef fi(model, X):\n    fi = pd.DataFrame({\n        'feature':X.columns.tolist(), \n        'importance':(model.feature_importances_ * 100).tolist()}).round(4).sort_values(by='importance', ascending=False)\n    bars = alt.Chart(fi).mark_bar().encode(\n      x='importance',\n      y=alt.Y('feature', sort='-x'),\n      tooltip=['feature','importance']\n    )\n\n    text = bars.mark_text(\n      align='left',\n      baseline='middle',\n      dx=3  # Nudges text to right so it doesn't appear on top of the bar\n    ).encode(\n      text='importance:Q'\n    )\n    return (bars + text).properties(width=400).configure_axis(labelFontSize=13, titleFontSize=16)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:05:41.577319Z","iopub.execute_input":"2021-07-21T17:05:41.577659Z","iopub.status.idle":"2021-07-21T17:05:41.605279Z","shell.execute_reply.started":"2021-07-21T17:05:41.577626Z","shell.execute_reply":"2021-07-21T17:05:41.604091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Define features & target\nnum_feats = train.drop(columns=['id','target']).select_dtypes(include='number').columns.tolist()\ncat_feats = train.select_dtypes(include='object').columns.tolist()\nfeats = num_feats+cat_feats\ntarget = 'target'\n\n# Create X & y datasets.\nX = train.loc[:, feats]\ny = train[target].ravel()\nX_test = test[feats].copy()\n\n# Slit the dataset.\nX_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=0.20, random_state=42)\n\n# To avoid setting with copy warnings we copy the data after splitting\nX_train = X_train.copy()\nX_val = X_val.copy()\n\n# Define the transformation steps needed for the data.\nct = compose.ColumnTransformer(\ntransformers = [\n    ('cat', preprocessing.OrdinalEncoder(categories='auto', dtype=np.int), cat_feats)\n], remainder='passthrough'\n)\n\n# Transform the data.\nX_train.loc[:, cat_feats] = ct.fit_transform(X_train.loc[:, cat_feats])\nX_val.loc[:, cat_feats] = ct.transform(X_val.loc[:, cat_feats])\nX_test.loc[:, cat_feats] = ct.transform(X_test.loc[:, cat_feats])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:19:02.331703Z","iopub.execute_input":"2021-07-21T13:19:02.331975Z","iopub.status.idle":"2021-07-21T13:19:02.365793Z","shell.execute_reply.started":"2021-07-21T13:19:02.33195Z","shell.execute_reply":"2021-07-21T13:19:02.364794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Baseline","metadata":{}},{"cell_type":"code","source":"%%time\nrf = ensemble.RandomForestRegressor(n_estimators=100, n_jobs=-1)\nmfe(rf, X_train, X_val, y_train, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:19:34.909299Z","iopub.execute_input":"2021-07-21T13:19:34.909922Z","iopub.status.idle":"2021-07-21T13:24:05.275708Z","shell.execute_reply.started":"2021-07-21T13:19:34.909873Z","shell.execute_reply":"2021-07-21T13:24:05.274671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi(rf, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:24:05.277311Z","iopub.execute_input":"2021-07-21T13:24:05.277616Z","iopub.status.idle":"2021-07-21T13:24:05.552214Z","shell.execute_reply.started":"2021-07-21T13:24:05.277587Z","shell.execute_reply":"2021-07-21T13:24:05.55121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = rf.predict(X_test)\ndsub = pd.DataFrame({'id': test.loc[:, 'id'], 'target': preds})\ndsub.to_csv('dsub2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:19:02.542043Z","iopub.status.idle":"2021-07-21T13:19:02.542521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = clf.predict(X_test)\ndsub = pd.DataFrame({'id': test.loc[:, 'id'], 'target': preds})\ndsub.to_csv('dsub4.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:29:41.091196Z","iopub.execute_input":"2021-07-20T20:29:41.09159Z","iopub.status.idle":"2021-07-20T20:29:41.90461Z","shell.execute_reply.started":"2021-07-20T20:29:41.09154Z","shell.execute_reply":"2021-07-20T20:29:41.903616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [XGBoost: Intro to python API](https://xgboost.readthedocs.io/en/latest/python/python_intro.html#)","metadata":{}},{"cell_type":"markdown","source":"* [XGB Hyperparameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n* [XGB specific notes](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)","metadata":{}},{"cell_type":"markdown","source":"### General Parameters\n**booster**\n* Default Values is 'gbtree'; Can be gbtree, gblinear or dart; \n* gbtree and dart use tree based models while gblinear uses linear functions.\n\n**Verbosity**\n* Verbosity of printing messages. \n* Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug)\n\n### Parameters for Tree Booster\n**eta**\n* Default value is 0.3. Range is [0,1], aka learning_rate.\n* Used to scale the output from each tree. Shrinkage parameter. Speed of learning. \n* Higher values may lead to overfitting.\n\n**gamma**\n* Default value is 0. Range [0,∞]. aka `min_split_loss`.\n* Is meant to encourage pruning in the tree, thought xgboost can prune even if gamma is zero.\n* Minimum loss reduction required to make a further partition on a leaf node. \n* Increasing `gamma` regularizes the model and reduces the overfitting. \n\n**max_depth**\n* Default value: 6; range: [0,∞]\n* Maximum depth of a tree. How many interactions do you want to allow in your model?  \n* Larger values increase complexity and lead to overfitting.\n* If the validation performance keeps increasing as the max_depth increases, it means there are a lot of interactions that can be extracted from the data. So it's better to stop tuning in that case any try to generate some features.\n* 7 is a good value to start.\n\n**min_child_weight**:1\n* Similar to `min_samples_leaf` in Random Forest.\n* Increasing the value regularizes the model and reduces the overfitting.\n* One of the most important parameters to tune in XGB & LGB\n\n**max_delta_step**:0\n* Default value:  0; range: [0,∞]\n* Maximum delta step we allow each leaf output to be.\n* Usually not needed, but it might help in logistic regression when class is extremely imbalanced. Suggested range: [1-10].\n\n**subsample**\n* Default value: 1, range: (0,1]\n* Subsample ratio of the training instances.\n* Lower values regularize the model and prevent overfitting.\n\n**colsample_bytree**\n* Default value: 1, range: (0,1]\n* What fraction of features will be used for building each tree. Subsampling occurs once for every tree constructed.\n* Lower values regularize the model and prevent overfitting.\n\n**colsample_bylevel**: 1 \n* Default value: 1, range: (0,1]\n* Subsample ratio of columns for each level. \n* Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n* Lower values regularize the model and prevent overfitting.\n\n**colsample_bynode**: 1 \n* Default value: 1, range: (0,1]\n* Subsample ratio of columns for each node (split). \n* Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n* Lower values regularize the model and prevent overfitting.\n\n**lambda**\n* Default value: 1, range: (0,1]; aka reg_lambda\n* Used to regularize the output values from leaves. As lambda increases the output value from a leaf shifts closer to zero.\n* L2 regularization term on weights. \n* Higher values reduce overfitting.\n\n**alpha**\n* Default value: 0, range: (0,1]; aka reg_alpga\n* L1 regularization term on weights.\n* Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n* Higher values reduce overfitting.\n\n**tree_method**\n* Specifies which tree construction algorithm to use in in XGBoost.\n* auto, exact, approx, hist, gpu_hist are different options.\n* For large datasets prefer approx, hist, gpu_hist. For smaller datasets used exact.\n\n**scale_pos_weight**\n* Control the balance of positive and negative weights, useful for unbalanced classes. \n* A typical value to consider: sum(negative instances) / sum(positive instances)\n\n**max_leaves**\n* Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set.\n\n### Learning Task Parameters\n**objective**: \n* reg:squarederror Optimization objective for regression.\n\n**base_score**\n* 0.5, The initial prediction score of all instances, global bias\n\n**eval_metric**:\n* Evaluation metrics for validation data, a default metric will be assigned according to objective\n\n**seed**: \n* 42 # Reproducibility","metadata":{"execution":{"iopub.status.busy":"2021-07-15T18:43:33.727589Z","iopub.execute_input":"2021-07-15T18:43:33.727968Z","iopub.status.idle":"2021-07-15T18:43:33.736718Z","shell.execute_reply.started":"2021-07-15T18:43:33.727936Z","shell.execute_reply":"2021-07-15T18:43:33.735299Z"}}},{"cell_type":"markdown","source":"### How to control overfitting?\n<br/>\n\nWhen you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.There are in general two ways that you can control overfitting in XGBoost \n* The first way is to directly control model complexity through `max_depth`, `min_child_weight` and `gamma`.\n* The second way is to add randomness to make training robust to noise by using `subsample` and `colsample_bytree`.\n* You can also reduce stepsize `eta`. Remember to increase num_round when you do so.","metadata":{}},{"cell_type":"markdown","source":"### General Tips\n* In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n* We can try a very small learning rate and a large number of boosting iterations to see how long it takes for the model to overfit. When the validation loss stops decreasing, we can exit the training. To get even more accuracy, we can multiply the `num_rounds` by k & divide `eta` by k. \n* We can change the `seed` parameter to see how different values affect out results.","metadata":{}},{"cell_type":"markdown","source":"## Training XGBoost Model ([Link](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train))","metadata":{}},{"cell_type":"code","source":"# Define the train, validation and test Dmatrix objects.\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\ndtest = xgb.DMatrix(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T19:26:45.417014Z","iopub.execute_input":"2021-07-21T19:26:45.417654Z","iopub.status.idle":"2021-07-21T19:26:45.624361Z","shell.execute_reply.started":"2021-07-21T19:26:45.417616Z","shell.execute_reply":"2021-07-21T19:26:45.62329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model params.\nparams = {\n    'eta': 0.15,\n    'gamma': 1,\n    'max_depth':6 ,\n    'min_child_weight': 8,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'colsample_bylevel': 1,\n    'colsample_bynode': 1, \n    'lambda': 1,\n    'alpha': 1,\n    'tree_method': 'exact',\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    #'tree_method':'gpu_hist',\n    'seed': 42\n} ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T19:58:04.270972Z","iopub.execute_input":"2021-07-21T19:58:04.271349Z","iopub.status.idle":"2021-07-21T19:58:04.276846Z","shell.execute_reply.started":"2021-07-21T19:58:04.271319Z","shell.execute_reply":"2021-07-21T19:58:04.275894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The argument `early_stopping_rounds` offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 10 is a reasonable value. Thus we stop after 10 straight rounds of deteriorating validation scores.","metadata":{}},{"cell_type":"code","source":"%%time\nmodel = xgb.train(\n    params, \n    dtrain, \n    num_boost_round = 100, \n    evals = [(dtrain, 'train'), (dval, 'val')], # List of validation sets for which metrics will evaluated during training\n    early_stopping_rounds = 10,\n    verbose_eval = 20\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T19:58:04.865661Z","iopub.execute_input":"2021-07-21T19:58:04.866293Z","iopub.status.idle":"2021-07-21T19:58:55.369939Z","shell.execute_reply.started":"2021-07-21T19:58:04.866255Z","shell.execute_reply":"2021-07-21T19:58:55.368866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(model);","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:18:03.791013Z","iopub.execute_input":"2021-07-21T13:18:03.791518Z","iopub.status.idle":"2021-07-21T13:18:04.392978Z","shell.execute_reply.started":"2021-07-21T13:18:03.791469Z","shell.execute_reply":"2021-07-21T13:18:04.39202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.attributes()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:14:11.779599Z","iopub.execute_input":"2021-07-21T12:14:11.779953Z","iopub.status.idle":"2021-07-21T12:14:11.787028Z","shell.execute_reply.started":"2021-07-21T12:14:11.779923Z","shell.execute_reply":"2021-07-21T12:14:11.786083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on mat/dataset.\nmodel.eval(dtrain, name='eval', iteration=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:14:37.711148Z","iopub.execute_input":"2021-07-21T12:14:37.711509Z","iopub.status.idle":"2021-07-21T12:14:37.836742Z","shell.execute_reply.started":"2021-07-21T12:14:37.711476Z","shell.execute_reply":"2021-07-21T12:14:37.835914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_fscore()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:14:49.277735Z","iopub.execute_input":"2021-07-21T12:14:49.278386Z","iopub.status.idle":"2021-07-21T12:14:49.316474Z","shell.execute_reply.started":"2021-07-21T12:14:49.278347Z","shell.execute_reply":"2021-07-21T12:14:49.315413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Importance types in XGBoost\n\n* `weight`: the number of times a feature is used to split the data across all trees.\n* `gain`: the average gain across all splits the feature is used in.\n* `cover`: the average coverage across all splits the feature is used in.\n* `total_gain`: the total gain across all splits the feature is used in.\n* `total_cover`: the total coverage across all splits the feature is used in.","metadata":{}},{"cell_type":"code","source":"model.get_score(importance_type='weight')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T08:06:08.572992Z","iopub.execute_input":"2021-07-21T08:06:08.573513Z","iopub.status.idle":"2021-07-21T08:06:08.616333Z","shell.execute_reply.started":"2021-07-21T08:06:08.573466Z","shell.execute_reply":"2021-07-21T08:06:08.61516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_split_value_histogram('cat5', bins=None, as_pandas=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T08:07:49.187414Z","iopub.execute_input":"2021-07-21T08:07:49.187877Z","iopub.status.idle":"2021-07-21T08:07:49.232399Z","shell.execute_reply.started":"2021-07-21T08:07:49.187831Z","shell.execute_reply":"2021-07-21T08:07:49.231292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.num_boosted_rounds()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T08:08:45.776584Z","iopub.execute_input":"2021-07-21T08:08:45.777047Z","iopub.status.idle":"2021-07-21T08:08:45.785035Z","shell.execute_reply.started":"2021-07-21T08:08:45.777003Z","shell.execute_reply":"2021-07-21T08:08:45.783835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.num_features()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T08:09:01.357242Z","iopub.execute_input":"2021-07-21T08:09:01.357694Z","iopub.status.idle":"2021-07-21T08:09:01.364396Z","shell.execute_reply.started":"2021-07-21T08:09:01.357657Z","shell.execute_reply":"2021-07-21T08:09:01.363218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost CV ([Link](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.cv))","metadata":{}},{"cell_type":"code","source":"xgb.cv(\n    params, #  Booster params.\n    dtrain, # Data to be trained.\n    num_boost_round=50, # Number of boosting iterations.\n    nfold=3, # Number of folds in CV\n    stratified=False, # Perform stratified sampling.\n    metrics='rmse', # Evaluation metrics to be watched in CV\n    early_stopping_rounds=10, \n    verbose_eval=10,  \n    seed=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:17:18.73734Z","iopub.execute_input":"2021-07-21T12:17:18.737739Z","iopub.status.idle":"2021-07-21T12:18:08.822436Z","shell.execute_reply.started":"2021-07-21T12:17:18.737702Z","shell.execute_reply":"2021-07-21T12:18:08.82137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using the [Scikit-Learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) for XGBoost","metadata":{}},{"cell_type":"markdown","source":"Here is the Scikit-Learn implementation of XGBoost. ","metadata":{}},{"cell_type":"code","source":"clf = xgb.XGBRegressor(\n    n_estimators = 100,\n    max_depth = 6,\n    learning_rate = 0.15,\n    #verbosity = 0,\n    objective = 'reg:squarederror',\n    n_jobs = -1,\n    gamma = 0,\n    min_child_weight = 7,\n    max_delta_step = 0,\n    subsample = 1,\n    colsample_bytree = 1,\n    colsample_bylevel = 1,\n    colsample_bynode = 1,\n    reg_alpha = 1,\n    reg_lambda = 1,\n    scale_pos_weight = 1,\n    random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T19:08:06.641235Z","iopub.execute_input":"2021-07-21T19:08:06.641923Z","iopub.status.idle":"2021-07-21T19:08:06.649539Z","shell.execute_reply.started":"2021-07-21T19:08:06.641865Z","shell.execute_reply":"2021-07-21T19:08:06.648746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"reg_alpha 0 to 1 decreases validation rmse from .84584 to .84576.","metadata":{}},{"cell_type":"code","source":"clf.fit(\n    X_train, y_train,\n    eval_set = [(X_train, y_train), (X_val, y_val)],\n    eval_metric = 'rmse',\n    early_stopping_rounds = 10,\n    verbose = 10);","metadata":{"execution":{"iopub.status.busy":"2021-07-21T19:08:18.100541Z","iopub.execute_input":"2021-07-21T19:08:18.101031Z","iopub.status.idle":"2021-07-21T19:09:08.925353Z","shell.execute_reply.started":"2021-07-21T19:08:18.101002Z","shell.execute_reply":"2021-07-21T19:09:08.92455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"Some Resources for Learning Model Tuning.\n\n* [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n* [Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)","metadata":{}},{"cell_type":"markdown","source":"#### Advice from top Kagglers\n\n1. Select the most influential parameters. There are tons of parameters and we can't tune them all.\n2. For the selected set of parameters, understand how they influence the training.\n3. Tune the selected parameters. In pracice it's faster to tune manually. ","metadata":{}},{"cell_type":"markdown","source":"Different values of parameters result in 3 different fitting behaviors. \n* Underfitting\n* Good fit and generalization\n* Overfitting\n\nWe will split the parameters into two groups. \n* Brakers: Parameters  whose value upon increasing regularize the model. If we increase their value the model will change its behaviour from  overfitting to underfitting. \n* Speeders: Parameters whose value upon increasing can lead to overfitting. Increase their value if model underfits & decrease if it underfits.\n","metadata":{}},{"cell_type":"code","source":"# Hyperparameters that matter and their XGBooot & LightGBM names.\npd.DataFrame({\n'Type': ['Speeder', 'Speeder', 'Speeder', 'Braker', 'Braker', 'Braker', 'Speeder', 'Breaker'],\n'XGBoost': ['max_depth', 'subsample', 'colsample_by_tree/level', 'min_child_weight', 'lambda', 'alpha', 'eta', 'num_round'],\n'LightGBM': ['max_depth/num_leaves', 'bagging_fraction', 'feature_fraction', 'min_data_in_leaf', 'lambda_l1', 'lambda_l2', 'learning_rate', 'num_iterations']\n})","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:06.97618Z","iopub.execute_input":"2021-07-22T08:11:06.976693Z","iopub.status.idle":"2021-07-22T08:11:06.994558Z","shell.execute_reply.started":"2021-07-22T08:11:06.976651Z","shell.execute_reply":"2021-07-22T08:11:06.993356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### Step 1: Fix learning rate and number of estimators for tuning tree-based parameters","metadata":{}},{"cell_type":"markdown","source":"Since we have set n_estimators = 100 & learning_rate = 0.15 from above, let's tune `max_depth` & `min_child_weight`","metadata":{}},{"cell_type":"markdown","source":"#### Step 2: Tune max_depth and min_child_weight","metadata":{}},{"cell_type":"code","source":"param_grid_1 = {\n    \"max_depth\": [6, 10, 15],\n    \"min_child_weight\": [1, 3, 5, 7]\n}\n\nclf_1 = xgb.XGBRegressor(n_estimators=100, learning_rate=0.15, objective = 'reg:squarederror', reg_alpha=1, n_jobs=-1)\n\ngs_1 = model_selection.GridSearchCV(\n    estimator = clf_1,\n    param_grid = param_grid_1,\n    scoring = 'neg_root_mean_squared_error',\n    verbose = 10,\n    n_jobs = 1,\n    cv = 5\n)\n\n# Fit the model and extract best score\ngs_1.fit(X_train, y_train)\nprint(f\"Best score: {gs_1.best_score_}\")\n\nprint(\"Best parameters set:\")\nbest_parameters = gs_1.best_estimator_.get_params()\nfor param_name in sorted(param_grid_1.keys()):\n    print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n\nprint(f'Overall Results are : {gs_1.cv_results_}')    ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:41:27.825555Z","iopub.execute_input":"2021-07-21T17:41:27.825933Z","iopub.status.idle":"2021-07-21T18:53:21.766332Z","shell.execute_reply.started":"2021-07-21T17:41:27.825896Z","shell.execute_reply":"2021-07-21T18:53:21.764715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes from above run.\n\nBest score: -0.8467124937346888\nBest parameters set: max_depth: 6; min_child_weight: 7","metadata":{}},{"cell_type":"code","source":"param_grid_2 = {\n    \"min_child_weight\": [5, 7, 9]\n}\n\nclf_2 = xgb.XGBRegressor(max_depth=6, n_estimators=100, learning_rate=0.15, objective = 'reg:squarederror', reg_alpha=1, n_jobs=-1)\n\ngs_2 = model_selection.GridSearchCV(\n    estimator = clf_1,\n    param_grid = param_grid_2,\n    scoring = 'neg_root_mean_squared_error',\n    verbose = 10,\n    n_jobs = 1,\n    cv = 5\n)\n\n# Fit the model and extract best score\ngs_2.fit(X_train, y_train)\nprint(f\"Best score: {gs_2.best_score_}\")\n\nprint(\"Best parameters set:\")\nbest_parameters = gs_2.best_estimator_.get_params()\nfor param_name in sorted(param_grid_2.keys()):\n    print(f\"\\t{param_name}: {best_parameters[param_name]}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-21T19:11:20.061114Z","iopub.execute_input":"2021-07-21T19:11:20.061507Z","iopub.status.idle":"2021-07-21T19:22:11.925795Z","shell.execute_reply.started":"2021-07-21T19:11:20.061476Z","shell.execute_reply":"2021-07-21T19:22:11.924898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Step 3: Tune gamma","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Step 4: Tune subsample and colsample_bytree","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 5: Tuning Regularization Parameters","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predictions on test data**","metadata":{}},{"cell_type":"code","source":"ypred = model.predict(dtest)\ndsub = pd.DataFrame({'id': test.loc[:, 'id'], 'target': ypred})\ndsub.to_csv('dsub.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}