{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <u> Crime Statistics </u>\n\n### <u> About Our Dataset </u>\n\nThis dataset was compiled for use in our capstone project for the M.S. in Data Science at Drexel University. \nWe collected our data through thousands of calls to the NYC OpenData platform. \nAfter collecting all the data, we dropped columns that we didn't think add value. D\nespite our best effort, due to time constraints, we weren't able to scrub all null values for the less f\nrequently used attributes like any of the \"computed_region\" colunns. However, the dataset is in a usable form and ~95% cleaned.\n\n### <u> Content </u> \n\nWhile most of the data like \"arrest_date\" and \"age_group\" are straightforward, here is a key for some items that may be less obvious.\n\nColumn\t\t\t\tDescription\npd_desc\t\t\t\tDescription of internal classification corresponding with PD code (more granular than Offense Description)\nofns_desc\t\t\tDescription of offense corresponding with key code\nlaw_code\t\t\tNY penal law code of offense.\nlaw_cat_cd\t\t\tLevel of offense: felony, misdemeanor, violation\narrest_boro\t\t\tThe borough of NYC where the arrest took place\narrest_precinct\t\tPolice precinct that the arrest took place\njurisdiction_code\tJurisdiction responsible for incident. \n\nEither internal, like Police, Transit, and Housing; or external, like Correction, Port Authority, etc.\n\n:@computed_region_f5dn_yrer\tCommunity Districts\n:@computed_region_yeji_bk3q\tBorough Boundaries\n:@computed_region_92fq_4b7q\tCity Council Districts\n:@computed_region_sbqj_enih\tPolice Precincts\n\n### <u> TASKS : </u>\n\narrest_precinct - validate the column by doing predicts over it with respect to the other features influencing the originality of this column in the datasets\n\n### <u> Acknowledgements </u>\n\nThanks to NYC Open Data for the data.\n\nThis project has been a collaboration between Ambrose Karella, Janam Patel, and Naimish Bizzu.\n\n### <u> Inspiration </u>\n\nWe thought this data was interesting because it allows for exploring crime in a geospatial way. \nWhile broad demographics are interesting, we can get more granular and answer questions like \n\"where is a tourist least likely to be a victim in a crime?\"\n","metadata":{}},{"cell_type":"code","source":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing all datasets\ncrime_stats = pd.read_csv(\"/kaggle/input/nyc-crime-stats/NYC_crime.csv\")\ncrime_stats.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats = crime_stats.rename(columns={\":@computed_region_f5dn_yrer\":\"computed_region1\",\":@computed_region_yeji_bk3q\":\"computed_region2\",\":@computed_region_92fq_4b7q\":\"computed_region3\",\":@computed_region_sbqj_enih\":\"computed_region4\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop 'Unnamed: 13' as this is not in use\ncrime_stats.drop(['Unnamed: 0'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats.law_cat_cd.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats.perp_sex.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting some binary variables (Yes/No) to 0/1","metadata":{}},{"cell_type":"code","source":"# List of variables to map\n\nvarlist =  ['perp_sex']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({\"M\": 1, \"F\": 0})\n\n# Applying the function to the housing list\ncrime_stats[varlist] = crime_stats[varlist].apply(binary_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inspecting the Null Values ","metadata":{}},{"cell_type":"code","source":"crime_stats.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputing the missing values in the columns  with the most common values","metadata":{}},{"cell_type":"code","source":"crime_stats['law_cat_cd'] = crime_stats['law_cat_cd'].fillna(crime_stats['law_cat_cd'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats['computed_region1'] = crime_stats['computed_region1'].fillna(crime_stats['computed_region1'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats['computed_region2'] = crime_stats['computed_region2'].fillna(crime_stats['computed_region2'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats['computed_region3'] = crime_stats['computed_region3'].fillna(crime_stats['computed_region3'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats['computed_region4'] = crime_stats['computed_region4'].fillna(crime_stats['computed_region4'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"# import preprocessing from sklearn\nfrom sklearn import preprocessing\n\n# 1. INSTANTIATE\n# encode labels with value between 0 and n_classes-1.\nle = preprocessing.LabelEncoder()\n\n\n# 2/3. FIT AND TRANSFORM\n# use df.apply() to apply le.fit_transform to all columns\ncrime_stats_2 = crime_stats.apply(le.fit_transform)\ncrime_stats_2.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rescaling the Features \n\nWe will use MinMax scaling.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = [\"arrest_key\", \"arrest_date\", \"pd_desc\", \"ofns_desc\", \"law_code\", \"age_group\", \"law_cat_cd\", \"perp_race\", \"latitude\", \"longitude\", \"arrest_boro\", \"arrest_precinct\", \"jurisdiction_code\", \"computed_region1\", \"computed_region2\", \"computed_region3\", \"computed_region4\"]\n\ncrime_stats_2[num_vars] = scaler.fit_transform(crime_stats_2[num_vars])\n\ncrime_stats_2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats_2.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for Outliers ","metadata":{}},{"cell_type":"code","source":"# Checking for outliers in the continuous variables\nnum_crime_stats_2 = crime_stats_2[[\"arrest_key\",\"arrest_date\",\"pd_desc\",\"ofns_desc\",\"law_code\",\"law_cat_cd\",\"age_group\",\"perp_sex\",\"perp_race\",\"latitude\",\"longitude\",\"arrest_boro\",\"arrest_precinct\",\"jurisdiction_code\",\"computed_region1\",\"computed_region2\",\"computed_region3\",\"computed_region4\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_crime_stats_2.describe(percentiles=[.25, .5, .75, .90, .95, .99])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Crime_statistics","metadata":{}},{"cell_type":"code","source":"#Apply matplotlib functionalities\n\n#Change the colour of bins to green\n#Change the number of bins\n\n#Create a distribution plot for rating\n\n#import the necessary libraries\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nplt.figure(figsize = [9,5])\nsns.distplot(num_crime_stats_2.arrest_precinct,  bins = 40, color = \"orange\")\nplt.title(\"Distribution of Crime_statistics\", fontsize = 20, fontweight = 10, verticalalignment = 'baseline')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test-Train Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crime_stats_2.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Putting feature variable to X\nX = crime_stats_2.drop(['arrest_precinct'], axis=1)\n\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Putting response variable to y\ny = crime_stats_2['arrest_precinct']\n\ny.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the correlation matrix \nplt.style.use(\"ggplot\")\nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(crime_stats_2.corr(),annot = True,cmap=\"Greens\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Building\nLet's start by splitting our data into a training set and a test set.","metadata":{}},{"cell_type":"markdown","source":"#### Running Your First Training Model","metadata":{}},{"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 11)             # running RFE\nrfe = rfe.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns[~rfe.support_]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(['pd_desc'], axis=1)\nX_train = X_train.drop(['ofns_desc'], axis=1)\nX_train = X_train.drop(['law_cat_cd'], axis=1)\nX_train = X_train.drop(['age_group'], axis=1)\nX_train = X_train.drop(['perp_sex'], axis=1)\nX_train = X_train.drop(['computed_region3'], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\n \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","metadata":{}},{"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(['arrest_key'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(['computed_region2'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(['longitude'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\n \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","metadata":{}},{"cell_type":"code","source":"y_train_cnt = lr_2.predict(X_train_lm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Predictions","metadata":{}},{"cell_type":"code","source":"X_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars = [\"arrest_key\",\"arrest_date\",\"pd_desc\",\"ofns_desc\",\"law_code\",\"law_cat_cd\",\"age_group\",\"perp_sex\",\"perp_race\",\"latitude\",\"longitude\",\"arrest_boro\",\"jurisdiction_code\",\"computed_region1\",\"computed_region2\",\"computed_region3\",\"computed_region4\"]\nX_test[num_vars] = scaler.transform(X_test[num_vars])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_new.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions\ny_pred = lr_2.predict(X_test_new)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_2.params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_test to dataframe\n\nX_test_df = pd.DataFrame(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the head\ny_pred_1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Putting CustID to index\nX_test_df['ID'] = X_test_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\nX_test_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([X_test_df, y_pred_1],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'arrest_precinct_Prob'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* GRADIENT DESCENCT ","metadata":{}},{"cell_type":"code","source":"crime_stats.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X['intercept'] = 1\nX = X.reindex([\"intercept\",\"arrest_key\",\"perp_sex\",\"latitude\",\"longitude\",\"arrest_precinct\",\"jurisdiction_code\",\"computed_region1\",\"computed_region2\",\"computed_region3\",\"\"computed_region4\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nX = np.array(X)\ny = np.array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Theta needed to be changed with the number of response varaible used.\ntheta = np.matrix(np.array([0,0,0,0])) \nalpha = 0.01\niterations = 1000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef compute_cost(X, y, theta):\n    return np.sum(np.square(np.matmul(X, theta) - y)) / (2 * len(y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent_multi(X, y, theta, alpha, iterations):\n    theta = np.zeros(X.shape[1])\n    m = len(X)\n    gdm_df = pd.DataFrame( columns = ['Bets','cost'])\n\n    for i in range(iterations):\n        gradient = (1/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n        theta = theta - alpha * gradient\n        cost = compute_cost(X, y, theta)\n        gdm_df.loc[i] = [theta,cost]\n\n    return gdm_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient_descent_multi(X, y, theta, alpha, iterations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(gradient_descent_multi(X, y, theta, alpha, iterations).values[999])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient_descent_multi(X, y, theta, alpha, iterations).reset_index().plot.line(x='index', y=['cost'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import LinearRegression from sklearn\nfrom sklearn.linear_model import LinearRegression\n\n# Representing LinearRegression as lr(Creating LinearRegression Object)\nlr = LinearRegression()\n\n#You don't need to specify an object to save the result because 'lr' will take the results of the fitted model.\nlr.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lr.intercept_)\nprint(lr.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}