{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Employee Churn rate model\n\n### Problem Definition\n\n#### Attrition in a company can happen due to various reasons. Voluntary, Involuntary or Retirement. In this project I had developed a machine learning model based on the attributes given for an employee whether an employee will stay in the company or leave the company.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import the libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import __version__\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot, iplot\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\ninit_notebook_mode(connected = True)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\ncf.go_offline()\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data file ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n\nprint('Shape of the dataframe is {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset. So, we don't have to preprocess the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# ** Exploratory Data Analysis **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pairplot of all the variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize = (20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The age of the employees ranges from 18 to 60.\n* Monthly Income of the employees range between 0 to 5000.\n* The Number of companies most of the employees had worked previously is between 1 to 2.\n* More than 400 employees have a work experience of in and around 10 years.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Heatmap to find the correlation between all the variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20))\nsns.heatmap(df.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average Age of Employees who left the company : {:.2f}'.format(df[df['Attrition'] == 'Yes']['Age'].mean()))\nprint('Average Age of Employees who left the company : {:.2f}'.format(df[df['Attrition'] == 'No']['Age'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,8))\nsns.countplot(x = 'BusinessTravel', hue = 'Attrition', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Employees who travel rarely tend to be the most who leave the company and stay in the company compared to other Business Travel criteria. But also it seems that this dataset seems to be imbalanced with more data towards Business Travel = 'Travel_Rarely'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Label encoding the Attrition variable ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(df['Attrition'])\ndf['Attrition'] = le.transform(df['Attrition'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = []\nb = [df['EducationField'].unique()]\nfor i in df['EducationField'].unique():\n    a.append((df[(df['EducationField']==i) & (df['Attrition'] == 1)].shape[0]/df[df['EducationField']==i].shape[0])*100)\npd.DataFrame(a,b, columns = ['Attrition percentage by Education Field'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xticklabels = ['Life Sciences','Other','Medical','Marketing','Technical Degree','Human Resources']\npd.DataFrame(a,b, columns = ['Attrition percentage by Education Field']).iplot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = (df1.groupby('WorkLifeBalance').agg('count')['Attrition']/df.groupby('WorkLifeBalance').agg('count')['Attrition']) * 100\nfig = px.bar(x = abc.index, y = abc.values, labels={'x': 'Work Life Balance', 'y' : 'Count'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Employees who have a work life balance = 1 tend to leave the company very soon when compared to other employees.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xticks = ['Female','Male']\nabc = ((df[df['Attrition'] == 1].groupby('Gender').agg('count'))/(df.groupby('Gender').agg('count')) *100)['Attrition']\nsns.barplot(xticks,abc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Male has a higher proportion of Attrition than Female.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df[df['Attrition'] == 1]\nfig = px.histogram(df1, x= 'MonthlyIncome')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most people having a MonthlyIncome between 2000-2999 are likely to leave the company and as the salary range increases the attrition rate decreases.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = px.histogram(df,x = 'HourlyRate')\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Average Hourly rate of most of the employees is between 30-99 whereas only 19 employees get an hourly rate of 100-104.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,8))\nsns.countplot(x = 'PerformanceRating', hue = 'Attrition', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Employees who have a performance rating - 3 are more likely to be leaving the company when compared to performance rating - 4.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Attr_Yes = df[df['Attrition'] == 1]\nabc = df_Attr_Yes['JobSatisfaction'].value_counts()/df['JobSatisfaction'].value_counts() * 100\nfig = px.bar(x = abc.index, y = abc.values)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Employees with the least JobSatisfaction = 1 has the highest possibility of leaving the company.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = px.histogram(df[df['Attrition'] == 1], x = 'PercentSalaryHike', color = 'Attrition')\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histogram of the percent salary hike for the employees whose Attrition == 'Yes' is plotted above and it shows that as the percent salary hike increases the attrition count decreases. This shows that employees who are not satisfied with the salary percent hike tend to leave the company.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3 = px.pie(df[df['Attrition'] == 1],values = 'Attrition', names = 'Department',title = 'Attrition by Department')\nfig3.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Attrition is mostly in the Reasearch & Development department and the least in the Human Resources department.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns[2:]:\n    if df[col].dtype == 'object':\n        if len(list(df[col].unique())) <=2:\n            print(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label encoding the Gender and OverTime variables as they have only 2 unique values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Gender'] = le.fit_transform(df['Gender'])\ndf['OverTime'] = le.fit_transform(df['OverTime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns = 'Over18',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can drop the Over18 column since it has only one unique value and won't add value to the machine learning model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df,drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have one hot encoded all other 'object' variables where the unique values in the column are greater than 2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We scale the whole dataset using StandardScaler to have all the features in a common scale while building a machine learning model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.loc[:,'Attrition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Attrition', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have splitted the dataset into train(X) and train(y) features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 42)\nprint('Shape of X_train {}'.format(X_train.shape))\nprint('Shape of X_test {}'.format(X_test.shape))\nprint('Shape of y_train {}'.format(y_train.shape))\nprint('Shape of y_test {}'.format(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Building**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\nfor i in range(1,40):\n    clf = KNeighborsClassifier(n_neighbors = i)\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    error_rate.append(np.mean(y_test != y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,8))\nplt.plot(range(1,40),error_rate)\nplt.xlabel('K value')\nplt.ylabel('Error Rate')\nplt.title('Error Rate vs K value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [LogisticRegression(max_iter=1200000, dual = False), RandomForestClassifier(), SVC(gamma = 'auto'), DecisionTreeClassifier(),GaussianNB(),KNeighborsClassifier(n_neighbors = 22)]\nmodel_names = ['Logistic Regression', 'Random Forest Classifier', 'SVC','Decision Tree Classifier', 'Naive Bayes','KNeigbors Classifier']\nf1_score_ = []\nroc_auc = []\noverall = []\nfor i,j in zip(model_names,models):\n    clf = j\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    f1_score_ =  f1_score(y_test,y_pred) * 100\n    roc_auc = roc_auc_score(y_test,y_pred) * 100\n    overall.append([i,f1_score_,roc_auc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = pd.DataFrame(overall, columns = ['Model Name','F1-Score','ROC AUC Score'])\nmetrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the dataset is imbalanced we are not using the accuracy score instead F1-Score and ROC AUC Score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,8))\nfig = px.bar(x = metrics['Model Name'], y = metrics['ROC AUC Score'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression and Naive Bayes has the highest ROC AUC score so we can go for further analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Fine Tuning for better accuracy of the model :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid={'C': np.arange(1e-03, 2, 0.01)}\nlog_GS = GridSearchCV(LogisticRegression(solver='liblinear',\n                                         class_weight=\"balanced\", \n                                         random_state=7), param_grid = grid, verbose=True,return_train_score=True,scoring = 'roc_auc', iid = True, cv = 10 )\nlog_GS.fit(X_train,y_train)\nprint('Best Estimator : {}'.format(log_GS.best_estimator_))\nprint('Best Score : {}'.format(log_GS.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(C=1.9109999999999996, class_weight='balanced', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=7, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1-Score',round(f1_score(y_test,y_pred)*100))\nprint('AUC ROC Score',round(roc_auc_score(y_test,y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the dataset is imbalanced, I would be using only the AUC ROC Score for comparison with the one without fine tuning.\nAUC ROC Score has an increase in it when compared to that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8,5))\nfig = ff.create_annotated_heatmap(confusion_matrix(y_test,y_pred))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"329 out of 441 values have been predicted correctly as 0 and 1.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since, Naive Bayes has no hyper-parameters to fine tune we donot use GridSearch CV for that. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Overall, after the fine tuning we see that Logistic Regression has the highest ROC AUC score when compared to any other algorithm.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> # Recommendations\n\n* Fields like Human Resources, Technical degree and Marketing has the high attrition percentage of 26%, 24% and 22%.So, the employees has to be introspected carefully to get to know their concerns.\n\n\n* Employees who have a job satisfaction = 1 has the highest attrition rate of 22.8% which needs to be addressed.\n\n\n* Research & Development department has the highest attrition percentage of 56.1%.\n\n\n* Employees who have a work life balance = 1 tend to leave the company very soon when compared to other employees.This should be considered as a benchmark and noted.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}