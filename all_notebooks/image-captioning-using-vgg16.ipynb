{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pickle import load\nimport numpy as np\nimport string\nimport argparse\nfrom os import listdir\nfrom pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.layers import Input, Reshape, Concatenate\nfrom progressbar import progressbar\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical,plot_model\nfrom keras.models import Model, Sequential , load_model\nfrom keras.layers import Input,Dense,LSTM,Embedding,Dropout,RepeatVector,TimeDistributed,concatenate\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nfrom nltk.translate.bleu_score import corpus_bleu","metadata":{"_uuid":"9c07a60aa33345953e1ddeb3f8bb19c5efad93fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To load files into memory\ndef load_doc(filename):\n  # open the file as read only\n  file = open(filename, 'r')\n  # read all text\n  text = file.read()\n  # close the file\n  file.close()\n  return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load a pre-defined list of photo identifiers\ndef load_set(filename):\n  doc = load_doc(filename)\n  dataset = list()\n  # process line by line\n  for line in doc.split('\\n'):\n    # skip empty lines\n    if len(line) < 1:\n      continue\n    # get the image identifier\n    identifier = line.split('.')[0]\n    dataset.append(identifier)\n  return set(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split a dataset into train/test elements\ndef train_test_split(dataset):\n  # order keys so the split is consistent\n  ordered = sorted(dataset)\n  # return split dataset as two new sets\n  return set(ordered[:100]), set(ordered[100:200])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n  # load document\n  doc = load_doc(filename)\n  descriptions = dict()\n  for line in doc.split('\\n'):\n    # split line by white space\n    tokens = line.split()\n    # split id from description\n    image_id, image_desc = tokens[0], tokens[1:]\n    # skip images not in the set\n    if image_id in dataset:\n      # create list\n      if image_id not in descriptions:\n        descriptions[image_id] = list()\n      # wrap description in tokens\n      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n      # store\n      descriptions[image_id].append(desc)\n  return descriptions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load photo features\ndef load_photo_features(filename, dataset):\n  # load all features\n  all_features = load(open(filename, 'rb'))\n  # filter features\n  features = {k: all_features[k] for k in dataset}\n  return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(data='dev'):\n\n  assert data in ['dev', 'train', 'test']\n\n  train_features = None\n  train_descriptions = None\n\n  if data == 'dev':\n    # load dev set (1K)\n    filename = '../input/flickr8k-text/flickr_8k.devImages.txt'\n    dataset = load_set(filename)\n    print('Dataset: %d' % len(dataset))\n\n    # train-test split\n    train, test = train_test_split(dataset)\n    #print('Train=%d, Test=%d' % (len(train), len(test)))\n\n    # descriptions\n    train_descriptions = load_clean_descriptions('../input/models/descriptions.txt', train)\n    test_descriptions = load_clean_descriptions('../input/models/descriptions.txt', test)\n    print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n\n    # photo features\n    train_features = load_photo_features('../input/models/features.pkl', train)\n    test_features = load_photo_features('../input/models/features.pkl', test)\n    print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n\n  elif data == 'train':\n    # load training dataset (6K)\n    filename = '../input/flickr8k-text/flickr_8k.trainImages.txt'\n    train = load_set(filename)\n\n    filename = '../input/flickr8k-text/flickr_8k.devImages.txt'\n    test = load_set(filename)\n    print('Dataset: %d' % len(train))\n\n    # descriptions\n    train_descriptions = load_clean_descriptions('../input/models/descriptions.txt', train)\n    test_descriptions = load_clean_descriptions('../input/models/descriptions.txt', test)\n    print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n\n    # photo features\n    train_features = load_photo_features('../input/models/features.pkl', train)\n    test_features = load_photo_features('../input/models/features.pkl', test)\n    print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n\n  elif data == 'test':\n    # load test set\n    filename = '../input/flickr8k-text/flickr_8k.testImages.txt'\n    test = load_set(filename)\n    print('Dataset: %d' % len(test))\n    # descriptions\n    test_descriptions = load_clean_descriptions('../input/models/descriptions.txt', test)\n    print('Descriptions: test=%d' % len(test_descriptions))\n    # photo features\n    test_features = load_photo_features('../input/models/features.pkl', test)\n    print('Photos: test=%d' % len(test_features))\n\n  return (train_features, train_descriptions), (test_features, test_descriptions)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(path):\n    img = load_img(path, target_size=(224,224))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return np.asarray(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract descriptions for images\ndef load_descriptions(doc):\n  mapping = dict()\n  # process lines\n  for line in doc.split('\\n'):\n    # split line by white space\n    tokens = line.split()\n    if len(line) < 2:\n      continue\n    # take the first token as the image id, the rest as the description\n    image_id, image_desc = tokens[0], tokens[1:]\n    # remove filename from image id\n    image_id = image_id.split('.')[0]\n    # convert description tokens back to string\n    image_desc = ' '.join(image_desc)\n    # create the list if needed\n    if image_id not in mapping:\n      mapping[image_id] = list()\n    # store description\n    mapping[image_id].append(image_desc)\n  return mapping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract features from each photo in the directory\ndef extract_features(directory,is_attention=False):\n  # load the model\n  if is_attention:\n    model = VGG16(include_top=True,weights=None)\n    model.load_weights(\"../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n    model.layers.pop()\n    # extract final 49x512 conv layer for context vectors\n    final_conv = Reshape([49,512])(model.layers[-4].output)\n    model = Model(inputs=model.inputs, outputs=final_conv)\n    print(model.summary())\n    features = dict()\n  else:\n    model = VGG16(include_top=True,weights=None)\n    model.load_weights(\"../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n    model.layers.pop()\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n    print(model.summary())\n    # extract features from each photo\n    features = dict()\n\n  for name in progressbar(listdir(directory)):\n    # ignore README\n    if name == '../README.md':\n      continue\n    filename = directory + '/' + name\n    image = load_image(filename)\n    # extract features\n    feature = model.predict(image, verbose=0)\n    # get image id\n    image_id = name.split('.')[0]\n    # store feature\n    features[image_id] = feature\n    print('>%s' % name)\n  return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_descriptions(descriptions):\n  # prepare translation table for removing punctuation\n  table = str.maketrans('', '', string.punctuation)\n  for key, desc_list in descriptions.items():\n    for i in range(len(desc_list)):\n      desc = desc_list[i]\n      # tokenize\n      desc = desc.split()\n      # convert to lower case\n      desc = [word.lower() for word in desc]\n      # remove punctuation from each token\n      desc = [w.translate(table) for w in desc]\n      # remove hanging 's' and 'a'\n      desc = [word for word in desc if len(word)>1]\n      # remove tokens with numbers in them\n      desc = [word for word in desc if word.isalpha()]\n      # store as string\n      desc_list[i] =  ' '.join(desc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the loaded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n  # build a list of all description strings\n  all_desc = set()\n  for key in descriptions.keys():\n    [all_desc.update(d.split()) for d in descriptions[key]]\n  return all_desc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n  lines = list()\n  for key, desc_list in descriptions.items():\n    for desc in desc_list:\n      lines.append(key + ' ' + desc)\n  data = '\\n'.join(lines)\n  file = open(filename, 'w')\n  file.write(data)\n  file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# extract features from all images\n\ndirectory = '../input/flickr8k-sau/flickr8k-sau/Flickr_Data/Images'\nfeatures = extract_features(directory)\nprint('Extracted Features: %d' % len(features))\n# save to file\n#dump(features, open('../input/models/features.pkl', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare descriptions\n\nfilename = '../input/flickr8k-text/flickr8k.token.txt'\n# load descriptions\ndoc = load_doc(filename)\n# parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))\n# clean descriptions\nclean_descriptions(descriptions)\n# summarize vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))\n# save to file\n#save_descriptions(descriptions, '../input/models/descriptions.txt')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = 256\n\nlstm_layers = 2\ndropout_rate = 0.2\nlearning_rate = 0.001","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n  all_desc = list()\n  for key in descriptions.keys():\n    [all_desc.append(d) for d in descriptions[key]]\n  return all_desc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n  lines = to_lines(descriptions)\n  tokenizer = Tokenizer()\n  tokenizer.fit_on_texts(lines)\n  return tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the length of the description with the most words\ndef max_length(descriptions):\n  lines = to_lines(descriptions)\n  return max(len(d.split()) for d in lines)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list, photo):\n  vocab_size = len(tokenizer.word_index) + 1\n\n  X1, X2, y = [], [], []\n  # walk through each description for the image\n  for desc in desc_list:\n    # encode the sequence\n    seq = tokenizer.texts_to_sequences([desc])[0]\n    # split one sequence into multiple X,y pairs\n    for i in range(1, len(seq)):\n      # split into input and output pair\n      in_seq, out_seq = seq[:i], seq[i]\n      # pad input sequence\n      in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n      # encode output sequence\n      out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n      # store\n      X1.append(photo)\n      X2.append(in_seq)\n      y.append(out_seq)\n  return np.array(X1), np.array(X2), np.array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length, n_step = 1):\n  # loop for ever over images\n  while 1:\n    # loop over photo identifiers in the dataset\n    keys = list(descriptions.keys())\n    for i in range(0, len(keys), n_step):\n      Ximages, XSeq, y = list(), list(),list()\n      for j in range(i, min(len(keys), i+n_step)):\n        image_id = keys[j]\n        # retrieve the photo feature\n        photo = photos[image_id][0]\n        desc_list = descriptions[image_id]\n        in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n        for k in range(len(in_img)):\n          Ximages.append(in_img[k])\n          XSeq.append(in_seq[k])\n          y.append(out_word[k])\n      yield [[np.array(Ximages), np.array(XSeq)], np.array(y)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def categorical_crossentropy_from_logits(y_true, y_pred):\n  y_true = y_true[:, :-1, :]  # Discard the last timestep\n  y_pred = y_pred[:, :-1, :]  # Discard the last timestep\n  loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true,\n                                                 logits=y_pred)\n  return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def categorical_accuracy_with_variable_timestep(y_true, y_pred):\n  y_true = y_true[:, :-1, :]  # Discard the last timestep\n  y_pred = y_pred[:, :-1, :]  # Discard the last timestep\n\n  # Flatten the timestep dimension\n  shape = tf.shape(y_true)\n  y_true = tf.reshape(y_true, [-1, shape[-1]])\n  y_pred = tf.reshape(y_pred, [-1, shape[-1]])\n\n  # Discard rows that are all zeros as they represent padding words.\n  is_zero_y_true = tf.equal(y_true, 0)\n  is_zero_row_y_true = tf.reduce_all(is_zero_y_true, axis=-1)\n  y_true = tf.boolean_mask(y_true, ~is_zero_row_y_true)\n  y_pred = tf.boolean_mask(y_pred, ~is_zero_row_y_true)\n\n  accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true, axis=1),\n                                              tf.argmax(y_pred, axis=1)),\n                                    dtype=tf.float32))\n  return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the captioning model\ndef define_model(vocab_size, max_length):\n  # feature extractor (encoder)\n  inputs1 = Input(shape=(4096,))\n  fe1 = Dropout(0.5)(inputs1)\n  fe2 = Dense(EMBEDDING_DIM, activation='relu')(fe1)\n  fe3 = RepeatVector(max_length)(fe2)\n\n  # embedding\n  inputs2 = Input(shape=(max_length,))\n  emb2 = Embedding(vocab_size, EMBEDDING_DIM, mask_zero=True)(inputs2)\n\n  # merge inputs\n  merged = concatenate([fe3, emb2])\n  # language model (decoder)\n  lm2 = LSTM(500, return_sequences=False)(merged)\n  #lm3 = Dense(500, activation='relu')(lm2)\n  outputs = Dense(vocab_size, activation='softmax')(lm2)\n\n  # tie it together [image, seq] [word]\n  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n  print(model.summary())\n  #plot_model(model, show_shapes=True, to_file='../input/val-set/dog.jpg')\n  return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(weight = None, epochs = 10):\n  # load dataset\n  data = prepare_dataset('train')\n  train_features, train_descriptions = data[0]\n  test_features, test_descriptions = data[1]\n\n  # prepare tokenizer\n  tokenizer = create_tokenizer(train_descriptions)\n  #dump(tokenizer, open('../models/tokenizer.pkl', 'wb'))\n  # index_word dict\n  #index_word = {v: k for k, v in tokenizer.word_index.items()}\n  # save dict\n  #dump(index_word, open('../models/index_word.pkl', 'wb'))\n \n  vocab_size = len(tokenizer.word_index) + 1\n  print('Vocabulary Size: %d' % vocab_size)\n  #tokenizer =open('../input/models/tokenizer.pkl', 'r') \n\n  # determine the maximum sequence length\n  maximum_length = max_length(train_descriptions)\n  print('Description Length: %d' % maximum_length)\n\n  # generate model\n  model = define_model(vocab_size, maximum_length)\n\n  # Check if pre-trained weights to be used\n  if weight != None:\n    model.load_weights(weight)\n\n  # define checkpoint callback\n  filepath= \"model.h5\"\n  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n                save_best_only=True, mode='min')\n\n  steps = len(train_descriptions)\n  val_steps = len(test_descriptions)\n  # create the data generator\n  train_generator = data_generator(train_descriptions, train_features, tokenizer, maximum_length)\n  val_generator =data_generator(test_descriptions, test_features, tokenizer, maximum_length)\n\n  # fit model\n  model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=steps, verbose=1,\n        callbacks=[checkpoint], validation_data=val_generator, validation_steps=val_steps)\n\n  try:\n      model.save('best_Model.h5', overwrite=True)\n      model.save_weights('weights.h5',overwrite=True)\n  except:\n      print(\"Error in saving model.\")\n  print(\"Training complete...\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(epochs=6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract features from each photo in the directory\ndef extract_features_photo(filename):\n  # load the model\n  model = VGG16()\n  # re-structure the model\n  model.layers.pop()\n  model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n  # load the photo\n  image = load_img(filename, target_size=(224, 224))\n  # convert the image pixels to a numpy array\n  image = img_to_array(image)\n  # reshape data for the model\n  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n  # prepare the image for the VGG model\n  image = preprocess_input(image)\n  # get features\n  feature = model.predict(image, verbose=0)\n  return feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate a description for an image\ndef generate_desc(model, tokenizer, photo, index_word, max_length, beam_size=5):\n\n  captions = [['startseq', 0.0]]\n  # seed the generation process\n  in_text = 'startseq'\n  # iterate over the whole length of the sequence\n  for i in range(max_length):\n    all_caps = []\n    # expand each current candidate\n    for cap in captions:\n      sentence, score = cap\n      # if final word is 'end' token, just add the current caption\n      if sentence.split()[-1] == 'endseq':\n        all_caps.append(cap)\n        continue\n      # integer encode input sequence\n      sequence = tokenizer.texts_to_sequences([sentence])[0]\n      # pad input\n      sequence = pad_sequences([sequence], maxlen=max_length)\n      # predict next words\n      y_pred = model.predict([photo,sequence], verbose=0)[0]\n      # convert probability to integer\n      yhats = np.argsort(y_pred)[-beam_size:]\n\n      for j in yhats:\n        # map integer to word\n        word = index_word.get(j)\n        # stop if we cannot map the word\n        if word is None:\n          continue\n        # Add word to caption, and generate log prob\n        caption = [sentence + ' ' + word, score + np.log(y_pred[j])]\n        all_caps.append(caption)\n\n    # order all candidates by score\n    ordered = sorted(all_caps, key=lambda tup:tup[1], reverse=True)\n    captions = ordered[:beam_size]\n\n  return captions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, index_word, max_length):\n  actual, predicted = list(), list()\n  # step over the whole set\n  for key, desc_list in descriptions.items():\n    # generate description\n    yhat = generate_desc(model, tokenizer, photos[key], index_word, max_length)[0]\n    # store actual and predicted\n    references = [d.split() for d in desc_list]\n    actual.append(references)\n    # Use best caption\n    predicted.append(yhat[0].split())\n  # calculate BLEU score\n  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_test_set(model, descriptions, photos, tokenizer, index_word, max_length):\n  actual, predicted = list(), list()\n  # step over the whole set\n  for key, desc_list in descriptions.items():\n    # generate description\n    yhat = generate_desc(model, tokenizer, photos[key], index_word, max_length)[0]\n    # store actual and predicted\n    references = [d.split() for d in desc_list]\n    actual.append(references)\n    # Use best caption\n    predicted.append(yhat[0].split())\n  predicted = sorted(predicted)\n  actual = [x for _,x in sorted(zip(actual,predicted))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='Generate image captions')\nparser.add_argument(\"-i\", \"--image\", help=\"Input image path\")\nparser.add_argument(\"-m\", \"--model\", help=\"model checkpoint\")\nargs = parser.parse_args()\n\n\n  # load the tokenizer\n  tokenizer = load(open('../models/tokenizer.pkl', 'rb'))\n  index_word = load(open('../models/index_word.pkl', 'rb'))\n  # pre-define the max sequence length (from training)\n  max_length = 34\n\n  # load the model\n  if args.model:\n    filename = args.model\n  else:\n    filename = '../test-set/model_weight.h5'\n  model = load_model(filename)\n\n  if args.image:\n    # load and prepare the photograph\n    photo = extract_features(args.image)\n    # generate description\n    captions = generate_desc(model, tokenizer, photo, index_word, max_length)\n    for cap in captions:\n      # remove start and end tokens\n      seq = cap[0].split()[1:-1]\n      desc = ' '.join(seq)\n      print('{} [log prob: {:1.2f}]'.format(desc,cap[1]))\n  else:\n    # load test set\n    test_features, test_descriptions = prepare_dataset('test')[1]\n\n    # evaluate model\n    evaluate_model(model, test_descriptions, test_features, tokenizer, index_word, max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}