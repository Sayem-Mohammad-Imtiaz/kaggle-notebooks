{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Real or Fake using LSTM & BERT","metadata":{}},{"cell_type":"markdown","source":"* This project was created in collaboration with Tomer Segall","metadata":{}},{"cell_type":"markdown","source":"In This note book we train two models:\n* LSTM (Long Short-Term Memory)\n* BERT (Bidirectional Encoder Representations from Transformers)\n\nThe models will predict if a news report is real or fake","metadata":{}},{"cell_type":"code","source":"#Imports\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom random import sample\nimport numpy as np\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nfrom random import shuffle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\n# Imports BERT\n\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns","metadata":{"id":"KzbwiCJSMuRb","execution":{"iopub.status.busy":"2021-05-24T16:19:06.066533Z","iopub.execute_input":"2021-05-24T16:19:06.066942Z","iopub.status.idle":"2021-05-24T16:19:06.077271Z","shell.execute_reply.started":"2021-05-24T16:19:06.066911Z","shell.execute_reply":"2021-05-24T16:19:06.076039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read dataset and create input column\n\nfilepath = \"../input/real-and-fake-news-dataset/news.csv\"\ndf = pd.read_csv(filepath)\ndf.drop('Unnamed: 0', axis=1, inplace=True)\ndf['titletext'] = df['title'] + \" \" + df['text']\n# Cap the sentences length\ndf['titletext'] = df = df[df['titletext'].str.split().str.len().lt(1000)]\ndf.head()","metadata":{"id":"hS8HoiSnNSTk","outputId":"88fbad8e-c1e6-4686-8af1-0f3ed7077b70","execution":{"iopub.status.busy":"2021-05-24T16:19:06.079324Z","iopub.execute_input":"2021-05-24T16:19:06.079836Z","iopub.status.idle":"2021-05-24T16:19:07.209922Z","shell.execute_reply.started":"2021-05-24T16:19:06.07979Z","shell.execute_reply":"2021-05-24T16:19:07.208837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vocabulary class\n\nUNK_TOKEN = 9\nclass Vocab:\n    def __init__(self):\n        self.word2id = {\"__unk__\": UNK_TOKEN}\n        self.id2word = {UNK_TOKEN: \"__unk__\"}\n        self.n_words = 1\n\n        self.tag2id = {\"FAKE\": 0, \"REAL\": 1}\n        self.id2tag = {0: \"FAKE\", 1: \"REAL\"}\n\n    def index_words(self, words):\n        word_indexes = [self.index_word(w) for w in words]\n        return word_indexes\n\n    def index_tags(self, tag):\n        tag_index = self.tag2id[tag]\n        return tag_index\n\n    def index_word(self, w):\n        if w not in self.word2id:\n            self.word2id[w] = self.n_words\n            self.id2word[self.n_words] = w\n            self.n_words += 1\n        return self.word2id[w]\n\n\n","metadata":{"id":"E6pGbPRvO7o4","execution":{"iopub.status.busy":"2021-05-24T16:19:07.212863Z","iopub.execute_input":"2021-05-24T16:19:07.213216Z","iopub.status.idle":"2021-05-24T16:19:07.224355Z","shell.execute_reply.started":"2021-05-24T16:19:07.213184Z","shell.execute_reply":"2021-05-24T16:19:07.222672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = Vocab()\ndef prepare_data(data, vocab, input_field):\n    data_sequences = []\n\n    for _, row in data.iterrows():\n        words = row[input_field].split()\n        tags = row[\"label\"]\n        word_ids = torch.tensor(vocab.index_words(words), dtype=torch.long).to(DEVICE)\n        tag_ids = torch.tensor(vocab.index_tags(tags), dtype=torch.long).to(DEVICE)\n        data_sequences.append([word_ids, tag_ids])\n\n    return data_sequences, vocab\n\n","metadata":{"id":"GwdYx8jEPE0P","execution":{"iopub.status.busy":"2021-05-24T16:19:07.227678Z","iopub.execute_input":"2021-05-24T16:19:07.228103Z","iopub.status.idle":"2021-05-24T16:19:07.256855Z","shell.execute_reply.started":"2021-05-24T16:19:07.228069Z","shell.execute_reply":"2021-05-24T16:19:07.255035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create data sequnce\n\nsequences, vocab = prepare_data(df, vocab, \"titletext\")\nx = [i[0] for i in sequences]\ny = [i[1] for i in sequences]\n\n# pad sentences to use batches\npadded_x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\nx = [i for i in padded_x]","metadata":{"id":"7hw0uQGDPQo0","execution":{"iopub.status.busy":"2021-05-24T16:19:07.259049Z","iopub.execute_input":"2021-05-24T16:19:07.25955Z","iopub.status.idle":"2021-05-24T16:19:10.541254Z","shell.execute_reply.started":"2021-05-24T16:19:07.25951Z","shell.execute_reply":"2021-05-24T16:19:10.539552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique words\n\nprint(vocab.n_words)","metadata":{"id":"wFt26gKZbyz_","outputId":"ce628ba9-6b7f-4fba-afa9-cdf306d3a9b4","execution":{"iopub.status.busy":"2021-05-24T16:19:10.543053Z","iopub.execute_input":"2021-05-24T16:19:10.543543Z","iopub.status.idle":"2021-05-24T16:19:10.549964Z","shell.execute_reply.started":"2021-05-24T16:19:10.543491Z","shell.execute_reply":"2021-05-24T16:19:10.548729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data to train, validation and test\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\ntest_sequences = list(zip(x_test,y_test))\ntest_sequences = [list(x) for x in test_sequences]\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\ntrain_sequences = list(zip(x_train,y_train))\ntrain_sequences = [list(x) for x in train_sequences]\nval_sequences = list(zip(x_val,y_val))\nval_sequences = [list(x) for x in val_sequences]","metadata":{"id":"pHE0oTrUPV0H","execution":{"iopub.status.busy":"2021-05-24T16:19:10.551743Z","iopub.execute_input":"2021-05-24T16:19:10.552216Z","iopub.status.idle":"2021-05-24T16:19:10.615641Z","shell.execute_reply.started":"2021-05-24T16:19:10.552164Z","shell.execute_reply":"2021-05-24T16:19:10.614542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LSTM class, architecture and forward\n\nclass LSTMNERNet(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers, directions, dropout):\n        super(LSTMNERNet, self).__init__()\n        self.input_size = input_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.directions = directions\n        self.bidirectional = True if directions == 2 else False\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, bidirectional=self.bidirectional, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size*directions, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_sentence):\n        num_dimensions = len(input_sentence)\n        sentence = input_sentence.clone().detach().to(DEVICE)\n        embedded = self.embedding(sentence)\n        packed_output, (hidden, cell) = self.lstm(embedded.view(num_dimensions, sentence.size()[1],embedding_size))\n        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n        output = self.dropout(self.fc1(hidden))\n        output = self.out(output)\n\n        return output","metadata":{"id":"V0p4LbwWPYzm","execution":{"iopub.status.busy":"2021-05-24T16:19:10.617732Z","iopub.execute_input":"2021-05-24T16:19:10.618193Z","iopub.status.idle":"2021-05-24T16:19:10.634329Z","shell.execute_reply.started":"2021-05-24T16:19:10.618143Z","shell.execute_reply":"2021-05-24T16:19:10.631007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(eval_sequences, batch_size):\n  eval_loader = DataLoader(eval_sequences, batch_size=batch_size, shuffle=True)\n  preds = []\n  tags = []\n  with torch.no_grad():\n      for words, tag in eval_loader:\n          preds.append(model(words).argmax(dim=1).cpu().data.numpy()) \n          tags.append(tag.cpu().data.numpy())\n  preds = np.concatenate(preds).ravel()\n  tags = np.concatenate(tags).ravel()\n  accuracy = (preds == tags).sum() / len(tags) * 100\n  return accuracy","metadata":{"id":"zlvgeF0zPZ6m","execution":{"iopub.status.busy":"2021-05-24T16:19:10.637666Z","iopub.execute_input":"2021-05-24T16:19:10.638253Z","iopub.status.idle":"2021-05-24T16:19:10.661013Z","shell.execute_reply.started":"2021-05-24T16:19:10.638185Z","shell.execute_reply":"2021-05-24T16:19:10.659383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, n_epochs, batch_size, train_set, test_set):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n\n    for e in range(1, n_epochs + 1):\n        count = 0\n        for words, tags in  iter(train_loader):\n            model.zero_grad()\n            seq_len = len(words)\n            sentence_loss = 0\n            output = model(words)\n            sentence_loss = criterion(output, tags)\n            sentence_loss.backward()\n            optimizer.step()\n            if count % 100 == 0:\n                print(f\"Epoch #{e}, Batch: {count},  Loss: {sentence_loss}\")\n            count += 1\n\n\n        train_accuracy = evaluate(train_set, batch_size)\n        print(f\"Epoch {e}, Training Accuracy: {train_accuracy}%\")\n\n        test_accuracy = evaluate(test_set, batch_size)\n        print(f\"Epoch {e}, Validation Accuracy: {test_accuracy}%\")","metadata":{"id":"veSVV3n9PcPs","execution":{"iopub.status.busy":"2021-05-24T16:19:10.662924Z","iopub.execute_input":"2021-05-24T16:19:10.663348Z","iopub.status.idle":"2021-05-24T16:19:10.682339Z","shell.execute_reply.started":"2021-05-24T16:19:10.663308Z","shell.execute_reply":"2021-05-24T16:19:10.680389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See what more important, title or text\n\n# PreProcess data\ntitle_sequences, vocab = prepare_data(df, vocab, \"title\")\ntitle_x = [i[0] for i in title_sequences]\ntitle_y = [i[1] for i in title_sequences]\n\n# pad sentences to use batches\ntitle_padded_x = torch.nn.utils.rnn.pad_sequence(title_x, batch_first=True)\ntitle_x = [i for i in title_padded_x]\n\ntitle_x_train, title_x_test, title_y_train, title_y_test = train_test_split(title_x, title_y, test_size=0.2, random_state=42)\ntitle_test_sequences = list(zip(title_x_test,title_y_test))\ntitle_test_sequences = [list(x) for x in title_test_sequences]\ntitle_train_sequences = list(zip(title_x_train,title_y_train))\ntitle_train_sequences = [list(x) for x in title_train_sequences]\n\ninput_size = vocab.n_words\nembedding_size = 300\nhidden_sizes = [300, 500]\noutput_size = len(vocab.id2tag)\nn_layers = [2,3]\ndirections = [2]\nn_epochs = 10\ndropouts = [0.2]\nbatch_sizes = [32]\nresult_df = pd.DataFrame([])\n\n#Only title\nprint(\"Train only on Titles\")\nfor hidden_size in hidden_sizes:\n    for layers in n_layers:\n        for direction in directions:\n            for dropout in dropouts:\n              for batch_size in batch_sizes:\n                caption = f\"hidden_size - {hidden_size}, n_layers - {layers}, directions - {direction}, dropout {dropout}\"\n                print(caption)\n                model = LSTMNERNet(input_size, embedding_size, hidden_size, output_size, layers, direction, dropout)\n                train_loop(model, n_epochs, batch_size, title_train_sequences, title_test_sequences)\n                train_accuracy = evaluate(title_train_sequences, batch_size)\n                test_accuracy = evaluate(title_test_sequences, batch_size)\n                temp_df = pd.DataFrame([[train_accuracy, test_accuracy]], index=[caption], columns=[\"training_accuracy\", \"test_accuracy\"])\n                result_df = result_df.append(temp_df)\n\n# PreProcess data\ntext_sequences, vocab = prepare_data(df, vocab, \"text\")\ntext_x = [i[0] for i in text_sequences]\ntext_y = [i[1] for i in text_sequences]\n\n# pad sentences to use batches\ntext_padded_x = torch.nn.utils.rnn.pad_sequence(text_x, batch_first=True)\ntext_x = [i for i in text_padded_x]\n\ntext_x_train, text_x_test, text_y_train, text_y_test = train_test_split(text_x, text_y, test_size=0.2, random_state=42)\ntext_test_sequences = list(zip(text_x_test,text_y_test))\ntext_test_sequences = [list(x) for x in text_test_sequences]\ntext_train_sequences = list(zip(text_x_train,text_y_train))\ntext_train_sequences = [list(x) for x in text_train_sequences]\n\ninput_size = vocab.n_words\nembedding_size = 300\nhidden_sizes = [500]\noutput_size = len(vocab.id2tag)\nn_layers = [2]\ndirections = [2]\nn_epochs = 6\ndropouts = [0.2]\nbatch_sizes = [32]\nresult_df = pd.DataFrame([])\n\n\n#Only on text\nprint(\"Train only on Text\")\nfor hidden_size in hidden_sizes:\n    for layers in n_layers:\n        for direction in directions:\n            for dropout in dropouts:\n              for batch_size in batch_sizes:\n                caption = f\"hidden_size - {hidden_size}, n_layers - {layers}, directions - {direction}, dropout {dropout}\"\n                print(caption)\n                model = LSTMNERNet(input_size, embedding_size, hidden_size, output_size, layers, direction, dropout)\n                train_loop(model, n_epochs, batch_size, text_train_sequences, text_test_sequences)\n                train_accuracy = evaluate(text_train_sequences, batch_size)\n                test_accuracy = evaluate(text_test_sequences, batch_size)\n                temp_df = pd.DataFrame([[train_accuracy, test_accuracy]], index=[caption], columns=[\"training_accuracy\", \"test_accuracy\"])\n                result_df = result_df.append(temp_df)","metadata":{"id":"x6AxlLGdhuCR","outputId":"01843e71-da59-48b0-b5f5-0927f95808c6","execution":{"iopub.status.busy":"2021-05-24T16:19:10.685017Z","iopub.execute_input":"2021-05-24T16:19:10.686482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try diffrent Hyper params for the model\n\ninput_size = vocab.n_words\nembedding_size = 300\nhidden_sizes = [300, 500]\noutput_size = len(vocab.id2tag)\nn_layers = [2,3]\ndirections = [2]\nn_epochs = 10\ndropouts = [0.2]\nbatch_sizes = [32]\nresult_df = pd.DataFrame([])\n\nfor hidden_size in hidden_sizes:\n    for layers in n_layers:\n        for direction in directions:\n            for dropout in dropouts:\n              for batch_size in batch_sizes:\n                caption = f\"hidden_size - {hidden_size}, n_layers - {layers}, directions - {direction}, dropout {dropout}\"\n                print(caption)\n                model = LSTMNERNet(input_size, embedding_size, hidden_size, output_size, layers, direction, dropout)\n                train_loop(model, n_epochs, batch_size, train_sequences, test_sequences)\n                train_accuracy = evaluate(train_sequences, batch_size)\n                test_accuracy = evaluate(test_sequences, batch_size)\n                temp_df = pd.DataFrame([[train_accuracy, test_accuracy]], index=[caption], columns=[\"training_accuracy\", \"test_accuracy\"])\n                result_df = result_df.append(temp_df)","metadata":{"id":"LVfXeyPLPfyG","outputId":"89e2845e-f4a5-49d2-94ee-b8d3eaa17b4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df","metadata":{"id":"U7ayuTF8rV_d","outputId":"c9207cb0-be40-44b7-fd14-86202b6336bc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data_path = '../input/real-and-fake-news-dataset/news.csv'\ndestination_folder = './'\nsource_folder = '../input/real-and-fake-news-dataset'\n\ntrain_test_ratio = 0.10\ntrain_valid_ratio = 0.80\n\nfirst_n_words = 200","metadata":{"id":"KD0KzUg89BQ3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trim_string(x):\n\n    x = x.split(maxsplit=first_n_words)\n    x = ' '.join(x[:first_n_words])\n\n    return x","metadata":{"id":"Cs5_Ychu8gkF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read raw data\ndf_raw = pd.read_csv(raw_data_path)\n\n# Prepare columns\ndf_raw['label'] = (df_raw['label'] == 'FAKE').astype('int')\ndf_raw['titletext'] = df_raw['title'] + \". \" + df_raw['text']\ndf_raw = df_raw.reindex(columns=['label', 'title', 'text', 'titletext'])\n\n# Drop rows with empty text\ndf_raw.drop( df_raw[df_raw.text.str.len() < 5].index, inplace=True)\n\n# Trim text and titletext to first_n_words\ndf_raw['text'] = df_raw['text'].apply(trim_string)\ndf_raw['titletext'] = df_raw['titletext'].apply(trim_string) \n\n# Split according to label\ndf_real = df_raw[df_raw['label'] == 0]\ndf_fake = df_raw[df_raw['label'] == 1]\n\n# Train-test split\ndf_real_full_train, df_real_test = train_test_split(df_real, train_size = train_test_ratio, random_state = 1)\ndf_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = train_test_ratio, random_state = 1)\n\n# Train-valid split\ndf_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = train_valid_ratio, random_state = 1)\ndf_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state = 1)\n\n# Concatenate splits of different labels\ndf_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\ndf_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\ndf_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)\n\n# Write preprocessed data\ndf_train.to_csv(destination_folder + '/train.csv', index=False)\ndf_valid.to_csv(destination_folder + '/valid.csv', index=False)\ndf_test.to_csv(destination_folder + '/test.csv', index=False)","metadata":{"id":"Va0jv-in821H","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Model parameter\nMAX_SEQ_LEN = 128\nPAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nUNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n\n# Fields\n\nlabel_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\ntext_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\nfields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n\n# TabularDataset\n\ntrain, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n\n# Iterators\n\ntrain_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n                            device=DEVICE, train=True, sort=True, sort_within_batch=True)\nvalid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n                            device=DEVICE, train=True, sort=True, sort_within_batch=True)\ntest_iter = Iterator(test, batch_size=16, device=DEVICE, train=False, shuffle=False, sort=False)\n","metadata":{"id":"rS7ls6AQ6cnO","outputId":"5022e2ff-ec52-4abb-b048-0bf1abb818e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT(nn.Module):\n\n    def __init__(self):\n        super(BERT, self).__init__()\n\n        options_name = \"bert-base-uncased\"\n        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n\n    def forward(self, text, label):\n        loss, text_fea = self.encoder(text, labels=label)[:2]\n\n        return loss, text_fea","metadata":{"id":"PkmyJ5JR11a-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Function\n\ndef train(model,\n          optimizer,\n          criterion = nn.BCELoss(),\n          train_loader = train_iter,\n          valid_loader = valid_iter,\n          num_epochs = 5,\n          eval_every = len(train_iter) // 2,\n          best_valid_loss = float(\"Inf\")):\n    \n    # initialize running values\n    running_loss = 0.0\n    valid_running_loss = 0.0\n    global_step = 0\n    train_loss_list = []\n    valid_loss_list = []\n    global_steps_list = []\n\n    # training loop\n    model.train()\n    for epoch in range(num_epochs):\n        for (labels, title, text, titletext), _ in train_loader:\n            labels = labels.type(torch.LongTensor)           \n            labels = labels.to(DEVICE)\n            titletext = titletext.type(torch.LongTensor)  \n            titletext = titletext.to(DEVICE)\n            output = model(titletext, labels)\n            loss, _ = output\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # update running values\n            running_loss += loss.item()\n            global_step += 1\n\n            # evaluation step\n            if global_step % eval_every == 0:\n                model.eval()\n                with torch.no_grad():                    \n\n                    # validation loop\n                    for (labels, title, text, titletext), _ in valid_loader:\n                        labels = labels.type(torch.LongTensor)           \n                        labels = labels.to(DEVICE)\n                        titletext = titletext.type(torch.LongTensor)  \n                        titletext = titletext.to(DEVICE)\n                        output = model(titletext, labels)\n                        loss, _ = output\n                        \n                        valid_running_loss += loss.item()\n\n                # evaluation\n                average_train_loss = running_loss / eval_every\n                average_valid_loss = valid_running_loss / len(valid_loader)\n                train_loss_list.append(average_train_loss)\n                valid_loss_list.append(average_valid_loss)\n                global_steps_list.append(global_step)\n\n                # resetting running values\n                running_loss = 0.0                \n                valid_running_loss = 0.0\n                model.train()\n\n                # print progress\n                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n                              average_train_loss, average_valid_loss))\n                \n    print('Finished Training!')\n\nmodel = BERT().to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\n\ntrain(model=model, optimizer=optimizer)","metadata":{"id":"V49T6BCN13Dg","outputId":"2817a811-2736-40c8-b80f-7ead7f6511b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation Function\n\ndef evaluate(model, test_loader):\n    y_pred = []\n    y_true = []\n\n    model.eval()\n    with torch.no_grad():\n        for (labels, title, text, titletext), _ in test_loader:\n\n                labels = labels.type(torch.LongTensor)           \n                labels = labels.to(DEVICE)\n                titletext = titletext.type(torch.LongTensor)  \n                titletext = titletext.to(DEVICE)\n                output = model(titletext, labels)\n\n                _, output = output\n                y_pred.extend(torch.argmax(output, 1).tolist())\n                y_true.extend(labels.tolist())\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n    \n    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n\n    ax.set_title('Confusion Matrix')\n\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n\n    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])\n\n\nevaluate(model, test_iter)","metadata":{"id":"cN7srEvODyia","outputId":"6d6307e0-c5aa-4ada-a683-49153bac8b05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank you for reading","metadata":{}}]}