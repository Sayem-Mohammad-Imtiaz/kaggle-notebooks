{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns\nimport plotly.express as px\nfrom itertools import product\nimport warnings\nimport statsmodels.api as sm\nplt.style.use('seaborn-darkgrid')\n\n#matplotlib inline\n\nfrom pandas.plotting import lag_plot\nimport datetime as dt\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n#arime as in notebook https://www.kaggle.com/akashmathur2212/bitcoin-price-prediction-arima-xgboost-lstm-fbprop/data#ARIMA-Model\n#import pmdarima as pm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading the csv file\nbitstamp = pd.read_csv(\"/kaggle/input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv\")\nbitstamp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at the data:\nbitstamp.info()\nprint(\"\\n\\/\\/Some basic aggregations\\/\\/\")\nbitstamp.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the Timestamp column from string to datetime\nbitstamp['Timestamp'] = [datetime.fromtimestamp(x) for x in bitstamp['Timestamp']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#some basic visualization:\nbitstamp.set_index(\"Timestamp\").Weighted_Price.plot(figsize=(14,7), title=\"Bitcoin Weighted Price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Handling missing or 'defect' data:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating missing values in the dataset\n#counts number of rows with NaN-values per column:\nmissing_values = bitstamp.isnull().sum()\n#calculates percentage of nan rows out of all:\nmissing_per = (missing_values/bitstamp.shape[0])*100\n\n#displaying missing values in a table:\nmissing_table = pd.concat([missing_values,missing_per], axis=1, ignore_index=True) \nmissing_table.rename(columns={0:'Total Missing Values',1:'Missing %'}, inplace=True)\n\n#printing the table:\nmissing_table\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### over a fourth of the dataset consists of NaN values"},{"metadata":{},"cell_type":"markdown","source":"## Imputation using Linear Interpolation method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_missing(df):\n    ### function to impute missing values using interpolation ###\n    df['Open'] = df['Open'].interpolate()\n    df['Close'] = df['Close'].interpolate()\n    df['Weighted_Price'] = df['Weighted_Price'].interpolate()\n\n    df['Volume_(BTC)'] = df['Volume_(BTC)'].interpolate()\n    df['Volume_(Currency)'] = df['Volume_(Currency)'].interpolate()\n    df['High'] = df['High'].interpolate()\n    df['Low'] = df['Low'].interpolate()\n\n#     print(df.head())\n#     print(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cleaning the dataset with linear interpolation\nfill_missing(bitstamp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#indexing the dataset:\n\n#created a copy \nbitstamp_non_indexed = bitstamp.copy()\n\nbitstamp = bitstamp.set_index('Timestamp')\nbitstamp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(bitstamp['Weighted_Price'], shade=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lag plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,13))\nplt.suptitle('Lag Plots', fontsize=22)\n\n#lag in minutes\nplt.subplot(3,3,1)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=1) #minute lag\nplt.title('1-Minute Lag')\n\nplt.subplot(3,3,2)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=60) #hourley lag\nplt.title('1-Hour Lag')\n\nplt.subplot(3,3,3)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=1440) #Daily lag\nplt.title('Daily Lag')\n\nplt.subplot(3,3,4)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=10080) #weekly lag\nplt.title('Weekly Lag')\n\nplt.subplot(3,3,5)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=43200) #month lag\nplt.title('1-Month Lag')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hourly_data = bitstamp.resample('1H').mean()\nhourly_data = hourly_data.reset_index()\n\nhd = hourly_data.copy()\n\nhourly_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitstamp_daily = bitstamp.resample(\"24H\").mean() #daily resampling","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time series decomposition and statistical tests"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import kpss\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing(bitstamp_daily)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,14))\nseries = bitstamp_daily.Weighted_Price\nresult = seasonal_decompose(series, model='additive',period=1).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting auto correlation function\nacf = plot_acf(series, lags=50, alpha=0.05)\nplt.title(\"ACF for Weighted Price\", size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting partial auto correlation function\nplot_pacf(series, lags=50, alpha=0.05, method='ols')\nplt.title(\"PACF for Weighted Price\", size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = bitstamp_daily","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reset_index(drop=False, inplace=True)\n\nlag_features = [\"Open\", \"High\", \"Low\", \"Close\",\"Volume_(BTC)\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Timestamp\", drop=False, inplace=True)\ndf.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"month\"] = df.Timestamp.dt.month\ndf[\"week\"] = df.Timestamp.dt.week\ndf[\"day\"] = df.Timestamp.dt.day\ndf[\"day_of_week\"] = df.Timestamp.dt.dayofweek\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df[df.Timestamp < \"2020\"]\ndf_valid = df[df.Timestamp >= \"2020\"]\n\nprint('train shape :', df_train.shape)\nprint('validation shape :', df_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# arima as in kaggle notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pmdarima as pm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exogenous_features = ['Open_mean_lag3',\n       'Open_mean_lag7', 'Open_mean_lag30', 'Open_std_lag3', 'Open_std_lag7',\n       'Open_std_lag30', 'High_mean_lag3', 'High_mean_lag7', 'High_mean_lag30',\n       'High_std_lag3', 'High_std_lag7', 'High_std_lag30', 'Low_mean_lag3',\n       'Low_mean_lag7', 'Low_mean_lag30', 'Low_std_lag3', 'Low_std_lag7',\n       'Low_std_lag30', 'Close_mean_lag3', 'Close_mean_lag7',\n       'Close_mean_lag30', 'Close_std_lag3', 'Close_std_lag7',\n       'Close_std_lag30', 'Volume_(BTC)_mean_lag3', 'Volume_(BTC)_mean_lag7',\n       'Volume_(BTC)_mean_lag30', 'Volume_(BTC)_std_lag3',\n       'Volume_(BTC)_std_lag7', 'Volume_(BTC)_std_lag30', 'month', 'week',\n       'day', 'day_of_week']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pm.auto_arima(df_train.Weighted_Price, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel_cheat = pm.auto_arima(df_train.Weighted_Price, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\n\nmodel_test = pm.auto_arima(df_train.Weighted_Price, trace=True, error_action=\"ignore\", suppress_warnings=True)\n\n# found best models are\n# 1: ARIMA(1,0,2)(0,0,0)[0]\n# 2: \n# 3: ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cheat.fit(df_train.Weighted_Price, exogenous=df_train[exogenous_features])\nmodel.fit(df_train.Weighted_Price)\n\nmodel_test.fit(df_train.Weighted_Price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_cheat, conf_int_cheat = model_cheat.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features], return_conf_int=True)\nforecast, conf_int = model.predict(n_periods=258, return_conf_int=True)\n\nprint(len(df_valid))\ndf_valid_cheat = df_valid.copy()\n\ndf_valid[\"Forecast_ARIMAX\"] = forecast\ndf_valid_cheat[\"Forecast_ARIMAX\"] = forecast_cheat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_int_lower_cheat = []\nconf_int_upper_cheat = []\nfor x in conf_int_cheat:\n    conf_int_lower_cheat.append(x[0])\n    conf_int_upper_cheat.append(x[1])\n\ndf_valid_cheat[\"conf_int_lower\"] = conf_int_lower_cheat\ndf_valid_cheat[\"conf_int_upper\"] = conf_int_upper_cheat\n\n#actual prediction\nconf_int_lower = []\nconf_int_upper = []\nfor x in conf_int:\n    conf_int_lower.append(x[0])\n    conf_int_upper.append(x[1])\n\ndf_valid[\"conf_int_lower\"] = conf_int_lower\ndf_valid[\"conf_int_upper\"] = conf_int_upper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid[[\"Weighted_Price\", \"Forecast_ARIMAX\", \"conf_int_lower\",\"conf_int_upper\"]].plot(figsize=(14, 7))\n\ndf_valid_cheat[[\"Weighted_Price\", \"Forecast_ARIMAX\"]].plot(figsize=(14, 7))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# using facebook prophet model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resampling originial data to day level and forward fill the missing values\npData_D = bitstamp.resample(\"24H\").mean() #daily resampling\nfill_missing(pData_D)\n\n#renaming for prophet\npData_D = pData_D.reset_index()[['Timestamp','Close']]\npData_D = pData_D.rename(columns = {\"Timestamp\":\"ds\",\"Close\":\"y\"})\npData_D.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\nm_prophet = Prophet(daily_seasonality = True) # the Prophet class (model)\nm_prophet.fit(pData_D) # fit the model using all data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#periods specifies how many days into the future the prediction should go\nfuture = m_prophet.make_future_dataframe(periods=365)\nprediction = m_prophet.predict(future)\nm_prophet.plot(prediction)\n\nplt.title(\"Prediction of BTC price using Prophet\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Close BTC Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"legend:\n* black dots: actual data\n* blue line: prediction\n* ligh blue area: confidence interval"},{"metadata":{"trusted":true},"cell_type":"code","source":"m_prophet.plot_components(prediction)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"conclusion from the above plots:\n* first plot: estimated trend is positiv -> prophet expects prices to rise in the future\n* second plot: based on the estimated trends, price is max mostly on Tuesdays and Saturdays\n* third plot: based on the estimated trends, price is max in August and late December"},{"metadata":{},"cell_type":"markdown","source":"# using LSTM modell"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nimport keras\nimport pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using copy of hourly_data: hd\nhd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_train, lstm_valid = hd[:int(len(hd)*0.7)], hd[int(len(hd)*0.7):]\n\nprint(\"train shape:\", lstm_train.shape)\nprint(\"valid shape:\", lstm_valid.shape)\n\nlstm_v = lstm_valid.iloc[:, 1:2].values\nlstm_t = lstm_train.iloc[:, 1:2].values\n\nprint(lstm_t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## building the input features with a timelag of 1 day"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nsc = MinMaxScaler(feature_range = (0, 1))\ntraining_set_scaled = sc.fit_transform(lstm_t)\n\n# Creating a data structure with 60 time-steps and 1 output\nX_train = []\ny_train = []\nfor i in range(60, int(len(lstm_train))):\n    X_train.append(training_set_scaled[i-60:i, 0])\n    y_train.append(training_set_scaled[i, 0])\n    \nX_train, y_train = np.array(X_train), np.array(y_train)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n#(740, 60, 1)\n\nprint(\"xtrain shape: \", X_train.shape)\nprint(\"ytrain shape: \", y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = Sequential()\n\n# #Adding the first LSTM layer and some Dropout regularisation\n# model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n# model.add(Dropout(0.2))\n\n# # Adding a second LSTM layer and some Dropout regularisation\n# model.add(LSTM(units = 50, return_sequences = True))\n# model.add(Dropout(0.2))\n\n# # Adding a third LSTM layer and some Dropout regularisation\n# model.add(LSTM(units = 50, return_sequences = True))\n# model.add(Dropout(0.2))\n\n# # Adding a fourth LSTM layer and some Dropout regularisation\n# model.add(LSTM(units = 50))\n# model.add(Dropout(0.2))\n\n# # Adding the output layer\n# model.add(Dense(units = 1))\n\n# # Compiling the RNN\n# model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# # Fitting the RNN to the Training set\n# model.fit(X_train, y_train, epochs = 100, batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_train, lstm_valid = hd[:int(len(hd)*0.7)], hd[int(len(hd)*0.7):]\n\nprint(\"train shape:\", lstm_train.shape)\nprint(\"valid shape:\", lstm_valid.shape)\n\nlstm_v = lstm_valid.iloc[:, 1:2]\nlstm_t = lstm_train.iloc[:, 1:2]\n\ndataset_total = pd.concat((lstm_t, lstm_v), axis = 0)\n\ninputs = dataset_total[len(dataset_total) - len(lstm_v) - 60:].values\n\nprint(inputs)\n\ninputs = inputs.reshape(-1,1)\ninputs = sc.transform(inputs)\n\nX_test = []\nfor i in range(60, 519):\n    X_test.append(inputs[i-60:i, 0])\n    \nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\nprint(X_test.shape)\n# (459, 60, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_btc_price = model.predict(X_test)\npredicted_btc_price = sc.inverse_transform(predicted_btc_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the results\n# plt.plot(hd.loc[int(len(hd)*0.7):int(len(hd)), \"Timestamp\"],lstm_t.values, color = \"red\", label = \"Real btc Price\")\nplt.plot(hd.loc[:458, \"Timestamp\"],predicted_stock_price, color = \"blue\", label = \"Predicted btc Price\")\n\nplt.xticks(np.arange(0,459,50))\n\nplt.title('btc Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('btc Price')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# using ARIMA modell"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Resampling originial data to day level and forward fill the missing values\n# aData_D = bitstamp_non_indexed\n# fill_missing(aData_D)\n\n# aData_D.Timestamp = aData_D.Timestamp.apply(lambda x: dt.datetime(x.year,x.month,x.day))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aData_D = aData_D.groupby(\"Timestamp\", as_index = False).agg(\"mean\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #checking for cross-correlation in the dataset:\n# plt.figure()\n# lag_plot(aData_D['Open'], lag=3)\n# plt.title('BTC price - Autocorrelation plot with lag = 3')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #splittin the data in test and training sets:\n# #training data: 70% of dataset\n# #test data: 30% of dataset\n# arima_train_data, arima_test_data = aData_D[int(len(aData_D)*0.3):int(len(aData_D)*0.7)], aData_D[int(len(aData_D)*0.7):]\n\n# arima_training_data = arima_train_data['Close'].values\n# arima_test_data = arima_test_data['Close'].values\n\n# #history array containing all observations, during model creation the real datapoint of the test data\n# # will be appended step by step\n# history = [x for x in arima_training_data]\n# model_predictions = []\n# N_test_observations = len(arima_test_data)\n\n\n# #p: The number of lag observations included in the model, also called the lag order.\n# #d: The number of times that the raw observations are differenced, also called the degree of dfferencing.\n# #q: The size of the moving average window, also called the order of moving average.\n\n# for time_point in range(N_test_observations):\n#     model = ARIMA(history, order=(1,1,20)) #param: p, d, q\n#     model_fit = model.fit(disp=0)\n#     output = model_fit.forecast()\n#     yhat = output[0]\n#     model_predictions.append(yhat)\n#     true_test_value = arima_test_data[time_point]\n#     history.append(true_test_value)\n    \n# MSE_error = mean_squared_error(arima_test_data, model_predictions)\n# print('Testing Mean Squared Error is {}'.format(MSE_error))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_set_range = aData_D[int(len(aData_D)*0.7):].index\n\n# plt.plot(test_set_range, model_predictions, color='blue', marker='o', linestyle='dashed',label='Predicted Price')\n# plt.plot(test_set_range, arima_test_data, color='red', label='Actual Price')\n\n# plt.title('BTC Prices Prediction')\n# plt.xlabel('Date')\n# plt.ylabel('Prices')\n# plt.xticks(np.arange(881,1259,50), aData_D.Timestamp[881:1259:50])\n# plt.legend()\n# plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}