{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/car-price-preprocessed/Car_price_preprocessed.csv')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the PDFs","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's first take a quick look at the descriptive statiscs of our features, this will give us a first idea of whether there are significant outliers or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = None\npd.options.display.max_rows = None\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the table above whe can say that there are not many outliers in our numerical variables, but in order to confirm this hypothesis let's take a look at the PDFs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['normalized-losses'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get rid of the outliers that we see in the right side of the PDF.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q = data['normalized-losses'].quantile(0.99)\ndata = data[data['normalized-losses']<q]\nsns.distplot(data['normalized-losses'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['wheel-base'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['length'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['width'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['height'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['curb-weight'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['engine-size'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Repeating the same process for engine size.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q = data['engine-size'].quantile(0.99)\ndata = data[data['engine-size']<q]\nsns.distplot(data['engine-size'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['bore'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case we see that the outliers are in the left side, but the logic of the process is still the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q = data['bore'].quantile(0.01)\ndata = data[data['bore']>q]\nsns.distplot(data['bore'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['stroke'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['compression-ratio'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first sight it seems like we have some outliers in the compression ratio feature, but a quick google search show us that the common range for compression ratio in for gas motors is 7-13 and for diesel motors is 16-24, so let's check if this explains what we see in the PDF.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['compression-ratio'] >= 20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['compression-ratio'] <=13]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as expected, the two groups we saw in the compression ratio PDF where separated based on fuel type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['horsepower'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['peak-rpm'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['city-mpg'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['highway-mpg'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the OLS assumptions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's plot the numerical values against the price to see what kind of relationship we can observe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = ['symboling', 'normalized-losses', 'wheel-base', 'length', 'width', 'height', 'curb-weight',\n       'num-of-cylinders', 'engine-size', 'bore', 'stroke', 'compression-ratio',\n       'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(16):\n    plt.scatter(data[num_features[i]], data['price'])\n    plt.title('Price and {}'.format(num_features[i]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(16):\n    plt.scatter(data[num_features[i]], np.log(data['price']))\n    plt.title('Log Price and {}'.format(num_features[i]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(16):\n    plt.scatter(np.log(data[num_features[i]]), np.log(data['price']))\n    plt.title('Log Price and Log {}'.format(num_features[i]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the previous plots we can say that a log transformation for the dependent variable will relax the linearity assumption for most of our variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Log-price'] = np.log(data['price'])\ndata.drop(['price'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also see that some features aren't helpful to represent the behavior of the price, so we can just drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['compression-ratio'], axis=1, inplace=True)\ndata.drop(['stroke'], axis=1, inplace=True)\ndata.drop(['peak-rpm'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots also show that the number of cylinders just make a difference in the price in two case, when there are 4 cilinders and when there are more than four, based on this we can turn it into a binomial variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['num-of-cylinders'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['num-of-cylinders'] = data['num-of-cylinders'].map({4:0, 5:1, 6:1})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename(columns={'num-of-cylinders':'+4 cylinders'})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also take the logarithm of city-mpg and highway-mpg, but we can expect this variables to be highly correlated so let's just keep one of them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['city-mpg'], axis=1, inplace=True)\ndata['highway-mpg'] = np.log(data['highway-mpg'])\ndata.rename(columns={'highway-mpg':'Log highway-mpg'}, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make an special transformation for symboling feature and try to make it more lineal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(abs(data['symboling'] - 1) , data['Log-price'] ** 0.5)\nplt.show()\ndata['symboling'] = abs(data['symboling'] - 1)\ndata.rename(columns={'symboling':'abs(symb-1)'}, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_data = data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's now check for multicolinearity with VIF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvariables = data[['abs(symb-1)', 'normalized-losses',\n       'wheel-base', 'length', 'width', 'height', 'curb-weight',\n       'engine-size', 'bore', 'horsepower', 'Log highway-mpg']]\n\nvif = pd.DataFrame()\nvif['Features'] = variables.columns\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We start eliminating highly correlated features until we end just with the following:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"variables = data[['abs(symb-1)','normalized-losses']]\n\nvif = pd.DataFrame()\nvif['Features'] = variables.columns\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['wheel-base', 'length', 'width', 'height', 'curb-weight',\n       'engine-size', 'bore', 'horsepower', 'Log highway-mpg']:\n    data.drop([i], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = data['Log-price']\ninputs = data.drop(['Log-price'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now scale our input data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(inputs)\nscaled_inputs = scaler.transform(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_inputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we should split our data into training and testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, test_size=0.2, random_state=97)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nreg =  LinearRegression()\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = reg.predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_train, y_hat)\n# Let's also name the axes\nplt.xlabel('Targets (y_train)',size=18)\nplt.ylabel('Predictions (y_hat)',size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another useful check of our model is a residual plot\n# We can plot the PDF of the residuals and check for anomalies\nsns.distplot(y_train - y_hat)\n\n# Include a title\nplt.title(\"Residuals PDF\", size=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now find the R-Squared of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the weights and bias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\nreg_summary['Weights'] = reg.coef_\nreg_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_test = reg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_hat_test)\nplt.xlabel('Targets (y_test)',size=18)\nplt.ylabel('Predictions (y_hat_test)',size=18)\nplt.xlim(8.5,10.5)\nplt.ylim(8.5,10.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pf = pd.DataFrame(np.exp(y_hat_test), columns=['Prediction'])\ndf_pf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = y_test.reset_index(drop=True)\ndf_pf['Target'] = np.exp(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pf['Residual'] = df_pf['Target'] - df_pf['Prediction']\ndf_pf['Difference%'] = np.absolute(df_pf['Residual']/df_pf['Target']*100)\ndf_pf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our average Difference% is 12.4% and the test R-Squared is 85.3%, so the predictive analysis has been succesful.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}