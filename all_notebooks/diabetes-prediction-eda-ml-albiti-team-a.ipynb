{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>EDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Column-이해하기\" data-toc-modified-id=\"Column-이해하기-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Column 이해하기</a></span></li><li><span><a href=\"#Null-&amp;-DType-확인\" data-toc-modified-id=\"Null-&amp;-DType-확인-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Null &amp; DType 확인</a></span></li><li><span><a href=\"#기초-통계량\" data-toc-modified-id=\"기초-통계량-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>기초 통계량</a></span></li><li><span><a href=\"#Outcome-별-Distribution-Plot\" data-toc-modified-id=\"Outcome-별-Distribution-Plot-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Outcome 별 Distribution Plot</a></span></li><li><span><a href=\"#Box-Plot\" data-toc-modified-id=\"Box-Plot-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Box Plot</a></span></li><li><span><a href=\"#상관계수\" data-toc-modified-id=\"상관계수-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>상관계수</a></span></li></ul></li><li><span><a href=\"#전처리\" data-toc-modified-id=\"전처리-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>전처리</a></span><ul class=\"toc-item\"><li><span><a href=\"#0(zero)-값-처리\" data-toc-modified-id=\"0(zero)-값-처리-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>0(zero) 값 처리</a></span></li><li><span><a href=\"#변수-변환\" data-toc-modified-id=\"변수-변환-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>변수 변환</a></span></li><li><span><a href=\"#스케일링\" data-toc-modified-id=\"스케일링-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>스케일링</a></span></li><li><span><a href=\"#오버-샘플링\" data-toc-modified-id=\"오버-샘플링-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>오버 샘플링</a></span></li></ul></li><li><span><a href=\"#모델\" data-toc-modified-id=\"모델-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>모델</a></span><ul class=\"toc-item\"><li><span><a href=\"#Confusion-Matrix-Function\" data-toc-modified-id=\"Confusion-Matrix-Function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Confusion Matrix Function</a></span></li><li><span><a href=\"#Import-Modules\" data-toc-modified-id=\"Import-Modules-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Import Modules</a></span></li><li><span><a href=\"#Models-we-tried\" data-toc-modified-id=\"Models-we-tried-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Models we tried</a></span></li><li><span><a href=\"#결과-도출\" data-toc-modified-id=\"결과-도출-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>결과 도출</a></span></li></ul></li><li><span><a href=\"#최종-결과\" data-toc-modified-id=\"최종-결과-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>최종 결과</a></span></li></ul></div>","metadata":{"toc":true}},{"cell_type":"markdown","source":"______________________________\n# Albiti-4th-Week-TeamA\n\\- **김서연, 서예진, 최정윤** - \n\n1. 목표 : 당뇨 관련 지표들에 대한 이해 – open data 및 meta info.\n2. 기한 : 2021.05.31 ~ 2021.06.06.\n3. Task 1. Pima dataset을 사용한 분류모델 구축.\n    - Kaggle에서 Pima dataset 다운로드.   \n    - Accuracy 70% 이상, F1 70% 이상 모델 구축! \n4. Task 2. Higher and higher.\n    - Accuracy 85% 이상, F1 85% 이상.\n\nhttps://www.kaggle.com/uciml/pima-indians-diabetes-database\n________________________________","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import PercentFormatter\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:20.975219Z","iopub.execute_input":"2021-06-06T17:07:20.975801Z","iopub.status.idle":"2021-06-06T17:07:21.392541Z","shell.execute_reply.started":"2021-06-06T17:07:20.975681Z","shell.execute_reply":"2021-06-06T17:07:21.391442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:21.394407Z","iopub.execute_input":"2021-06-06T17:07:21.394685Z","iopub.status.idle":"2021-06-06T17:07:21.420328Z","shell.execute_reply.started":"2021-06-06T17:07:21.394659Z","shell.execute_reply":"2021-06-06T17:07:21.419124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Column 이해하기","metadata":{}},{"cell_type":"markdown","source":"`Pregnancies`  : Number of times pregnant\n\n`Glucose` : Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n`BloodPressure` : Diastolic blood pressure (mm Hg)\n\n`SkinThickness` : Triceps skin fold thickness (mm)\n\n`Insulin` : 2-Hour serum insulin (mu U/ml)\n\n`BMI` : Body mass index (weight in kg/(height in m)^2)\n\n`DiadbetesPedigreeFunction` : Diabetes pedigree function\n\n`Age` : Age (years)\n\n`Outcome` : Class variable (0 or 1) 268 of 768 are 1, the others are 0","metadata":{}},{"cell_type":"markdown","source":"### Null & DType 확인\n- 결측치 없음.\n- object column 없음.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:21.421712Z","iopub.execute_input":"2021-06-06T17:07:21.422004Z","iopub.status.idle":"2021-06-06T17:07:21.437823Z","shell.execute_reply.started":"2021-06-06T17:07:21.421977Z","shell.execute_reply":"2021-06-06T17:07:21.436907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 기초 통계량\n- min이 0인 column이 많은 것을 확인할 수 있다.\n- 그런데, 대다수의 column은 0이라는 값을 가질 수 없다. (eg. BloodPressure)","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:21.439301Z","iopub.execute_input":"2021-06-06T17:07:21.439615Z","iopub.status.idle":"2021-06-06T17:07:21.481966Z","shell.execute_reply.started":"2021-06-06T17:07:21.439584Z","shell.execute_reply":"2021-06-06T17:07:21.480885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Outcome 별 Distribution Plot\n- 0 값을 가지는 data가 상당히 많다는 것을 확인할 수 있다. (eg. Insulin)\n- Outcome의 비율이 2:1 정도로 imbalance 한 것도 확인할 수 있다.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(12,10))\n\nfor i in range(len(df.columns)-1):\n    name = df.columns[i]\n    sns.distplot(df[df['Outcome'] == 0][name], color='green', ax=ax[i//3, i%3])\n    sns.distplot(df[df['Outcome'] == 1][name], color='red', ax=ax[i//3, i%3])\n    ax[i//3, i%3].set_title(f'Healthy vs Diabetic by {name}')\n\nax[2, 2] = sns.countplot(x='Outcome', data=df)\nax[2, 2].set_title('Outcome')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:21.485763Z","iopub.execute_input":"2021-06-06T17:07:21.486063Z","iopub.status.idle":"2021-06-06T17:07:23.76365Z","shell.execute_reply.started":"2021-06-06T17:07:21.486036Z","shell.execute_reply":"2021-06-06T17:07:23.762434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Box Plot\n- 몇 feature들에게서 엄청나게 많은 outlier들을 확인할 수 있다.\n- 0 값의 영향을 받은 것처럼 보이는 feature도 존재한다.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(12,8))\n\nfor i in range(len(df.columns)-1):\n    name = df.columns[i]\n    sns.boxplot(x= df[name], ax=ax[i//3, i%3])\n    ax[i//3, i%3].set_title(f'{name} Box Plot')\n\nax[2, 2] = sns.countplot(x='Outcome', data=df)\nax[2, 2].set_title('Outcome')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:23.765846Z","iopub.execute_input":"2021-06-06T17:07:23.766334Z","iopub.status.idle":"2021-06-06T17:07:24.932438Z","shell.execute_reply.started":"2021-06-06T17:07:23.76629Z","shell.execute_reply":"2021-06-06T17:07:24.931406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 상관계수\n- 변수 삭제는 하지 않는 것으로 하였다.","metadata":{}},{"cell_type":"code","source":"cor = df.corr()\ncor = cor.corr(method = 'pearson')\nmask = np.triu(np.ones_like(cor, dtype=bool))\nfig, ax = plt.subplots(figsize=(6, 6))  \ncorr_heatmap = sns.heatmap(cor, mask = mask, cbar = True, annot = True, annot_kws={'size' : 9}, fmt = '.2f', square = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:24.93423Z","iopub.execute_input":"2021-06-06T17:07:24.934681Z","iopub.status.idle":"2021-06-06T17:07:25.372578Z","shell.execute_reply.started":"2021-06-06T17:07:24.934636Z","shell.execute_reply":"2021-06-06T17:07:25.371538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 전처리\n### 0(zero) 값 처리","metadata":{}},{"cell_type":"markdown","source":"- 0 값을 가질 수 없는 data 처리의 필요성이 느껴짐\n- 대상 Column: Glucose, BloodPressure, SkinThickness, Insulin, BMI","metadata":{}},{"cell_type":"code","source":"lst_null = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\ndf[lst_null] = df[lst_null].replace(0, np.nan)\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:25.373705Z","iopub.execute_input":"2021-06-06T17:07:25.373977Z","iopub.status.idle":"2021-06-06T17:07:25.388163Z","shell.execute_reply.started":"2021-06-06T17:07:25.37395Z","shell.execute_reply":"2021-06-06T17:07:25.387463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace를 위한 dataframe\noutcome_mean_df = df.groupby('Outcome').mean()\noutcome_mean_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:25.3892Z","iopub.execute_input":"2021-06-06T17:07:25.389725Z","iopub.status.idle":"2021-06-06T17:07:25.412893Z","shell.execute_reply.started":"2021-06-06T17:07:25.389673Z","shell.execute_reply":"2021-06-06T17:07:25.411876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in lst_null:\n    df[col] = np.where((df[col].isnull())&(df['Outcome']==0), outcome_mean_df[col].iloc[0], df[col])\n    df[col] = np.where((df[col].isnull())&(df['Outcome']==1), outcome_mean_df[col].iloc[1], df[col])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:25.414617Z","iopub.execute_input":"2021-06-06T17:07:25.415096Z","iopub.status.idle":"2021-06-06T17:07:25.435109Z","shell.execute_reply.started":"2021-06-06T17:07:25.415048Z","shell.execute_reply":"2021-06-06T17:07:25.433695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(12,10))\n\nfor i in range(len(df.columns)-1):\n    name = df.columns[i]\n    sns.distplot(df[df['Outcome'] == 0][name], color='green', ax=ax[i//3, i%3])\n    sns.distplot(df[df['Outcome'] == 1][name], color='red', ax=ax[i//3, i%3])\n    ax[i//3, i%3].set_title(f'Healthy vs Diabetic by {name}')\n\nax[2, 2] = sns.countplot(x='Outcome', data=df)\nax[2, 2].set_title('Outcome')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:25.436671Z","iopub.execute_input":"2021-06-06T17:07:25.437155Z","iopub.status.idle":"2021-06-06T17:07:27.950352Z","shell.execute_reply.started":"2021-06-06T17:07:25.437111Z","shell.execute_reply":"2021-06-06T17:07:27.949365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 변수 변환\n- 로그 변환, 제곱근 변환, boxcox 변환 모두 시도\n- boxcox 후에 정규분포 모양으로 변환이 더 잘 이루어졌음\n- 최종적으로 boxcox 변환 사용","metadata":{}},{"cell_type":"code","source":"# 변수 변환\nfrom sklearn import preprocessing\nfrom scipy.stats import boxcox\nskewed_cols = ['Pregnancies', 'Insulin', 'DiabetesPedigreeFunction', 'Age']\n\nfor col in skewed_cols :\n    df[col] = preprocessing.scale(boxcox(df[col]+1)[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:27.951468Z","iopub.execute_input":"2021-06-06T17:07:27.95174Z","iopub.status.idle":"2021-06-06T17:07:28.021856Z","shell.execute_reply.started":"2021-06-06T17:07:27.951714Z","shell.execute_reply":"2021-06-06T17:07:28.020829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(12,10))\n\nfor i in range(len(df.columns)-1):\n    name = df.columns[i]\n    sns.distplot(df[df['Outcome'] == 0][name], color='green', ax=ax[i//3, i%3])\n    sns.distplot(df[df['Outcome'] == 1][name], color='red', ax=ax[i//3, i%3])\n    ax[i//3, i%3].set_title(f'Healthy vs Diabetic by {name}')\n\nax[2, 2] = sns.countplot(x='Outcome', data=df)\nax[2, 2].set_title('Outcome')\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:28.022945Z","iopub.execute_input":"2021-06-06T17:07:28.023206Z","iopub.status.idle":"2021-06-06T17:07:30.340072Z","shell.execute_reply.started":"2021-06-06T17:07:28.02318Z","shell.execute_reply":"2021-06-06T17:07:30.339063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 스케일링\n- RobustScaler, MinMaxScaler, StandardScaler 모두 시도\n- 최종적으로 모델 성능이 근소하게 더 높게 나온 RobustScaler 사용","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(['Outcome'], axis=1)\ny = df.Outcome","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:30.341168Z","iopub.execute_input":"2021-06-06T17:07:30.341445Z","iopub.status.idle":"2021-06-06T17:07:30.363721Z","shell.execute_reply.started":"2021-06-06T17:07:30.341418Z","shell.execute_reply":"2021-06-06T17:07:30.362793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nrobust_scaler = RobustScaler()\n\nX_robust_scaled = robust_scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:30.36507Z","iopub.execute_input":"2021-06-06T17:07:30.365349Z","iopub.status.idle":"2021-06-06T17:07:30.376774Z","shell.execute_reply.started":"2021-06-06T17:07:30.365322Z","shell.execute_reply":"2021-06-06T17:07:30.375767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 오버 샘플링\n- 예측 변수의 클래스 개수를 보면 '0'이 '1'보다 더 많음\n- 데이터 불균형을 해소하기 위해 오버 샘플링 기법인 SMOTE 사용\n- '0'과 '1' 클래스의 개수를 동일하게 맞춰줌","metadata":{}},{"cell_type":"code","source":"# 오버 샘플링\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy='auto', random_state=1234)\nX_resampled, y_resampled= sm.fit_resample(X_robust_scaled,y)\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_resampled.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(X_resampled.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_resampled==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_resampled==0)))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:30.378114Z","iopub.execute_input":"2021-06-06T17:07:30.378422Z","iopub.status.idle":"2021-06-06T17:07:30.609434Z","shell.execute_reply.started":"2021-06-06T17:07:30.378369Z","shell.execute_reply":"2021-06-06T17:07:30.608544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 모델","metadata":{}},{"cell_type":"markdown","source":"### Confusion Matrix Function\n- model : fit 하기 전 모델\n- X : X 데이터 (전체 데이터)\n- y : y 데이터 (전체 데이터)\n- name : dataframe의 index명 설정","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import KFold, cross_val_predict\n\n# data의 70%로 학습\ndef model_confusion(model, X, y, name):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_test).reshape(-1,)\n    result_dict = {'Accuracy': [accuracy_score(y_test, pred)], \n                   'Precision': [precision_score(y_test, pred)], \n                   'Recall': [recall_score(y_test, pred)], \n                   'F1 score': [f1_score(y_test, pred)]}\n    \n    result = pd.DataFrame(result_dict, index=[name])\n    return result\n\n# Kfold 학습\ndef Kfold_model_confusion(model, X, y, name):\n    y_pred = cross_val_predict(model, X_resampled, y_resampled, cv=10)\n    result_dict = {'Accuracy': [accuracy_score(y_resampled, y_pred)], \n                  'Precision': [precision_score(y_resampled, y_pred)], \n                  'Recall': [recall_score(y_resampled, y_pred)], \n                  'F1 score': [f1_score(y_resampled, y_pred)]}\n    result = pd.DataFrame(result_dict, index=[name])\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:30.610634Z","iopub.execute_input":"2021-06-06T17:07:30.610915Z","iopub.status.idle":"2021-06-06T17:07:30.620984Z","shell.execute_reply.started":"2021-06-06T17:07:30.610888Z","shell.execute_reply":"2021-06-06T17:07:30.620004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Modules","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nimport xgboost","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:30.622153Z","iopub.execute_input":"2021-06-06T17:07:30.622449Z","iopub.status.idle":"2021-06-06T17:07:30.81398Z","shell.execute_reply.started":"2021-06-06T17:07:30.622416Z","shell.execute_reply":"2021-06-06T17:07:30.813199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Models we tried\n- Logistic Regression\n- SVM\n- Gaussian NB\n- Random Forest\n- Gradient Boosting\n- AdaBoost\n- Decision Tree\n- LGBM\n- XGBoost\n- Ensemble with Random Forest, Gradient Boost, XGBoost, LGBM\n- Ensemble with all models we tried","metadata":{}},{"cell_type":"code","source":"lr_clf = LogisticRegression()\nsvm_clf = svm.SVC(kernel='linear', C=0.7, gamma=5)\nnb_clf = GaussianNB()\nforest_clf = RandomForestClassifier(n_estimators=500)\ngra_clf = GradientBoostingClassifier(n_estimators=500)\nada_clf = AdaBoostClassifier(n_estimators=500)\ndt_clf = DecisionTreeClassifier(random_state=1234)\nlgbm_clf = LGBMClassifier(n_estimators=500)\nxgb_clf = xgboost.XGBClassifier(n_estimators=500, learning_rate=0.2, \n                                gamma=0.5, max_depth=20, verbosity=0)\nen_clf = VotingClassifier(estimators=[('rf', forest_clf), ('gb', gra_clf), ('xgb', xgb_clf), ('lgbm', lgbm_clf)],\n                         voting='soft',weights=[2, 3, 5, 4])\nall_clf = VotingClassifier(estimators=[('lr', lr_clf), ('svm', svm_clf), ('nb', nb_clf), ('for', forest_clf),\n                                      ('gra', gra_clf), ('ada', ada_clf), ('dt', dt_clf), ('lgbm', lgbm_clf),\n                                      ('xgb', xgb_clf)], voting='hard', weights=[1,2,2,8,5,4,4,5,6])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:30.814929Z","iopub.execute_input":"2021-06-06T17:07:30.815185Z","iopub.status.idle":"2021-06-06T17:07:30.824766Z","shell.execute_reply.started":"2021-06-06T17:07:30.815159Z","shell.execute_reply":"2021-06-06T17:07:30.823612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 결과 도출\n1. Train Data : Test Data = 7 : 3 으로 학습\n    - Scaling과 Over Sampling 하지 않은 데이터\n    - Scaling과 Over Sampling 한 데이터\n2. K-fold Cross Validation 학습","metadata":{}},{"cell_type":"code","source":"print(\"< 7:3 split dataset before scaling and over sampling >\")\n\nresult1 = model_confusion(lr_clf, X, y, 'Logistic Regression')\nresult1 = pd.concat([result1, model_confusion(svm_clf, X, y, 'SVM')])\nresult1 = pd.concat([result1, model_confusion(nb_clf, X, y, 'Gaussian NB')])\nresult1 = pd.concat([result1, model_confusion(forest_clf, X, y, 'Random Forest')])\nresult1 = pd.concat([result1, model_confusion(gra_clf, X, y, 'Gradient Boosting')])\nresult1 = pd.concat([result1, model_confusion(ada_clf, X, y, 'AdaBoosting')])\nresult1 = pd.concat([result1, model_confusion(dt_clf, X, y, 'Decision Tree')])\nresult1 = pd.concat([result1, model_confusion(lgbm_clf, X, y, 'LGBM')])\nresult1 = pd.concat([result1, model_confusion(xgb_clf, X, y, 'XGBoost')])\nresult1 = pd.concat([result1, model_confusion(en_clf, X, y, 'Ensemble')])\nresult1 = pd.concat([result1, model_confusion(all_clf, X, y, 'Ensemble_all')])\nresult1","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:30.825828Z","iopub.execute_input":"2021-06-06T17:07:30.826088Z","iopub.status.idle":"2021-06-06T17:07:40.591819Z","shell.execute_reply.started":"2021-06-06T17:07:30.826063Z","shell.execute_reply":"2021-06-06T17:07:40.590786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"< 7:3 split dataset after scaling and over sampling >\")\n\nresult2 = model_confusion(lr_clf, X, y, 'Logistic Regression')\nresult2 = pd.concat([result2, model_confusion(svm_clf, X_resampled, y_resampled, 'SVM')])\nresult2 = pd.concat([result2, model_confusion(nb_clf, X_resampled, y_resampled, 'Gaussian NB')])\nresult2 = pd.concat([result2, model_confusion(forest_clf, X_resampled, y_resampled, 'Random Forest')])\nresult2 = pd.concat([result2, model_confusion(gra_clf, X_resampled, y_resampled, 'Gradient Boosting')])\nresult2 = pd.concat([result2, model_confusion(ada_clf, X_resampled, y_resampled, 'AdaBoosting')])\nresult2 = pd.concat([result2, model_confusion(dt_clf, X_resampled, y_resampled, 'Decision Tree')])\nresult2 = pd.concat([result2, model_confusion(lgbm_clf, X_resampled, y_resampled, 'LGBM')])\nresult2 = pd.concat([result2, model_confusion(xgb_clf, X_resampled, y_resampled, 'XGBoost')])\nresult2 = pd.concat([result2, model_confusion(en_clf, X_resampled, y_resampled, 'Ensemble')])\nresult2 = pd.concat([result2, model_confusion(all_clf, X_resampled, y_resampled, 'Ensemble_all')])\nresult2","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:40.593247Z","iopub.execute_input":"2021-06-06T17:07:40.59382Z","iopub.status.idle":"2021-06-06T17:07:51.225485Z","shell.execute_reply.started":"2021-06-06T17:07:40.593776Z","shell.execute_reply":"2021-06-06T17:07:51.224638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"< K-fold Cross Validation after scaling and over sampling >\")\n\nresult3 = Kfold_model_confusion(lr_clf, X_resampled, y_resampled, 'Logistic Regression')\nresult3 = pd.concat([result3, Kfold_model_confusion(svm_clf, X_resampled, y_resampled, 'SVM')])\nresult3 = pd.concat([result3, Kfold_model_confusion(nb_clf, X_resampled, y_resampled, 'Gaussian NB')])\nresult3 = pd.concat([result3, Kfold_model_confusion(forest_clf, X_resampled, y_resampled, 'Random Forest')])\nresult3 = pd.concat([result3, Kfold_model_confusion(gra_clf, X_resampled, y_resampled, 'Gradient Boosting')])\nresult3 = pd.concat([result3, Kfold_model_confusion(ada_clf, X_resampled, y_resampled, 'AdaBoosting')])\nresult3 = pd.concat([result3, Kfold_model_confusion(dt_clf, X_resampled, y_resampled, 'Decision Tree')])\nresult3 = pd.concat([result3, Kfold_model_confusion(lgbm_clf, X_resampled, y_resampled, 'LGBM')])\nresult3 = pd.concat([result3, Kfold_model_confusion(xgb_clf, X_resampled, y_resampled, 'XGBoost')])\nresult3 = pd.concat([result3, Kfold_model_confusion(en_clf, X_resampled, y_resampled, 'Ensemble')])\nresult3 = pd.concat([result3, Kfold_model_confusion(all_clf, X_resampled, y_resampled, 'Ensemble_all')])\nresult3","metadata":{"execution":{"iopub.status.busy":"2021-06-06T17:07:51.226589Z","iopub.execute_input":"2021-06-06T17:07:51.227026Z","iopub.status.idle":"2021-06-06T17:09:49.087215Z","shell.execute_reply.started":"2021-06-06T17:07:51.226997Z","shell.execute_reply":"2021-06-06T17:09:49.086212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 최종 결과\n\n**1. Train:Test=7:3 Split**\n\n    - model: Random Forest\n    \n    - Accuracy: 0.923\n    \n    - F1 score: 0.925\n    \n\n**2. K-fold Cross Validation**\n\n    - model: Ensemble with Random Forest, Gradient Boost, XGBoost, LGBM\n    \n    - Accuracy: 0.913\n    \n    - F1 score: 0.915","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}