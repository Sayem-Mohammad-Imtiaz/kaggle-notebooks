{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CONTENTS\n\n<font color=\"red\">\n\n    \n1. [Introduction](#1)\n2. [Libraries](#2)\n3. [Uploading Data](#3)\n4. [Multivariate Linear Regression](#4)\n    * 4.1. [Dependent And Independent Variables](#5)\n    * 4.2. [Diving Data For Training And Test](#6)\n    * 4.3. [Scaling](#7)\n    * 4.4. [Multivariate Linear Regression and Prediction](#8)\n5. [Simple Linear Regression](#9)\n    * 5.1. [Dependent and Independent Variables](#10)\n    * 5.2. [Diving Data For Training And Test](#11)\n    * 5.3. [Multivariate Linear Regression and Prediction](#12)\n    * 5.4. [$ùëÖ^2$  Score For Simple Linear Regression](#13)\n    * 5.5. [Visualization For Simple Linear Regression](#14)\n6. [Backward Elimination For Simple Linear Regression](#15)\n    * 6.1. [Uploading Data](#16)\n    * 6.2. [Dependent and Independent Variables](#17)\n    * 6.3. [One Hot Encoder](#18)\n    * 6.4. [Transforming Numpy Arrays To Dataframe](#19)\n    * 6.5. [Concatinating Dataframes](#20)\n    * 6.6. [Diving Data For Training And Test](#21)\n    * 6.7. [Simple Linear Regression And Prediction](#22)\n    * 6.8. [Backward Elimination For Simple Linear Regression](#23)\n    * 6.9. [Information About Data](#24)\n    * 6.10. [OLS Regression Results](#25)\n    * 6.11. [Creating A New OLS Model According To First OLS Model](#26)\n7. [Polynomial Regression](#27)\n    * 7.1. [Dependent And Independent Variables](#28)\n    * 7.2. [Linear Model(1st Order)](#29)\n    * 7.3. [Nonlinear Model](#30)\n        * 7.3.1. [2nd Order](#31)\n        * 7.3.2. [4th Order](#32)\n    * 7.4. [Visualization Of Polynomial Regression](#33)\n    * 7.5. [Predictions](#34)\n    * 7.6. [$ùëÖ^2$ Score For Polynomial Regression](#35)\n8. [SVR (Support Vector Regression](#36)\n    * 8.1. [Scaling](#37)\n    * 8.2. [Support Vector Regression](#38)\n    * 8.3. [Prediction Of SVR](#39)\n    * 8.4. [$ùëÖ^2$ Score For SVR](#40)\n9. [Decision Tree Regression](#41)\n    * 9.1. [Decision Tree Regression](#42)\n    * 9.2. [Prediction Of Decision Tree Regression](#43)\n    * 9.3. [$ùëÖ^2$  Score Of Decision Tree Regression](#44)\n10. [Random Forest Regression](#45)\n    * 10.1. [Random Forest Regression](#46)\n    * 10.2. [Prediction Of Decision Tree Regression](#47)\n    * 10.3. [Visualization For Random Forest Regression](#48)\n    * 10.4. [$ùëÖ^2$  Score Of Random Forest Regression](#49)\n11. [$ùëÖ^2$  Scores Abstract](#50)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a><br>\n# 1. Introduction\nIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable and one or more independent variables.\n# $$Y_i = f(X_i,Œ≤) + e_i$$","metadata":{}},{"cell_type":"markdown","source":"* $Y_i$ = dependent variable\n* f = function\n* $X_i$ = independent variable\n* Œ≤ = unknown parameters\n* $e_i$ = error terms","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a><br>\n# 2. Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a><br>\n# 3. Uploading Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/car-price-prediction/CarPrice_Assignment.csv\")\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a><br>\n# 4. Multivariate Linear Regression","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a><br>\n## 4.1. Dependent And Independent Variables","metadata":{}},{"cell_type":"code","source":"x = data[[\"wheelbase\",\"carwidth\",\"curbweight\",\"compressionratio\",\"horsepower\",\"highwaympg\"]].values\ny = data[[\"price\"]].values\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#s1=pd.DataFrame(data=x, columns=[\"wheelbase\",\"carwidth\",\"curbweight\",\"compressionratio\",\"horsepower\",\"highwaympg\"])\n#s1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#s2=pd.DataFrame(data=y)\n#s2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a><br>\n## 4.2. Diving Data For Training And Test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a><br>\n## 4.3. Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(x_train)\nX_test = sc.fit_transform(x_test)\n\nY_train = sc.fit_transform(y_train)\nY_test = sc.fit_transform(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a><br>\n## 4.4. Multivariate Linear Regression and Prediction","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train, y_train)\n\npred1 = lr.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a><br>\n# 5. Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a><br>\n## 5.1. Dependent and Independent Variables","metadata":{}},{"cell_type":"code","source":"x2=data[[\"curbweight\"]]\nx2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y2 = data[[\"price\"]]\ny2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a><br>\n## 5.2. Diving Data For Training And Test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.33,random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a><br>\n## 5.3. Multivariate Linear Regression and Prediction","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train2,y_train2)\n\npred2 = lr.predict(x_test2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"13\"></a><br>\n## 5.4. $R^2$ Score For Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"Simple linear regression R2 score:\", r2_score(y2,lr.predict(x2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"14\"></a><br>\n## 5.5. Visualization For Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"x_train2=x_train2.sort_values(by=\"curbweight\")\ny_train2=y_train2.sort_index()\n\nplt.plot(x_train2,y_train2,)\nplt.plot(x_test2,pred2)\n\nplt.title(\"car prices\")\nplt.xlabel(\"curbweight\")\nplt.ylabel(\"price\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"15\"></a><br>\n# 6. Backward Elimination For Simple Linear Regression","metadata":{}},{"cell_type":"markdown","source":"<a id=\"16\"></a><br>\n## 6.1. Uploading Data","metadata":{}},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"17\"></a><br>\n## 6.2. Dependent And Independent Variables","metadata":{}},{"cell_type":"code","source":"carbody=data[[\"carbody\"]].values\ncarbody","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3=data[[\"horsepower\"]].values\ndata3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price3=data[[\"price\"]].values\nprice3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"door=data[[\"doornumber\"]].values\ndoor\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"18\"></a><br>\n## 6.3. One Hot Encoder","metadata":{}},{"cell_type":"code","source":"#OneHotEncoder\nfrom sklearn import preprocessing\n\nohe=preprocessing.OneHotEncoder()\ncarbody=ohe.fit_transform(carbody).toarray()\nprint(carbody)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"ohe=preprocessing.OneHotEncoder()\ndoor=ohe.fit_transform(door).toarray()\nprint(door)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"19\"></a><br>\n## 6.4. Transforming Numpy Arrays To Dataframe","metadata":{}},{"cell_type":"code","source":"cb1=pd.DataFrame(data=carbody,index=range(205),columns=[\"convertible\",\"hardtop\",\"hatchback\",\"sedan\",\"wagon\"])\ncb1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb2=pd.DataFrame(data=data3,index=range(205),columns=[\"horsepower\"])\ncb2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb3=pd.DataFrame(data=price3,index=range(205),columns=[\"price3\"])\ncb3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"20\"></a><br>\n## 6.5. Concatinating Dataframes","metadata":{}},{"cell_type":"code","source":"concat1=pd.concat([cb1,cb2],axis=1)\nconcat1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"21\"></a><br>\n## 6.6. Diving Data For Training And Test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train3,x_test3,y_train3,y_test3 = train_test_split(concat1,cb3,test_size=0.33, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"22\"></a><br>\n## 6.7. Simple Linear Regression And Prediction","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(x_train3,y_train3)\npred3=regressor.predict(x_test3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"23\"></a><br>\n## 6.8. Backward Elimination For Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"data_b=pd.concat([concat1,cb3],axis=1)\ndata_b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sym=data[[\"symboling\"]].values\nsym","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train4,x_test4,y_train4,y_test4 = train_test_split(data_b,sym,test_size=0.33,random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r4 = LinearRegression()\nr4.fit(x_train4,y_train4)\n\npred4 = r4.predict(x_test4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"24\"></a><br>\n## 6.9. Information About Data","metadata":{}},{"cell_type":"code","source":"data_b.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"25\"></a><br>\n## 6.10. OLS Regression Results","metadata":{}},{"cell_type":"markdown","source":"> OLS = Ordinary Least Squares","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\n\nx4=np.append(arr=np.ones((205,1)).astype(int),values=data_b,axis=1) #column x0\n\nX4_l = data_b.iloc[:,[0,1,2,3,4,5,6]].values #we get all columns\nX4_l= np.array(X4_l,dtype=float)\nmodel = sm.OLS(sym,X4_l).fit() #generate a report with OLS\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> For more: [https://www.datarobot.com/blog/ordinary-least-squares-in-python/](https://www.datarobot.com/blog/ordinary-least-squares-in-python/)","metadata":{}},{"cell_type":"markdown","source":"> For $x_5$, P>|t| value is large. So, we can delete that column.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"26\"></a><br>\n## 6.11. Creating A New OLS Model According To First OLS Model","metadata":{}},{"cell_type":"code","source":"#new OLS model\nX4_l = data_b.iloc[:,[0,1,2,3,5,6]].values #we get all columns\nX4_l= np.array(X4_l,dtype=float)\nmodel = sm.OLS(sym,X4_l).fit() #generate a report with OLS\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"27\"></a><br>\n# 7. Polynomial Regression  ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"28\"></a><br>\n## 7.1. Dependent And Independent Variables","metadata":{}},{"cell_type":"code","source":"x=data[[\"wheelbase\"]].values\ny=data[[\"price\"]].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"29\"></a><br>\n## 7.2. Linear Model (1st Order)","metadata":{}},{"cell_type":"code","source":"linreg1 = LinearRegression()\nlinreg1.fit(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"30\"></a><br>\n## 7.3. Nonlinear Model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"31\"></a><br>\n### 7.3.1. 2nd Order","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg2=PolynomialFeatures(degree=2)\nx_poly2=poly_reg2.fit_transform(x)\nlinreg2=LinearRegression()\nlinreg2.fit(x_poly2,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_poly2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"32\"></a><br>\n### 7.3.2. 4th Order","metadata":{}},{"cell_type":"code","source":"#4th order\npoly_reg4=PolynomialFeatures(degree=4)\nx_poly4=poly_reg4.fit_transform(x)\nlinreg4=LinearRegression()\nlinreg4.fit(x_poly4,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_poly4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"33\"></a><br>\n## 7.4. Visualization Of Polynomial Regression","metadata":{}},{"cell_type":"code","source":"plt.scatter(x,y,color='red')\nplt.plot(x,linreg1.predict(x), color = 'blue')\nplt.show()\n\nplt.scatter(x,y,color = 'red')\nplt.plot(x,linreg2.predict(poly_reg2.fit_transform(x)), color = 'blue')\nplt.show()\n\nplt.scatter(x,y,color = 'red')\nplt.plot(x,linreg4.predict(poly_reg4.fit_transform(x)), color = 'blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"34\"></a><br>\n## 7.5. Predictions","metadata":{}},{"cell_type":"code","source":"print(\"lin reg 1st order(linear): \",linreg1.predict([[6.6]]))\nprint(\"lin reg 1st order(linear): \",linreg1.predict([[110]]))\n\nprint(\"lin reg 2nd order(linear): \",linreg2.predict(poly_reg2.fit_transform([[6.6]])))\nprint(\"lin reg 2nd order(linear): \",linreg2.predict(poly_reg2.fit_transform([[110]])))\n\nprint(\"lin reg 4th order(linear): \",linreg4.predict(poly_reg4.fit_transform([[6.6]])))\nprint(\"lin reg 4th order(linear): \",linreg4.predict(poly_reg4.fit_transform([[110]])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"35\"></a><br>\n## 7.6. $R^2$ Score For Polynomial Regression\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"Polynomial regression R2 score:\", r2_score(y,linreg2.predict(x_poly2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"36\"></a><br>\n# 8. SVR (Support Vector Regression)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"37\"></a><br>\n## 8.1. Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc1=StandardScaler()\nx_scaled = sc1.fit_transform(x)\nsc2=StandardScaler()\ny_scaled = np.ravel(sc2.fit_transform(y.reshape(-1,1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"38\"></a><br>\n## 8.2. Support Vector Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvr_reg = SVR(kernel='rbf')\nsvr_reg.fit(x_scaled,y_scaled)\n\nplt.scatter(x_scaled,y_scaled,color='red')\nplt.plot(x_scaled,svr_reg.predict(x_scaled),color='blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr_reg2 = SVR(kernel='poly')\nsvr_reg2.fit(x_scaled,y_scaled)\n\nplt.scatter(x_scaled,y_scaled,color='red')\nplt.plot(x_scaled,svr_reg2.predict(x_scaled),color='blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"39\"></a><br>\n## 8.3. Prediction Of SVR","metadata":{}},{"cell_type":"code","source":"print(\"SVR rbf pred:\", svr_reg.predict([[110]]))\nprint(\"SVR rbf pred:\", svr_reg.predict([[6.6]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"SVR poly pred:\", svr_reg2.predict([[110]]))\nprint(\"SVR poly pred:\", svr_reg2.predict([[6.6]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"40\"></a><br>\n## 8.4. $R^2$ Score For SVR","metadata":{}},{"cell_type":"code","source":"print(\"SVR rbf  R2 score:\" ,r2_score(y_scaled,svr_reg.predict(x_scaled)))\nprint(\"SVR poly R2 score:\",r2_score(y_scaled,svr_reg2.predict(x_scaled)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **rbf** is more convinient than **poly** *(0.42>0.12)*","metadata":{}},{"cell_type":"markdown","source":"<a id=\"41\"></a><br>\n# 9. Decision Tree Regression","metadata":{}},{"cell_type":"markdown","source":"<a id=\"42\"></a><br>\n## 9.1. Decision Tree Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(random_state=0)\ndtr.fit(x,y)\nZ = x + 0.5\nK = y - 0.4\n\nplt.scatter(x,y, color='red')\nplt.plot(x,dtr.predict(x), color='blue')\nplt.plot(x,dtr.predict(Z),color='green')\nplt.plot(x,dtr.predict(K),color='yellow')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"43\"></a><br>\n## 9.2. Prediction Of Decision Tree Regression","metadata":{}},{"cell_type":"code","source":"print(dtr.predict([[110]]))\nprint(dtr.predict([[11]]))\nprint(dtr.predict([[6.6]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"44\"></a><br>\n## 9.3. $R^2$ Score Of Decision Tree Regression","metadata":{}},{"cell_type":"code","source":"print(\"Decision tree regression R2 score:\", r2_score(y,dtr.predict(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"45\"></a><br>\n# 10. Random Forest Regression","metadata":{}},{"cell_type":"markdown","source":"<a id=\"46\"></a><br>\n## 10.1. Random Forest Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(n_estimators = 10,random_state=0) #We give how many decision trees to draw with #n_estimators\nrfr.fit(x,y.ravel()) #ravel combines individual data. Because we separate the random forest, we reunite it.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"47\"></a><br>\n## 10.2. Prediction Of Random Forest Regression","metadata":{}},{"cell_type":"code","source":"print(rfr.predict([[6.6]]))\nprint(rfr.predict([[11]]))\nprint(rfr.predict([[110]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"48\"></a><br>\n## 10.3. Visualization For Random Forest Regression","metadata":{}},{"cell_type":"code","source":"plt.scatter(x,y,color='red')\nplt.plot(x,rfr.predict(x),color='blue')\n\nplt.plot(x,rfr.predict(Z),color='green')\nplt.plot(x,rfr.predict(K),color='yellow')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"49\"></a><br>\n## 10.4. $R^2$  Score Of Random Forest Regression","metadata":{}},{"cell_type":"code","source":"print(\"Random forest R2 score\", r2_score(y,rfr.predict(x)))\nprint(\"Random forest R2 score\", r2_score(y,rfr.predict(K)))\nprint(\"Random forest R2 score\", r2_score(y,rfr.predict(Z)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"50\"></a><br>\n# 11. $R^2$ Scores Abstract","metadata":{}},{"cell_type":"code","source":"print('-----------------------')\nprint('Linear R2 degeri')\nprint(\"Simple linear regression R2 score:\", r2_score(y2,lr.predict(x2)))\n\nprint('\\nPolynomial R2 degeri')\nprint(\"Polynomial regression R2 score:\", r2_score(y,linreg2.predict(x_poly2)))\n\nprint('\\nSVR R2 degeri')\nprint(\"SVR poly R2 score:\",r2_score(y_scaled,svr_reg2.predict(x_scaled)))\n\nprint('\\nDecision Tree R2 degeri')\nprint(\"Decision tree regression R2 score:\", r2_score(y,dtr.predict(x)))\n\nprint('\\nRandom Forest R2 degeri')\nprint(\"Random forest R2 score\", r2_score(y,rfr.predict(x)))\n\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}