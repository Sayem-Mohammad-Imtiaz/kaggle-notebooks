{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# df = import_data(\"/kaggle/input/crimes-in-boston/crime.csv\")\ndf = pd.read_csv(\"/kaggle/input/crimes-in-boston/crime.csv\", engine = 'python')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['OFFENSE_CODE_GROUP','OCCURRED_ON_DATE','YEAR','MONTH','DAY_OF_WEEK','Lat','Long']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering the locations to get all coordinates in the Boston area\n\nlis = []\n\nLat = list(df['Lat'])\nLong = list(df['Long'])\n\nfor i in range(len(Lat)):\n    li = []\n    if Lat[i] > 30 and Long[i] < -40:\n        li.append(Lat[i])\n        li.append(Long[i])\n    else:\n        li.append(np.nan)\n        li.append(np.nan)\n    lis.append(li)\n\ndel Lat\ndel Long\n\nL = np.array(lis)\nLatitude = L[:, 0]\nLongitude = L[:, 1]\n\ndf['Latitude'] = Latitude\ndf['Longitude'] = Longitude\n\ndf = df.dropna()\ndf.reset_index(inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['OFFENSE_CODE_GROUP','OCCURRED_ON_DATE','YEAR','MONTH','DAY_OF_WEEK','Latitude','Longitude']]\n\nLatitude = list(df['Latitude'])\nLongitude = list(df['Longitude'])\n\nL = [[Latitude[i], Longitude[i]] for i in range(len(Latitude))]\n\nX = np.array(L)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\nimport matplotlib.pyplot as plt\n\n# Choose a small value for eps, because greater the eps, more the neighboring points and hence, more storage is required.\n\n# Sample Values:\n# 1) eps = 0.001454, min_samples = 40 => 67 clusters\n# 2) eps = 0.00365, min_samples = 4 => 11 clusters\n\nepsilon = 0.00365\nmin_sample_points = 4\n\ndbscan_clustering = DBSCAN(eps = epsilon, min_samples = min_sample_points).fit(X)\n\nClusters = dbscan_clustering.labels_.tolist()\n\nprint(\"Clusters: \" + str(list(set(Clusters))))\n\n# For a particular cluster, enter the cluster number instead of 0 in the next 2 lines and then un-comment them\n\n# X_ = np.array([X[i] for i in range(len(X)) if Clusters[i] == 0])\n# Clusters0 = [0 for i in range(len(X_))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the cluster assignments\n\nplt.scatter(X[:, 0], X[:, 1], c = Clusters, cmap = \"plasma\")\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Longitude\")\n\n# For a specific cluster, use X_ (from the above cell) instead of X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cluster'] = Clusters\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['OCCURRED_ON_DATE'] = pd.to_datetime(df.OCCURRED_ON_DATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sort_values(by='OCCURRED_ON_DATE')\ndf.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lisp = [i.date() for i in list(df['OCCURRED_ON_DATE'])]\n\ndf['OCCURRED_ON_DATE'] = lisp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_frames(df, Clusters):\n    frame_cache = {}\n    \n    for i in range(len(Clusters)):\n        df_x = df[df['Cluster'] == Clusters[i]]\n        frame_cache['df' + str(i)] = df_x\n    \n    return frame_cache\n\nset_of_clusters = list(set(Clusters))\nframe_cache = initialize_frames(df, set_of_clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ndef get_metrics(y_actual, y_predicted, algorithm = '', model_number = '', save_to_file = False):\n    if save_to_file:\n        if algorithm == '' or algorithm is None:\n            print(\"Please enter algorithm name for the metrics file\")\n            return\n    \n        f = open('DBSCAN_' + algorithm + '_metrics.txt', \"a+\")\n    \n        if model_number == '' or model_number is None:\n            print(\"Please enter model number\")\n            return\n        \n        f.write(\"\\n\\n***************** Model \" + model_number + \" *******************\\n\\n\")\n        f.write(\"Root mean squared error (RMSE) => \" + str(sqrt(mean_squared_error(y_actual, y_predicted))) + \"\\n\")\n        f.write(\"Mean squared error (MSE) => \" + str(mean_squared_error(y_actual, y_predicted)) + \"\\n\")\n        f.write(\"Mean abolute error (MAE) => \" + str(mean_absolute_error(y_actual, y_predicted)) + \"\\n\")\n    else:\n        print(\"***************** Prediction Metrics *******************\\n\\n\")\n        print(\"Root mean squared error (RMSE) => \" , sqrt(mean_squared_error(y_actual, y_predicted)))\n        print(\"Mean squared error (MSE) => \" , mean_squared_error(y_actual, y_predicted))\n        print(\"Mean absolute error (MAE) => \", mean_absolute_error(y_actual, y_predicted))\n        \n    print(\"Model_\" + model_number + \"'s metrics have been recorded\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_model(model, model_number, model_type, last_date):\n    \n    pkl_path = ''\n    if model_type == 'Prophet':\n        pkl_path = 'DBSCAN_' + model_type + '_' + model_number + '.pkl'\n    elif model_type == 'ETS' or model_type == 'ARIMA':\n        if last_date is None or last_date == '':\n            print(\"Please enter the last training date for the model\")\n            return\n        if model_number is None or model_number == '':\n            print(\"Please enter the model number\")\n            return\n        pkl_path = 'DBSCAN_' + model_type + '_' + model_number + '_' + last_date + '.pkl'\n    else:\n        print(\"Please enter the correct model type: (Prophet, ETS or ARIMA)\")\n        return\n    \n    with open(pkl_path, 'wb') as f:\n        pickle.dump(model, f)\n\ndef load_model(pkl_path):\n    model = None\n    try:\n        with open(pkl_path, 'rb') as f:\n            model = pickle.load(f)\n    except pickle.UnpicklingError:\n        with open(pkl_path, 'rb') as f:\n            model = f.read()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecasting for the test data using Prophet\n\ndef ProphetsForecast(model, no_of_days):\n    \n    future = model.make_future_dataframe(periods = no_of_days)\n    forecast = model.predict(future)\n    return forecast\n\ndef TestProphet(model, df_train, df_test):\n    actual_dates = list(df_test['ds'])\n    LatestDate = actual_dates[-1]\n    LastDate = list(df_train['ds'])[-1]\n    \n    no_of_days = (LatestDate - LastDate).days\n    if no_of_days <= 0:\n        print(\"Please enter a date after \" + str(LastDate))\n        return\n    \n    forecast = ProphetsForecast(model, no_of_days)\n    yhat = list(forecast['yhat'])\n    \n    predictions = list()\n    for DATE in actual_dates:\n        diff = (DATE - LastDate).days\n        predictions.append(yhat[diff - 1])\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Prophet models\n\nimport datetime\nfrom fbprophet import Prophet\n\ndef createProphet(x, df_x, save_metrics = False, save_Model = False):\n    \n    df_x.reset_index(inplace = True)\n    df_x = df_x.groupby(['OCCURRED_ON_DATE']).count()[['level_0','index']]\n    df_x.drop(columns = ['index'], inplace = True)\n    \n    df_train = df_x[0 : int(len(df_x) * 0.8)]\n    df_train = df_train.reset_index()\n    df_train.rename(columns = {'OCCURRED_ON_DATE':'ds', 'level_0':'y'} , inplace = True)\n    \n    df_test = df_x[int(len(df_x) * 0.8) : ]\n    df_test = df_test.reset_index()\n    df_test.rename(columns = {'OCCURRED_ON_DATE':'ds', 'level_0':'y'} , inplace = True)\n    \n    # Prophet models cannot make predictions using just 1 training sample\n    \n    while len(df_train) < 2:\n        train_date = list(df_train['ds'])\n        train_y = list(df_train['y'])\n        \n        test_date = list(df_test['ds'])\n        test_y = list(df_test['y'])\n        \n        train_date.append(test_date[0])\n        train_y.append(test_y[0])\n        \n        test_date.remove(test_date[0])\n        test_y.remove(test_y[0])\n        \n        if len(test_date) > 0:\n            Next_Date = test_date[-1] + datetime.timedelta(days = 1)\n            if len(test_y) == 0:\n                Next_Y = 0\n            else:\n                Next_Y = sum(test_y) // len(test_y)\n            test_date.append(Next_Date)\n            test_y.append(Next_Y)\n        else:\n            Next_Date = train_date[-1] + datetime.timedelta(days = 1)\n            if len(train_y) == 0:\n                Next_Y = 0\n            else:\n                Next_Y = sum(train_y) // len(train_y)\n            test_date.append(Next_Date)\n            test_y.append(Next_Y)\n        \n        df_train = pd.DataFrame()\n        df_test = pd.DataFrame()\n        \n        df_train['ds'] = train_date\n        df_test['ds'] = test_date\n        \n        df_train['y'] = train_y\n        df_test['y'] = test_y\n    \n    prophet = Prophet()\n    prophet.fit(df_train)\n    \n    predicted = TestProphet(prophet, df_train, df_test)\n    \n    get_metrics(df_test['y'], predicted, 'Prophet', str(x), save_metrics)\n    \n    if save_Model:\n        save_model(prophet, str(x), 'Prophet', last_date = '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: While creating the models, always initialize the frame_cache before calling the create functions for any model\nframe_cache = initialize_frames(df, set_of_clusters)\nfor i in range(len(set_of_clusters)):\n    createProphet(i, frame_cache['df' + str(i)], save_metrics = False, save_Model = False)\n    \n# Metrics are stored in DBSCAN_Prophet_metrics.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecasting for the test data using ETS\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\ndef ETSForecast(model, no_of_days):\n    \n    test_predictions = model.forecast(no_of_days).rename('TES Forecast')\n    return list(test_predictions)\n\ndef TestETS(model, df_train, df_test):\n    actual_dates = list(df_test['ds'])\n    LatestDate = actual_dates[-1]\n    LastDate = list(df_train['ds'])[-1]\n    \n    no_of_days = (LatestDate - LastDate).days\n    if no_of_days <= 0:\n        print(\"Please enter a date after \" + str(LastDate))\n        return\n    \n    predicted = ETSForecast(model, no_of_days)\n#     print(len(predicted))\n    \n    predictions = list()\n    for DATE in actual_dates:\n        diff = (DATE - LastDate).days\n#         print(diff)\n        predictions.append(predicted[diff - 1])\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ETS\n\nimport datetime\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\ndef createETS(x, df_x, save_metrics = False, save_Model = False):\n    df_x.reset_index(inplace = True)\n    df_x = df_x.groupby(['OCCURRED_ON_DATE']).count()[['level_0','index']]\n    df_x.drop(columns=['index'], inplace = True)\n    \n    df_train = df_x[0 : int(len(df_x) * 0.8)]\n    df_train = df_train.reset_index()\n    df_train.rename(columns = {'OCCURRED_ON_DATE':'ds', 'level_0':'y'} , inplace = True)\n    \n    df_test = df_x[int(len(df_x) * 0.8) : ]\n    df_test = df_test.reset_index()\n    df_test.rename(columns = {'OCCURRED_ON_DATE':'ds', 'level_0':'y'} , inplace = True)\n    \n    while len(df_train) < 2:\n        train_date = list(df_train['ds'])\n        train_y = list(df_train['y'])\n        \n        test_date = list(df_test['ds'])\n        test_y = list(df_test['y'])\n        \n        train_date.append(test_date[0])\n        train_y.append(test_y[0])\n        \n        test_date.remove(test_date[0])\n        test_y.remove(test_y[0])\n        \n        if len(test_date) > 0:\n            Next_Date = test_date[-1] + datetime.timedelta(days = 1)\n            if len(test_y) == 0:\n                Next_Y = 0\n            else:\n                Next_Y = sum(test_y) // len(test_y)\n            test_date.append(Next_Date)\n            test_y.append(Next_Y)\n        else:\n            Next_Date = train_date[-1] + datetime.timedelta(days = 1)\n            if len(train_y) == 0:\n                Next_Y = 0\n            else:\n                Next_Y = sum(train_y) // len(train_y)\n            test_date.append(Next_Date)\n            test_y.append(Next_Y)\n        \n        df_train = pd.DataFrame()\n        df_test = pd.DataFrame()\n        \n        df_train['ds'] = train_date\n        df_test['ds'] = test_date\n        \n        df_train['y'] = train_y\n        df_test['y'] = test_y\n    \n    # For some cluster organizations, Triple Exponential Smoothing may not work\n    \n    s_p = 2\n    ets_model = ExponentialSmoothing(df_train['y']).fit()\n\n#     triple_model = ExponentialSmoothing(df_train['y'], trend = 'add', seasonal = 'add', seasonal_periods = s_p).fit()\n    test_predictions = TestETS(ets_model, df_train, df_test)\n    \n    get_metrics(df_test['y'], test_predictions, 'ETS', str(x), save_metrics)\n    \n    if save_Model:\n        save_model(ets_model, str(x), 'ETS', str(list(df_train['ds'])[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame_cache = initialize_frames(df, set_of_clusters)\nfor i in range(len(set_of_clusters)):\n    createETS(i, frame_cache['df' + str(i)], save_metrics = False, save_Model = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare frames for ARIMA model\n\nimport datetime\n\ndef initialize_ARIMA_frames(df, Clusters):\n    frame_ARIMA_cache = {}\n    \n    for i in range(len(Clusters)):\n        df_x = df[df['Cluster'] == Clusters[i]]\n        df_x.reset_index(inplace = True)\n        df_x = df_x.groupby(['OCCURRED_ON_DATE']).count()[['level_0','index']]\n        df_x.drop(columns = ['index'], inplace = True)\n        df_x.reset_index(inplace = True)\n        df_x.rename(columns = {'OCCURRED_ON_DATE':'ds', 'level_0':'y'} , inplace = True)\n        frame_ARIMA_cache['df' + str(i)] = df_x\n    \n    return frame_ARIMA_cache\n\nframe_ARIMA_cache = initialize_ARIMA_frames(df, set_of_clusters)\n\ndef prepare_data(frame_ARIMA_cache, set_of_clusters):\n    for i in range(len(set_of_clusters)):\n        df_x = frame_ARIMA_cache['df' + str(i)]\n        df_date = list(df_x['ds'])\n        df_y = list(df_x['y'])\n    \n        new_dates = [df_date[0]]\n        new_y = [df_y[0]]\n    \n        for I in range(1, len(df_date)):\n            no_of_days = (df_date[I] - df_date[I - 1]).days\n            starting_date = df_date[I - 1]\n        \n            while no_of_days > 1:\n                next_date = starting_date + datetime.timedelta(days = 1)\n                next_y = 0\n            \n                new_dates.append(next_date)\n                new_y.append(next_y)\n            \n                no_of_days -= 1\n                starting_date = next_date\n            \n            new_dates.append(df_date[I])\n            new_y.append(df_y[I])\n        \n        df_x = pd.DataFrame()\n        df_x['ds'] = new_dates\n        df_x['y'] = new_y\n    \n        frame_ARIMA_cache['df' + str(i)] = df_x\n    return frame_ARIMA_cache\n\nframe_ARIMA_cache = prepare_data(frame_ARIMA_cache, set_of_clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecasting for the test data using ARIMA\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\ndef ARIMAForecast(model, no_of_days):\n    forecast = model.forecast(no_of_days)[0]\n    return forecast\n\ndef TestARIMA(model, df_train, df_test):\n    actual_dates = list(df_test['ds'])\n    LatestDate = actual_dates[-1]\n    LastDate = list(df_train['ds'])[-1]\n    \n    no_of_days = (LatestDate - LastDate).days\n    if no_of_days <= 0:\n        print(\"Please enter a date after \" + str(LastDate))\n        return\n    \n    predicted = ARIMAForecast(model, no_of_days)\n#     print(len(predicted))\n    \n    predictions = list()\n    for DATE in actual_dates:\n        diff = (DATE - LastDate).days\n#         print(diff)\n        predictions.append(predicted[diff - 1])\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ARIMA\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\ndef createARIMA(x, df_x, save_metrics = False, save_Model = False):\n    \n    df_train = df_x[0 : int(len(df_x) * 0.8)]\n    \n    df_test = df_x[int(len(df_x) * 0.8) : ]\n    \n    model = ARIMA(np.asarray(df_train['y']), order = (1, 1, 0))\n    model_fit = model.fit(disp = 0)\n    predicted = TestARIMA(model_fit, df_train, df_test)\n    \n    get_metrics(df_test['y'], predicted, 'ARIMA', str(x), save_metrics)\n    \n    if save_Model:\n        save_model(model_fit, str(x), 'ARIMA', last_date = str(list(df_train['ds'])[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(set_of_clusters)):\n    createARIMA(str(i), frame_ARIMA_cache['df' + str(i)], save_metrics = False, save_Model = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\ndef delete_output_files(specific_file = ''):\n    if specific_file == '' or specific_file is None:\n        output_files = os.listdir('/kaggle/working')\n    \n        for i in output_files:\n            os.remove(i)\n    else:\n        os.remove(specific_file)\n        \n# un-comment the next line this cell after you have downloaded the files you need\n# delete_output_files()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing out the models for a given cluster\n\nimport os\nimport datetime\n\n# input_date = input(\"Enter date (dd-mm-YYYY): \")\n# input_date = datetime.strptime(input_date, '%d-%m-%Y')\n# cl = int(input(\"Enter the cluster for which you want to test: \"))\n\ninput_date = datetime.datetime.now()\ncl = 0\n\n# Create the models\n\nframe_cache = initialize_frames(df, set_of_clusters)\ncreateProphet(cl, frame_cache['df' + str(cl)], save_metrics = False, save_Model = True)\n\nframe_cache = initialize_frames(df, set_of_clusters)\ncreateETS(cl, frame_cache['df' + str(cl)], save_metrics = False, save_Model = True)\n\ncreateARIMA(cl, frame_ARIMA_cache['df' + str(cl)], save_metrics = False, save_Model = True)\n\noutput_list = os.listdir()\n\n# Load the models\n\nprophet = ets = arima = None\npkl_prophet = pkl_ets = pkl_arima = ''\n\nfor i in output_list:\n    if 'Prophet' in i and not 'metrics' in i:\n        pkl_prophet = i\n    elif 'ETS' in i and not 'metrics' in i:\n        pkl_ets = i\n    elif 'ARIMA' in i and not 'metrics' in i:\n        pkl_arima = i\n        \nprophet = load_model(pkl_prophet)\nets = load_model(pkl_ets)\narima = load_model(pkl_arima)\n\n# Testing prophet\n\nw_prophet = None\nlast_date = list(prophet.history_dates)[-1]\nno_of_days = (input_date - last_date).days\n\nif no_of_days > 0:\n    \n    forecast = ProphetsForecast(prophet, no_of_days)\n    w_prophet = list(forecast['yhat'])[-1]\n    print(\"Prophet forecasts: \" + str(w_prophet))\nelse:\n    print(\"Please enter a date which is after \" + str(last_date))\n    \n# Testing ETS\n\nw_ets = None\nlast_str = pkl_ets.split('_')[-1].split('.')[0]\nlast_date = datetime.datetime.strptime(last_str, '%Y-%m-%d')\n\nno_of_days = (input_date - last_date).days\n\nif no_of_days > 0:\n    predictions = ets.forecast(no_of_days).rename('TES Forecast')\n    w_ets = list(predictions)[-1]\n    print(\"ETS forecasts: \" + str(w_ets))\nelse:\n    print(\"Please enter a date which is after \" + str(last_date))\n    \n# Testing ARIMA\n\nw_arima = None\nlast_str = pkl_arima.split('_')[-1].split('.')[0]\nlast_date = datetime.datetime.strptime(last_str, '%Y-%m-%d')\n\nno_of_days = (input_date - last_date).days\n\nif no_of_days > 0:\n    forecast = arima.forecast(no_of_days)[0]\n    w_arima = forecast[-1]\n    print(\"ARIMA forecasts: \" + str(w_arima))\nelse:\n    print(\"Please enter a date which is after \" + str(last_date))\n\ndelete_output_files(specific_file = 'DBSCAN_Prophet_0.pkl')\ndelete_output_files(specific_file = 'DBSCAN_ETS_0_2018-01-10.pkl')\ndelete_output_files(specific_file = 'DBSCAN_ARIMA_0_2018-01-10.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}