{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PCA on Sentence Embeddings with RAPIDS","metadata":{}},{"cell_type":"markdown","source":"This notebook is intended on providing [**optimal text embeddings**](https://www.kaggle.com/louise2001/embeddings-actuarial-loss-competition) for the claim descriptions of the [actuarial loss competition](https://www.kaggle.com/c/actuarial-loss-estimation). I will detail the whole procedure with additional illustrations of the obtained vectorized representations.","metadata":{}},{"cell_type":"markdown","source":"## Summary :\n1. Obtaining Sentence Embeddings from Transformers\n2. Principal Components Analysis with RAPIDS\n2. Data Analysis and Vizualization","metadata":{}},{"cell_type":"markdown","source":"### Installing RAPIDS and other requirements","metadata":{}},{"cell_type":"markdown","source":"[RAPIDS](https://rapids.ai) enable you to perform every numpy, pandas or sklearn manipulation & modeling, entirely on GPU for higher performance.","metadata":{}},{"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cudf as pd # pandas on GPU\nimport cupy as np # numpy on GPU\nfrom cuml.decomposition import PCA # scikit-learn on GPU\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer # PyTorch supported\nimport gc\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading and Processing Text Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/actuarial-loss-estimation/train.csv')\ndf_test  = pd.read_csv('../input/actuarial-loss-estimation/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will concatenate text data from train and test database in order to process them globally. That will provide better fitting in the PCA as well as prevent any unpleasant surprises.","metadata":{}},{"cell_type":"code","source":"n0 = df_train.shape[0]\n# text is upper case in source data, but models are lower case\ntxt = [t.lower() for df in (df_train, df_test) for t in df.ClaimDescription.to_array()]\ntxt[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformers are a very efficient way of getting optimal text embeddings.\nI will compute raw sentence embeddings based on the paraphrase-trained DistilRoberta. You can see more on this model [here](https://github.com/UKPLab/sentence-transformers) or [here](https://www.sbert.net).","metadata":{}},{"cell_type":"markdown","source":"Why did I choose the model trained on the paraphrase-scoring task ? We are trying to extract the global meaning of the description, in order to get an idea of the gravity or lasting consequences of an accident. It therefore seemed to me to be a related and adequate model to use for getting my raw embeddings.","metadata":{}},{"cell_type":"markdown","source":"### Model Preparation","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available(): # check if GPU enabled kernel\n    print('Cuda !')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer('paraphrase-distilroberta-base-v1', device='cuda')\nprint(f'Initial sequence length in paraphrase distilroberta : {model.max_seq_length}')\nprint(f'First sentence : {txt[0]}\\nCorresponding tokens : {model.tokenizer(txt[0])}')\nprint(f\"Maximal sequence length in our text data : {max([len(model.tokenizer(t)['input_ids']) for t in txt])}\")\n# resizing model max_seq_length for faster computations (remove a loooot of unuseful <PAD> tokens)\nmodel.max_seq_length = 25\nprint(f'Resized sequence length in paraphrase distilroberta : {model.max_seq_length}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Raw Roberta embeddings","metadata":{}},{"cell_type":"code","source":"txt_encoded = np.array(model.encode(txt, normalize_embeddings=True))\ntxt_encoded.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get 768-dimensional vectorized and normalized representations of our 90_000 sentences.That's quite big, and not very efficient : as our sentences are from the same writing-style, they are probably highly correlated, that's not great for building a model that would be able to differentiate injury severity. Moreover, we would like the coordinates to bear explainability power, and ideally to have them orthonormal.","metadata":{}},{"cell_type":"code","source":"plt.hist(np.var(txt_encoded, axis=0).get(), bins=100)\nplt.title('Variance on the 768 Embedding Coordinates')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, we can see that the variance on each embedding axis is very low (for comparison, the expected variance had the coordinates been sampled from a $\\mathcal{U}([0,1])$ would have been of $\\frac{1}{12} \\approx 0.0833$ along each of the 768 axes). It means, as expected, that the data is highly colinear, all embedding coordinates being centered on the same values.","metadata":{}},{"cell_type":"markdown","source":"### Principal Component Analysis","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2, copy=True, random_state=0, svd_solver='jacobi', whiten=True, verbose=True)\ntxt_encoded_pca = pca.fit_transform(txt_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How much of the total variance are explained by these 2 first axes ?","metadata":{}},{"cell_type":"code","source":"pca.explained_variance_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's not much... Let's get a visualization of how our claim descriptions are scattered based on the first 2 explainability axes.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(txt_encoded_pca[:, 0].get(), txt_encoded_pca[:, 1].get(), c='r')\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Isn't that cute ? Now, let's see how these axes bear explainability towards our problem. We will try to see how the accidents scatter along these 2 axes, based on the target ultimate cost that we wish to predict.","metadata":{}},{"cell_type":"markdown","source":"### Clustering on Target Value","metadata":{}},{"cell_type":"code","source":"def quantile(array, threshold):\n    array_ = np.sort(array.flatten())\n    return array_[int(threshold * array_.shape[0])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = txt_encoded_pca[:n0, 0].get(), txt_encoded_pca[:n0, 1].get()\n# take only train data as we want to explain target, here ultimate cost\nc = df_train.UltimateIncurredClaimCost.values\ntrunc = quantile(c, 0.8)\n# for more visibility, as there are some very extreme values, I have to truncate too big values\ncriteria = np.where(c <= trunc)[0].get()\nx, y, c = x[criteria], y[criteria], c[criteria]\nfig, ax = plt.subplots(figsize=(10,10))\nax.scatter(x, y, c=c.get(), cmap='jet')\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\n\ncmap = mpl.cm.jet\nnorm = mpl.colors.Normalize(vmin=min(c), vmax=max(c))\n\ncbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n             ax=ax, orientation='vertical', label='Ultimate Cost')\n\nplt.savefig('Ultimate_cost_based_on_text.png', dpi=100)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice a very clear separation of the two halves of the heart into 2 subgroups, the left half with lower ultimate costs, the right half with higher ultimate costs.","metadata":{}},{"cell_type":"markdown","source":"As a conclusion, the data provided in the [Embeddings dataset](https://www.kaggle.com/louise2001/embeddings-actuarial-loss-competition) has been generated on the first 20 principal orthonormal components. I provide the source code hereunder.","metadata":{}},{"cell_type":"code","source":"n_comp = 20\npca = PCA(n_components=n_comp, copy=True, random_state=0, svd_solver='jacobi', whiten=True, verbose=True)\ntxt_encoded_pca = pca.fit_transform(txt_encoded)\nembeddings_train, embeddings_test = pd.DataFrame(txt_encoded_pca[:n0, :], columns=[f'X_{i}' for i in range(n_comp)]), pd.DataFrame(txt_encoded_pca[n0:, :], columns=[f'X_{i}' for i in range(n_comp)])\nembeddings_train.to_csv(f'embeddings_train_{n_comp}.csv', index=False)\nembeddings_test.to_csv(f'embeddings_test_{n_comp}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}