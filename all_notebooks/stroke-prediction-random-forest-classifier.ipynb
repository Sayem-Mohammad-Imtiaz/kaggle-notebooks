{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizate fist columns\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for general data an non-nulls\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling NA from \"bmi\" column with the mean of the column\ndata[\"bmi\"] = data[\"bmi\"].fillna(data[\"bmi\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Values from categories\n\nprint(data[\"gender\"].unique())\nprint(data[\"ever_married\"].unique())\nprint(data[\"work_type\"].unique())\nprint(data[\"Residence_type\"].unique())\nprint(data[\"smoking_status\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the replacement of the categorical variables\n\n#Gender column\ndata[\"gender\"] = data[\"gender\"].replace([\"Male\"], 0)\ndata[\"gender\"] = data[\"gender\"].replace([\"Female\"], 1)\ndata[\"gender\"] = data[\"gender\"].replace([\"Other\"], 2)\n\n#Ever married column\ndata[\"ever_married\"] = data[\"ever_married\"].replace([\"Yes\"], 0)\ndata[\"ever_married\"] = data[\"ever_married\"].replace([\"No\"], 1)\n\n#work_type column\ndata[\"work_type\"] = data[\"work_type\"].replace([\"Private\"], 0)\ndata[\"work_type\"] = data[\"work_type\"].replace([\"Self-employed\"], 1)\ndata[\"work_type\"] = data[\"work_type\"].replace([\"Govt_job\"], 2)\ndata[\"work_type\"] = data[\"work_type\"].replace([\"children\"], 3)\ndata[\"work_type\"] = data[\"work_type\"].replace([\"Never_worked\"], 4)\n\n#residence_type column\ndata[\"Residence_type\"] = data[\"Residence_type\"].replace([\"Urban\"], 0)\ndata[\"Residence_type\"] = data[\"Residence_type\"].replace([\"Rural\"], 1)\n\n#smoking_status\ndata[\"smoking_status\"] = data[\"smoking_status\"].replace([\"formerly smoked\"], 0)\ndata[\"smoking_status\"] = data[\"smoking_status\"].replace([\"never smoked\"], 1)\ndata[\"smoking_status\"] = data[\"smoking_status\"].replace([\"smokes\"], 2)\ndata[\"smoking_status\"] = data[\"smoking_status\"].replace([\"Unknown\"], 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining prectors variables and target\n#For predictors i will use all de columns and the target value are de strokes\n\npredictors = data.columns[1:11]\ntarget = data.columns[11]\n\nX = data[predictors]\nY = data[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rand_forest(criterion=\"entropy\", n_estimators=500, min_samples_split=5, threshold=0.5):\n    \n    #importing the libraries\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report\n    import seaborn as sns\n    from sklearn import metrics\n    import matplotlib.pyplot as plt\n\n    forest_class = RandomForestClassifier(criterion=criterion, n_jobs=2, oob_score=True, \n                                             n_estimators=n_estimators, min_samples_split=min_samples_split)\n    forest_class.fit(X,Y)\n\n    pred = forest_class.predict(X)\n\n    prob = forest_class.predict_proba(X)\n\n    #Defining the threshold to decide the probability limit for forecasting an stroke\n    prob = prob[:, 1]\n    prob_df = pd.DataFrame(prob)\n    prob_df[\"prediction\"] = np.where(prob_df[0] >= threshold, 1, 0 )\n\n    matrix = confusion_matrix(Y, prob_df.prediction)\n    matrix\n\n\n    fig, g = plt.subplots(figsize=(5,5))\n    g = sns.heatmap(matrix.T, annot=True, cbar= False, fmt=\"\",\n                xticklabels=[\"0\", \"1\"], \n                yticklabels=[\"0\", \"1\"])\n    g.set_title(\"Confusion Matrix\")\n    g.set_xlabel(\"Reals\")\n    g.set_ylabel(\"Predictions\")\n\n\n    print(classification_report(Y, prob_df.prediction, target_names= [\"0\",\"1\"]))\n    \n    #Making a cross validation with 10 splits\n    \n    from sklearn.model_selection import KFold, cross_val_score\n    cv = KFold(n_splits= 10, shuffle=True, random_state=1)\n    score = np.mean(cross_val_score(forest_class, X, Y, scoring = \"accuracy\", cv= cv))\n    \n    print(\"--------------------------------------\")\n    \n    print(\"The Cross Validation with 10 samples got an  accuracy score of {}%\".format(round(score*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_forest(threshold=0.4)\n#Using a theshold of 0.45 to catalog an stroke","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Random Forest model adjusted very well to the data, having a 100% of accuracy. In my opinion is very important to have good accuracy predicting the strokes, even if in the effort of increasing that acc the model predicts non strokes as strokes. For checking if the model doesnÂ´t overfit the data, i made a cross validation taking 10 splits. In that cross validation the model have an 95% of accuracy.\n\nThis is my first publish, any suggestiongs ar well welcome!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}