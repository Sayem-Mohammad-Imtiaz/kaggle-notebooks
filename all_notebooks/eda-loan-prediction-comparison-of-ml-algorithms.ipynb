{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center><u> Credit DataSet </u></center>"},{"metadata":{},"cell_type":"raw","source":"1.\tData Loading\n\n2.\tData Description\n\n3.\tData Preparation\n    3.1.\tHandling Null and Incorrect Values\n4.\tUnivariate Analysis\n\n5.\tBi Variate Analysis\n    5.1.\tBarplots for Categorical Attributes\n    5.1.1.\tResults\n    5.2.\tBox plots for Numerical Attributes\n    5.2.1.\tResults\n6.\tFeatures Selection\n    6.1.\tChi Square Statistics\n    6.2.\tPearson Correlation of Target Variables and Numerical Attributes\n    6.2.1.\tManual Encoding of Target Variable\n    6.2.2.\tPearson Correlation\n7.\tPreprocessing\n    7.1.\tFeature Encoding and Discretization\n    7.1.1.\tManual Encoding\n    7.1.2.\tOne Hot Encoding\n8.\tMachine Learning Models\n    8.1.\tLogistic Regression\n        8.1.1. Transforming Features and Target Variables into Arrays\n        8.1.2. Train Test Split\n        8.1.3. Applying Logistic Regression\n        8.1.4. Logistic Regression Evaluation\n        8.1.5. Actual vs Prediction\n        8.1.5.1. Actual vs Predicted Graph\n    8.2.\tLogistic Regression (Features Selected)\n        8.2.1. Dropping Attributes\n        8.2.2. Transforming into arrays\n        8.2.3. Train Test Split\n        8.2.4. Applying Logistic Regression (Selected Features)\n        8.2.5. Logistic Regression Evaluation (Selected Features)\n    8.3.\tKNN Algorithm\n        8.3.1. Applying KNN\n        8.3.2. Knn Evaluation\n    \n    8.4.\tNaive Bayes Classifier\n        8.4.1. Applying Naive Bayes Classifier\n        8.4.2. Naive Bayes Evaluation\n\n    8.5.\tDecision Tree Classifier\n        8.5.1. Applying Decision Tree\n        8.5.1. Decision Tree Evaluation\n    \n    8.6. SVM Classifier\n        8.6.1. Applying SVM Classifier\n        8.6.2. SVM Evaluation\n      \n    8.7. Random Forest Classifier\n        8.7.1. Applying Random Forest Classifer\n        8.7.2. Random Forest Evaluation\n    \n    8.8. Model Metrics Comparision"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport plotly.figure_factory as ff\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score,recall_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf =  pd.read_csv('../input/loadpred/train_AV3.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Description"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Handling Null and Incorrect Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender.fillna(df.Gender.mode().values[0], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Married"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Married.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Married.fillna(df.Married.mode().values[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Married.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dependents"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dependents.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dependents.fillna(df.Dependents.mode().values[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dependents.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Self Employed"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Self_Employed.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Self_Employed.fillna(df.Self_Employed.mode().values[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Self_Employed.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loan Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.LoanAmount.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.LoanAmount.fillna(round(df.LoanAmount.mean(),0),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.LoanAmount.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loan Amount Term"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Amount_Term.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Amount_Term.fillna(round(df.Loan_Amount_Term.mean(),0), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Amount_Term.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Credit History"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Credit_History.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Credit_History.fillna(df.Credit_History.mode().values[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Credit_History.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <center> 4. Uni-Variate Analysis</center>"},{"metadata":{},"cell_type":"markdown","source":"The purpose of univriate analysis here is to only check and get the idea of distribution of attributes in the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorical_plots(var, data):\n    \n    \n    #Adjustment of plots, bigger size and space b/w subplots\n    \n    fig = plt.figure(figsize=(15,5))\n    fig.subplots_adjust(wspace=0.7)\n    \n    #1st Plot:  Bar plot     \n        \n    plt.subplot(1,3,1)\n    sns.countplot(x=var, data= data)\n    plt.xticks(rotation = 45, horizontalalignment='right')\n    plt.xlabel(var.name + ' Distribution')\n\n    #2nd Plot: PIE Chart\n    \n    labels =var.value_counts().index  #Labels that will be written against slices in pie charts\n    \n    #For the slice with highest value to be exploded, explode parameter is passed. Using for loop to make a tuple of \n    # number of slice using len(unique) and exploding the first slice by mentioning 0.1 at first index. Atlast converted list to tuple\n    \n    a=[0.1]\n    for i in range ((len(var.unique()))-1):\n        a.append(0)\n\n    explode1= tuple(a)\n    #if var.name != 'Customer Name':\n    ax1 = plt.subplot(1,3,2)\n    ax1.pie(var.value_counts(), labels=labels,autopct='%1.1f%%', shadow=True,explode= explode1 )\n    ax1.axis('equal')\n    plt.xlabel(var.name + ' Distribution')\n    \n    #3rd Plot: Line Plot\n    \n    plt.subplot(1,3,3)\n    var.value_counts().sort_index().plot.line()\n    plt.xticks(rotation = 45, horizontalalignment='right')\n    plt.xlabel(var.name + ' Distribution')\n    \n    show=plt.show()\n    \n    return(show)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FOR NUMERICL PLOTS WE WILL BE USING THE FOLLOWING FUNCTION\n\ndef numerical_plots(var):\n    \n    #Adjustment of plots, bigger size and space b/w subplots\n    \n    fig = plt.figure(figsize=(15,4))\n    fig.subplots_adjust(wspace=0.3)\n    \n    #1st Plot:  Histogram with KDE plot          \n \n    plt.subplot(1,3,1)\n    sns.distplot(var, color='b')\n    plt.xlabel(var.name + ' Distribution')\n\n    \n    #2nd Plot:  Box plot\n    \n    plt.subplot(1,3,2)\n    sns.boxplot(y=var)\n    plt.xlabel(var.name + ' Distribution')\n\n\n    #3rd Plot:  Histogram without plot     \n\n    plt.subplot(1,3,3)\n    sns.distplot(var, color='b', kde=False)\n    plt.xlabel(var.name + ' Distribution')\n    \n    #plt.subplot(1,3,3)\n    #sns.kdeplot(var, color='b')\n    #plt.xlabel(var.name + ' Distribution')\n    \n    show=plt.show()\n    \n    return(show)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Gender, df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Married, df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Dependents,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Education,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Self_Employed,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_plots(df.ApplicantIncome)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_plots(df.CoapplicantIncome)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_plots(df.LoanAmount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Loan_Amount_Term,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Credit_History,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Property_Area,df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_plots(df.Loan_Status, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <center> 5. Bi-Variate Analysis</center>"},{"metadata":{},"cell_type":"markdown","source":"<b>Our main target is to find out customers that are elgible for Loan. Therefore we will plot bar charts of every categorical attribute agaist Loan Status and boxplot for every numerical attribute against Loan Status </b>"},{"metadata":{},"cell_type":"markdown","source":"For the purpose of better understanding of Credit History attribute relation with Target variable we will categorize it into Yes and No."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Credit_History = np.where(df.Credit_History== 1., 'Yes','No')\ndf.Credit_History.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1. Barplots for Categorical Attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    if df[i].dtype =='O' and i!='Loan_ID':\n        sns.countplot(x=df[i], hue=df.Loan_Status)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> 5.1.1 Results: </b>\n    \nGender: There are more males than females who have been granted Loan.\n\nMarital Status: From the plots it seems that people who are married have a greater proability for loan.\n    \nDependents: From the plots it seems that people who have no dependents are the one who have mostly applied for the loan. And interestingly are also in majority who have been granted loan.\n\nEducation: Majority of the people who applied for loan are graduates and also have a higher probability of getting loan as compared to non graduates.\n\nSelf Employed: A large portion of the people who applied for loan are of salaried class and majority of them have been granted loan.\n\nCredit History:It is pretty clear from the plot that applicants who have a cerdit history are very likely to be granted loan.\n\nProperty Area: Customers residing in Semiurban area seem to have a higher probability of getting loans.\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Box plots for Numeric Attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    if df[i].dtype !='O':\n        sns.boxplot(y=df[i], x=df.Loan_Status)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>5.2.1 Results : </b> Numerical attributes dont seem to have a noticable relation with the target variable. This further will be conifrmed in the correlation matrix."},{"metadata":{},"cell_type":"markdown","source":"## 6. Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"To find if any categorical attribute is independent of our target variable (Loan Status), we will calculate chi square statistics."},{"metadata":{},"cell_type":"markdown","source":"### 6.1. Chi Square Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(include='O').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chi2(data,target,alpha):\n    \n    for i in df.columns:\n    \n        if df[i].dtype == 'O' and i != target:\n            col = i\n\n            ov = pd.crosstab(data[col], data[target])\n            #max_least_income = ov.loc[ov[' <=50K'].idxmax()].name\n            #max_highest_income = ov.loc[ov[' >50K'].idxmax()].name\n            plt.style.use('ggplot')\n            ov.plot(kind='bar', figsize=(5,5), stacked=True)\n            plt.xlabel(i.title())\n                 \n            chi = s.chi2_contingency(ov)\n            chi2_s = chi[0]\n            p_value = chi[1]\n            dof = chi[2]\n            critical_value = s.chi2.ppf(q=1-alpha, df=dof)\n            \n            print('\\n\\033[1m\\033[4m', col.upper(),':\\033[0m \\n')\n            print('Significance Level = ', alpha)\n            print('Degree of Freedom = ', dof)\n            print('chi2 = ', chi2_s)\n            print('Critical Value = ',critical_value)\n            print('p-value = ', p_value)\n\n            if chi2_s >=critical_value or p_value <= alpha :\n                print('\\nWe reject the null hypotheses, there is a relationship between the two variables \\n')\n            else:\n                print('\\nThere is no relationship between the two variables and the null hypotheses is retained \\n')\n            \n            plt.show()\n            #print('\\033[1mThe bar chart shows that', max_least_income,i,'has the highest number of people with <=50k income and',max_highest_income,i,'has the highest number of people having income >50K \\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi2(df, 'Loan_Status', 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['Loan_ID'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Pearson Correlation of Target Variables and Numerical Attributes\\"},{"metadata":{},"cell_type":"markdown","source":"For Numerical Attributed we will check correlation of each with target variable. For this will first manually encode our target variable to 0 and 1."},{"metadata":{},"cell_type":"markdown","source":"#### 6.2.1 Manual Encoding of Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Status.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Status = np.where(df.Loan_Status=='Y', 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.2.2 Pearson Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation =df.corr()\ncorrelation.Loan_Status","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Based on chi square statistics and Pearson correlation matrix we can drop some attributes but first we will train and evaluate model without dropping any and later on evaluate model using selected features. </b>"},{"metadata":{},"cell_type":"markdown","source":"## 7. Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"### 7.1 Feature Encoding and Descretization"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all categorical variabes that have more than two unique we will hot encode them and for the rest we will manually encode them."},{"metadata":{},"cell_type":"markdown","source":"#### 7.1.1 Manual Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender = np.where(df.Gender =='Male', 1,0)\ndf.Married = np.where(df.Married == 'Yes',1,0)\ndf.Education = np.where(df.Education == 'Graduate',1,0)\ndf.Self_Employed = np.where(df.Self_Employed =='No', 1,0)\ndf.Credit_History = np.where(df.Credit_History =='Yes',1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.1.2 One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dummies = pd.get_dummies(df.Property_Area)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dummies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,df.Dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['Property_Area'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nzscore = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will used copied dataframe here. And 3+ values needs to be changed for normalization, doing so.\ndf['Dependents'] = np.where(df.Dependents == '3+', 4, df.Dependents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['ApplicantIncome', 'CoapplicantIncome','LoanAmount','Loan_Amount_Term']\nfor i in cols:\n    df[i] = zscore.fit_transform(df[[i]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 =df.copy() # Saving a copy of dataframe to be utilized later on to see the effects of normalization and feature selection on model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center><u>8. Machine Learning Models </u></center>"},{"metadata":{},"cell_type":"markdown","source":"## 8.1 Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"### 8.1.1 Tranforming features and Target Variables into Arrays"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Loan_Status'] #Separating Target Variable\ndf.drop(columns=['Loan_Status'], inplace=True)\nx = df\nx= x.to_dict(orient='records')\n\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\nx = vec.fit_transform(x).toarray()\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.asarray(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1.2 Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use this split data for all algorithms\nxtrain,xtest,ytrain,ytest =train_test_split(x,y,test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1.3 Applying Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nmodel_LG =LogisticRegression()\nmodel_LG.fit(xtrain,ytrain);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_LG = model_LG.predict(xtest)\nprobabilities = model_LG.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a functio to be used for evaluation for all algorithms\ndef evaluation(algorithm):\n    #Classification Report\n    print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest,algorithm ))\n\n    #Accuracy\n    print (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, algorithm))\n\n    #conf_matrix\n    conf_matrix = confusion_matrix(ytest,algorithm)\n\n\n    #roc_auc_score\n    model_roc_auc = round(roc_auc_score(ytest, algorithm),3) \n    print (\"\\033[1mArea under curve : \\033[0m\",model_roc_auc)\n    fpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])\n\n    # roc curve plot\n    trace1 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                       )\n    #confusion matrix plot\n    trace2 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Not Granted\",\"Granted\"],\n                        y = [\"Not Granted\",\"Granted\"],\n                        colorscale = \"Viridis\",name = \"matrix\" )\n    #subplots\n    fig = tls.make_subplots(rows=1, cols=2, horizontal_spacing = 0.40,subplot_titles=('ROC Curve','Confusion Matrix'))\n\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n\n\n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 400,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         xaxis = dict(title = \"false positive rate\",\n                                 gridcolor = 'rgb(255, 255, 255)',\n                                 domain=[0, 0.6],\n                                 ticklen=5,gridwidth=2),\n                        yaxis = dict(title = \"true positive rate\",\n                                  gridcolor = 'rgb(255, 255, 255)',\n                                  zerolinewidth=1),\n                        margin = dict(b = 20))\n\n    py.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1.4 Logistic Regression Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"\\n\\033[1m Classification report : \\033[0m\\n\",classification_report(ytest,y_pred_LG))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest,y_pred_LG))\nevaluation(y_pred_LG)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1.5 Actual vs Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame({'Actual': ytest.flatten(), 'Predicted': y_pred_LG.flatten()})\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.1.5.1 Actual vs Predicted Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.head(20)\ndata.plot(kind='bar',figsize=(15,5))\nplt.title('Actual vs Predicted')\nplt.grid(which='major', linestyle=':', linewidth='0.99', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.2 Logistic Regression (Features Selected)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2.1 Dropping Attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df1 is a normzalized dataframe saved earlier\ny2 =df1['Loan_Status']\ndf1.drop(columns=['Gender','Dependents','Self_Employed','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Loan_Status'], inplace=True)\n\ndf1.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2.2 Transforming into arrays"},{"metadata":{"trusted":true},"cell_type":"code","source":"x2=df1\nx2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x2=df1\nx2 = x2.to_dict(orient='records')\nx2 =vec.fit_transform(x2).toarray()\ny2=np.asarray(y2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2.3 Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain2,xtest2,ytrain2,ytest2 =train_test_split(x2,y2,test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2.4 Applying Logistic Regression (Selected Features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression model training\nmodel_LG.fit(xtrain2,ytrain2); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction\ny_pred_LG2 = model_LG.predict(xtest2)\nprobabilities = model_LG.predict_proba(xtest2)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2.5 Logistic Regression Evaluation (Selected Features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(y_pred_LG2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.3 KNN Algorithm"},{"metadata":{},"cell_type":"markdown","source":"### 8.3.1 Applying KNN "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n#Model Traning\nmodel_knn = KNeighborsClassifier()\nmodel_knn.fit(xtrain,ytrain);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction\ny_pred_knn = model_knn.predict(xtest)\nprobabilities = model_knn.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.3.2 Knn Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(y_pred_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.4 Naive Bayes Classifier"},{"metadata":{},"cell_type":"markdown","source":"### 8.4.1 Applying Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n#Model Training\nmodel_nb = GaussianNB()\nmodel_nb.fit(xtrain, ytrain);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Prediction\ny_pred_nb = model_nb.predict(xtest)\nprobabilities = model_nb.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.4.2 Naive Bayes Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(y_pred_nb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.5 Decision Tree Classifier"},{"metadata":{},"cell_type":"markdown","source":"### 8.5.1 Applying Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\n# Model Traning\nmodel_DT = tree.DecisionTreeClassifier()\nmodel_DT.fit(xtrain,ytrain)\ny_pred_DT = model_DT.predict(xtest)\nprobabilities = model_DT.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.5.1 Decision Tree Evaluation "},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(y_pred_DT)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.6. SVM Classifer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.6.1. Applying SVM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvm_classifier = SVC(kernel='rbf', random_state=0, probability=True)\nsvm_classifier.fit(xtrain,ytrain);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm = svm_classifier.predict(xtest)\nprobabilities = svm_classifier.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.6.2 SVM Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(y_pred_svm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.7 Random Forest"},{"metadata":{},"cell_type":"markdown","source":"### 8.7.1 Applying Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel = rfc.fit(xtrain, ytrain)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rfc = rfc.predict(xtest)\nprobabilities = rfc.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.7.2 Random Forest Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(y_pred_rfc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.8 Model Metrics Comparision"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    ROC          = roc_auc_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area Under Curve\": [ROC]\n                       })\n    return df\n\nmodel1 = model_report(model_LG,xtrain,xtest,ytrain,ytest,\"Logistic Reg. \")\n\nmodel2 = model_report(model_LG,xtrain2,xtest2,ytrain2,ytest2,\"Log.Reg.Selected Feat.\")\n\nmodel3 = model_report(rfc,xtrain,xtest,ytrain,ytest,\"Random Forest\")\n\nmodel4 = model_report(model_knn,xtrain,xtest,ytrain,ytest,\"KNN Classifier\")\n\nmodel5 = model_report(model_nb,xtrain,xtest,ytrain,ytest,\"Naive Bayes\")\n\nmodel6 = model_report(model_DT,xtrain,xtest,ytrain,ytest,\"Decision Tree\")\n\nmodel7 = model_report(svm_classifier,xtrain,xtest,ytrain,ytest,\"SVM Classifier\")\n\n\nmodel_performances = pd.concat([model1,model2, model3,model4,model5,model6, model7],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"If SVM, Naive Bayes and Decision Tree are compared Naive Bayes has performed better than others in terms of accuracy, f1 score as well as area under the curve. Although the scores are not too high for any of the algorithm but if hyperparameters are tuned, we will be able to see better performances. The performances calculated above are based on when complete data set without any feature selection was passed to the algorithms. With selected features the performance of all algorithms was not good (not listed in table).\n\nIf we are to select the best among all the algorithms above then logistic regression performed well than all others."},{"metadata":{},"cell_type":"markdown","source":"## <center>------End------</center>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}