{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# load all necessary libraries\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\npd.set_option('max_colwidth', 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's build a basic bag of words model on three sample documents","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"documents = [\"Gangs of Wasseypur is a great movie.\", \"Nawaz performance in Scared games is just amazing. \", \"Ustad Zakir hussain is performing in new Delhi this evening on bollywood based theme.\" , \n             \"The success of a movie depends on the performance of the actors.\", \"There are no new movies releasing this week.\",\n             \"Manoj bajpayee is one of the finest movie actor of his genre.\", \"OTT is now the prefered medium for cinema lovers than tradition theatre.\", \n             \"Netflix is outperforming it's competions but Amazon prime not too far behind.\"]\nprint(documents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(text):\n    'changes document to lower case and removes stopwords'\n\n    # change sentence to lower case\n    text = text.lower()\n\n    # tokenize into words\n    words = word_tokenize(text)\n\n    # remove stop words\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n\n    # join words to make sentence\n    text = \" \".join(words)\n    \n    return text\n\ndocuments = [preprocess_text(text) for text in documents]\nprint(documents)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating bag of words model using count vectorizer function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer()\nmodel = vect.fit_transform(documents)\nprint(model) # returns the rown and column number of cells which have 1 as value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the full sparse matrix\nprint(model.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the shape of the matrix created, there are 45 unique words/features identified by the CountVectorizer\nprint(model.shape)\n# get the feature names\nprint(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the vectorized matrix above that there are no stopwords but than we can see that few words which convey the same meaning are included twice such as performance and performing or actor and actors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ### Now as we know basics of bag of words model, let's create a bag of words model on the spam dataset.\n https://en.wikipedia.org/wiki/Bag-of-words_model","execution_count":null},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# read the file into a panda dataframe\nfilename = \"../input/sms-spam-collection-dataset/spam.csv\"\nspam = pd.read_csv(filename,encoding='latin-1')\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop unused columns\nspam = spam.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\nspam = spam.rename(columns={\"v1\":\"label\", \"v2\":\"message\"})\n# Let's check the shape of DataFrame\nprint(spam.shape) #we have 5572 messages\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract the messages from the dataframe\nmessages = spam.message\nprint(messages)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's preprocess the messages by tokenizing and removing the stopwords from the text","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# convert messages into list\nmessages = [message for message in messages]\n# preprocess messages using the preprocess function\nmessages = [preprocess_text(message) for message in messages]\nprint(messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bag of words model\nvect = CountVectorizer()\nmodel = vect.fit_transform(messages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# look at the dataframe\npd.DataFrame(model.toarray(), columns = vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's have a look on the features we got using the CountVectorizer\nprint(vect.get_feature_names())  # these features are the bag of words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that although we have done some preprocessing but some issue still exists like reduntant features which are not adding any new meaning , 00 & 000 , 'important'& 'importantly' -> such features can be represented using one feature thus further processing almost everytime requires after creating bag of words model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}