{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Insurance Forecast by using Linear Regression - Ganesh Nagappa Shetty"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nTo make profit, insurance companies should collect higher premium than the amount paid to the insured person. Due to this, insurance companies invests a lot of time, effort, and money in creating models that accurately predicts health care costs. In this kernel, I will try to build the most accurate model as possible but at the same time I would keep everything simple."},{"metadata":{},"cell_type":"markdown","source":"## 1. Reading and Understanding the Data\n\nLet us first import necessary libraries, dataset and try to understand the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Import all important libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the dataset and check initial entries of the dataset\ndf=pd.read_csv('/kaggle/input/insurance/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of the dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information Summary of the dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for Null Values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** There no missing values in the dataset. Lets check for outliers in the dataset"},{"metadata":{},"cell_type":"markdown","source":"## 2. Cleaning the data"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Checking for Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n\nfig.suptitle('Outlier Analysis')\nsns.boxplot(ax=axes[0, 0], data=df['age'])\naxes[0, 0].set_title('Age')\nsns.boxplot(ax=axes[0, 1], data=df['bmi'])\naxes[0, 1].set_title('BMI')\nsns.boxplot(ax=axes[1, 0], data=df['children'])\naxes[1, 0].set_title('Children')\nsns.boxplot(ax=axes[1, 1], data=df['charges'])\naxes[1, 1].set_title('Charges')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** There are no outliers in the numerical variables of the dataset. The datapoints beyond 75th percentile in Charges and BMI are continuous in nature. This is quite clean dataset without outliers."},{"metadata":{},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.title('Effect of Sex on Charges')\nsns.boxplot(x='sex',y='charges',data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.title('Effect of Age on Charges')\nsns.scatterplot(x='age',y='charges',data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** Males are spending more than females for healthcare. Another obvious observation is healthcare expenditure is continuously increasing with age"},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Multivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.title('Effect of Smokers on Charges')\nsns.boxplot(x='smoker',y='charges',data=df,hue='sex',palette='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** \n- Smokers are spending more in hospital. \n- Majority of spendings by non-smokers between males and females are in the similar rages. Females spend fractionally higher\n- However among smokers males endup spending more in hospital"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.title('Effect of Regions and sex on Charges')\nsns.boxplot(x='region',y='charges',data=df,hue='sex',palette='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.title('Effect of Regions and smokers on Charges')\nsns.boxplot(x='region',y='charges',data=df,hue='smoker',palette='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** \n- People in Southeast spend more on healthcare compared to other regions\n- Irrespective of regions generally its males who are spending more in hospitals\n- Again irrespective of regions its smokers who spend heavily in hospitals. Here as well Southeast region has upper hand over other regions"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Lets check the correlation of different variables\nsns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap of variables to check correlation between variables\ncorrMatt = df.corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsns.heatmap(corrMatt, mask=mask,cmap='viridis', square=True,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** We can observe some kind of linear relationship between `age` and `charges`"},{"metadata":{},"cell_type":"markdown","source":"## 4. Preparation of dataset for Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First few lines of data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Handling Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets convert Sex and Smoker as binary categorical variables(Male: 1, Female: 0  & Smoker_yes: 1 , Smoker_no: 0)\ndf.sex=df.sex.apply(lambda x: 1 if x=='male' else 0)\ndf.smoker=df.smoker.apply(lambda x: 1 if x=='yes' else 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets convert region as dummy variables\nregion = pd.get_dummies(df['region'], drop_first = True,prefix='region')\ndf = pd.concat([df, region], axis = 1)\n\n#Dropping season variable\ndf.drop('region',axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Splitting the Data into Training and Testing Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets split the data into Training and testing sets (70%-30% combination)\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Rescaling the Features"},{"metadata":{},"cell_type":"markdown","source":"We see that charges, age and BMI variables are in larger scale compared to other. Lets scale Training data sets using minmax scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['charges', 'age', 'bmi']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Dividing into X and Y sets for the model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('charges')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Building the Model - Considering all variables"},{"metadata":{},"cell_type":"markdown","source":"Lets build the model with all variables first and then compare the performance with the model with eliminated features"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Fitting Linear regression model onto Train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Fitting LinearRegression onto the train data\nlm = LinearRegression()\nlm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Making Predictions Using the  Model\n#### 5.2.1 Applying the scaling on the test sets. Only transforming not fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['charges', 'age', 'bmi']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2.2 Dividing into X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('charges')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2.3 Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions using the third model\ny_pred = lm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\n# Plot heading \nfig.suptitle('y_test vs y_pred', fontsize = 20) \n# X-label\nplt.xlabel('y_test', fontsize = 18) \n# y-label\nplt.ylabel('y_pred', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** We now have a model with r2_score of **77.7%** which is not bad. Lets now try to check the possibility of building mode efficient model(devoid of insignificant variables and multicolinearity)"},{"metadata":{},"cell_type":"markdown","source":"## 6. Building the Model - Considering significant variables and avoiding Multicolinearity if any"},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Splitting the Data into Training and Testing Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets split the data into Training and testing sets (70%-30% combination)\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Rescaling the Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['charges', 'age', 'bmi']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.3  Dividing into X and Y sets for the model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('charges')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.4 Fitting Linear regression model onto Train data"},{"metadata":{},"cell_type":"markdown","source":"Lets use stats model library for its great statistical output"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets have a backup of X_train data\nX_train_bc=X_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** As we can see `sex` with p-value of 1.00 (much higher than 0.03) is highly insignificant. Lets drop this variable and rebuild the model again"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping sex variable\nX_train_bc = X_train_bc.drop(['sex'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rebuilding second model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm2 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** We can see that after dropping `sex` we have no alarming multicolinearity in the model(all VIFs are less than 5). We can also see that Adj. R-squared has not changed. We still have `region_northwest` & `region_southeast` as insignificant (p-value more than 0.03). We shall drop `region_northwest` and rebuild the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping region_northwest variable\nX_train_bc = X_train_bc.drop(['region_northwest'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rebuilding third model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm3 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm3.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** We can see that after dropping `region_northwest` we have no alarming multicolinearity in the model(all VIFs are less than 5). We can also see that Adj. R-squared has not changed. We still have `region_southeast` as insignificant (p-value more than 0.03). We shall drop `region_southeast` and rebuild the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping region_southeast variable\nX_train_bc = X_train_bc.drop(['region_southeast'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rebuilding fourth model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm4 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm4.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** We can see that after dropping `region_southeast` we have no alarming multicolinearity in the model(all VIFs are less than 5). We can also see that Adj. R-squared has not changed. We still have `region_southwest` as insignificant (p-value more than 0.03). We shall drop `region_southwest` and rebuild the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping region_southwest variable\nX_train_bc = X_train_bc.drop(['region_southwest'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rebuilding fifth model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm5 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm5.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:** After leaving out `region_southwest` variable, we now have a model which is free of multicolinearity (all VIFs are less than 5) and all the remaining variables are significant (p-values are less than 0.03). We can also observe that Adj. R-squared has only fractionally come down(by 0.001). Now we have a efficient model. Lets use this as final model and predict the charges for test data."},{"metadata":{},"cell_type":"markdown","source":"### 6.5 Making Predictions Using the Final Model"},{"metadata":{},"cell_type":"markdown","source":"#### 6.5.1 Applying the scaling on the test sets. Only transforming not fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['charges', 'age', 'bmi']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])\n\ndf_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.5.2 Dividing into X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('charges')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.5.3 Preparing the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding constant variable to test dataframe\nimport statsmodels.api as sm \nX_test_m5 = sm.add_constant(X_test)\n\n# Creating X_test_m5 dataframe by dropping variables from X_test_m5\nX_test_m5 = X_test_m5.drop(['sex', 'region_northwest', 'region_southeast', 'region_southwest'], axis = 1)\n\n# Making predictions using the fifth model\ny_pred = lm5.predict(X_test_m5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.6 Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\n# Plot heading \nfig.suptitle('y_test vs y_pred', fontsize = 20) \n# X-label\nplt.xlabel('y_test', fontsize = 18) \n# y-label\nplt.ylabel('y_pred', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model parameters\nround(lm5.params,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** The model now has fewer variables (4 insignificant variables are left out). We now have a R2_squared value of **78%**. \n\nThis model has a R2_square value marginally better than previous model (With all variables). Now the model has no insignificant variables hence making it cost and time effective.\n\nThe final equation of the model is\n\n$ charges = -0.043 + 0.191  \\times  age + 0.165  \\times  bmi + 0.007 \\times children + 0.383 \\times smoker $ \n\n**`Smoker`** turned out to be the most significant variable in deciding hospital charges"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}