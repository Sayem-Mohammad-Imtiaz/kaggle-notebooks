{"cells":[{"metadata":{"_cell_guid":"cbd587d7-567a-41e7-9787-3e0213aadff9","_uuid":"1401330d8701ddc19d933357b51d63c215fbb37c"},"cell_type":"markdown","source":"This kernel was inspired by [NIPS papers visualized with NMF and t-SNE\n](https://www.kaggle.com/dschniertshauer/nips-papers-visualized-with-nmf-and-t-sne) (Lurchi)."},{"metadata":{"_cell_guid":"3f6672fe-a98e-4ab8-8569-2ffa42f205f7","_uuid":"d70f0f19e5881195ded76767e39e72f50ce3af38","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# LDA, tSNE\nfrom sklearn.manifold import TSNE\nfrom gensim.models.ldamodel import LdaModel\n# NLTK\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nimport re\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns\n# Bokeh\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f1d92b9-af21-47ba-a756-e70bf7ca984c","_uuid":"4f55c83e1638be7971359264469ba7185d1e5656"},"cell_type":"markdown","source":"## Loading data\nLet's load the dataset with papers and glimpse some first rows of a paper."},{"metadata":{"_cell_guid":"eda986a3-0134-4d61-881a-3573182b7843","_uuid":"eec14a4ce07160a4e1de9f61a542662e24652f5d","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/papers.csv\")\nprint(df.paper_text[0][:500])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e68a7238-1a5f-4f6e-828e-d80fe9892ce8","_uuid":"addfb8559a9354d5e2797a6efd947b49e9fb9694"},"cell_type":"markdown","source":"## Processing\nHere we'll process our corpus using some standard technics ..."},{"metadata":{"_cell_guid":"c70555aa-8e61-4834-9da6-d84bba5ee9a1","_uuid":"cf4243b89e203ef688bca66acae03e49c394aed2"},"cell_type":"markdown","source":"### Initial cleaning\nJust removing numbers and reducing all words to the lowercase. Let also see what we'll get:"},{"metadata":{"_cell_guid":"d7089734-97df-46c1-9761-93e07aff91a2","_uuid":"eabf59cfe5f4b051574fa71a6044bb84b280da05","trusted":true},"cell_type":"code","source":"%%time\n# Removing numerals:\ndf['paper_text_tokens'] = df.paper_text.map(lambda x: re.sub(r'\\d+', '', x))\n# Lower case:\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: x.lower())\nprint(df['paper_text_tokens'][0][:500])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59851aa9-4871-4ead-85e1-9ec490070357","_uuid":"83dad662040fee212402879e0d9fd1c809ed82f5"},"cell_type":"markdown","source":"### Tokenize\nSpliting texts into separete words, also removing punctuanions and other stuff. After that procedure we should obtain texts as lists of words in lowercase:"},{"metadata":{"_cell_guid":"d179b209-346e-4115-ab19-f1a82f291440","_uuid":"fadb1151dc68bcbaf674b10d13506e30d538fd77","scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: RegexpTokenizer(r'\\w+').tokenize(x))\nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"21b77826-361f-4752-80ef-e4538c4b4016","_uuid":"ddd4528233edc88588624b893d335366074db83f"},"cell_type":"markdown","source":"### Stemming\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form ... The stem need not be identical to the morphological root of the word (see  [[Wikipedia]](https://en.wikipedia.org/wiki/Stemming) for more details). We'll use `SnowballStemmer` from `nltk` package."},{"metadata":{"_cell_guid":"db774646-833a-4c22-8b3a-7e204fce0b79","_uuid":"a27597f04427eea99a861cbcbec1c6e854ddca01","trusted":true},"cell_type":"code","source":"%%time\nsnowball = SnowballStemmer(\"english\")  \ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: [snowball.stem(token) for token in x])\nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2fd438f7-f8c1-4250-a63b-747db42d9a49","_uuid":"f01314e437a838fd4cbf20b81cfe89bb6b93bb85"},"cell_type":"markdown","source":"### Stop words\n Removing common English words like  `and`, `the`, `of` and so on."},{"metadata":{"_cell_guid":"bf3b85ad-8c2a-4786-9c25-eeb3c299e257","_uuid":"29982c301e061c837df7138ec561844a393aa382","trusted":true},"cell_type":"code","source":"%%time\nstop_en = stopwords.words('english')\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: [t for t in x if t not in stop_en]) \nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c590d3f1-c930-4554-9db8-f644292da77d","_uuid":"e7746cdf1611036902e3d7d0422d2c36355e6ae9"},"cell_type":"markdown","source":"### Final cleaning\nHere we'll remove all \"extremely short\" words (that have less than 2 characters):"},{"metadata":{"_cell_guid":"fd8efbac-1c24-42b3-85d6-eff646daf7ed","_uuid":"e957e276e204260504f56973c3418c9d61630e21","trusted":true},"cell_type":"code","source":"%%time\ndf['paper_text_tokens'] = df.paper_text_tokens.map(lambda x: [t for t in x if len(t) > 1])\nprint(df['paper_text_tokens'][0][:25])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7daa2f1-cc2c-42d5-9a2e-84d66ab16a08","_uuid":"748485c63116b471a7334c32e6623c55ca89bc1d","collapsed":true},"cell_type":"markdown","source":"## LDA\nFinally, let's use LDA ([Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)) to extract topic structure from the corpus of texts."},{"metadata":{"_cell_guid":"43974244-e1c6-4bd1-adfd-20f24adfc141","_uuid":"6a7693efaf4cd6ffd355c0dae5a61827641a4c36","trusted":true},"cell_type":"code","source":"from gensim import corpora, models\nnp.random.seed(2017)\ntexts = df['paper_text_tokens'].values\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\nldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, \n                                    num_topics=8, passes=5, minimum_probability=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0549bf09-2256-42fe-9e28-602cd6206790","_uuid":"bd304c6d5db35917fc45c578e8f8d7fd4086eb4e"},"cell_type":"markdown","source":"Refactoring results of LDA into numpy matrix (`number_of_papers` x `number_of_topics`)."},{"metadata":{"_cell_guid":"bd31b9d1-4fa8-4f0c-ab2a-58c0103f5647","_uuid":"bc1c2d4decad1e1366cc798160739e07b02d0e31","trusted":true},"cell_type":"code","source":"hm = np.array([[y for (x,y) in ldamodel[corpus[i]]] for i in range(len(corpus))])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6adc9d41-e65e-41ff-bb8d-b6e880c54fde","_uuid":"ae589e1203f467b2768f702d7c9a185e1782d730"},"cell_type":"markdown","source":"And reduce dimensionality using t-SNE algorithm:"},{"metadata":{"_cell_guid":"681d18a4-5d6e-4e09-aea0-ca99149129d0","_uuid":"22aac87fec4d7fa651d8920fe6bb4cb3d8b86eba","trusted":true},"cell_type":"code","source":"tsne = TSNE(random_state=2017, perplexity=30, early_exaggeration=120)\nembedding = tsne.fit_transform(hm)\nembedding = pd.DataFrame(embedding, columns=['x','y'])\nembedding['hue'] = hm.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"811e2566-9fb6-41cc-aa2e-79151fa2fd76","_uuid":"1fa143e6622087f04174e10af61a56a6a11d427a","_kg_hide-input":false},"cell_type":"markdown","source":"## Ploting\nUsing Bokeh for scatter plot with interactions. Hover mouse over a dot to see the title of the respective paper:"},{"metadata":{"_cell_guid":"97fa8b2e-7cda-4146-9c1a-8207c71f18f2","_uuid":"fec8f21fc2db8037e5724c71c5c23b66b49b02c8","scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"source = ColumnDataSource(\n        data=dict(\n            x = embedding.x,\n            y = embedding.y,\n            colors = [all_palettes['Set1'][8][i] for i in embedding.hue],\n            title = df.title,\n            year = df.year,\n            alpha = [0.9] * embedding.shape[0],\n            size = [7] * embedding.shape[0]\n        )\n    )\nhover_tsne = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n            <span style=\"font-size: 12px\">@title</span>\n            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n            <span style=\"font-size: 12px\">@year</span>\n        </div>\n    </div>\n    \"\"\")\ntools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\nplot_tsne = figure(plot_width=700, plot_height=700, tools=tools_tsne, title='Papers')\nplot_tsne.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\")\n\ncallback = CustomJS(args=dict(source=source), code=\n    \"\"\"\n    var data = source.data;\n    var f = cb_obj.value\n    x = data['x']\n    y = data['y']\n    colors = data['colors']\n    alpha = data['alpha']\n    title = data['title']\n    year = data['year']\n    size = data['size']\n    for (i = 0; i < x.length; i++) {\n        if (year[i] <= f) {\n            alpha[i] = 0.9\n            size[i] = 7\n        } else {\n            alpha[i] = 0.05\n            size[i] = 4\n        }\n    }\n    source.change.emit();\n    \"\"\")\n\nslider = Slider(start=df.year.min(), end=df.year.max(), value=2016, step=1, title=\"Before year\")\nslider.js_on_change('value', callback)\n\nlayout = column(slider, plot_tsne)\nshow(layout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1}