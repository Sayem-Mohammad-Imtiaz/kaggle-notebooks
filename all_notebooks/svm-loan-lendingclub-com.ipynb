{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVN - Excersice\n\nBased on the Udemy course \"Data Science and ML Bootcamp\", by Jose Portilla.\n\nFor this project we will be exploring publicly available data from [LendingClub.com](www.lendingclub.com). Lending Club connects people who need money (borrowers) with people who have money (investors). Hopefully, as an investor you would want to invest in people who showed a profile of having a high probability of paying you back. We will try to create a model that will help predict this.\n\nLending club had a [very interesting year in 2016](https://en.wikipedia.org/wiki/Lending_Club#2016), so let's check out some of their data and keep the context in mind. This data is from before they even went public.\n\nWe will use lending data from 2007-2010 and be trying to classify and predict whether or not the borrower paid back their loan in full. You can download the data from [here](https://www.lendingclub.com/info/download-data.action) or just use the csv already provided. It's recommended you use the csv provided as it has been cleaned of NA values.\n\nHere are what the columns represent:\n* credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n* purpose: The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\").\n* int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n* installment: The monthly installments owed by the borrower if the loan is funded.\n* log.annual.inc: The natural log of the self-reported annual income of the borrower.\n* dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n* fico: The FICO credit score of the borrower.\n* days.with.cr.line: The number of days the borrower has had a credit line.\n* revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n* revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n* inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.\n* delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n* pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments)."},{"metadata":{"trusted":true},"cell_type":"code","source":"loans = pd.read_csv('../input/loan_borowwer_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nLet's do some data visualization! We'll use seaborn and pandas built-in plotting capabilities, but feel free to use whatever library you want. Don't worry about the colors matching, just worry about getting the main idea of the plot.\n\n** Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nloans[loans['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='Credit.Policy=1')\nloans[loans['credit.policy']==0]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='Credit.Policy=0')\nplt.legend()\nplt.xlabel('FICO')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Create a similar figure, except this time select by the not.fully.paid column.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nloans[loans['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='not.fully.paid=1')\nloans[loans['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='not.fully.paid=0')\nplt.legend()\nplt.xlabel('FICO')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,7))\nsns.countplot(x='purpose',hue='not.fully.paid',data=loans,palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Let's see the trend between FICO score and interest rate. Recreate the following jointplot.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='fico',y='int.rate',data=loans,color='purple')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,7))\nsns.lmplot(y='int.rate',x='fico',data=loans,hue='credit.policy',\n           col='not.fully.paid',palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features\n\nNotice that the **purpose** column as categorical\n\nThat means we need to transform them using dummy variables so sklearn will be able to understand them. Let's do this in one clean step using pd.get_dummies.\n\nLet's show you a way of dealing with these columns that can be expanded to multiple categorical features if necessary.\n\n**Create a list of 1 element containing the string 'purpose'. Call this list cat_feats.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = ['purpose']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now use pd.get_dummies(loans,columns=cat_feats,drop_first=True) to create a fixed larger dataframe that has new feature columns with dummy variables. Set this dataframe as final_data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data = pd.get_dummies(loans,columns=cat_feats,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(final_data.drop('not.fully.paid',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_features = scaler.transform(final_data.drop('not.fully.paid',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data_scaled = pd.DataFrame(scaled_features,columns=final_data.columns[:-1])\nfinal_data_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split\n\nNow its time to split our data into a training set and a testing set!\n\n** Use sklearn to split your data into a training set and a testing set as we've done in the past.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_data_scaled.drop('not.fully.paid',axis=1)\ny = final_data['not.fully.paid']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training a Decision SVM model\n\nLet's start by training a single decision tree first!\n\n** Import SVM Classifier**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(C=1,gamma=0.1)\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions and Evaluation of SVM\n**Create predictions from the test set and create a classification report and a confusion matrix.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gridsearch\n\nFinding the right parameters (like what C or gamma values to use) is a tricky task! But luckily, we can be a little lazy and just try a bunch of combinations and see what works best! This idea of creating a 'grid' of parameters and just trying out all the possible combinations is called a Gridsearch, this method is common enough that Scikit-learn has this functionality built in with GridSearchCV! The CV stands for cross-validation which is the\n\nGridSearchCV takes a dictionary that describes the parameters that should be tried and a model to train. The grid of parameters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.1,1, 10, 100], 'gamma': [10,1,0.1,0.01], 'kernel': ['rbf']} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# May take awhile!\ngrid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_predictions = grid.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,grid_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,grid_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}