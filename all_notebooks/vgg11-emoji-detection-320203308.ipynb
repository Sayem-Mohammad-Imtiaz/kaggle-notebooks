{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAMPLE_DATASET = \"../input/icml-face-data/icml_face_data.csv\"\n#print(SAMPLE_DATASET)\nNUM_CLASSES = 7\nTRAIN_HDF5 = \"./train.hdf5\"\nVAL_HDF5 = \"./val.hdf5\"\nTEST_HDF5 = \"./test.hdf5\"\nMODEL_FILE = \"./model.h5\"\nOUTPUT_PATH = \"./\"\nBATCH_SIZE = 128\n\nprint(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入必要的包\nimport os\n\nfrom tensorflow.keras.callbacks import Callback\n\n\n# 模型存盘检查点，每训练5趟保存一次模型\nclass EpochCheckpoint(Callback):\n    def __init__(self, output_path, every=5, start_at=0):\n        # 调用父类的构造函数\n        super(Callback, self).__init__()\n        self.output_path = output_path  # 模型保存目录\n        # 间隔趟数\n        self.every = every\n        # 起始趟数（当前趟数）\n        self.start_epoch = start_at\n\n    def on_epoch_end(self, epoch, logs={}):\n        # 检查是否要向磁盘保存模型\n        if (self.start_epoch + 1) % self.every == 0:\n            p = os.path.sep.join([self.output_path,\n                                  \"epoch_{}.hdf5\".format(self.start_epoch + 1)])\n            self.model.save(p, overwrite=True)\n        # 增加内部的趟数计数器\n        self.start_epoch += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import img_to_array\n\n# 维度重置预处理器\n# 图片样本维度重置预处理类\n\n# 定义ImageToArrayPreprocessor类\nclass ImageToArrayPreprocessor:\n    def __init__(self, data_format=None):\n        # 保存图像数据的格式\n        self.data_format = data_format\n\n    def preprocess(self, image):\n        \"\"\"\n        重置图像image的维度\n        :param image: 要预处理的图像\n        :return: 维度重置后的图像\n        \"\"\"\n\n        # 必调用tensorflow.keras的img_to_array方法正确重置图像的维度\n        return img_to_array(image, data_format=self.data_format)\n\n\n# if __name__ == '__main__':\n#     import cv2\n#     itap = ImageToArrayPreprocessor(\"channels_first\")\n#     # itap = ImageToArrayPreprocessor(\"channels_last\")  # 默认channels_last\n#     print(cv2.imread(\"../demo/1.jpg\").shape)\n#     print(itap.preprocess(cv2.imread(\"../demo/1.jpg\")).shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport h5py\n\n\n# HDF5数据集生成器\n\n# 定义HDF5DatasetWriter类\nclass HDF5DatasetWriter:\n\n    # 如果我们要保存扁平化的28×28 = 784 MNIST数据集原始像素数据，则dims = (70000，784)，\n    # 因为NNIST数据集共有70000个样本，每个样本的维度是784。\n    # 如果我们要存储原始CIFAR-10图像，则dims = (60000，32，32，3)\n    # 因为CIFAR-10数据集共有60000图像，每个样本表示为一个 32×32×3 RGB图像。\n\n    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n        \"\"\"\n\n        :param dims: 用来控制将要保存到数据集中的数据维度，类似于numpy数组的shape。\n        :param output_path: 生成的HDF5文件存储在磁盘上的路径\n        :param data_key: 数据集的名称，默认值为\"images\"\n        :param buf_size: 缓存大小，默认值1000\n        :return:\n        \"\"\"\n        # 相当于numpy的shape\n        # 检查输出路径是否存在，如果存在则抛出异常\n        if os.path.exists(output_path):\n            raise ValueError(\"您提供的输出文件{}已经存在，请先手工删除！\".format(output_path))\n        # 创建并打开可写入HDF5文件\n        # 然后在其中创建两个数据集\n        # dataset : 类似数组组织的数据的集合，像 numpy 数组一样工作\n        self.db = h5py.File(output_path, \"w\")  # 读取文件\n        self.data = self.db.create_dataset(data_key, dims, dtype=\"float\")  # 一个用于存储图像/特征，\n        self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")  # 另一个用于存储分类标签\n\n        # 保存缓存大小，然后初始化存缓和数据集索引\n        self.buf_size = buf_size  # 1000\n        self.buffer = {\"data\": [], \"labels\": []}\n        self.idx = 0\n\n    def add(self, raw, label):\n        \"\"\"\n        将数据和标签添加到缓存\n        :param raw: 图像\n        :param label: 对应的标签\n        :return:\n        \"\"\"\n        self.buffer[\"data\"].extend(raw)\n        self.buffer[\"labels\"].extend(label)\n        if len(self.buffer[\"data\"]) >= self.buf_size:  # 缓存小桶盛满了，放入水池\n            self.flush()\n\n    def flush(self):\n        # 将缓存内容写入磁盘文件，然后清空缓存\n        # 块状文件，顺序读写\n        i = self.idx + len(self.buffer[\"data\"])\n        self.data[self.idx:i] = self.buffer[\"data\"]\n        self.labels[self.idx:i] = self.buffer[\"labels\"]\n        self.idx = i\n        self.buffer = {\"data\": [], \"labels\": []}  # 清空缓存\n\n    def store_class_labels(self, class_labels):\n        # dataset 是类 numpy array 所以，你能写进的数据 只能 是数组\n        # 创建一个数据集用来存储分类标签名称，然后保存分类标签\n        dt = h5py.special_dtype(vlen=str)\n        label_dim = (len(class_labels),)\n        label_set = self.db.create_dataset(\"label_names\", label_dim, dtype=dt)\n        label_set[:] = class_labels\n\n    def close(self):\n        # 检查缓存中是否有记录，如果有，则必须写入磁盘文件\n        if len(self.buffer[\"data\"]) > 0:\n            self.flush()\n        # 关闭数据集\n        self.db.close()\n\n\n# if __name__ == '__main__':\n#     from config import setting\n#\n#     dfw = HDF5DatasetWriter((32, 32, 3), setting.OUTPUT_PATH)\n#     filepath = \"../demo/1.jpg\"\n#     image = cv2.imread(filepath)\n#     dfw.add(dfw, \"cat\")\n#     dfw.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport numpy as np\n# from tensorflow.keras.utils.np_utils import to_categorical\nfrom tensorflow.python.keras.utils import np_utils\n\n\n# 生成HDF5文件\n\nclass HDF5DatasetGenerator:\n    def __init__(self, db_file, batch_size, preprocessors=None, aug=None, binarize=True, classes=2):\n        \"\"\"\n        :param db_file: 数据集文件\n        :param batch_size: 样本数量\n        :param preprocessors: 预处理器列表\n        :param aug: 数据增强处理器列表\n        :param binarize: 标签是否二值化\n        :param classes: 分类数，这里为二，我们只有猫狗两个分类\n        \"\"\"\n        # 每批次样本数量\n        self.batchSize = batch_size\n\n        # 预处理器列表\n        self.preprocessors = preprocessors\n\n        # 数据增强处理器列表，可以使用 Keras ImageDat\n # 数据增强处理器列表，可以使用 Keras ImageDataGenerator实现的数据增强算法\n        self.aug = aug\n\n        # 标签是否二值化，我们在HDF5数据集中保存的类别标签为单个整型的列表，\n        # 然而，当我们使用分类交叉熵(categorical cross-entropy)或二值交叉熵(binary cross-entropy)\n        # 作为计算损失的方法： 我们必须将标签二值化为独热编码向量组(one-hot encoded vectors)\n        self.binarize = binarize\n\n        # 不重复的类别数量，在计算标签二值化独热编码向量组时需要该值\n        self.classes = classes\n\n        # 打开HDF5数据集文件\n        self.db = h5py.File(db_file, mode='r')\n\n        # 数据集样本数量\n        self.numImages = self.db[\"labels\"].shape[0]\n\n    def generator(self, passes=np.inf):\n        # hdf5中的数据分批读取到内存中\n        # 初始化训练趟数计数器\n        epochs = 0\n\n        # 执行无限循环，一旦达到规定的训练趟数,模型会自动停止训练\n        while epochs < passes:\n            # 遍历HDF5数据集\n            for i in np.arange(0, self.numImages, self.batchSize):\n\n                # 从HDF5数据集取出一批样本和标签\n                images = self.db[\"images\"][i:i + self.batchSize]\n                labels = self.db[\"labels\"][i:i + self.batchSize]\n\n                # 检查标签是否有转化为独热编码向量组\n                if self.binarize:\n                    labels = np_utils.to_categorical(labels, self.classes)\n\n                # 如果有预处理器\n                if self.preprocessors is not None:\n                    # 初始化预处理结果图像列表\n                    processed_images = []\n\n                    # 遍历图像\n                    for image in images:\n                        # 遍历预处理器，对每个图像执行全部预处理\n                        for p in self.preprocessors:\n                            image = p.preprocess(image)\n\n                        # 更新预处理结果图像列表\n                        processed_images.append(image)\n\n                    # 将图像数组更新为预处理结果图像\n                    images = np.array(processed_images)\n\n            # 如果指定了数据增强器，则实施之\n            if self.aug is not None:\n                # next() 函数要和生成迭代器的 iter() 函数一起使用。\n                (images, labels) = next(self.aug.flow(images, labels, batch_size=self.batchSize))\n\n            # 生成图像和标记元组\n            yield images, labels\n\n        # 增加训练趟数计数器\n        epochs += 1\n\n    def close(self):\n        # 关闭HDF5数据集\n        self.db.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport numpy as np\nfrom keras.callbacks import BaseLogger\nimport matplotlib.pyplot as plt\n\n\n# 定义TrainingMonitor类\nclass TrainingMonitor(BaseLogger):\n    def __init__(self, fig_path, json_path=None, start_at=0):\n        super(TrainingMonitor, self).__init__()\n        self.fig_path = fig_path  # 绘图文件保存路径\n        self.json_path = json_path  # Json文件保存路径\n        self.start_at = start_at  # 开始的趟数\n\n    def on_train_begin(self, logs={}):\n        self.history = {}  # 训练日志历史字典\n\n        # 若json日志文件存在，则加载训练日志字典\n        if self.json_path is not None:\n            if os.path.exists(self.json_path):\n                self.history = json.loads(open(self.json_path).read())  # append 4 value after each epoch\n                # 如果指定了训练趟数起点\n                if self.start_at > 0:\n                    # 遍历训练日志字典，解掉起点趟数之后的日志\n                    for k in self.history.keys():\n                        self.history[k] = self.history[k][:self.start_at]\n\n    def on_epoch_end(self, epoch, logs={}):\n\n        # 针对整个训练过程，遍历日志，更新训练损失、训练准确度等\n        for (k, v) in logs.items():\n            log = self.history.get(k, [])\n            log.append(float(v))\n            self.history[k] = log\n\n        if self.json_path is not None:\n            f = open(self.json_path, 'w')\n            f.write(json.dumps(self.history, skipkeys=True))  # 序列化为json文件\n\n            f.close()\n\n        # 训练两趟后开始绘图\n        if len(self.history[\"loss\"]) > 1:\n            # 绘图训练损失和准确度趋势图\n            N = np.arange(0, len(self.history[\"loss\"]))\n            plt.style.use(\"ggplot\")\n            plt.figure()\n            plt.plot(N, self.history[\"loss\"], label=\"train_loss\")\n            plt.plot(N, self.history[\"val_loss\"], label=\"val_loss\")\n            # plt.plot(N,self.history[\"acc\"],label=\"train_acc\")  # GPU version\n            plt.plot(N, self.history[\"accuracy\"], label=\"train_acc\")  # CPU version\n            # plt.plot(N,self.history[\"val_acc\"],label=\"val_acc\") # GPU version\n            plt.plot(N, self.history[\"val_accuracy\"], label=\"val_acc\")  # CPU version\n            epochs = len(self.history[\"loss\"])\n            plt.title(\"Training Loss & Accuracy [Epoch {}]\".format(epochs))\n            plt.xlabel(\"Epoch #\")\n            plt.ylabel(\"Loss/Accuracy\")\n            plt.legend()\n            plt.savefig(self.fig_path)\n            plt.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import HDF5DatasetWriter\n\n\nprint(\"[信息] 加载csv格式数据集文件\")\n\nfile = open(SAMPLE_DATASET)\nfile.__next__()#跳过第一行\n(train_images, train_label) = ([], [])\n(val_images, val_label) = ([], [])\n(test_images, test_label) = ([], [])\ncount_by_label_train = {}\ncount_by_label_val = {}\ncount_by_label_test = {}\nfor row in file:\n    (label, usage, image) = row.strip().split(\",\")\n    label = int(label)\n    image = np.array(image.split(\" \"), dtype=\"uint8\")\n    image = image.reshape((48, 48))\n\n    if usage == \"Training\":\n        train_images.append(image)\n        train_label.append(label)\n        count = count_by_label_train.get(label, 0)\n        count_by_label_train[label] = count + 1\n\n    elif usage == \"PublicTest\":\n        val_images.append(image)\n        val_label.append(label)\n        count = count_by_label_val.get(label, 0)\n        count_by_label_val[label] = count + 1\n\n    elif usage == \"PrivateTest\":\n        test_images.append(image)\n        test_label.append(label)\n        count = count_by_label_test.get(label, 0)\n        count_by_label_test[label] = count + 1\n\nfile.close()\nprint(\"[信息] 训练集样本数量：{}\".format(len(train_images)))\nprint(\"[信息] 校验集样本数量：{}\".format(len(val_images)))\nprint(\"[信息] 测试集样本数量：{}\".format(len(test_images)))\n#训练集样本分布\nprint(count_by_label_train)\n#校正集样本分布\nprint(count_by_label_val)\n#测试集样本分布\nprint(count_by_label_test)\n\ndatasets = [(train_images,train_label,TRAIN_HDF5),\n            (val_images,val_label,VAL_HDF5),\n            (test_images,test_label,TEST_HDF5)]\n\nfor (images,labels,outputPath) in datasets:\n    print(\"[信息]构建{}...\".format(outputPath))\n    writer = HDF5DatasetWriter((len(images),48,48),outputPath)\n\n    for (image,label) in zip(images,labels):\n        writer.add([image],[label])\n\n    writer.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.regularizers import l1,l2\n\n\nclass VGG11:\n    @staticmethod\n    def build(width, height, channel, classes, reg=0.0002):\n        \"\"\"\n\n        :param width: 输入样本的宽度\n        :param height: 输入样本的高度\n        :param channel: 输入样本的通道\n        :param classes:分类数量\n        :param reg:正则化因子\n        :return:VGG网络模型\n        \"\"\"\n\n        model = Sequential(name=\"VGG11\")\n        # 缺省输入格式为通道后首 (\"channels-last\")\n        shape = (height, width, channel)\n\n        channel_dimension = -1\n        # 如果输入格式为通道前罱\n        # 重新设首输入格式和通道位首指示\n        if backend.image_data_format() == \"channels_first\":\n            shape = (channel, height, width)\n            channel_dimension = 1\n\n        # 第一卷积块\n        model.add(Conv2D(64, (3, 3), input_shape=shape, padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        # 第二卷积块\n        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        # 第三卷积块\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        # 第四卷积块\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        # 第五卷积块\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n\n        # 第一全连接层\n        model.add(Flatten())\n        model.add(Dense(256, kernel_regularizer=l2(reg)))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n\n        # 第二全连接层\n        model.add(Dense(128, kernel_regularizer=l2(reg)))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n\n        # 第三全连接层\n        model.add(Dense(classes, kernel_regularizer=l1(reg)))\n        model.add(Activation(\"softmax\"))\n\n        return model\n\n\n# 测 试 \nif __name__ == '__main__':\n    my_model = VGG11.build(width=48, height=48, channel=1, classes=7, reg=0.0002)\n    print(my_model.summary())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib\n# from config import setting\n# from utils.ImageToArrayPreprocessor import ImageToArrayPreprocessor\n# from utils.TrainingMonitor import TrainingMonitor\n# from utils.HDF5DatasetGenerator import HDF5DatasetGenerator\n# from MiniVGG13 import MiniVGG13Net\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nimport os\nmatplotlib.use(\"Agg\")\n\ntrain_aug = ImageDataGenerator(rotation_range=10,\n                   zoom_range = 0.1,\n                   rescale=1 / 255.0,\n                   fill_mode=\"nearest\")\nval_aug = ImageDataGenerator(rescale=1/255.0)\n\niap = ImageToArrayPreprocessor()\n\ntrain_gen = HDF5DatasetGenerator(TRAIN_HDF5,\n                                 BATCH_SIZE,\n                                 aug=train_aug,\n                                 preprocessors=[iap],\n                                 classes=NUM_CLASSES)\nval_gen = HDF5DatasetGenerator(VAL_HDF5,\n                                 BATCH_SIZE,\n                                 aug=val_aug,\n                                 preprocessors = [iap],\n                                 classes=NUM_CLASSES)\n\nopt = Adam(lr = 1e-3)\nmodel = VGG11.build(width=48,height=48,channel=1,classes=NUM_CLASSES)\nmodel.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\nfig_path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\ncallbacks = [TrainingMonitor(fig_path=fig_path)]\nmodel.fit_generator(train_gen.generator(),\n                    steps_per_epoch=train_gen.numImages//BATCH_SIZE,\n                    validation_data=val_gen.generator(),\n                    validation_steps=val_gen.numImages // BATCH_SIZE,\n                    epochs=50,\n                    max_queue_size=BATCH_SIZE*2,\n                    callbacks=callbacks,\n                    verbose=1)\nprint(\"[信息] 保存模型...\")\nmodel.save(MODEL_FILE,overwrite=True)\ntrain_gen.close()\nval_gen.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 导入必须的包\n# from config import setting\n# from utils.ImageToArrayPreprocessor import  ImageToArrayPreprocessor\n# from utils.HDF5DatasetGenerator import HDF5DatasetGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import load_model\n\n# 初始化图像预处理器\n# 归一化处理\ntestAug = ImageDataGenerator(rescale=1 / 255.0)\niap = ImageToArrayPreprocessor()\n\n# 初始化测试数据集生成器\ntestGen = HDF5DatasetGenerator(TEST_HDF5,\n                               BATCH_SIZE,\n                               aug = testAug,\n                               preprocessors=[iap],\n                               classes=NUM_CLASSES)\n\n# 加载前面训练好的网络\nprint(\"[信息] 加载网络模型...\")\nmodel = load_model(MODEL_FILE)\n\n# 评估网络模型\n(loss, acc) = model.evaluate_generator(testGen.generator(),\n                                       steps=testGen.numImages // BATCH_SIZE,\n                                       max_queue_size=BATCH_SIZE * 2)\nprint(\"[信息] 测试集准确率: {:.2f}%\".format(acc * 100))  # 63.84%\n\n# 关闭HDF5数据集\ntestGen.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}