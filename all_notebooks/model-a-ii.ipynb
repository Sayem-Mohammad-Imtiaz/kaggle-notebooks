{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T08:51:59.677452Z","iopub.execute_input":"2021-08-20T08:51:59.677764Z","iopub.status.idle":"2021-08-20T08:51:59.692239Z","shell.execute_reply.started":"2021-08-20T08:51:59.677737Z","shell.execute_reply":"2021-08-20T08:51:59.691166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nimport numpy as np\nimport re\nimport nltk\nfrom sklearn.datasets import load_files\nnltk.download('stopwords')\nimport pickle\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:51:59.693886Z","iopub.execute_input":"2021-08-20T08:51:59.694278Z","iopub.status.idle":"2021-08-20T08:52:07.115339Z","shell.execute_reply.started":"2021-08-20T08:51:59.694243Z","shell.execute_reply":"2021-08-20T08:52:07.114441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bargain-dataset/Dataset Bargain.csv')\nX, y = df.A, df.Label","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:52:07.116742Z","iopub.execute_input":"2021-08-20T08:52:07.117002Z","iopub.status.idle":"2021-08-20T08:52:07.159065Z","shell.execute_reply.started":"2021-08-20T08:52:07.116977Z","shell.execute_reply":"2021-08-20T08:52:07.15813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents = []\n\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = WordNetLemmatizer()\n\nfor sen in range(0, len(X)):\n    # Remove all the special characters\n    document = re.sub(r'\\W', ' ', str(X[sen]))\n    \n    # remove all single characters\n    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n    \n    # Remove single characters from the start\n    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n    \n    # Substituting multiple spaces with single space\n    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n    \n    # Removing prefixed 'b'\n    document = re.sub(r'^b\\s+', '', document)\n    \n    # Converting to Lowercase\n    document = document.lower()\n    \n    # Lemmatization\n    document = document.split()\n\n    document = [stemmer.lemmatize(word) for word in document]\n    document = ' '.join(document)\n    \n    documents.append(document)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:52:07.16054Z","iopub.execute_input":"2021-08-20T08:52:07.16082Z","iopub.status.idle":"2021-08-20T08:52:09.268355Z","shell.execute_reply.started":"2021-08-20T08:52:07.160792Z","shell.execute_reply":"2021-08-20T08:52:09.267481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\nX = vectorizer.fit_transform(documents).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:52:09.269449Z","iopub.execute_input":"2021-08-20T08:52:09.269701Z","iopub.status.idle":"2021-08-20T08:52:09.395505Z","shell.execute_reply.started":"2021-08-20T08:52:09.269677Z","shell.execute_reply":"2021-08-20T08:52:09.394503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The script above uses CountVectorizer class from the sklearn.feature_extraction.text library. There are some important parameters that are required to be passed to the constructor of the class. The first parameter is the max_features parameter, which is set to 1500. This is because when you convert words to numbers using the bag of words approach, all the unique words in all the documents are converted into features. All the documents can contain tens of thousands of unique words. But the words that have a very low frequency of occurrence are unusually not a good parameter for classifying documents. Therefore we set the max_features parameter to 1500, which means that we want to use 1500 most occurring words as features for training our classifier.\n\nThe next parameter is min_df and it has been set to 5. This corresponds to the minimum number of documents that should contain this feature. So we only include those words that occur in at least 5 documents. Similarly, for the max_df, feature the value is set to 0.7; in which the fraction corresponds to a percentage. Here 0.7 means that we should include only those words that occur in a maximum of 70% of all the documents. Words that occur in almost every document are usually not suitable for classification because they do not provide any unique information about the document.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidfconverter = TfidfTransformer()\nX = tfidfconverter.fit_transform(X).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:52:09.396926Z","iopub.execute_input":"2021-08-20T08:52:09.397247Z","iopub.status.idle":"2021-08-20T08:52:09.650698Z","shell.execute_reply.started":"2021-08-20T08:52:09.397217Z","shell.execute_reply":"2021-08-20T08:52:09.649854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\nX = tfidfconverter.fit_transform(documents).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:52:09.652731Z","iopub.execute_input":"2021-08-20T08:52:09.65302Z","iopub.status.idle":"2021-08-20T08:52:09.816071Z","shell.execute_reply.started":"2021-08-20T08:52:09.652982Z","shell.execute_reply":"2021-08-20T08:52:09.815385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:52:09.81722Z","iopub.execute_input":"2021-08-20T08:52:09.817584Z","iopub.status.idle":"2021-08-20T08:52:09.851831Z","shell.execute_reply.started":"2021-08-20T08:52:09.81755Z","shell.execute_reply":"2021-08-20T08:52:09.851162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=1000, random_state=0)\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:52:09.852807Z","iopub.execute_input":"2021-08-20T08:52:09.853167Z","iopub.status.idle":"2021-08-20T08:53:29.374499Z","shell.execute_reply.started":"2021-08-20T08:52:09.853139Z","shell.execute_reply":"2021-08-20T08:53:29.373546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:53:29.37565Z","iopub.execute_input":"2021-08-20T08:53:29.375923Z","iopub.status.idle":"2021-08-20T08:53:30.321951Z","shell.execute_reply.started":"2021-08-20T08:53:29.375893Z","shell.execute_reply":"2021-08-20T08:53:30.32097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:53:30.323146Z","iopub.execute_input":"2021-08-20T08:53:30.323462Z","iopub.status.idle":"2021-08-20T08:53:30.339923Z","shell.execute_reply.started":"2021-08-20T08:53:30.323433Z","shell.execute_reply":"2021-08-20T08:53:30.338943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nfilename = 'finalized_model.sav'\npickle.dump(classifier, open(filename, 'wb'))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:53:30.341229Z","iopub.execute_input":"2021-08-20T08:53:30.341581Z","iopub.status.idle":"2021-08-20T08:53:30.657706Z","shell.execute_reply.started":"2021-08-20T08:53:30.341542Z","shell.execute_reply":"2021-08-20T08:53:30.656798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nfilename = 'finalized_model.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T08:53:30.658993Z","iopub.execute_input":"2021-08-20T08:53:30.659299Z","iopub.status.idle":"2021-08-20T08:53:30.934542Z","shell.execute_reply.started":"2021-08-20T08:53:30.659269Z","shell.execute_reply":"2021-08-20T08:53:30.933672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}