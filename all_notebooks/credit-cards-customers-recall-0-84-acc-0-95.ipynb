{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading data\ndata    = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\nm, n    = data.shape\ndata.drop(columns=[\n    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n    'CLIENTNUM'], \n          inplace=True)\ncolumns = data.columns.values\n\n# Data doesn't have any NaN\n# data.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data are transformed from categorical to numerical, trying to preserve some sort of ordinality (e.g., card category \"Gold\" is an higher option than \"Silver\", so the map $M : \\text{Categorical} \\rightarrow \\mathbb{N}$ should be such that $M(\\text{Gold}) > M(\\text{Silver})$. Unknows are labelled $0$ for convenience."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming variable (categorical -> numerical)\ndata['Attrition_Flag'].replace({'Existing Customer': 0, \n                                'Attrited Customer': 1}, inplace=True)\ndata['Gender'].replace({'M': 1, \n                        'F': 0}, inplace=True)\ndata['Education_Level'].replace({'Unknown': 0, \n                                 'Uneducated': 1, \n                                 'High School': 2, \n                                 'College': 3, \n                                 'Graduate': 4, \n                                 'Post-Graduate': 5, \n                                 'Doctorate': 6}, inplace=True)\ndata['Marital_Status'].replace({'Unknown': 0,\n                                'Divorced': 1,\n                                'Married': 2,\n                                'Single': 3}, inplace=True)\ndata['Income_Category'].replace({'Unknown': 0,\n                                'Less than $40K': 1,\n                                '$40K - $60K': 2,\n                                '$60K - $80K': 3,\n                                '$80K - $120K': 4,\n                                '$120K +': 5}, inplace=True)\ndata['Card_Category'].replace({'Blue': 1,\n                               'Gold': 2,\n                               'Silver': 3,\n                               'Platinum': 4}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No particulary strong correlations has been found among the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Corrleations  \ncorr = data.corr()\n\nfig, ax = plt.subplots(figsize=(8,8))\nim = ax.imshow(corr)\nax.set_xticks(np.arange(len(columns)))\nax.set_yticks(np.arange(len(columns)))\nax.set_xticklabels(columns)\nax.set_yticklabels(columns)\nax.set_title(\"Correlation matrix\")\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Being the dataset unbalanced with respect to the target feature (\"Attrition_Flag\"), a stratified strategy has been chosen for the splitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\n\n# Taking the train/target dataset\ntarget = data['Attrition_Flag'].values\ntrain  = data.drop(columns=['Attrition_Flag']).values.astype('float')\n\nTEST_SIZE = 0.2\nsss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=232)\nfor train_index, test_index in sss.split(train, target):\n    x_train, x_test = train[train_index], train[test_index]\n    y_train, y_test = target[train_index], target[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data scaling\nfrom sklearn.preprocessing import StandardScaler\n\nscaler  = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test  = scaler.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A bunch of classification models are tested. The business rule puts a focus on finding churners, so the models are tuned using the Recall as cost function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.neighbors       import KNeighborsClassifier\nfrom sklearn.naive_bayes     import BernoulliNB\nfrom sklearn.ensemble        import AdaBoostClassifier\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import make_scorer\n\ndef recall(y, y_pred):\n    cm = confusion_matrix(y, y_pred)\n    return cm[1,1] / (cm[1,0] + cm[1,1])\n\ndef accuracy(y, y_pred):\n    cm = confusion_matrix(y, y_pred)\n    return (cm[0,0] + cm[1,1]) / (cm.sum())\n\nestimators = {'LogisticRegression': {'func': LogisticRegression(),\n                                     'params': {'C': [0.1, 0.5, 1, 1.2, 1.5]},\n                                     'rec': None},\n              'KNeighborsClassifier': {'func': KNeighborsClassifier(),\n                                     'params': {'n_neighbors': [5, 6, 7, 8, 9, 10, 11]},\n                                      'rec': None},\n              'BernoulliNB'        : {'func': BernoulliNB(),\n                                     'params': {'alpha': [0.1, 0.5, 1, 1.2, 1.5]},\n                                     'rec': None},\n              'AdaBoostClassifier' : {'func': AdaBoostClassifier(),\n                                      'params': {'learning_rate': [0.1, 0.5, 1, 1.2, 1.5]},\n                                      'rec': None}\n             }\n\nmodels_to_test = estimators.keys()\n\nfor name, estimator in estimators.items():\n    if name in models_to_test:\n        model = GridSearchCV(estimator=estimator['func'], \n                             param_grid=estimator['params'],\n                             scoring=make_scorer(recall))\n        model.fit(x_train, y_train)\n        preds = model.predict(x_test)\n        rec   = recall(y_test, preds)\n        acc   = accuracy(y_test, preds)\n        estimator[\"rec\"] = rec\n        print(f\"{name} Recall: {rec}, Accuracy: {acc} Best Params: {model.best_params_}\")\n\nbest_spec  = 0\nbest_model = None\nfor name, estimator in estimators.items():\n    if estimator[\"rec\"] > best_spec:\n        best_spec  = estimator[\"rec\"]\n        best_model = name\n    \nprint(f'Best model: {best_model}')      ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}