{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt \nimport seaborn as sn\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T17:42:16.967212Z","iopub.execute_input":"2021-08-08T17:42:16.967825Z","iopub.status.idle":"2021-08-08T17:42:18.356511Z","shell.execute_reply.started":"2021-08-08T17:42:16.967731Z","shell.execute_reply":"2021-08-08T17:42:18.355753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diabetes = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndiabetes.info()\n\nprint(diabetes.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:18.357945Z","iopub.execute_input":"2021-08-08T17:42:18.358214Z","iopub.status.idle":"2021-08-08T17:42:18.399218Z","shell.execute_reply.started":"2021-08-08T17:42:18.358189Z","shell.execute_reply":"2021-08-08T17:42:18.398162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis: EDA","metadata":{}},{"cell_type":"markdown","source":"### EDA#1: Correlation Matrix","metadata":{}},{"cell_type":"code","source":"# Create Correlation Matrix\ncorrMatrix = diabetes.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:18.400903Z","iopub.execute_input":"2021-08-08T17:42:18.401191Z","iopub.status.idle":"2021-08-08T17:42:19.084689Z","shell.execute_reply.started":"2021-08-08T17:42:18.401164Z","shell.execute_reply":"2021-08-08T17:42:19.083559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> The correlation matrix suggests that there is notable positive corelation of value 0.47 between Glucose and Outcome. <b>","metadata":{}},{"cell_type":"markdown","source":"## Split predictors and target variable","metadata":{}},{"cell_type":"code","source":"# Split predictors and target variable\n\nX = diabetes.loc[:, diabetes.columns != 'Outcome']\ny = diabetes[\"Outcome\"]\n\n# Split data into Test & Training dataset\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:19.086672Z","iopub.execute_input":"2021-08-08T17:42:19.087104Z","iopub.status.idle":"2021-08-08T17:42:19.097406Z","shell.execute_reply.started":"2021-08-08T17:42:19.087059Z","shell.execute_reply":"2021-08-08T17:42:19.096342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build a Confusion Matrix","metadata":{}},{"cell_type":"code","source":"##Importing the metrics  from sklearn import metrics \n##Defining the matrix to draw the confusion matrix from actual and  ##predicted class labels \ndef draw_cm( actual, predicted):\n    #Invoking confusion_matrix from metric package. The matrix  \n    #will be oriented as [1,0] i.e. the classes with label 1 will be  \n    #represented by the first row and 0 as second row  \n    cm = metrics.confusion_matrix( actual, predicted, [1,0])  \n    #Confusion will be plotted as heatmap for better visualization  \n    #The labels are configured to better interpretation from the plot  \n    sn.heatmap(cm, annot=True, fmt='.2f',  \n               xticklabels = [\"Diabetes-Yes\", \"Diabetes-No\"],  \n               yticklabels = [\"Diabetes-Yes\", \"Diabetes-No\"])  \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:19.099331Z","iopub.execute_input":"2021-08-08T17:42:19.099759Z","iopub.status.idle":"2021-08-08T17:42:19.110085Z","shell.execute_reply.started":"2021-08-08T17:42:19.099692Z","shell.execute_reply":"2021-08-08T17:42:19.108315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build / Train Random Forest Model","metadata":{}},{"cell_type":"code","source":"## Initializing the Random Forest Classifier with max_depth and n_estimators\nrf_classifier = RandomForestClassifier(max_depth=10, n_estimators=10)\nrf_classifier.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:19.1113Z","iopub.execute_input":"2021-08-08T17:42:19.111678Z","iopub.status.idle":"2021-08-08T17:42:19.271983Z","shell.execute_reply.started":"2021-08-08T17:42:19.111648Z","shell.execute_reply":"2021-08-08T17:42:19.270909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Precision, Recall, f1-score, Support values","metadata":{}},{"cell_type":"code","source":"pred_y = rf_classifier.predict(test_X)\nprint( metrics.classification_report( test_y, pred_y)) ","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:19.273434Z","iopub.execute_input":"2021-08-08T17:42:19.273744Z","iopub.status.idle":"2021-08-08T17:42:19.288414Z","shell.execute_reply.started":"2021-08-08T17:42:19.273716Z","shell.execute_reply":"2021-08-08T17:42:19.287543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print confusion matrix","metadata":{}},{"cell_type":"code","source":"#Print confusion matrix\ncm = draw_cm(test_y, pred_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:19.28953Z","iopub.execute_input":"2021-08-08T17:42:19.289925Z","iopub.status.idle":"2021-08-08T17:42:19.479154Z","shell.execute_reply.started":"2021-08-08T17:42:19.289893Z","shell.execute_reply":"2021-08-08T17:42:19.478014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding important Features","metadata":{}},{"cell_type":"code","source":"#Create a dataframe to store the features and their corresponding #importances \nfeature_rank = pd.DataFrame( {'feature': train_X.columns,  'importance': rf_classifier.feature_importances_  }) \n##Sorting the features based on their importances with most  ##important feature at top. \nfeature_rank = feature_rank.sort_values('importance', ascending =  False) \nplt.figure(figsize=(8, 6)) \n#plot the values \nsn.barplot( y = 'feature', x = 'importance', data = feature_rank[0:10]);","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:42:19.481155Z","iopub.execute_input":"2021-08-08T17:42:19.481455Z","iopub.status.idle":"2021-08-08T17:42:19.698096Z","shell.execute_reply.started":"2021-08-08T17:42:19.481428Z","shell.execute_reply":"2021-08-08T17:42:19.697359Z"},"trusted":true},"execution_count":null,"outputs":[]}]}