{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nPROJECT_ROOT_DIR ='/kaggle/input/singapore-airbnb'\nHOUSING_PATH = os.path.join(PROJECT_ROOT_DIR,\"listings.csv\")\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = HOUSING_PATH\n    return pd.read_csv(csv_path)\nhousing = load_housing_data()\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are some missing values in last_review and reviews_per_month\n"},{"metadata":{},"cell_type":"markdown","source":"### Let see if we have some correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncorr = housing[['price',\t'minimum_nights',\t'number_of_reviews', 'calculated_host_listings_count',\t'availability_365']].corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(240, 10, n=9),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The correlation between the price and the listed features is poor."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix[\"price\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing for ML"},{"metadata":{},"cell_type":"markdown","source":"### Spliting data in training and test set "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef getting_working_vars (df,column_target):\n    X_train = df.drop(column_target, axis=1)\n    y_train = df[column_target].values\n   \n    return X_train,y_train\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\nX_train,Y_train = getting_working_vars (train_set,['price'])\nX_test,Y_test = getting_working_vars (test_set,['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Before to do more trans"},{"metadata":{},"cell_type":"markdown","source":"### Now let's build a pipeline using ColumnTransformer from scikit-learn to prerpocessing the numerical and cathegorical features separated:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = ['minimum_nights', 'number_of_reviews','calculated_host_listings_count']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['neighbourhood_group', 'neighbourhood', 'room_type']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nfull_pipeline_m = Pipeline(steps=[('preprocessor', preprocessor),\n                                  #('pca', TruncatedSVD()),\n                                  ('model', LinearRegression())])\n\n\n\n\nparam_grid = [  \n        {'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n         #'pca__n_components':[1,2,3,4,5],\n         'model': [SGDRegressor()],\n         #'model__penalty': ['l1', 'l2'],\n         #'model__loss': ['squared_loss', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive']\n         'model__alpha': [0.0001 * x for x in range(1, 6)]\n         },                \n         {'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n          'model': [LinearRegression()]\n          \n          },                 \n        {'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n         'model': [DecisionTreeRegressor()],\n         \n         \n         },\n         {'preprocessor__num__imputer__strategy': ['mean','median','most_frequent'],\n         'model': [RandomForestRegressor()],\n         \n        \n         }\n         ]   \n           \nscoring = ['precision_macro', 'recall_macro', 'f1_macro',\n               'balanced_accuracy']\ngrid_search = GridSearchCV(full_pipeline_m, param_grid, cv=10,\n                                     n_jobs=-1, verbose=0);\n    #\ngrid_search.fit(X_train,Y_train);\nprint(grid_search.best_params_)\nhousing_predictions = grid_search.predict(X_test)\ntree_mse = mean_squared_error(Y_test, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint(tree_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check the null model by guessing the median price for all the predicted values and we will compare with the rmse obtained above"},{"metadata":{"trusted":true},"cell_type":"code","source":"#\nmedian_price = np.median(Y_test)\nnum_rows = len(Y_test)\n#num_rows\nfrom sklearn import metrics\nnull_model_predictions = [median_price]*num_rows\n#null_model_predictions\nnp.sqrt(metrics.mean_squared_error(Y_test, null_model_predictions))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}