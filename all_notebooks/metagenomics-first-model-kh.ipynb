{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook we explore metagenomics data. This dataset was created by the team of Edoardo Pasolli, Duy Tin Truong, Faizan Malik, Levi Waldron, and Nicola Segata; they published [a research article in July of 2016](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004977). The authors used 8 publicly available metagenomic datasets, and applied [MetaPhlAn2](https://github.com/segatalab/metaml#metaml---metagenomic-prediction-analysis-based-on-machine-learning) to generate species abundance features.\n\n## Logistics behind the Input Data\n\nThis notebook was created to further explore the meta-genomics data on kaggle. The link to the data-set is: https://www.kaggle.com/antaresnyc/metagenomics. The datasets include:\n* abundance.txt: a table containing the abundances of each organism type\n  * the first 210 features include meta-data about the samples\n  * the rest of the features include the abundance data in float-type\n* marker_presence.txt: a table containing the presence of strain-specific markers. \n  * the first 210 features include meta-data about the samples (same as abundance.txt)\n  * In a previous notebook I converted the marker presence feature data into a sparse matrix for easier downloading. This sparse matrix is found on [kaggle](https://www.kaggle.com/sklasfeld/metagenomics-marker-presence-sparse-matrix).\n* markers2clades_DB.txt: a lookup table to associate each marker identifier to the corresponding species.","metadata":{}},{"cell_type":"markdown","source":"In summary we have 210 samples. We know the abundance of the organisms in the sample. If an organism is in a sample we have strain-specific marker information.","metadata":{}},{"cell_type":"markdown","source":"## Libraries\nBelow I import some librarys that may be useful and then print the input files","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy\nimport scipy.sparse\nimport networkx as nx\nfrom sklearn import preprocessing\n\n# plot with matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#from plotnine import * # used to plot data\n\n# progress bar\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T03:30:40.730045Z","iopub.execute_input":"2021-06-21T03:30:40.730414Z","iopub.status.idle":"2021-06-21T03:30:41.991897Z","shell.execute_reply.started":"2021-06-21T03:30:40.730334Z","shell.execute_reply":"2021-06-21T03:30:41.990808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"marker_presence_matrix_file=\"/kaggle/input/metagenomics-marker-presence-sparse-matrix/marker_presence_matrix.npz\"\nmarkers2clades_DB_file=\"/kaggle/input/human-metagenomics/markers2clades_DB.csv\"\nabundance_file=\"/kaggle/input/human-metagenomics/abundance.csv\"\nmarker_presence_table_file=\"/kaggle/input/human-metagenomics/marker_presence.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:32.977453Z","iopub.execute_input":"2021-06-20T20:23:32.977838Z","iopub.status.idle":"2021-06-20T20:23:32.984045Z","shell.execute_reply.started":"2021-06-20T20:23:32.977801Z","shell.execute_reply":"2021-06-20T20:23:32.983304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the Data\nThe marker matrix is dependent on the abundance table in that strain-specific markers can only appear if a specific strain is abundant. Both tables can be merged together using a join-function on the 210 sample meta-data columns. However these columns are very messy. Therefore let's clean them before we move on to understanding the rest of the data.","metadata":{}},{"cell_type":"markdown","source":"## Testing the meta data","metadata":{}},{"cell_type":"markdown","source":"The meta data information is given in both the marker_presence and abundance tables. I just wanted to make sure they contain the same information.","metadata":{}},{"cell_type":"code","source":"%%time\n\nsamples_df = pd.read_csv(abundance_file,\n                         sep=\",\", dtype=object,usecols=range(0,210))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:32.986078Z","iopub.execute_input":"2021-06-20T20:23:32.986865Z","iopub.status.idle":"2021-06-20T20:23:33.670388Z","shell.execute_reply.started":"2021-06-20T20:23:32.986839Z","shell.execute_reply":"2021-06-20T20:23:33.669616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif 1 == 0:\n    samples_df2 = pd.read_csv(marker_presence_table_file,\n                         sep=\",\", dtype=object,usecols=range(0,210))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:33.671764Z","iopub.execute_input":"2021-06-20T20:23:33.672112Z","iopub.status.idle":"2021-06-20T20:23:33.68012Z","shell.execute_reply.started":"2021-06-20T20:23:33.672074Z","shell.execute_reply":"2021-06-20T20:23:33.678884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 1 == 0:\n    samples_df.compare(samples_df2, align_axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:33.684021Z","iopub.execute_input":"2021-06-20T20:23:33.684345Z","iopub.status.idle":"2021-06-20T20:23:33.688562Z","shell.execute_reply.started":"2021-06-20T20:23:33.684308Z","shell.execute_reply":"2021-06-20T20:23:33.687446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like they are basically the same so I can move forward using `samples_df`","metadata":{}},{"cell_type":"code","source":"samples_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:33.69017Z","iopub.execute_input":"2021-06-20T20:23:33.69057Z","iopub.status.idle":"2021-06-20T20:23:34.261419Z","shell.execute_reply.started":"2021-06-20T20:23:33.690536Z","shell.execute_reply":"2021-06-20T20:23:34.260677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples_df.query('dataset_name in [\"t2dmeta_long\",\"t2dmeta_short\"]')['disease'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:34.264632Z","iopub.execute_input":"2021-06-20T20:23:34.264916Z","iopub.status.idle":"2021-06-20T20:23:34.285296Z","shell.execute_reply.started":"2021-06-20T20:23:34.264887Z","shell.execute_reply":"2021-06-20T20:23:34.284026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning meta features","metadata":{}},{"cell_type":"markdown","source":"remove all column with only one value","metadata":{}},{"cell_type":"code","source":"samples_df = samples_df.loc[:, samples_df.nunique() > 1].copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:34.286999Z","iopub.execute_input":"2021-06-20T20:23:34.287382Z","iopub.status.idle":"2021-06-20T20:23:34.391058Z","shell.execute_reply.started":"2021-06-20T20:23:34.287342Z","shell.execute_reply":"2021-06-20T20:23:34.390251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next I look at categorical columns (AKA any feature that has 20 possible values or less)","metadata":{}},{"cell_type":"code","source":"if 1 == 0:\n    for col in samples_df.loc[:, samples_df.nunique() < 20]:\n        print(\"%s:%i\" % (col,samples_df[col].nunique()))\n        print(samples_df[col].unique())\n        print(\"\")","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-20T20:23:34.392457Z","iopub.execute_input":"2021-06-20T20:23:34.392849Z","iopub.status.idle":"2021-06-20T20:23:34.398336Z","shell.execute_reply.started":"2021-06-20T20:23:34.39281Z","shell.execute_reply":"2021-06-20T20:23:34.397437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like `nd`, `na`, `unknown` and `-` all stands for no data. Therefore let's replace these values all with np.NaN","metadata":{}},{"cell_type":"code","source":"samples_df = samples_df.replace(\"nd\", np.NaN)\nsamples_df = samples_df.replace(\"na\", np.NaN)\nsamples_df = samples_df.replace(\"-\", np.NaN)\nsamples_df = samples_df.replace(' -', np.NaN)\nsamples_df = samples_df.replace('unknown', np.NaN)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:34.399751Z","iopub.execute_input":"2021-06-20T20:23:34.400342Z","iopub.status.idle":"2021-06-20T20:23:35.165115Z","shell.execute_reply.started":"2021-06-20T20:23:34.400301Z","shell.execute_reply":"2021-06-20T20:23:35.164351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can remove all columns that have only 1 values and NaN. These do not seem to be too informative anyway.","metadata":{}},{"cell_type":"code","source":"# change the if statement to visualize\nif 1==0:\n    for col in samples_df.loc[:, samples_df.nunique() == 1].columns:\n        samples_df[col].fillna(\"NaN\").value_counts().sort_values().plot(\n            kind = 'bar', title=col)\n        plt.show()\n        \nsamples_df = samples_df.loc[:, samples_df.nunique() > 1].copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:35.166647Z","iopub.execute_input":"2021-06-20T20:23:35.166989Z","iopub.status.idle":"2021-06-20T20:23:35.313397Z","shell.execute_reply.started":"2021-06-20T20:23:35.166952Z","shell.execute_reply":"2021-06-20T20:23:35.312675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to convert some columns into booleans. For example if the values are either:\n* \"yes\",\"no\", or null\n* \"y\",\"n\", or null\n* \"positve\", \"negative\", or null\n* \"a\"(affected), \"u\" (unaffected), or null\n\nI want to convert them into `2`, `1`, and `0` respectively.","metadata":{}},{"cell_type":"code","source":"bool_vals={'True':2,\n          'False':1,\n          'Null':0}\nfor col in samples_df.loc[:, samples_df.nunique() < 4]:\n    if (\"yes\" in samples_df[col].unique() and \"no\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'yes': bool_vals['True'], 'no': bool_vals['False']}})\n    elif (\"y\" in samples_df[col].unique() and \"n\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'y': bool_vals['True'], 'n': bool_vals['False']}})\n    elif (\"positive\" in samples_df[col].unique() and \"negative\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'positive': bool_vals['True'], 'negative': bool_vals['False']}})\n    elif (\"a\" in samples_df[col].unique() and \"u\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'a': bool_vals['True'], 'u': bool_vals['False']}})","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:35.31486Z","iopub.execute_input":"2021-06-20T20:23:35.315195Z","iopub.status.idle":"2021-06-20T20:23:35.926071Z","shell.execute_reply.started":"2021-06-20T20:23:35.31516Z","shell.execute_reply":"2021-06-20T20:23:35.924631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, for columns that contain 2 values (not including null) I will convert the values to numbers. For example, I will change the column named \"gender\" to \"gender:Female|Male\". The values will be 1 for Female, 2 for Male, and 0 for null.","metadata":{}},{"cell_type":"code","source":"for col in samples_df.loc[:, samples_df.nunique() == 2].columns:\n    if (not(True in samples_df[col].unique() and \n             False in samples_df[col].unique())):\n        val_i = 0\n        first_val_null=True\n        first_val = np.NaN\n        while (first_val_null):\n            first_val = samples_df[col].unique()[val_i]\n            if first_val == first_val:\n                first_val_null = False\n            else:\n                val_i += 1\n        val_i += 1\n        second_val_null=True\n        second_val= np.NaN\n        while (second_val_null):\n            second_val = samples_df[col].unique()[val_i]\n            if second_val == second_val:\n                second_val_null = False\n            else:\n                val_i += 1\n        new_col_name=(\"%s:%s|%s\" % (col,first_val, second_val))\n        # change the column name\n        samples_df = (samples_df.rename(\n            columns={col:new_col_name}))\n        # change values in the column\n        samples_df[new_col_name] = samples_df[new_col_name].fillna(bool_vals['Null'])\n        samples_df =samples_df.replace({new_col_name: {first_val: bool_vals['False'],\n                                                       second_val: bool_vals['True']}})\ncategorical_cols=samples_df.loc[:, samples_df.nunique() < 20].columns","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:35.927994Z","iopub.execute_input":"2021-06-20T20:23:35.928327Z","iopub.status.idle":"2021-06-20T20:23:36.393089Z","shell.execute_reply.started":"2021-06-20T20:23:35.928291Z","shell.execute_reply":"2021-06-20T20:23:36.392339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It was brought to my attention that most samples come from stool. Therefore it makes sense that we remove other types of samples.","metadata":{}},{"cell_type":"code","source":"samples_df['bodysite'].value_counts().plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:36.394641Z","iopub.execute_input":"2021-06-20T20:23:36.394982Z","iopub.status.idle":"2021-06-20T20:23:36.674124Z","shell.execute_reply.started":"2021-06-20T20:23:36.394947Z","shell.execute_reply":"2021-06-20T20:23:36.673292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples_df['bodysite'] == 'stool'\nprint(np.sum(samples_df.nunique() < 3))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:36.675699Z","iopub.execute_input":"2021-06-20T20:23:36.676042Z","iopub.status.idle":"2021-06-20T20:23:36.76268Z","shell.execute_reply.started":"2021-06-20T20:23:36.676005Z","shell.execute_reply":"2021-06-20T20:23:36.761865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortonately this didn't help remove any features from the meta data.","metadata":{}},{"cell_type":"code","source":"stool_samp_df = samples_df.loc[samples_df['bodysite'] == 'stool',:].copy()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:36.764228Z","iopub.execute_input":"2021-06-20T20:23:36.764678Z","iopub.status.idle":"2021-06-20T20:23:36.776494Z","shell.execute_reply.started":"2021-06-20T20:23:36.764641Z","shell.execute_reply":"2021-06-20T20:23:36.775791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning abundance file","metadata":{}},{"cell_type":"markdown","source":"import abundance file without the first 211 columns (since we already dealt with those above)","metadata":{}},{"cell_type":"code","source":"%%time\n\nabundance_df = (pd.read_csv(abundance_file,sep=\",\", dtype=object)\n               .iloc[:,211:])\nabundance_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:36.778093Z","iopub.execute_input":"2021-06-20T20:23:36.778422Z","iopub.status.idle":"2021-06-20T20:23:39.399917Z","shell.execute_reply.started":"2021-06-20T20:23:36.778388Z","shell.execute_reply":"2021-06-20T20:23:39.399225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am wondering if we can remove any columns that are redundant. In other words, I would like to remove columns that have identical values.\n\nIn the following for-loop I simply check columns that are consecutive of one another to see if they are identical. I then use the biggest category as the key in `redundant_dict` and all the sub-categories that are equal in the list values.","metadata":{}},{"cell_type":"code","source":"#seen_list=[]\nredundant_dict={}\nremove_cols=[]\ni=0\nwhile i < abundance_df.shape[1]:\n    j=i+1\n    next_step=abundance_df.shape[1]\n    while j < abundance_df.shape[1]:\n        #print(\"%i,%i\" % (i,j))\n        col_i = abundance_df.columns[i]\n        col_j = abundance_df.columns[j]\n        if col_i in col_j:\n            #print(abundance_df.iloc[:,i].equals(abundance_df.iloc[:,j]))\n            if abundance_df.iloc[:,i].equals(abundance_df.iloc[:,j]):\n                # add redundant column name to data-structure\n                remove_cols.append(col_j)\n                if col_i in redundant_dict: redundant_dict[col_i].append(col_j)\n                else: redundant_dict[col_i]= [col_j]\n                # next look at i vs j+1\n                \n            else:\n                #print(\"next_step: \"+ str(next_step))\n                if next_step > j:\n                    next_step=j\n            \n            j += 1\n            if j == abundance_df.shape[1]:\n                i = j\n        else:\n            if next_step < j:\n                i=next_step\n                next_step=abundance_df.shape[1]\n            else:\n                i=j\n            j=abundance_df.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:39.401194Z","iopub.execute_input":"2021-06-20T20:23:39.401535Z","iopub.status.idle":"2021-06-20T20:23:40.818208Z","shell.execute_reply.started":"2021-06-20T20:23:39.401499Z","shell.execute_reply":"2021-06-20T20:23:40.817222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"drop redundant columns","metadata":{}},{"cell_type":"code","source":"abundance_df = (\n        abundance_df.drop(\n            remove_cols,\n            axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:40.819584Z","iopub.execute_input":"2021-06-20T20:23:40.819908Z","iopub.status.idle":"2021-06-20T20:23:40.893152Z","shell.execute_reply.started":"2021-06-20T20:23:40.819876Z","shell.execute_reply":"2021-06-20T20:23:40.892434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(remove_cols))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:23:40.894714Z","iopub.execute_input":"2021-06-20T20:23:40.895069Z","iopub.status.idle":"2021-06-20T20:23:40.899734Z","shell.execute_reply.started":"2021-06-20T20:23:40.895031Z","shell.execute_reply":"2021-06-20T20:23:40.898893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1,441 columns were dropped from further analysis since they were redundant with parent columns. ","metadata":{}},{"cell_type":"code","source":"# Note: to get the full abudance file back with cleaning using the following code:\nif 1==0:\n    c = samples_df.merge(abundance_df, how='left',\n                               left_index=True, right_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:27:00.942872Z","iopub.execute_input":"2021-06-20T20:27:00.943219Z","iopub.status.idle":"2021-06-20T20:27:00.949233Z","shell.execute_reply.started":"2021-06-20T20:27:00.943186Z","shell.execute_reply":"2021-06-20T20:27:00.948229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples_df['dataset_name']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:27:29.003396Z","iopub.execute_input":"2021-06-20T20:27:29.003783Z","iopub.status.idle":"2021-06-20T20:27:29.011521Z","shell.execute_reply.started":"2021-06-20T20:27:29.003751Z","shell.execute_reply":"2021-06-20T20:27:29.010575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning Marker Presence file\nImport this file without the first 211 columns (since we already dealt with those previously). This file could be imported as a sparse numpy matrix. it is very large.","metadata":{}},{"cell_type":"code","source":"%%time \n\nmarkers_reader = pd.read_csv(\n        marker_presence_table_file,\n        sep=\",\", \n        dtype=object,\n        usecols=range(211,288558),\n        nrows=10)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T20:27:33.150979Z","iopub.execute_input":"2021-06-20T20:27:33.15129Z","iopub.status.idle":"2021-06-20T20:28:17.282093Z","shell.execute_reply.started":"2021-06-20T20:27:33.151261Z","shell.execute_reply":"2021-06-20T20:28:17.281174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"markers_reader","metadata":{"execution":{"iopub.status.busy":"2021-06-05T21:51:06.403597Z","iopub.execute_input":"2021-06-05T21:51:06.403943Z","iopub.status.idle":"2021-06-05T21:51:06.466691Z","shell.execute_reply.started":"2021-06-05T21:51:06.403907Z","shell.execute_reply":"2021-06-05T21:51:06.465862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Construct a graph for genomic part\nIn order to capture a tree structure of the genes, we construct a directed graph where an node represents each bacteria and edge represents parent-child relationship. To quantify the presence of each bacteria, I set up a vector as a node property.","metadata":{}},{"cell_type":"code","source":"def graph_label(samples_df,abundance_df,dataset=None):\n    if dataset:\n        dataset = dataset if isinstance(dataset,list) else [dataset]\n        ids = samples_df['dataset_name'].isin(dataset)\n        samples_df = samples_df[ids].reset_index(drop=False)\n        abundance_df = abundance_df[ids].reset_index(drop=False)\n    le = preprocessing.LabelEncoder()\n    # target values\n    y = le.fit_transform(samples_df['disease'])\n    # get emnedding of all nodes\n    le_nodes = preprocessing.LabelEncoder()\n    # encode labels between 0 and n_classes-1 for each bacterial label\n    le_nodes.fit([gene for col in abundance_df.columns for gene in col.split('|')])\n    max_id = np.max(le_nodes.transform([gene for col in abundance_df.columns for gene in col.split('|')]))\n    data_list = []\n    for i in range(len(abundance_df)):\n        node_list = [] # list of [$cur_bacteria_name,$abundance_val]\n        edge_list = [] # list of [$parent_bacteria_name,$cur_bacteria_name]\n        for key, val in abundance_df.iloc[i].to_dict().items():\n            if float(val) > 0:\n                bacteria_list = key.split('|')\n                node = [le_nodes.transform([bacteria_list[-1]])[0],float(val)]\n                node_list.append(node)\n                if len(bacteria_list) >= 2:\n                    edge_list.append(le_nodes.transform(bacteria_list[-2:]))\n        # convert `y`, `node_list`, and `edge_list` into Tensor formats\n        edge_array = np.array(edge_list)\n        \n        edge_index = torch.tensor([edge_array[:,0],edge_array[:,1]],dtype=torch.long)\n        #print(np.array(node_list))\n        node_features = torch.LongTensor(np.array(node_list))\n        label = torch.FloatTensor([y[i]])\n        # set these Tensors into a pytorch Data() object\n        # which is used to model graphs\n        data = Data(node_features,edge_index=edge_index,y=label)\n        data_list.append(data)\n    return data_list,max_id","metadata":{"execution":{"iopub.status.busy":"2021-06-05T21:51:06.484301Z","iopub.execute_input":"2021-06-05T21:51:06.484918Z","iopub.status.idle":"2021-06-05T21:51:15.390232Z","shell.execute_reply.started":"2021-06-05T21:51:06.484879Z","shell.execute_reply":"2021-06-05T21:51:15.389246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch; print(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T21:51:15.391485Z","iopub.execute_input":"2021-06-05T21:51:15.391849Z","iopub.status.idle":"2021-06-05T21:51:15.399707Z","shell.execute_reply.started":"2021-06-05T21:51:15.391813Z","shell.execute_reply":"2021-06-05T21:51:15.398735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.version.cuda)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T21:51:15.401047Z","iopub.execute_input":"2021-06-05T21:51:15.401409Z","iopub.status.idle":"2021-06-05T21:51:15.411581Z","shell.execute_reply.started":"2021-06-05T21:51:15.401373Z","shell.execute_reply":"2021-06-05T21:51:15.410734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-geometric","metadata":{"execution":{"iopub.status.busy":"2021-06-05T21:51:15.412936Z","iopub.execute_input":"2021-06-05T21:51:15.413307Z","iopub.status.idle":"2021-06-05T22:30:21.622823Z","shell.execute_reply.started":"2021-06-05T21:51:15.413271Z","shell.execute_reply":"2021-06-05T22:30:21.621966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch_geometric.utils.convert import from_networkx\nfrom torch_geometric.data import InMemoryDataset\nfrom torch_geometric.data import Data","metadata":{"execution":{"iopub.status.busy":"2021-06-05T22:30:21.626552Z","iopub.execute_input":"2021-06-05T22:30:21.626857Z","iopub.status.idle":"2021-06-05T22:30:23.376372Z","shell.execute_reply.started":"2021-06-05T22:30:21.626827Z","shell.execute_reply":"2021-06-05T22:30:23.375634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nclass AbundanceDataset(InMemoryDataset):\n    def __init__(self, root, transform=None, pre_transform=None,dataset = None):\n        super(AbundanceDataset, self).__init__(root, transform, pre_transform)\n        self.dataset = dataset\n        self.data, self.slices = torch.load(self.processed_paths[0])\n    @property\n    def raw_file_names(self):\n        return []\n    @property\n    def processed_file_names(self):\n        return ['../input/yoochoose_click_binary_1M_sess.dataset']\n\n    def download(self):\n        pass\n    def process(self):\n        data_list = []\n        graph_label_pair = graph_label()\n        for G,value in zip(graph_label_pair['graph'],graph_label_pair['value']):\n            data = from_networkx(G)\n            data.y = torch.float\n            data_list.append(data)\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])\n        \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-05T22:30:56.518557Z","iopub.execute_input":"2021-06-05T22:30:56.518898Z","iopub.status.idle":"2021-06-05T22:30:56.583678Z","shell.execute_reply.started":"2021-06-05T22:30:56.518868Z","shell.execute_reply":"2021-06-05T22:30:56.582753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch_geometric.data import DataLoader\nfrom torch_geometric.nn import GINConv, global_add_pool","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndata_list = []\ngraph_label_pair = graph_label(samples_df,abundance_df,dataset = [\"t2dmeta_long\",\"t2dmeta_short\"])\nfor G,value in zip(graph_label_pair['graph'],graph_label_pair['value']):\n    print(G)\n    data = from_networkx(G)\n    data.y = torch.float(value)\n    data_list.append(data)\n'''\nt2dml_samples_df = samples_df.loc[samples_df['dataset_name']==\"t2dmeta_long\",:].copy()\nt2dml_abundance_values_df = abundance_df.iloc[(list(t2dml_samples_df.index))]\n# merge meta-data features with abundance features\nt2dml_abundance_df = t2dml_samples_df.merge(abundance_df, how='inner',left_index=True, right_index=True)\ndata_list,max_id = graph_label(t2dml_samples_df,t2dml_abundance_values_df)\nprint(max_id)\ntrain_datalist = data_list[len(data_list) // 10:]\ntest_datalist = data_list[:len(data_list) // 10]\n\ntrain_loader = DataLoader(train_datalist, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_datalist, batch_size=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 32\nCUDA_LAUNCH_BLOCKING=1\nfrom torch.nn import Sequential as Seq, Linear, ReLU\nfrom torch_geometric.utils import remove_self_loops, add_self_loops\nfrom torch_geometric.nn import TopKPooling,MessagePassing\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nimport torch.nn.functional as F\n\nclass SAGEConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super(SAGEConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n        self.lin = torch.nn.Linear(in_channels, out_channels)\n        self.act = torch.nn.ReLU()\n        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n        self.update_act = torch.nn.ReLU()\n        \n    def forward(self, x, edge_index):\n        # x has shape [N, in_channels]\n        # edge_index has shape [2, E]\n        \n        \n        edge_index, _ = remove_self_loops(edge_index)\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        \n        \n        return self.propagate(edge_index,x=x)\n\n    def message(self, x_j):\n        # x_j has shape [E, in_channels]\n\n        x_j = self.lin(x_j)\n        x_j = self.act(x_j)\n        \n        return x_j\n\n    def update(self, aggr_out, x):\n        # aggr_out has shape [N, out_channels]\n\n\n        new_embedding = torch.cat([aggr_out, x], dim=1)\n        \n        new_embedding = self.update_lin(new_embedding)\n        new_embedding = self.update_act(new_embedding)\n        \n        return new_embedding\n\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv1 = SAGEConv(embed_dim, embed_dim)\n        self.pool1 = TopKPooling(embed_dim, ratio=0.8)\n        self.conv2 = SAGEConv(embed_dim, embed_dim)\n        self.pool2 = TopKPooling(embed_dim, ratio=0.8)\n        self.conv3 = SAGEConv(embed_dim, embed_dim)\n        self.pool3 = TopKPooling(embed_dim, ratio=0.8)\n        self.item_embedding = torch.nn.Embedding(num_embeddings=max_id +1, embedding_dim=embed_dim)\n        self.lin1 = torch.nn.Linear(embed_dim * 2, embed_dim)\n        self.lin2 = torch.nn.Linear(embed_dim, int(embed_dim / 2))\n        self.lin3 = torch.nn.Linear(int(embed_dim / 2), 1)\n        self.bn1 = torch.nn.BatchNorm1d(embed_dim)\n        self.bn2 = torch.nn.BatchNorm1d(int(embed_dim / 2))\n        self.act1 = torch.nn.ReLU()\n        self.act2 = torch.nn.ReLU()        \n  \n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.item_embedding(x)\n        x = x.squeeze(1)        \n\n        x = F.relu(self.conv1(x, edge_index))\n\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = F.relu(self.conv2(x, edge_index))\n     \n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = F.relu(self.conv3(x, edge_index))\n\n        x, edge_index, _, batch, _ = self.pool3(x, edge_index, None, batch)\n        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = x1 + x2 + x3\n\n        x = self.lin1(x)\n        x = self.act1(x)\n        x = self.lin2(x)\n        x = self.act2(x)      \n        x = F.dropout(x, p=0.5, training=self.training)\n\n        x = F.log_softmax(self.lin3(x)).squeeze(1)\n\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    model.train()\n\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += float(loss) * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndevice = torch.device('cuda')\nmodel = Net().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(loader):\n    model.eval()\n\n    predictions = []\n    labels = []\n\n    with torch.no_grad():\n        for data in loader:\n\n            data = data.to(device)\n            pred = model(data).detach().cpu().numpy()\n\n            label = data.y.detach().cpu().numpy()\n            predictions.append(pred)\n            labels.append(label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(30):\n    loss = train()\n    train_acc = evaluate(train_loader)\n    #val_acc = evaluate(val_loader)    \n    test_acc = evaluate(test_loader)\n    print('Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}, Val Auc: {:.5f}, Test Auc: {:.5f}'.\n          format(epoch, loss, train_acc, val_acc, test_acc))","metadata":{},"execution_count":null,"outputs":[]}]}