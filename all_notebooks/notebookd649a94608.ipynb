{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3.Twitter data to conduct NLP and sentiment analysis.","metadata":{}},{"cell_type":"markdown","source":"Data Source:https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\nData files:\nThe data from train.csv will be will be used to train the model, as its name implies.The data from test.csv will be tested by the model MultinomialNB, built by us.NLP will be used to clean and pre-process the data, before vectorization.\n ","metadata":{}},{"cell_type":"markdown","source":"Read data into a DataFrame, print the size of the DataFrame and the first few lines of data.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport collections\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:03.633561Z","iopub.execute_input":"2021-07-30T02:30:03.633986Z","iopub.status.idle":"2021-07-30T02:30:03.641332Z","shell.execute_reply.started":"2021-07-30T02:30:03.633948Z","shell.execute_reply":"2021-07-30T02:30:03.639897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/train.csv\")\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:03.661527Z","iopub.execute_input":"2021-07-30T02:30:03.661903Z","iopub.status.idle":"2021-07-30T02:30:03.737881Z","shell.execute_reply.started":"2021-07-30T02:30:03.661871Z","shell.execute_reply":"2021-07-30T02:30:03.736845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get an overview of the data by finding the percentage of 'good' tweets (not racist/sexist) and the percentage of 'bad' tweets (racist/sexist). This helps us determine if there is enough representation of both types in the dataset.\nPrint the percentages.","metadata":{}},{"cell_type":"code","source":"countD = collections.defaultdict(int)\nfor data in train['label'] :\n    countD[data] += 1 \n\nfor(k,v) in countD.items() :\n    print(str(k) + ':' , round(v/len(train)*100,2), '%')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:03.73935Z","iopub.execute_input":"2021-07-30T02:30:03.739617Z","iopub.status.idle":"2021-07-30T02:30:03.75348Z","shell.execute_reply.started":"2021-07-30T02:30:03.73959Z","shell.execute_reply":"2021-07-30T02:30:03.752437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find and plot the 10 most used hashtags for the good tweets and 10 most used hashtags for the bad tweets. The plot shows the hashtags and their counts.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\n\n#for good tweets\ngood = train[train.label==0]\ntokenizer = RegexpTokenizer('\\#\\w+')\nl1 = []\nfor word in good['tweet']:\n    hashtags = tokenizer.tokenize(str(word))\n    l1=l1+hashtags\nfreqDict = nltk.FreqDist(l1)\nprint(list(freqDict)[0:9]) #Find the 10 most used hashtags for the good tweets\nplt.style.use('seaborn-whitegrid')\nfor tag in list(freqDict)[0:9]: # Plot the 10 most used hashtags for the good tweets\n    plt.bar(tag,freqDict[tag])\n    plt.xlabel('hashtags')\n    plt.ylabel('counts')\n    plt.xticks(rotation=90)\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:03.755342Z","iopub.execute_input":"2021-07-30T02:30:03.755605Z","iopub.status.idle":"2021-07-30T02:30:11.482213Z","shell.execute_reply.started":"2021-07-30T02:30:03.755579Z","shell.execute_reply":"2021-07-30T02:30:11.4815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for bad tweets\n\nbad = train[train.label==1]\ntokenizer = RegexpTokenizer('\\#\\w+')\nl2 = []\nfor word in bad['tweet'] :\n    hashtags = tokenizer.tokenize(str(word))\n    l2=l2+hashtags\nfreqDict = nltk.FreqDist(l2)\nprint(list(freqDict)[0:9]) #Find the 10 most used hashtags for the good tweets\nfreqDict.plot(10)   # Plot the 10 most used hashtags for the good tweets\n# Here you can see two ways of ploting the most used hashtags","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:11.483385Z","iopub.execute_input":"2021-07-30T02:30:11.48376Z","iopub.status.idle":"2021-07-30T02:30:11.646642Z","shell.execute_reply.started":"2021-07-30T02:30:11.483733Z","shell.execute_reply":"2021-07-30T02:30:11.645476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the y variable from the DataFrame.\nPrint the size of y.","metadata":{}},{"cell_type":"code","source":"y = train.label\ny.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:11.648051Z","iopub.execute_input":"2021-07-30T02:30:11.648458Z","iopub.status.idle":"2021-07-30T02:30:11.654544Z","shell.execute_reply.started":"2021-07-30T02:30:11.648416Z","shell.execute_reply":"2021-07-30T02:30:11.653511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the X variable from the DataFrame.\nPrint the size of X and the first few lines of x.","metadata":{}},{"cell_type":"code","source":"X = train[['tweet']]\nprint(X.shape)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:11.655853Z","iopub.execute_input":"2021-07-30T02:30:11.656112Z","iopub.status.idle":"2021-07-30T02:30:11.67628Z","shell.execute_reply.started":"2021-07-30T02:30:11.656088Z","shell.execute_reply":"2021-07-30T02:30:11.675248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing the data","metadata":{}},{"cell_type":"markdown","source":"The user names don't correlate with the labeling so to start pre-processing the data,you can remove them from the data.\nRemove all @names from the data and show the first few lines of X to see that they're gone.\nVectorized strings can make the job easier.","metadata":{}},{"cell_type":"code","source":"l3=[]\nfor word in X.tweet:\n        a = word.split(' ') #vectorized string operations\n        b = [v for v in a if not v.startswith('@')]  #vectorized string operations\n        c = ' '.join(b)\n        l3.append(c)\nX = pd.Series(l3).to_frame()\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:11.678423Z","iopub.execute_input":"2021-07-30T02:30:11.678714Z","iopub.status.idle":"2021-07-30T02:30:11.829765Z","shell.execute_reply.started":"2021-07-30T02:30:11.678666Z","shell.execute_reply":"2021-07-30T02:30:11.829034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Continue the pre-processing step by \n - removing non-words \n - lowercase all words \n - removing stop words \n - stemming the words.\n \nPrint the resulting X variable. ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n\nstop_words=set(stopwords.words(\"english\"))\nstemmer = PorterStemmer()\ntokenizer = RegexpTokenizer('[a-zA-Z]+')\n\ndef preprocess(s) :\n    w = tokenizer.tokenize(s.lower()) #removes non-words after lowercasing all words\n    w = [word for word in w if word not in stop_words] # removes stop words\n    w = [stemmer.stem(word) for word in w]  #stems the words\n    return ' '.join(w)\n\nX_processed = pd.Series([preprocess(X.loc[i,0]) for i in range(len(X))]).to_frame()\nX_processed.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:11.831274Z","iopub.execute_input":"2021-07-30T02:30:11.831752Z","iopub.status.idle":"2021-07-30T02:30:17.378581Z","shell.execute_reply.started":"2021-07-30T02:30:11.831715Z","shell.execute_reply":"2021-07-30T02:30:17.377449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Convert the strings in X to vectors of numbers. Show the size of the vectors.","metadata":{}},{"cell_type":"markdown","source":"# Vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(X_processed[0])\nX_vectors = vect.transform(X_processed[0])\nprint(X_vectors[0])\nX_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:17.379829Z","iopub.execute_input":"2021-07-30T02:30:17.380105Z","iopub.status.idle":"2021-07-30T02:30:18.044979Z","shell.execute_reply.started":"2021-07-30T02:30:17.380078Z","shell.execute_reply":"2021-07-30T02:30:18.043908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the dataset into training and testing sets and Show the size of the sets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_vectors,y,test_size=0.2)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.046341Z","iopub.execute_input":"2021-07-30T02:30:18.046626Z","iopub.status.idle":"2021-07-30T02:30:18.058801Z","shell.execute_reply.started":"2021-07-30T02:30:18.046596Z","shell.execute_reply":"2021-07-30T02:30:18.057863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the model","metadata":{}},{"cell_type":"markdown","source":"Use the Multinomial Naive Bayes model to train, then test the model.","metadata":{}},{"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.060097Z","iopub.execute_input":"2021-07-30T02:30:18.060637Z","iopub.status.idle":"2021-07-30T02:30:18.076249Z","shell.execute_reply.started":"2021-07-30T02:30:18.060604Z","shell.execute_reply":"2021-07-30T02:30:18.075106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the model is found and displayed by displaying the accuracy score, confusion matrix, and F1 score.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nprint(metrics.accuracy_score(y_test, y_pred)) #  accuracy score\nprint(metrics.confusion_matrix(y_test, y_pred, labels=[0,1])) # confusion matrix\nf1_score(y_test, y_pred, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.077478Z","iopub.execute_input":"2021-07-30T02:30:18.07793Z","iopub.status.idle":"2021-07-30T02:30:18.096915Z","shell.execute_reply.started":"2021-07-30T02:30:18.077897Z","shell.execute_reply":"2021-07-30T02:30:18.09614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy score = 0.9488503050211169 means the model was able to predict the results correctly 94% of the time.  Accuracy score is the rate of correct predictions.Out of every 100 predictions made, the model was correct 94 times.\nIt is used when we want to know the number of correct predictions, which is, when the algorithm correctly predicts a type T when it is actually type T. It takes into account all the possible classes and how much we predicted correctly.The score should be as high as possible.\n\nConfusion matrix represents accurate predictions made along the diagonal of the matrix.\n\nIf there are values not on the diagonal, it means the classifier made a wrong prediction.It gives us an exact count of correct and incorrect predictions in each categoy, rather than the ratio or a percentage.Confusion matrix here shows that a total of 5830 'good tweets' were classified correctly while 150 \nof them were wrongly classified to be bad tweets. A total of 236 bad tweets were classfied correctly,but 177  of them were wrongly classified as good tweets.The prediction is fairly accurate for good tweets but inaccurate for bad tweets.This could be due to a heavy bias towards good tweets,as they vastly outnumber(93%) the bad tweets(7%) in the given dataset,used for training the model.\n\nThe F1 score is a type of weighted  mean between precision and recall. \n\nIt is used when the datasets don't have an equal representation for each type that's being classified.\nF-score helps to measure Recall and Precision at the same time and makes two models with low precision and high recall or vice-versa, comparable.\n\nFor the above dataset, F1 score should be used because there is likely to be a bias in the workings of the model due to the fact that there is not enough representation of 'spam' types in the dataset. The percentages of type 'good' tweets' is 93% while that of 'bad tweets' is 7 %.\nDue to this imbalance in the data(skewness), F-1 score is a better measure of accuracy,as it is the weighted mean that takes into account the imbalance of data.","metadata":{}},{"cell_type":"markdown","source":"# 'Testing the testing set'","metadata":{}},{"cell_type":"markdown","source":"Prepare the data in test.csv to test the model,just like we did for the training set.","metadata":{}},{"cell_type":"markdown","source":"Read the file \"test.csv\" into a DataFrame.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/test.csv\")\nprint(test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.097993Z","iopub.execute_input":"2021-07-30T02:30:18.098426Z","iopub.status.idle":"2021-07-30T02:30:18.143837Z","shell.execute_reply.started":"2021-07-30T02:30:18.098385Z","shell.execute_reply":"2021-07-30T02:30:18.142979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop all the NaN values\n","metadata":{}},{"cell_type":"code","source":"test.dropna(inplace=True)\ntest","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.145134Z","iopub.execute_input":"2021-07-30T02:30:18.145429Z","iopub.status.idle":"2021-07-30T02:30:18.164065Z","shell.execute_reply.started":"2021-07-30T02:30:18.145399Z","shell.execute_reply":"2021-07-30T02:30:18.162883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the X variable from the DataFrame.\n\nPrint the size of X and the first few lines of X.\n","metadata":{}},{"cell_type":"code","source":"X = test[['tweet']]\nprint(X.shape)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.165214Z","iopub.execute_input":"2021-07-30T02:30:18.165474Z","iopub.status.idle":"2021-07-30T02:30:18.176165Z","shell.execute_reply.started":"2021-07-30T02:30:18.165448Z","shell.execute_reply":"2021-07-30T02:30:18.175278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pre-process the data and remove the @names from the data as they don't correlate with the labeling,print the first few lines to see that they're gone.","metadata":{}},{"cell_type":"code","source":"l3=[]\nfor word in X.tweet:\n        a = word.split(' ') #vectorized string operations\n        b = [v for v in a if not v.startswith('@')]  #vectorized string operations\n        c = ' '.join(b)\n        l3.append(c)\nY = pd.Series(l3).to_frame()\nY.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.177166Z","iopub.execute_input":"2021-07-30T02:30:18.177699Z","iopub.status.idle":"2021-07-30T02:30:18.270415Z","shell.execute_reply.started":"2021-07-30T02:30:18.177646Z","shell.execute_reply":"2021-07-30T02:30:18.269717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Continue the pre-processing step by removing non-words, lowercasing all words, removing stop words, \nand stemming the words. Print the resulting X variable.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n\nstop_words=set(stopwords.words(\"english\"))\nstemmer = PorterStemmer()\ntokenizer = RegexpTokenizer('[a-zA-Z]+')\n\ndef preprocess(s) :\n    w = tokenizer.tokenize(s.lower()) #removes non-words after lowercasing all words\n    w = [word for word in w if word not in stop_words] # removes stop words\n    w = [stemmer.stem(word) for word in w]  #stems the words\n    return ' '.join(w)\n\nX_processed = pd.Series([preprocess(Y.loc[i,0]) for i in range(len(Y))]).to_frame()\nX_processed.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:18.27136Z","iopub.execute_input":"2021-07-30T02:30:18.27176Z","iopub.status.idle":"2021-07-30T02:30:21.185553Z","shell.execute_reply.started":"2021-07-30T02:30:18.271727Z","shell.execute_reply":"2021-07-30T02:30:21.184607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert the strings in X to vectors of numbers. Show the size of the vectors\n","metadata":{}},{"cell_type":"code","source":"X_vectors = vect.transform(X_processed[0])\nprint(X_vectors[0])\nX_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:21.187958Z","iopub.execute_input":"2021-07-30T02:30:21.188247Z","iopub.status.idle":"2021-07-30T02:30:21.351928Z","shell.execute_reply.started":"2021-07-30T02:30:21.188219Z","shell.execute_reply":"2021-07-30T02:30:21.350953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test the testing data,using the model built in 1i that analyzes tweets and determines if they're \nracist/sexist or not. The same accuracy level stands good for the model, as was found for the training set(train.csv)\n","metadata":{}},{"cell_type":"code","source":"y_pred = classifier.predict(X_vectors)\ny_pred = pd.DataFrame(y_pred)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:21.353311Z","iopub.execute_input":"2021-07-30T02:30:21.353617Z","iopub.status.idle":"2021-07-30T02:30:21.366594Z","shell.execute_reply.started":"2021-07-30T02:30:21.353586Z","shell.execute_reply":"2021-07-30T02:30:21.365693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After we have the outcome of the test data, print the number of good tweets and the number of bad tweets that the model determined from the test data.\nBased on the accuracy measurement of the model, we want to answer these questions, to have a fair assessment of the working of the model. \n\n- How confident are we with the 2 numbers?\n- Is the model more likely to predict the wrong way for a good tweet? how about for a bad tweet?","metadata":{}},{"cell_type":"markdown","source":"Printing the outcome of the test data.","metadata":{}},{"cell_type":"code","source":"y_pred.columns = ['tweet']\ntweet = {0:'Good',1:'Bad'}  #Change the numerical values into strings to describe the data.\ny_pred.replace(tweet,inplace=True)\n\n\ncountD = collections.defaultdict(int)\nfor data in y_pred['tweet'] :\n    countD[data] += 1 \n\nfor(k,v) in countD.items() :\n    print(k,v) #Print the number of good and bad tweets","metadata":{"execution":{"iopub.status.busy":"2021-07-30T02:30:21.367742Z","iopub.execute_input":"2021-07-30T02:30:21.367995Z","iopub.status.idle":"2021-07-30T02:30:21.383322Z","shell.execute_reply.started":"2021-07-30T02:30:21.36797Z","shell.execute_reply":"2021-07-30T02:30:21.382037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final analysis","metadata":{}},{"cell_type":"markdown","source":"I am very confident with the number of good tweets,but not very confident with the number of bad tweets. \n\nAs this is a test data only, it will not have any Confusion matrix, accuracy score or f1 score.This model is built by us in the previous steps and we are now using it to test our testing dataset.Accuracy of the model is about 94%-95%, as we found both according to accuracy score and F1 score.\nSince we are using the model trained above, we are taking the accuracy levels(all 3)as displayed for it, but the high accuracy level works for good tweets only, because there is imbalance in the training data. A very high occurrence of good tweets(93%) in the training dataset leads to an imbalanced \ndataset and because it can calculate probabilities, our model learns the wrong way and makes incorrect predictions for bad tweets, but is right in predicting good tweets almost 93%-94% of the time, which is the accuracy level of the model.\n\nOn a lighter note, it is heartening to note that the overwhelmingly large number of good tweets indicate that only a handful of people create a bad name and spread hate in the world! The world is still a wondderful place to live in. :) ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}