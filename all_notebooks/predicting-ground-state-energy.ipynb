{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed = 13\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction  \n\nPlease check out my new [study](https://www.kaggle.com/mjmurphy28/predicting-atomization-energy-qm7), using a different but similar dataset. \n\n**Note**: atomization energies here are measured in Rydberg units, however, a more commonly used unit is kcal/mol. So conversion is necessary:  \n\n  * **1 Ry = 313.495392 kcal/mol**  \n  \nThe \"acceptable chemical accuracy” for atomization energy is 1 kcal/mol ([source](https://papers.nips.cc/paper/4830-learning-invariant-representations-of-molecules-for-atomization-energy-prediction.pdf)).\n\n# Data  \n\nThe data provided was originally downloaded from PubChem, a JSON file listing every atom in the molecule with it's corresponding element type (eg. hydrogen) as well as it's Cartesian coordinates. From this, as well as atomic charges (which can be found in any basic chemistry textbook), a Coulomb Matrix was computed for every molecule according to [Rupp et al. PRL, 2012], as so:\n\n  * $C_{i,i} = 0.5 \\cdot Z^2.4$  \n  * $C_{i,j} = Z_i \\cdot \\frac{Z_j}{|(R_i−R_j)|}$ \n  * $Z_i$ - nuclear charge of atom i  \n  * $R_i$ - cartesian coordinates of atom i"},{"metadata":{"trusted":true,"_uuid":"16349ab287cd7408000e0166f4b6cc77b02003cb"},"cell_type":"code","source":"raw_data = pd.read_csv(\"../input/roboBohr.csv\")\nX = raw_data.drop(['Unnamed: 0', 'pubchem_id', 'Eat'], axis = 1)\n# not sure what the last 25 features are, so I am just going to drop them for now\ny = raw_data['Eat']\nX.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"734f6c4893d555f3a1765be319469a458864d188"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, normalize\n\nX_standardized = StandardScaler().fit_transform(X)\nX_normalized = normalize(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91f141a1dd2fcb18eccf3317225ce7fde3a63221"},"cell_type":"markdown","source":"# Visualization  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA**"},{"metadata":{"trusted":true,"_uuid":"81881b3a52e8357917bb85e633ffb5eb6b40323f"},"cell_type":"code","source":"from sklearn.decomposition import PCA, KernelPCA\n## PCA\n\npca = PCA(n_components=2, random_state=seed)\n\nstart_time = time.time()\nX_reduced = pca.fit_transform(X_normalized)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nprint(\"Number of components: {}\".format(pca.components_.shape[0]))\nprint(\"Explained variance: \", pca.explained_variance_ratio_.sum())\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_reduced[:,0], X_reduced[:,1], c=y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=0.5)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_2$')\nplt.title('PCA')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because PCA (with 2 components) only explains less than 63% of the original variance of our data, if our goal of PCA is to visualize the data in 2 dimensions, we might want to explore something else like Kernel PCA or t-SNE."},{"metadata":{"trusted":true,"_uuid":"3707c9e10f133e424d6c93d9b0d7e1b644304a8c"},"cell_type":"code","source":"start_time = time.time()\nkpca = KernelPCA(n_components=2, kernel=\"linear\")\nX_kpca = kpca.fit_transform(X)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nexplained_variance = np.var(X_kpca, axis=0)\nexplained_variance_ratio = explained_variance / np.sum(explained_variance)\nprint(\"Variance Explained: \", np.sum(explained_variance_ratio))\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_kpca[:,0], X_kpca[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$k-PCA_1$')\nplt.ylabel(r'$k-PCA_1$')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kernel PCA, with a linear kernel, explains 100% of the original variance of our data with only 2 components, perfect for visualization purposes! However, it is important to note that the range of the entries in the Coulomb Matrices range from 2.906146 to 388.023441, so we should normalize these values first."},{"metadata":{"trusted":true,"_uuid":"e5ad7de814194ba74aa9d7571e4a7df4df4e58fc"},"cell_type":"code","source":"start_time = time.time()\nkpca3 = KernelPCA(n_components=2, kernel=\"linear\", random_state=seed)\nX_kpca3 = kpca3.fit_transform(X_normalized)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nexplained_variance = np.var(X_kpca3, axis=0)\nexplained_variance_ratio = explained_variance / np.sum(explained_variance)\nprint(\"Variance Explained: \", np.sum(explained_variance_ratio))\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_kpca3[:,0], X_kpca3[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_1$')\nplt.title('Kernel PCA: Normalized')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c138d2fc91b6d428ba00b0f49b59658e2b2b486b"},"cell_type":"markdown","source":"**t-SNE**"},{"metadata":{"trusted":true,"_uuid":"f54aebedc63a2f7a5f6697db06800b0b0de94f95"},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=seed)\nX_tsne = tsne.fit_transform(X_normalized)\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_tsne[:,0], X_tsne[:,1], c=y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=0.5)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_2$')\nplt.title('T-SNE: Perplexity = 30')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9459b2f3a21da95c061f592617edcecf6d3c1500"},"cell_type":"markdown","source":"# Regression  \n\nFirst we are going to split our data into 3 sets: train (70%), dev (15%) and test (15%). "},{"metadata":{"trusted":true,"_uuid":"3da75536c15dc09511651964ea07436e16284e30"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, \n                                                    test_size=0.15, \n                                                    random_state=seed)\n\nX_train_train, X_dev, y_train_train, y_dev = train_test_split(X_train, y_train, \n                                                    test_size=0.18, \n                                                    random_state=seed)\n\n\nprint(\"X: \", X.shape[0])\nprint(\"Train: {}\".format(X_train_train.shape[0]))\nprint(\"Dev: {}\".format(X_dev.shape[0]))\nprint(\"Val: {}\".format(X_val.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11c85c68c04ce05b147e8bacd1b77de6694c046b"},"cell_type":"markdown","source":"**XGBoost**"},{"metadata":{"trusted":true,"_uuid":"11f44e4065cbf715d4b29429570bb36f06743417","scrolled":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, r2_score\nfrom xgboost import XGBRegressor\n\n# Parameters for XGBoost model\n\nparams = {}\nparams['learning_rate'] = 0.09\nparams['max_depth'] = 8\nparams['n_estimators'] = 100\nparams['objective'] = 'reg:linear'\nparams['booster'] = 'gbtree'\nparams['gamma'] = 1e-3\nparams['subsample'] = 0.6\nparams['reg_alpha'] = 0.115\nparams['reg_lambda'] = 0.58\nparams['scale_pos_weight'] = 1\nparams['base_score'] = 0.5\nparams['random_state'] = seed\nparams['silent'] = True\nparams['num_leaves'] = 17\n\nprint('XGBoost')\nprint('--------------------------------------')\nstart_time = time.time()\n\nXGB = XGBRegressor(**params)\n#XGB.fit(X_train, y_train, verbose=True, eval_metric='rmse')\neval_set = [(X_train_train, y_train_train), (X_dev, y_dev)]\nXGB.fit(X_train_train, y_train_train, eval_metric='mae', eval_set=eval_set, verbose=False)\nY_pred_XGB = XGB.predict(X_dev)\nprint(\"Mean absolute error\", mean_absolute_error(y_dev, Y_pred_XGB))\nprint('R2 score: %0.5f'% r2_score(y_dev, Y_pred_XGB))\n\nprint(\"Took %s seconds\" % (time.time() - start_time))\nprint('--------------------------------------')\n\n# learning_rate = 0.09\n#Mean squared error 0.008758109152032869\n#R2 score: 0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluate**  \n\nNow let's evaluate our final model on the validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_pred = XGB.predict(X_val)\n\nprint(\"Mean absoulte error: {} kcal/mol\".format(313.495392 * mean_absolute_error(y_val, y_val_pred)))\nprint(\"R^2: \", r2_score(y_val, y_val_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}