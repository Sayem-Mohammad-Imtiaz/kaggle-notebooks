{"cells":[{"metadata":{"_cell_guid":"af558cd8-a6c1-465e-a663-8501336f3183","_uuid":"adf1692281d4d683bdc5f1f33a6d889aa6e0aaa6"},"cell_type":"markdown","source":"# Ch. 10 - New frills for our neural network\nAlternative title: Lots of important features that deep learning research has brought us to build better models.\n\nSo far, we have worked with four components:\n- Sigmoid activations for binary classification\n- Softmax activations for multiclass classification\n- Tanh activations for hidden layers\n- Vanilla gradient descent\n\nBut of course there is more. In this chapter we will explore 6 important tools:\n- The relu activation function for hidden layers\n- momentum\n- The adam optimizer\n- Regularization\n- Dropout\n- Batchnorm\n\nBut first, of course, let's load some libraries.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"b15f65a3-2c0c-4a04-8c13-12c00bfb0ca1","_uuid":"c455ebbd393e0585c6ca68c39877bd573253d5a6","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n# Set seed for reproducability \nnp.random.seed(42)\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26446641-d2be-4f4c-a419-7dd12361293b","_uuid":"911cd33fd96202120307a9c014cd7b6f38033269"},"cell_type":"markdown","source":"## The data\nIn the last chapter we already preprocessed the data:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"af6b45cc-9827-4a7b-aac8-8ea17d3dabff","_uuid":"1040bc11613f4497935b38e8298676ef55dcf774","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/processed_bank.csv',index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ad2d71bc-c5b9-4e6e-9bd2-7cada94f2de6","_uuid":"d4ddd973f5e633215eaa4b69dd67f30717072f30"},"cell_type":"markdown","source":"To make sure that nothing got lost in transition, let's check the data by displaying the first few rows and checking the dimensions of our dataset:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"a922ada9-3a24-4e25-8958-403e23fb6381","_uuid":"4f12e03fe1e281d6b41b23f25f0722e1d5bfd4ba","trusted":false},"cell_type":"code","source":"# Display first few (5) rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7d6dda01-72ee-4bc9-b4e2-3f7297b118c4","_uuid":"cac02aab8e45ddcbc4bb1da98dc8bda607d0a0e1","trusted":false},"cell_type":"code","source":"# Check shape of dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16bc5e04-1ed8-4d6b-a497-03a1514fa543","_uuid":"6b75143f70d2e08f0406c44ee38d22f1f4234443"},"cell_type":"markdown","source":"Everything looks good, like we would have expected it. Now it is time to split the input $X$ from the labels $y$","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"5383ed27-8178-484d-b9d4-3445bab31369","_uuid":"eb45bd447d086d7141b5cb9d86cd5de6060396c8","trusted":false},"cell_type":"code","source":"# X is everything that is not y\nX = df.loc[:, df.columns != 'y'].values\n# y is y\ny = df['y'].values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"830fbb31-760d-47a2-a50c-79cbdefb1a1f","_uuid":"30986dc26d53deebb4c88716bc0bf029d2a4d521"},"cell_type":"markdown","source":"## Train / Dev / Test\nSo far, we have worked with a trainings and a test set. From now on, we will have a training, test, and at least one dev set. The dev set is used for hyper parameter optimization. We train our model with the trainings set, and then we try it out on the dev set. We can then use the insight gained from evaluating the model on the dev set to tweak the parameters. We can not do this with the test set, because every time we use the insight gained from evaluation on a certain dataset we compromise the datasets neutrality. When we would tweak our hyper parameters on the test set, it would no longer be a totally new and unseen dataset for our model. By tweaking the hyper parameters to fit on the test set, we would run the risk of over fitting to the test set. That is why we have a dev set, we can tweak hyper parameters on. After we have tweaked the hyper parameters for a bit and are satisfied with the results on the dev set, we might try our model on the test set. The important part is that we do not use any insight from the test set evaluation to tweak our model, or we would have to get a new, unseen, test set.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"9f866227-20c9-44c7-aa65-d1f2df459ee4","_uuid":"1bc424b18af8a4252fa638a50fd29b9a4d26c0f6","trusted":false},"cell_type":"code","source":"# First split in train / test_dev\nfrom sklearn.model_selection import train_test_split\nX_train, X_test_dev, y_train, y_test_dev = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Second split in dev / test\nX_dev, X_test, y_dev, y_test = train_test_split(X_test_dev, y_test_dev, test_size=0.5, random_state=0)\n\n# Remove test_dev set from memory\ndel X_test_dev\ndel y_test_dev","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"93ca454b-2117-431c-8669-72c1824c6bfd","_uuid":"2fdaebdeb3c658f87389e69d1d843f27e598b6b9"},"cell_type":"markdown","source":"## The baseline model\nBefore diving into more complicated models and adding a lot of features to our network, it makes sense to start out with just a very simple logistic regression model. That way, we can verify that our data is ready for training and see how well a very simple model would do. Whenever we add new features, we should make sure that our new model actually does better than simple logistic regression.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"d51fdce9-17bf-4d9d-950a-d206c6436b66","_uuid":"b138545b34168978276bd0da3c8c505cb6e15897","trusted":false},"cell_type":"code","source":"# Get Keras\n# We will build a simple sequential model\nfrom keras.models import Sequential\n# Using fully connected layers\nfrom keras.layers import Dense\n# With vanilla gradient descent\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"986b217e-21bd-4802-abef-967c4fb821ac","_uuid":"f062ea4b7983fda814151f07c310f52d905b228c"},"cell_type":"markdown","source":"Remember that logistic regression is a neural network with only one layer. In this layer, every input feature is connected to the output. It uses a sigmoid activation function and a binary crossentropy loss (also called logistic loss or log loss). Here, we are setting the learning rate to 0.01, which is actually the standard value that Keras would have used anyway.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"e6337a7a-db21-48d4-a5bb-34a5ee3721f0","_uuid":"e0173467b4a631f8001490d5c9efbea5e6852cad","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# Logistic regresison is a single layer network\nmodel.add(Dense(1,activation='sigmoid',input_dim=64))\n\n# Compile the model\nmodel.compile(optimizer=SGD(lr=0.01),loss='binary_crossentropy',metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7078d210-02cb-4fac-9a42-b12067b3bb8a","_uuid":"e7a4758be8dccd6280d0cc0ffed406810453b463"},"cell_type":"markdown","source":"We will train this model using batch gradient descent, that is we will process all of our training examples at once. We can do this since we do not have very many training examples and the size of each individual example is quite small, just a 64 number per row. If you have a computer with little RAM you might consider using a smaller batch size than the whole trainings set.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"09f6670b-29ac-47e1-b73a-d8c24d622071","_uuid":"b06c17b7a436323336d84889d6db6454415f2645","trusted":false,"collapsed":true},"cell_type":"code","source":"history_log = model.fit(X_train, y_train, epochs=1000, batch_size=X_train.shape[0], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7393ea01-a978-482f-9006-ecca8a4e5a4f","_uuid":"041b75fa0d04346971724aecd78dde70576c6a1d"},"cell_type":"markdown","source":"In this notebook we suppress the Keras output to keep it readable but plot the accuracy and loss over the training epochs:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"3e318054-4adf-49a2-85ed-01755650cf0f","_uuid":"fe5e581e1588aaef0b4f7ce01166605965b6f3b3","trusted":false},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.plot(history_log.history['acc'], label = 'Logistic regression')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0ff44751-7e52-4fbe-a5cf-ae2847e70470","_uuid":"63f0bbe5a3aa074f828ccfa8a5ca4b9130f62893","trusted":false},"cell_type":"code","source":"plt.plot(history_log.history['loss'], label='Logistic regression')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f15f6299-a4bd-43ac-bb1d-50ebc16868d7","_uuid":"41a3420798e29f4ccbad152b6b34a656f4ef6231","trusted":false},"cell_type":"code","source":"model.evaluate(x=X_dev,y=y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13c5bd9d-d988-416e-bfc3-0b124d32bfea","_uuid":"e748c3010f7c4b16a6fbb7109c2917643c121e56"},"cell_type":"markdown","source":"Our model achieves about 71% accuracy, that is, for 71% of all times it was right. Let's see if we can do better.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4597865e-bfbc-4153-9b2f-caccae468476","_uuid":"ad3a970a6a944302af91bfa1cdb07b3eb6f48125"},"cell_type":"markdown","source":"## A bigger network (tanh activation)\nEarlier we already learned how to build a bigger network with a tanh activation for hidden layers, lets try the same thing here:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"3057a45e-78c4-4748-96ea-b76ee76f3bb0","_uuid":"4b3724b57cd1bc963cc60992e1e9a801b4a2d1f2","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# First hidden layer\nmodel.add(Dense(32,activation='tanh',input_dim=64))\n\n# Second hidden layer\nmodel.add(Dense(16,activation='tanh'))\n\n# Output layer\nmodel.add(Dense(1,activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer=SGD(lr=0.01),\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n\n# Train\nhistory_tanh = model.fit(X_train, y_train, # Train on training set\n                         epochs=1000, # We will train over 1,000 epochs\n                         batch_size=X_train.shape[0], # Batch size = training set size\n                         verbose=0) # Suppress Keras output","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d7fed31-6501-4049-90fb-abd8086c85b0","_uuid":"76201ed16d55b73df44919a85603c23bbacd0fe7"},"cell_type":"markdown","source":"Let's plot accuracy and loss development in comparison to plain logistic regression:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"db30a385-ad4a-49c6-9e79-44098fa27c96","_uuid":"a3cc3b0c416b80efb7f75c5815b0063ca39a21af","trusted":false},"cell_type":"code","source":"plt.plot(history_log.history['acc'], label= 'Logistic Regeression')\nplt.plot(history_tanh.history['acc'], label= '2 hidden layer Tanh')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1bfc37df-2818-493f-a06e-b47a7b9ee7fc","_uuid":"ee98ffc5afacbb7d28edcaf63669fc18700912d8","trusted":false},"cell_type":"code","source":"plt.plot(history_log.history['loss'], label= 'Logistic Regeression')\nplt.plot(history_tanh.history['loss'], label= '2 hidden layer Tanh')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d133247-d05c-425b-99ab-331f8eae3659","_uuid":"ca3179d02055d5c1fdb36079696055e4ef7f66fc"},"cell_type":"markdown","source":"From these graphs it looks like the model learned a lot better than logistic regression, let's evaluate it on the dev set:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"6237f97c-97fb-4cbd-9e82-1a1ae2d280ac","_uuid":"0ddbb2b20006a2fb3b45a2d76305fb05f0c64cff","trusted":false},"cell_type":"code","source":"model.evaluate(x=X_dev,y=y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1ff3e99-015a-48fe-ac3e-81fbd1bf56db","_uuid":"f0b0df48d71a2b811b5f97ebb02ee36e95b0a4d4"},"cell_type":"markdown","source":"In fact, a bigger network with a tanh activation does better here than just logistic regression, but there are other activations than tanh.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c216417d-2624-4802-8afc-a47434ee59e8","_uuid":"14ec402ff91a7f585423edea6b3ea863ed3ebb1e"},"cell_type":"markdown","source":"## The ReLu activation function\nReLu stands for rectified linear function. It can be expressed as follows:\n\n$$relu(x) = \\max(0,x)$$\n\nAnd looks like this:\n![ReLu](https://storage.googleapis.com/aibootcamp/Week%202/assets/relu.jpeg)\n\nThis very simple function has been shown to be quite useful, making gradient descent converge faster (see [Krizhevsky et. al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)). It is often argued that ReLu is faster because its derivative for all values above zero is just one, and does not become very small as the derivative for extreme values does with sigmoid or tanh. ReLu is also less computationally expensive than either sigmoid or tanh. It does not require any computationally expensive calculations, input values below zero are just set to zero and the rest is outputted as is. Unfortunately, ReLu activations are a bit fragile and can 'die'. When the gradient is very large and moves many weights in the negative direction then the derivative of ReLu will also always be zero, so the weights never get updated again. This might mean that a neuron never fires again. This can be mitigated through a smaller learning rate.\n\nBecause ReLu is fast and computationally cheap it has become the default activation functions for many practitioners. In Keras, we can swap tanh for ReLu simply by changing the name of the activation. Keras supports a wide range of different application functions, to learn more about them, check out the [official documentation](https://keras.io/activations/).","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"6a00764d-0747-424b-832b-88e0b6205743","_uuid":"922f2b2061dfd977d96aeebccc23e6cb65f8ca41","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# First hidden layer now with relu!\nmodel.add(Dense(32,activation='relu',input_dim=64))\n\n# Second hidden layer now with relu!\nmodel.add(Dense(16,activation='relu'))\n\n# Output layer stayed sigmoid\nmodel.add(Dense(1,activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer=SGD(lr=0.01),\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n# Train\nhistory_relu = model.fit(X_train, y_train, # Train on training set\n                         epochs=1000, # We will train over 1,000 epochs\n                         batch_size=X_train.shape[0], # Batch size = training set size\n                         verbose=0) # Suppress Keras output","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be357ceb-f5d5-419d-a309-67b3490a094d","_uuid":"180524f89864a08451c128f780e4d5f2329c002d"},"cell_type":"markdown","source":"If we plot the loss curves of both tanh and relu (in red) next to each other you can see that relu does in fact learn a little bit faster on this dataset. There is no real right or wrong for activation functions, but ReLu is a popular choice.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"ce870b3b-0be9-4784-aaec-4508cf4b7818","_uuid":"518c8955de02b1672f0ee5b412a42372b4ab0064","trusted":false},"cell_type":"code","source":"plt.plot(history_tanh.history['acc'], label='2 hidden layer Tanh')\nplt.plot(history_relu.history['acc'], label='2 hidden layer ReLu')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b7003299-8bf7-4be7-837b-0a4ac3af20f6","_uuid":"bb1e761121ce5d5428cecdec20d0a63e89c4a90a","trusted":false},"cell_type":"code","source":"plt.plot(history_tanh.history['loss'], label='2 hidden layer Tanh')\nplt.plot(history_relu.history['loss'], label='2 hidden layer ReLu')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72305ce1-e9a2-49a2-80e1-a4fd1e6b2f2b","_uuid":"fe9ff0cc894f5f757db9d7eb68cc2d59ac11af63"},"cell_type":"markdown","source":"## Momentum\nEarlier, we motivated gradient descent as someone trying to find the way down a hill by just following the slope of the floor. Momentum can be motivated with an analogy to physics, where a ball is rolling down the same hill. A small bump in the hill would not make the ball roll in a completely different direction. The ball has some momentum, meaning that its movement gets influenced by its previous movement. The same can be added to gradient descent. \n\nInstead of directly updating the model parameters with their gradient we update them with the moving average (technically the [exponentially weighted moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average)). We can also motivate this from statistics: To avoid that we update our parameter with an outlier gradient, we take the moving average, which will smoothen out outliers and capture the general direction of the gradient.\n![Momentum](https://storage.googleapis.com/aibootcamp/Week%202/assets/momentum.jpg)\nMathematically:\n\n$$V_{dW_t} = \\beta * V_{dW_{t-1}} + (1 - \\beta) * dW$$\n$$W = W - \\alpha * V_{dW_t}$$\n\nA typical value for beta would be 0.9, meaning that 90% of the update comes from previous gradients. In Keras we can add momentum through gradient descent by specifying a value for beta, by default it is set to 0.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"5b3c1c46-845d-4a68-9a18-029661470a57","_uuid":"df10e56f8da340b9c367fc197ab12652e2434a52","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# First hidden layer\nmodel.add(Dense(32,activation='relu',input_dim=64))\n\n# Second hidden layer\nmodel.add(Dense(16,activation='relu'))\n\n# Output layer\nmodel.add(Dense(1,activation='sigmoid'))\n\n# Setup optimizer with learning rate of 0.01 and momentum (beta) of 0.9\nmomentum_optimizer = SGD(lr=0.01, momentum=0.9)\n\n# Compile the model\nmodel.compile(optimizer=momentum_optimizer,\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n# Train\nhistory_momentum = model.fit(X_train, y_train, # Train on training set\n                             epochs=1000, # We will train over 1,000 epochs\n                             batch_size=X_train.shape[0], # Batch size = training set size\n                             verbose=0) # Suppress Keras output","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ece46f5e-cf2c-4ad5-8c06-dbf213188fad","_uuid":"9fb86e132faed52bdbb702b16d7502ccd4742a88","trusted":false},"cell_type":"code","source":"plt.plot(history_relu.history['acc'], label= '2 hidden layer ReLu')\nplt.plot(history_momentum.history['acc'], label= '2 hidden layer ReLu + Momentum')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2fa01637-2356-49d3-b2d2-419ee93bc2a0","_uuid":"aa710df97b3d9219d3db0bbe3c2114d90534814e","trusted":false},"cell_type":"code","source":"plt.plot(history_relu.history['loss'], label= '2 hidden layer ReLu')\nplt.plot(history_momentum.history['loss'], label= '2 hidden layer ReLu + Momentum')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9ea5435-54f5-4842-93bd-1531394d5989","_uuid":"4192e53babb9829b712ed88fc05a864787e8786b"},"cell_type":"markdown","source":"As you can see, with momentum, our model learned much better than before! Let's evaluate it on the dev set:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"f19869b8-314b-459a-b31d-0b8a1447f25a","_uuid":"4699d649e535fd2a6a2f336bd83b939c743c8727","trusted":false},"cell_type":"code","source":"model.evaluate(x=X_dev,y=y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"885fa94f-cb34-40db-9fe9-0b97a9c1d665","_uuid":"25fc577abe658e8f87d6ce03ec58ed5828f20f1f"},"cell_type":"markdown","source":"## The adam optimizer\n[Kingma, D. P., & Ba, J. L. (2015)](https://arxiv.org/abs/1412.6980)'s adam (adaptive momentum estimation) optimizer is another way to make gradient descent work better that has shown very good results and has therefore become a standard choice for many practitioners. First it computes the exponentially weighted average of the gradients, just like a momentum optimizer does:\n$$V_{dW_t} = \\beta_1 * V_{dW_{t-1}} + (1 - \\beta_1) * dW$$\n\nBut then it also computes the exponentially weighted average of the _squared_ gradients:\n\n$$S_{dW_t} = \\beta_2 * S_{dW_{t-1}} + (1 - \\beta_2) * dW^2$$\n\nIt then updates the model parameters like this:\n\n$$ W = W - \\alpha * \\frac{V_{dW_t}}{\\sqrt{S_{dW_t}} + \\epsilon}$$\n\nWhere $\\epsilon$ is a very small number to avoid division by zero.\n\nThis division by the root of squared gradients reduces the update speed when gradients are very large. This stabilizes learning as the learning algorithm does not get thrown off track by outliers as much. Together with adam, we got a new hyper parameter. Instead of having just one momentum factor $\\beta$ we now have two, $\\beta_1$ and $\\beta_2$. The recommended values for $\\beta_1$ and $\\beta_2$ are 0.9 and 0.999 respectively. We can use adam in keras like this:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"7a1f03d7-ad40-4742-9447-921030184178","_uuid":"ffb92996554ba16ed24951783e3ebcc7b9e8f4ae","trusted":false},"cell_type":"code","source":"from keras.optimizers import adam","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"617a8545-f86b-44ca-b715-f7d66c5b7c8a","_uuid":"e7cd01c7c60f47a3c5d27399b7c675a4fc070c7e","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# First hidden layer\nmodel.add(Dense(32,activation='relu',input_dim=64))\n\n# Second hidden layer\nmodel.add(Dense(16,activation='relu'))\n\n# Output layer stayed sigmoid\nmodel.add(Dense(1,activation='sigmoid'))\n\n# Setup adam optimizer\nadam_optimizer=adam(lr=0.1,\n                beta_1=0.9, \n                beta_2=0.999, \n                epsilon=1e-08)\n\n# Compile the model\nmodel.compile(optimizer=adam_optimizer,\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n\n# Train\nhistory_adam = model.fit(X_train, y_train, # Train on training set\n                         epochs=1000, # We will train over 1,000 epochs\n                         batch_size=X_train.shape[0], # Batch size = training set size\n                         verbose=0) # Suppress Keras output","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0a5c1627-aa47-438b-b927-4fd0f935bc13","_uuid":"e0a30190f626e8c92e26b6e0310e5a3bc47d0284","trusted":false},"cell_type":"code","source":"plt.plot(history_relu.history['acc'], label = 'Vanilla Gradient Descent')\nplt.plot(history_momentum.history['acc'], label = 'Momentum Gradient Descent')\nplt.plot(history_adam.history['acc'], label = 'Adam')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"eaff0a55-fce8-4396-bae4-33966fadff4f","_uuid":"f8b1dec16afec15b5f9579575f23b6e65a5de826","trusted":false},"cell_type":"code","source":"plt.plot(history_relu.history['loss'], label = 'Vanilla Gradient Descent')\nplt.plot(history_momentum.history['loss'], label = 'Momentum Gradient Descent')\nplt.plot(history_adam.history['loss'], label = 'Adam')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6c18661a-54d1-48f4-ae0d-19d9fbc123a5","_uuid":"c9dc13f3bb6ed21427971ef645e6e0717b0027db","trusted":false},"cell_type":"code","source":"model.evaluate(x=X_dev,y=y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"284087f4-787b-470e-96e1-549b1e0569bd","_uuid":"5f3cb0c2c00a3c560eb95ecd055396d05c900056"},"cell_type":"markdown","source":"The model learns very well on the training set, but it does less well on the dev set. It is overfitting the training set. We can counter this with a technique called regularization.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"594f2ef3-e6b1-4718-b92f-f436289b336d","_uuid":"349d3fe349950ee25ced9b29a408020395694011"},"cell_type":"markdown","source":"## Regularization\nRegularization is a technique to avoid overfitting. Overfitting is when the model fits the training data too well and does not generalize well to dev or test data. \n\n### L2 Regularization\nOne popular technique to counter this is L2 regularization. L2 regularization adds the sum of squared weights to the loss function like this:\n\n$$L(W)_{Regularized} = L(W) + \\frac{\\lambda}{2N}\\sum W^2$$\n\nWhere $N$ is the number of training examples, and $\\lambda$ is the regularization hyper parameter that influences how much we want to regularize.\n\nAdding this regularization to the loss function means that high weights increase losses and the algorithm is incentivised to reduce weights. Small weights (around zero) mean that the neural network relies less on them. Therefore, a regularized algorithm will rely less on every single feature and every single node activation and will have a more holistic view, taking into account many features and activations. This will prevent the algorithm from overfitting. A common value for $\\lambda$ is around 0.01.\n\n### L1 Regularization\nL1 regularization is very similar to L2 regularization but instead of adding the sum of squares it adds the sum of absolute values:\n\n$$L(W)_{Regularized} = L(W) + \\frac{\\lambda}{2N}\\sum||W||$$\n\nIn practice it is often a bit experimental which of the two works best, the difference is not very large.\n\n### L2 & L1 in Keras\nIn Keras, regularizers that are applied to the weights are called ``kernel_regularizer``, regularizers that are applied to the bias are called ``bias_regularizer``. You can also apply regularization directly to the activation of the nodes to prevent them from being activated very strongly with ``activity_regularizer``. Let's add some L2 regularization to our overfitting network:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"1c96c530-e9f6-4d58-a41d-1d6d2a3f130d","_uuid":"17393e3d9ce5df4a25d7905031c33accf03f34ae","trusted":false},"cell_type":"code","source":"from keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"87c04439-a61a-4855-bb09-ff67b017fb83","_uuid":"3ac33e8ee7ce83c9711b80efe7d445a31edb705e","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# First hidden layer now regularized\nmodel.add(Dense(32,activation='relu',\n                input_dim=64,\n                kernel_regularizer = regularizers.l2(0.01)))\n\n# Second hidden layer now regularized\nmodel.add(Dense(16,activation='relu',\n                   kernel_regularizer = regularizers.l2(0.01)))\n\n# Output layer stayed sigmoid\nmodel.add(Dense(1,activation='sigmoid'))\n\n# Setup adam optimizer\nadam_optimizer=adam(lr=0.1,\n                beta_1=0.9, \n                beta_2=0.999, \n                epsilon=1e-08)\n\n# Compile the model\nmodel.compile(optimizer=adam_optimizer,\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n# Train\nhistory_regularized=model.fit(X_train, y_train, # Train on training set\n                             epochs=1000, # We will train over 1,000 epochs\n                             batch_size=X_train.shape[0], # Batch size = training set size\n                             verbose=0) # Suppress Keras output","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f71ecb8a-9460-46f7-8dfe-8a1159e17d46","_uuid":"55b10a99ec68188b5219a7d3de7bf66ffec75dae","trusted":false},"cell_type":"code","source":"plt.plot(history_adam.history['acc'], label = 'No Regularization')\nplt.plot(history_regularized.history['acc'], label = 'Regularization')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1ecb0d08-0c06-4ad2-ac89-0be301786cb6","_uuid":"86ae058896017d6b2c6f84d6c32b23ea032d3d86","trusted":false},"cell_type":"code","source":"plt.plot(history_adam.history['loss'], label = 'No Regularization')\nplt.plot(history_regularized.history['loss'], label = 'Regularization')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"30b781ab-3c66-4462-a489-10406b89ef66","_uuid":"a4d5faf9a69c99368757023f2d137067e04f4c92","trusted":false},"cell_type":"code","source":"model.evaluate(x=X_dev,y=y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5408331a-b5ed-4575-bcc7-939f9850ecc9","_uuid":"61c70c62cdbf600ceb916ed0a10b60bb716b025b"},"cell_type":"markdown","source":"The regularized model achieves a lower performance on the training set but if we test it with the dev set it does significantly better. We have successfully reduced overfitting!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8fd6db22-b6b8-470a-bbcb-1bb2471bc702","_uuid":"5462e3bb45c3e7c5ce3e19d0cc4f7ac4ec74a9e0"},"cell_type":"markdown","source":"## Dropout\nAs the original paper title gives away, Dropout is [\"A Simple Way to Prevent Neural Networks from\nOverfitting\"](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). And it works by randomly removing nodes from the neural network:\n![Dropout](https://storage.googleapis.com/aibootcamp/Week%202/assets/dropout.png)\n\nWith dropout, each node has a small probability of having it's activation set to zero. This means that the learning algorithm can no longer rely heavily on single nodes, much like in L2 and L1 regularization. Dropout therefore also has a regularizing effect.\n\nIn Keras, dropout is [a new type of layer](https://keras.io/layers/core/#dropout). It is put after the activations you want to apply dropout to. It passes on activations, but sometimes it sets them to zero, achiving the same effect as dropout in the cells directly:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"ca422733-77dd-46c9-a247-1a771c239e83","_uuid":"09fddc48bfcf090672bc1d423322f60c0549a5ca","trusted":false},"cell_type":"code","source":"from keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b2480875-6317-49a3-acfb-be95ebd09f5a","_uuid":"20a493bbbff1cb85568585c2a2b6bed424f78bea","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# First hidden layer\nmodel.add(Dense(32,activation='relu',\n                input_dim=64))\n\n# Add dropout layer\nmodel.add(Dropout(rate=0.5))\n\n# Second hidden layer\nmodel.add(Dense(16,activation='relu'))\n\n\n# Add another dropout layer\nmodel.add(Dropout(rate=0.5))\n\n# Output layer stayed sigmoid\nmodel.add(Dense(1,activation='sigmoid'))\n\n# Setup adam optimizer\nadam_optimizer=adam(lr=0.1,\n                beta_1=0.9, \n                beta_2=0.999, \n                epsilon=1e-08)\n\n# Compile the model\nmodel.compile(optimizer=adam_optimizer,\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n# Train\nhistory_dropout = model.fit(X_train, y_train, # Train on training set\n                             epochs=1000, # We will train over 1,000 epochs\n                             batch_size=X_train.shape[0], # Batch size = training set size\n                             verbose=0) # Suppress Keras output","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a7702d8e-540f-4fbf-937e-6f0e7e26e99f","_uuid":"2be21272c354a4efc17604fe2c83deb60c38fffb","trusted":false},"cell_type":"code","source":"plt.plot(history_adam.history['acc'], label = 'No Regularization')\nplt.plot(history_regularized.history['acc'], label = 'L2 Regularization')\nplt.plot(history_dropout.history['acc'], label = 'Dropout Regularization')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6a9936b5-a12f-4a66-a4ca-7f3eb4b52e43","_uuid":"5c6214015f4a89ba180d3586601ce4744b53bb54","trusted":false},"cell_type":"code","source":"plt.plot(history_adam.history['loss'], label = 'No Regularization')\nplt.plot(history_regularized.history['loss'], label = 'L2 Regularization')\nplt.plot(history_dropout.history['loss'], label = 'Dropout Regularization')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07f2b566-0f48-4e57-8c46-4e4cd87367d9","_uuid":"26abf6a84b1018a394fb84d91c87f79d355a9618"},"cell_type":"markdown","source":"Note that when the model is evaluated or used to make actual predictions dropout should be deactived to get the best results. Keras does this automatically.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"9eb8ddfe-88dc-44f8-ae52-e1e2906efc68","_uuid":"13e71bf1b8a06be64620087789d722b35c3ec797","trusted":false},"cell_type":"code","source":"model.evaluate(x=X_dev,y=y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f3d3c40-1f6c-45eb-ad2e-73a0a4d60c41","_uuid":"78fe2b1761338b2e3969d84d0ed07f608f77d83d"},"cell_type":"markdown","source":"## Batchnorm\nRemember how we normalized our input in chapter 9? \n![Feature Scaling](https://storage.googleapis.com/aibootcamp/Week%202/assets/feature_scaling.jpg)\n\nThat made it easier for our learning algorithm because the loss function was more 'round'. What was true for features is true inside the neural network as well. When we add normalization into the neural network, the mean and variance that are required for normalization get calculate from the batch that the network is currently training on, not the whole dataset. This is why this process is called batch normalization, or batchnorm.\n\nIn Keras, batchnorm is a new layer that gets put behind the linear step and before the normalization. This requires is the separate the linear step and activation in Keras.\n\n### Batchnorm VS Feature scaling\n\nBatchnorm and feature scaling have many similar traits. They are often used together.\n\n|Batchnorm|Feature Scaling|\n|---------|---------------|\n|Applied to layer activations|Applied to input only|\n|Adaptively calculates mean and standard deviation|Mean & Std caluclated once over dataset|","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"4b94bd39-410f-498e-8504-227a4914948d","_uuid":"08ae6993e5315b0f622fdc6aecf9515685a7c0f9","trusted":false},"cell_type":"code","source":"from keras.layers import BatchNormalization\nfrom keras.layers import Activation","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"dcba80d7-675e-407d-bfeb-0119f80b32a8","_uuid":"a59964048262dc8145f8af73716c128a4df496b9","trusted":false},"cell_type":"code","source":"# Sequential model\nmodel = Sequential()\n\n# Input layer linear step\nmodel.add(Dense(32, input_dim=64))\n\n# Input layer normalization\nmodel.add(BatchNormalization())\n\n# Input layer activation\nmodel.add(Activation('relu'))\n\n# Add dropout layer\nmodel.add(Dropout(rate=0.5))\n\n# hidden layer linear step\nmodel.add(Dense(16))\n\n# Hidden layer normalization\nmodel.add(BatchNormalization())\n\n# Hidden layer activation\nmodel.add(Activation('relu'))\n\n# Add another dropout layer\nmodel.add(Dropout(rate=0.5))\n\n# Output layer, \nmodel.add(Dense(1))\n\n# Output normalization\nmodel.add(BatchNormalization())\n\n# Output layer activation\nmodel.add(Activation('sigmoid'))\n\n# Setup adam optimizer\nadam_optimizer=adam(lr=0.1,\n                beta_1=0.9, \n                beta_2=0.999, \n                epsilon=1e-08)\n\n# Compile the model\nmodel.compile(optimizer=adam_optimizer,\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\n\n# Train\nhistory_batchnorm = model.fit(X_train, y_train, # Train on training set\n                             epochs=1000, # We will train over 1,000 epochs\n                             batch_size=X_train.shape[0], # Batch size = training set size\n                             verbose=0) # Suppress Keras output","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"17e97833-efd4-4684-b2f6-6f1647c16911","_uuid":"ea43bbcf5448f30d128fd1ed93c7c39429d23116","trusted":false},"cell_type":"code","source":"plt.plot(history_dropout.history['acc'], label = 'Dropout')\nplt.plot(history_batchnorm.history['acc'], label = 'Dropout + Batchnorm')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9d986fdf-bd92-4458-a323-1dbf62beb3f9","_uuid":"1d37a18f1b3229f9781483cab97483d76907e4f6","trusted":false},"cell_type":"code","source":"plt.plot(history_dropout.history['loss'], label = 'Dropout')\nplt.plot(history_batchnorm.history['loss'], label = 'Dropout + Batchnorm')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f8608724-673b-4462-8e77-39a37096d6aa","_uuid":"b71f9309383462b123739ab1d06b9c0d1ba4136b","trusted":false},"cell_type":"code","source":"model.evaluate(x=X_dev,y=y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e8e6a70c-8f9f-4f79-8f5a-8d7d4148ed1e","_uuid":"b91fda4603a0cec12797edbcebc494a36b07b593"},"cell_type":"markdown","source":"## Summary\nAnd thus concludes our little safari through the wonderful world of things you can add to your vanilla neural network. There are many more [optimizers](https://keras.io/optimizers/), [activation functions](https://keras.io/activations/) and things we have not even touched at such as better [random initializers](https://keras.io/initializers/). The tools shown here however are kind of the evergreens that get used quite often.","outputs":[],"execution_count":null}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}