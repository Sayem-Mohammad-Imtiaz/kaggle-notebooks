{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport random\n\nwarnings.filterwarnings('ignore')\nsns.set()\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"**Motivation**: When we have a large number of predictors it might not all of them that are useful for our algorithm to learn their relationship with target. Moreover, having a large number of predictors can lead to overfit and take much more time to converge. Thus, it would be good if we could discard the bad predictors, the ones that are lack of explanatory power, and leave model only useful ones.<br>\n\nTo have a clear picture, consider the following example dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"example_1 = pd.DataFrame()\n\ndef my_func(x):\n    return (5*x**3+10+np.random.uniform(low=0.1, high=5000))/10\n\nexample_1['Predictor_1'] = np.random.uniform(low=1, high=20, size=100)\nexample_1['Predictor_2'] = np.random.uniform(low=1, high=20, size=100)\nexample_1['Target'] = example_1['Predictor_1'].apply(my_func)\n\nexample_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I defined the example_1 DataFrame which have 2 continuous predictors('Predictor_1', 'Predictor_1') and one continuous target('Target').\nWe can clearly see their relationship in the scatterplot below. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, sharey=True)\nfig.set_size_inches(13,5)\nsns.scatterplot(data=example_1, x='Predictor_1', y='Target', ax=ax[0])\nsns.scatterplot(data=example_1, x='Predictor_2', y='Target', ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we see the relationships, the first thought is that 'Predictor_2' is useless in guessing the relationship between predictors and 'Target'. That said even we know 'Predictor_2' it doesn't seem to be helpful.  The question is, should we discard it?. <br>\n\n\nThe reason why we feel in that way is we visually see that 'Predictor_1' and 'Target' are highly correlated in someway. But not for 'Predictor_2', it seems to be a mess, random, and not related to'Target'. <br>\n\nLet's see another example."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from scipy.stats import norm\nexample_2 = pd.DataFrame()\n\nexample_2['Gender'] = random.choices(['Male','Female'], k=100)\nexample_2['Graduated'] = random.choices(['Yes','No'], k=100)\nexample_2['Salary'] = norm.rvs(loc=1000,scale=50,size=100)\nexample_2['Salary'] =example_2.apply(lambda x:x[2]+100 if x[1]=='Yes' else x[2]-100, axis=1)\n\nexample_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, sharey=True)\nfig.set_size_inches(13, 5)\nsns.boxplot(data=example_2, x='Gender', y='Salary', ax=ax[0])\nsns.boxplot(data=example_2, x='Graduated', y='Salary', ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, the first thought is that 'Gender' makes no different in 'Salary'. That said, it's no matter that one is male or female, his/her salary is almost the same. But not for 'Graduated', we clearly see that if one is graduated, his/her salary is obviously much higher than who isn't. The question is, since 'Gender' make no differences, should we discard it?"},{"metadata":{},"cell_type":"markdown","source":"If we carefully consider every single relationship between each predictor and target, we will definitely have our own guesses. <br>\nBut **\"first thought or guessing\"** are bad. It's not scientific. In this notebook, I'll try to illustrate some common methods that are more reliable, scientific than that.\n"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Feature selection <br>\nis the process of reducing the number of input variables when developing a predictive model. The goal of feature selection in machine learning is to find the best set of features that allows one to build useful models <br>\nThe techniques for feature selection in machine learning can be broadly classified into the following categories:<br>\n1. **Filter methods** : These methods are more reliable methods I said earlier. They select subsets of features based on their relationship with the target using statistical test. Filter methods pick feature subset by not including any machine-learning algorithm. \n<!-- ![Alt Text](image path \"title\") -->\n![Alt Text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537552825/Image3_fqsh79.png \"Optional Title\")\n\n2. **Wrapper methods** : Different from the first ones, rather than using statistical test, these methods take advantages of machine-learning algorithms since many of them internally identify which predictors are good for performance. Wrapper method uses one machine learning algorithm and its performance to evaluate features' importance. This method searches for a feature which is best-suited for the machine learning algorithm and aims to improve the performance.\n<!-- ![Alt Text](image path \"title\") -->\n![Alt Text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537549832/Image2_ajaeo8.png \"Optional Title\")\n\n\n3. **Embedded methods** : Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 1) Filter methods <br>\nLet's first state the common data types:\n- **Numerical Variables**: Integer, Floating point\n\n\n- **Categorical Variables**\n    - Boolean, Binary\n    - Ordinal : Categories with implied order\n    - Norminal : Named categories\n\n**___SUMMARY OF FILTER METHODS** ___<br>\n\nThe following cases are all possible cases you might encounter when trying to build some predictive model.\n\n## 1.1) Numerical Input, Numerical Output\n\n- Pearson’s correlation coefficient (linear) -> [sklearn.feature_selection.f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html \"sklearn doc\")<br>\n- Spearman’s rank coefficient (nonlinear)  -> [scipy.stats.spearmanr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)\n\n## 1.2) Numerical Input, Categorical Output\n\n- ANOVA correlation coefficient (linear) -> [sklearn.feature_selection.f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html \"Compute the ANOVA F-value for the provided sample.\")<br>\n- Kendall’s rank coefficient (nonlinear) -> [scipy.stats.kendalltau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html)<br>\n\nNote: Kendall does assume that the categorical variable is ordinal. <br>\n\n\n## 1.3)Categorical Input, Categorical Output\n\n- Chi-Squared test -> [sklearn.feature_selection.chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n- Mutual Information, Information gain -> [sklearn.feature_selection.mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) for a discrete target variable,  [sklearn.feature_selection.mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html) for a continuous target variable. <br>\n\n## 1.4) Categorical Input, Numerical Output\nThis is a regression problem with categorical input variables.\nThis is a strange example of a regression problem (e.g. you would not encounter it often). <br>"},{"metadata":{},"cell_type":"markdown","source":"___"},{"metadata":{},"cell_type":"markdown","source":"In this section, I'll show some examples for each case using 2 dataset."},{"metadata":{},"cell_type":"markdown","source":"### \"\"HR-Analytics\" Dataset used for classification problem. <br>\n- y_class : target variable of classification problem\n- X_category_class : categorical predictors of classification problem\n- X_numeric_class : numerical predictors of classification problem\n\nNote: for X_category_class, I'll use just 4 variables *(\"company_size\", \"company_type\", \"last_new_job\", and \"training_hours\")* just for simplicity."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"data_classification = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ndata_classification.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess a little bit."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"category = data_classification.select_dtypes(include='object')\nnumeric = data_classification.select_dtypes(exclude='object')\n\n# Let's preprocess real quick.\n\n# Deal with missing values: introduce new class, \"No\", for missing value.\ncategory_notNull = category.replace(np.nan, \"No\")\n\n# recheck for missing values\nassert category_notNull.isnull().any().any() == False\nassert numeric.isnull().any().any() == False\n\n# Declare predictor(X) and dependent variable(y) for classificaiton problem (suffix '_class').\ny_class = numeric['target']\nX_category_class = category_notNull.copy().iloc[:,1:6]\nX_numeric_class = numeric.drop('target',axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"50-Startups Profit\" Dataset used for regression problem. <br>\n- y_reg : target variable of regression prblem\n- X_category_reg : categorical predictors of regression prblem\n- X_numeric_reg : numerical predictors of regression prblem\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_regression = pd.read_csv('/kaggle/input/startup/50_Startups.csv')\ndata_regression.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess a little bit."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Declare predictor(X) and dependent variable(y) for regression problem (suffix '_reg')\ny_reg = data_regression['Profit']\nX_category_reg = data_regression.iloc[:,-2]\nX_numeric_reg = data_regression.iloc[:,:-2]\n\n# Check for missing values\nassert X_category_reg.isnull().any().any() == False\nassert X_numeric_reg.isnull().any().any() == False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1) Numerical Input, Numerical Output"},{"metadata":{},"cell_type":"markdown","source":"For regression problem, we should first check for a correlation between numerical variables and target.\nGood variables are highly correlated with the target. Furthermore, variables should be correlated with the target but should be uncorrelated among themselves.\n\nIf two variables are correlated, we can predict one from the other. Therefore, if two features are correlated, the model only really needs one of them, as the second one does not add additional information."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([X_numeric_reg, y_reg],axis=1).corr().style.background_gradient(sns.light_palette('green', as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that \"R&D Spend\" is highly correlated to our target (\"Profit\") and can be a good variable. But if it's due by chance? We should do a statistical test to be confident to say that."},{"metadata":{},"cell_type":"markdown","source":"- Pearson’s correlation coefficient (linear) -> [sklearn.feature_selection.f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html \"sklearn doc\")<br>\n- Spearman’s rank coefficient (nonlinear)  -> [scipy.stats.spearmanr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import f, spearmanr, pearsonr\n\ndef f_test(X, y, target, mode):\n    \n    # Do F-test using Pearson correlation\n    if mode.lower() == 'pearsonr':\n        \n        temp = []\n        for col in X.columns:\n            corr, _ = pearsonr(X[col].values, y)\n            temp.append(corr)\n\n        df = pd.DataFrame(temp, columns=['pearsonr'], index=X.columns)\n    \n    \n    # Do F-test using Pearson's rank correlation\n    elif mode.lower() == 'spearmanr':\n        \n        corr, _ = spearmanr(X, y_reg)\n        df = pd.DataFrame(corr[:-1,-1], columns=['spearman'], index=X.columns)\n        \n    else:\n        return\n    \n    \n    degree_of_freedom = len(y)-2\n    df['F-score'] = df.iloc[:,0].apply(lambda x: x**2/(1-x**2)*degree_of_freedom)\n    df['p-value'] = f.sf(df['F-score'], 1, degree_of_freedom)\n    return df\n\nf_test(X_numeric_reg, y_reg, target='Profit', mode='pearsonr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or more simply, you can use [sklearn.feature_selection.f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) which is done exactly the same thing: compute correlation and do F-test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression\n\nF_score, p_value = f_regression(X_numeric_reg, y_reg)\ndf = pd.DataFrame(list(zip(X_numeric_reg.columns, F_score, p_value)), \n                  columns=['feature', 'F-score', 'p-value'])\ndf.set_index('feature')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that \"R&D Spend\" is highly linearly correlated with our target variable(\"Profit\"). Let's plot the graph to ensure that."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3, sharey=True)\nfig.set_size_inches(20,5)\n\nax[0].scatter(X_numeric_reg['R&D Spend'], y_reg)\nax[0].set_xlabel('R&D Spend')\nax[0].set_ylabel('Profit')\nax[1].scatter(X_numeric_reg['Marketing Spend'], y_reg)\nax[1].set_xlabel('Marketing Spend')\nax[2].scatter(X_numeric_reg['Administration'], y_reg)\nax[2].set_xlabel('Administration')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation and F-score of 'R&D Spend' and 'Marketing Spend' are very high relative to the others. Moreover, their p-value are very small (<0.01). Thus, we can conclude that those 2 features are much more important than 'Adminstration'."},{"metadata":{},"cell_type":"markdown","source":"### Next, I'll try non-linear transformation of 'R&D Spend' and see what will happen. <br>\nI will transform 'R&D Spend' using this non-linear function: \n\n$$f(x) = ln(x+1)$$\n\nNow, we introduce non-linear relationship between non-linearly transformed R&D Spend(namely 'Non-linear R&D') and 'Profit'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Non-linearly transform\nX_numeric_reg_copy = X_numeric_reg.copy()\nX_numeric_reg_copy['Non-linear R&D'] = X_numeric_reg_copy['R&D Spend'].apply(lambda x:np.log(x+1))\n\nplt.figure(figsize=(10,5))\nplt.scatter(X_numeric_reg_copy['Non-linear R&D'], y_reg)\nplt.xlabel('Non-linear R&D')\nplt.ylabel('Profit')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_test(X_numeric_reg_copy, y_reg, target='Profit', mode='pearsonr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that after we non-linearly transform 'R&D Spend' into 'Non-linear R&D'. Its F-score decreases very quickly and its p-value also increases. It's because \"Pearson\" correlation is a statistic that measures **linear correlation** between two variables X and Y."},{"metadata":{},"cell_type":"markdown","source":"### Spearman's rank correlation coefficient <br>\nis a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described in a **monotonic function**.\n\nWhile Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not)."},{"metadata":{"trusted":true},"cell_type":"code","source":"f_test(X_numeric_reg_copy, y_reg, target='Profit', mode='spearmanr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we see that spearman's rank gives \"R&D Spend\" and \"Non-linear R&D\" exact same coefficient."},{"metadata":{},"cell_type":"markdown","source":"___"},{"metadata":{},"cell_type":"markdown","source":"## 1.2) Numerical Input, Categorical Output\n\n- ANOVA correlation coefficient (linear) -> [sklearn.feature_selection.f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html \"Compute the ANOVA F-value for the provided sample.\")<br>\n- Kendall’s rank coefficient (nonlinear) -> [scipy.stats.kendalltau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html)<br>\n\nNote: Kendall does assume that the categorical variable is ordinal. <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import StandardScaler\n\nF_score, p_value = f_classif(X_numeric_class, y_class)\n\ndf = pd.DataFrame([F_score,p_value], index=['F-score','p-value'], columns=X_numeric_class.columns).T\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3)Categorical Input, Categorical Output\n\n- Chi-Squared test -> [sklearn.feature_selection.chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n- Mutual Information, Information gain -> [sklearn.feature_selection.mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) for a discrete target variable,  [sklearn.feature_selection.mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html) for a continuous target variable."},{"metadata":{},"cell_type":"markdown","source":"### Chi-square test <br>\nIf you don't familiar with Chi-square testing, this [link](https://www.youtube.com/watch?v=VskmMgXmkMQ) have a great, brief explanation for you!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import chi2\n\nX_category_dummies = pd.get_dummies(X_category_class)\n\nscore, pval = chi2(X_category_dummies, y_class)\nfeature_importance_chi2 = pd.Series(score, index=X_category_dummies.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots()\nfig.set_size_inches(15,6)\nax.barh(feature_importance_chi2.sort_values().index, feature_importance_chi2.sort_values().values)\nplt.xticks(rotation=90)\nplt.title('Chi-square Test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Information gain <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\nimportance = mutual_info_classif(X_category_dummies, y_class)\n\nfeature_importance_Ig = pd.Series(importance, index=X_category_dummies.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots()\nfig.set_size_inches(15,6)\nax.barh(feature_importance_Ig.sort_values().index, feature_importance_Ig.sort_values().values)\nplt.xticks(rotation=90)\nplt.title('Information Gain for each feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 2) Wrapper methods"},{"metadata":{},"cell_type":"markdown","source":"## 2.1) Forward selection\nThe procedure starts with an empty set of features(reduced set). The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the that set.\n\n## 2.2) Backward elimination\nThe procedure starts with the full set of features. At each step, it removes the worst attribute remaining in the set.\n\n## 2.3) Recursive feature elimination (RFE)<br>\nis a feature selection method that recursively fits a model and removes the weakest feature until the specified number of features is reached. Features are ranked by the model’s coef\\_ or feature\\_importances\\_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model. The choice of model does not matter too much as long as it is skillful and consistent."},{"metadata":{},"cell_type":"markdown","source":"sklearn.feature_selection.RFE  \n    - .ranking_ attribute that gives a rank of each feature \n    - .support_ attribute that is mark True for selected features and False otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\ndef do_RFE(X_category, X_numeric, y, n, estimator):\n    \n    X = pd.concat([X_category, X_numeric], axis=1)\n\n    rfe = RFE(estimator, n_features_to_select=n).fit(X, y)\n    \n    return sorted([(x,y) for (y,x) in list(zip(X.columns,rfe.ranking_))]), X.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"HR analytics\" dataset classification problem\nrank, selected_features = do_RFE(pd.get_dummies(X_category_class), X_numeric_class, y_class, 20, LogisticRegression())\nselected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"50-Startups\" dataset regression problem\nrank, selected_features = do_RFE(pd.get_dummies(X_category_reg), X_numeric_reg, y_reg, 1, RandomForestRegressor())\nrank","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n---"},{"metadata":{},"cell_type":"markdown","source":"# 3) Embedded methods\n\n\n\n## 3.1) Regularization <br>\n\nIn Linear Regression, the predicted values($\\hat{y}$) are calculated by $\\hat{y} = XW_1+W_0$ \nwhere $X$ is data matrix, $W_1$ is a weight vector($W_1 = [w_1,w_2,w_3,...w_p]$), $W_0$ is a bias.\nWe obtain $W_1$ and $W_0$ by trying to minimize sum of square residuals to find the best fit line. i.e. minimizing \n$$\\sum_{i=1}^{n}(\\underbrace{y_i-\\hat{y_i}}_{residual})^{2}$$\n\n### L1-Regularization\n\nLASSO(Least Absolute Shrinkage and Selection operator)\nminimzes $$\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2 + \\alpha\\sum_{j=1}^{p}|w_j|$$\nThe second term is called penalization term. It's there to prevent overfitting. In the fitting process, LASSO automatically sets some coefficients to zero. Generally, these are the weak predictor(s). This is a form of built-in(embedded) feature-selection. The variables with non-zero coefficients were the ones selected.\nSince all the coefficients will be multiplied by the same number ($\\alpha$),it's important that all the predictors are on the same scale. Otherwise, some variables will be unfairly penalized more than others. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge, Lasso, LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\ndef get_embedded_report(X_numeric, X_category, y, estimator):\n    \n    # First normalize our predictors\n    X_numeric_scaled = StandardScaler().fit_transform(X_numeric)\n    y_scaled     = StandardScaler().fit_transform(y.values.reshape(-1,1))\n\n    # Save the columns name into variable named 'features'\n    X_category_dummies = pd.get_dummies(X_category)\n    features = list(X_numeric_reg.columns) + list(X_category_dummies.columns)\n\n    # Fit the regressor\n    X = np.concatenate([X_numeric_scaled, X_category_dummies.values], axis=1)\n\n    model = estimator.fit(X, y_scaled)\n    try:\n        c = model.coef_\n    except:\n        c = model.feature_importances_\n    finally:\n        return c, features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using L1-regularization\ncoef, features = get_embedded_report(X_numeric_reg, X_category_reg, y_reg, Lasso(alpha=0.1))\nlist(zip(features,coef))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using L2-regularization\ncoef, features = get_embedded_report(X_numeric_reg, X_category_reg, y_reg, Ridge())\nlist(zip(features,coef[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using feature_importances_'s DecisionTree\nfrom sklearn.tree import DecisionTreeRegressor\n\ncoef, features = get_embedded_report(X_numeric_reg, X_category_reg, y_reg, DecisionTreeRegressor())\nlist(zip(features,coef))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## There're more useful tools in sklearn.feature_selection\n- SelectPercentile(score_func ex: f_classif, percentile: int) : Select features according to a percentile of the highest scores.\n- SelectKBest(score_func, k: int,\"all\") : Select features according to the k highest scores.\n- VarianceThreshold(threshold: Float) : Feature selector that removes all low-variance features.\n\n***Thank you for checking this notebook!! If there're mistakes, please let me know. <br> Feel free to give any advices in the comment section :)***"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Reference\n- https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/ <br>\n- https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/ <br>\n- https://www.datacamp.com/community/tutorials/feature-selection-python <br>\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}