{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"7eTqpzIKg6sk"},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"id":"MlURIHIrg8WX","outputId":"1493fff5-9565-4508-f034-cdefa91161f1","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math \nimport matplotlib.pyplot as plt\n\nimport matplotlib.dates as dt\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, LSTM, Dropout, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport time\nfrom datetime import datetime, timedelta\n","execution_count":null,"outputs":[]},{"metadata":{"id":"dqjByd23g9Jm"},"cell_type":"markdown","source":"# Load Datasets"},{"metadata":{"id":"Q0amgwNDhA3D","trusted":true},"cell_type":"code","source":"df_confirmed = pd.read_csv(\"../input/ece657aw20asg4coronavirus/time_series_covid19_confirmed_global.csv\")\ndf_recovered = pd.read_csv(\"../input/ece657aw20asg4coronavirus/time_series_covid19_recovered_global.csv\")\ndf_deaths = pd.read_csv(\"../input/ece657aw20asg4coronavirus/time_series_covid19_deaths_global.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"DIT0URXSha7-","outputId":"814144d7-fce8-4116-dd89-d9e286a8dd6b","trusted":true},"cell_type":"code","source":"df_deaths.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"TCimVoo9heAq"},"cell_type":"markdown","source":"# Data preprocessing\n\nWe drop the unwanted columns which we won't be using for our neural network computations."},{"metadata":{"id":"NnAxyiTkhftm","trusted":true},"cell_type":"code","source":"def restructure(df):\n    df['Country'] = df['Country/Region'].map(str) + '_' + df['Province/State'].map(str)\n    df =  df.drop(['Province/State', 'Country/Region' , 'Lat' , 'Long'], axis=1)\n    df = df.set_index('Country')\n    df = df.T\n    df = df.fillna(0)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"Nt69dSPxSd4B"},"cell_type":"markdown","source":"Next, we take each dataset for confirmed cases, recovered cases and deaths and convert them into a time-series dataframe"},{"metadata":{"id":"qGG4oXcnib_W","trusted":true},"cell_type":"code","source":"# Create dataframes for each category\nconfirmed = restructure(df_confirmed)\nconfirmed.index = pd.to_datetime(confirmed.index)\n\nrecovered = restructure(df_recovered)\nrecovered.index = pd.to_datetime(recovered.index)\n\ndeaths = restructure(df_deaths)\ndeaths.index = pd.to_datetime(deaths.index)\n\n# Create dataframes for the world\nworld_conf = confirmed.sum(axis=1)\nworld_recv = recovered.sum(axis=1)\nworld_dead = deaths.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"mGtbyIcFKRlD","outputId":"8837101e-cae8-4dba-cca3-d435f63a63a0","trusted":true},"cell_type":"code","source":"world_conf.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"By6jGtxPq2BC"},"cell_type":"markdown","source":"# Plots\n\nWe plot the cases in 3 countries and also plot total worldwide cases"},{"metadata":{"id":"WvmgDHOfmWTo","trusted":true},"cell_type":"code","source":"# Creating a new dataframe for Countries\nitaly = pd.DataFrame()\nindia = pd.DataFrame()\ngermany = pd.DataFrame()\n\nitaly['Confirmed'] = confirmed['Italy_nan']\nitaly['Recovered'] = recovered['Italy_nan']\nitaly['Deaths'] = deaths['Italy_nan']\n\n\nindia['Confirmed'] = confirmed['India_nan']\nindia['Recovered'] = recovered['India_nan']\nindia['Deaths'] = deaths['India_nan']\n\ngermany['Confirmed'] = confirmed['Germany_nan']\ngermany['Recovered'] = recovered['Germany_nan']\ngermany['Deaths'] = deaths['Germany_nan']\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"3PLjG6WimWYE","outputId":"e3c3a2d2-506e-4039-8ad3-a8add0028823","trusted":true},"cell_type":"code","source":"# Plots for Italy, India, Germany and The WORLD\nfig = plt.figure(figsize=(18,15))\n\nplt.subplot(2,2,1)\nplt.plot(italy.Confirmed, label='Confirmed Cases')\nplt.plot(italy.Recovered, label='Recovered Cases')\nplt.plot(italy.Deaths, label='Deaths')\nplt.xlabel('Dates', fontsize=15)\nplt.ylabel('Population Count', fontsize=15)\nplt.title('Statistics of ITALY', fontsize=15)\nplt.legend()\n\nplt.subplot(2,2,2)\nplt.plot(india.Confirmed, label='Confirmed Cases')\nplt.plot(india.Recovered, label='Recovered Cases')\nplt.plot(india.Deaths, label='Deaths')\nplt.xlabel('Dates', fontsize=15)\nplt.ylabel('Population Count', fontsize=15)\nplt.title('Statistics of INDIA', fontsize=15)\nplt.legend()\n\nplt.subplot(2,2,3)\nplt.plot(germany.Confirmed, label='Confirmed Cases')\nplt.plot(germany.Recovered, label='Recovered Cases')\nplt.plot(germany.Deaths, label='Deaths')\nplt.xlabel('Dates', fontsize=15)\nplt.ylabel('Population Count', fontsize=15)\nplt.title('Statistics of GERMANY', fontsize=15)\nplt.legend()\n\nplt.subplot(2,2,4)\nplt.plot(world_conf.iloc[:], label='World Confirmed Cases ')\nplt.plot(world_recv.iloc[:], label='World Recovered Cases')\nplt.plot(world_dead.iloc[:], label='World Deaths')\nplt.xlabel('Dates', fontsize=15)\nplt.ylabel('Population Count', fontsize=15)\nplt.title('Statistics of World', fontsize=15)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"n7Drq35VU2vT"},"cell_type":"markdown","source":"Now we plot Global Top 10 regions in each category"},{"metadata":{"id":"-_nDUFxb7Zr8","outputId":"ae24beab-7695-4f0e-cfc2-579ced5aa9f9","trusted":true},"cell_type":"code","source":"# Plot Global top 10 regions\n\n# fig = plt.figure(figsize=(12,18))\n\nconf_sort = confirmed.reindex(confirmed.max().sort_values(ascending=False).index, axis=1)\nconf_top = conf_sort.iloc[:,0:10]\n\nrec_sort = recovered.reindex(recovered.max().sort_values(ascending=False).index, axis=1)\nrec_top = rec_sort.iloc[:,0:10]\n\ndead_sort = deaths.reindex(deaths.max().sort_values(ascending=False).index, axis=1)\ndead_top = dead_sort.iloc[:,0:10]\n\nconf_top.plot(figsize=(12,5))\nplt.title('Confirmed Cases in most affected areas', fontsize=15)\n\nrec_top.plot(figsize=(12,5))\nplt.title('Recovered Cases in most affected areas', fontsize=15)\n\ndead_top.plot(figsize=(12,5))\nplt.title('Deaths in most affected areas', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"id":"nB9fDH_N2e11"},"cell_type":"markdown","source":"# Italy\n\nHere, we will be training an LSTM Model on our time-series data.\nWe will split our data in each category into train and test. <br>\nThen we will try training our model and plot the predicted train and test values. <br>\nOnce satisfied with the performance, we will use the same model to compute future trends and predict the spread of virus among the population for a set number of days."},{"metadata":{"id":"6lkxW9WZWYwj","outputId":"9d3c4d7e-1a2e-45b1-8caa-b09ae562f3ac","trusted":true},"cell_type":"code","source":"italy.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"UxHgm3Lhf9b_","trusted":true},"cell_type":"code","source":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back):\n    dataX = []\n    dataY = []\n    print(len(dataset))\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back)]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back])\n        \n    return np.array(dataX), np.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{"id":"uOjohlcNmQag","trusted":true},"cell_type":"code","source":"italy_confirmed = italy.iloc[:,0]\n\nitaly_recovered = italy.iloc[:,1]\n\nitaly_dead = italy.iloc[:,2]","execution_count":null,"outputs":[]},{"metadata":{"id":"9vpwUJj_2Yr5"},"cell_type":"markdown","source":"### For Confirmed Cases"},{"metadata":{"id":"o7fmslmFWJs_","outputId":"993eeb4e-8ab0-4f45-86c1-f7e4140a0cf3","trusted":true},"cell_type":"code","source":"# Split the series for training and testing\nsize = italy_confirmed.shape[0]\ntr =int(round(size*0.8))\nX_train, X_test = italy_confirmed[:tr] , italy_confirmed[tr:]\n\n# Reshape the series for further computations\nX_train = X_train.values.reshape(-1,1)\nX_test = X_test.values.reshape(-1,1)\n\n# Create train and test windows\nlook_back = 6\ntrainX, trainY = create_dataset(X_train, look_back)\ntestX, testY = create_dataset(X_test, look_back)\n\n# reshape input to be [samples, time steps, features] for LSTM\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"hOdM42tvVkgN"},"cell_type":"markdown","source":"Train the LSTM Model"},{"metadata":{"id":"SxiE9BcalDsi","outputId":"ecbba6d7-7eb0-47d5-b947-d7026201b938","trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\n\nmodel.add(LSTM(30, input_shape=(1, look_back), activation='relu', dropout=0.2))\n\nmodel.add(Dense(1, activation=LeakyReLU(alpha=0.1)))\nmodel.summary()\n\nopt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )\n\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n\nstart = time.time()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)\nend = time.time()\n\nruntime = end-start\nprint('Runtime: ', runtime, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"id":"pkOJEpB858t1","trusted":true},"cell_type":"code","source":"# Make predictions using LSTM Model\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\n# shift train predictions for plotting\nsize = len(italy_confirmed)\n\n# Create NULL arrays\ntrainPredictPlot = np.zeros(size)\ntestPredictPlot = np.zeros(size)\n\nfor i in range(size):\n    trainPredictPlot[i] = np.nan\n    testPredictPlot[i] = np.nan\n\n# Add predicted values to new arrays\n\nfor i in range(len(trainPredict)):\n    trainPredictPlot[look_back + i] = trainPredict[i]\n\nfor i in range(len(testPredict)):\n    testPredictPlot[len(trainPredict)+(look_back*2)+ i : size-1] = testPredict[i]\n\n# Create Dataframes for each and merge everything\ntrainPredictPlot = pd.DataFrame(trainPredictPlot, columns=['Train Predictions'])\ntestPredictPlot = pd.DataFrame(testPredictPlot, columns=['Test Predictions'])\n\nitaly_conf = pd.DataFrame(italy_confirmed.values.astype(\"float\"), columns=['Actual Confirmed'])\nitaly_conf = italy_conf.join(trainPredictPlot)\nitaly_conf = italy_conf.join(testPredictPlot)\nitaly_conf.index = italy_confirmed.index","execution_count":null,"outputs":[]},{"metadata":{"id":"3czDakiqNQIu"},"cell_type":"markdown","source":"### Future predictions"},{"metadata":{"id":"FEN2xaeYOdxl","trusted":true},"cell_type":"code","source":"dates_range = 15\nitaly_conf_preds= italy_confirmed.copy()\nlength = italy_confirmed.shape[0]\nitaly_conf_preds = italy_conf_preds.reset_index()\n\npreds = np.zeros(dates_range)\ndatelist = pd.date_range(italy.index[-1], periods=31)\n\nfor i in range(dates_range-1):\n    col = italy_conf_preds['Confirmed']\n    value = col[-look_back:]\n    value = value.values.reshape(1, 1, look_back)\n    preds = model.predict(value)\n    df = pd.DataFrame([[datelist[i+1], preds[0,0]]], columns=['index', 'Confirmed'] )\n    italy_conf_preds = italy_conf_preds.append(df, ignore_index=True)\n\nitaly_conf_preds = italy_conf_preds.set_index('index')","execution_count":null,"outputs":[]},{"metadata":{"id":"R3SHMXMwNT6i","outputId":"47710b6b-37d9-4a9c-9390-74f6bb71ee57","trusted":true},"cell_type":"code","source":"italy_conf_preds.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"77GpK91538NB"},"cell_type":"markdown","source":"### For Recovered Cases"},{"metadata":{"outputId":"cb428683-287e-478a-e098-0aa1a3047210","id":"XgNZST3y4H-o","trusted":true},"cell_type":"code","source":"# Split the series for training and testing\nsize = italy_recovered.shape[0]\ntr =int(round(size*0.8))\nX_train, X_test = italy_recovered[:tr] , italy_recovered[tr:]\n\n# Reshape the series for further computations\nX_train = X_train.values.reshape(-1,1)\nX_test = X_test.values.reshape(-1,1)\n\n\n# Create Windows\nlook_back = 6\ntrainX, trainY = create_dataset(X_train, look_back)\ntestX, testY = create_dataset(X_test, look_back)\n\n# reshape input to be [samples, time steps, features] for LSTM\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"outputId":"6e384e7a-373d-42fa-a2e6-f372c6812c18","id":"JJSWhUzh4J6M","trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\n\nmodel.add(LSTM(40, input_shape=(1, look_back), activation='relu', dropout=0.2))\n\nmodel.add(Dense(1, activation=LeakyReLU(alpha=0.1)))\nmodel.summary()\n\nopt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )\n\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n\nstart = time.time()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)\nend = time.time()\n\nruntime = end-start\nprint('Runtime: ', runtime, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"id":"mi9OUZwO4XCK","trusted":true},"cell_type":"code","source":"# Make predictions using LSTM Model\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\n# shift train predictions for plotting\nsize = len(italy_recovered)\n\n# Create NULL arrays\ntrainPredictPlot = np.zeros(size)\ntestPredictPlot = np.zeros(size)\n\nfor i in range(size):\n    trainPredictPlot[i] = np.nan\n    testPredictPlot[i] = np.nan\n\n# Add predicted values to new arrays\n\nfor i in range(len(trainPredict)):\n    trainPredictPlot[look_back + i] = trainPredict[i]\n\nfor i in range(len(testPredict)):\n    testPredictPlot[len(trainPredict)+(look_back*2)+ i : len(italy_recovered)-1] = testPredict[i]\n\n# Create Dataframes for each and merge everything\ntrainPredictPlot = pd.DataFrame(trainPredictPlot, columns=['Train Predictions'])\ntestPredictPlot = pd.DataFrame(testPredictPlot, columns=['Test Predictions'])\n\nitaly_recv = pd.DataFrame(italy_recovered.values.astype(\"float\"), columns=['Actual Confirmed'])\nitaly_recv = italy_recv.join(trainPredictPlot)\nitaly_recv = italy_recv.join(testPredictPlot)\nitaly_recv.index = italy_recovered.index","execution_count":null,"outputs":[]},{"metadata":{"id":"TvMmP1M_34fV"},"cell_type":"markdown","source":"### Future Predictions"},{"metadata":{"id":"uMflVzSv36-e","trusted":true},"cell_type":"code","source":"dates_range = 15\nitaly_recv_preds= italy_recovered.copy()\nlength = italy_recovered.shape[0]\nitaly_recv_preds = italy_recv_preds.reset_index()\n\npreds = np.zeros(dates_range)\ndatelist = pd.date_range(italy.index[-1], periods=31)\n\nfor i in range(dates_range-1):\n    col = italy_recv_preds['Recovered']\n    value = col[-look_back:]\n    value = value.values.reshape(1, 1, look_back)\n    preds = model.predict(value)\n    df = pd.DataFrame([[datelist[i+1], preds[0,0]]], columns=['index', 'Recovered'] )\n    italy_recv_preds = italy_recv_preds.append(df, ignore_index=True)\n\nitaly_recv_preds = italy_recv_preds.set_index('index')","execution_count":null,"outputs":[]},{"metadata":{"id":"ScwuW6_B38C0","outputId":"77061ba9-dd02-4807-a5f0-7709f49cd5cd","trusted":true},"cell_type":"code","source":"italy_recv_preds.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"7Zeh_rQV67pS"},"cell_type":"markdown","source":"### For Deaths"},{"metadata":{"id":"eHSCWrHg6-2c","outputId":"26e9af2f-ef6e-4fd9-9acf-314551f535b6","trusted":true},"cell_type":"code","source":"# Split the series for training and testing\nsize = italy_dead.shape[0]\ntr =int(round(size*0.8))\nX_train, X_test = italy_dead[:tr] , italy_dead[tr:]\n\n# Reshape the series for further computations\nX_train = X_train.values.reshape(-1,1)\nX_test = X_test.values.reshape(-1,1)\n\n\n# Create windows\nlook_back = 6\ntrainX, trainY = create_dataset(X_train, look_back)\ntestX, testY = create_dataset(X_test, look_back)\n\n# reshape input to be [samples, time steps, features] for LSTM\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"outputId":"dbddd666-f81a-45cb-a9ca-5fcebacda0dc","id":"OOtfH7Zg5QKl","trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\n\nmodel.add(LSTM(200, input_shape=(1, look_back), activation='relu', dropout=0.2))\n\nmodel.add(Dense(1, activation=LeakyReLU(alpha=0.1)))\nmodel.summary()\n\nopt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )\n\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n\nstart = time.time()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)\nend = time.time()\n\nruntime = end-start\nprint('Runtime: ', runtime, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"id":"pTP4RjYG7HJz","trusted":true},"cell_type":"code","source":"# Make predictions using LSTM Model\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\n# shift train predictions for plotting\nsize = len(italy_dead)\n\n# Create NULL arrays\ntrainPredictPlot = np.zeros(size)\ntestPredictPlot = np.zeros(size)\n\nfor i in range(size):\n    trainPredictPlot[i] = np.nan\n    testPredictPlot[i] = np.nan\n\n# Add predicted values to new arrays\n\nfor i in range(len(trainPredict)):\n    trainPredictPlot[look_back + i] = trainPredict[i]\n\nfor i in range(len(testPredict)):\n    testPredictPlot[len(trainPredict)+(look_back*2)+ i : len(italy_dead)-1] = testPredict[i]\n\n# Create Dataframes for each and merge everything\ntrainPredictPlot = pd.DataFrame(trainPredictPlot, columns=['Train Predictions'])\ntestPredictPlot = pd.DataFrame(testPredictPlot, columns=['Test Predictions'])\n\nitaly_deads = pd.DataFrame(italy_dead.values.astype(\"float\"), columns=['Actual Deaths'])\nitaly_deads = italy_deads.join(trainPredictPlot)\nitaly_deads = italy_deads.join(testPredictPlot)\nitaly_deads.index = italy_dead.index","execution_count":null,"outputs":[]},{"metadata":{"id":"cjWJqBkV4uAk"},"cell_type":"markdown","source":"### Future Predictions"},{"metadata":{"id":"PL5Pbx-m4wrS","trusted":true},"cell_type":"code","source":"dates_range = 15\nitaly_dead_preds= italy_dead.copy()\nlength = italy_dead.shape[0]\nitaly_dead_preds = italy_dead_preds.reset_index()\n\npreds = np.zeros(dates_range)\ndatelist = pd.date_range(italy.index[-1], periods=31)\n\nfor i in range(dates_range-1):\n    col = italy_dead_preds['Deaths']\n    value = col[-look_back:]\n    value = value.values.reshape(1, 1, look_back)\n    preds = model.predict(value)\n    df = pd.DataFrame([[datelist[i+1], preds[0,0]]], columns=['index', 'Deaths'] )\n    italy_dead_preds = italy_dead_preds.append(df, ignore_index=True)\n\nitaly_dead_preds = italy_dead_preds.set_index('index')","execution_count":null,"outputs":[]},{"metadata":{"id":"VTchYA9PBENF","outputId":"78981078-044b-4cc3-bae4-61f060d8e47e","trusted":true},"cell_type":"code","source":"italy_dead_preds.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"HEwSRIIV9hyI"},"cell_type":"markdown","source":"# Plots for Italy\n\nWe generate plots for Italy. <br>\nOn the left, we plotted the Trained and tested predictions with the actual values for each category. <br>\n\nOn the right, we plotted the predicted future trends of the virus in Italy along with the actual values. Actual values stop at one point, which is the point until when the data was collected."},{"metadata":{"outputId":"03971922-e526-4605-8937-584689af7a8b","id":"EbWI61Fz7MqZ","trusted":true},"cell_type":"code","source":"# plot Confirmed actual cases and predictions\nfig = plt.figure(figsize=(17,22))\n\nplt.subplot(3,2,1)\nplt.plot(italy_conf.iloc[:,0], label=italy_conf.iloc[:,0].name, marker='o')\nplt.plot(italy_conf.iloc[:,1], label=italy_conf.iloc[:,1].name, marker='s')\nplt.plot(italy_conf.iloc[:,2], label=italy_conf.iloc[:,2].name, marker='v')\nplt.xlabel('Date', fontsize=15)\n# plt.ylabel('Number of Cases', fontsize=15)\nplt.title('For Confirmed Cases in Italy', fontsize=15)\nplt.legend()\n\n\nplt.subplot(3,2,2)\nplt.plot(italy_conf_preds.Confirmed, label='Predicted', marker='*')\nplt.plot(italy_conf.iloc[:,0], label='Actual', marker='o')\nplt.xlabel('Date', fontsize=15)\n# plt.ylabel('Number of Cases', fontsize=15)\nplt.title('Predictions for Confirmed Cases', fontsize=15)\nplt.legend()\n\n# plot Recovered actual cases and predictions\nplt.subplot(3,2,3)\nplt.plot(italy_recv.iloc[:,0], label=italy_recv.iloc[:,0].name, marker='o')\nplt.plot(italy_recv.iloc[:,1], label=italy_recv.iloc[:,1].name, marker='s')\nplt.plot(italy_recv.iloc[:,2], label=italy_recv.iloc[:,2].name, marker='v')\nplt.xlabel('Date', fontsize=15)\n# plt.ylabel('Number of Cases', fontsize=15)\nplt.title('For Recovered Cases in Italy', fontsize=15)\nplt.legend()\n\nplt.subplot(3,2,4)\nplt.plot(italy_recv_preds.Recovered, label='Predictions', marker='*')\nplt.plot(italy_recv.iloc[:,0], label='Actual', marker='o')\nplt.xlabel('Date', fontsize=15)\n# plt.ylabel('Number of Cases', fontsize=15)\nplt.title('Predictions for Recovered Cases', fontsize=15)\nplt.legend()\n\n# plot actual Deaths and predictions\nplt.subplot(3,2,5)\nplt.plot(italy_deads.iloc[:,0], label=italy_deads.iloc[:,0].name, marker='o')\nplt.plot(italy_deads.iloc[:,1], label=italy_deads.iloc[:,1].name, marker='s')\nplt.plot(italy_deads.iloc[:,2], label=italy_deads.iloc[:,2].name, marker='v')\nplt.xlabel('Date', fontsize=15)\n# plt.ylabel('Number of Cases', fontsize=15)\nplt.title('For Death Cases in Italy', fontsize=15)\nplt.legend()\n\nplt.subplot(3,2,6)\nplt.plot(italy_dead_preds.Deaths, label='Predictions', marker='*')\nplt.plot(italy_deads.iloc[:,0], label='Actual', marker='o')\nplt.xlabel('Date', fontsize=15)\nplt.title('Predictions for Deaths', fontsize=15)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"wyHSdRUw9917"},"cell_type":"markdown","source":"# World\n\nNow we shift our focus to the World demographics that are affected by COVID-19. <br>\n\nWe tune our LSTM model for the world data, generate predictions and plot those predictions against the actual values.<br>\nThese predictions generated try to foresee the next 20 days of disease spread, until April 22, 2020."},{"metadata":{"id":"-jKoDg5_CG3Z","trusted":true},"cell_type":"code","source":"world_confirmed = pd.DataFrame(world_conf)\nworld_confirmed = world_confirmed.rename(columns={0: 'Confirmed'})\n\nworld_recovered = pd.DataFrame(world_recv)\nworld_recovered = world_recovered.rename(columns={0: 'Recovered'})\n\nworld_dead = pd.DataFrame(world_dead)\nworld_dead = world_dead.rename(columns={0: 'Deaths'})\n","execution_count":null,"outputs":[]},{"metadata":{"id":"0fJUhpehEtpQ","outputId":"4e37f69b-8d44-4937-dae0-5bf50ceeac31","trusted":true},"cell_type":"code","source":"world_confirmed.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"wVkUvoUeBLcb"},"cell_type":"markdown","source":"## For Worldwide Confirmed Cases"},{"metadata":{"outputId":"06cd816f-ad24-4735-852a-31c0bc208a63","id":"cSuudw1HBrKQ","trusted":true},"cell_type":"code","source":"# Split the series for training and testing\nsize = world_confirmed.shape[0]\ntr =int(round(size*0.8))\nX_train, X_test = world_confirmed[:tr] , world_confirmed[tr:]\n\n# Reshape the series for further computations\nX_train = X_train.values.reshape(-1,1)\nX_test = X_test.values.reshape(-1,1)\n\n# Create train and test windows\nlook_back = 12\ntrainX, trainY = create_dataset(X_train, look_back)\ntestX, testY = create_dataset(X_test, look_back)\n\n# reshape input to be [samples, time steps, features] for LSTM\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"outputId":"868b6ca3-b765-4cf6-b40e-37691ec4f9a0","id":"fp89TP1OB2Z5","trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\n\nmodel.add(LSTM(30, input_shape=(1, look_back), activation='relu', dropout=0.3))\n\nmodel.add(Dense(1, activation=LeakyReLU(alpha=0.1)))\nmodel.summary()\n\nopt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )\n\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n\nstart = time.time()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)\nend = time.time()\n\nruntime = end-start\nprint('Runtime: ', runtime, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"id":"sfq4jMH8CA04","trusted":true},"cell_type":"code","source":"dates_range = 20\nworld_conf_preds= world_confirmed.copy()\nlength = world_confirmed.shape[0]\nworld_conf_preds = world_conf_preds.reset_index()\n\npreds = np.zeros(dates_range)\ndatelist = pd.date_range(world_conf.index[-1], periods=dates_range)\n\nfor i in range(dates_range-1):\n    value = world_conf_preds.iloc[:,1][-look_back:]\n    value = value.values.reshape(1, 1, look_back)\n    preds = model.predict(value)\n    df = pd.DataFrame([[datelist[i+1], preds[0,0]]], columns=['index', 'Confirmed'] )\n    world_conf_preds = world_conf_preds.append(df, ignore_index=True)\n\nworld_conf_preds = world_conf_preds.set_index('index')","execution_count":null,"outputs":[]},{"metadata":{"id":"citsw-upHrUj","outputId":"75fdd82a-b979-42c1-ec04-7c57203b36cb","trusted":true},"cell_type":"code","source":"world_conf_preds.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"94pz2f7JBP0a"},"cell_type":"markdown","source":"## For Worldwide Recovered Cases"},{"metadata":{"outputId":"1597ab1d-12cd-4a79-d8c3-f372584be27e","id":"JdpRrlCoBrZE","trusted":true},"cell_type":"code","source":"# Split the series for training and testing\nsize = world_recovered.shape[0]\ntr =int(round(size*0.8))\nX_train, X_test = world_recovered[:tr] , world_recovered[tr:]\n\n# Reshape the series for further computations\nX_train = X_train.values.reshape(-1,1)\nX_test = X_test.values.reshape(-1,1)\n\n# Create train and test windows\nlook_back = 12\ntrainX, trainY = create_dataset(X_train, look_back)\ntestX, testY = create_dataset(X_test, look_back)\n\n# reshape input to be [samples, time steps, features] for LSTM\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"outputId":"3df39264-dc8d-4b4b-834e-33988271a23d","id":"vitI2Z12B2md","trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\n\nmodel.add(LSTM(40, input_shape=(1, look_back), activation='relu', dropout=0.2))\n\nmodel.add(Dense(1, activation=LeakyReLU(alpha=0.1)))\nmodel.summary()\n\nopt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )\n\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n\nstart = time.time()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)\nend = time.time()\n\nruntime = end-start\nprint('Runtime: ', runtime, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"id":"ycJnp79qIM2o","trusted":true},"cell_type":"code","source":"dates_range = 20\nworld_recv_preds= world_recovered.copy()\nlength = world_recovered.shape[0]\nworld_recv_preds = world_recv_preds.reset_index()\n\npreds = np.zeros(dates_range)\ndatelist = pd.date_range(world_recv.index[-1], periods=dates_range)\n\nfor i in range(dates_range-1):\n    value = world_recv_preds.iloc[:,1][-look_back:]\n    value = value.values.reshape(1, 1, look_back)\n    preds = model.predict(value)\n    df = pd.DataFrame([[datelist[i+1], preds[0,0]]], columns=['index', 'Recovered'] )\n    world_recv_preds = world_recv_preds.append(df, ignore_index=True)\n\nworld_recv_preds = world_recv_preds.set_index('index')","execution_count":null,"outputs":[]},{"metadata":{"id":"WAU5uQLrIW6_","outputId":"aa6a3621-6f77-4904-a472-32ad0e1d2c3e","trusted":true},"cell_type":"code","source":"world_recv_preds.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"Qpb_p2cABQCy"},"cell_type":"markdown","source":"## For Worldwide Deaths"},{"metadata":{"outputId":"b2c8dd22-4786-401f-cefb-f1a4f903811c","id":"efEujtK5Brfp","trusted":true},"cell_type":"code","source":"# Split the series for training and testing\nsize = world_dead.shape[0]\ntr =int(round(size*0.8))\nX_train, X_test = world_dead[:tr] , world_dead[tr:]\n\n# Reshape the series for further computations\nX_train = X_train.values.reshape(-1,1)\nX_test = X_test.values.reshape(-1,1)\n\n# Create train and test windows\nlook_back = 7\ntrainX, trainY = create_dataset(X_train, look_back)\ntestX, testY = create_dataset(X_test, look_back)\n\n# reshape input to be [samples, time steps, features] for LSTM\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"outputId":"5c1888d8-48f7-44e9-a2f1-11cf4b718de1","id":"orgHvjtYB2xI","trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\n\nmodel.add(LSTM(30, input_shape=(1, look_back), activation='relu', dropout=0.2))\n\nmodel.add(Dense(1, activation=LeakyReLU(alpha=0.1)))\nmodel.summary()\n\nopt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )\n\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n\nstart = time.time()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)\nend = time.time()\n\nruntime = end-start\nprint('Runtime: ', runtime, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"id":"UCvcvuFkIo1a","trusted":true},"cell_type":"code","source":"dates_range = 20\nworld_dead_preds= world_dead.copy()\nlength = world_dead.shape[0]\nworld_dead_preds = world_dead_preds.reset_index()\n\npreds = np.zeros(dates_range)\ndatelist = pd.date_range(world_dead.index[-1], periods=dates_range)\n\nfor i in range(dates_range-1):\n    value = world_dead_preds.iloc[:,1][-look_back:]\n    value = value.values.reshape(1, 1, look_back)\n    preds = model.predict(value)\n    df = pd.DataFrame([[datelist[i+1], preds[0,0]]], columns=['index', 'Deaths'] )\n    world_dead_preds = world_dead_preds.append(df, ignore_index=True)\n\nworld_dead_preds = world_dead_preds.set_index('index')","execution_count":null,"outputs":[]},{"metadata":{"id":"xFhIkOuD-vVi","outputId":"d521e077-a871-4369-83ad-de658a72c3d2","trusted":true},"cell_type":"code","source":"world_dead_preds.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZVOwruVTI2XX"},"cell_type":"markdown","source":"# Plots for the World"},{"metadata":{"id":"HCltH7Fp-wCU","outputId":"d675d4da-0e0e-4878-e34d-367a8c8c95fe","trusted":true},"cell_type":"code","source":"# plot Confirmed actual cases and predictions\nfig = plt.figure(figsize=(12,20))\n\nplt.subplot(3,1,1)\nplt.plot(world_conf_preds['Confirmed'], label='Predictions', marker='*')\nplt.plot(world_confirmed['Confirmed'], label='Actual', marker='o')\nplt.xlabel('Date', fontsize=15)\nplt.ylabel('Cases Count', fontsize=15)\nplt.title('World Confirmed Cases ', fontsize=15)\nplt.legend()\n\n\nplt.subplot(3,1,2)\nplt.plot(world_recv_preds['Recovered'], label='Predictions', marker='*')\nplt.plot(world_recovered['Recovered'], label='Actual', marker='o')\nplt.xlabel('Date', fontsize=15)\nplt.ylabel('Cases Count', fontsize=15)\nplt.title('World Recovered Cases ', fontsize=15)\nplt.legend()\n\nplt.subplot(3,1,3)\nplt.plot(world_dead_preds['Deaths'], label='Predictions', marker='*')\nplt.plot(world_dead['Deaths'], label='Actual', marker='o')\nplt.xlabel('Date', fontsize=15)\nplt.ylabel('Cases Count', fontsize=15)\nplt.title('World Deaths', fontsize=15)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"e68Mh1ZgW1iN"},"cell_type":"markdown","source":"# Final Thoughts"},{"metadata":{"id":"QAfGVPg9W3hD"},"cell_type":"markdown","source":"\n\n\n*   The Final day in the training dataset is April 17, 2020 and the final day for submission of this assignment was April 22, 2020. \n\n*   So,we used this opportunity to compare our model to real world data and draw insightful comparisons.\n\n\n*   The Prediction plots of Italy overestimate the spread of the virus. They predict far more confirmed cases, recovered cases and deaths than in real life.\n\n*   But our model does a good job of estimating the increasing trend that is observed in real life as well.\n\n\n\n*   For the World Predictions, we notice a similar trend that is happening in the world right now.\n\n*   The numbers are continuing to increase, but **let's hope that our model is wrong and this pandemic ends soon!**\n\n\n\n\n\n"}],"metadata":{"colab":{"name":"covid19.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}