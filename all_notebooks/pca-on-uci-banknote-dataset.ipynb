{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input/\"))\n\n#Data from\n#http://archive.ics.uci.edu/ml/datasets/banknote+authentication\n\n#opening file\ndf=pd.read_csv('../input/data_banknote_authentication.csv')\ndf = df.rename(columns=({'3.6216':'variance','8.6661':'skewness','-2.8073':'kurtosis','-0.44699':'entropy','0':'class'}))\n#applying class output labels\ndf['class'] = np.where(df['class']==1, 'genuine', 'forged')#put 'genuine' if it is 1 labeled and 'forged' otherwise\ndf.head()\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Splitting the class table into data X and class label y\nX=df.ix[:,0:4].values  #(1371, 4)\ny=df.ix[:,4].values #(1371,)\n\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualizing the histograms of the features\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport math\n\nfeature_dict={0:'variance',\n            1:'skewness',\n            2:'kurtosis',\n            3:'entropy'\n            }\nlabel_dict={1:'genuine',\n            2:'forged'\n            }\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(8,6))\n    for cnt in range(X.shape[1]):#num of cols in design matrix\n        plt.subplot(2,2,cnt+1)\n        for lab in ('genuine','forged'):\n            plt.hist(X[y==lab,cnt],\n            label=lab,\n            bins=10,\n            alpha=0.3,)\n        plt.xlabel(feature_dict[cnt])\n    plt.legend(loc='upper-right',fancybox=True,fontsize=8)\n    \n    plt.tight_layout()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardizing\nfrom sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA Starts here\n\n#Step 1: Eigendecomposition- Computing Eigenvectors and Eigenvalues\n\n#finding covariance (manually)\nmean_vec=np.mean(X_std,axis=0)#find mean in row-wise (axis=0) for each column..will be a (1,X_std_col) dimensional vector\ncov_mat=((X_std-mean_vec).T.dot(X_std-mean_vec))/(X_std.shape[0]-1)#cov_mat will be a (X_std[1],X_std[1]) dimensional matrix...that is a (#features,#features) dimensional matrix    \nprint('Covariance matrix \\n%s' %cov_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Or we could have used the numpy's covariance finding function called 'cov()' to do the same task...both will yield the same result   \nprint('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next we perform an eiendecomposition on the covariance matrix\ncov_mat=np.cov(X_std.T)#Covariance finding using NumPy\neig_vals,eig_vecs=np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)#a (#features,#features) matrix\nprint('\\nEigenvalues \\n%s' %eig_vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In the field of Finance the covarrelation matrix is more used then covariance matrix, like, if we have found out the correlation matrix   \ncor_mat1 = np.corrcoef(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cor_mat1)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Eigendecomposition of the raw data based on the correlation matrix\ncor_mat2=np.corrcoef(X.T)\neig_val,eig_vecs=np.linalg.eig(cor_mat2)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observation: Eigendecomposition of the covariance matrix on standard data is same as eigendecomposition of correlation matrix on standard or non standard data...i.e, correlation matrix doesn't care if the data is standardized or not    \n#correlation_between_two_variables=(covariance_between_two_variables/multiplication_of_these_variables'_standard_deviation)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Singular vector decomposition\n#Although eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency. So, let us perform an SVD to confirm that the result are indeed the same:   \n\n#u,s,v = np.linalg.svd(X_std.T)\n#u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 2: Selecting Principle Components\n\n#Sorting eigenparis:\n\n#First making sure the eigenvectors have all the same unit (1) length...as their task only to show the direction\n#Eigenvectors will form axes in new subspace...taking a few of them will approximate the original dimensions by occupying less memory   \n\nfor ev in eig_vecs.T:\n    for ev in eig_vecs.T:\n        np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\nprint('Everything ok!')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To choose which eigenvector(s) can be dropped (to reduce dimension) without lossing too much information we have to sort them w.r.t corresponding eigenvalues  \n#If we take all the eigenvectors it will exactly mimic the real dimensions but as we want to reduce dimension so we have to discard less important eigenvectors (having corresponding lower eigenvalues)    \n#lower eigenvalue means data points on graph doesn't vary too much towards that corresponding eigenvector..i.e, that eigenvector doesn't posess so much information and it is safe to discard it\n#although discarding eigenvectors will make it unable to exactly mimic the real dimensions (i.e, real information)...but as we want to reduce dimension (i.e, reduce space complixity) so we have to compensate through the loss   \n\n#Making a list of (eigenvalue, eigenvector) tuples\neig_pairs=[(np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]#..pair like (eval,array[corresponding_evec])  \n#Sorting the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding Explained Variance (a calculation to decide how many principle components we are going to choose from new feature subspace)   \ntot=sum(eig_vals)\nvar_exp=[(i/tot)*100 for i in sorted(eig_vals,reverse=True)]#descending-ordered sorted\ncum_var_exp=np.cumsum(var_exp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As the first two PCs are having larger variances so taking only them will ensure a good approximation to the data\n#N.B: eigenvectors are direction cosines for principal components, while eigenvalues are the magnitude (the variance) in the principal components.  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Projection Matrix (basically just a matrix of our top k eigenvectors...associated with corresponding eigenvalues in \"eig_pairs\" variable)    \n#Here reducing 4 dimensional feature space to 2 dimensional feature subspace, by choosing \"top2\" eigenvectors with the highest eigenvalues to construct our dxk-dimensional eigenvector matrix W   \n\nmatrix_w=np.hstack((eig_pairs[0][1].reshape(4,1),\n                    eig_pairs[1][1].reshape(4,1)))#eig_pair[index][eig_val=0 or eig_vec=1]...reshape is converting them to a (4x1) dimensional row matrix  \n                   #hstak will concatenate these row matrices together side by side\nprint('Matrix W:\\n',matrix_w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 3: Projection onto the feature space\n#In the last step we got 4x2-dimensional projection  matrix W. Now we will use it to transform our samples onto the new dimensional space via the equation:  \n#Y=X x W, where Y will be a (X_row x W_col) diensional matrix of our transformed samples\n\nY=X_std.dot(matrix_w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6,4))\n    for lab,col in zip(('genuine','forged'),('green','red')):\n        plt.scatter(Y[y==lab,0],\n                    Y[y==lab,1],\n                    label=lab,\n                    c=col)#taking for two PCs from corresponding two cols 0 and 1 when correnponding class label from Y is matched with the given class label y   \n    plt.xlabel('Principle Component 1')\n    plt.ylabel('Principle Component 2')\n    plt.legend(loc='lower-center')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using scikit-learn we can do the same thing in a very short length of manual coding\nfrom sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca=sklearnPCA(n_components=2)#number of components=2\nY_sklearn=sklearn_pca.fit_transform(X_std)#It will take the X_std and do everything to reduce the dimensions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now displaying the result got by using scikit-learn library\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6,4))\n    for lab,col in zip(('genuine','forged'),('green','red')):\n        plt.scatter(Y_sklearn[y==lab,0],\n                    Y_sklearn[y==lab,1],\n                    label=lab,\n                    c=col)#taking for two PCs from corresponding two cols 0 and 1 when correnponding class label from Y is matched with the given class label y   \n    plt.xlabel('Principle Component 1')\n    plt.ylabel('Principle Component 2')\n    plt.legend(loc='lower-center')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observation: Both yielding the same result. So the implementation is correct","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}